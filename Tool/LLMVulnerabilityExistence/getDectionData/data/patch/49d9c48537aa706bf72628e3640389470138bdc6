{
    "client/python/gradio_client/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 895,
                "afterPatchRowNumber": 895,
                "PatchRowcode": "         raise APIInfoParseError(f\"Cannot parse type for {schema}\")"
            },
            "1": {
                "beforePatchRowNumber": 896,
                "afterPatchRowNumber": 896,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 897,
                "afterPatchRowNumber": 897,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 898,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-FILE_DATA = \"Dict(path: str, url: str | None, size: int | None, orig_name: str | None, mime_type: str | None)\""
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 898,
                "PatchRowcode": "+FILE_DATA = \"Dict(path: str, url: str | None, size: int | None, orig_name: str | None, mime_type: str | None, is_stream: bool)\""
            },
            "5": {
                "beforePatchRowNumber": 899,
                "afterPatchRowNumber": 899,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 900,
                "afterPatchRowNumber": 900,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 901,
                "afterPatchRowNumber": 901,
                "PatchRowcode": " def json_schema_to_python_type(schema: Any) -> str:"
            },
            "8": {
                "beforePatchRowNumber": 1010,
                "afterPatchRowNumber": 1010,
                "PatchRowcode": "     return isinstance(d, dict) and \"path\" in d"
            },
            "9": {
                "beforePatchRowNumber": 1011,
                "afterPatchRowNumber": 1011,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 1012,
                "afterPatchRowNumber": 1012,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1013,
                "PatchRowcode": "+def is_file_obj_with_url(d):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1014,
                "PatchRowcode": "+    return ("
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1015,
                "PatchRowcode": "+        isinstance(d, dict) and \"path\" in d and \"url\" in d and isinstance(d[\"url\"], str)"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1016,
                "PatchRowcode": "+    )"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1017,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1018,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": 1013,
                "afterPatchRowNumber": 1019,
                "PatchRowcode": " SKIP_COMPONENTS = {"
            },
            "18": {
                "beforePatchRowNumber": 1014,
                "afterPatchRowNumber": 1020,
                "PatchRowcode": "     \"state\","
            },
            "19": {
                "beforePatchRowNumber": 1015,
                "afterPatchRowNumber": 1021,
                "PatchRowcode": "     \"row\","
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import base64",
            "import copy",
            "import hashlib",
            "import json",
            "import mimetypes",
            "import os",
            "import pkgutil",
            "import secrets",
            "import shutil",
            "import tempfile",
            "import warnings",
            "from concurrent.futures import CancelledError",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "from enum import Enum",
            "from pathlib import Path",
            "from threading import Lock",
            "from typing import Any, Callable, Literal, Optional, TypedDict",
            "",
            "import fsspec.asyn",
            "import httpx",
            "import huggingface_hub",
            "from huggingface_hub import SpaceStage",
            "from websockets.legacy.protocol import WebSocketCommonProtocol",
            "",
            "API_URL = \"api/predict/\"",
            "SSE_URL_V0 = \"queue/join\"",
            "SSE_DATA_URL_V0 = \"queue/data\"",
            "SSE_URL = \"queue/data\"",
            "SSE_DATA_URL = \"queue/join\"",
            "WS_URL = \"queue/join\"",
            "UPLOAD_URL = \"upload\"",
            "LOGIN_URL = \"login\"",
            "CONFIG_URL = \"config\"",
            "API_INFO_URL = \"info\"",
            "RAW_API_INFO_URL = \"info?serialize=False\"",
            "SPACE_FETCHER_URL = \"https://gradio-space-api-fetcher-v2.hf.space/api\"",
            "RESET_URL = \"reset\"",
            "SPACE_URL = \"https://hf.space/{}\"",
            "",
            "STATE_COMPONENT = \"state\"",
            "INVALID_RUNTIME = [",
            "    SpaceStage.NO_APP_FILE,",
            "    SpaceStage.CONFIG_ERROR,",
            "    SpaceStage.BUILD_ERROR,",
            "    SpaceStage.RUNTIME_ERROR,",
            "    SpaceStage.PAUSED,",
            "]",
            "",
            "",
            "class Message(TypedDict, total=False):",
            "    msg: str",
            "    output: dict[str, Any]",
            "    event_id: str",
            "    rank: int",
            "    rank_eta: float",
            "    queue_size: int",
            "    success: bool",
            "    progress_data: list[dict]",
            "    log: str",
            "    level: str",
            "",
            "",
            "def get_package_version() -> str:",
            "    try:",
            "        package_json_data = (",
            "            pkgutil.get_data(__name__, \"package.json\").decode(\"utf-8\").strip()  # type: ignore",
            "        )",
            "        package_data = json.loads(package_json_data)",
            "        version = package_data.get(\"version\", \"\")",
            "        return version",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "__version__ = get_package_version()",
            "",
            "",
            "class TooManyRequestsError(Exception):",
            "    \"\"\"Raised when the API returns a 429 status code.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class QueueError(Exception):",
            "    \"\"\"Raised when the queue is full or there is an issue adding a job to the queue.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class InvalidAPIEndpointError(Exception):",
            "    \"\"\"Raised when the API endpoint is invalid.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class SpaceDuplicationError(Exception):",
            "    \"\"\"Raised when something goes wrong with a Space Duplication.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class ServerMessage(str, Enum):",
            "    send_hash = \"send_hash\"",
            "    queue_full = \"queue_full\"",
            "    estimation = \"estimation\"",
            "    send_data = \"send_data\"",
            "    process_starts = \"process_starts\"",
            "    process_generating = \"process_generating\"",
            "    process_completed = \"process_completed\"",
            "    log = \"log\"",
            "    progress = \"progress\"",
            "    heartbeat = \"heartbeat\"",
            "    server_stopped = \"server_stopped\"",
            "",
            "",
            "class Status(Enum):",
            "    \"\"\"Status codes presented to client users.\"\"\"",
            "",
            "    STARTING = \"STARTING\"",
            "    JOINING_QUEUE = \"JOINING_QUEUE\"",
            "    QUEUE_FULL = \"QUEUE_FULL\"",
            "    IN_QUEUE = \"IN_QUEUE\"",
            "    SENDING_DATA = \"SENDING_DATA\"",
            "    PROCESSING = \"PROCESSING\"",
            "    ITERATING = \"ITERATING\"",
            "    PROGRESS = \"PROGRESS\"",
            "    FINISHED = \"FINISHED\"",
            "    CANCELLED = \"CANCELLED\"",
            "    LOG = \"LOG\"",
            "",
            "    @staticmethod",
            "    def ordering(status: Status) -> int:",
            "        \"\"\"Order of messages. Helpful for testing.\"\"\"",
            "        order = [",
            "            Status.STARTING,",
            "            Status.JOINING_QUEUE,",
            "            Status.QUEUE_FULL,",
            "            Status.IN_QUEUE,",
            "            Status.SENDING_DATA,",
            "            Status.PROCESSING,",
            "            Status.PROGRESS,",
            "            Status.ITERATING,",
            "            Status.FINISHED,",
            "            Status.CANCELLED,",
            "        ]",
            "        return order.index(status)",
            "",
            "    def __lt__(self, other: Status):",
            "        return self.ordering(self) < self.ordering(other)",
            "",
            "    @staticmethod",
            "    def msg_to_status(msg: str) -> Status:",
            "        \"\"\"Map the raw message from the backend to the status code presented to users.\"\"\"",
            "        return {",
            "            ServerMessage.send_hash: Status.JOINING_QUEUE,",
            "            ServerMessage.queue_full: Status.QUEUE_FULL,",
            "            ServerMessage.estimation: Status.IN_QUEUE,",
            "            ServerMessage.send_data: Status.SENDING_DATA,",
            "            ServerMessage.process_starts: Status.PROCESSING,",
            "            ServerMessage.process_generating: Status.ITERATING,",
            "            ServerMessage.process_completed: Status.FINISHED,",
            "            ServerMessage.progress: Status.PROGRESS,",
            "            ServerMessage.log: Status.LOG,",
            "            ServerMessage.server_stopped: Status.FINISHED,",
            "        }[msg]  # type: ignore",
            "",
            "",
            "@dataclass",
            "class ProgressUnit:",
            "    index: Optional[int]",
            "    length: Optional[int]",
            "    unit: Optional[str]",
            "    progress: Optional[float]",
            "    desc: Optional[str]",
            "",
            "    @classmethod",
            "    def from_msg(cls, data: list[dict]) -> list[ProgressUnit]:",
            "        return [",
            "            cls(",
            "                index=d.get(\"index\"),",
            "                length=d.get(\"length\"),",
            "                unit=d.get(\"unit\"),",
            "                progress=d.get(\"progress\"),",
            "                desc=d.get(\"desc\"),",
            "            )",
            "            for d in data",
            "        ]",
            "",
            "",
            "@dataclass",
            "class StatusUpdate:",
            "    \"\"\"Update message sent from the worker thread to the Job on the main thread.\"\"\"",
            "",
            "    code: Status",
            "    rank: int | None",
            "    queue_size: int | None",
            "    eta: float | None",
            "    success: bool | None",
            "    time: datetime | None",
            "    progress_data: list[ProgressUnit] | None",
            "    log: tuple[str, str] | None = None",
            "",
            "",
            "def create_initial_status_update():",
            "    return StatusUpdate(",
            "        code=Status.STARTING,",
            "        rank=None,",
            "        queue_size=None,",
            "        eta=None,",
            "        success=None,",
            "        time=datetime.now(),",
            "        progress_data=None,",
            "    )",
            "",
            "",
            "@dataclass",
            "class JobStatus:",
            "    \"\"\"The job status.",
            "",
            "    Keeps track of the latest status update and intermediate outputs (not yet implements).",
            "    \"\"\"",
            "",
            "    latest_status: StatusUpdate = field(default_factory=create_initial_status_update)",
            "    outputs: list[Any] = field(default_factory=list)",
            "",
            "",
            "@dataclass",
            "class Communicator:",
            "    \"\"\"Helper class to help communicate between the worker thread and main thread.\"\"\"",
            "",
            "    lock: Lock",
            "    job: JobStatus",
            "    prediction_processor: Callable[..., tuple]",
            "    reset_url: str",
            "    should_cancel: bool = False",
            "    event_id: str | None = None",
            "",
            "",
            "########################",
            "# Network utils",
            "########################",
            "",
            "",
            "def is_http_url_like(possible_url: str) -> bool:",
            "    \"\"\"",
            "    Check if the given string looks like an HTTP(S) URL.",
            "    \"\"\"",
            "    return possible_url.startswith((\"http://\", \"https://\"))",
            "",
            "",
            "def probe_url(possible_url: str) -> bool:",
            "    \"\"\"",
            "    Probe the given URL to see if it responds with a 200 status code (to HEAD, then to GET).",
            "    \"\"\"",
            "    headers = {\"User-Agent\": \"gradio (https://gradio.app/; gradio-team@huggingface.co)\"}",
            "    try:",
            "        with httpx.Client() as client:",
            "            head_request = client.head(possible_url, headers=headers)",
            "            if head_request.status_code == 405:",
            "                return client.get(possible_url, headers=headers).is_success",
            "            return head_request.is_success",
            "    except Exception:",
            "        return False",
            "",
            "",
            "def is_valid_url(possible_url: str) -> bool:",
            "    \"\"\"",
            "    Check if the given string is a valid URL.",
            "    \"\"\"",
            "    warnings.warn(",
            "        \"is_valid_url should not be used. \"",
            "        \"Use is_http_url_like() and probe_url(), as suitable, instead.\",",
            "    )",
            "    return is_http_url_like(possible_url) and probe_url(possible_url)",
            "",
            "",
            "async def get_pred_from_ws(",
            "    websocket: WebSocketCommonProtocol,",
            "    data: str,",
            "    hash_data: str,",
            "    helper: Communicator | None = None,",
            ") -> dict[str, Any]:",
            "    completed = False",
            "    resp = {}",
            "    while not completed:",
            "        # Receive message in the background so that we can",
            "        # cancel even while running a long pred",
            "        task = asyncio.create_task(websocket.recv())",
            "        while not task.done():",
            "            if helper:",
            "                with helper.lock:",
            "                    if helper.should_cancel:",
            "                        # Need to reset the iterator state since the client",
            "                        # will not reset the session",
            "                        async with httpx.AsyncClient() as http:",
            "                            reset = http.post(",
            "                                helper.reset_url, json=json.loads(hash_data)",
            "                            )",
            "                            # Retrieve cancel exception from task",
            "                            # otherwise will get nasty warning in console",
            "                            task.cancel()",
            "                            await asyncio.gather(task, reset, return_exceptions=True)",
            "                        raise CancelledError()",
            "            # Need to suspend this coroutine so that task actually runs",
            "            await asyncio.sleep(0.01)",
            "        msg = task.result()",
            "        resp = json.loads(msg)",
            "        if helper:",
            "            with helper.lock:",
            "                has_progress = \"progress_data\" in resp",
            "                status_update = StatusUpdate(",
            "                    code=Status.msg_to_status(resp[\"msg\"]),",
            "                    queue_size=resp.get(\"queue_size\"),",
            "                    rank=resp.get(\"rank\", None),",
            "                    success=resp.get(\"success\"),",
            "                    time=datetime.now(),",
            "                    eta=resp.get(\"rank_eta\"),",
            "                    progress_data=ProgressUnit.from_msg(resp[\"progress_data\"])",
            "                    if has_progress",
            "                    else None,",
            "                )",
            "                output = resp.get(\"output\", {}).get(\"data\", [])",
            "                if output and status_update.code != Status.FINISHED:",
            "                    try:",
            "                        result = helper.prediction_processor(*output)",
            "                    except Exception as e:",
            "                        result = [e]",
            "                    helper.job.outputs.append(result)",
            "                helper.job.latest_status = status_update",
            "        if resp[\"msg\"] == \"queue_full\":",
            "            raise QueueError(\"Queue is full! Please try again.\")",
            "        if resp[\"msg\"] == \"send_hash\":",
            "            await websocket.send(hash_data)",
            "        elif resp[\"msg\"] == \"send_data\":",
            "            await websocket.send(data)",
            "        completed = resp[\"msg\"] == \"process_completed\"",
            "    return resp[\"output\"]",
            "",
            "",
            "async def get_pred_from_sse_v0(",
            "    client: httpx.AsyncClient,",
            "    data: dict,",
            "    hash_data: dict,",
            "    helper: Communicator,",
            "    sse_url: str,",
            "    sse_data_url: str,",
            "    headers: dict[str, str],",
            "    cookies: dict[str, str] | None,",
            ") -> dict[str, Any] | None:",
            "    done, pending = await asyncio.wait(",
            "        [",
            "            asyncio.create_task(check_for_cancel(helper, headers, cookies)),",
            "            asyncio.create_task(",
            "                stream_sse_v0(",
            "                    client,",
            "                    data,",
            "                    hash_data,",
            "                    helper,",
            "                    sse_url,",
            "                    sse_data_url,",
            "                    headers,",
            "                    cookies,",
            "                )",
            "            ),",
            "        ],",
            "        return_when=asyncio.FIRST_COMPLETED,",
            "    )",
            "",
            "    for task in pending:",
            "        task.cancel()",
            "        try:",
            "            await task",
            "        except asyncio.CancelledError:",
            "            pass",
            "",
            "    assert len(done) == 1",
            "    for task in done:",
            "        return task.result()",
            "",
            "",
            "async def get_pred_from_sse_v1_v2(",
            "    helper: Communicator,",
            "    headers: dict[str, str],",
            "    cookies: dict[str, str] | None,",
            "    pending_messages_per_event: dict[str, list[Message | None]],",
            "    event_id: str,",
            "    protocol: Literal[\"sse_v1\", \"sse_v2\"],",
            ") -> dict[str, Any] | None:",
            "    done, pending = await asyncio.wait(",
            "        [",
            "            asyncio.create_task(check_for_cancel(helper, headers, cookies)),",
            "            asyncio.create_task(",
            "                stream_sse_v1_v2(helper, pending_messages_per_event, event_id, protocol)",
            "            ),",
            "        ],",
            "        return_when=asyncio.FIRST_COMPLETED,",
            "    )",
            "",
            "    for task in pending:",
            "        task.cancel()",
            "        try:",
            "            await task",
            "        except asyncio.CancelledError:",
            "            pass",
            "",
            "    assert len(done) == 1",
            "    for task in done:",
            "        exception = task.exception()",
            "        if exception:",
            "            raise exception",
            "        return task.result()",
            "",
            "",
            "async def check_for_cancel(",
            "    helper: Communicator, headers: dict[str, str], cookies: dict[str, str] | None",
            "):",
            "    while True:",
            "        await asyncio.sleep(0.05)",
            "        with helper.lock:",
            "            if helper.should_cancel:",
            "                break",
            "    if helper.event_id:",
            "        async with httpx.AsyncClient() as http:",
            "            await http.post(",
            "                helper.reset_url,",
            "                json={\"event_id\": helper.event_id},",
            "                headers=headers,",
            "                cookies=cookies,",
            "            )",
            "    raise CancelledError()",
            "",
            "",
            "async def stream_sse_v0(",
            "    client: httpx.AsyncClient,",
            "    data: dict,",
            "    hash_data: dict,",
            "    helper: Communicator,",
            "    sse_url: str,",
            "    sse_data_url: str,",
            "    headers: dict[str, str],",
            "    cookies: dict[str, str] | None,",
            ") -> dict[str, Any]:",
            "    try:",
            "        async with client.stream(",
            "            \"GET\",",
            "            sse_url,",
            "            params=hash_data,",
            "            headers=headers,",
            "            cookies=cookies,",
            "        ) as response:",
            "            async for line in response.aiter_lines():",
            "                line = line.rstrip(\"\\n\")",
            "                if len(line) == 0:",
            "                    continue",
            "                if line.startswith(\"data:\"):",
            "                    resp = json.loads(line[5:])",
            "                    if resp[\"msg\"] in [ServerMessage.log, ServerMessage.heartbeat]:",
            "                        continue",
            "                    with helper.lock:",
            "                        has_progress = \"progress_data\" in resp",
            "                        status_update = StatusUpdate(",
            "                            code=Status.msg_to_status(resp[\"msg\"]),",
            "                            queue_size=resp.get(\"queue_size\"),",
            "                            rank=resp.get(\"rank\", None),",
            "                            success=resp.get(\"success\"),",
            "                            time=datetime.now(),",
            "                            eta=resp.get(\"rank_eta\"),",
            "                            progress_data=ProgressUnit.from_msg(resp[\"progress_data\"])",
            "                            if has_progress",
            "                            else None,",
            "                        )",
            "                        output = resp.get(\"output\", {}).get(\"data\", [])",
            "                        if output and status_update.code != Status.FINISHED:",
            "                            try:",
            "                                result = helper.prediction_processor(*output)",
            "                            except Exception as e:",
            "                                result = [e]",
            "                            helper.job.outputs.append(result)",
            "                        helper.job.latest_status = status_update",
            "",
            "                    if resp[\"msg\"] == \"queue_full\":",
            "                        raise QueueError(\"Queue is full! Please try again.\")",
            "                    elif resp[\"msg\"] == \"send_data\":",
            "                        event_id = resp[\"event_id\"]",
            "                        helper.event_id = event_id",
            "                        req = await client.post(",
            "                            sse_data_url,",
            "                            json={\"event_id\": event_id, **data, **hash_data},",
            "                            headers=headers,",
            "                            cookies=cookies,",
            "                        )",
            "                        req.raise_for_status()",
            "                    elif resp[\"msg\"] == \"process_completed\":",
            "                        return resp[\"output\"]",
            "                else:",
            "                    raise ValueError(f\"Unexpected message: {line}\")",
            "        raise ValueError(\"Did not receive process_completed message.\")",
            "    except asyncio.CancelledError:",
            "        raise",
            "",
            "",
            "async def stream_sse_v1_v2(",
            "    helper: Communicator,",
            "    pending_messages_per_event: dict[str, list[Message | None]],",
            "    event_id: str,",
            "    protocol: Literal[\"sse_v1\", \"sse_v2\"],",
            ") -> dict[str, Any]:",
            "    try:",
            "        pending_messages = pending_messages_per_event[event_id]",
            "        pending_responses_for_diffs = None",
            "",
            "        while True:",
            "            if len(pending_messages) > 0:",
            "                msg = pending_messages.pop(0)",
            "            else:",
            "                await asyncio.sleep(0.05)",
            "                continue",
            "",
            "            if msg is None:",
            "                raise CancelledError()",
            "",
            "            with helper.lock:",
            "                log_message = None",
            "                if msg[\"msg\"] == ServerMessage.log:",
            "                    log = msg.get(\"log\")",
            "                    level = msg.get(\"level\")",
            "                    if log and level:",
            "                        log_message = (log, level)",
            "                status_update = StatusUpdate(",
            "                    code=Status.msg_to_status(msg[\"msg\"]),",
            "                    queue_size=msg.get(\"queue_size\"),",
            "                    rank=msg.get(\"rank\", None),",
            "                    success=msg.get(\"success\"),",
            "                    time=datetime.now(),",
            "                    eta=msg.get(\"rank_eta\"),",
            "                    progress_data=ProgressUnit.from_msg(msg[\"progress_data\"])",
            "                    if \"progress_data\" in msg",
            "                    else None,",
            "                    log=log_message,",
            "                )",
            "                output = msg.get(\"output\", {}).get(\"data\", [])",
            "                if (",
            "                    msg[\"msg\"] == ServerMessage.process_generating",
            "                    and protocol == \"sse_v2\"",
            "                ):",
            "                    if pending_responses_for_diffs is None:",
            "                        pending_responses_for_diffs = list(output)",
            "                    else:",
            "                        for i, value in enumerate(output):",
            "                            prev_output = pending_responses_for_diffs[i]",
            "                            new_output = apply_diff(prev_output, value)",
            "                            pending_responses_for_diffs[i] = new_output",
            "                            output[i] = new_output",
            "",
            "                if output and status_update.code != Status.FINISHED:",
            "                    try:",
            "                        result = helper.prediction_processor(*output)",
            "                    except Exception as e:",
            "                        result = [e]",
            "                    helper.job.outputs.append(result)",
            "                helper.job.latest_status = status_update",
            "            if msg[\"msg\"] == ServerMessage.process_completed:",
            "                del pending_messages_per_event[event_id]",
            "                return msg[\"output\"]",
            "            elif msg[\"msg\"] == ServerMessage.server_stopped:",
            "                raise ValueError(\"Server stopped.\")",
            "",
            "    except asyncio.CancelledError:",
            "        raise",
            "",
            "",
            "def apply_diff(obj, diff):",
            "    obj = copy.deepcopy(obj)",
            "",
            "    def apply_edit(target, path, action, value):",
            "        if len(path) == 0:",
            "            if action == \"replace\":",
            "                return value",
            "            elif action == \"append\":",
            "                return target + value",
            "            else:",
            "                raise ValueError(f\"Unsupported action: {action}\")",
            "",
            "        current = target",
            "        for i in range(len(path) - 1):",
            "            current = current[path[i]]",
            "",
            "        last_path = path[-1]",
            "        if action == \"replace\":",
            "            current[last_path] = value",
            "        elif action == \"append\":",
            "            current[last_path] += value",
            "        elif action == \"add\":",
            "            if isinstance(current, list):",
            "                current.insert(int(last_path), value)",
            "            else:",
            "                current[last_path] = value",
            "        elif action == \"delete\":",
            "            if isinstance(current, list):",
            "                del current[int(last_path)]",
            "            else:",
            "                del current[last_path]",
            "        else:",
            "            raise ValueError(f\"Unknown action: {action}\")",
            "",
            "        return target",
            "",
            "    for action, path, value in diff:",
            "        obj = apply_edit(obj, path, action, value)",
            "",
            "    return obj",
            "",
            "",
            "########################",
            "# Data processing utils",
            "########################",
            "",
            "",
            "def download_file(",
            "    url_path: str,",
            "    dir: str,",
            "    hf_token: str | None = None,",
            ") -> str:",
            "    if dir is not None:",
            "        os.makedirs(dir, exist_ok=True)",
            "    headers = {\"Authorization\": \"Bearer \" + hf_token} if hf_token else {}",
            "",
            "    sha1 = hashlib.sha1()",
            "    temp_dir = Path(tempfile.gettempdir()) / secrets.token_hex(20)",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    with httpx.stream(",
            "        \"GET\", url_path, headers=headers, follow_redirects=True",
            "    ) as response:",
            "        response.raise_for_status()",
            "        with open(temp_dir / Path(url_path).name, \"wb\") as f:",
            "            for chunk in response.iter_bytes(chunk_size=128 * sha1.block_size):",
            "                sha1.update(chunk)",
            "                f.write(chunk)",
            "",
            "    directory = Path(dir) / sha1.hexdigest()",
            "    directory.mkdir(exist_ok=True, parents=True)",
            "    dest = directory / Path(url_path).name",
            "    shutil.move(temp_dir / Path(url_path).name, dest)",
            "    return str(dest.resolve())",
            "",
            "",
            "def create_tmp_copy_of_file(file_path: str, dir: str | None = None) -> str:",
            "    directory = Path(dir or tempfile.gettempdir()) / secrets.token_hex(20)",
            "    directory.mkdir(exist_ok=True, parents=True)",
            "    dest = directory / Path(file_path).name",
            "    shutil.copy2(file_path, dest)",
            "    return str(dest.resolve())",
            "",
            "",
            "def download_tmp_copy_of_file(",
            "    url_path: str, hf_token: str | None = None, dir: str | None = None",
            ") -> str:",
            "    \"\"\"Kept for backwards compatibility for 3.x spaces.\"\"\"",
            "    if dir is not None:",
            "        os.makedirs(dir, exist_ok=True)",
            "    headers = {\"Authorization\": \"Bearer \" + hf_token} if hf_token else {}",
            "    directory = Path(dir or tempfile.gettempdir()) / secrets.token_hex(20)",
            "    directory.mkdir(exist_ok=True, parents=True)",
            "    file_path = directory / Path(url_path).name",
            "",
            "    with httpx.stream(",
            "        \"GET\", url_path, headers=headers, follow_redirects=True",
            "    ) as response:",
            "        response.raise_for_status()",
            "        with open(file_path, \"wb\") as f:",
            "            for chunk in response.iter_raw():",
            "                f.write(chunk)",
            "    return str(file_path.resolve())",
            "",
            "",
            "def get_mimetype(filename: str) -> str | None:",
            "    if filename.endswith(\".vtt\"):",
            "        return \"text/vtt\"",
            "    mimetype = mimetypes.guess_type(filename)[0]",
            "    if mimetype is not None:",
            "        mimetype = mimetype.replace(\"x-wav\", \"wav\").replace(\"x-flac\", \"flac\")",
            "    return mimetype",
            "",
            "",
            "def get_extension(encoding: str) -> str | None:",
            "    encoding = encoding.replace(\"audio/wav\", \"audio/x-wav\")",
            "    type = mimetypes.guess_type(encoding)[0]",
            "    if type == \"audio/flac\":  # flac is not supported by mimetypes",
            "        return \"flac\"",
            "    elif type is None:",
            "        return None",
            "    extension = mimetypes.guess_extension(type)",
            "    if extension is not None and extension.startswith(\".\"):",
            "        extension = extension[1:]",
            "    return extension",
            "",
            "",
            "def encode_file_to_base64(f: str | Path):",
            "    with open(f, \"rb\") as file:",
            "        encoded_string = base64.b64encode(file.read())",
            "        base64_str = str(encoded_string, \"utf-8\")",
            "        mimetype = get_mimetype(str(f))",
            "        return (",
            "            \"data:\"",
            "            + (mimetype if mimetype is not None else \"\")",
            "            + \";base64,\"",
            "            + base64_str",
            "        )",
            "",
            "",
            "def encode_url_to_base64(url: str):",
            "    resp = httpx.get(url)",
            "    resp.raise_for_status()",
            "    encoded_string = base64.b64encode(resp.content)",
            "    base64_str = str(encoded_string, \"utf-8\")",
            "    mimetype = get_mimetype(url)",
            "    return (",
            "        \"data:\" + (mimetype if mimetype is not None else \"\") + \";base64,\" + base64_str",
            "    )",
            "",
            "",
            "def encode_url_or_file_to_base64(path: str | Path):",
            "    path = str(path)",
            "    if is_http_url_like(path):",
            "        return encode_url_to_base64(path)",
            "    return encode_file_to_base64(path)",
            "",
            "",
            "def download_byte_stream(url: str, hf_token=None):",
            "    arr = bytearray()",
            "    headers = {\"Authorization\": \"Bearer \" + hf_token} if hf_token else {}",
            "    with httpx.stream(\"GET\", url, headers=headers) as r:",
            "        for data in r.iter_bytes():",
            "            arr += data",
            "            yield data",
            "    yield arr",
            "",
            "",
            "def decode_base64_to_binary(encoding: str) -> tuple[bytes, str | None]:",
            "    extension = get_extension(encoding)",
            "    data = encoding.rsplit(\",\", 1)[-1]",
            "    return base64.b64decode(data), extension",
            "",
            "",
            "def strip_invalid_filename_characters(filename: str, max_bytes: int = 200) -> str:",
            "    \"\"\"Strips invalid characters from a filename and ensures that the file_length is less than `max_bytes` bytes.\"\"\"",
            "    filename = \"\".join([char for char in filename if char.isalnum() or char in \"._- \"])",
            "    filename_len = len(filename.encode())",
            "    if filename_len > max_bytes:",
            "        while filename_len > max_bytes:",
            "            if len(filename) == 0:",
            "                break",
            "            filename = filename[:-1]",
            "            filename_len = len(filename.encode())",
            "    return filename",
            "",
            "",
            "def sanitize_parameter_names(original_name: str) -> str:",
            "    \"\"\"Cleans up a Python parameter name to make the API info more readable.\"\"\"",
            "    return (",
            "        \"\".join([char for char in original_name if char.isalnum() or char in \" _\"])",
            "        .replace(\" \", \"_\")",
            "        .lower()",
            "    )",
            "",
            "",
            "def decode_base64_to_file(",
            "    encoding: str,",
            "    file_path: str | None = None,",
            "    dir: str | Path | None = None,",
            "    prefix: str | None = None,",
            "):",
            "    directory = Path(dir or tempfile.gettempdir()) / secrets.token_hex(20)",
            "    directory.mkdir(exist_ok=True, parents=True)",
            "    data, extension = decode_base64_to_binary(encoding)",
            "    if file_path is not None and prefix is None:",
            "        filename = Path(file_path).name",
            "        prefix = filename",
            "        if \".\" in filename:",
            "            prefix = filename[0 : filename.index(\".\")]",
            "            extension = filename[filename.index(\".\") + 1 :]",
            "",
            "    if prefix is not None:",
            "        prefix = strip_invalid_filename_characters(prefix)",
            "",
            "    if extension is None:",
            "        file_obj = tempfile.NamedTemporaryFile(",
            "            delete=False, prefix=prefix, dir=directory",
            "        )",
            "    else:",
            "        file_obj = tempfile.NamedTemporaryFile(",
            "            delete=False,",
            "            prefix=prefix,",
            "            suffix=\".\" + extension,",
            "            dir=directory,",
            "        )",
            "    file_obj.write(data)",
            "    file_obj.flush()",
            "    return file_obj",
            "",
            "",
            "def dict_or_str_to_json_file(jsn: str | dict | list, dir: str | Path | None = None):",
            "    if dir is not None:",
            "        os.makedirs(dir, exist_ok=True)",
            "",
            "    file_obj = tempfile.NamedTemporaryFile(",
            "        delete=False, suffix=\".json\", dir=dir, mode=\"w+\"",
            "    )",
            "    if isinstance(jsn, str):",
            "        jsn = json.loads(jsn)",
            "    json.dump(jsn, file_obj)",
            "    file_obj.flush()",
            "    return file_obj",
            "",
            "",
            "def file_to_json(file_path: str | Path) -> dict | list:",
            "    with open(file_path) as f:",
            "        return json.load(f)",
            "",
            "",
            "###########################",
            "# HuggingFace Hub API Utils",
            "###########################",
            "def set_space_timeout(",
            "    space_id: str,",
            "    hf_token: str | None = None,",
            "    timeout_in_seconds: int = 300,",
            "):",
            "    headers = huggingface_hub.utils.build_hf_headers(",
            "        token=hf_token,",
            "        library_name=\"gradio_client\",",
            "        library_version=__version__,",
            "    )",
            "    try:",
            "        httpx.post(",
            "            f\"https://huggingface.co/api/spaces/{space_id}/sleeptime\",",
            "            json={\"seconds\": timeout_in_seconds},",
            "            headers=headers,",
            "        )",
            "    except httpx.HTTPStatusError as e:",
            "        raise SpaceDuplicationError(",
            "            f\"Could not set sleep timeout on duplicated Space. Please visit {SPACE_URL.format(space_id)} \"",
            "            \"to set a timeout manually to reduce billing charges.\"",
            "        ) from e",
            "",
            "",
            "########################",
            "# Misc utils",
            "########################",
            "",
            "",
            "def synchronize_async(func: Callable, *args, **kwargs) -> Any:",
            "    \"\"\"",
            "    Runs async functions in sync scopes. Can be used in any scope.",
            "",
            "    Example:",
            "        if inspect.iscoroutinefunction(block_fn.fn):",
            "            predictions = utils.synchronize_async(block_fn.fn, *processed_input)",
            "",
            "    Args:",
            "        func:",
            "        *args:",
            "        **kwargs:",
            "    \"\"\"",
            "    return fsspec.asyn.sync(fsspec.asyn.get_loop(), func, *args, **kwargs)  # type: ignore",
            "",
            "",
            "class APIInfoParseError(ValueError):",
            "    pass",
            "",
            "",
            "def get_type(schema: dict):",
            "    if \"const\" in schema:",
            "        return \"const\"",
            "    if \"enum\" in schema:",
            "        return \"enum\"",
            "    elif \"type\" in schema:",
            "        return schema[\"type\"]",
            "    elif schema.get(\"$ref\"):",
            "        return \"$ref\"",
            "    elif schema.get(\"oneOf\"):",
            "        return \"oneOf\"",
            "    elif schema.get(\"anyOf\"):",
            "        return \"anyOf\"",
            "    elif schema.get(\"allOf\"):",
            "        return \"allOf\"",
            "    elif \"type\" not in schema:",
            "        return {}",
            "    else:",
            "        raise APIInfoParseError(f\"Cannot parse type for {schema}\")",
            "",
            "",
            "FILE_DATA = \"Dict(path: str, url: str | None, size: int | None, orig_name: str | None, mime_type: str | None)\"",
            "",
            "",
            "def json_schema_to_python_type(schema: Any) -> str:",
            "    type_ = _json_schema_to_python_type(schema, schema.get(\"$defs\"))",
            "    return type_.replace(FILE_DATA, \"filepath\")",
            "",
            "",
            "def _json_schema_to_python_type(schema: Any, defs) -> str:",
            "    \"\"\"Convert the json schema into a python type hint\"\"\"",
            "    if schema == {}:",
            "        return \"Any\"",
            "    type_ = get_type(schema)",
            "    if type_ == {}:",
            "        if \"json\" in schema.get(\"description\", {}):",
            "            return \"Dict[Any, Any]\"",
            "        else:",
            "            return \"Any\"",
            "    elif type_ == \"$ref\":",
            "        return _json_schema_to_python_type(defs[schema[\"$ref\"].split(\"/\")[-1]], defs)",
            "    elif type_ == \"null\":",
            "        return \"None\"",
            "    elif type_ == \"const\":",
            "        return f\"Literal[{schema['const']}]\"",
            "    elif type_ == \"enum\":",
            "        return (",
            "            \"Literal[\" + \", \".join([\"'\" + str(v) + \"'\" for v in schema[\"enum\"]]) + \"]\"",
            "        )",
            "    elif type_ == \"integer\":",
            "        return \"int\"",
            "    elif type_ == \"string\":",
            "        return \"str\"",
            "    elif type_ == \"boolean\":",
            "        return \"bool\"",
            "    elif type_ == \"number\":",
            "        return \"float\"",
            "    elif type_ == \"array\":",
            "        items = schema.get(\"items\", [])",
            "        if \"prefixItems\" in items:",
            "            elements = \", \".join(",
            "                [_json_schema_to_python_type(i, defs) for i in items[\"prefixItems\"]]",
            "            )",
            "            return f\"Tuple[{elements}]\"",
            "        elif \"prefixItems\" in schema:",
            "            elements = \", \".join(",
            "                [_json_schema_to_python_type(i, defs) for i in schema[\"prefixItems\"]]",
            "            )",
            "            return f\"Tuple[{elements}]\"",
            "        else:",
            "            elements = _json_schema_to_python_type(items, defs)",
            "            return f\"List[{elements}]\"",
            "    elif type_ == \"object\":",
            "",
            "        def get_desc(v):",
            "            return f\" ({v.get('description')})\" if v.get(\"description\") else \"\"",
            "",
            "        props = schema.get(\"properties\", {})",
            "",
            "        des = [",
            "            f\"{n}: {_json_schema_to_python_type(v, defs)}{get_desc(v)}\"",
            "            for n, v in props.items()",
            "            if n != \"$defs\"",
            "        ]",
            "",
            "        if \"additionalProperties\" in schema:",
            "            des += [",
            "                f\"str, {_json_schema_to_python_type(schema['additionalProperties'], defs)}\"",
            "            ]",
            "        des = \", \".join(des)",
            "        return f\"Dict({des})\"",
            "    elif type_ in [\"oneOf\", \"anyOf\"]:",
            "        desc = \" | \".join([_json_schema_to_python_type(i, defs) for i in schema[type_]])",
            "        return desc",
            "    elif type_ == \"allOf\":",
            "        data = \", \".join(_json_schema_to_python_type(i, defs) for i in schema[type_])",
            "        desc = f\"All[{data}]\"",
            "        return desc",
            "    else:",
            "        raise APIInfoParseError(f\"Cannot parse schema {schema}\")",
            "",
            "",
            "def traverse(json_obj: Any, func: Callable, is_root: Callable) -> Any:",
            "    if is_root(json_obj):",
            "        return func(json_obj)",
            "    elif isinstance(json_obj, dict):",
            "        new_obj = {}",
            "        for key, value in json_obj.items():",
            "            new_obj[key] = traverse(value, func, is_root)",
            "        return new_obj",
            "    elif isinstance(json_obj, (list, tuple)):",
            "        new_obj = []",
            "        for item in json_obj:",
            "            new_obj.append(traverse(item, func, is_root))",
            "        return new_obj",
            "    else:",
            "        return json_obj",
            "",
            "",
            "def value_is_file(api_info: dict) -> bool:",
            "    info = _json_schema_to_python_type(api_info, api_info.get(\"$defs\"))",
            "    return FILE_DATA in info",
            "",
            "",
            "def is_filepath(s):",
            "    return isinstance(s, str) and Path(s).exists()",
            "",
            "",
            "def is_url(s):",
            "    return isinstance(s, str) and is_http_url_like(s)",
            "",
            "",
            "def is_file_obj(d):",
            "    return isinstance(d, dict) and \"path\" in d",
            "",
            "",
            "SKIP_COMPONENTS = {",
            "    \"state\",",
            "    \"row\",",
            "    \"column\",",
            "    \"tabs\",",
            "    \"tab\",",
            "    \"tabitem\",",
            "    \"box\",",
            "    \"form\",",
            "    \"accordion\",",
            "    \"group\",",
            "    \"interpretation\",",
            "    \"dataset\",",
            "}"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import base64",
            "import copy",
            "import hashlib",
            "import json",
            "import mimetypes",
            "import os",
            "import pkgutil",
            "import secrets",
            "import shutil",
            "import tempfile",
            "import warnings",
            "from concurrent.futures import CancelledError",
            "from dataclasses import dataclass, field",
            "from datetime import datetime",
            "from enum import Enum",
            "from pathlib import Path",
            "from threading import Lock",
            "from typing import Any, Callable, Literal, Optional, TypedDict",
            "",
            "import fsspec.asyn",
            "import httpx",
            "import huggingface_hub",
            "from huggingface_hub import SpaceStage",
            "from websockets.legacy.protocol import WebSocketCommonProtocol",
            "",
            "API_URL = \"api/predict/\"",
            "SSE_URL_V0 = \"queue/join\"",
            "SSE_DATA_URL_V0 = \"queue/data\"",
            "SSE_URL = \"queue/data\"",
            "SSE_DATA_URL = \"queue/join\"",
            "WS_URL = \"queue/join\"",
            "UPLOAD_URL = \"upload\"",
            "LOGIN_URL = \"login\"",
            "CONFIG_URL = \"config\"",
            "API_INFO_URL = \"info\"",
            "RAW_API_INFO_URL = \"info?serialize=False\"",
            "SPACE_FETCHER_URL = \"https://gradio-space-api-fetcher-v2.hf.space/api\"",
            "RESET_URL = \"reset\"",
            "SPACE_URL = \"https://hf.space/{}\"",
            "",
            "STATE_COMPONENT = \"state\"",
            "INVALID_RUNTIME = [",
            "    SpaceStage.NO_APP_FILE,",
            "    SpaceStage.CONFIG_ERROR,",
            "    SpaceStage.BUILD_ERROR,",
            "    SpaceStage.RUNTIME_ERROR,",
            "    SpaceStage.PAUSED,",
            "]",
            "",
            "",
            "class Message(TypedDict, total=False):",
            "    msg: str",
            "    output: dict[str, Any]",
            "    event_id: str",
            "    rank: int",
            "    rank_eta: float",
            "    queue_size: int",
            "    success: bool",
            "    progress_data: list[dict]",
            "    log: str",
            "    level: str",
            "",
            "",
            "def get_package_version() -> str:",
            "    try:",
            "        package_json_data = (",
            "            pkgutil.get_data(__name__, \"package.json\").decode(\"utf-8\").strip()  # type: ignore",
            "        )",
            "        package_data = json.loads(package_json_data)",
            "        version = package_data.get(\"version\", \"\")",
            "        return version",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "__version__ = get_package_version()",
            "",
            "",
            "class TooManyRequestsError(Exception):",
            "    \"\"\"Raised when the API returns a 429 status code.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class QueueError(Exception):",
            "    \"\"\"Raised when the queue is full or there is an issue adding a job to the queue.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class InvalidAPIEndpointError(Exception):",
            "    \"\"\"Raised when the API endpoint is invalid.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class SpaceDuplicationError(Exception):",
            "    \"\"\"Raised when something goes wrong with a Space Duplication.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class ServerMessage(str, Enum):",
            "    send_hash = \"send_hash\"",
            "    queue_full = \"queue_full\"",
            "    estimation = \"estimation\"",
            "    send_data = \"send_data\"",
            "    process_starts = \"process_starts\"",
            "    process_generating = \"process_generating\"",
            "    process_completed = \"process_completed\"",
            "    log = \"log\"",
            "    progress = \"progress\"",
            "    heartbeat = \"heartbeat\"",
            "    server_stopped = \"server_stopped\"",
            "",
            "",
            "class Status(Enum):",
            "    \"\"\"Status codes presented to client users.\"\"\"",
            "",
            "    STARTING = \"STARTING\"",
            "    JOINING_QUEUE = \"JOINING_QUEUE\"",
            "    QUEUE_FULL = \"QUEUE_FULL\"",
            "    IN_QUEUE = \"IN_QUEUE\"",
            "    SENDING_DATA = \"SENDING_DATA\"",
            "    PROCESSING = \"PROCESSING\"",
            "    ITERATING = \"ITERATING\"",
            "    PROGRESS = \"PROGRESS\"",
            "    FINISHED = \"FINISHED\"",
            "    CANCELLED = \"CANCELLED\"",
            "    LOG = \"LOG\"",
            "",
            "    @staticmethod",
            "    def ordering(status: Status) -> int:",
            "        \"\"\"Order of messages. Helpful for testing.\"\"\"",
            "        order = [",
            "            Status.STARTING,",
            "            Status.JOINING_QUEUE,",
            "            Status.QUEUE_FULL,",
            "            Status.IN_QUEUE,",
            "            Status.SENDING_DATA,",
            "            Status.PROCESSING,",
            "            Status.PROGRESS,",
            "            Status.ITERATING,",
            "            Status.FINISHED,",
            "            Status.CANCELLED,",
            "        ]",
            "        return order.index(status)",
            "",
            "    def __lt__(self, other: Status):",
            "        return self.ordering(self) < self.ordering(other)",
            "",
            "    @staticmethod",
            "    def msg_to_status(msg: str) -> Status:",
            "        \"\"\"Map the raw message from the backend to the status code presented to users.\"\"\"",
            "        return {",
            "            ServerMessage.send_hash: Status.JOINING_QUEUE,",
            "            ServerMessage.queue_full: Status.QUEUE_FULL,",
            "            ServerMessage.estimation: Status.IN_QUEUE,",
            "            ServerMessage.send_data: Status.SENDING_DATA,",
            "            ServerMessage.process_starts: Status.PROCESSING,",
            "            ServerMessage.process_generating: Status.ITERATING,",
            "            ServerMessage.process_completed: Status.FINISHED,",
            "            ServerMessage.progress: Status.PROGRESS,",
            "            ServerMessage.log: Status.LOG,",
            "            ServerMessage.server_stopped: Status.FINISHED,",
            "        }[msg]  # type: ignore",
            "",
            "",
            "@dataclass",
            "class ProgressUnit:",
            "    index: Optional[int]",
            "    length: Optional[int]",
            "    unit: Optional[str]",
            "    progress: Optional[float]",
            "    desc: Optional[str]",
            "",
            "    @classmethod",
            "    def from_msg(cls, data: list[dict]) -> list[ProgressUnit]:",
            "        return [",
            "            cls(",
            "                index=d.get(\"index\"),",
            "                length=d.get(\"length\"),",
            "                unit=d.get(\"unit\"),",
            "                progress=d.get(\"progress\"),",
            "                desc=d.get(\"desc\"),",
            "            )",
            "            for d in data",
            "        ]",
            "",
            "",
            "@dataclass",
            "class StatusUpdate:",
            "    \"\"\"Update message sent from the worker thread to the Job on the main thread.\"\"\"",
            "",
            "    code: Status",
            "    rank: int | None",
            "    queue_size: int | None",
            "    eta: float | None",
            "    success: bool | None",
            "    time: datetime | None",
            "    progress_data: list[ProgressUnit] | None",
            "    log: tuple[str, str] | None = None",
            "",
            "",
            "def create_initial_status_update():",
            "    return StatusUpdate(",
            "        code=Status.STARTING,",
            "        rank=None,",
            "        queue_size=None,",
            "        eta=None,",
            "        success=None,",
            "        time=datetime.now(),",
            "        progress_data=None,",
            "    )",
            "",
            "",
            "@dataclass",
            "class JobStatus:",
            "    \"\"\"The job status.",
            "",
            "    Keeps track of the latest status update and intermediate outputs (not yet implements).",
            "    \"\"\"",
            "",
            "    latest_status: StatusUpdate = field(default_factory=create_initial_status_update)",
            "    outputs: list[Any] = field(default_factory=list)",
            "",
            "",
            "@dataclass",
            "class Communicator:",
            "    \"\"\"Helper class to help communicate between the worker thread and main thread.\"\"\"",
            "",
            "    lock: Lock",
            "    job: JobStatus",
            "    prediction_processor: Callable[..., tuple]",
            "    reset_url: str",
            "    should_cancel: bool = False",
            "    event_id: str | None = None",
            "",
            "",
            "########################",
            "# Network utils",
            "########################",
            "",
            "",
            "def is_http_url_like(possible_url: str) -> bool:",
            "    \"\"\"",
            "    Check if the given string looks like an HTTP(S) URL.",
            "    \"\"\"",
            "    return possible_url.startswith((\"http://\", \"https://\"))",
            "",
            "",
            "def probe_url(possible_url: str) -> bool:",
            "    \"\"\"",
            "    Probe the given URL to see if it responds with a 200 status code (to HEAD, then to GET).",
            "    \"\"\"",
            "    headers = {\"User-Agent\": \"gradio (https://gradio.app/; gradio-team@huggingface.co)\"}",
            "    try:",
            "        with httpx.Client() as client:",
            "            head_request = client.head(possible_url, headers=headers)",
            "            if head_request.status_code == 405:",
            "                return client.get(possible_url, headers=headers).is_success",
            "            return head_request.is_success",
            "    except Exception:",
            "        return False",
            "",
            "",
            "def is_valid_url(possible_url: str) -> bool:",
            "    \"\"\"",
            "    Check if the given string is a valid URL.",
            "    \"\"\"",
            "    warnings.warn(",
            "        \"is_valid_url should not be used. \"",
            "        \"Use is_http_url_like() and probe_url(), as suitable, instead.\",",
            "    )",
            "    return is_http_url_like(possible_url) and probe_url(possible_url)",
            "",
            "",
            "async def get_pred_from_ws(",
            "    websocket: WebSocketCommonProtocol,",
            "    data: str,",
            "    hash_data: str,",
            "    helper: Communicator | None = None,",
            ") -> dict[str, Any]:",
            "    completed = False",
            "    resp = {}",
            "    while not completed:",
            "        # Receive message in the background so that we can",
            "        # cancel even while running a long pred",
            "        task = asyncio.create_task(websocket.recv())",
            "        while not task.done():",
            "            if helper:",
            "                with helper.lock:",
            "                    if helper.should_cancel:",
            "                        # Need to reset the iterator state since the client",
            "                        # will not reset the session",
            "                        async with httpx.AsyncClient() as http:",
            "                            reset = http.post(",
            "                                helper.reset_url, json=json.loads(hash_data)",
            "                            )",
            "                            # Retrieve cancel exception from task",
            "                            # otherwise will get nasty warning in console",
            "                            task.cancel()",
            "                            await asyncio.gather(task, reset, return_exceptions=True)",
            "                        raise CancelledError()",
            "            # Need to suspend this coroutine so that task actually runs",
            "            await asyncio.sleep(0.01)",
            "        msg = task.result()",
            "        resp = json.loads(msg)",
            "        if helper:",
            "            with helper.lock:",
            "                has_progress = \"progress_data\" in resp",
            "                status_update = StatusUpdate(",
            "                    code=Status.msg_to_status(resp[\"msg\"]),",
            "                    queue_size=resp.get(\"queue_size\"),",
            "                    rank=resp.get(\"rank\", None),",
            "                    success=resp.get(\"success\"),",
            "                    time=datetime.now(),",
            "                    eta=resp.get(\"rank_eta\"),",
            "                    progress_data=ProgressUnit.from_msg(resp[\"progress_data\"])",
            "                    if has_progress",
            "                    else None,",
            "                )",
            "                output = resp.get(\"output\", {}).get(\"data\", [])",
            "                if output and status_update.code != Status.FINISHED:",
            "                    try:",
            "                        result = helper.prediction_processor(*output)",
            "                    except Exception as e:",
            "                        result = [e]",
            "                    helper.job.outputs.append(result)",
            "                helper.job.latest_status = status_update",
            "        if resp[\"msg\"] == \"queue_full\":",
            "            raise QueueError(\"Queue is full! Please try again.\")",
            "        if resp[\"msg\"] == \"send_hash\":",
            "            await websocket.send(hash_data)",
            "        elif resp[\"msg\"] == \"send_data\":",
            "            await websocket.send(data)",
            "        completed = resp[\"msg\"] == \"process_completed\"",
            "    return resp[\"output\"]",
            "",
            "",
            "async def get_pred_from_sse_v0(",
            "    client: httpx.AsyncClient,",
            "    data: dict,",
            "    hash_data: dict,",
            "    helper: Communicator,",
            "    sse_url: str,",
            "    sse_data_url: str,",
            "    headers: dict[str, str],",
            "    cookies: dict[str, str] | None,",
            ") -> dict[str, Any] | None:",
            "    done, pending = await asyncio.wait(",
            "        [",
            "            asyncio.create_task(check_for_cancel(helper, headers, cookies)),",
            "            asyncio.create_task(",
            "                stream_sse_v0(",
            "                    client,",
            "                    data,",
            "                    hash_data,",
            "                    helper,",
            "                    sse_url,",
            "                    sse_data_url,",
            "                    headers,",
            "                    cookies,",
            "                )",
            "            ),",
            "        ],",
            "        return_when=asyncio.FIRST_COMPLETED,",
            "    )",
            "",
            "    for task in pending:",
            "        task.cancel()",
            "        try:",
            "            await task",
            "        except asyncio.CancelledError:",
            "            pass",
            "",
            "    assert len(done) == 1",
            "    for task in done:",
            "        return task.result()",
            "",
            "",
            "async def get_pred_from_sse_v1_v2(",
            "    helper: Communicator,",
            "    headers: dict[str, str],",
            "    cookies: dict[str, str] | None,",
            "    pending_messages_per_event: dict[str, list[Message | None]],",
            "    event_id: str,",
            "    protocol: Literal[\"sse_v1\", \"sse_v2\"],",
            ") -> dict[str, Any] | None:",
            "    done, pending = await asyncio.wait(",
            "        [",
            "            asyncio.create_task(check_for_cancel(helper, headers, cookies)),",
            "            asyncio.create_task(",
            "                stream_sse_v1_v2(helper, pending_messages_per_event, event_id, protocol)",
            "            ),",
            "        ],",
            "        return_when=asyncio.FIRST_COMPLETED,",
            "    )",
            "",
            "    for task in pending:",
            "        task.cancel()",
            "        try:",
            "            await task",
            "        except asyncio.CancelledError:",
            "            pass",
            "",
            "    assert len(done) == 1",
            "    for task in done:",
            "        exception = task.exception()",
            "        if exception:",
            "            raise exception",
            "        return task.result()",
            "",
            "",
            "async def check_for_cancel(",
            "    helper: Communicator, headers: dict[str, str], cookies: dict[str, str] | None",
            "):",
            "    while True:",
            "        await asyncio.sleep(0.05)",
            "        with helper.lock:",
            "            if helper.should_cancel:",
            "                break",
            "    if helper.event_id:",
            "        async with httpx.AsyncClient() as http:",
            "            await http.post(",
            "                helper.reset_url,",
            "                json={\"event_id\": helper.event_id},",
            "                headers=headers,",
            "                cookies=cookies,",
            "            )",
            "    raise CancelledError()",
            "",
            "",
            "async def stream_sse_v0(",
            "    client: httpx.AsyncClient,",
            "    data: dict,",
            "    hash_data: dict,",
            "    helper: Communicator,",
            "    sse_url: str,",
            "    sse_data_url: str,",
            "    headers: dict[str, str],",
            "    cookies: dict[str, str] | None,",
            ") -> dict[str, Any]:",
            "    try:",
            "        async with client.stream(",
            "            \"GET\",",
            "            sse_url,",
            "            params=hash_data,",
            "            headers=headers,",
            "            cookies=cookies,",
            "        ) as response:",
            "            async for line in response.aiter_lines():",
            "                line = line.rstrip(\"\\n\")",
            "                if len(line) == 0:",
            "                    continue",
            "                if line.startswith(\"data:\"):",
            "                    resp = json.loads(line[5:])",
            "                    if resp[\"msg\"] in [ServerMessage.log, ServerMessage.heartbeat]:",
            "                        continue",
            "                    with helper.lock:",
            "                        has_progress = \"progress_data\" in resp",
            "                        status_update = StatusUpdate(",
            "                            code=Status.msg_to_status(resp[\"msg\"]),",
            "                            queue_size=resp.get(\"queue_size\"),",
            "                            rank=resp.get(\"rank\", None),",
            "                            success=resp.get(\"success\"),",
            "                            time=datetime.now(),",
            "                            eta=resp.get(\"rank_eta\"),",
            "                            progress_data=ProgressUnit.from_msg(resp[\"progress_data\"])",
            "                            if has_progress",
            "                            else None,",
            "                        )",
            "                        output = resp.get(\"output\", {}).get(\"data\", [])",
            "                        if output and status_update.code != Status.FINISHED:",
            "                            try:",
            "                                result = helper.prediction_processor(*output)",
            "                            except Exception as e:",
            "                                result = [e]",
            "                            helper.job.outputs.append(result)",
            "                        helper.job.latest_status = status_update",
            "",
            "                    if resp[\"msg\"] == \"queue_full\":",
            "                        raise QueueError(\"Queue is full! Please try again.\")",
            "                    elif resp[\"msg\"] == \"send_data\":",
            "                        event_id = resp[\"event_id\"]",
            "                        helper.event_id = event_id",
            "                        req = await client.post(",
            "                            sse_data_url,",
            "                            json={\"event_id\": event_id, **data, **hash_data},",
            "                            headers=headers,",
            "                            cookies=cookies,",
            "                        )",
            "                        req.raise_for_status()",
            "                    elif resp[\"msg\"] == \"process_completed\":",
            "                        return resp[\"output\"]",
            "                else:",
            "                    raise ValueError(f\"Unexpected message: {line}\")",
            "        raise ValueError(\"Did not receive process_completed message.\")",
            "    except asyncio.CancelledError:",
            "        raise",
            "",
            "",
            "async def stream_sse_v1_v2(",
            "    helper: Communicator,",
            "    pending_messages_per_event: dict[str, list[Message | None]],",
            "    event_id: str,",
            "    protocol: Literal[\"sse_v1\", \"sse_v2\"],",
            ") -> dict[str, Any]:",
            "    try:",
            "        pending_messages = pending_messages_per_event[event_id]",
            "        pending_responses_for_diffs = None",
            "",
            "        while True:",
            "            if len(pending_messages) > 0:",
            "                msg = pending_messages.pop(0)",
            "            else:",
            "                await asyncio.sleep(0.05)",
            "                continue",
            "",
            "            if msg is None:",
            "                raise CancelledError()",
            "",
            "            with helper.lock:",
            "                log_message = None",
            "                if msg[\"msg\"] == ServerMessage.log:",
            "                    log = msg.get(\"log\")",
            "                    level = msg.get(\"level\")",
            "                    if log and level:",
            "                        log_message = (log, level)",
            "                status_update = StatusUpdate(",
            "                    code=Status.msg_to_status(msg[\"msg\"]),",
            "                    queue_size=msg.get(\"queue_size\"),",
            "                    rank=msg.get(\"rank\", None),",
            "                    success=msg.get(\"success\"),",
            "                    time=datetime.now(),",
            "                    eta=msg.get(\"rank_eta\"),",
            "                    progress_data=ProgressUnit.from_msg(msg[\"progress_data\"])",
            "                    if \"progress_data\" in msg",
            "                    else None,",
            "                    log=log_message,",
            "                )",
            "                output = msg.get(\"output\", {}).get(\"data\", [])",
            "                if (",
            "                    msg[\"msg\"] == ServerMessage.process_generating",
            "                    and protocol == \"sse_v2\"",
            "                ):",
            "                    if pending_responses_for_diffs is None:",
            "                        pending_responses_for_diffs = list(output)",
            "                    else:",
            "                        for i, value in enumerate(output):",
            "                            prev_output = pending_responses_for_diffs[i]",
            "                            new_output = apply_diff(prev_output, value)",
            "                            pending_responses_for_diffs[i] = new_output",
            "                            output[i] = new_output",
            "",
            "                if output and status_update.code != Status.FINISHED:",
            "                    try:",
            "                        result = helper.prediction_processor(*output)",
            "                    except Exception as e:",
            "                        result = [e]",
            "                    helper.job.outputs.append(result)",
            "                helper.job.latest_status = status_update",
            "            if msg[\"msg\"] == ServerMessage.process_completed:",
            "                del pending_messages_per_event[event_id]",
            "                return msg[\"output\"]",
            "            elif msg[\"msg\"] == ServerMessage.server_stopped:",
            "                raise ValueError(\"Server stopped.\")",
            "",
            "    except asyncio.CancelledError:",
            "        raise",
            "",
            "",
            "def apply_diff(obj, diff):",
            "    obj = copy.deepcopy(obj)",
            "",
            "    def apply_edit(target, path, action, value):",
            "        if len(path) == 0:",
            "            if action == \"replace\":",
            "                return value",
            "            elif action == \"append\":",
            "                return target + value",
            "            else:",
            "                raise ValueError(f\"Unsupported action: {action}\")",
            "",
            "        current = target",
            "        for i in range(len(path) - 1):",
            "            current = current[path[i]]",
            "",
            "        last_path = path[-1]",
            "        if action == \"replace\":",
            "            current[last_path] = value",
            "        elif action == \"append\":",
            "            current[last_path] += value",
            "        elif action == \"add\":",
            "            if isinstance(current, list):",
            "                current.insert(int(last_path), value)",
            "            else:",
            "                current[last_path] = value",
            "        elif action == \"delete\":",
            "            if isinstance(current, list):",
            "                del current[int(last_path)]",
            "            else:",
            "                del current[last_path]",
            "        else:",
            "            raise ValueError(f\"Unknown action: {action}\")",
            "",
            "        return target",
            "",
            "    for action, path, value in diff:",
            "        obj = apply_edit(obj, path, action, value)",
            "",
            "    return obj",
            "",
            "",
            "########################",
            "# Data processing utils",
            "########################",
            "",
            "",
            "def download_file(",
            "    url_path: str,",
            "    dir: str,",
            "    hf_token: str | None = None,",
            ") -> str:",
            "    if dir is not None:",
            "        os.makedirs(dir, exist_ok=True)",
            "    headers = {\"Authorization\": \"Bearer \" + hf_token} if hf_token else {}",
            "",
            "    sha1 = hashlib.sha1()",
            "    temp_dir = Path(tempfile.gettempdir()) / secrets.token_hex(20)",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    with httpx.stream(",
            "        \"GET\", url_path, headers=headers, follow_redirects=True",
            "    ) as response:",
            "        response.raise_for_status()",
            "        with open(temp_dir / Path(url_path).name, \"wb\") as f:",
            "            for chunk in response.iter_bytes(chunk_size=128 * sha1.block_size):",
            "                sha1.update(chunk)",
            "                f.write(chunk)",
            "",
            "    directory = Path(dir) / sha1.hexdigest()",
            "    directory.mkdir(exist_ok=True, parents=True)",
            "    dest = directory / Path(url_path).name",
            "    shutil.move(temp_dir / Path(url_path).name, dest)",
            "    return str(dest.resolve())",
            "",
            "",
            "def create_tmp_copy_of_file(file_path: str, dir: str | None = None) -> str:",
            "    directory = Path(dir or tempfile.gettempdir()) / secrets.token_hex(20)",
            "    directory.mkdir(exist_ok=True, parents=True)",
            "    dest = directory / Path(file_path).name",
            "    shutil.copy2(file_path, dest)",
            "    return str(dest.resolve())",
            "",
            "",
            "def download_tmp_copy_of_file(",
            "    url_path: str, hf_token: str | None = None, dir: str | None = None",
            ") -> str:",
            "    \"\"\"Kept for backwards compatibility for 3.x spaces.\"\"\"",
            "    if dir is not None:",
            "        os.makedirs(dir, exist_ok=True)",
            "    headers = {\"Authorization\": \"Bearer \" + hf_token} if hf_token else {}",
            "    directory = Path(dir or tempfile.gettempdir()) / secrets.token_hex(20)",
            "    directory.mkdir(exist_ok=True, parents=True)",
            "    file_path = directory / Path(url_path).name",
            "",
            "    with httpx.stream(",
            "        \"GET\", url_path, headers=headers, follow_redirects=True",
            "    ) as response:",
            "        response.raise_for_status()",
            "        with open(file_path, \"wb\") as f:",
            "            for chunk in response.iter_raw():",
            "                f.write(chunk)",
            "    return str(file_path.resolve())",
            "",
            "",
            "def get_mimetype(filename: str) -> str | None:",
            "    if filename.endswith(\".vtt\"):",
            "        return \"text/vtt\"",
            "    mimetype = mimetypes.guess_type(filename)[0]",
            "    if mimetype is not None:",
            "        mimetype = mimetype.replace(\"x-wav\", \"wav\").replace(\"x-flac\", \"flac\")",
            "    return mimetype",
            "",
            "",
            "def get_extension(encoding: str) -> str | None:",
            "    encoding = encoding.replace(\"audio/wav\", \"audio/x-wav\")",
            "    type = mimetypes.guess_type(encoding)[0]",
            "    if type == \"audio/flac\":  # flac is not supported by mimetypes",
            "        return \"flac\"",
            "    elif type is None:",
            "        return None",
            "    extension = mimetypes.guess_extension(type)",
            "    if extension is not None and extension.startswith(\".\"):",
            "        extension = extension[1:]",
            "    return extension",
            "",
            "",
            "def encode_file_to_base64(f: str | Path):",
            "    with open(f, \"rb\") as file:",
            "        encoded_string = base64.b64encode(file.read())",
            "        base64_str = str(encoded_string, \"utf-8\")",
            "        mimetype = get_mimetype(str(f))",
            "        return (",
            "            \"data:\"",
            "            + (mimetype if mimetype is not None else \"\")",
            "            + \";base64,\"",
            "            + base64_str",
            "        )",
            "",
            "",
            "def encode_url_to_base64(url: str):",
            "    resp = httpx.get(url)",
            "    resp.raise_for_status()",
            "    encoded_string = base64.b64encode(resp.content)",
            "    base64_str = str(encoded_string, \"utf-8\")",
            "    mimetype = get_mimetype(url)",
            "    return (",
            "        \"data:\" + (mimetype if mimetype is not None else \"\") + \";base64,\" + base64_str",
            "    )",
            "",
            "",
            "def encode_url_or_file_to_base64(path: str | Path):",
            "    path = str(path)",
            "    if is_http_url_like(path):",
            "        return encode_url_to_base64(path)",
            "    return encode_file_to_base64(path)",
            "",
            "",
            "def download_byte_stream(url: str, hf_token=None):",
            "    arr = bytearray()",
            "    headers = {\"Authorization\": \"Bearer \" + hf_token} if hf_token else {}",
            "    with httpx.stream(\"GET\", url, headers=headers) as r:",
            "        for data in r.iter_bytes():",
            "            arr += data",
            "            yield data",
            "    yield arr",
            "",
            "",
            "def decode_base64_to_binary(encoding: str) -> tuple[bytes, str | None]:",
            "    extension = get_extension(encoding)",
            "    data = encoding.rsplit(\",\", 1)[-1]",
            "    return base64.b64decode(data), extension",
            "",
            "",
            "def strip_invalid_filename_characters(filename: str, max_bytes: int = 200) -> str:",
            "    \"\"\"Strips invalid characters from a filename and ensures that the file_length is less than `max_bytes` bytes.\"\"\"",
            "    filename = \"\".join([char for char in filename if char.isalnum() or char in \"._- \"])",
            "    filename_len = len(filename.encode())",
            "    if filename_len > max_bytes:",
            "        while filename_len > max_bytes:",
            "            if len(filename) == 0:",
            "                break",
            "            filename = filename[:-1]",
            "            filename_len = len(filename.encode())",
            "    return filename",
            "",
            "",
            "def sanitize_parameter_names(original_name: str) -> str:",
            "    \"\"\"Cleans up a Python parameter name to make the API info more readable.\"\"\"",
            "    return (",
            "        \"\".join([char for char in original_name if char.isalnum() or char in \" _\"])",
            "        .replace(\" \", \"_\")",
            "        .lower()",
            "    )",
            "",
            "",
            "def decode_base64_to_file(",
            "    encoding: str,",
            "    file_path: str | None = None,",
            "    dir: str | Path | None = None,",
            "    prefix: str | None = None,",
            "):",
            "    directory = Path(dir or tempfile.gettempdir()) / secrets.token_hex(20)",
            "    directory.mkdir(exist_ok=True, parents=True)",
            "    data, extension = decode_base64_to_binary(encoding)",
            "    if file_path is not None and prefix is None:",
            "        filename = Path(file_path).name",
            "        prefix = filename",
            "        if \".\" in filename:",
            "            prefix = filename[0 : filename.index(\".\")]",
            "            extension = filename[filename.index(\".\") + 1 :]",
            "",
            "    if prefix is not None:",
            "        prefix = strip_invalid_filename_characters(prefix)",
            "",
            "    if extension is None:",
            "        file_obj = tempfile.NamedTemporaryFile(",
            "            delete=False, prefix=prefix, dir=directory",
            "        )",
            "    else:",
            "        file_obj = tempfile.NamedTemporaryFile(",
            "            delete=False,",
            "            prefix=prefix,",
            "            suffix=\".\" + extension,",
            "            dir=directory,",
            "        )",
            "    file_obj.write(data)",
            "    file_obj.flush()",
            "    return file_obj",
            "",
            "",
            "def dict_or_str_to_json_file(jsn: str | dict | list, dir: str | Path | None = None):",
            "    if dir is not None:",
            "        os.makedirs(dir, exist_ok=True)",
            "",
            "    file_obj = tempfile.NamedTemporaryFile(",
            "        delete=False, suffix=\".json\", dir=dir, mode=\"w+\"",
            "    )",
            "    if isinstance(jsn, str):",
            "        jsn = json.loads(jsn)",
            "    json.dump(jsn, file_obj)",
            "    file_obj.flush()",
            "    return file_obj",
            "",
            "",
            "def file_to_json(file_path: str | Path) -> dict | list:",
            "    with open(file_path) as f:",
            "        return json.load(f)",
            "",
            "",
            "###########################",
            "# HuggingFace Hub API Utils",
            "###########################",
            "def set_space_timeout(",
            "    space_id: str,",
            "    hf_token: str | None = None,",
            "    timeout_in_seconds: int = 300,",
            "):",
            "    headers = huggingface_hub.utils.build_hf_headers(",
            "        token=hf_token,",
            "        library_name=\"gradio_client\",",
            "        library_version=__version__,",
            "    )",
            "    try:",
            "        httpx.post(",
            "            f\"https://huggingface.co/api/spaces/{space_id}/sleeptime\",",
            "            json={\"seconds\": timeout_in_seconds},",
            "            headers=headers,",
            "        )",
            "    except httpx.HTTPStatusError as e:",
            "        raise SpaceDuplicationError(",
            "            f\"Could not set sleep timeout on duplicated Space. Please visit {SPACE_URL.format(space_id)} \"",
            "            \"to set a timeout manually to reduce billing charges.\"",
            "        ) from e",
            "",
            "",
            "########################",
            "# Misc utils",
            "########################",
            "",
            "",
            "def synchronize_async(func: Callable, *args, **kwargs) -> Any:",
            "    \"\"\"",
            "    Runs async functions in sync scopes. Can be used in any scope.",
            "",
            "    Example:",
            "        if inspect.iscoroutinefunction(block_fn.fn):",
            "            predictions = utils.synchronize_async(block_fn.fn, *processed_input)",
            "",
            "    Args:",
            "        func:",
            "        *args:",
            "        **kwargs:",
            "    \"\"\"",
            "    return fsspec.asyn.sync(fsspec.asyn.get_loop(), func, *args, **kwargs)  # type: ignore",
            "",
            "",
            "class APIInfoParseError(ValueError):",
            "    pass",
            "",
            "",
            "def get_type(schema: dict):",
            "    if \"const\" in schema:",
            "        return \"const\"",
            "    if \"enum\" in schema:",
            "        return \"enum\"",
            "    elif \"type\" in schema:",
            "        return schema[\"type\"]",
            "    elif schema.get(\"$ref\"):",
            "        return \"$ref\"",
            "    elif schema.get(\"oneOf\"):",
            "        return \"oneOf\"",
            "    elif schema.get(\"anyOf\"):",
            "        return \"anyOf\"",
            "    elif schema.get(\"allOf\"):",
            "        return \"allOf\"",
            "    elif \"type\" not in schema:",
            "        return {}",
            "    else:",
            "        raise APIInfoParseError(f\"Cannot parse type for {schema}\")",
            "",
            "",
            "FILE_DATA = \"Dict(path: str, url: str | None, size: int | None, orig_name: str | None, mime_type: str | None, is_stream: bool)\"",
            "",
            "",
            "def json_schema_to_python_type(schema: Any) -> str:",
            "    type_ = _json_schema_to_python_type(schema, schema.get(\"$defs\"))",
            "    return type_.replace(FILE_DATA, \"filepath\")",
            "",
            "",
            "def _json_schema_to_python_type(schema: Any, defs) -> str:",
            "    \"\"\"Convert the json schema into a python type hint\"\"\"",
            "    if schema == {}:",
            "        return \"Any\"",
            "    type_ = get_type(schema)",
            "    if type_ == {}:",
            "        if \"json\" in schema.get(\"description\", {}):",
            "            return \"Dict[Any, Any]\"",
            "        else:",
            "            return \"Any\"",
            "    elif type_ == \"$ref\":",
            "        return _json_schema_to_python_type(defs[schema[\"$ref\"].split(\"/\")[-1]], defs)",
            "    elif type_ == \"null\":",
            "        return \"None\"",
            "    elif type_ == \"const\":",
            "        return f\"Literal[{schema['const']}]\"",
            "    elif type_ == \"enum\":",
            "        return (",
            "            \"Literal[\" + \", \".join([\"'\" + str(v) + \"'\" for v in schema[\"enum\"]]) + \"]\"",
            "        )",
            "    elif type_ == \"integer\":",
            "        return \"int\"",
            "    elif type_ == \"string\":",
            "        return \"str\"",
            "    elif type_ == \"boolean\":",
            "        return \"bool\"",
            "    elif type_ == \"number\":",
            "        return \"float\"",
            "    elif type_ == \"array\":",
            "        items = schema.get(\"items\", [])",
            "        if \"prefixItems\" in items:",
            "            elements = \", \".join(",
            "                [_json_schema_to_python_type(i, defs) for i in items[\"prefixItems\"]]",
            "            )",
            "            return f\"Tuple[{elements}]\"",
            "        elif \"prefixItems\" in schema:",
            "            elements = \", \".join(",
            "                [_json_schema_to_python_type(i, defs) for i in schema[\"prefixItems\"]]",
            "            )",
            "            return f\"Tuple[{elements}]\"",
            "        else:",
            "            elements = _json_schema_to_python_type(items, defs)",
            "            return f\"List[{elements}]\"",
            "    elif type_ == \"object\":",
            "",
            "        def get_desc(v):",
            "            return f\" ({v.get('description')})\" if v.get(\"description\") else \"\"",
            "",
            "        props = schema.get(\"properties\", {})",
            "",
            "        des = [",
            "            f\"{n}: {_json_schema_to_python_type(v, defs)}{get_desc(v)}\"",
            "            for n, v in props.items()",
            "            if n != \"$defs\"",
            "        ]",
            "",
            "        if \"additionalProperties\" in schema:",
            "            des += [",
            "                f\"str, {_json_schema_to_python_type(schema['additionalProperties'], defs)}\"",
            "            ]",
            "        des = \", \".join(des)",
            "        return f\"Dict({des})\"",
            "    elif type_ in [\"oneOf\", \"anyOf\"]:",
            "        desc = \" | \".join([_json_schema_to_python_type(i, defs) for i in schema[type_]])",
            "        return desc",
            "    elif type_ == \"allOf\":",
            "        data = \", \".join(_json_schema_to_python_type(i, defs) for i in schema[type_])",
            "        desc = f\"All[{data}]\"",
            "        return desc",
            "    else:",
            "        raise APIInfoParseError(f\"Cannot parse schema {schema}\")",
            "",
            "",
            "def traverse(json_obj: Any, func: Callable, is_root: Callable) -> Any:",
            "    if is_root(json_obj):",
            "        return func(json_obj)",
            "    elif isinstance(json_obj, dict):",
            "        new_obj = {}",
            "        for key, value in json_obj.items():",
            "            new_obj[key] = traverse(value, func, is_root)",
            "        return new_obj",
            "    elif isinstance(json_obj, (list, tuple)):",
            "        new_obj = []",
            "        for item in json_obj:",
            "            new_obj.append(traverse(item, func, is_root))",
            "        return new_obj",
            "    else:",
            "        return json_obj",
            "",
            "",
            "def value_is_file(api_info: dict) -> bool:",
            "    info = _json_schema_to_python_type(api_info, api_info.get(\"$defs\"))",
            "    return FILE_DATA in info",
            "",
            "",
            "def is_filepath(s):",
            "    return isinstance(s, str) and Path(s).exists()",
            "",
            "",
            "def is_url(s):",
            "    return isinstance(s, str) and is_http_url_like(s)",
            "",
            "",
            "def is_file_obj(d):",
            "    return isinstance(d, dict) and \"path\" in d",
            "",
            "",
            "def is_file_obj_with_url(d):",
            "    return (",
            "        isinstance(d, dict) and \"path\" in d and \"url\" in d and isinstance(d[\"url\"], str)",
            "    )",
            "",
            "",
            "SKIP_COMPONENTS = {",
            "    \"state\",",
            "    \"row\",",
            "    \"column\",",
            "    \"tabs\",",
            "    \"tab\",",
            "    \"tabitem\",",
            "    \"box\",",
            "    \"form\",",
            "    \"accordion\",",
            "    \"group\",",
            "    \"interpretation\",",
            "    \"dataset\",",
            "}"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "898": [
                "FILE_DATA"
            ]
        },
        "addLocation": []
    },
    "gradio/blocks.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1325,
                "afterPatchRowNumber": 1325,
                "PatchRowcode": "                     if input_id in state:"
            },
            "1": {
                "beforePatchRowNumber": 1326,
                "afterPatchRowNumber": 1326,
                "PatchRowcode": "                         block = state[input_id]"
            },
            "2": {
                "beforePatchRowNumber": 1327,
                "afterPatchRowNumber": 1327,
                "PatchRowcode": "                     inputs_cached = processing_utils.move_files_to_cache("
            },
            "3": {
                "beforePatchRowNumber": 1328,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        inputs[i], block"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1328,
                "PatchRowcode": "+                        inputs[i], block, add_urls=True"
            },
            "5": {
                "beforePatchRowNumber": 1329,
                "afterPatchRowNumber": 1329,
                "PatchRowcode": "                     )"
            },
            "6": {
                "beforePatchRowNumber": 1330,
                "afterPatchRowNumber": 1330,
                "PatchRowcode": "                     if getattr(block, \"data_model\", None) and inputs_cached is not None:"
            },
            "7": {
                "beforePatchRowNumber": 1331,
                "afterPatchRowNumber": 1331,
                "PatchRowcode": "                         if issubclass(block.data_model, GradioModel):  # type: ignore"
            },
            "8": {
                "beforePatchRowNumber": 1454,
                "afterPatchRowNumber": 1454,
                "PatchRowcode": "                     prediction_value,"
            },
            "9": {
                "beforePatchRowNumber": 1455,
                "afterPatchRowNumber": 1455,
                "PatchRowcode": "                     block,  # type: ignore"
            },
            "10": {
                "beforePatchRowNumber": 1456,
                "afterPatchRowNumber": 1456,
                "PatchRowcode": "                     postprocess=True,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1457,
                "PatchRowcode": "+                    add_urls=True,"
            },
            "12": {
                "beforePatchRowNumber": 1457,
                "afterPatchRowNumber": 1458,
                "PatchRowcode": "                 )"
            },
            "13": {
                "beforePatchRowNumber": 1458,
                "afterPatchRowNumber": 1459,
                "PatchRowcode": "                 output.append(outputs_cached)"
            },
            "14": {
                "beforePatchRowNumber": 1459,
                "afterPatchRowNumber": 1460,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import copy",
            "import hashlib",
            "import inspect",
            "import json",
            "import os",
            "import random",
            "import secrets",
            "import string",
            "import sys",
            "import tempfile",
            "import threading",
            "import time",
            "import warnings",
            "import webbrowser",
            "from collections import defaultdict",
            "from pathlib import Path",
            "from types import ModuleType",
            "from typing import TYPE_CHECKING, Any, AsyncIterator, Callable, Literal, Sequence, cast",
            "from urllib.parse import urlparse, urlunparse",
            "",
            "import anyio",
            "import httpx",
            "from anyio import CapacityLimiter",
            "from gradio_client import utils as client_utils",
            "from gradio_client.documentation import document",
            "",
            "from gradio import (",
            "    analytics,",
            "    components,",
            "    networking,",
            "    processing_utils,",
            "    queueing,",
            "    routes,",
            "    strings,",
            "    themes,",
            "    utils,",
            "    wasm_utils,",
            ")",
            "from gradio.blocks_events import BlocksEvents, BlocksMeta",
            "from gradio.context import Context",
            "from gradio.data_classes import FileData, GradioModel, GradioRootModel",
            "from gradio.events import (",
            "    EventData,",
            "    EventListener,",
            "    EventListenerMethod,",
            ")",
            "from gradio.exceptions import (",
            "    DuplicateBlockError,",
            "    InvalidApiNameError,",
            "    InvalidBlockError,",
            "    InvalidComponentError,",
            ")",
            "from gradio.helpers import create_tracker, skip, special_args",
            "from gradio.state_holder import SessionState",
            "from gradio.themes import Default as DefaultTheme",
            "from gradio.themes import ThemeClass as Theme",
            "from gradio.tunneling import (",
            "    BINARY_FILENAME,",
            "    BINARY_FOLDER,",
            "    BINARY_PATH,",
            "    BINARY_URL,",
            "    CURRENT_TUNNELS,",
            ")",
            "from gradio.utils import (",
            "    TupleNoPrint,",
            "    check_function_inputs_match,",
            "    component_or_layout_class,",
            "    get_cancel_function,",
            "    get_continuous_fn,",
            "    get_package_version,",
            ")",
            "",
            "try:",
            "    import spaces  # type: ignore",
            "except Exception:",
            "    spaces = None",
            "",
            "",
            "if TYPE_CHECKING:  # Only import for type checking (is False at runtime).",
            "    from fastapi.applications import FastAPI",
            "",
            "    from gradio.components.base import Component",
            "",
            "BUILT_IN_THEMES: dict[str, Theme] = {",
            "    t.name: t",
            "    for t in [",
            "        themes.Base(),",
            "        themes.Default(),",
            "        themes.Monochrome(),",
            "        themes.Soft(),",
            "        themes.Glass(),",
            "    ]",
            "}",
            "",
            "",
            "class Block:",
            "    def __init__(",
            "        self,",
            "        *,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        render: bool = True,",
            "        visible: bool = True,",
            "        proxy_url: str | None = None,",
            "    ):",
            "        self._id = Context.id",
            "        Context.id += 1",
            "        self.visible = visible",
            "        self.elem_id = elem_id",
            "        self.elem_classes = (",
            "            [elem_classes] if isinstance(elem_classes, str) else elem_classes",
            "        )",
            "        self.proxy_url = proxy_url",
            "        self.share_token = secrets.token_urlsafe(32)",
            "        self.parent: BlockContext | None = None",
            "        self.is_rendered: bool = False",
            "        self._constructor_args: list[dict]",
            "        self.state_session_capacity = 10000",
            "        self.temp_files: set[str] = set()",
            "        self.GRADIO_CACHE = str(",
            "            Path(",
            "                os.environ.get(\"GRADIO_TEMP_DIR\")",
            "                or str(Path(tempfile.gettempdir()) / \"gradio\")",
            "            ).resolve()",
            "        )",
            "",
            "        if render:",
            "            self.render()",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return False",
            "",
            "    @property",
            "    def constructor_args(self) -> dict[str, Any]:",
            "        \"\"\"Get the arguments passed to the component's initializer.",
            "",
            "        Only set classes whose metaclass is ComponentMeta",
            "        \"\"\"",
            "        # the _constructor_args list is appended based on the mro of the class",
            "        # so the first entry is for the bottom of the hierarchy",
            "        return self._constructor_args[0] if self._constructor_args else {}",
            "",
            "    @property",
            "    def events(",
            "        self,",
            "    ) -> list[EventListener]:",
            "        return getattr(self, \"EVENTS\", [])",
            "",
            "    def render(self):",
            "        \"\"\"",
            "        Adds self into appropriate BlockContext",
            "        \"\"\"",
            "        if Context.root_block is not None and self._id in Context.root_block.blocks:",
            "            raise DuplicateBlockError(",
            "                f\"A block with id: {self._id} has already been rendered in the current Blocks.\"",
            "            )",
            "        if Context.block is not None:",
            "            Context.block.add(self)",
            "        if Context.root_block is not None:",
            "            Context.root_block.blocks[self._id] = self",
            "            self.is_rendered = True",
            "            if isinstance(self, components.Component):",
            "                Context.root_block.temp_file_sets.append(self.temp_files)",
            "        return self",
            "",
            "    def unrender(self):",
            "        \"\"\"",
            "        Removes self from BlockContext if it has been rendered (otherwise does nothing).",
            "        Removes self from the layout and collection of blocks, but does not delete any event triggers.",
            "        \"\"\"",
            "        if Context.block is not None:",
            "            try:",
            "                Context.block.children.remove(self)",
            "            except ValueError:",
            "                pass",
            "        if Context.root_block is not None:",
            "            try:",
            "                del Context.root_block.blocks[self._id]",
            "                self.is_rendered = False",
            "            except KeyError:",
            "                pass",
            "        return self",
            "",
            "    def get_block_name(self) -> str:",
            "        \"\"\"",
            "        Gets block's class name.",
            "",
            "        If it is template component it gets the parent's class name.",
            "",
            "        @return: class name",
            "        \"\"\"",
            "        return (",
            "            self.__class__.__base__.__name__.lower()",
            "            if hasattr(self, \"is_template\")",
            "            else self.__class__.__name__.lower()",
            "        )",
            "",
            "    def get_expected_parent(self) -> type[BlockContext] | None:",
            "        return None",
            "",
            "    def get_config(self):",
            "        config = {}",
            "        signature = inspect.signature(self.__class__.__init__)",
            "        for parameter in signature.parameters.values():",
            "            if hasattr(self, parameter.name):",
            "                value = getattr(self, parameter.name)",
            "                config[parameter.name] = utils.convert_to_dict_if_dataclass(value)",
            "        for e in self.events:",
            "            to_add = e.config_data()",
            "            if to_add:",
            "                config = {**to_add, **config}",
            "        config.pop(\"render\", None)",
            "        config = {**config, \"proxy_url\": self.proxy_url, \"name\": self.get_block_name()}",
            "        if (_selectable := getattr(self, \"_selectable\", None)) is not None:",
            "            config[\"_selectable\"] = _selectable",
            "        return config",
            "",
            "    @classmethod",
            "    def recover_kwargs(",
            "        cls, props: dict[str, Any], additional_keys: list[str] | None = None",
            "    ):",
            "        \"\"\"",
            "        Recovers kwargs from a dict of props.",
            "        \"\"\"",
            "        additional_keys = additional_keys or []",
            "        signature = inspect.signature(cls.__init__)",
            "        kwargs = {}",
            "        for parameter in signature.parameters.values():",
            "            if parameter.name in props and parameter.name not in additional_keys:",
            "                kwargs[parameter.name] = props[parameter.name]",
            "        return kwargs",
            "",
            "    def move_resource_to_block_cache(",
            "        self, url_or_file_path: str | Path | None",
            "    ) -> str | None:",
            "        \"\"\"Moves a file or downloads a file from a url to a block's cache directory, adds",
            "        to to the block's temp_files, and returns the path to the file in cache. This",
            "        ensures that the file is accessible to the Block and can be served to users.",
            "        \"\"\"",
            "        if url_or_file_path is None:",
            "            return None",
            "        if isinstance(url_or_file_path, Path):",
            "            url_or_file_path = str(url_or_file_path)",
            "",
            "        if client_utils.is_http_url_like(url_or_file_path):",
            "            temp_file_path = processing_utils.save_url_to_cache(",
            "                url_or_file_path, cache_dir=self.GRADIO_CACHE",
            "            )",
            "",
            "            self.temp_files.add(temp_file_path)",
            "        else:",
            "            url_or_file_path = str(utils.abspath(url_or_file_path))",
            "            if not utils.is_in_or_equal(url_or_file_path, self.GRADIO_CACHE):",
            "                temp_file_path = processing_utils.save_file_to_cache(",
            "                    url_or_file_path, cache_dir=self.GRADIO_CACHE",
            "                )",
            "            else:",
            "                temp_file_path = url_or_file_path",
            "            self.temp_files.add(temp_file_path)",
            "",
            "        return temp_file_path",
            "",
            "",
            "class BlockContext(Block):",
            "    def __init__(",
            "        self,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        visible: bool = True,",
            "        render: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            elem_id: An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",
            "            elem_classes: An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",
            "            visible: If False, this will be hidden but included in the Blocks config file (its visibility can later be updated).",
            "            render: If False, this will not be included in the Blocks config file at all.",
            "        \"\"\"",
            "        self.children: list[Block] = []",
            "        Block.__init__(",
            "            self,",
            "            elem_id=elem_id,",
            "            elem_classes=elem_classes,",
            "            visible=visible,",
            "            render=render,",
            "        )",
            "",
            "    TEMPLATE_DIR = \"./templates/\"",
            "    FRONTEND_DIR = \"../../frontend/\"",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return True",
            "",
            "    @classmethod",
            "    def get_component_class_id(cls) -> str:",
            "        module_name = cls.__module__",
            "        module_path = sys.modules[module_name].__file__",
            "        module_hash = hashlib.md5(f\"{cls.__name__}_{module_path}\".encode()).hexdigest()",
            "        return module_hash",
            "",
            "    @property",
            "    def component_class_id(self):",
            "        return self.get_component_class_id()",
            "",
            "    def add_child(self, child: Block):",
            "        self.children.append(child)",
            "",
            "    def __enter__(self):",
            "        self.parent = Context.block",
            "        Context.block = self",
            "        return self",
            "",
            "    def add(self, child: Block):",
            "        child.parent = self",
            "        self.children.append(child)",
            "",
            "    def fill_expected_parents(self):",
            "        children = []",
            "        pseudo_parent = None",
            "        for child in self.children:",
            "            expected_parent = child.get_expected_parent()",
            "            if not expected_parent or isinstance(self, expected_parent):",
            "                pseudo_parent = None",
            "                children.append(child)",
            "            else:",
            "                if pseudo_parent is not None and isinstance(",
            "                    pseudo_parent, expected_parent",
            "                ):",
            "                    pseudo_parent.add_child(child)",
            "                else:",
            "                    pseudo_parent = expected_parent(render=False)",
            "                    pseudo_parent.parent = self",
            "                    children.append(pseudo_parent)",
            "                    pseudo_parent.add_child(child)",
            "                    if Context.root_block:",
            "                        Context.root_block.blocks[pseudo_parent._id] = pseudo_parent",
            "                child.parent = pseudo_parent",
            "        self.children = children",
            "",
            "    def __exit__(self, exc_type: type[BaseException] | None = None, *args):",
            "        Context.block = self.parent",
            "        if exc_type is not None:",
            "            return",
            "        if getattr(self, \"allow_expected_parents\", True):",
            "            self.fill_expected_parents()",
            "",
            "    def postprocess(self, y):",
            "        \"\"\"",
            "        Any postprocessing needed to be performed on a block context.",
            "        \"\"\"",
            "        return y",
            "",
            "",
            "class BlockFunction:",
            "    def __init__(",
            "        self,",
            "        fn: Callable | None,",
            "        inputs: list[Component],",
            "        outputs: list[Component],",
            "        preprocess: bool,",
            "        postprocess: bool,",
            "        inputs_as_dict: bool,",
            "        batch: bool = False,",
            "        max_batch_size: int = 4,",
            "        concurrency_limit: int | None | Literal[\"default\"] = \"default\",",
            "        concurrency_id: str | None = None,",
            "        tracks_progress: bool = False,",
            "    ):",
            "        self.fn = fn",
            "        self.inputs = inputs",
            "        self.outputs = outputs",
            "        self.preprocess = preprocess",
            "        self.postprocess = postprocess",
            "        self.tracks_progress = tracks_progress",
            "        self.concurrency_limit: int | None | Literal[\"default\"] = concurrency_limit",
            "        self.concurrency_id = concurrency_id or str(id(fn))",
            "        self.batch = batch",
            "        self.max_batch_size = max_batch_size",
            "        self.total_runtime = 0",
            "        self.total_runs = 0",
            "        self.inputs_as_dict = inputs_as_dict",
            "        self.name = getattr(fn, \"__name__\", \"fn\") if fn is not None else None",
            "        self.spaces_auto_wrap()",
            "",
            "    def spaces_auto_wrap(self):",
            "        if spaces is None:",
            "            return",
            "        if utils.get_space() is None:",
            "            return",
            "        self.fn = spaces.gradio_auto_wrap(self.fn)",
            "",
            "    def __str__(self):",
            "        return str(",
            "            {",
            "                \"fn\": self.name,",
            "                \"preprocess\": self.preprocess,",
            "                \"postprocess\": self.postprocess,",
            "            }",
            "        )",
            "",
            "    def __repr__(self):",
            "        return str(self)",
            "",
            "",
            "def postprocess_update_dict(",
            "    block: Component | BlockContext, update_dict: dict, postprocess: bool = True",
            "):",
            "    \"\"\"",
            "    Converts a dictionary of updates into a format that can be sent to the frontend to update the component.",
            "    E.g. {\"value\": \"2\", \"visible\": True, \"invalid_arg\": \"hello\"}",
            "    Into -> {\"__type__\": \"update\", \"value\": 2.0, \"visible\": True}",
            "    Parameters:",
            "        block: The Block that is being updated with this update dictionary.",
            "        update_dict: The original update dictionary",
            "        postprocess: Whether to postprocess the \"value\" key of the update dictionary.",
            "    \"\"\"",
            "    value = update_dict.pop(\"value\", components._Keywords.NO_VALUE)",
            "    update_dict = {k: getattr(block, k) for k in update_dict if hasattr(block, k)}",
            "    if value is not components._Keywords.NO_VALUE:",
            "        if postprocess:",
            "            update_dict[\"value\"] = block.postprocess(value)",
            "            if isinstance(update_dict[\"value\"], (GradioModel, GradioRootModel)):",
            "                update_dict[\"value\"] = update_dict[\"value\"].model_dump()",
            "        else:",
            "            update_dict[\"value\"] = value",
            "    update_dict[\"__type__\"] = \"update\"",
            "    return update_dict",
            "",
            "",
            "def convert_component_dict_to_list(",
            "    outputs_ids: list[int], predictions: dict",
            ") -> list | dict:",
            "    \"\"\"",
            "    Converts a dictionary of component updates into a list of updates in the order of",
            "    the outputs_ids and including every output component. Leaves other types of dictionaries unchanged.",
            "    E.g. {\"textbox\": \"hello\", \"number\": {\"__type__\": \"generic_update\", \"value\": \"2\"}}",
            "    Into -> [\"hello\", {\"__type__\": \"generic_update\"}, {\"__type__\": \"generic_update\", \"value\": \"2\"}]",
            "    \"\"\"",
            "    keys_are_blocks = [isinstance(key, Block) for key in predictions]",
            "    if all(keys_are_blocks):",
            "        reordered_predictions = [skip() for _ in outputs_ids]",
            "        for component, value in predictions.items():",
            "            if component._id not in outputs_ids:",
            "                raise ValueError(",
            "                    f\"Returned component {component} not specified as output of function.\"",
            "                )",
            "            output_index = outputs_ids.index(component._id)",
            "            reordered_predictions[output_index] = value",
            "        predictions = utils.resolve_singleton(reordered_predictions)",
            "    elif any(keys_are_blocks):",
            "        raise ValueError(",
            "            \"Returned dictionary included some keys as Components. Either all keys must be Components to assign Component values, or return a List of values to assign output values in order.\"",
            "        )",
            "    return predictions",
            "",
            "",
            "@document(\"launch\", \"queue\", \"integrate\", \"load\")",
            "class Blocks(BlockContext, BlocksEvents, metaclass=BlocksMeta):",
            "    \"\"\"",
            "    Blocks is Gradio's low-level API that allows you to create more custom web",
            "    applications and demos than Interfaces (yet still entirely in Python).",
            "",
            "",
            "    Compared to the Interface class, Blocks offers more flexibility and control over:",
            "    (1) the layout of components (2) the events that",
            "    trigger the execution of functions (3) data flows (e.g. inputs can trigger outputs,",
            "    which can trigger the next level of outputs). Blocks also offers ways to group",
            "    together related demos such as with tabs.",
            "",
            "",
            "    The basic usage of Blocks is as follows: create a Blocks object, then use it as a",
            "    context (with the \"with\" statement), and then define layouts, components, or events",
            "    within the Blocks context. Finally, call the launch() method to launch the demo.",
            "",
            "    Example:",
            "        import gradio as gr",
            "        def update(name):",
            "            return f\"Welcome to Gradio, {name}!\"",
            "",
            "        with gr.Blocks() as demo:",
            "            gr.Markdown(\"Start typing below and then click **Run** to see the output.\")",
            "            with gr.Row():",
            "                inp = gr.Textbox(placeholder=\"What is your name?\")",
            "                out = gr.Textbox()",
            "            btn = gr.Button(\"Run\")",
            "            btn.click(fn=update, inputs=inp, outputs=out)",
            "",
            "        demo.launch()",
            "    Demos: blocks_hello, blocks_flipper, blocks_speech_text_sentiment, generate_english_german",
            "    Guides: blocks-and-event-listeners, controlling-layout, state-in-blocks, custom-CSS-and-JS, using-blocks-like-functions",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        theme: Theme | str | None = None,",
            "        analytics_enabled: bool | None = None,",
            "        mode: str = \"blocks\",",
            "        title: str = \"Gradio\",",
            "        css: str | None = None,",
            "        js: str | None = None,",
            "        head: str | None = None,",
            "        fill_height: bool = False,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            theme: A Theme object or a string representing a theme. If a string, will look for a built-in theme with that name (e.g. \"soft\" or \"default\"), or will attempt to load a theme from the Hugging Face Hub (e.g. \"gradio/monochrome\"). If None, will use the Default theme.",
            "            analytics_enabled: Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable or default to True.",
            "            mode: A human-friendly name for the kind of Blocks or Interface being created. Used internally for analytics.",
            "            title: The tab title to display when this is opened in a browser window.",
            "            css: Custom css as a string or path to a css file. This css will be included in the demo webpage.",
            "            js: Custom js or path to js file to run when demo is first loaded. This javascript will be included in the demo webpage.",
            "            head: Custom html to insert into the head of the demo webpage. This can be used to add custom meta tags, scripts, stylesheets, etc. to the page.",
            "            fill_height: Whether to vertically expand top-level child components to the height of the window. If True, expansion occurs when the scale value of the child components >= 1.",
            "        \"\"\"",
            "        self.limiter = None",
            "        if theme is None:",
            "            theme = DefaultTheme()",
            "        elif isinstance(theme, str):",
            "            if theme.lower() in BUILT_IN_THEMES:",
            "                theme = BUILT_IN_THEMES[theme.lower()]",
            "            else:",
            "                try:",
            "                    theme = Theme.from_hub(theme)",
            "                except Exception as e:",
            "                    warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")",
            "                    theme = DefaultTheme()",
            "        if not isinstance(theme, Theme):",
            "            warnings.warn(\"Theme should be a class loaded from gradio.themes\")",
            "            theme = DefaultTheme()",
            "        self.theme: Theme = theme",
            "        self.theme_css = theme._get_theme_css()",
            "        self.stylesheets = theme._stylesheets",
            "        self.encrypt = False",
            "        self.share = False",
            "        self.enable_queue = True",
            "        self.max_threads = 40",
            "        self.pending_streams = defaultdict(dict)",
            "        self.pending_diff_streams = defaultdict(dict)",
            "        self.show_error = True",
            "        self.head = head",
            "        self.fill_height = fill_height",
            "        if css is not None and os.path.exists(css):",
            "            with open(css) as css_file:",
            "                self.css = css_file.read()",
            "        else:",
            "            self.css = css",
            "        if js is not None and os.path.exists(js):",
            "            with open(js) as js_file:",
            "                self.js = js_file.read()",
            "        else:",
            "            self.js = js",
            "",
            "        # For analytics_enabled and allow_flagging: (1) first check for",
            "        # parameter, (2) check for env variable, (3) default to True/\"manual\"",
            "        self.analytics_enabled = (",
            "            analytics_enabled",
            "            if analytics_enabled is not None",
            "            else analytics.analytics_enabled()",
            "        )",
            "        if self.analytics_enabled:",
            "            if not wasm_utils.IS_WASM:",
            "                t = threading.Thread(target=analytics.version_check)",
            "                t.start()",
            "        else:",
            "            os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"True\"",
            "        super().__init__(render=False, **kwargs)",
            "        self.blocks: dict[int, Component | Block] = {}",
            "        self.fns: list[BlockFunction] = []",
            "        self.dependencies = []",
            "        self.mode = mode",
            "",
            "        self.is_running = False",
            "        self.local_url = None",
            "        self.share_url = None",
            "        self.width = None",
            "        self.height = None",
            "        self.api_open = utils.get_space() is None",
            "",
            "        self.space_id = utils.get_space()",
            "        self.favicon_path = None",
            "        self.auth = None",
            "        self.dev_mode = bool(os.getenv(\"GRADIO_WATCH_DIRS\", \"\"))",
            "        self.app_id = random.getrandbits(64)",
            "        self.temp_file_sets = []",
            "        self.title = title",
            "        self.show_api = not wasm_utils.IS_WASM",
            "",
            "        # Only used when an Interface is loaded from a config",
            "        self.predict = None",
            "        self.input_components = None",
            "        self.output_components = None",
            "        self.__name__ = None",
            "        self.api_mode = None",
            "",
            "        self.progress_tracking = None",
            "        self.ssl_verify = True",
            "",
            "        self.allowed_paths = []",
            "        self.blocked_paths = []",
            "        self.root_path = os.environ.get(\"GRADIO_ROOT_PATH\", \"\")",
            "        self.proxy_urls = set()",
            "",
            "        if self.analytics_enabled:",
            "            is_custom_theme = not any(",
            "                self.theme.to_dict() == built_in_theme.to_dict()",
            "                for built_in_theme in BUILT_IN_THEMES.values()",
            "            )",
            "            data = {",
            "                \"mode\": self.mode,",
            "                \"custom_css\": self.css is not None,",
            "                \"theme\": self.theme.name,",
            "                \"is_custom_theme\": is_custom_theme,",
            "                \"version\": get_package_version(),",
            "            }",
            "            analytics.initiated_analytics(data)",
            "",
            "        self.queue()",
            "",
            "    def get_component(self, id: int) -> Component | BlockContext:",
            "        comp = self.blocks[id]",
            "        assert isinstance(comp, (components.Component, BlockContext)), f\"{comp}\"",
            "        return comp",
            "",
            "    @property",
            "    def _is_running_in_reload_thread(self):",
            "        from gradio.cli.commands.reload import reload_thread",
            "",
            "        return getattr(reload_thread, \"running_reload\", False)",
            "",
            "    @classmethod",
            "    def from_config(",
            "        cls,",
            "        config: dict,",
            "        fns: list[Callable],",
            "        proxy_url: str,",
            "    ) -> Blocks:",
            "        \"\"\"",
            "        Factory method that creates a Blocks from a config and list of functions. Used",
            "        internally by the gradio.external.load() method.",
            "",
            "        Parameters:",
            "        config: a dictionary containing the configuration of the Blocks.",
            "        fns: a list of functions that are used in the Blocks. Must be in the same order as the dependencies in the config.",
            "        proxy_url: an external url to use as a root URL when serving files for components in the Blocks.",
            "        \"\"\"",
            "        config = copy.deepcopy(config)",
            "        components_config = config[\"components\"]",
            "        theme = config.get(\"theme\", \"default\")",
            "        original_mapping: dict[int, Block] = {}",
            "        proxy_urls = {proxy_url}",
            "",
            "        def get_block_instance(id: int) -> Block:",
            "            for block_config in components_config:",
            "                if block_config[\"id\"] == id:",
            "                    break",
            "            else:",
            "                raise ValueError(f\"Cannot find block with id {id}\")",
            "            cls = component_or_layout_class(block_config[\"type\"])",
            "",
            "            # If a Gradio app B is loaded into a Gradio app A, and B itself loads a",
            "            # Gradio app C, then the proxy_urls of the components in A need to be the",
            "            # URL of C, not B. The else clause below handles this case.",
            "            if block_config[\"props\"].get(\"proxy_url\") is None:",
            "                block_config[\"props\"][\"proxy_url\"] = f\"{proxy_url}/\"",
            "            postprocessed_value = block_config[\"props\"].pop(\"value\", None)",
            "",
            "            constructor_args = cls.recover_kwargs(block_config[\"props\"])",
            "            block = cls(**constructor_args)",
            "            if postprocessed_value is not None:",
            "                block.value = postprocessed_value  # type: ignore",
            "",
            "            block_proxy_url = block_config[\"props\"][\"proxy_url\"]",
            "            block.proxy_url = block_proxy_url",
            "            proxy_urls.add(block_proxy_url)",
            "            if (",
            "                _selectable := block_config[\"props\"].pop(\"_selectable\", None)",
            "            ) is not None:",
            "                block._selectable = _selectable  # type: ignore",
            "",
            "            return block",
            "",
            "        def iterate_over_children(children_list):",
            "            for child_config in children_list:",
            "                id = child_config[\"id\"]",
            "                block = get_block_instance(id)",
            "",
            "                original_mapping[id] = block",
            "",
            "                children = child_config.get(\"children\")",
            "                if children is not None:",
            "                    if not isinstance(block, BlockContext):",
            "                        raise ValueError(",
            "                            f\"Invalid config, Block with id {id} has children but is not a BlockContext.\"",
            "                        )",
            "                    with block:",
            "                        iterate_over_children(children)",
            "",
            "        derived_fields = [\"types\"]",
            "",
            "        with Blocks(theme=theme) as blocks:",
            "            # ID 0 should be the root Blocks component",
            "            original_mapping[0] = Context.root_block or blocks",
            "",
            "            iterate_over_children(config[\"layout\"][\"children\"])",
            "",
            "            first_dependency = None",
            "",
            "            # add the event triggers",
            "            for dependency, fn in zip(config[\"dependencies\"], fns):",
            "                # We used to add a \"fake_event\" to the config to cache examples",
            "                # without removing it. This was causing bugs in calling gr.load",
            "                # We fixed the issue by removing \"fake_event\" from the config in examples.py",
            "                # but we still need to skip these events when loading the config to support",
            "                # older demos",
            "                if \"trigger\" in dependency and dependency[\"trigger\"] == \"fake_event\":",
            "                    continue",
            "                for field in derived_fields:",
            "                    dependency.pop(field, None)",
            "",
            "                # older versions had a separate trigger field, but now it is part of the",
            "                # targets field",
            "                _targets = dependency.pop(\"targets\")",
            "                trigger = dependency.pop(\"trigger\", None)",
            "                targets = [",
            "                    getattr(",
            "                        original_mapping[",
            "                            target if isinstance(target, int) else target[0]",
            "                        ],",
            "                        trigger if isinstance(target, int) else target[1],",
            "                    )",
            "                    for target in _targets",
            "                ]",
            "                dependency.pop(\"backend_fn\")",
            "                dependency.pop(\"documentation\", None)",
            "                dependency[\"inputs\"] = [",
            "                    original_mapping[i] for i in dependency[\"inputs\"]",
            "                ]",
            "                dependency[\"outputs\"] = [",
            "                    original_mapping[o] for o in dependency[\"outputs\"]",
            "                ]",
            "                dependency.pop(\"status_tracker\", None)",
            "                dependency[\"preprocess\"] = False",
            "                dependency[\"postprocess\"] = False",
            "                targets = [",
            "                    EventListenerMethod(",
            "                        t.__self__ if t.has_trigger else None, t.event_name",
            "                    )",
            "                    for t in targets",
            "                ]",
            "                dependency = blocks.set_event_trigger(",
            "                    targets=targets, fn=fn, **dependency",
            "                )[0]",
            "                if first_dependency is None:",
            "                    first_dependency = dependency",
            "",
            "            # Allows some use of Interface-specific methods with loaded Spaces",
            "            if first_dependency and Context.root_block:",
            "                blocks.predict = [fns[0]]",
            "                blocks.input_components = [",
            "                    Context.root_block.blocks[i] for i in first_dependency[\"inputs\"]",
            "                ]",
            "                blocks.output_components = [",
            "                    Context.root_block.blocks[o] for o in first_dependency[\"outputs\"]",
            "                ]",
            "                blocks.__name__ = \"Interface\"",
            "                blocks.api_mode = True",
            "        blocks.proxy_urls = proxy_urls",
            "        return blocks",
            "",
            "    def __str__(self):",
            "        return self.__repr__()",
            "",
            "    def __repr__(self):",
            "        num_backend_fns = len([d for d in self.dependencies if d[\"backend_fn\"]])",
            "        repr = f\"Gradio Blocks instance: {num_backend_fns} backend functions\"",
            "        repr += f\"\\n{'-' * len(repr)}\"",
            "        for d, dependency in enumerate(self.dependencies):",
            "            if dependency[\"backend_fn\"]:",
            "                repr += f\"\\nfn_index={d}\"",
            "                repr += \"\\n inputs:\"",
            "                for input_id in dependency[\"inputs\"]:",
            "                    block = self.blocks[input_id]",
            "                    repr += f\"\\n |-{block}\"",
            "                repr += \"\\n outputs:\"",
            "                for output_id in dependency[\"outputs\"]:",
            "                    block = self.blocks[output_id]",
            "                    repr += f\"\\n |-{block}\"",
            "        return repr",
            "",
            "    @property",
            "    def expects_oauth(self):",
            "        \"\"\"Return whether the app expects user to authenticate via OAuth.\"\"\"",
            "        return any(",
            "            isinstance(block, (components.LoginButton, components.LogoutButton))",
            "            for block in self.blocks.values()",
            "        )",
            "",
            "    def set_event_trigger(",
            "        self,",
            "        targets: Sequence[EventListenerMethod],",
            "        fn: Callable | None,",
            "        inputs: Component | list[Component] | set[Component] | None,",
            "        outputs: Component | list[Component] | None,",
            "        preprocess: bool = True,",
            "        postprocess: bool = True,",
            "        scroll_to_output: bool = False,",
            "        show_progress: Literal[\"full\", \"minimal\", \"hidden\"] = \"full\",",
            "        api_name: str | None | Literal[False] = None,",
            "        js: str | None = None,",
            "        no_target: bool = False,",
            "        queue: bool | None = None,",
            "        batch: bool = False,",
            "        max_batch_size: int = 4,",
            "        cancels: list[int] | None = None,",
            "        every: float | None = None,",
            "        collects_event_data: bool | None = None,",
            "        trigger_after: int | None = None,",
            "        trigger_only_on_success: bool = False,",
            "        trigger_mode: Literal[\"once\", \"multiple\", \"always_last\"] | None = \"once\",",
            "        concurrency_limit: int | None | Literal[\"default\"] = \"default\",",
            "        concurrency_id: str | None = None,",
            "        show_api: bool = True,",
            "    ) -> tuple[dict[str, Any], int]:",
            "        \"\"\"",
            "        Adds an event to the component's dependencies.",
            "        Parameters:",
            "            targets: a list of EventListenerMethod objects that define the event trigger",
            "            fn: Callable function",
            "            inputs: input list",
            "            outputs: output list",
            "            preprocess: whether to run the preprocess methods of components",
            "            postprocess: whether to run the postprocess methods of components",
            "            scroll_to_output: whether to scroll to output of dependency on trigger",
            "            show_progress: whether to show progress animation while running.",
            "            api_name: defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",
            "            js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components",
            "            no_target: if True, sets \"targets\" to [], used for Blocks \"load\" event",
            "            queue: If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",
            "            batch: whether this function takes in a batch of inputs",
            "            max_batch_size: the maximum batch size to send to the function",
            "            cancels: a list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.",
            "            every: Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds.",
            "            collects_event_data: whether to collect event data for this event",
            "            trigger_after: if set, this event will be triggered after 'trigger_after' function index",
            "            trigger_only_on_success: if True, this event will only be triggered if the previous event was successful (only applies if `trigger_after` is set)",
            "            trigger_mode: If \"once\" (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to \"multiple\", unlimited submissions are allowed while pending, and \"always_last\" (default for `.change()` event) would allow a second submission after the pending event is complete.",
            "            concurrency_limit: If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `queue()`, which itself is 1 by default).",
            "            concurrency_id: If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",
            "            show_api: whether to show this event in the \"view API\" page of the Gradio app, or in the \".view_api()\" method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps to use this event. If fn is None, show_api will automatically be set to False.",
            "        Returns: dependency information, dependency index",
            "        \"\"\"",
            "        # Support for singular parameter",
            "        _targets = [",
            "            (",
            "                target.block._id if target.block and not no_target else None,",
            "                target.event_name,",
            "            )",
            "            for target in targets",
            "        ]",
            "        if isinstance(inputs, set):",
            "            inputs_as_dict = True",
            "            inputs = sorted(inputs, key=lambda x: x._id)",
            "        else:",
            "            inputs_as_dict = False",
            "            if inputs is None:",
            "                inputs = []",
            "            elif not isinstance(inputs, list):",
            "                inputs = [inputs]",
            "",
            "        if isinstance(outputs, set):",
            "            outputs = sorted(outputs, key=lambda x: x._id)",
            "        elif outputs is None:",
            "            outputs = []",
            "        elif not isinstance(outputs, list):",
            "            outputs = [outputs]",
            "",
            "        if fn is not None and not cancels:",
            "            check_function_inputs_match(fn, inputs, inputs_as_dict)",
            "        if every is not None and every <= 0:",
            "            raise ValueError(\"Parameter every must be positive or None\")",
            "        if every and batch:",
            "            raise ValueError(",
            "                f\"Cannot run event in a batch and every {every} seconds. \"",
            "                \"Either batch is True or every is non-zero but not both.\"",
            "            )",
            "",
            "        if every and fn:",
            "            fn = get_continuous_fn(fn, every)",
            "        elif every:",
            "            raise ValueError(\"Cannot set a value for `every` without a `fn`.\")",
            "        if every and concurrency_limit is not None:",
            "            if concurrency_limit == \"default\":",
            "                concurrency_limit = None",
            "            else:",
            "                raise ValueError(",
            "                    \"Cannot set a value for `concurrency_limit` with `every`.\"",
            "                )",
            "",
            "        if _targets[0][1] == \"change\" and trigger_mode is None:",
            "            trigger_mode = \"always_last\"",
            "        elif trigger_mode is None:",
            "            trigger_mode = \"once\"",
            "        elif trigger_mode not in [\"once\", \"multiple\", \"always_last\"]:",
            "            raise ValueError(",
            "                f\"Invalid value for parameter `trigger_mode`: {trigger_mode}. Please choose from: {['once', 'multiple', 'always_last']}\"",
            "            )",
            "",
            "        _, progress_index, event_data_index = (",
            "            special_args(fn) if fn else (None, None, None)",
            "        )",
            "        self.fns.append(",
            "            BlockFunction(",
            "                fn,",
            "                inputs,",
            "                outputs,",
            "                preprocess,",
            "                postprocess,",
            "                inputs_as_dict=inputs_as_dict,",
            "                concurrency_limit=concurrency_limit,",
            "                concurrency_id=concurrency_id,",
            "                batch=batch,",
            "                max_batch_size=max_batch_size,",
            "                tracks_progress=progress_index is not None,",
            "            )",
            "        )",
            "",
            "        # If api_name is None or empty string, use the function name",
            "        if api_name is None or isinstance(api_name, str) and api_name.strip() == \"\":",
            "            if fn is not None:",
            "                if not hasattr(fn, \"__name__\"):",
            "                    if hasattr(fn, \"__class__\") and hasattr(fn.__class__, \"__name__\"):",
            "                        name = fn.__class__.__name__",
            "                    else:",
            "                        name = \"unnamed\"",
            "                else:",
            "                    name = fn.__name__",
            "                api_name = \"\".join(",
            "                    [s for s in name if s not in set(string.punctuation) - {\"-\", \"_\"}]",
            "                )",
            "            elif js is not None:",
            "                api_name = \"js_fn\"",
            "                show_api = False",
            "            else:",
            "                api_name = \"unnamed\"",
            "                show_api = False",
            "",
            "        if api_name is not False:",
            "            api_name = utils.append_unique_suffix(",
            "                api_name, [dep[\"api_name\"] for dep in self.dependencies]",
            "            )",
            "        else:",
            "            show_api = False",
            "",
            "        # The `show_api` parameter is False if: (1) the user explicitly sets it (2) the user sets `api_name` to False",
            "        # or (3) the user sets `fn` to None (there's no backend function)",
            "",
            "        if collects_event_data is None:",
            "            collects_event_data = event_data_index is not None",
            "",
            "        dependency = {",
            "            \"targets\": _targets,",
            "            \"inputs\": [block._id for block in inputs],",
            "            \"outputs\": [block._id for block in outputs],",
            "            \"backend_fn\": fn is not None,",
            "            \"js\": js,",
            "            \"queue\": False if fn is None else queue,",
            "            \"api_name\": api_name,",
            "            \"scroll_to_output\": False if utils.get_space() else scroll_to_output,",
            "            \"show_progress\": show_progress,",
            "            \"every\": every,",
            "            \"batch\": batch,",
            "            \"max_batch_size\": max_batch_size,",
            "            \"cancels\": cancels or [],",
            "            \"types\": {",
            "                \"continuous\": bool(every),",
            "                \"generator\": inspect.isgeneratorfunction(fn)",
            "                or inspect.isasyncgenfunction(fn)",
            "                or bool(every),",
            "            },",
            "            \"collects_event_data\": collects_event_data,",
            "            \"trigger_after\": trigger_after,",
            "            \"trigger_only_on_success\": trigger_only_on_success,",
            "            \"trigger_mode\": trigger_mode,",
            "            \"show_api\": show_api,",
            "        }",
            "        self.dependencies.append(dependency)",
            "        return dependency, len(self.dependencies) - 1",
            "",
            "    def render(self):",
            "        if Context.root_block is not None:",
            "            if self._id in Context.root_block.blocks:",
            "                raise DuplicateBlockError(",
            "                    f\"A block with id: {self._id} has already been rendered in the current Blocks.\"",
            "                )",
            "            overlapping_ids = set(Context.root_block.blocks).intersection(self.blocks)",
            "            for id in overlapping_ids:",
            "                # State components are allowed to be reused between Blocks",
            "                if not isinstance(self.blocks[id], components.State):",
            "                    raise DuplicateBlockError(",
            "                        \"At least one block in this Blocks has already been rendered.\"",
            "                    )",
            "",
            "            Context.root_block.blocks.update(self.blocks)",
            "            Context.root_block.fns.extend(self.fns)",
            "            dependency_offset = len(Context.root_block.dependencies)",
            "            for i, dependency in enumerate(self.dependencies):",
            "                api_name = dependency[\"api_name\"]",
            "                if api_name is not None and api_name is not False:",
            "                    api_name_ = utils.append_unique_suffix(",
            "                        api_name,",
            "                        [dep[\"api_name\"] for dep in Context.root_block.dependencies],",
            "                    )",
            "                    if api_name != api_name_:",
            "                        dependency[\"api_name\"] = api_name_",
            "                dependency[\"cancels\"] = [",
            "                    c + dependency_offset for c in dependency[\"cancels\"]",
            "                ]",
            "                if dependency.get(\"trigger_after\") is not None:",
            "                    dependency[\"trigger_after\"] += dependency_offset",
            "                # Recreate the cancel function so that it has the latest",
            "                # dependency fn indices. This is necessary to properly cancel",
            "                # events in the backend",
            "                if dependency[\"cancels\"]:",
            "                    updated_cancels = [",
            "                        Context.root_block.dependencies[i]",
            "                        for i in dependency[\"cancels\"]",
            "                    ]",
            "                    new_fn = BlockFunction(",
            "                        get_cancel_function(updated_cancels)[0],",
            "                        [],",
            "                        [],",
            "                        False,",
            "                        True,",
            "                        False,",
            "                    )",
            "                    Context.root_block.fns[dependency_offset + i] = new_fn",
            "                Context.root_block.dependencies.append(dependency)",
            "            Context.root_block.temp_file_sets.extend(self.temp_file_sets)",
            "            Context.root_block.proxy_urls.update(self.proxy_urls)",
            "",
            "        if Context.block is not None:",
            "            Context.block.children.extend(self.children)",
            "        return self",
            "",
            "    def is_callable(self, fn_index: int = 0) -> bool:",
            "        \"\"\"Checks if a particular Blocks function is callable (i.e. not stateful or a generator).\"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        if inspect.isasyncgenfunction(block_fn.fn):",
            "            return False",
            "        if inspect.isgeneratorfunction(block_fn.fn):",
            "            return False",
            "        for input_id in dependency[\"inputs\"]:",
            "            block = self.blocks[input_id]",
            "            if getattr(block, \"stateful\", False):",
            "                return False",
            "        for output_id in dependency[\"outputs\"]:",
            "            block = self.blocks[output_id]",
            "            if getattr(block, \"stateful\", False):",
            "                return False",
            "",
            "        return True",
            "",
            "    def __call__(self, *inputs, fn_index: int = 0, api_name: str | None = None):",
            "        \"\"\"",
            "        Allows Blocks objects to be called as functions. Supply the parameters to the",
            "        function as positional arguments. To choose which function to call, use the",
            "        fn_index parameter, which must be a keyword argument.",
            "",
            "        Parameters:",
            "        *inputs: the parameters to pass to the function",
            "        fn_index: the index of the function to call (defaults to 0, which for Interfaces, is the default prediction function)",
            "        api_name: The api_name of the dependency to call. Will take precedence over fn_index.",
            "        \"\"\"",
            "        if api_name is not None:",
            "            inferred_fn_index = next(",
            "                (",
            "                    i",
            "                    for i, d in enumerate(self.dependencies)",
            "                    if d.get(\"api_name\") == api_name",
            "                ),",
            "                None,",
            "            )",
            "            if inferred_fn_index is None:",
            "                raise InvalidApiNameError(",
            "                    f\"Cannot find a function with api_name {api_name}\"",
            "                )",
            "            fn_index = inferred_fn_index",
            "        if not (self.is_callable(fn_index)):",
            "            raise ValueError(",
            "                \"This function is not callable because it is either stateful or is a generator. Please use the .launch() method instead to create an interactive user interface.\"",
            "            )",
            "",
            "        inputs = list(inputs)",
            "        processed_inputs = self.serialize_data(fn_index, inputs)",
            "        batch = self.dependencies[fn_index][\"batch\"]",
            "        if batch:",
            "            processed_inputs = [[inp] for inp in processed_inputs]",
            "",
            "        outputs = client_utils.synchronize_async(",
            "            self.process_api,",
            "            fn_index=fn_index,",
            "            inputs=processed_inputs,",
            "            request=None,",
            "            state={},",
            "        )",
            "        outputs = outputs[\"data\"]",
            "",
            "        if batch:",
            "            outputs = [out[0] for out in outputs]",
            "",
            "        outputs = self.deserialize_data(fn_index, outputs)",
            "        processed_outputs = utils.resolve_singleton(outputs)",
            "",
            "        return processed_outputs",
            "",
            "    async def call_function(",
            "        self,",
            "        fn_index: int,",
            "        processed_input: list[Any],",
            "        iterator: AsyncIterator[Any] | None = None,",
            "        requests: routes.Request | list[routes.Request] | None = None,",
            "        event_id: str | None = None,",
            "        event_data: EventData | None = None,",
            "        in_event_listener: bool = False,",
            "    ):",
            "        \"\"\"",
            "        Calls function with given index and preprocessed input, and measures process time.",
            "        Parameters:",
            "            fn_index: index of function to call",
            "            processed_input: preprocessed input to pass to function",
            "            iterator: iterator to use if function is a generator",
            "            requests: requests to pass to function",
            "            event_id: id of event in queue",
            "            event_data: data associated with event trigger",
            "        \"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        if not block_fn.fn:",
            "            raise IndexError(f\"function with index {fn_index} not defined.\")",
            "        is_generating = False",
            "        request = requests[0] if isinstance(requests, list) else requests",
            "        start = time.time()",
            "",
            "        fn = utils.get_function_with_locals(",
            "            fn=block_fn.fn,",
            "            blocks=self,",
            "            event_id=event_id,",
            "            in_event_listener=in_event_listener,",
            "            request=request,",
            "        )",
            "",
            "        if iterator is None:  # If not a generator function that has already run",
            "            if block_fn.inputs_as_dict:",
            "                processed_input = [dict(zip(block_fn.inputs, processed_input))]",
            "",
            "            processed_input, progress_index, _ = special_args(",
            "                block_fn.fn, processed_input, request, event_data",
            "            )",
            "            progress_tracker = (",
            "                processed_input[progress_index] if progress_index is not None else None",
            "            )",
            "",
            "            if progress_tracker is not None and progress_index is not None:",
            "                progress_tracker, fn = create_tracker(fn, progress_tracker.track_tqdm)",
            "                processed_input[progress_index] = progress_tracker",
            "",
            "            if inspect.iscoroutinefunction(fn):",
            "                prediction = await fn(*processed_input)",
            "            else:",
            "                prediction = await anyio.to_thread.run_sync(",
            "                    fn, *processed_input, limiter=self.limiter",
            "                )",
            "        else:",
            "            prediction = None",
            "",
            "        if inspect.isgeneratorfunction(fn) or inspect.isasyncgenfunction(fn):",
            "            try:",
            "                if iterator is None:",
            "                    iterator = cast(AsyncIterator[Any], prediction)",
            "                if inspect.isgenerator(iterator):",
            "                    iterator = utils.SyncToAsyncIterator(iterator, self.limiter)",
            "                prediction = await utils.async_iteration(iterator)",
            "                is_generating = True",
            "            except StopAsyncIteration:",
            "                n_outputs = len(self.dependencies[fn_index].get(\"outputs\"))",
            "                prediction = (",
            "                    components._Keywords.FINISHED_ITERATING",
            "                    if n_outputs == 1",
            "                    else (components._Keywords.FINISHED_ITERATING,) * n_outputs",
            "                )",
            "                iterator = None",
            "",
            "        duration = time.time() - start",
            "",
            "        return {",
            "            \"prediction\": prediction,",
            "            \"duration\": duration,",
            "            \"is_generating\": is_generating,",
            "            \"iterator\": iterator,",
            "        }",
            "",
            "    def serialize_data(self, fn_index: int, inputs: list[Any]) -> list[Any]:",
            "        dependency = self.dependencies[fn_index]",
            "        processed_input = []",
            "",
            "        def format_file(s):",
            "            return FileData(path=s).model_dump()",
            "",
            "        for i, input_id in enumerate(dependency[\"inputs\"]):",
            "            try:",
            "                block = self.blocks[input_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Input component with id {input_id} used in {dependency['trigger']}() event is not defined in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "            if not isinstance(block, components.Component):",
            "                raise InvalidComponentError(",
            "                    f\"{block.__class__} Component with id {input_id} not a valid input component.\"",
            "                )",
            "            api_info = block.api_info()",
            "            if client_utils.value_is_file(api_info):",
            "                serialized_input = client_utils.traverse(",
            "                    inputs[i],",
            "                    format_file,",
            "                    lambda s: client_utils.is_filepath(s) or client_utils.is_url(s),",
            "                )",
            "            else:",
            "                serialized_input = inputs[i]",
            "            processed_input.append(serialized_input)",
            "",
            "        return processed_input",
            "",
            "    def deserialize_data(self, fn_index: int, outputs: list[Any]) -> list[Any]:",
            "        dependency = self.dependencies[fn_index]",
            "        predictions = []",
            "",
            "        for o, output_id in enumerate(dependency[\"outputs\"]):",
            "            try:",
            "                block = self.blocks[output_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Output component with id {output_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "            if not isinstance(block, components.Component):",
            "                raise InvalidComponentError(",
            "                    f\"{block.__class__} Component with id {output_id} not a valid output component.\"",
            "                )",
            "",
            "            deserialized = client_utils.traverse(",
            "                outputs[o], lambda s: s[\"path\"], client_utils.is_file_obj",
            "            )",
            "            predictions.append(deserialized)",
            "",
            "        return predictions",
            "",
            "    def validate_inputs(self, fn_index: int, inputs: list[Any]):",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        dep_inputs = dependency[\"inputs\"]",
            "",
            "        # This handles incorrect inputs when args are changed by a JS function",
            "        # Only check not enough args case, ignore extra arguments (for now)",
            "        # TODO: make this stricter?",
            "        if len(inputs) < len(dep_inputs):",
            "            name = (",
            "                f\" ({block_fn.name})\"",
            "                if block_fn.name and block_fn.name != \"<lambda>\"",
            "                else \"\"",
            "            )",
            "",
            "            wanted_args = []",
            "            received_args = []",
            "            for input_id in dep_inputs:",
            "                block = self.blocks[input_id]",
            "                wanted_args.append(str(block))",
            "            for inp in inputs:",
            "                v = f'\"{inp}\"' if isinstance(inp, str) else str(inp)",
            "                received_args.append(v)",
            "",
            "            wanted = \", \".join(wanted_args)",
            "            received = \", \".join(received_args)",
            "",
            "            # JS func didn't pass enough arguments",
            "            raise ValueError(",
            "                f\"\"\"An event handler{name} didn't receive enough input values (needed: {len(dep_inputs)}, got: {len(inputs)}).",
            "Check if the event handler calls a Javascript function, and make sure its return value is correct.",
            "Wanted inputs:",
            "    [{wanted}]",
            "Received inputs:",
            "    [{received}]\"\"\"",
            "            )",
            "",
            "    def preprocess_data(",
            "        self, fn_index: int, inputs: list[Any], state: SessionState | None",
            "    ):",
            "        state = state or SessionState(self)",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        self.validate_inputs(fn_index, inputs)",
            "",
            "        if block_fn.preprocess:",
            "            processed_input = []",
            "            for i, input_id in enumerate(dependency[\"inputs\"]):",
            "                try:",
            "                    block = self.blocks[input_id]",
            "                except KeyError as e:",
            "                    raise InvalidBlockError(",
            "                        f\"Input component with id {input_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                    ) from e",
            "                if not isinstance(block, components.Component):",
            "                    raise InvalidComponentError(",
            "                        f\"{block.__class__} Component with id {input_id} not a valid input component.\"",
            "                    )",
            "                if getattr(block, \"stateful\", False):",
            "                    processed_input.append(state[input_id])",
            "                else:",
            "                    if input_id in state:",
            "                        block = state[input_id]",
            "                    inputs_cached = processing_utils.move_files_to_cache(",
            "                        inputs[i], block",
            "                    )",
            "                    if getattr(block, \"data_model\", None) and inputs_cached is not None:",
            "                        if issubclass(block.data_model, GradioModel):  # type: ignore",
            "                            inputs_cached = block.data_model(**inputs_cached)  # type: ignore",
            "                        elif issubclass(block.data_model, GradioRootModel):  # type: ignore",
            "                            inputs_cached = block.data_model(root=inputs_cached)  # type: ignore",
            "                    processed_input.append(block.preprocess(inputs_cached))",
            "        else:",
            "            processed_input = inputs",
            "        return processed_input",
            "",
            "    def validate_outputs(self, fn_index: int, predictions: Any | list[Any]):",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        dep_outputs = dependency[\"outputs\"]",
            "",
            "        if not isinstance(predictions, (list, tuple)):",
            "            predictions = [predictions]",
            "",
            "        if len(predictions) < len(dep_outputs):",
            "            name = (",
            "                f\" ({block_fn.name})\"",
            "                if block_fn.name and block_fn.name != \"<lambda>\"",
            "                else \"\"",
            "            )",
            "",
            "            wanted_args = []",
            "            received_args = []",
            "            for output_id in dep_outputs:",
            "                block = self.blocks[output_id]",
            "                wanted_args.append(str(block))",
            "            for pred in predictions:",
            "                v = f'\"{pred}\"' if isinstance(pred, str) else str(pred)",
            "                received_args.append(v)",
            "",
            "            wanted = \", \".join(wanted_args)",
            "            received = \", \".join(received_args)",
            "",
            "            raise ValueError(",
            "                f\"\"\"An event handler{name} didn't receive enough output values (needed: {len(dep_outputs)}, received: {len(predictions)}).",
            "Wanted outputs:",
            "    [{wanted}]",
            "Received outputs:",
            "    [{received}]\"\"\"",
            "            )",
            "",
            "    def postprocess_data(",
            "        self, fn_index: int, predictions: list | dict, state: SessionState | None",
            "    ):",
            "        state = state or SessionState(self)",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "        batch = dependency[\"batch\"]",
            "",
            "        if isinstance(predictions, dict) and len(predictions) > 0:",
            "            predictions = convert_component_dict_to_list(",
            "                dependency[\"outputs\"], predictions",
            "            )",
            "",
            "        if len(dependency[\"outputs\"]) == 1 and not (batch):",
            "            predictions = [",
            "                predictions,",
            "            ]",
            "",
            "        self.validate_outputs(fn_index, predictions)  # type: ignore",
            "",
            "        output = []",
            "        for i, output_id in enumerate(dependency[\"outputs\"]):",
            "            try:",
            "                if predictions[i] is components._Keywords.FINISHED_ITERATING:",
            "                    output.append(None)",
            "                    continue",
            "            except (IndexError, KeyError) as err:",
            "                raise ValueError(",
            "                    \"Number of output components does not match number \"",
            "                    f\"of values returned from from function {block_fn.name}\"",
            "                ) from err",
            "",
            "            try:",
            "                block = self.blocks[output_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Output component with id {output_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "",
            "            if getattr(block, \"stateful\", False):",
            "                if not utils.is_update(predictions[i]):",
            "                    state[output_id] = predictions[i]",
            "                output.append(None)",
            "            else:",
            "                prediction_value = predictions[i]",
            "                if utils.is_update(",
            "                    prediction_value",
            "                ):  # if update is passed directly (deprecated), remove Nones",
            "                    prediction_value = utils.delete_none(",
            "                        prediction_value, skip_value=True",
            "                    )",
            "",
            "                if isinstance(prediction_value, Block):",
            "                    prediction_value = prediction_value.constructor_args.copy()",
            "                    prediction_value[\"__type__\"] = \"update\"",
            "                if utils.is_update(prediction_value):",
            "                    if output_id in state:",
            "                        kwargs = state[output_id].constructor_args.copy()",
            "                    else:",
            "                        kwargs = self.blocks[output_id].constructor_args.copy()",
            "                    kwargs.update(prediction_value)",
            "                    kwargs.pop(\"value\", None)",
            "                    kwargs.pop(\"__type__\")",
            "                    kwargs[\"render\"] = False",
            "                    state[output_id] = self.blocks[output_id].__class__(**kwargs)",
            "",
            "                    prediction_value = postprocess_update_dict(",
            "                        block=state[output_id],",
            "                        update_dict=prediction_value,",
            "                        postprocess=block_fn.postprocess,",
            "                    )",
            "                elif block_fn.postprocess:",
            "                    if not isinstance(block, components.Component):",
            "                        raise InvalidComponentError(",
            "                            f\"{block.__class__} Component with id {output_id} not a valid output component.\"",
            "                        )",
            "                    prediction_value = block.postprocess(prediction_value)",
            "                outputs_cached = processing_utils.move_files_to_cache(",
            "                    prediction_value,",
            "                    block,  # type: ignore",
            "                    postprocess=True,",
            "                )",
            "                output.append(outputs_cached)",
            "",
            "        return output",
            "",
            "    def handle_streaming_outputs(",
            "        self,",
            "        fn_index: int,",
            "        data: list,",
            "        session_hash: str | None,",
            "        run: int | None,",
            "    ) -> list:",
            "        if session_hash is None or run is None:",
            "            return data",
            "        if run not in self.pending_streams[session_hash]:",
            "            self.pending_streams[session_hash][run] = {}",
            "        stream_run = self.pending_streams[session_hash][run]",
            "",
            "        for i, output_id in enumerate(self.dependencies[fn_index][\"outputs\"]):",
            "            block = self.blocks[output_id]",
            "            if isinstance(block, components.StreamingOutput) and block.streaming:",
            "                first_chunk = output_id not in stream_run",
            "                binary_data, output_data = block.stream_output(",
            "                    data[i], f\"{session_hash}/{run}/{output_id}\", first_chunk",
            "                )",
            "                if first_chunk:",
            "                    stream_run[output_id] = []",
            "                self.pending_streams[session_hash][run][output_id].append(binary_data)",
            "                data[i] = output_data",
            "        return data",
            "",
            "    def handle_streaming_diffs(",
            "        self,",
            "        fn_index: int,",
            "        data: list,",
            "        session_hash: str | None,",
            "        run: int | None,",
            "        final: bool,",
            "    ) -> list:",
            "        if session_hash is None or run is None:",
            "            return data",
            "        first_run = run not in self.pending_diff_streams[session_hash]",
            "        if first_run:",
            "            self.pending_diff_streams[session_hash][run] = [None] * len(data)",
            "        last_diffs = self.pending_diff_streams[session_hash][run]",
            "",
            "        for i in range(len(self.dependencies[fn_index][\"outputs\"])):",
            "            if final:",
            "                data[i] = last_diffs[i]",
            "                continue",
            "",
            "            if first_run:",
            "                last_diffs[i] = data[i]",
            "            else:",
            "                prev_chunk = last_diffs[i]",
            "                last_diffs[i] = data[i]",
            "                data[i] = utils.diff(prev_chunk, data[i])",
            "",
            "        if final:",
            "            del self.pending_diff_streams[session_hash][run]",
            "",
            "        return data",
            "",
            "    async def process_api(",
            "        self,",
            "        fn_index: int,",
            "        inputs: list[Any],",
            "        state: SessionState | None = None,",
            "        request: routes.Request | list[routes.Request] | None = None,",
            "        iterator: AsyncIterator | None = None,",
            "        session_hash: str | None = None,",
            "        event_id: str | None = None,",
            "        event_data: EventData | None = None,",
            "        in_event_listener: bool = True,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Processes API calls from the frontend. First preprocesses the data,",
            "        then runs the relevant function, then postprocesses the output.",
            "        Parameters:",
            "            fn_index: Index of function to run.",
            "            inputs: input data received from the frontend",
            "            state: data stored from stateful components for session (key is input block id)",
            "            request: the gr.Request object containing information about the network request (e.g. IP address, headers, query parameters, username)",
            "            iterators: the in-progress iterators for each generator function (key is function index)",
            "            event_id: id of event that triggered this API call",
            "            event_data: data associated with the event trigger itself",
            "        Returns: None",
            "        \"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        batch = self.dependencies[fn_index][\"batch\"]",
            "",
            "        if batch:",
            "            max_batch_size = self.dependencies[fn_index][\"max_batch_size\"]",
            "            batch_sizes = [len(inp) for inp in inputs]",
            "            batch_size = batch_sizes[0]",
            "            if inspect.isasyncgenfunction(block_fn.fn) or inspect.isgeneratorfunction(",
            "                block_fn.fn",
            "            ):",
            "                raise ValueError(\"Gradio does not support generators in batch mode.\")",
            "            if not all(x == batch_size for x in batch_sizes):",
            "                raise ValueError(",
            "                    f\"All inputs to a batch function must have the same length but instead have sizes: {batch_sizes}.\"",
            "                )",
            "            if batch_size > max_batch_size:",
            "                raise ValueError(",
            "                    f\"Batch size ({batch_size}) exceeds the max_batch_size for this function ({max_batch_size})\"",
            "                )",
            "",
            "            inputs = [",
            "                self.preprocess_data(fn_index, list(i), state) for i in zip(*inputs)",
            "            ]",
            "            result = await self.call_function(",
            "                fn_index,",
            "                list(zip(*inputs)),",
            "                None,",
            "                request,",
            "                event_id,",
            "                event_data,",
            "                in_event_listener,",
            "            )",
            "            preds = result[\"prediction\"]",
            "            data = [",
            "                self.postprocess_data(fn_index, list(o), state) for o in zip(*preds)",
            "            ]",
            "            data = list(zip(*data))",
            "            is_generating, iterator = None, None",
            "        else:",
            "            old_iterator = iterator",
            "            if old_iterator:",
            "                inputs = []",
            "            else:",
            "                inputs = self.preprocess_data(fn_index, inputs, state)",
            "            was_generating = old_iterator is not None",
            "            result = await self.call_function(",
            "                fn_index,",
            "                inputs,",
            "                old_iterator,",
            "                request,",
            "                event_id,",
            "                event_data,",
            "                in_event_listener,",
            "            )",
            "            data = self.postprocess_data(fn_index, result[\"prediction\"], state)",
            "            is_generating, iterator = result[\"is_generating\"], result[\"iterator\"]",
            "            if is_generating or was_generating:",
            "                run = id(old_iterator) if was_generating else id(iterator)",
            "                data = self.handle_streaming_outputs(",
            "                    fn_index,",
            "                    data,",
            "                    session_hash=session_hash,",
            "                    run=run,",
            "                )",
            "                data = self.handle_streaming_diffs(",
            "                    fn_index,",
            "                    data,",
            "                    session_hash=session_hash,",
            "                    run=run,",
            "                    final=not is_generating,",
            "                )",
            "",
            "        block_fn.total_runtime += result[\"duration\"]",
            "        block_fn.total_runs += 1",
            "        return {",
            "            \"data\": data,",
            "            \"is_generating\": is_generating,",
            "            \"iterator\": iterator,",
            "            \"duration\": result[\"duration\"],",
            "            \"average_duration\": block_fn.total_runtime / block_fn.total_runs,",
            "        }",
            "",
            "    def create_limiter(self):",
            "        self.limiter = (",
            "            None",
            "            if self.max_threads == 40",
            "            else CapacityLimiter(total_tokens=self.max_threads)",
            "        )",
            "",
            "    def get_config(self):",
            "        return {\"type\": \"column\"}",
            "",
            "    def get_config_file(self):",
            "        config = {",
            "            \"version\": routes.VERSION,",
            "            \"mode\": self.mode,",
            "            \"app_id\": self.app_id,",
            "            \"dev_mode\": self.dev_mode,",
            "            \"analytics_enabled\": self.analytics_enabled,",
            "            \"components\": [],",
            "            \"css\": self.css,",
            "            \"js\": self.js,",
            "            \"head\": self.head,",
            "            \"title\": self.title or \"Gradio\",",
            "            \"space_id\": self.space_id,",
            "            \"enable_queue\": True,  # launch attributes",
            "            \"show_error\": getattr(self, \"show_error\", False),",
            "            \"show_api\": self.show_api,",
            "            \"is_colab\": utils.colab_check(),",
            "            \"stylesheets\": self.stylesheets,",
            "            \"theme\": self.theme.name,",
            "            \"protocol\": \"sse_v2\",",
            "            \"body_css\": {",
            "                \"body_background_fill\": self.theme._get_computed_value(",
            "                    \"body_background_fill\"",
            "                ),",
            "                \"body_text_color\": self.theme._get_computed_value(\"body_text_color\"),",
            "                \"body_background_fill_dark\": self.theme._get_computed_value(",
            "                    \"body_background_fill_dark\"",
            "                ),",
            "                \"body_text_color_dark\": self.theme._get_computed_value(",
            "                    \"body_text_color_dark\"",
            "                ),",
            "            },",
            "            \"fill_height\": self.fill_height,",
            "        }",
            "",
            "        def get_layout(block):",
            "            if not isinstance(block, BlockContext):",
            "                return {\"id\": block._id}",
            "            children_layout = []",
            "            for child in block.children:",
            "                children_layout.append(get_layout(child))",
            "            return {\"id\": block._id, \"children\": children_layout}",
            "",
            "        config[\"layout\"] = get_layout(self)",
            "",
            "        for _id, block in self.blocks.items():",
            "            props = block.get_config() if hasattr(block, \"get_config\") else {}",
            "            block_config = {",
            "                \"id\": _id,",
            "                \"type\": block.get_block_name(),",
            "                \"props\": utils.delete_none(props),",
            "            }",
            "            block_config[\"skip_api\"] = block.skip_api",
            "            block_config[\"component_class_id\"] = getattr(",
            "                block, \"component_class_id\", None",
            "            )",
            "",
            "            if not block.skip_api:",
            "                block_config[\"api_info\"] = block.api_info()  # type: ignore",
            "                block_config[\"example_inputs\"] = block.example_inputs()  # type: ignore",
            "            config[\"components\"].append(block_config)",
            "        config[\"dependencies\"] = self.dependencies",
            "        return config",
            "",
            "    def __enter__(self):",
            "        if Context.block is None:",
            "            Context.root_block = self",
            "        self.parent = Context.block",
            "        Context.block = self",
            "        self.exited = False",
            "        return self",
            "",
            "    def __exit__(self, exc_type: type[BaseException] | None = None, *args):",
            "        if exc_type is not None:",
            "            Context.block = None",
            "            Context.root_block = None",
            "            return",
            "        super().fill_expected_parents()",
            "        Context.block = self.parent",
            "        # Configure the load events before root_block is reset",
            "        self.attach_load_events()",
            "        if self.parent is None:",
            "            Context.root_block = None",
            "        else:",
            "            self.parent.children.extend(self.children)",
            "        self.config = self.get_config_file()",
            "        self.app = routes.App.create_app(self)",
            "        self.progress_tracking = any(block_fn.tracks_progress for block_fn in self.fns)",
            "        self.exited = True",
            "",
            "    def clear(self):",
            "        \"\"\"Resets the layout of the Blocks object.\"\"\"",
            "        self.blocks = {}",
            "        self.fns = []",
            "        self.dependencies = []",
            "        self.children = []",
            "        return self",
            "",
            "    @document()",
            "    def queue(",
            "        self,",
            "        status_update_rate: float | Literal[\"auto\"] = \"auto\",",
            "        api_open: bool | None = None,",
            "        max_size: int | None = None,",
            "        concurrency_count: int | None = None,",
            "        *,",
            "        default_concurrency_limit: int | None | Literal[\"not_set\"] = \"not_set\",",
            "    ):",
            "        \"\"\"",
            "        By enabling the queue you can control when users know their position in the queue, and set a limit on maximum number of events allowed.",
            "        Parameters:",
            "            status_update_rate: If \"auto\", Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.",
            "            api_open: If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.",
            "            max_size: The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.",
            "            concurrency_count: Deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().",
            "            default_concurrency_limit: The default value of `concurrency_limit` to use for event listeners that don't specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise.",
            "        Example: (Blocks)",
            "            with gr.Blocks() as demo:",
            "                button = gr.Button(label=\"Generate Image\")",
            "                button.click(fn=image_generator, inputs=gr.Textbox(), outputs=gr.Image())",
            "            demo.queue(max_size=10)",
            "            demo.launch()",
            "        Example: (Interface)",
            "            demo = gr.Interface(image_generator, gr.Textbox(), gr.Image())",
            "            demo.queue(max_size=20)",
            "            demo.launch()",
            "        \"\"\"",
            "        if concurrency_count:",
            "            raise DeprecationWarning(",
            "                \"concurrency_count has been deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().\"",
            "            )",
            "        if api_open is not None:",
            "            self.api_open = api_open",
            "        if utils.is_zero_gpu_space():",
            "            max_size = 1 if max_size is None else max_size",
            "        self._queue = queueing.Queue(",
            "            live_updates=status_update_rate == \"auto\",",
            "            concurrency_count=self.max_threads,",
            "            update_intervals=status_update_rate if status_update_rate != \"auto\" else 1,",
            "            max_size=max_size,",
            "            block_fns=self.fns,",
            "            default_concurrency_limit=default_concurrency_limit,",
            "        )",
            "        self.config = self.get_config_file()",
            "        self.app = routes.App.create_app(self)",
            "        return self",
            "",
            "    def validate_queue_settings(self):",
            "        for dep in self.dependencies:",
            "            for i in dep[\"cancels\"]:",
            "                if not self.queue_enabled_for_fn(i):",
            "                    raise ValueError(",
            "                        \"Queue needs to be enabled! \"",
            "                        \"You may get this error by either 1) passing a function that uses the yield keyword \"",
            "                        \"into an interface without enabling the queue or 2) defining an event that cancels \"",
            "                        \"another event without enabling the queue. Both can be solved by calling .queue() \"",
            "                        \"before .launch()\"",
            "                    )",
            "            if dep[\"batch\"] and dep[\"queue\"] is False:",
            "                raise ValueError(\"In order to use batching, the queue must be enabled.\")",
            "",
            "    def launch(",
            "        self,",
            "        inline: bool | None = None,",
            "        inbrowser: bool = False,",
            "        share: bool | None = None,",
            "        debug: bool = False,",
            "        max_threads: int = 40,",
            "        auth: Callable | tuple[str, str] | list[tuple[str, str]] | None = None,",
            "        auth_message: str | None = None,",
            "        prevent_thread_lock: bool = False,",
            "        show_error: bool = False,",
            "        server_name: str | None = None,",
            "        server_port: int | None = None,",
            "        *,",
            "        height: int = 500,",
            "        width: int | str = \"100%\",",
            "        favicon_path: str | None = None,",
            "        ssl_keyfile: str | None = None,",
            "        ssl_certfile: str | None = None,",
            "        ssl_keyfile_password: str | None = None,",
            "        ssl_verify: bool = True,",
            "        quiet: bool = False,",
            "        show_api: bool = True,",
            "        allowed_paths: list[str] | None = None,",
            "        blocked_paths: list[str] | None = None,",
            "        root_path: str | None = None,",
            "        app_kwargs: dict[str, Any] | None = None,",
            "        state_session_capacity: int = 10000,",
            "        share_server_address: str | None = None,",
            "        share_server_protocol: Literal[\"http\", \"https\"] | None = None,",
            "        _frontend: bool = True,",
            "    ) -> tuple[FastAPI, str, str]:",
            "        \"\"\"",
            "        Launches a simple web server that serves the demo. Can also be used to create a",
            "        public link used by anyone to access the demo from their browser by setting share=True.",
            "",
            "        Parameters:",
            "            inline: whether to display in the interface inline in an iframe. Defaults to True in python notebooks; False otherwise.",
            "            inbrowser: whether to automatically launch the interface in a new tab on the default browser.",
            "            share: whether to create a publicly shareable link for the interface. Creates an SSH tunnel to make your UI accessible from anywhere. If not provided, it is set to False by default every time, except when running in Google Colab. When localhost is not accessible (e.g. Google Colab), setting share=False is not supported.",
            "            debug: if True, blocks the main thread from running. If running in Google Colab, this is needed to print the errors in the cell output.",
            "            auth: If provided, username and password (or list of username-password tuples) required to access interface. Can also provide function that takes username and password and returns True if valid login.",
            "            auth_message: If provided, HTML message provided on login page.",
            "            prevent_thread_lock: If True, the interface will block the main thread while the server is running.",
            "            show_error: If True, any errors in the interface will be displayed in an alert modal and printed in the browser console log",
            "            server_port: will start gradio app on this port (if available). Can be set by environment variable GRADIO_SERVER_PORT. If None, will search for an available port starting at 7860.",
            "            server_name: to make app accessible on local network, set this to \"0.0.0.0\". Can be set by environment variable GRADIO_SERVER_NAME. If None, will use \"127.0.0.1\".",
            "            max_threads: the maximum number of total threads that the Gradio app can generate in parallel. The default is inherited from the starlette library (currently 40).",
            "            width: The width in pixels of the iframe element containing the interface (used if inline=True)",
            "            height: The height in pixels of the iframe element containing the interface (used if inline=True)",
            "            favicon_path: If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for the web page.",
            "            ssl_keyfile: If a path to a file is provided, will use this as the private key file to create a local server running on https.",
            "            ssl_certfile: If a path to a file is provided, will use this as the signed certificate for https. Needs to be provided if ssl_keyfile is provided.",
            "            ssl_keyfile_password: If a password is provided, will use this with the ssl certificate for https.",
            "            ssl_verify: If False, skips certificate validation which allows self-signed certificates to be used.",
            "            quiet: If True, suppresses most print statements.",
            "            show_api: If True, shows the api docs in the footer of the app. Default True.",
            "            allowed_paths: List of complete filepaths or parent directories that gradio is allowed to serve (in addition to the directory containing the gradio python file). Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",
            "            blocked_paths: List of complete filepaths or parent directories that gradio is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",
            "            root_path: The root path (or \"mount point\") of the application, if it's not served from the root (\"/\") of the domain. Often used when the application is behind a reverse proxy that forwards requests to the application. For example, if the application is served at \"https://example.com/myapp\", the `root_path` should be set to \"/myapp\". Can be set by environment variable GRADIO_ROOT_PATH. Defaults to \"\".",
            "            app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`",
            "            state_session_capacity: The maximum number of sessions whose information to store in memory. If the number of sessions exceeds this number, the oldest sessions will be removed. Reduce capacity to reduce memory usage when using gradio.State or returning updated components from functions. Defaults to 10000.",
            "            share_server_address: Use this to specify a custom FRP server and port for sharing Gradio apps (only applies if share=True). If not provided, will use the default FRP server at https://gradio.live. See https://github.com/huggingface/frp for more information.",
            "            share_server_protocol: Use this to specify the protocol to use for the share links. Defaults to \"https\", unless a custom share_server_address is provided, in which case it defaults to \"http\". If you are using a custom share_server_address and want to use https, you must set this to \"https\".",
            "        Returns:",
            "            app: FastAPI app object that is running the demo",
            "            local_url: Locally accessible link to the demo",
            "            share_url: Publicly accessible link to the demo (if share=True, otherwise None)",
            "        Example: (Blocks)",
            "            import gradio as gr",
            "            def reverse(text):",
            "                return text[::-1]",
            "            with gr.Blocks() as demo:",
            "                button = gr.Button(value=\"Reverse\")",
            "                button.click(reverse, gr.Textbox(), gr.Textbox())",
            "            demo.launch(share=True, auth=(\"username\", \"password\"))",
            "        Example:  (Interface)",
            "            import gradio as gr",
            "            def reverse(text):",
            "                return text[::-1]",
            "            demo = gr.Interface(reverse, \"text\", \"text\")",
            "            demo.launch(share=True, auth=(\"username\", \"password\"))",
            "        \"\"\"",
            "        if self._is_running_in_reload_thread:",
            "            # We have already launched the demo",
            "            return None, None, None  # type: ignore",
            "",
            "        if not self.exited:",
            "            self.__exit__()",
            "",
            "        if (",
            "            auth",
            "            and not callable(auth)",
            "            and not isinstance(auth[0], tuple)",
            "            and not isinstance(auth[0], list)",
            "        ):",
            "            self.auth = [auth]",
            "        else:",
            "            self.auth = auth",
            "        self.auth_message = auth_message",
            "        self.show_error = show_error",
            "        self.height = height",
            "        self.width = width",
            "        self.favicon_path = favicon_path",
            "        self.ssl_verify = ssl_verify",
            "        self.state_session_capacity = state_session_capacity",
            "        if root_path is None:",
            "            self.root_path = os.environ.get(\"GRADIO_ROOT_PATH\", \"\")",
            "        else:",
            "            self.root_path = root_path",
            "",
            "        self.show_api = show_api",
            "",
            "        self.allowed_paths = allowed_paths or []",
            "        self.blocked_paths = blocked_paths or []",
            "",
            "        if not isinstance(self.allowed_paths, list):",
            "            raise ValueError(\"`allowed_paths` must be a list of directories.\")",
            "        if not isinstance(self.blocked_paths, list):",
            "            raise ValueError(\"`blocked_paths` must be a list of directories.\")",
            "",
            "        self.validate_queue_settings()",
            "",
            "        self.config = self.get_config_file()",
            "        self.max_threads = max_threads",
            "        self._queue.max_thread_count = max_threads",
            "",
            "        if self.is_running:",
            "            if not isinstance(self.local_url, str):",
            "                raise ValueError(f\"Invalid local_url: {self.local_url}\")",
            "            if not (quiet):",
            "                print(",
            "                    \"Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\\n----\"",
            "                )",
            "        else:",
            "            if wasm_utils.IS_WASM:",
            "                server_name = \"xxx\"",
            "                server_port = 99999",
            "                local_url = \"\"",
            "                server = None",
            "",
            "                # In the Wasm environment, we only need the app object",
            "                # which the frontend app will directly communicate with through the Worker API,",
            "                # and we don't need to start a server.",
            "                # So we just create the app object and register it here,",
            "                # and avoid using `networking.start_server` that would start a server that don't work in the Wasm env.",
            "                from gradio.routes import App",
            "",
            "                app = App.create_app(self, app_kwargs=app_kwargs)",
            "                wasm_utils.register_app(app)",
            "            else:",
            "                (",
            "                    server_name,",
            "                    server_port,",
            "                    local_url,",
            "                    app,",
            "                    server,",
            "                ) = networking.start_server(",
            "                    self,",
            "                    server_name,",
            "                    server_port,",
            "                    ssl_keyfile,",
            "                    ssl_certfile,",
            "                    ssl_keyfile_password,",
            "                    app_kwargs=app_kwargs,",
            "                )",
            "            self.server_name = server_name",
            "            self.local_url = local_url",
            "            self.server_port = server_port",
            "            self.server_app = (",
            "                self.app",
            "            ) = app  # server_app is included for backwards compatibility",
            "            self.server = server",
            "            self.is_running = True",
            "            self.is_colab = utils.colab_check()",
            "            self.is_kaggle = utils.kaggle_check()",
            "            self.share_server_address = share_server_address",
            "            self.share_server_protocol = share_server_protocol or (",
            "                \"http\" if share_server_address is not None else \"https\"",
            "            )",
            "",
            "            self.protocol = (",
            "                \"https\"",
            "                if self.local_url.startswith(\"https\") or self.is_colab",
            "                else \"http\"",
            "            )",
            "            if not wasm_utils.IS_WASM and not self.is_colab:",
            "                print(",
            "                    strings.en[\"RUNNING_LOCALLY_SEPARATED\"].format(",
            "                        self.protocol, self.server_name, self.server_port",
            "                    )",
            "                )",
            "",
            "            self._queue.set_server_app(self.server_app)",
            "",
            "            if not wasm_utils.IS_WASM:",
            "                # Cannot run async functions in background other than app's scope.",
            "                # Workaround by triggering the app endpoint",
            "                httpx.get(f\"{self.local_url}startup-events\", verify=ssl_verify)",
            "            else:",
            "                # NOTE: One benefit of the code above dispatching `startup_events()` via a self HTTP request is",
            "                # that `self._queue.start()` is called in another thread which is managed by the HTTP server, `uvicorn`",
            "                # so all the asyncio tasks created by the queue runs in an event loop in that thread and",
            "                # will be cancelled just by stopping the server.",
            "                # In contrast, in the Wasm env, we can't do that because `threading` is not supported and all async tasks will run in the same event loop, `pyodide.webloop.WebLoop` in the main thread.",
            "                # So we need to manually cancel them. See `self.close()`..",
            "                self.startup_events()",
            "",
            "        utils.launch_counter()",
            "        self.is_sagemaker = utils.sagemaker_check()",
            "        if share is None:",
            "            if self.is_colab:",
            "                if not quiet:",
            "                    print(",
            "                        \"Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            elif self.is_kaggle:",
            "                if not quiet:",
            "                    print(",
            "                        \"Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            elif self.is_sagemaker:",
            "                if not quiet:",
            "                    print(",
            "                        \"Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            else:",
            "                self.share = False",
            "        else:",
            "            self.share = share",
            "",
            "        # If running in a colab or not able to access localhost,",
            "        # a shareable link must be created.",
            "        if (",
            "            _frontend",
            "            and not wasm_utils.IS_WASM",
            "            and not networking.url_ok(self.local_url)",
            "            and not self.share",
            "        ):",
            "            raise ValueError(",
            "                \"When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.\"",
            "            )",
            "",
            "        if self.is_colab and not quiet:",
            "            if debug:",
            "                print(strings.en[\"COLAB_DEBUG_TRUE\"])",
            "            else:",
            "                print(strings.en[\"COLAB_DEBUG_FALSE\"])",
            "            if not self.share:",
            "                print(strings.en[\"COLAB_WARNING\"].format(self.server_port))",
            "",
            "        if self.share:",
            "            if self.space_id:",
            "                warnings.warn(",
            "                    \"Setting share=True is not supported on Hugging Face Spaces\"",
            "                )",
            "                self.share = False",
            "            if wasm_utils.IS_WASM:",
            "                warnings.warn(",
            "                    \"Setting share=True is not supported in the Wasm environment\"",
            "                )",
            "                self.share = False",
            "",
            "        if self.share:",
            "            try:",
            "                if self.share_url is None:",
            "                    share_url = networking.setup_tunnel(",
            "                        local_host=self.server_name,",
            "                        local_port=self.server_port,",
            "                        share_token=self.share_token,",
            "                        share_server_address=self.share_server_address,",
            "                    )",
            "                    parsed_url = urlparse(share_url)",
            "                    self.share_url = urlunparse(",
            "                        (self.share_server_protocol,) + parsed_url[1:]",
            "                    )",
            "                print(strings.en[\"SHARE_LINK_DISPLAY\"].format(self.share_url))",
            "                if not (quiet):",
            "                    print(strings.en[\"SHARE_LINK_MESSAGE\"])",
            "            except (RuntimeError, httpx.ConnectError):",
            "                if self.analytics_enabled:",
            "                    analytics.error_analytics(\"Not able to set up tunnel\")",
            "                self.share_url = None",
            "                self.share = False",
            "                if Path(BINARY_PATH).exists():",
            "                    print(strings.en[\"COULD_NOT_GET_SHARE_LINK\"])",
            "                else:",
            "                    print(",
            "                        strings.en[\"COULD_NOT_GET_SHARE_LINK_MISSING_FILE\"].format(",
            "                            BINARY_PATH,",
            "                            BINARY_URL,",
            "                            BINARY_FILENAME,",
            "                            BINARY_FOLDER,",
            "                        )",
            "                    )",
            "        else:",
            "            if not quiet and not wasm_utils.IS_WASM:",
            "                print(strings.en[\"PUBLIC_SHARE_TRUE\"])",
            "            self.share_url = None",
            "",
            "        if inbrowser and not wasm_utils.IS_WASM:",
            "            link = self.share_url if self.share and self.share_url else self.local_url",
            "            webbrowser.open(link)",
            "",
            "        # Check if running in a Python notebook in which case, display inline",
            "        if inline is None:",
            "            inline = utils.ipython_check()",
            "        if inline:",
            "            try:",
            "                from IPython.display import HTML, Javascript, display  # type: ignore",
            "",
            "                if self.share and self.share_url:",
            "                    while not networking.url_ok(self.share_url):",
            "                        time.sleep(0.25)",
            "                    artifact = HTML(",
            "                        f'<div><iframe src=\"{self.share_url}\" width=\"{self.width}\" height=\"{self.height}\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>'",
            "                    )",
            "",
            "                elif self.is_colab:",
            "                    # modified from /usr/local/lib/python3.7/dist-packages/google/colab/output/_util.py within Colab environment",
            "                    code = \"\"\"(async (port, path, width, height, cache, element) => {",
            "                        if (!google.colab.kernel.accessAllowed && !cache) {",
            "                            return;",
            "                        }",
            "                        element.appendChild(document.createTextNode(''));",
            "                        const url = await google.colab.kernel.proxyPort(port, {cache});",
            "",
            "                        const external_link = document.createElement('div');",
            "                        external_link.innerHTML = `",
            "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">",
            "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">",
            "                                    https://localhost:${port}${path}",
            "                                </a>",
            "                            </div>",
            "                        `;",
            "                        element.appendChild(external_link);",
            "",
            "                        const iframe = document.createElement('iframe');",
            "                        iframe.src = new URL(path, url).toString();",
            "                        iframe.height = height;",
            "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"",
            "                        iframe.width = width;",
            "                        iframe.style.border = 0;",
            "                        element.appendChild(iframe);",
            "                    })\"\"\" + \"({port}, {path}, {width}, {height}, {cache}, window.element)\".format(",
            "                        port=json.dumps(self.server_port),",
            "                        path=json.dumps(\"/\"),",
            "                        width=json.dumps(self.width),",
            "                        height=json.dumps(self.height),",
            "                        cache=json.dumps(False),",
            "                    )",
            "",
            "                    artifact = Javascript(code)",
            "                else:",
            "                    artifact = HTML(",
            "                        f'<div><iframe src=\"{self.local_url}\" width=\"{self.width}\" height=\"{self.height}\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>'",
            "                    )",
            "                self.artifact = artifact",
            "                display(artifact)",
            "            except ImportError:",
            "                pass",
            "",
            "        if getattr(self, \"analytics_enabled\", False):",
            "            data = {",
            "                \"launch_method\": \"browser\" if inbrowser else \"inline\",",
            "                \"is_google_colab\": self.is_colab,",
            "                \"is_sharing_on\": self.share,",
            "                \"share_url\": self.share_url,",
            "                \"enable_queue\": True,",
            "                \"server_name\": server_name,",
            "                \"server_port\": server_port,",
            "                \"is_space\": self.space_id is not None,",
            "                \"mode\": self.mode,",
            "            }",
            "            analytics.launched_analytics(self, data)",
            "",
            "        # Block main thread if debug==True",
            "        if debug or int(os.getenv(\"GRADIO_DEBUG\", \"0\")) == 1 and not wasm_utils.IS_WASM:",
            "            self.block_thread()",
            "        # Block main thread if running in a script to stop script from exiting",
            "        is_in_interactive_mode = bool(getattr(sys, \"ps1\", sys.flags.interactive))",
            "",
            "        if (",
            "            not prevent_thread_lock",
            "            and not is_in_interactive_mode",
            "            # In the Wasm env, we don't have to block the main thread because the server won't be shut down after the execution finishes.",
            "            # Moreover, we MUST NOT do it because there is only one thread in the Wasm env and blocking it will stop the subsequent code from running.",
            "            and not wasm_utils.IS_WASM",
            "        ):",
            "            self.block_thread()",
            "",
            "        return TupleNoPrint((self.server_app, self.local_url, self.share_url))  # type: ignore",
            "",
            "    def integrate(",
            "        self,",
            "        comet_ml=None,",
            "        wandb: ModuleType | None = None,",
            "        mlflow: ModuleType | None = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        A catch-all method for integrating with other libraries. This method should be run after launch()",
            "        Parameters:",
            "            comet_ml: If a comet_ml Experiment object is provided, will integrate with the experiment and appear on Comet dashboard",
            "            wandb: If the wandb module is provided, will integrate with it and appear on WandB dashboard",
            "            mlflow: If the mlflow module  is provided, will integrate with the experiment and appear on ML Flow dashboard",
            "        \"\"\"",
            "        analytics_integration = \"\"",
            "        if comet_ml is not None:",
            "            analytics_integration = \"CometML\"",
            "            comet_ml.log_other(\"Created from\", \"Gradio\")",
            "            if self.share_url is not None:",
            "                comet_ml.log_text(f\"gradio: {self.share_url}\")",
            "                comet_ml.end()",
            "            elif self.local_url:",
            "                comet_ml.log_text(f\"gradio: {self.local_url}\")",
            "                comet_ml.end()",
            "            else:",
            "                raise ValueError(\"Please run `launch()` first.\")",
            "        if wandb is not None:",
            "            analytics_integration = \"WandB\"",
            "            if self.share_url is not None:",
            "                wandb.log(",
            "                    {",
            "                        \"Gradio panel\": wandb.Html(",
            "                            '<iframe src=\"'",
            "                            + self.share_url",
            "                            + '\" width=\"'",
            "                            + str(self.width)",
            "                            + '\" height=\"'",
            "                            + str(self.height)",
            "                            + '\" frameBorder=\"0\"></iframe>'",
            "                        )",
            "                    }",
            "                )",
            "            else:",
            "                print(",
            "                    \"The WandB integration requires you to \"",
            "                    \"`launch(share=True)` first.\"",
            "                )",
            "        if mlflow is not None:",
            "            analytics_integration = \"MLFlow\"",
            "            if self.share_url is not None:",
            "                mlflow.log_param(\"Gradio Interface Share Link\", self.share_url)",
            "            else:",
            "                mlflow.log_param(\"Gradio Interface Local Link\", self.local_url)",
            "        if self.analytics_enabled and analytics_integration:",
            "            data = {\"integration\": analytics_integration}",
            "            analytics.integration_analytics(data)",
            "",
            "    def close(self, verbose: bool = True) -> None:",
            "        \"\"\"",
            "        Closes the Interface that was launched and frees the port.",
            "        \"\"\"",
            "        try:",
            "            if wasm_utils.IS_WASM:",
            "                # NOTE:",
            "                # Normally, queue-related async tasks (e.g. continuous events created by `gr.Blocks.load(..., every=interval)`, whose async tasks are started at the `/queue/data` endpoint function)",
            "                # are running in an event loop in the server thread,",
            "                # so they will be cancelled by `self.server.close()` below.",
            "                # However, in the Wasm env, we don't have the `server` and",
            "                # all async tasks are running in the same event loop, `pyodide.webloop.WebLoop` in the main thread,",
            "                # so we have to cancel them explicitly so that these tasks won't run after a new app is launched.",
            "                self._queue._cancel_asyncio_tasks()",
            "                self.server_app._cancel_asyncio_tasks()",
            "            self._queue.close()",
            "            if self.server:",
            "                self.server.close()",
            "            self.is_running = False",
            "            # So that the startup events (starting the queue)",
            "            # happen the next time the app is launched",
            "            self.app.startup_events_triggered = False",
            "            if verbose:",
            "                print(f\"Closing server running on port: {self.server_port}\")",
            "        except (AttributeError, OSError):  # can't close if not running",
            "            pass",
            "",
            "    def block_thread(",
            "        self,",
            "    ) -> None:",
            "        \"\"\"Block main thread until interrupted by user.\"\"\"",
            "        try:",
            "            while True:",
            "                time.sleep(0.1)",
            "        except (KeyboardInterrupt, OSError):",
            "            print(\"Keyboard interruption in main thread... closing server.\")",
            "            if self.server:",
            "                self.server.close()",
            "            for tunnel in CURRENT_TUNNELS:",
            "                tunnel.kill()",
            "",
            "    def attach_load_events(self):",
            "        \"\"\"Add a load event for every component whose initial value should be randomized.\"\"\"",
            "        if Context.root_block:",
            "            for component in Context.root_block.blocks.values():",
            "                if (",
            "                    isinstance(component, components.Component)",
            "                    and component.load_event_to_attach",
            "                ):",
            "                    load_fn, every = component.load_event_to_attach",
            "                    # Use set_event_trigger to avoid ambiguity between load class/instance method",
            "",
            "                    dep = self.set_event_trigger(",
            "                        [EventListenerMethod(self, \"load\")],",
            "                        load_fn,",
            "                        None,",
            "                        component,",
            "                        no_target=True,",
            "                        # If every is None, for sure skip the queue",
            "                        # else, let the enable_queue parameter take precedence",
            "                        # this will raise a nice error message is every is used",
            "                        # without queue",
            "                        queue=False if every is None else None,",
            "                        every=every,",
            "                    )[0]",
            "                    component.load_event = dep",
            "",
            "    def startup_events(self):",
            "        \"\"\"Events that should be run when the app containing this block starts up.\"\"\"",
            "        self._queue.start()",
            "        # So that processing can resume in case the queue was stopped",
            "        self._queue.stopped = False",
            "        self.create_limiter()",
            "",
            "    def queue_enabled_for_fn(self, fn_index: int):",
            "        return self.dependencies[fn_index][\"queue\"] is not False",
            "",
            "    def get_api_info(self):",
            "        \"\"\"",
            "        Gets the information needed to generate the API docs from a Blocks.",
            "        \"\"\"",
            "        config = self.config",
            "        api_info = {\"named_endpoints\": {}, \"unnamed_endpoints\": {}}",
            "",
            "        for dependency in config[\"dependencies\"]:",
            "            if (",
            "                not dependency[\"backend_fn\"]",
            "                or not dependency[\"show_api\"]",
            "                or dependency[\"api_name\"] is False",
            "            ):",
            "                continue",
            "",
            "            dependency_info = {\"parameters\": [], \"returns\": []}",
            "            skip_endpoint = False",
            "",
            "            inputs = dependency[\"inputs\"]",
            "            for i in inputs:",
            "                for component in config[\"components\"]:",
            "                    if component[\"id\"] == i:",
            "                        break",
            "                else:",
            "                    skip_endpoint = True  # if component not found, skip endpoint",
            "                    break",
            "                type = component[\"type\"]",
            "                if self.blocks[component[\"id\"]].skip_api:",
            "                    continue",
            "                label = component[\"props\"].get(\"label\", f\"parameter_{i}\")",
            "                comp = self.get_component(component[\"id\"])",
            "                assert isinstance(comp, components.Component)",
            "                info = component[\"api_info\"]",
            "                example = comp.example_inputs()",
            "                python_type = client_utils.json_schema_to_python_type(info)",
            "                dependency_info[\"parameters\"].append(",
            "                    {",
            "                        \"label\": label,",
            "                        \"type\": info,",
            "                        \"python_type\": {",
            "                            \"type\": python_type,",
            "                            \"description\": info.get(\"description\", \"\"),",
            "                        },",
            "                        \"component\": type.capitalize(),",
            "                        \"example_input\": example,",
            "                    }",
            "                )",
            "",
            "            outputs = dependency[\"outputs\"]",
            "            for o in outputs:",
            "                for component in config[\"components\"]:",
            "                    if component[\"id\"] == o:",
            "                        break",
            "                else:",
            "                    skip_endpoint = True  # if component not found, skip endpoint",
            "                    break",
            "                type = component[\"type\"]",
            "                if self.blocks[component[\"id\"]].skip_api:",
            "                    continue",
            "                label = component[\"props\"].get(\"label\", f\"value_{o}\")",
            "                comp = self.get_component(component[\"id\"])",
            "                assert isinstance(comp, components.Component)",
            "                info = component[\"api_info\"]",
            "                example = comp.example_inputs()",
            "                python_type = client_utils.json_schema_to_python_type(info)",
            "                dependency_info[\"returns\"].append(",
            "                    {",
            "                        \"label\": label,",
            "                        \"type\": info,",
            "                        \"python_type\": {",
            "                            \"type\": python_type,",
            "                            \"description\": info.get(\"description\", \"\"),",
            "                        },",
            "                        \"component\": type.capitalize(),",
            "                    }",
            "                )",
            "",
            "            if not skip_endpoint:",
            "                api_info[\"named_endpoints\"][",
            "                    f\"/{dependency['api_name']}\"",
            "                ] = dependency_info",
            "",
            "        return api_info"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import copy",
            "import hashlib",
            "import inspect",
            "import json",
            "import os",
            "import random",
            "import secrets",
            "import string",
            "import sys",
            "import tempfile",
            "import threading",
            "import time",
            "import warnings",
            "import webbrowser",
            "from collections import defaultdict",
            "from pathlib import Path",
            "from types import ModuleType",
            "from typing import TYPE_CHECKING, Any, AsyncIterator, Callable, Literal, Sequence, cast",
            "from urllib.parse import urlparse, urlunparse",
            "",
            "import anyio",
            "import httpx",
            "from anyio import CapacityLimiter",
            "from gradio_client import utils as client_utils",
            "from gradio_client.documentation import document",
            "",
            "from gradio import (",
            "    analytics,",
            "    components,",
            "    networking,",
            "    processing_utils,",
            "    queueing,",
            "    routes,",
            "    strings,",
            "    themes,",
            "    utils,",
            "    wasm_utils,",
            ")",
            "from gradio.blocks_events import BlocksEvents, BlocksMeta",
            "from gradio.context import Context",
            "from gradio.data_classes import FileData, GradioModel, GradioRootModel",
            "from gradio.events import (",
            "    EventData,",
            "    EventListener,",
            "    EventListenerMethod,",
            ")",
            "from gradio.exceptions import (",
            "    DuplicateBlockError,",
            "    InvalidApiNameError,",
            "    InvalidBlockError,",
            "    InvalidComponentError,",
            ")",
            "from gradio.helpers import create_tracker, skip, special_args",
            "from gradio.state_holder import SessionState",
            "from gradio.themes import Default as DefaultTheme",
            "from gradio.themes import ThemeClass as Theme",
            "from gradio.tunneling import (",
            "    BINARY_FILENAME,",
            "    BINARY_FOLDER,",
            "    BINARY_PATH,",
            "    BINARY_URL,",
            "    CURRENT_TUNNELS,",
            ")",
            "from gradio.utils import (",
            "    TupleNoPrint,",
            "    check_function_inputs_match,",
            "    component_or_layout_class,",
            "    get_cancel_function,",
            "    get_continuous_fn,",
            "    get_package_version,",
            ")",
            "",
            "try:",
            "    import spaces  # type: ignore",
            "except Exception:",
            "    spaces = None",
            "",
            "",
            "if TYPE_CHECKING:  # Only import for type checking (is False at runtime).",
            "    from fastapi.applications import FastAPI",
            "",
            "    from gradio.components.base import Component",
            "",
            "BUILT_IN_THEMES: dict[str, Theme] = {",
            "    t.name: t",
            "    for t in [",
            "        themes.Base(),",
            "        themes.Default(),",
            "        themes.Monochrome(),",
            "        themes.Soft(),",
            "        themes.Glass(),",
            "    ]",
            "}",
            "",
            "",
            "class Block:",
            "    def __init__(",
            "        self,",
            "        *,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        render: bool = True,",
            "        visible: bool = True,",
            "        proxy_url: str | None = None,",
            "    ):",
            "        self._id = Context.id",
            "        Context.id += 1",
            "        self.visible = visible",
            "        self.elem_id = elem_id",
            "        self.elem_classes = (",
            "            [elem_classes] if isinstance(elem_classes, str) else elem_classes",
            "        )",
            "        self.proxy_url = proxy_url",
            "        self.share_token = secrets.token_urlsafe(32)",
            "        self.parent: BlockContext | None = None",
            "        self.is_rendered: bool = False",
            "        self._constructor_args: list[dict]",
            "        self.state_session_capacity = 10000",
            "        self.temp_files: set[str] = set()",
            "        self.GRADIO_CACHE = str(",
            "            Path(",
            "                os.environ.get(\"GRADIO_TEMP_DIR\")",
            "                or str(Path(tempfile.gettempdir()) / \"gradio\")",
            "            ).resolve()",
            "        )",
            "",
            "        if render:",
            "            self.render()",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return False",
            "",
            "    @property",
            "    def constructor_args(self) -> dict[str, Any]:",
            "        \"\"\"Get the arguments passed to the component's initializer.",
            "",
            "        Only set classes whose metaclass is ComponentMeta",
            "        \"\"\"",
            "        # the _constructor_args list is appended based on the mro of the class",
            "        # so the first entry is for the bottom of the hierarchy",
            "        return self._constructor_args[0] if self._constructor_args else {}",
            "",
            "    @property",
            "    def events(",
            "        self,",
            "    ) -> list[EventListener]:",
            "        return getattr(self, \"EVENTS\", [])",
            "",
            "    def render(self):",
            "        \"\"\"",
            "        Adds self into appropriate BlockContext",
            "        \"\"\"",
            "        if Context.root_block is not None and self._id in Context.root_block.blocks:",
            "            raise DuplicateBlockError(",
            "                f\"A block with id: {self._id} has already been rendered in the current Blocks.\"",
            "            )",
            "        if Context.block is not None:",
            "            Context.block.add(self)",
            "        if Context.root_block is not None:",
            "            Context.root_block.blocks[self._id] = self",
            "            self.is_rendered = True",
            "            if isinstance(self, components.Component):",
            "                Context.root_block.temp_file_sets.append(self.temp_files)",
            "        return self",
            "",
            "    def unrender(self):",
            "        \"\"\"",
            "        Removes self from BlockContext if it has been rendered (otherwise does nothing).",
            "        Removes self from the layout and collection of blocks, but does not delete any event triggers.",
            "        \"\"\"",
            "        if Context.block is not None:",
            "            try:",
            "                Context.block.children.remove(self)",
            "            except ValueError:",
            "                pass",
            "        if Context.root_block is not None:",
            "            try:",
            "                del Context.root_block.blocks[self._id]",
            "                self.is_rendered = False",
            "            except KeyError:",
            "                pass",
            "        return self",
            "",
            "    def get_block_name(self) -> str:",
            "        \"\"\"",
            "        Gets block's class name.",
            "",
            "        If it is template component it gets the parent's class name.",
            "",
            "        @return: class name",
            "        \"\"\"",
            "        return (",
            "            self.__class__.__base__.__name__.lower()",
            "            if hasattr(self, \"is_template\")",
            "            else self.__class__.__name__.lower()",
            "        )",
            "",
            "    def get_expected_parent(self) -> type[BlockContext] | None:",
            "        return None",
            "",
            "    def get_config(self):",
            "        config = {}",
            "        signature = inspect.signature(self.__class__.__init__)",
            "        for parameter in signature.parameters.values():",
            "            if hasattr(self, parameter.name):",
            "                value = getattr(self, parameter.name)",
            "                config[parameter.name] = utils.convert_to_dict_if_dataclass(value)",
            "        for e in self.events:",
            "            to_add = e.config_data()",
            "            if to_add:",
            "                config = {**to_add, **config}",
            "        config.pop(\"render\", None)",
            "        config = {**config, \"proxy_url\": self.proxy_url, \"name\": self.get_block_name()}",
            "        if (_selectable := getattr(self, \"_selectable\", None)) is not None:",
            "            config[\"_selectable\"] = _selectable",
            "        return config",
            "",
            "    @classmethod",
            "    def recover_kwargs(",
            "        cls, props: dict[str, Any], additional_keys: list[str] | None = None",
            "    ):",
            "        \"\"\"",
            "        Recovers kwargs from a dict of props.",
            "        \"\"\"",
            "        additional_keys = additional_keys or []",
            "        signature = inspect.signature(cls.__init__)",
            "        kwargs = {}",
            "        for parameter in signature.parameters.values():",
            "            if parameter.name in props and parameter.name not in additional_keys:",
            "                kwargs[parameter.name] = props[parameter.name]",
            "        return kwargs",
            "",
            "    def move_resource_to_block_cache(",
            "        self, url_or_file_path: str | Path | None",
            "    ) -> str | None:",
            "        \"\"\"Moves a file or downloads a file from a url to a block's cache directory, adds",
            "        to to the block's temp_files, and returns the path to the file in cache. This",
            "        ensures that the file is accessible to the Block and can be served to users.",
            "        \"\"\"",
            "        if url_or_file_path is None:",
            "            return None",
            "        if isinstance(url_or_file_path, Path):",
            "            url_or_file_path = str(url_or_file_path)",
            "",
            "        if client_utils.is_http_url_like(url_or_file_path):",
            "            temp_file_path = processing_utils.save_url_to_cache(",
            "                url_or_file_path, cache_dir=self.GRADIO_CACHE",
            "            )",
            "",
            "            self.temp_files.add(temp_file_path)",
            "        else:",
            "            url_or_file_path = str(utils.abspath(url_or_file_path))",
            "            if not utils.is_in_or_equal(url_or_file_path, self.GRADIO_CACHE):",
            "                temp_file_path = processing_utils.save_file_to_cache(",
            "                    url_or_file_path, cache_dir=self.GRADIO_CACHE",
            "                )",
            "            else:",
            "                temp_file_path = url_or_file_path",
            "            self.temp_files.add(temp_file_path)",
            "",
            "        return temp_file_path",
            "",
            "",
            "class BlockContext(Block):",
            "    def __init__(",
            "        self,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        visible: bool = True,",
            "        render: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            elem_id: An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",
            "            elem_classes: An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",
            "            visible: If False, this will be hidden but included in the Blocks config file (its visibility can later be updated).",
            "            render: If False, this will not be included in the Blocks config file at all.",
            "        \"\"\"",
            "        self.children: list[Block] = []",
            "        Block.__init__(",
            "            self,",
            "            elem_id=elem_id,",
            "            elem_classes=elem_classes,",
            "            visible=visible,",
            "            render=render,",
            "        )",
            "",
            "    TEMPLATE_DIR = \"./templates/\"",
            "    FRONTEND_DIR = \"../../frontend/\"",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return True",
            "",
            "    @classmethod",
            "    def get_component_class_id(cls) -> str:",
            "        module_name = cls.__module__",
            "        module_path = sys.modules[module_name].__file__",
            "        module_hash = hashlib.md5(f\"{cls.__name__}_{module_path}\".encode()).hexdigest()",
            "        return module_hash",
            "",
            "    @property",
            "    def component_class_id(self):",
            "        return self.get_component_class_id()",
            "",
            "    def add_child(self, child: Block):",
            "        self.children.append(child)",
            "",
            "    def __enter__(self):",
            "        self.parent = Context.block",
            "        Context.block = self",
            "        return self",
            "",
            "    def add(self, child: Block):",
            "        child.parent = self",
            "        self.children.append(child)",
            "",
            "    def fill_expected_parents(self):",
            "        children = []",
            "        pseudo_parent = None",
            "        for child in self.children:",
            "            expected_parent = child.get_expected_parent()",
            "            if not expected_parent or isinstance(self, expected_parent):",
            "                pseudo_parent = None",
            "                children.append(child)",
            "            else:",
            "                if pseudo_parent is not None and isinstance(",
            "                    pseudo_parent, expected_parent",
            "                ):",
            "                    pseudo_parent.add_child(child)",
            "                else:",
            "                    pseudo_parent = expected_parent(render=False)",
            "                    pseudo_parent.parent = self",
            "                    children.append(pseudo_parent)",
            "                    pseudo_parent.add_child(child)",
            "                    if Context.root_block:",
            "                        Context.root_block.blocks[pseudo_parent._id] = pseudo_parent",
            "                child.parent = pseudo_parent",
            "        self.children = children",
            "",
            "    def __exit__(self, exc_type: type[BaseException] | None = None, *args):",
            "        Context.block = self.parent",
            "        if exc_type is not None:",
            "            return",
            "        if getattr(self, \"allow_expected_parents\", True):",
            "            self.fill_expected_parents()",
            "",
            "    def postprocess(self, y):",
            "        \"\"\"",
            "        Any postprocessing needed to be performed on a block context.",
            "        \"\"\"",
            "        return y",
            "",
            "",
            "class BlockFunction:",
            "    def __init__(",
            "        self,",
            "        fn: Callable | None,",
            "        inputs: list[Component],",
            "        outputs: list[Component],",
            "        preprocess: bool,",
            "        postprocess: bool,",
            "        inputs_as_dict: bool,",
            "        batch: bool = False,",
            "        max_batch_size: int = 4,",
            "        concurrency_limit: int | None | Literal[\"default\"] = \"default\",",
            "        concurrency_id: str | None = None,",
            "        tracks_progress: bool = False,",
            "    ):",
            "        self.fn = fn",
            "        self.inputs = inputs",
            "        self.outputs = outputs",
            "        self.preprocess = preprocess",
            "        self.postprocess = postprocess",
            "        self.tracks_progress = tracks_progress",
            "        self.concurrency_limit: int | None | Literal[\"default\"] = concurrency_limit",
            "        self.concurrency_id = concurrency_id or str(id(fn))",
            "        self.batch = batch",
            "        self.max_batch_size = max_batch_size",
            "        self.total_runtime = 0",
            "        self.total_runs = 0",
            "        self.inputs_as_dict = inputs_as_dict",
            "        self.name = getattr(fn, \"__name__\", \"fn\") if fn is not None else None",
            "        self.spaces_auto_wrap()",
            "",
            "    def spaces_auto_wrap(self):",
            "        if spaces is None:",
            "            return",
            "        if utils.get_space() is None:",
            "            return",
            "        self.fn = spaces.gradio_auto_wrap(self.fn)",
            "",
            "    def __str__(self):",
            "        return str(",
            "            {",
            "                \"fn\": self.name,",
            "                \"preprocess\": self.preprocess,",
            "                \"postprocess\": self.postprocess,",
            "            }",
            "        )",
            "",
            "    def __repr__(self):",
            "        return str(self)",
            "",
            "",
            "def postprocess_update_dict(",
            "    block: Component | BlockContext, update_dict: dict, postprocess: bool = True",
            "):",
            "    \"\"\"",
            "    Converts a dictionary of updates into a format that can be sent to the frontend to update the component.",
            "    E.g. {\"value\": \"2\", \"visible\": True, \"invalid_arg\": \"hello\"}",
            "    Into -> {\"__type__\": \"update\", \"value\": 2.0, \"visible\": True}",
            "    Parameters:",
            "        block: The Block that is being updated with this update dictionary.",
            "        update_dict: The original update dictionary",
            "        postprocess: Whether to postprocess the \"value\" key of the update dictionary.",
            "    \"\"\"",
            "    value = update_dict.pop(\"value\", components._Keywords.NO_VALUE)",
            "    update_dict = {k: getattr(block, k) for k in update_dict if hasattr(block, k)}",
            "    if value is not components._Keywords.NO_VALUE:",
            "        if postprocess:",
            "            update_dict[\"value\"] = block.postprocess(value)",
            "            if isinstance(update_dict[\"value\"], (GradioModel, GradioRootModel)):",
            "                update_dict[\"value\"] = update_dict[\"value\"].model_dump()",
            "        else:",
            "            update_dict[\"value\"] = value",
            "    update_dict[\"__type__\"] = \"update\"",
            "    return update_dict",
            "",
            "",
            "def convert_component_dict_to_list(",
            "    outputs_ids: list[int], predictions: dict",
            ") -> list | dict:",
            "    \"\"\"",
            "    Converts a dictionary of component updates into a list of updates in the order of",
            "    the outputs_ids and including every output component. Leaves other types of dictionaries unchanged.",
            "    E.g. {\"textbox\": \"hello\", \"number\": {\"__type__\": \"generic_update\", \"value\": \"2\"}}",
            "    Into -> [\"hello\", {\"__type__\": \"generic_update\"}, {\"__type__\": \"generic_update\", \"value\": \"2\"}]",
            "    \"\"\"",
            "    keys_are_blocks = [isinstance(key, Block) for key in predictions]",
            "    if all(keys_are_blocks):",
            "        reordered_predictions = [skip() for _ in outputs_ids]",
            "        for component, value in predictions.items():",
            "            if component._id not in outputs_ids:",
            "                raise ValueError(",
            "                    f\"Returned component {component} not specified as output of function.\"",
            "                )",
            "            output_index = outputs_ids.index(component._id)",
            "            reordered_predictions[output_index] = value",
            "        predictions = utils.resolve_singleton(reordered_predictions)",
            "    elif any(keys_are_blocks):",
            "        raise ValueError(",
            "            \"Returned dictionary included some keys as Components. Either all keys must be Components to assign Component values, or return a List of values to assign output values in order.\"",
            "        )",
            "    return predictions",
            "",
            "",
            "@document(\"launch\", \"queue\", \"integrate\", \"load\")",
            "class Blocks(BlockContext, BlocksEvents, metaclass=BlocksMeta):",
            "    \"\"\"",
            "    Blocks is Gradio's low-level API that allows you to create more custom web",
            "    applications and demos than Interfaces (yet still entirely in Python).",
            "",
            "",
            "    Compared to the Interface class, Blocks offers more flexibility and control over:",
            "    (1) the layout of components (2) the events that",
            "    trigger the execution of functions (3) data flows (e.g. inputs can trigger outputs,",
            "    which can trigger the next level of outputs). Blocks also offers ways to group",
            "    together related demos such as with tabs.",
            "",
            "",
            "    The basic usage of Blocks is as follows: create a Blocks object, then use it as a",
            "    context (with the \"with\" statement), and then define layouts, components, or events",
            "    within the Blocks context. Finally, call the launch() method to launch the demo.",
            "",
            "    Example:",
            "        import gradio as gr",
            "        def update(name):",
            "            return f\"Welcome to Gradio, {name}!\"",
            "",
            "        with gr.Blocks() as demo:",
            "            gr.Markdown(\"Start typing below and then click **Run** to see the output.\")",
            "            with gr.Row():",
            "                inp = gr.Textbox(placeholder=\"What is your name?\")",
            "                out = gr.Textbox()",
            "            btn = gr.Button(\"Run\")",
            "            btn.click(fn=update, inputs=inp, outputs=out)",
            "",
            "        demo.launch()",
            "    Demos: blocks_hello, blocks_flipper, blocks_speech_text_sentiment, generate_english_german",
            "    Guides: blocks-and-event-listeners, controlling-layout, state-in-blocks, custom-CSS-and-JS, using-blocks-like-functions",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        theme: Theme | str | None = None,",
            "        analytics_enabled: bool | None = None,",
            "        mode: str = \"blocks\",",
            "        title: str = \"Gradio\",",
            "        css: str | None = None,",
            "        js: str | None = None,",
            "        head: str | None = None,",
            "        fill_height: bool = False,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            theme: A Theme object or a string representing a theme. If a string, will look for a built-in theme with that name (e.g. \"soft\" or \"default\"), or will attempt to load a theme from the Hugging Face Hub (e.g. \"gradio/monochrome\"). If None, will use the Default theme.",
            "            analytics_enabled: Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable or default to True.",
            "            mode: A human-friendly name for the kind of Blocks or Interface being created. Used internally for analytics.",
            "            title: The tab title to display when this is opened in a browser window.",
            "            css: Custom css as a string or path to a css file. This css will be included in the demo webpage.",
            "            js: Custom js or path to js file to run when demo is first loaded. This javascript will be included in the demo webpage.",
            "            head: Custom html to insert into the head of the demo webpage. This can be used to add custom meta tags, scripts, stylesheets, etc. to the page.",
            "            fill_height: Whether to vertically expand top-level child components to the height of the window. If True, expansion occurs when the scale value of the child components >= 1.",
            "        \"\"\"",
            "        self.limiter = None",
            "        if theme is None:",
            "            theme = DefaultTheme()",
            "        elif isinstance(theme, str):",
            "            if theme.lower() in BUILT_IN_THEMES:",
            "                theme = BUILT_IN_THEMES[theme.lower()]",
            "            else:",
            "                try:",
            "                    theme = Theme.from_hub(theme)",
            "                except Exception as e:",
            "                    warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")",
            "                    theme = DefaultTheme()",
            "        if not isinstance(theme, Theme):",
            "            warnings.warn(\"Theme should be a class loaded from gradio.themes\")",
            "            theme = DefaultTheme()",
            "        self.theme: Theme = theme",
            "        self.theme_css = theme._get_theme_css()",
            "        self.stylesheets = theme._stylesheets",
            "        self.encrypt = False",
            "        self.share = False",
            "        self.enable_queue = True",
            "        self.max_threads = 40",
            "        self.pending_streams = defaultdict(dict)",
            "        self.pending_diff_streams = defaultdict(dict)",
            "        self.show_error = True",
            "        self.head = head",
            "        self.fill_height = fill_height",
            "        if css is not None and os.path.exists(css):",
            "            with open(css) as css_file:",
            "                self.css = css_file.read()",
            "        else:",
            "            self.css = css",
            "        if js is not None and os.path.exists(js):",
            "            with open(js) as js_file:",
            "                self.js = js_file.read()",
            "        else:",
            "            self.js = js",
            "",
            "        # For analytics_enabled and allow_flagging: (1) first check for",
            "        # parameter, (2) check for env variable, (3) default to True/\"manual\"",
            "        self.analytics_enabled = (",
            "            analytics_enabled",
            "            if analytics_enabled is not None",
            "            else analytics.analytics_enabled()",
            "        )",
            "        if self.analytics_enabled:",
            "            if not wasm_utils.IS_WASM:",
            "                t = threading.Thread(target=analytics.version_check)",
            "                t.start()",
            "        else:",
            "            os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"True\"",
            "        super().__init__(render=False, **kwargs)",
            "        self.blocks: dict[int, Component | Block] = {}",
            "        self.fns: list[BlockFunction] = []",
            "        self.dependencies = []",
            "        self.mode = mode",
            "",
            "        self.is_running = False",
            "        self.local_url = None",
            "        self.share_url = None",
            "        self.width = None",
            "        self.height = None",
            "        self.api_open = utils.get_space() is None",
            "",
            "        self.space_id = utils.get_space()",
            "        self.favicon_path = None",
            "        self.auth = None",
            "        self.dev_mode = bool(os.getenv(\"GRADIO_WATCH_DIRS\", \"\"))",
            "        self.app_id = random.getrandbits(64)",
            "        self.temp_file_sets = []",
            "        self.title = title",
            "        self.show_api = not wasm_utils.IS_WASM",
            "",
            "        # Only used when an Interface is loaded from a config",
            "        self.predict = None",
            "        self.input_components = None",
            "        self.output_components = None",
            "        self.__name__ = None",
            "        self.api_mode = None",
            "",
            "        self.progress_tracking = None",
            "        self.ssl_verify = True",
            "",
            "        self.allowed_paths = []",
            "        self.blocked_paths = []",
            "        self.root_path = os.environ.get(\"GRADIO_ROOT_PATH\", \"\")",
            "        self.proxy_urls = set()",
            "",
            "        if self.analytics_enabled:",
            "            is_custom_theme = not any(",
            "                self.theme.to_dict() == built_in_theme.to_dict()",
            "                for built_in_theme in BUILT_IN_THEMES.values()",
            "            )",
            "            data = {",
            "                \"mode\": self.mode,",
            "                \"custom_css\": self.css is not None,",
            "                \"theme\": self.theme.name,",
            "                \"is_custom_theme\": is_custom_theme,",
            "                \"version\": get_package_version(),",
            "            }",
            "            analytics.initiated_analytics(data)",
            "",
            "        self.queue()",
            "",
            "    def get_component(self, id: int) -> Component | BlockContext:",
            "        comp = self.blocks[id]",
            "        assert isinstance(comp, (components.Component, BlockContext)), f\"{comp}\"",
            "        return comp",
            "",
            "    @property",
            "    def _is_running_in_reload_thread(self):",
            "        from gradio.cli.commands.reload import reload_thread",
            "",
            "        return getattr(reload_thread, \"running_reload\", False)",
            "",
            "    @classmethod",
            "    def from_config(",
            "        cls,",
            "        config: dict,",
            "        fns: list[Callable],",
            "        proxy_url: str,",
            "    ) -> Blocks:",
            "        \"\"\"",
            "        Factory method that creates a Blocks from a config and list of functions. Used",
            "        internally by the gradio.external.load() method.",
            "",
            "        Parameters:",
            "        config: a dictionary containing the configuration of the Blocks.",
            "        fns: a list of functions that are used in the Blocks. Must be in the same order as the dependencies in the config.",
            "        proxy_url: an external url to use as a root URL when serving files for components in the Blocks.",
            "        \"\"\"",
            "        config = copy.deepcopy(config)",
            "        components_config = config[\"components\"]",
            "        theme = config.get(\"theme\", \"default\")",
            "        original_mapping: dict[int, Block] = {}",
            "        proxy_urls = {proxy_url}",
            "",
            "        def get_block_instance(id: int) -> Block:",
            "            for block_config in components_config:",
            "                if block_config[\"id\"] == id:",
            "                    break",
            "            else:",
            "                raise ValueError(f\"Cannot find block with id {id}\")",
            "            cls = component_or_layout_class(block_config[\"type\"])",
            "",
            "            # If a Gradio app B is loaded into a Gradio app A, and B itself loads a",
            "            # Gradio app C, then the proxy_urls of the components in A need to be the",
            "            # URL of C, not B. The else clause below handles this case.",
            "            if block_config[\"props\"].get(\"proxy_url\") is None:",
            "                block_config[\"props\"][\"proxy_url\"] = f\"{proxy_url}/\"",
            "            postprocessed_value = block_config[\"props\"].pop(\"value\", None)",
            "",
            "            constructor_args = cls.recover_kwargs(block_config[\"props\"])",
            "            block = cls(**constructor_args)",
            "            if postprocessed_value is not None:",
            "                block.value = postprocessed_value  # type: ignore",
            "",
            "            block_proxy_url = block_config[\"props\"][\"proxy_url\"]",
            "            block.proxy_url = block_proxy_url",
            "            proxy_urls.add(block_proxy_url)",
            "            if (",
            "                _selectable := block_config[\"props\"].pop(\"_selectable\", None)",
            "            ) is not None:",
            "                block._selectable = _selectable  # type: ignore",
            "",
            "            return block",
            "",
            "        def iterate_over_children(children_list):",
            "            for child_config in children_list:",
            "                id = child_config[\"id\"]",
            "                block = get_block_instance(id)",
            "",
            "                original_mapping[id] = block",
            "",
            "                children = child_config.get(\"children\")",
            "                if children is not None:",
            "                    if not isinstance(block, BlockContext):",
            "                        raise ValueError(",
            "                            f\"Invalid config, Block with id {id} has children but is not a BlockContext.\"",
            "                        )",
            "                    with block:",
            "                        iterate_over_children(children)",
            "",
            "        derived_fields = [\"types\"]",
            "",
            "        with Blocks(theme=theme) as blocks:",
            "            # ID 0 should be the root Blocks component",
            "            original_mapping[0] = Context.root_block or blocks",
            "",
            "            iterate_over_children(config[\"layout\"][\"children\"])",
            "",
            "            first_dependency = None",
            "",
            "            # add the event triggers",
            "            for dependency, fn in zip(config[\"dependencies\"], fns):",
            "                # We used to add a \"fake_event\" to the config to cache examples",
            "                # without removing it. This was causing bugs in calling gr.load",
            "                # We fixed the issue by removing \"fake_event\" from the config in examples.py",
            "                # but we still need to skip these events when loading the config to support",
            "                # older demos",
            "                if \"trigger\" in dependency and dependency[\"trigger\"] == \"fake_event\":",
            "                    continue",
            "                for field in derived_fields:",
            "                    dependency.pop(field, None)",
            "",
            "                # older versions had a separate trigger field, but now it is part of the",
            "                # targets field",
            "                _targets = dependency.pop(\"targets\")",
            "                trigger = dependency.pop(\"trigger\", None)",
            "                targets = [",
            "                    getattr(",
            "                        original_mapping[",
            "                            target if isinstance(target, int) else target[0]",
            "                        ],",
            "                        trigger if isinstance(target, int) else target[1],",
            "                    )",
            "                    for target in _targets",
            "                ]",
            "                dependency.pop(\"backend_fn\")",
            "                dependency.pop(\"documentation\", None)",
            "                dependency[\"inputs\"] = [",
            "                    original_mapping[i] for i in dependency[\"inputs\"]",
            "                ]",
            "                dependency[\"outputs\"] = [",
            "                    original_mapping[o] for o in dependency[\"outputs\"]",
            "                ]",
            "                dependency.pop(\"status_tracker\", None)",
            "                dependency[\"preprocess\"] = False",
            "                dependency[\"postprocess\"] = False",
            "                targets = [",
            "                    EventListenerMethod(",
            "                        t.__self__ if t.has_trigger else None, t.event_name",
            "                    )",
            "                    for t in targets",
            "                ]",
            "                dependency = blocks.set_event_trigger(",
            "                    targets=targets, fn=fn, **dependency",
            "                )[0]",
            "                if first_dependency is None:",
            "                    first_dependency = dependency",
            "",
            "            # Allows some use of Interface-specific methods with loaded Spaces",
            "            if first_dependency and Context.root_block:",
            "                blocks.predict = [fns[0]]",
            "                blocks.input_components = [",
            "                    Context.root_block.blocks[i] for i in first_dependency[\"inputs\"]",
            "                ]",
            "                blocks.output_components = [",
            "                    Context.root_block.blocks[o] for o in first_dependency[\"outputs\"]",
            "                ]",
            "                blocks.__name__ = \"Interface\"",
            "                blocks.api_mode = True",
            "        blocks.proxy_urls = proxy_urls",
            "        return blocks",
            "",
            "    def __str__(self):",
            "        return self.__repr__()",
            "",
            "    def __repr__(self):",
            "        num_backend_fns = len([d for d in self.dependencies if d[\"backend_fn\"]])",
            "        repr = f\"Gradio Blocks instance: {num_backend_fns} backend functions\"",
            "        repr += f\"\\n{'-' * len(repr)}\"",
            "        for d, dependency in enumerate(self.dependencies):",
            "            if dependency[\"backend_fn\"]:",
            "                repr += f\"\\nfn_index={d}\"",
            "                repr += \"\\n inputs:\"",
            "                for input_id in dependency[\"inputs\"]:",
            "                    block = self.blocks[input_id]",
            "                    repr += f\"\\n |-{block}\"",
            "                repr += \"\\n outputs:\"",
            "                for output_id in dependency[\"outputs\"]:",
            "                    block = self.blocks[output_id]",
            "                    repr += f\"\\n |-{block}\"",
            "        return repr",
            "",
            "    @property",
            "    def expects_oauth(self):",
            "        \"\"\"Return whether the app expects user to authenticate via OAuth.\"\"\"",
            "        return any(",
            "            isinstance(block, (components.LoginButton, components.LogoutButton))",
            "            for block in self.blocks.values()",
            "        )",
            "",
            "    def set_event_trigger(",
            "        self,",
            "        targets: Sequence[EventListenerMethod],",
            "        fn: Callable | None,",
            "        inputs: Component | list[Component] | set[Component] | None,",
            "        outputs: Component | list[Component] | None,",
            "        preprocess: bool = True,",
            "        postprocess: bool = True,",
            "        scroll_to_output: bool = False,",
            "        show_progress: Literal[\"full\", \"minimal\", \"hidden\"] = \"full\",",
            "        api_name: str | None | Literal[False] = None,",
            "        js: str | None = None,",
            "        no_target: bool = False,",
            "        queue: bool | None = None,",
            "        batch: bool = False,",
            "        max_batch_size: int = 4,",
            "        cancels: list[int] | None = None,",
            "        every: float | None = None,",
            "        collects_event_data: bool | None = None,",
            "        trigger_after: int | None = None,",
            "        trigger_only_on_success: bool = False,",
            "        trigger_mode: Literal[\"once\", \"multiple\", \"always_last\"] | None = \"once\",",
            "        concurrency_limit: int | None | Literal[\"default\"] = \"default\",",
            "        concurrency_id: str | None = None,",
            "        show_api: bool = True,",
            "    ) -> tuple[dict[str, Any], int]:",
            "        \"\"\"",
            "        Adds an event to the component's dependencies.",
            "        Parameters:",
            "            targets: a list of EventListenerMethod objects that define the event trigger",
            "            fn: Callable function",
            "            inputs: input list",
            "            outputs: output list",
            "            preprocess: whether to run the preprocess methods of components",
            "            postprocess: whether to run the postprocess methods of components",
            "            scroll_to_output: whether to scroll to output of dependency on trigger",
            "            show_progress: whether to show progress animation while running.",
            "            api_name: defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",
            "            js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components",
            "            no_target: if True, sets \"targets\" to [], used for Blocks \"load\" event",
            "            queue: If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",
            "            batch: whether this function takes in a batch of inputs",
            "            max_batch_size: the maximum batch size to send to the function",
            "            cancels: a list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.",
            "            every: Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds.",
            "            collects_event_data: whether to collect event data for this event",
            "            trigger_after: if set, this event will be triggered after 'trigger_after' function index",
            "            trigger_only_on_success: if True, this event will only be triggered if the previous event was successful (only applies if `trigger_after` is set)",
            "            trigger_mode: If \"once\" (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to \"multiple\", unlimited submissions are allowed while pending, and \"always_last\" (default for `.change()` event) would allow a second submission after the pending event is complete.",
            "            concurrency_limit: If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `queue()`, which itself is 1 by default).",
            "            concurrency_id: If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",
            "            show_api: whether to show this event in the \"view API\" page of the Gradio app, or in the \".view_api()\" method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps to use this event. If fn is None, show_api will automatically be set to False.",
            "        Returns: dependency information, dependency index",
            "        \"\"\"",
            "        # Support for singular parameter",
            "        _targets = [",
            "            (",
            "                target.block._id if target.block and not no_target else None,",
            "                target.event_name,",
            "            )",
            "            for target in targets",
            "        ]",
            "        if isinstance(inputs, set):",
            "            inputs_as_dict = True",
            "            inputs = sorted(inputs, key=lambda x: x._id)",
            "        else:",
            "            inputs_as_dict = False",
            "            if inputs is None:",
            "                inputs = []",
            "            elif not isinstance(inputs, list):",
            "                inputs = [inputs]",
            "",
            "        if isinstance(outputs, set):",
            "            outputs = sorted(outputs, key=lambda x: x._id)",
            "        elif outputs is None:",
            "            outputs = []",
            "        elif not isinstance(outputs, list):",
            "            outputs = [outputs]",
            "",
            "        if fn is not None and not cancels:",
            "            check_function_inputs_match(fn, inputs, inputs_as_dict)",
            "        if every is not None and every <= 0:",
            "            raise ValueError(\"Parameter every must be positive or None\")",
            "        if every and batch:",
            "            raise ValueError(",
            "                f\"Cannot run event in a batch and every {every} seconds. \"",
            "                \"Either batch is True or every is non-zero but not both.\"",
            "            )",
            "",
            "        if every and fn:",
            "            fn = get_continuous_fn(fn, every)",
            "        elif every:",
            "            raise ValueError(\"Cannot set a value for `every` without a `fn`.\")",
            "        if every and concurrency_limit is not None:",
            "            if concurrency_limit == \"default\":",
            "                concurrency_limit = None",
            "            else:",
            "                raise ValueError(",
            "                    \"Cannot set a value for `concurrency_limit` with `every`.\"",
            "                )",
            "",
            "        if _targets[0][1] == \"change\" and trigger_mode is None:",
            "            trigger_mode = \"always_last\"",
            "        elif trigger_mode is None:",
            "            trigger_mode = \"once\"",
            "        elif trigger_mode not in [\"once\", \"multiple\", \"always_last\"]:",
            "            raise ValueError(",
            "                f\"Invalid value for parameter `trigger_mode`: {trigger_mode}. Please choose from: {['once', 'multiple', 'always_last']}\"",
            "            )",
            "",
            "        _, progress_index, event_data_index = (",
            "            special_args(fn) if fn else (None, None, None)",
            "        )",
            "        self.fns.append(",
            "            BlockFunction(",
            "                fn,",
            "                inputs,",
            "                outputs,",
            "                preprocess,",
            "                postprocess,",
            "                inputs_as_dict=inputs_as_dict,",
            "                concurrency_limit=concurrency_limit,",
            "                concurrency_id=concurrency_id,",
            "                batch=batch,",
            "                max_batch_size=max_batch_size,",
            "                tracks_progress=progress_index is not None,",
            "            )",
            "        )",
            "",
            "        # If api_name is None or empty string, use the function name",
            "        if api_name is None or isinstance(api_name, str) and api_name.strip() == \"\":",
            "            if fn is not None:",
            "                if not hasattr(fn, \"__name__\"):",
            "                    if hasattr(fn, \"__class__\") and hasattr(fn.__class__, \"__name__\"):",
            "                        name = fn.__class__.__name__",
            "                    else:",
            "                        name = \"unnamed\"",
            "                else:",
            "                    name = fn.__name__",
            "                api_name = \"\".join(",
            "                    [s for s in name if s not in set(string.punctuation) - {\"-\", \"_\"}]",
            "                )",
            "            elif js is not None:",
            "                api_name = \"js_fn\"",
            "                show_api = False",
            "            else:",
            "                api_name = \"unnamed\"",
            "                show_api = False",
            "",
            "        if api_name is not False:",
            "            api_name = utils.append_unique_suffix(",
            "                api_name, [dep[\"api_name\"] for dep in self.dependencies]",
            "            )",
            "        else:",
            "            show_api = False",
            "",
            "        # The `show_api` parameter is False if: (1) the user explicitly sets it (2) the user sets `api_name` to False",
            "        # or (3) the user sets `fn` to None (there's no backend function)",
            "",
            "        if collects_event_data is None:",
            "            collects_event_data = event_data_index is not None",
            "",
            "        dependency = {",
            "            \"targets\": _targets,",
            "            \"inputs\": [block._id for block in inputs],",
            "            \"outputs\": [block._id for block in outputs],",
            "            \"backend_fn\": fn is not None,",
            "            \"js\": js,",
            "            \"queue\": False if fn is None else queue,",
            "            \"api_name\": api_name,",
            "            \"scroll_to_output\": False if utils.get_space() else scroll_to_output,",
            "            \"show_progress\": show_progress,",
            "            \"every\": every,",
            "            \"batch\": batch,",
            "            \"max_batch_size\": max_batch_size,",
            "            \"cancels\": cancels or [],",
            "            \"types\": {",
            "                \"continuous\": bool(every),",
            "                \"generator\": inspect.isgeneratorfunction(fn)",
            "                or inspect.isasyncgenfunction(fn)",
            "                or bool(every),",
            "            },",
            "            \"collects_event_data\": collects_event_data,",
            "            \"trigger_after\": trigger_after,",
            "            \"trigger_only_on_success\": trigger_only_on_success,",
            "            \"trigger_mode\": trigger_mode,",
            "            \"show_api\": show_api,",
            "        }",
            "        self.dependencies.append(dependency)",
            "        return dependency, len(self.dependencies) - 1",
            "",
            "    def render(self):",
            "        if Context.root_block is not None:",
            "            if self._id in Context.root_block.blocks:",
            "                raise DuplicateBlockError(",
            "                    f\"A block with id: {self._id} has already been rendered in the current Blocks.\"",
            "                )",
            "            overlapping_ids = set(Context.root_block.blocks).intersection(self.blocks)",
            "            for id in overlapping_ids:",
            "                # State components are allowed to be reused between Blocks",
            "                if not isinstance(self.blocks[id], components.State):",
            "                    raise DuplicateBlockError(",
            "                        \"At least one block in this Blocks has already been rendered.\"",
            "                    )",
            "",
            "            Context.root_block.blocks.update(self.blocks)",
            "            Context.root_block.fns.extend(self.fns)",
            "            dependency_offset = len(Context.root_block.dependencies)",
            "            for i, dependency in enumerate(self.dependencies):",
            "                api_name = dependency[\"api_name\"]",
            "                if api_name is not None and api_name is not False:",
            "                    api_name_ = utils.append_unique_suffix(",
            "                        api_name,",
            "                        [dep[\"api_name\"] for dep in Context.root_block.dependencies],",
            "                    )",
            "                    if api_name != api_name_:",
            "                        dependency[\"api_name\"] = api_name_",
            "                dependency[\"cancels\"] = [",
            "                    c + dependency_offset for c in dependency[\"cancels\"]",
            "                ]",
            "                if dependency.get(\"trigger_after\") is not None:",
            "                    dependency[\"trigger_after\"] += dependency_offset",
            "                # Recreate the cancel function so that it has the latest",
            "                # dependency fn indices. This is necessary to properly cancel",
            "                # events in the backend",
            "                if dependency[\"cancels\"]:",
            "                    updated_cancels = [",
            "                        Context.root_block.dependencies[i]",
            "                        for i in dependency[\"cancels\"]",
            "                    ]",
            "                    new_fn = BlockFunction(",
            "                        get_cancel_function(updated_cancels)[0],",
            "                        [],",
            "                        [],",
            "                        False,",
            "                        True,",
            "                        False,",
            "                    )",
            "                    Context.root_block.fns[dependency_offset + i] = new_fn",
            "                Context.root_block.dependencies.append(dependency)",
            "            Context.root_block.temp_file_sets.extend(self.temp_file_sets)",
            "            Context.root_block.proxy_urls.update(self.proxy_urls)",
            "",
            "        if Context.block is not None:",
            "            Context.block.children.extend(self.children)",
            "        return self",
            "",
            "    def is_callable(self, fn_index: int = 0) -> bool:",
            "        \"\"\"Checks if a particular Blocks function is callable (i.e. not stateful or a generator).\"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        if inspect.isasyncgenfunction(block_fn.fn):",
            "            return False",
            "        if inspect.isgeneratorfunction(block_fn.fn):",
            "            return False",
            "        for input_id in dependency[\"inputs\"]:",
            "            block = self.blocks[input_id]",
            "            if getattr(block, \"stateful\", False):",
            "                return False",
            "        for output_id in dependency[\"outputs\"]:",
            "            block = self.blocks[output_id]",
            "            if getattr(block, \"stateful\", False):",
            "                return False",
            "",
            "        return True",
            "",
            "    def __call__(self, *inputs, fn_index: int = 0, api_name: str | None = None):",
            "        \"\"\"",
            "        Allows Blocks objects to be called as functions. Supply the parameters to the",
            "        function as positional arguments. To choose which function to call, use the",
            "        fn_index parameter, which must be a keyword argument.",
            "",
            "        Parameters:",
            "        *inputs: the parameters to pass to the function",
            "        fn_index: the index of the function to call (defaults to 0, which for Interfaces, is the default prediction function)",
            "        api_name: The api_name of the dependency to call. Will take precedence over fn_index.",
            "        \"\"\"",
            "        if api_name is not None:",
            "            inferred_fn_index = next(",
            "                (",
            "                    i",
            "                    for i, d in enumerate(self.dependencies)",
            "                    if d.get(\"api_name\") == api_name",
            "                ),",
            "                None,",
            "            )",
            "            if inferred_fn_index is None:",
            "                raise InvalidApiNameError(",
            "                    f\"Cannot find a function with api_name {api_name}\"",
            "                )",
            "            fn_index = inferred_fn_index",
            "        if not (self.is_callable(fn_index)):",
            "            raise ValueError(",
            "                \"This function is not callable because it is either stateful or is a generator. Please use the .launch() method instead to create an interactive user interface.\"",
            "            )",
            "",
            "        inputs = list(inputs)",
            "        processed_inputs = self.serialize_data(fn_index, inputs)",
            "        batch = self.dependencies[fn_index][\"batch\"]",
            "        if batch:",
            "            processed_inputs = [[inp] for inp in processed_inputs]",
            "",
            "        outputs = client_utils.synchronize_async(",
            "            self.process_api,",
            "            fn_index=fn_index,",
            "            inputs=processed_inputs,",
            "            request=None,",
            "            state={},",
            "        )",
            "        outputs = outputs[\"data\"]",
            "",
            "        if batch:",
            "            outputs = [out[0] for out in outputs]",
            "",
            "        outputs = self.deserialize_data(fn_index, outputs)",
            "        processed_outputs = utils.resolve_singleton(outputs)",
            "",
            "        return processed_outputs",
            "",
            "    async def call_function(",
            "        self,",
            "        fn_index: int,",
            "        processed_input: list[Any],",
            "        iterator: AsyncIterator[Any] | None = None,",
            "        requests: routes.Request | list[routes.Request] | None = None,",
            "        event_id: str | None = None,",
            "        event_data: EventData | None = None,",
            "        in_event_listener: bool = False,",
            "    ):",
            "        \"\"\"",
            "        Calls function with given index and preprocessed input, and measures process time.",
            "        Parameters:",
            "            fn_index: index of function to call",
            "            processed_input: preprocessed input to pass to function",
            "            iterator: iterator to use if function is a generator",
            "            requests: requests to pass to function",
            "            event_id: id of event in queue",
            "            event_data: data associated with event trigger",
            "        \"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        if not block_fn.fn:",
            "            raise IndexError(f\"function with index {fn_index} not defined.\")",
            "        is_generating = False",
            "        request = requests[0] if isinstance(requests, list) else requests",
            "        start = time.time()",
            "",
            "        fn = utils.get_function_with_locals(",
            "            fn=block_fn.fn,",
            "            blocks=self,",
            "            event_id=event_id,",
            "            in_event_listener=in_event_listener,",
            "            request=request,",
            "        )",
            "",
            "        if iterator is None:  # If not a generator function that has already run",
            "            if block_fn.inputs_as_dict:",
            "                processed_input = [dict(zip(block_fn.inputs, processed_input))]",
            "",
            "            processed_input, progress_index, _ = special_args(",
            "                block_fn.fn, processed_input, request, event_data",
            "            )",
            "            progress_tracker = (",
            "                processed_input[progress_index] if progress_index is not None else None",
            "            )",
            "",
            "            if progress_tracker is not None and progress_index is not None:",
            "                progress_tracker, fn = create_tracker(fn, progress_tracker.track_tqdm)",
            "                processed_input[progress_index] = progress_tracker",
            "",
            "            if inspect.iscoroutinefunction(fn):",
            "                prediction = await fn(*processed_input)",
            "            else:",
            "                prediction = await anyio.to_thread.run_sync(",
            "                    fn, *processed_input, limiter=self.limiter",
            "                )",
            "        else:",
            "            prediction = None",
            "",
            "        if inspect.isgeneratorfunction(fn) or inspect.isasyncgenfunction(fn):",
            "            try:",
            "                if iterator is None:",
            "                    iterator = cast(AsyncIterator[Any], prediction)",
            "                if inspect.isgenerator(iterator):",
            "                    iterator = utils.SyncToAsyncIterator(iterator, self.limiter)",
            "                prediction = await utils.async_iteration(iterator)",
            "                is_generating = True",
            "            except StopAsyncIteration:",
            "                n_outputs = len(self.dependencies[fn_index].get(\"outputs\"))",
            "                prediction = (",
            "                    components._Keywords.FINISHED_ITERATING",
            "                    if n_outputs == 1",
            "                    else (components._Keywords.FINISHED_ITERATING,) * n_outputs",
            "                )",
            "                iterator = None",
            "",
            "        duration = time.time() - start",
            "",
            "        return {",
            "            \"prediction\": prediction,",
            "            \"duration\": duration,",
            "            \"is_generating\": is_generating,",
            "            \"iterator\": iterator,",
            "        }",
            "",
            "    def serialize_data(self, fn_index: int, inputs: list[Any]) -> list[Any]:",
            "        dependency = self.dependencies[fn_index]",
            "        processed_input = []",
            "",
            "        def format_file(s):",
            "            return FileData(path=s).model_dump()",
            "",
            "        for i, input_id in enumerate(dependency[\"inputs\"]):",
            "            try:",
            "                block = self.blocks[input_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Input component with id {input_id} used in {dependency['trigger']}() event is not defined in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "            if not isinstance(block, components.Component):",
            "                raise InvalidComponentError(",
            "                    f\"{block.__class__} Component with id {input_id} not a valid input component.\"",
            "                )",
            "            api_info = block.api_info()",
            "            if client_utils.value_is_file(api_info):",
            "                serialized_input = client_utils.traverse(",
            "                    inputs[i],",
            "                    format_file,",
            "                    lambda s: client_utils.is_filepath(s) or client_utils.is_url(s),",
            "                )",
            "            else:",
            "                serialized_input = inputs[i]",
            "            processed_input.append(serialized_input)",
            "",
            "        return processed_input",
            "",
            "    def deserialize_data(self, fn_index: int, outputs: list[Any]) -> list[Any]:",
            "        dependency = self.dependencies[fn_index]",
            "        predictions = []",
            "",
            "        for o, output_id in enumerate(dependency[\"outputs\"]):",
            "            try:",
            "                block = self.blocks[output_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Output component with id {output_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "            if not isinstance(block, components.Component):",
            "                raise InvalidComponentError(",
            "                    f\"{block.__class__} Component with id {output_id} not a valid output component.\"",
            "                )",
            "",
            "            deserialized = client_utils.traverse(",
            "                outputs[o], lambda s: s[\"path\"], client_utils.is_file_obj",
            "            )",
            "            predictions.append(deserialized)",
            "",
            "        return predictions",
            "",
            "    def validate_inputs(self, fn_index: int, inputs: list[Any]):",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        dep_inputs = dependency[\"inputs\"]",
            "",
            "        # This handles incorrect inputs when args are changed by a JS function",
            "        # Only check not enough args case, ignore extra arguments (for now)",
            "        # TODO: make this stricter?",
            "        if len(inputs) < len(dep_inputs):",
            "            name = (",
            "                f\" ({block_fn.name})\"",
            "                if block_fn.name and block_fn.name != \"<lambda>\"",
            "                else \"\"",
            "            )",
            "",
            "            wanted_args = []",
            "            received_args = []",
            "            for input_id in dep_inputs:",
            "                block = self.blocks[input_id]",
            "                wanted_args.append(str(block))",
            "            for inp in inputs:",
            "                v = f'\"{inp}\"' if isinstance(inp, str) else str(inp)",
            "                received_args.append(v)",
            "",
            "            wanted = \", \".join(wanted_args)",
            "            received = \", \".join(received_args)",
            "",
            "            # JS func didn't pass enough arguments",
            "            raise ValueError(",
            "                f\"\"\"An event handler{name} didn't receive enough input values (needed: {len(dep_inputs)}, got: {len(inputs)}).",
            "Check if the event handler calls a Javascript function, and make sure its return value is correct.",
            "Wanted inputs:",
            "    [{wanted}]",
            "Received inputs:",
            "    [{received}]\"\"\"",
            "            )",
            "",
            "    def preprocess_data(",
            "        self, fn_index: int, inputs: list[Any], state: SessionState | None",
            "    ):",
            "        state = state or SessionState(self)",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        self.validate_inputs(fn_index, inputs)",
            "",
            "        if block_fn.preprocess:",
            "            processed_input = []",
            "            for i, input_id in enumerate(dependency[\"inputs\"]):",
            "                try:",
            "                    block = self.blocks[input_id]",
            "                except KeyError as e:",
            "                    raise InvalidBlockError(",
            "                        f\"Input component with id {input_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                    ) from e",
            "                if not isinstance(block, components.Component):",
            "                    raise InvalidComponentError(",
            "                        f\"{block.__class__} Component with id {input_id} not a valid input component.\"",
            "                    )",
            "                if getattr(block, \"stateful\", False):",
            "                    processed_input.append(state[input_id])",
            "                else:",
            "                    if input_id in state:",
            "                        block = state[input_id]",
            "                    inputs_cached = processing_utils.move_files_to_cache(",
            "                        inputs[i], block, add_urls=True",
            "                    )",
            "                    if getattr(block, \"data_model\", None) and inputs_cached is not None:",
            "                        if issubclass(block.data_model, GradioModel):  # type: ignore",
            "                            inputs_cached = block.data_model(**inputs_cached)  # type: ignore",
            "                        elif issubclass(block.data_model, GradioRootModel):  # type: ignore",
            "                            inputs_cached = block.data_model(root=inputs_cached)  # type: ignore",
            "                    processed_input.append(block.preprocess(inputs_cached))",
            "        else:",
            "            processed_input = inputs",
            "        return processed_input",
            "",
            "    def validate_outputs(self, fn_index: int, predictions: Any | list[Any]):",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        dep_outputs = dependency[\"outputs\"]",
            "",
            "        if not isinstance(predictions, (list, tuple)):",
            "            predictions = [predictions]",
            "",
            "        if len(predictions) < len(dep_outputs):",
            "            name = (",
            "                f\" ({block_fn.name})\"",
            "                if block_fn.name and block_fn.name != \"<lambda>\"",
            "                else \"\"",
            "            )",
            "",
            "            wanted_args = []",
            "            received_args = []",
            "            for output_id in dep_outputs:",
            "                block = self.blocks[output_id]",
            "                wanted_args.append(str(block))",
            "            for pred in predictions:",
            "                v = f'\"{pred}\"' if isinstance(pred, str) else str(pred)",
            "                received_args.append(v)",
            "",
            "            wanted = \", \".join(wanted_args)",
            "            received = \", \".join(received_args)",
            "",
            "            raise ValueError(",
            "                f\"\"\"An event handler{name} didn't receive enough output values (needed: {len(dep_outputs)}, received: {len(predictions)}).",
            "Wanted outputs:",
            "    [{wanted}]",
            "Received outputs:",
            "    [{received}]\"\"\"",
            "            )",
            "",
            "    def postprocess_data(",
            "        self, fn_index: int, predictions: list | dict, state: SessionState | None",
            "    ):",
            "        state = state or SessionState(self)",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "        batch = dependency[\"batch\"]",
            "",
            "        if isinstance(predictions, dict) and len(predictions) > 0:",
            "            predictions = convert_component_dict_to_list(",
            "                dependency[\"outputs\"], predictions",
            "            )",
            "",
            "        if len(dependency[\"outputs\"]) == 1 and not (batch):",
            "            predictions = [",
            "                predictions,",
            "            ]",
            "",
            "        self.validate_outputs(fn_index, predictions)  # type: ignore",
            "",
            "        output = []",
            "        for i, output_id in enumerate(dependency[\"outputs\"]):",
            "            try:",
            "                if predictions[i] is components._Keywords.FINISHED_ITERATING:",
            "                    output.append(None)",
            "                    continue",
            "            except (IndexError, KeyError) as err:",
            "                raise ValueError(",
            "                    \"Number of output components does not match number \"",
            "                    f\"of values returned from from function {block_fn.name}\"",
            "                ) from err",
            "",
            "            try:",
            "                block = self.blocks[output_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Output component with id {output_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "",
            "            if getattr(block, \"stateful\", False):",
            "                if not utils.is_update(predictions[i]):",
            "                    state[output_id] = predictions[i]",
            "                output.append(None)",
            "            else:",
            "                prediction_value = predictions[i]",
            "                if utils.is_update(",
            "                    prediction_value",
            "                ):  # if update is passed directly (deprecated), remove Nones",
            "                    prediction_value = utils.delete_none(",
            "                        prediction_value, skip_value=True",
            "                    )",
            "",
            "                if isinstance(prediction_value, Block):",
            "                    prediction_value = prediction_value.constructor_args.copy()",
            "                    prediction_value[\"__type__\"] = \"update\"",
            "                if utils.is_update(prediction_value):",
            "                    if output_id in state:",
            "                        kwargs = state[output_id].constructor_args.copy()",
            "                    else:",
            "                        kwargs = self.blocks[output_id].constructor_args.copy()",
            "                    kwargs.update(prediction_value)",
            "                    kwargs.pop(\"value\", None)",
            "                    kwargs.pop(\"__type__\")",
            "                    kwargs[\"render\"] = False",
            "                    state[output_id] = self.blocks[output_id].__class__(**kwargs)",
            "",
            "                    prediction_value = postprocess_update_dict(",
            "                        block=state[output_id],",
            "                        update_dict=prediction_value,",
            "                        postprocess=block_fn.postprocess,",
            "                    )",
            "                elif block_fn.postprocess:",
            "                    if not isinstance(block, components.Component):",
            "                        raise InvalidComponentError(",
            "                            f\"{block.__class__} Component with id {output_id} not a valid output component.\"",
            "                        )",
            "                    prediction_value = block.postprocess(prediction_value)",
            "                outputs_cached = processing_utils.move_files_to_cache(",
            "                    prediction_value,",
            "                    block,  # type: ignore",
            "                    postprocess=True,",
            "                    add_urls=True,",
            "                )",
            "                output.append(outputs_cached)",
            "",
            "        return output",
            "",
            "    def handle_streaming_outputs(",
            "        self,",
            "        fn_index: int,",
            "        data: list,",
            "        session_hash: str | None,",
            "        run: int | None,",
            "    ) -> list:",
            "        if session_hash is None or run is None:",
            "            return data",
            "        if run not in self.pending_streams[session_hash]:",
            "            self.pending_streams[session_hash][run] = {}",
            "        stream_run = self.pending_streams[session_hash][run]",
            "",
            "        for i, output_id in enumerate(self.dependencies[fn_index][\"outputs\"]):",
            "            block = self.blocks[output_id]",
            "            if isinstance(block, components.StreamingOutput) and block.streaming:",
            "                first_chunk = output_id not in stream_run",
            "                binary_data, output_data = block.stream_output(",
            "                    data[i], f\"{session_hash}/{run}/{output_id}\", first_chunk",
            "                )",
            "                if first_chunk:",
            "                    stream_run[output_id] = []",
            "                self.pending_streams[session_hash][run][output_id].append(binary_data)",
            "                data[i] = output_data",
            "        return data",
            "",
            "    def handle_streaming_diffs(",
            "        self,",
            "        fn_index: int,",
            "        data: list,",
            "        session_hash: str | None,",
            "        run: int | None,",
            "        final: bool,",
            "    ) -> list:",
            "        if session_hash is None or run is None:",
            "            return data",
            "        first_run = run not in self.pending_diff_streams[session_hash]",
            "        if first_run:",
            "            self.pending_diff_streams[session_hash][run] = [None] * len(data)",
            "        last_diffs = self.pending_diff_streams[session_hash][run]",
            "",
            "        for i in range(len(self.dependencies[fn_index][\"outputs\"])):",
            "            if final:",
            "                data[i] = last_diffs[i]",
            "                continue",
            "",
            "            if first_run:",
            "                last_diffs[i] = data[i]",
            "            else:",
            "                prev_chunk = last_diffs[i]",
            "                last_diffs[i] = data[i]",
            "                data[i] = utils.diff(prev_chunk, data[i])",
            "",
            "        if final:",
            "            del self.pending_diff_streams[session_hash][run]",
            "",
            "        return data",
            "",
            "    async def process_api(",
            "        self,",
            "        fn_index: int,",
            "        inputs: list[Any],",
            "        state: SessionState | None = None,",
            "        request: routes.Request | list[routes.Request] | None = None,",
            "        iterator: AsyncIterator | None = None,",
            "        session_hash: str | None = None,",
            "        event_id: str | None = None,",
            "        event_data: EventData | None = None,",
            "        in_event_listener: bool = True,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Processes API calls from the frontend. First preprocesses the data,",
            "        then runs the relevant function, then postprocesses the output.",
            "        Parameters:",
            "            fn_index: Index of function to run.",
            "            inputs: input data received from the frontend",
            "            state: data stored from stateful components for session (key is input block id)",
            "            request: the gr.Request object containing information about the network request (e.g. IP address, headers, query parameters, username)",
            "            iterators: the in-progress iterators for each generator function (key is function index)",
            "            event_id: id of event that triggered this API call",
            "            event_data: data associated with the event trigger itself",
            "        Returns: None",
            "        \"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        batch = self.dependencies[fn_index][\"batch\"]",
            "",
            "        if batch:",
            "            max_batch_size = self.dependencies[fn_index][\"max_batch_size\"]",
            "            batch_sizes = [len(inp) for inp in inputs]",
            "            batch_size = batch_sizes[0]",
            "            if inspect.isasyncgenfunction(block_fn.fn) or inspect.isgeneratorfunction(",
            "                block_fn.fn",
            "            ):",
            "                raise ValueError(\"Gradio does not support generators in batch mode.\")",
            "            if not all(x == batch_size for x in batch_sizes):",
            "                raise ValueError(",
            "                    f\"All inputs to a batch function must have the same length but instead have sizes: {batch_sizes}.\"",
            "                )",
            "            if batch_size > max_batch_size:",
            "                raise ValueError(",
            "                    f\"Batch size ({batch_size}) exceeds the max_batch_size for this function ({max_batch_size})\"",
            "                )",
            "",
            "            inputs = [",
            "                self.preprocess_data(fn_index, list(i), state) for i in zip(*inputs)",
            "            ]",
            "            result = await self.call_function(",
            "                fn_index,",
            "                list(zip(*inputs)),",
            "                None,",
            "                request,",
            "                event_id,",
            "                event_data,",
            "                in_event_listener,",
            "            )",
            "            preds = result[\"prediction\"]",
            "            data = [",
            "                self.postprocess_data(fn_index, list(o), state) for o in zip(*preds)",
            "            ]",
            "            data = list(zip(*data))",
            "            is_generating, iterator = None, None",
            "        else:",
            "            old_iterator = iterator",
            "            if old_iterator:",
            "                inputs = []",
            "            else:",
            "                inputs = self.preprocess_data(fn_index, inputs, state)",
            "            was_generating = old_iterator is not None",
            "            result = await self.call_function(",
            "                fn_index,",
            "                inputs,",
            "                old_iterator,",
            "                request,",
            "                event_id,",
            "                event_data,",
            "                in_event_listener,",
            "            )",
            "            data = self.postprocess_data(fn_index, result[\"prediction\"], state)",
            "            is_generating, iterator = result[\"is_generating\"], result[\"iterator\"]",
            "            if is_generating or was_generating:",
            "                run = id(old_iterator) if was_generating else id(iterator)",
            "                data = self.handle_streaming_outputs(",
            "                    fn_index,",
            "                    data,",
            "                    session_hash=session_hash,",
            "                    run=run,",
            "                )",
            "                data = self.handle_streaming_diffs(",
            "                    fn_index,",
            "                    data,",
            "                    session_hash=session_hash,",
            "                    run=run,",
            "                    final=not is_generating,",
            "                )",
            "",
            "        block_fn.total_runtime += result[\"duration\"]",
            "        block_fn.total_runs += 1",
            "        return {",
            "            \"data\": data,",
            "            \"is_generating\": is_generating,",
            "            \"iterator\": iterator,",
            "            \"duration\": result[\"duration\"],",
            "            \"average_duration\": block_fn.total_runtime / block_fn.total_runs,",
            "        }",
            "",
            "    def create_limiter(self):",
            "        self.limiter = (",
            "            None",
            "            if self.max_threads == 40",
            "            else CapacityLimiter(total_tokens=self.max_threads)",
            "        )",
            "",
            "    def get_config(self):",
            "        return {\"type\": \"column\"}",
            "",
            "    def get_config_file(self):",
            "        config = {",
            "            \"version\": routes.VERSION,",
            "            \"mode\": self.mode,",
            "            \"app_id\": self.app_id,",
            "            \"dev_mode\": self.dev_mode,",
            "            \"analytics_enabled\": self.analytics_enabled,",
            "            \"components\": [],",
            "            \"css\": self.css,",
            "            \"js\": self.js,",
            "            \"head\": self.head,",
            "            \"title\": self.title or \"Gradio\",",
            "            \"space_id\": self.space_id,",
            "            \"enable_queue\": True,  # launch attributes",
            "            \"show_error\": getattr(self, \"show_error\", False),",
            "            \"show_api\": self.show_api,",
            "            \"is_colab\": utils.colab_check(),",
            "            \"stylesheets\": self.stylesheets,",
            "            \"theme\": self.theme.name,",
            "            \"protocol\": \"sse_v2\",",
            "            \"body_css\": {",
            "                \"body_background_fill\": self.theme._get_computed_value(",
            "                    \"body_background_fill\"",
            "                ),",
            "                \"body_text_color\": self.theme._get_computed_value(\"body_text_color\"),",
            "                \"body_background_fill_dark\": self.theme._get_computed_value(",
            "                    \"body_background_fill_dark\"",
            "                ),",
            "                \"body_text_color_dark\": self.theme._get_computed_value(",
            "                    \"body_text_color_dark\"",
            "                ),",
            "            },",
            "            \"fill_height\": self.fill_height,",
            "        }",
            "",
            "        def get_layout(block):",
            "            if not isinstance(block, BlockContext):",
            "                return {\"id\": block._id}",
            "            children_layout = []",
            "            for child in block.children:",
            "                children_layout.append(get_layout(child))",
            "            return {\"id\": block._id, \"children\": children_layout}",
            "",
            "        config[\"layout\"] = get_layout(self)",
            "",
            "        for _id, block in self.blocks.items():",
            "            props = block.get_config() if hasattr(block, \"get_config\") else {}",
            "            block_config = {",
            "                \"id\": _id,",
            "                \"type\": block.get_block_name(),",
            "                \"props\": utils.delete_none(props),",
            "            }",
            "            block_config[\"skip_api\"] = block.skip_api",
            "            block_config[\"component_class_id\"] = getattr(",
            "                block, \"component_class_id\", None",
            "            )",
            "",
            "            if not block.skip_api:",
            "                block_config[\"api_info\"] = block.api_info()  # type: ignore",
            "                block_config[\"example_inputs\"] = block.example_inputs()  # type: ignore",
            "            config[\"components\"].append(block_config)",
            "        config[\"dependencies\"] = self.dependencies",
            "        return config",
            "",
            "    def __enter__(self):",
            "        if Context.block is None:",
            "            Context.root_block = self",
            "        self.parent = Context.block",
            "        Context.block = self",
            "        self.exited = False",
            "        return self",
            "",
            "    def __exit__(self, exc_type: type[BaseException] | None = None, *args):",
            "        if exc_type is not None:",
            "            Context.block = None",
            "            Context.root_block = None",
            "            return",
            "        super().fill_expected_parents()",
            "        Context.block = self.parent",
            "        # Configure the load events before root_block is reset",
            "        self.attach_load_events()",
            "        if self.parent is None:",
            "            Context.root_block = None",
            "        else:",
            "            self.parent.children.extend(self.children)",
            "        self.config = self.get_config_file()",
            "        self.app = routes.App.create_app(self)",
            "        self.progress_tracking = any(block_fn.tracks_progress for block_fn in self.fns)",
            "        self.exited = True",
            "",
            "    def clear(self):",
            "        \"\"\"Resets the layout of the Blocks object.\"\"\"",
            "        self.blocks = {}",
            "        self.fns = []",
            "        self.dependencies = []",
            "        self.children = []",
            "        return self",
            "",
            "    @document()",
            "    def queue(",
            "        self,",
            "        status_update_rate: float | Literal[\"auto\"] = \"auto\",",
            "        api_open: bool | None = None,",
            "        max_size: int | None = None,",
            "        concurrency_count: int | None = None,",
            "        *,",
            "        default_concurrency_limit: int | None | Literal[\"not_set\"] = \"not_set\",",
            "    ):",
            "        \"\"\"",
            "        By enabling the queue you can control when users know their position in the queue, and set a limit on maximum number of events allowed.",
            "        Parameters:",
            "            status_update_rate: If \"auto\", Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.",
            "            api_open: If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.",
            "            max_size: The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.",
            "            concurrency_count: Deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().",
            "            default_concurrency_limit: The default value of `concurrency_limit` to use for event listeners that don't specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise.",
            "        Example: (Blocks)",
            "            with gr.Blocks() as demo:",
            "                button = gr.Button(label=\"Generate Image\")",
            "                button.click(fn=image_generator, inputs=gr.Textbox(), outputs=gr.Image())",
            "            demo.queue(max_size=10)",
            "            demo.launch()",
            "        Example: (Interface)",
            "            demo = gr.Interface(image_generator, gr.Textbox(), gr.Image())",
            "            demo.queue(max_size=20)",
            "            demo.launch()",
            "        \"\"\"",
            "        if concurrency_count:",
            "            raise DeprecationWarning(",
            "                \"concurrency_count has been deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().\"",
            "            )",
            "        if api_open is not None:",
            "            self.api_open = api_open",
            "        if utils.is_zero_gpu_space():",
            "            max_size = 1 if max_size is None else max_size",
            "        self._queue = queueing.Queue(",
            "            live_updates=status_update_rate == \"auto\",",
            "            concurrency_count=self.max_threads,",
            "            update_intervals=status_update_rate if status_update_rate != \"auto\" else 1,",
            "            max_size=max_size,",
            "            block_fns=self.fns,",
            "            default_concurrency_limit=default_concurrency_limit,",
            "        )",
            "        self.config = self.get_config_file()",
            "        self.app = routes.App.create_app(self)",
            "        return self",
            "",
            "    def validate_queue_settings(self):",
            "        for dep in self.dependencies:",
            "            for i in dep[\"cancels\"]:",
            "                if not self.queue_enabled_for_fn(i):",
            "                    raise ValueError(",
            "                        \"Queue needs to be enabled! \"",
            "                        \"You may get this error by either 1) passing a function that uses the yield keyword \"",
            "                        \"into an interface without enabling the queue or 2) defining an event that cancels \"",
            "                        \"another event without enabling the queue. Both can be solved by calling .queue() \"",
            "                        \"before .launch()\"",
            "                    )",
            "            if dep[\"batch\"] and dep[\"queue\"] is False:",
            "                raise ValueError(\"In order to use batching, the queue must be enabled.\")",
            "",
            "    def launch(",
            "        self,",
            "        inline: bool | None = None,",
            "        inbrowser: bool = False,",
            "        share: bool | None = None,",
            "        debug: bool = False,",
            "        max_threads: int = 40,",
            "        auth: Callable | tuple[str, str] | list[tuple[str, str]] | None = None,",
            "        auth_message: str | None = None,",
            "        prevent_thread_lock: bool = False,",
            "        show_error: bool = False,",
            "        server_name: str | None = None,",
            "        server_port: int | None = None,",
            "        *,",
            "        height: int = 500,",
            "        width: int | str = \"100%\",",
            "        favicon_path: str | None = None,",
            "        ssl_keyfile: str | None = None,",
            "        ssl_certfile: str | None = None,",
            "        ssl_keyfile_password: str | None = None,",
            "        ssl_verify: bool = True,",
            "        quiet: bool = False,",
            "        show_api: bool = True,",
            "        allowed_paths: list[str] | None = None,",
            "        blocked_paths: list[str] | None = None,",
            "        root_path: str | None = None,",
            "        app_kwargs: dict[str, Any] | None = None,",
            "        state_session_capacity: int = 10000,",
            "        share_server_address: str | None = None,",
            "        share_server_protocol: Literal[\"http\", \"https\"] | None = None,",
            "        _frontend: bool = True,",
            "    ) -> tuple[FastAPI, str, str]:",
            "        \"\"\"",
            "        Launches a simple web server that serves the demo. Can also be used to create a",
            "        public link used by anyone to access the demo from their browser by setting share=True.",
            "",
            "        Parameters:",
            "            inline: whether to display in the interface inline in an iframe. Defaults to True in python notebooks; False otherwise.",
            "            inbrowser: whether to automatically launch the interface in a new tab on the default browser.",
            "            share: whether to create a publicly shareable link for the interface. Creates an SSH tunnel to make your UI accessible from anywhere. If not provided, it is set to False by default every time, except when running in Google Colab. When localhost is not accessible (e.g. Google Colab), setting share=False is not supported.",
            "            debug: if True, blocks the main thread from running. If running in Google Colab, this is needed to print the errors in the cell output.",
            "            auth: If provided, username and password (or list of username-password tuples) required to access interface. Can also provide function that takes username and password and returns True if valid login.",
            "            auth_message: If provided, HTML message provided on login page.",
            "            prevent_thread_lock: If True, the interface will block the main thread while the server is running.",
            "            show_error: If True, any errors in the interface will be displayed in an alert modal and printed in the browser console log",
            "            server_port: will start gradio app on this port (if available). Can be set by environment variable GRADIO_SERVER_PORT. If None, will search for an available port starting at 7860.",
            "            server_name: to make app accessible on local network, set this to \"0.0.0.0\". Can be set by environment variable GRADIO_SERVER_NAME. If None, will use \"127.0.0.1\".",
            "            max_threads: the maximum number of total threads that the Gradio app can generate in parallel. The default is inherited from the starlette library (currently 40).",
            "            width: The width in pixels of the iframe element containing the interface (used if inline=True)",
            "            height: The height in pixels of the iframe element containing the interface (used if inline=True)",
            "            favicon_path: If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for the web page.",
            "            ssl_keyfile: If a path to a file is provided, will use this as the private key file to create a local server running on https.",
            "            ssl_certfile: If a path to a file is provided, will use this as the signed certificate for https. Needs to be provided if ssl_keyfile is provided.",
            "            ssl_keyfile_password: If a password is provided, will use this with the ssl certificate for https.",
            "            ssl_verify: If False, skips certificate validation which allows self-signed certificates to be used.",
            "            quiet: If True, suppresses most print statements.",
            "            show_api: If True, shows the api docs in the footer of the app. Default True.",
            "            allowed_paths: List of complete filepaths or parent directories that gradio is allowed to serve (in addition to the directory containing the gradio python file). Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",
            "            blocked_paths: List of complete filepaths or parent directories that gradio is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",
            "            root_path: The root path (or \"mount point\") of the application, if it's not served from the root (\"/\") of the domain. Often used when the application is behind a reverse proxy that forwards requests to the application. For example, if the application is served at \"https://example.com/myapp\", the `root_path` should be set to \"/myapp\". Can be set by environment variable GRADIO_ROOT_PATH. Defaults to \"\".",
            "            app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`",
            "            state_session_capacity: The maximum number of sessions whose information to store in memory. If the number of sessions exceeds this number, the oldest sessions will be removed. Reduce capacity to reduce memory usage when using gradio.State or returning updated components from functions. Defaults to 10000.",
            "            share_server_address: Use this to specify a custom FRP server and port for sharing Gradio apps (only applies if share=True). If not provided, will use the default FRP server at https://gradio.live. See https://github.com/huggingface/frp for more information.",
            "            share_server_protocol: Use this to specify the protocol to use for the share links. Defaults to \"https\", unless a custom share_server_address is provided, in which case it defaults to \"http\". If you are using a custom share_server_address and want to use https, you must set this to \"https\".",
            "        Returns:",
            "            app: FastAPI app object that is running the demo",
            "            local_url: Locally accessible link to the demo",
            "            share_url: Publicly accessible link to the demo (if share=True, otherwise None)",
            "        Example: (Blocks)",
            "            import gradio as gr",
            "            def reverse(text):",
            "                return text[::-1]",
            "            with gr.Blocks() as demo:",
            "                button = gr.Button(value=\"Reverse\")",
            "                button.click(reverse, gr.Textbox(), gr.Textbox())",
            "            demo.launch(share=True, auth=(\"username\", \"password\"))",
            "        Example:  (Interface)",
            "            import gradio as gr",
            "            def reverse(text):",
            "                return text[::-1]",
            "            demo = gr.Interface(reverse, \"text\", \"text\")",
            "            demo.launch(share=True, auth=(\"username\", \"password\"))",
            "        \"\"\"",
            "        if self._is_running_in_reload_thread:",
            "            # We have already launched the demo",
            "            return None, None, None  # type: ignore",
            "",
            "        if not self.exited:",
            "            self.__exit__()",
            "",
            "        if (",
            "            auth",
            "            and not callable(auth)",
            "            and not isinstance(auth[0], tuple)",
            "            and not isinstance(auth[0], list)",
            "        ):",
            "            self.auth = [auth]",
            "        else:",
            "            self.auth = auth",
            "        self.auth_message = auth_message",
            "        self.show_error = show_error",
            "        self.height = height",
            "        self.width = width",
            "        self.favicon_path = favicon_path",
            "        self.ssl_verify = ssl_verify",
            "        self.state_session_capacity = state_session_capacity",
            "        if root_path is None:",
            "            self.root_path = os.environ.get(\"GRADIO_ROOT_PATH\", \"\")",
            "        else:",
            "            self.root_path = root_path",
            "",
            "        self.show_api = show_api",
            "",
            "        self.allowed_paths = allowed_paths or []",
            "        self.blocked_paths = blocked_paths or []",
            "",
            "        if not isinstance(self.allowed_paths, list):",
            "            raise ValueError(\"`allowed_paths` must be a list of directories.\")",
            "        if not isinstance(self.blocked_paths, list):",
            "            raise ValueError(\"`blocked_paths` must be a list of directories.\")",
            "",
            "        self.validate_queue_settings()",
            "",
            "        self.config = self.get_config_file()",
            "        self.max_threads = max_threads",
            "        self._queue.max_thread_count = max_threads",
            "",
            "        if self.is_running:",
            "            if not isinstance(self.local_url, str):",
            "                raise ValueError(f\"Invalid local_url: {self.local_url}\")",
            "            if not (quiet):",
            "                print(",
            "                    \"Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\\n----\"",
            "                )",
            "        else:",
            "            if wasm_utils.IS_WASM:",
            "                server_name = \"xxx\"",
            "                server_port = 99999",
            "                local_url = \"\"",
            "                server = None",
            "",
            "                # In the Wasm environment, we only need the app object",
            "                # which the frontend app will directly communicate with through the Worker API,",
            "                # and we don't need to start a server.",
            "                # So we just create the app object and register it here,",
            "                # and avoid using `networking.start_server` that would start a server that don't work in the Wasm env.",
            "                from gradio.routes import App",
            "",
            "                app = App.create_app(self, app_kwargs=app_kwargs)",
            "                wasm_utils.register_app(app)",
            "            else:",
            "                (",
            "                    server_name,",
            "                    server_port,",
            "                    local_url,",
            "                    app,",
            "                    server,",
            "                ) = networking.start_server(",
            "                    self,",
            "                    server_name,",
            "                    server_port,",
            "                    ssl_keyfile,",
            "                    ssl_certfile,",
            "                    ssl_keyfile_password,",
            "                    app_kwargs=app_kwargs,",
            "                )",
            "            self.server_name = server_name",
            "            self.local_url = local_url",
            "            self.server_port = server_port",
            "            self.server_app = (",
            "                self.app",
            "            ) = app  # server_app is included for backwards compatibility",
            "            self.server = server",
            "            self.is_running = True",
            "            self.is_colab = utils.colab_check()",
            "            self.is_kaggle = utils.kaggle_check()",
            "            self.share_server_address = share_server_address",
            "            self.share_server_protocol = share_server_protocol or (",
            "                \"http\" if share_server_address is not None else \"https\"",
            "            )",
            "",
            "            self.protocol = (",
            "                \"https\"",
            "                if self.local_url.startswith(\"https\") or self.is_colab",
            "                else \"http\"",
            "            )",
            "            if not wasm_utils.IS_WASM and not self.is_colab:",
            "                print(",
            "                    strings.en[\"RUNNING_LOCALLY_SEPARATED\"].format(",
            "                        self.protocol, self.server_name, self.server_port",
            "                    )",
            "                )",
            "",
            "            self._queue.set_server_app(self.server_app)",
            "",
            "            if not wasm_utils.IS_WASM:",
            "                # Cannot run async functions in background other than app's scope.",
            "                # Workaround by triggering the app endpoint",
            "                httpx.get(f\"{self.local_url}startup-events\", verify=ssl_verify)",
            "            else:",
            "                # NOTE: One benefit of the code above dispatching `startup_events()` via a self HTTP request is",
            "                # that `self._queue.start()` is called in another thread which is managed by the HTTP server, `uvicorn`",
            "                # so all the asyncio tasks created by the queue runs in an event loop in that thread and",
            "                # will be cancelled just by stopping the server.",
            "                # In contrast, in the Wasm env, we can't do that because `threading` is not supported and all async tasks will run in the same event loop, `pyodide.webloop.WebLoop` in the main thread.",
            "                # So we need to manually cancel them. See `self.close()`..",
            "                self.startup_events()",
            "",
            "        utils.launch_counter()",
            "        self.is_sagemaker = utils.sagemaker_check()",
            "        if share is None:",
            "            if self.is_colab:",
            "                if not quiet:",
            "                    print(",
            "                        \"Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            elif self.is_kaggle:",
            "                if not quiet:",
            "                    print(",
            "                        \"Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            elif self.is_sagemaker:",
            "                if not quiet:",
            "                    print(",
            "                        \"Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            else:",
            "                self.share = False",
            "        else:",
            "            self.share = share",
            "",
            "        # If running in a colab or not able to access localhost,",
            "        # a shareable link must be created.",
            "        if (",
            "            _frontend",
            "            and not wasm_utils.IS_WASM",
            "            and not networking.url_ok(self.local_url)",
            "            and not self.share",
            "        ):",
            "            raise ValueError(",
            "                \"When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.\"",
            "            )",
            "",
            "        if self.is_colab and not quiet:",
            "            if debug:",
            "                print(strings.en[\"COLAB_DEBUG_TRUE\"])",
            "            else:",
            "                print(strings.en[\"COLAB_DEBUG_FALSE\"])",
            "            if not self.share:",
            "                print(strings.en[\"COLAB_WARNING\"].format(self.server_port))",
            "",
            "        if self.share:",
            "            if self.space_id:",
            "                warnings.warn(",
            "                    \"Setting share=True is not supported on Hugging Face Spaces\"",
            "                )",
            "                self.share = False",
            "            if wasm_utils.IS_WASM:",
            "                warnings.warn(",
            "                    \"Setting share=True is not supported in the Wasm environment\"",
            "                )",
            "                self.share = False",
            "",
            "        if self.share:",
            "            try:",
            "                if self.share_url is None:",
            "                    share_url = networking.setup_tunnel(",
            "                        local_host=self.server_name,",
            "                        local_port=self.server_port,",
            "                        share_token=self.share_token,",
            "                        share_server_address=self.share_server_address,",
            "                    )",
            "                    parsed_url = urlparse(share_url)",
            "                    self.share_url = urlunparse(",
            "                        (self.share_server_protocol,) + parsed_url[1:]",
            "                    )",
            "                print(strings.en[\"SHARE_LINK_DISPLAY\"].format(self.share_url))",
            "                if not (quiet):",
            "                    print(strings.en[\"SHARE_LINK_MESSAGE\"])",
            "            except (RuntimeError, httpx.ConnectError):",
            "                if self.analytics_enabled:",
            "                    analytics.error_analytics(\"Not able to set up tunnel\")",
            "                self.share_url = None",
            "                self.share = False",
            "                if Path(BINARY_PATH).exists():",
            "                    print(strings.en[\"COULD_NOT_GET_SHARE_LINK\"])",
            "                else:",
            "                    print(",
            "                        strings.en[\"COULD_NOT_GET_SHARE_LINK_MISSING_FILE\"].format(",
            "                            BINARY_PATH,",
            "                            BINARY_URL,",
            "                            BINARY_FILENAME,",
            "                            BINARY_FOLDER,",
            "                        )",
            "                    )",
            "        else:",
            "            if not quiet and not wasm_utils.IS_WASM:",
            "                print(strings.en[\"PUBLIC_SHARE_TRUE\"])",
            "            self.share_url = None",
            "",
            "        if inbrowser and not wasm_utils.IS_WASM:",
            "            link = self.share_url if self.share and self.share_url else self.local_url",
            "            webbrowser.open(link)",
            "",
            "        # Check if running in a Python notebook in which case, display inline",
            "        if inline is None:",
            "            inline = utils.ipython_check()",
            "        if inline:",
            "            try:",
            "                from IPython.display import HTML, Javascript, display  # type: ignore",
            "",
            "                if self.share and self.share_url:",
            "                    while not networking.url_ok(self.share_url):",
            "                        time.sleep(0.25)",
            "                    artifact = HTML(",
            "                        f'<div><iframe src=\"{self.share_url}\" width=\"{self.width}\" height=\"{self.height}\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>'",
            "                    )",
            "",
            "                elif self.is_colab:",
            "                    # modified from /usr/local/lib/python3.7/dist-packages/google/colab/output/_util.py within Colab environment",
            "                    code = \"\"\"(async (port, path, width, height, cache, element) => {",
            "                        if (!google.colab.kernel.accessAllowed && !cache) {",
            "                            return;",
            "                        }",
            "                        element.appendChild(document.createTextNode(''));",
            "                        const url = await google.colab.kernel.proxyPort(port, {cache});",
            "",
            "                        const external_link = document.createElement('div');",
            "                        external_link.innerHTML = `",
            "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">",
            "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">",
            "                                    https://localhost:${port}${path}",
            "                                </a>",
            "                            </div>",
            "                        `;",
            "                        element.appendChild(external_link);",
            "",
            "                        const iframe = document.createElement('iframe');",
            "                        iframe.src = new URL(path, url).toString();",
            "                        iframe.height = height;",
            "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"",
            "                        iframe.width = width;",
            "                        iframe.style.border = 0;",
            "                        element.appendChild(iframe);",
            "                    })\"\"\" + \"({port}, {path}, {width}, {height}, {cache}, window.element)\".format(",
            "                        port=json.dumps(self.server_port),",
            "                        path=json.dumps(\"/\"),",
            "                        width=json.dumps(self.width),",
            "                        height=json.dumps(self.height),",
            "                        cache=json.dumps(False),",
            "                    )",
            "",
            "                    artifact = Javascript(code)",
            "                else:",
            "                    artifact = HTML(",
            "                        f'<div><iframe src=\"{self.local_url}\" width=\"{self.width}\" height=\"{self.height}\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>'",
            "                    )",
            "                self.artifact = artifact",
            "                display(artifact)",
            "            except ImportError:",
            "                pass",
            "",
            "        if getattr(self, \"analytics_enabled\", False):",
            "            data = {",
            "                \"launch_method\": \"browser\" if inbrowser else \"inline\",",
            "                \"is_google_colab\": self.is_colab,",
            "                \"is_sharing_on\": self.share,",
            "                \"share_url\": self.share_url,",
            "                \"enable_queue\": True,",
            "                \"server_name\": server_name,",
            "                \"server_port\": server_port,",
            "                \"is_space\": self.space_id is not None,",
            "                \"mode\": self.mode,",
            "            }",
            "            analytics.launched_analytics(self, data)",
            "",
            "        # Block main thread if debug==True",
            "        if debug or int(os.getenv(\"GRADIO_DEBUG\", \"0\")) == 1 and not wasm_utils.IS_WASM:",
            "            self.block_thread()",
            "        # Block main thread if running in a script to stop script from exiting",
            "        is_in_interactive_mode = bool(getattr(sys, \"ps1\", sys.flags.interactive))",
            "",
            "        if (",
            "            not prevent_thread_lock",
            "            and not is_in_interactive_mode",
            "            # In the Wasm env, we don't have to block the main thread because the server won't be shut down after the execution finishes.",
            "            # Moreover, we MUST NOT do it because there is only one thread in the Wasm env and blocking it will stop the subsequent code from running.",
            "            and not wasm_utils.IS_WASM",
            "        ):",
            "            self.block_thread()",
            "",
            "        return TupleNoPrint((self.server_app, self.local_url, self.share_url))  # type: ignore",
            "",
            "    def integrate(",
            "        self,",
            "        comet_ml=None,",
            "        wandb: ModuleType | None = None,",
            "        mlflow: ModuleType | None = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        A catch-all method for integrating with other libraries. This method should be run after launch()",
            "        Parameters:",
            "            comet_ml: If a comet_ml Experiment object is provided, will integrate with the experiment and appear on Comet dashboard",
            "            wandb: If the wandb module is provided, will integrate with it and appear on WandB dashboard",
            "            mlflow: If the mlflow module  is provided, will integrate with the experiment and appear on ML Flow dashboard",
            "        \"\"\"",
            "        analytics_integration = \"\"",
            "        if comet_ml is not None:",
            "            analytics_integration = \"CometML\"",
            "            comet_ml.log_other(\"Created from\", \"Gradio\")",
            "            if self.share_url is not None:",
            "                comet_ml.log_text(f\"gradio: {self.share_url}\")",
            "                comet_ml.end()",
            "            elif self.local_url:",
            "                comet_ml.log_text(f\"gradio: {self.local_url}\")",
            "                comet_ml.end()",
            "            else:",
            "                raise ValueError(\"Please run `launch()` first.\")",
            "        if wandb is not None:",
            "            analytics_integration = \"WandB\"",
            "            if self.share_url is not None:",
            "                wandb.log(",
            "                    {",
            "                        \"Gradio panel\": wandb.Html(",
            "                            '<iframe src=\"'",
            "                            + self.share_url",
            "                            + '\" width=\"'",
            "                            + str(self.width)",
            "                            + '\" height=\"'",
            "                            + str(self.height)",
            "                            + '\" frameBorder=\"0\"></iframe>'",
            "                        )",
            "                    }",
            "                )",
            "            else:",
            "                print(",
            "                    \"The WandB integration requires you to \"",
            "                    \"`launch(share=True)` first.\"",
            "                )",
            "        if mlflow is not None:",
            "            analytics_integration = \"MLFlow\"",
            "            if self.share_url is not None:",
            "                mlflow.log_param(\"Gradio Interface Share Link\", self.share_url)",
            "            else:",
            "                mlflow.log_param(\"Gradio Interface Local Link\", self.local_url)",
            "        if self.analytics_enabled and analytics_integration:",
            "            data = {\"integration\": analytics_integration}",
            "            analytics.integration_analytics(data)",
            "",
            "    def close(self, verbose: bool = True) -> None:",
            "        \"\"\"",
            "        Closes the Interface that was launched and frees the port.",
            "        \"\"\"",
            "        try:",
            "            if wasm_utils.IS_WASM:",
            "                # NOTE:",
            "                # Normally, queue-related async tasks (e.g. continuous events created by `gr.Blocks.load(..., every=interval)`, whose async tasks are started at the `/queue/data` endpoint function)",
            "                # are running in an event loop in the server thread,",
            "                # so they will be cancelled by `self.server.close()` below.",
            "                # However, in the Wasm env, we don't have the `server` and",
            "                # all async tasks are running in the same event loop, `pyodide.webloop.WebLoop` in the main thread,",
            "                # so we have to cancel them explicitly so that these tasks won't run after a new app is launched.",
            "                self._queue._cancel_asyncio_tasks()",
            "                self.server_app._cancel_asyncio_tasks()",
            "            self._queue.close()",
            "            if self.server:",
            "                self.server.close()",
            "            self.is_running = False",
            "            # So that the startup events (starting the queue)",
            "            # happen the next time the app is launched",
            "            self.app.startup_events_triggered = False",
            "            if verbose:",
            "                print(f\"Closing server running on port: {self.server_port}\")",
            "        except (AttributeError, OSError):  # can't close if not running",
            "            pass",
            "",
            "    def block_thread(",
            "        self,",
            "    ) -> None:",
            "        \"\"\"Block main thread until interrupted by user.\"\"\"",
            "        try:",
            "            while True:",
            "                time.sleep(0.1)",
            "        except (KeyboardInterrupt, OSError):",
            "            print(\"Keyboard interruption in main thread... closing server.\")",
            "            if self.server:",
            "                self.server.close()",
            "            for tunnel in CURRENT_TUNNELS:",
            "                tunnel.kill()",
            "",
            "    def attach_load_events(self):",
            "        \"\"\"Add a load event for every component whose initial value should be randomized.\"\"\"",
            "        if Context.root_block:",
            "            for component in Context.root_block.blocks.values():",
            "                if (",
            "                    isinstance(component, components.Component)",
            "                    and component.load_event_to_attach",
            "                ):",
            "                    load_fn, every = component.load_event_to_attach",
            "                    # Use set_event_trigger to avoid ambiguity between load class/instance method",
            "",
            "                    dep = self.set_event_trigger(",
            "                        [EventListenerMethod(self, \"load\")],",
            "                        load_fn,",
            "                        None,",
            "                        component,",
            "                        no_target=True,",
            "                        # If every is None, for sure skip the queue",
            "                        # else, let the enable_queue parameter take precedence",
            "                        # this will raise a nice error message is every is used",
            "                        # without queue",
            "                        queue=False if every is None else None,",
            "                        every=every,",
            "                    )[0]",
            "                    component.load_event = dep",
            "",
            "    def startup_events(self):",
            "        \"\"\"Events that should be run when the app containing this block starts up.\"\"\"",
            "        self._queue.start()",
            "        # So that processing can resume in case the queue was stopped",
            "        self._queue.stopped = False",
            "        self.create_limiter()",
            "",
            "    def queue_enabled_for_fn(self, fn_index: int):",
            "        return self.dependencies[fn_index][\"queue\"] is not False",
            "",
            "    def get_api_info(self):",
            "        \"\"\"",
            "        Gets the information needed to generate the API docs from a Blocks.",
            "        \"\"\"",
            "        config = self.config",
            "        api_info = {\"named_endpoints\": {}, \"unnamed_endpoints\": {}}",
            "",
            "        for dependency in config[\"dependencies\"]:",
            "            if (",
            "                not dependency[\"backend_fn\"]",
            "                or not dependency[\"show_api\"]",
            "                or dependency[\"api_name\"] is False",
            "            ):",
            "                continue",
            "",
            "            dependency_info = {\"parameters\": [], \"returns\": []}",
            "            skip_endpoint = False",
            "",
            "            inputs = dependency[\"inputs\"]",
            "            for i in inputs:",
            "                for component in config[\"components\"]:",
            "                    if component[\"id\"] == i:",
            "                        break",
            "                else:",
            "                    skip_endpoint = True  # if component not found, skip endpoint",
            "                    break",
            "                type = component[\"type\"]",
            "                if self.blocks[component[\"id\"]].skip_api:",
            "                    continue",
            "                label = component[\"props\"].get(\"label\", f\"parameter_{i}\")",
            "                comp = self.get_component(component[\"id\"])",
            "                assert isinstance(comp, components.Component)",
            "                info = component[\"api_info\"]",
            "                example = comp.example_inputs()",
            "                python_type = client_utils.json_schema_to_python_type(info)",
            "                dependency_info[\"parameters\"].append(",
            "                    {",
            "                        \"label\": label,",
            "                        \"type\": info,",
            "                        \"python_type\": {",
            "                            \"type\": python_type,",
            "                            \"description\": info.get(\"description\", \"\"),",
            "                        },",
            "                        \"component\": type.capitalize(),",
            "                        \"example_input\": example,",
            "                    }",
            "                )",
            "",
            "            outputs = dependency[\"outputs\"]",
            "            for o in outputs:",
            "                for component in config[\"components\"]:",
            "                    if component[\"id\"] == o:",
            "                        break",
            "                else:",
            "                    skip_endpoint = True  # if component not found, skip endpoint",
            "                    break",
            "                type = component[\"type\"]",
            "                if self.blocks[component[\"id\"]].skip_api:",
            "                    continue",
            "                label = component[\"props\"].get(\"label\", f\"value_{o}\")",
            "                comp = self.get_component(component[\"id\"])",
            "                assert isinstance(comp, components.Component)",
            "                info = component[\"api_info\"]",
            "                example = comp.example_inputs()",
            "                python_type = client_utils.json_schema_to_python_type(info)",
            "                dependency_info[\"returns\"].append(",
            "                    {",
            "                        \"label\": label,",
            "                        \"type\": info,",
            "                        \"python_type\": {",
            "                            \"type\": python_type,",
            "                            \"description\": info.get(\"description\", \"\"),",
            "                        },",
            "                        \"component\": type.capitalize(),",
            "                    }",
            "                )",
            "",
            "            if not skip_endpoint:",
            "                api_info[\"named_endpoints\"][",
            "                    f\"/{dependency['api_name']}\"",
            "                ] = dependency_info",
            "",
            "        return api_info"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1328": [
                "Blocks",
                "preprocess_data"
            ]
        },
        "addLocation": []
    },
    "gradio/components/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": 195,
                "PatchRowcode": "         self.load_event_to_attach: None | tuple[Callable, float | None] = None"
            },
            "1": {
                "beforePatchRowNumber": 196,
                "afterPatchRowNumber": 196,
                "PatchRowcode": "         load_fn, initial_value = self.get_load_fn_and_initial_value(value)"
            },
            "2": {
                "beforePatchRowNumber": 197,
                "afterPatchRowNumber": 197,
                "PatchRowcode": "         initial_value = self.postprocess(initial_value)"
            },
            "3": {
                "beforePatchRowNumber": 198,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.value = move_files_to_cache(initial_value, self, postprocess=True)  # type: ignore"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+        self.value = move_files_to_cache("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+            initial_value,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+            self,  # type: ignore"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+            postprocess=True,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+            add_urls=True,"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+        )"
            },
            "10": {
                "beforePatchRowNumber": 199,
                "afterPatchRowNumber": 204,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 200,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "         if callable(load_fn):"
            },
            "12": {
                "beforePatchRowNumber": 201,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "             self.attach_load_event(load_fn, every)"
            }
        },
        "frontPatchFile": [
            "\"\"\"Contains all of the components that can be used with Gradio Interface / Blocks.",
            "Along with the docs for each component, you can find the names of example demos that use",
            "each component. These demos are located in the `demo` directory.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import abc",
            "import hashlib",
            "import json",
            "import sys",
            "import warnings",
            "from abc import ABC, abstractmethod",
            "from enum import Enum",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Any, Callable",
            "",
            "from gradio import utils",
            "from gradio.blocks import Block, BlockContext",
            "from gradio.component_meta import ComponentMeta",
            "from gradio.data_classes import GradioDataModel",
            "from gradio.events import EventListener",
            "from gradio.layouts import Form",
            "from gradio.processing_utils import move_files_to_cache",
            "",
            "if TYPE_CHECKING:",
            "    from typing import TypedDict",
            "",
            "    class DataframeData(TypedDict):",
            "        headers: list[str]",
            "        data: list[list[str | int | bool]]",
            "",
            "",
            "class _Keywords(Enum):",
            "    NO_VALUE = \"NO_VALUE\"  # Used as a sentinel to determine if nothing is provided as a argument for `value` in `Component.update()`",
            "    FINISHED_ITERATING = \"FINISHED_ITERATING\"  # Used to skip processing of a component's value (needed for generators + state)",
            "",
            "",
            "class ComponentBase(ABC, metaclass=ComponentMeta):",
            "    EVENTS: list[EventListener | str] = []",
            "",
            "    @abstractmethod",
            "    def preprocess(self, payload: Any) -> Any:",
            "        \"\"\"",
            "        Any preprocessing needed to be performed on function input.",
            "        Parameters:",
            "            payload: The input data received by the component from the frontend.",
            "        Returns:",
            "            The preprocessed input data sent to the user's function in the backend.",
            "        \"\"\"",
            "        return payload",
            "",
            "    @abstractmethod",
            "    def postprocess(self, value):",
            "        \"\"\"",
            "        Any postprocessing needed to be performed on function output.",
            "        Parameters:",
            "            value: The output data received by the component from the user's function in the backend.",
            "        Returns:",
            "            The postprocessed output data sent to the frontend.",
            "        \"\"\"",
            "        return value",
            "",
            "    @abstractmethod",
            "    def process_example(self, value):",
            "        \"\"\"",
            "        Process the input data in a way that can be displayed by the examples dataset component in the front-end.",
            "",
            "        For example, only return the name of a file as opposed to a full path. Or get the head of a dataframe.",
            "        The return value must be able to be json-serializable to put in the config.",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def api_info(self) -> dict[str, list[str]]:",
            "        \"\"\"",
            "        The typing information for this component as a dictionary whose values are a list of 2 strings: [Python type, language-agnostic description].",
            "        Keys of the dictionary are: raw_input, raw_output, serialized_input, serialized_output",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def example_inputs(self) -> Any:",
            "        \"\"\"",
            "        The example inputs for this component as a dictionary whose values are example inputs compatible with this component.",
            "        Keys of the dictionary are: raw, serialized",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def flag(self, payload: Any | GradioDataModel, flag_dir: str | Path = \"\") -> str:",
            "        \"\"\"",
            "        Write the component's value to a format that can be stored in a csv or jsonl format for flagging.",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def read_from_flag(self, payload: Any) -> GradioDataModel | Any:",
            "        \"\"\"",
            "        Convert the data from the csv or jsonl file into the component state.",
            "        \"\"\"",
            "        return payload",
            "",
            "    @property",
            "    @abstractmethod",
            "    def skip_api(self):",
            "        \"\"\"Whether this component should be skipped from the api return value\"\"\"",
            "",
            "    @classmethod",
            "    def has_event(cls, event: str | EventListener) -> bool:",
            "        return event in cls.EVENTS",
            "",
            "    @classmethod",
            "    def get_component_class_id(cls) -> str:",
            "        module_name = cls.__module__",
            "        module_path = sys.modules[module_name].__file__",
            "        module_hash = hashlib.md5(f\"{cls.__name__}_{module_path}\".encode()).hexdigest()",
            "        return module_hash",
            "",
            "",
            "def server(fn):",
            "    fn._is_server_fn = True",
            "    return fn",
            "",
            "",
            "class Component(ComponentBase, Block):",
            "    \"\"\"",
            "    A base class for defining methods that all input/output components should have.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        value: Any = None,",
            "        *,",
            "        label: str | None = None,",
            "        info: str | None = None,",
            "        show_label: bool | None = None,",
            "        container: bool = True,",
            "        scale: int | None = None,",
            "        min_width: int | None = None,",
            "        interactive: bool | None = None,",
            "        visible: bool = True,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        render: bool = True,",
            "        load_fn: Callable | None = None,",
            "        every: float | None = None,",
            "    ):",
            "        self.server_fns = [",
            "            value",
            "            for value in self.__class__.__dict__.values()",
            "            if callable(value) and getattr(value, \"_is_server_fn\", False)",
            "        ]",
            "",
            "        # Svelte components expect elem_classes to be a list",
            "        # If we don't do this, returning a new component for an",
            "        # update will break the frontend",
            "        if not elem_classes:",
            "            elem_classes = []",
            "",
            "        # This gets overridden when `select` is called",
            "        self._selectable = False",
            "        if not hasattr(self, \"data_model\"):",
            "            self.data_model: type[GradioDataModel] | None = None",
            "",
            "        Block.__init__(",
            "            self,",
            "            elem_id=elem_id,",
            "            elem_classes=elem_classes,",
            "            visible=visible,",
            "            render=render,",
            "        )",
            "        if isinstance(self, StreamingInput):",
            "            self.check_streamable()",
            "",
            "        self.label = label",
            "        self.info = info",
            "        if not container:",
            "            if show_label:",
            "                warnings.warn(\"show_label has no effect when container is False.\")",
            "            show_label = False",
            "        if show_label is None:",
            "            show_label = True",
            "        self.show_label = show_label",
            "        self.container = container",
            "        if scale is not None and scale != round(scale):",
            "            warnings.warn(",
            "                f\"'scale' value should be an integer. Using {scale} will cause issues.\"",
            "            )",
            "        self.scale = scale",
            "        self.min_width = min_width",
            "        self.interactive = interactive",
            "",
            "        # load_event is set in the Blocks.attach_load_events method",
            "        self.load_event: None | dict[str, Any] = None",
            "        self.load_event_to_attach: None | tuple[Callable, float | None] = None",
            "        load_fn, initial_value = self.get_load_fn_and_initial_value(value)",
            "        initial_value = self.postprocess(initial_value)",
            "        self.value = move_files_to_cache(initial_value, self, postprocess=True)  # type: ignore",
            "",
            "        if callable(load_fn):",
            "            self.attach_load_event(load_fn, every)",
            "",
            "        self.component_class_id = self.__class__.get_component_class_id()",
            "",
            "    TEMPLATE_DIR = \"./templates/\"",
            "    FRONTEND_DIR = \"../../frontend/\"",
            "",
            "    def get_config(self):",
            "        config = super().get_config()",
            "        if self.info:",
            "            config[\"info\"] = self.info",
            "        if len(self.server_fns):",
            "            config[\"server_fns\"] = [fn.__name__ for fn in self.server_fns]",
            "        config.pop(\"render\", None)",
            "        return config",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return False",
            "",
            "    @staticmethod",
            "    def get_load_fn_and_initial_value(value):",
            "        if callable(value):",
            "            initial_value = value()",
            "            load_fn = value",
            "        else:",
            "            initial_value = value",
            "            load_fn = None",
            "        return load_fn, initial_value",
            "",
            "    def __str__(self):",
            "        return self.__repr__()",
            "",
            "    def __repr__(self):",
            "        return f\"{self.get_block_name()}\"",
            "",
            "    def attach_load_event(self, callable: Callable, every: float | None):",
            "        \"\"\"Add a load event that runs `callable`, optionally every `every` seconds.\"\"\"",
            "        self.load_event_to_attach = (callable, every)",
            "",
            "    def process_example(self, value):",
            "        \"\"\"",
            "        Process the input data in a way that can be displayed by the examples dataset component in the front-end.",
            "        By default, this calls the `.postprocess()` method of the component. However, if the `.postprocess()` method is",
            "        computationally intensive, or returns a large payload, a custom implementation may be appropriate.",
            "",
            "        For example,  the `process_example()` method of the `gr.Audio()` component only returns the name of the file, not",
            "        the processed audio file. The `.process_example()` method of the `gr.Dataframe()` returns the head of a dataframe",
            "        instead of the full dataframe.",
            "",
            "        The return value of this method must be json-serializable to put in the config.",
            "        \"\"\"",
            "        return self.postprocess(value)",
            "",
            "    def as_example(self, value):",
            "        \"\"\"Deprecated and replaced by `process_example()`.\"\"\"",
            "        return self.process_example(value)",
            "",
            "    def api_info(self) -> dict[str, Any]:",
            "        \"\"\"",
            "        The typing information for this component as a dictionary whose values are a list of 2 strings: [Python type, language-agnostic description].",
            "        Keys of the dictionary are: raw_input, raw_output, serialized_input, serialized_output",
            "        \"\"\"",
            "        if self.data_model is not None:",
            "            return self.data_model.model_json_schema()",
            "        raise NotImplementedError(",
            "            f\"The api_info method has not been implemented for {self.get_block_name()}\"",
            "        )",
            "",
            "    def flag(self, payload: Any, flag_dir: str | Path = \"\") -> str:",
            "        \"\"\"",
            "        Write the component's value to a format that can be stored in a csv or jsonl format for flagging.",
            "        \"\"\"",
            "        if self.data_model:",
            "            payload = self.data_model.from_json(payload)",
            "            Path(flag_dir).mkdir(exist_ok=True)",
            "            return payload.copy_to_dir(flag_dir).model_dump_json()",
            "        return payload",
            "",
            "    def read_from_flag(self, payload: Any):",
            "        \"\"\"",
            "        Convert the data from the csv or jsonl file into the component state.",
            "        \"\"\"",
            "        if self.data_model:",
            "            return self.data_model.from_json(json.loads(payload))",
            "        return payload",
            "",
            "",
            "class FormComponent(Component):",
            "    def get_expected_parent(self) -> type[Form] | None:",
            "        if getattr(self, \"container\", None) is False:",
            "            return None",
            "        return Form",
            "",
            "    def preprocess(self, payload: Any) -> Any:",
            "        return payload",
            "",
            "    def postprocess(self, value):",
            "        return value",
            "",
            "",
            "class StreamingOutput(metaclass=abc.ABCMeta):",
            "    def __init__(self, *args, **kwargs) -> None:",
            "        super().__init__(*args, **kwargs)",
            "        self.streaming: bool",
            "",
            "    @abc.abstractmethod",
            "    def stream_output(",
            "        self, value, output_id: str, first_chunk: bool",
            "    ) -> tuple[bytes, Any]:",
            "        pass",
            "",
            "",
            "class StreamingInput(metaclass=abc.ABCMeta):",
            "    def __init__(self, *args, **kwargs) -> None:",
            "        super().__init__(*args, **kwargs)",
            "",
            "    @abc.abstractmethod",
            "    def check_streamable(self):",
            "        \"\"\"Used to check if streaming is supported given the input.\"\"\"",
            "        pass",
            "",
            "",
            "def component(cls_name: str, render: bool) -> Component:",
            "    obj = utils.component_or_layout_class(cls_name)(render=render)",
            "    if isinstance(obj, BlockContext):",
            "        raise ValueError(f\"Invalid component: {obj.__class__}\")",
            "    assert isinstance(obj, Component)",
            "    return obj",
            "",
            "",
            "def get_component_instance(",
            "    comp: str | dict | Component, render: bool = False, unrender: bool = False",
            ") -> Component:",
            "    \"\"\"",
            "    Returns a component instance from a string, dict, or Component object.",
            "    Parameters:",
            "        comp: the component to instantiate. If a string, must be the name of a component, e.g. \"dropdown\". If a dict, must have a \"name\" key, e.g. {\"name\": \"dropdown\", \"choices\": [\"a\", \"b\"]}. If a Component object, will be returned as is.",
            "        render: whether to render the component. If True, renders the component (if not already rendered). If False, does not do anything.",
            "        unrender: whether to unrender the component. If True, unrenders the the component (if already rendered) -- this is useful when constructing an Interface or ChatInterface inside of a Blocks. If False, does not do anything.",
            "    \"\"\"",
            "    if isinstance(comp, str):",
            "        component_obj = component(comp, render=render)",
            "    elif isinstance(comp, dict):",
            "        name = comp.pop(\"name\")",
            "        component_cls = utils.component_or_layout_class(name)",
            "        component_obj = component_cls(**comp, render=render)",
            "        if isinstance(component_obj, BlockContext):",
            "            raise ValueError(f\"Invalid component: {name}\")",
            "    elif isinstance(comp, Component):",
            "        component_obj = comp",
            "    else:",
            "        raise ValueError(",
            "            f\"Component must provided as a `str` or `dict` or `Component` but is {comp}\"",
            "        )",
            "",
            "    if render and not component_obj.is_rendered:",
            "        component_obj.render()",
            "    elif unrender and component_obj.is_rendered:",
            "        component_obj.unrender()",
            "    assert isinstance(component_obj, Component)",
            "    return component_obj"
        ],
        "afterPatchFile": [
            "\"\"\"Contains all of the components that can be used with Gradio Interface / Blocks.",
            "Along with the docs for each component, you can find the names of example demos that use",
            "each component. These demos are located in the `demo` directory.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import abc",
            "import hashlib",
            "import json",
            "import sys",
            "import warnings",
            "from abc import ABC, abstractmethod",
            "from enum import Enum",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Any, Callable",
            "",
            "from gradio import utils",
            "from gradio.blocks import Block, BlockContext",
            "from gradio.component_meta import ComponentMeta",
            "from gradio.data_classes import GradioDataModel",
            "from gradio.events import EventListener",
            "from gradio.layouts import Form",
            "from gradio.processing_utils import move_files_to_cache",
            "",
            "if TYPE_CHECKING:",
            "    from typing import TypedDict",
            "",
            "    class DataframeData(TypedDict):",
            "        headers: list[str]",
            "        data: list[list[str | int | bool]]",
            "",
            "",
            "class _Keywords(Enum):",
            "    NO_VALUE = \"NO_VALUE\"  # Used as a sentinel to determine if nothing is provided as a argument for `value` in `Component.update()`",
            "    FINISHED_ITERATING = \"FINISHED_ITERATING\"  # Used to skip processing of a component's value (needed for generators + state)",
            "",
            "",
            "class ComponentBase(ABC, metaclass=ComponentMeta):",
            "    EVENTS: list[EventListener | str] = []",
            "",
            "    @abstractmethod",
            "    def preprocess(self, payload: Any) -> Any:",
            "        \"\"\"",
            "        Any preprocessing needed to be performed on function input.",
            "        Parameters:",
            "            payload: The input data received by the component from the frontend.",
            "        Returns:",
            "            The preprocessed input data sent to the user's function in the backend.",
            "        \"\"\"",
            "        return payload",
            "",
            "    @abstractmethod",
            "    def postprocess(self, value):",
            "        \"\"\"",
            "        Any postprocessing needed to be performed on function output.",
            "        Parameters:",
            "            value: The output data received by the component from the user's function in the backend.",
            "        Returns:",
            "            The postprocessed output data sent to the frontend.",
            "        \"\"\"",
            "        return value",
            "",
            "    @abstractmethod",
            "    def process_example(self, value):",
            "        \"\"\"",
            "        Process the input data in a way that can be displayed by the examples dataset component in the front-end.",
            "",
            "        For example, only return the name of a file as opposed to a full path. Or get the head of a dataframe.",
            "        The return value must be able to be json-serializable to put in the config.",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def api_info(self) -> dict[str, list[str]]:",
            "        \"\"\"",
            "        The typing information for this component as a dictionary whose values are a list of 2 strings: [Python type, language-agnostic description].",
            "        Keys of the dictionary are: raw_input, raw_output, serialized_input, serialized_output",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def example_inputs(self) -> Any:",
            "        \"\"\"",
            "        The example inputs for this component as a dictionary whose values are example inputs compatible with this component.",
            "        Keys of the dictionary are: raw, serialized",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def flag(self, payload: Any | GradioDataModel, flag_dir: str | Path = \"\") -> str:",
            "        \"\"\"",
            "        Write the component's value to a format that can be stored in a csv or jsonl format for flagging.",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def read_from_flag(self, payload: Any) -> GradioDataModel | Any:",
            "        \"\"\"",
            "        Convert the data from the csv or jsonl file into the component state.",
            "        \"\"\"",
            "        return payload",
            "",
            "    @property",
            "    @abstractmethod",
            "    def skip_api(self):",
            "        \"\"\"Whether this component should be skipped from the api return value\"\"\"",
            "",
            "    @classmethod",
            "    def has_event(cls, event: str | EventListener) -> bool:",
            "        return event in cls.EVENTS",
            "",
            "    @classmethod",
            "    def get_component_class_id(cls) -> str:",
            "        module_name = cls.__module__",
            "        module_path = sys.modules[module_name].__file__",
            "        module_hash = hashlib.md5(f\"{cls.__name__}_{module_path}\".encode()).hexdigest()",
            "        return module_hash",
            "",
            "",
            "def server(fn):",
            "    fn._is_server_fn = True",
            "    return fn",
            "",
            "",
            "class Component(ComponentBase, Block):",
            "    \"\"\"",
            "    A base class for defining methods that all input/output components should have.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        value: Any = None,",
            "        *,",
            "        label: str | None = None,",
            "        info: str | None = None,",
            "        show_label: bool | None = None,",
            "        container: bool = True,",
            "        scale: int | None = None,",
            "        min_width: int | None = None,",
            "        interactive: bool | None = None,",
            "        visible: bool = True,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        render: bool = True,",
            "        load_fn: Callable | None = None,",
            "        every: float | None = None,",
            "    ):",
            "        self.server_fns = [",
            "            value",
            "            for value in self.__class__.__dict__.values()",
            "            if callable(value) and getattr(value, \"_is_server_fn\", False)",
            "        ]",
            "",
            "        # Svelte components expect elem_classes to be a list",
            "        # If we don't do this, returning a new component for an",
            "        # update will break the frontend",
            "        if not elem_classes:",
            "            elem_classes = []",
            "",
            "        # This gets overridden when `select` is called",
            "        self._selectable = False",
            "        if not hasattr(self, \"data_model\"):",
            "            self.data_model: type[GradioDataModel] | None = None",
            "",
            "        Block.__init__(",
            "            self,",
            "            elem_id=elem_id,",
            "            elem_classes=elem_classes,",
            "            visible=visible,",
            "            render=render,",
            "        )",
            "        if isinstance(self, StreamingInput):",
            "            self.check_streamable()",
            "",
            "        self.label = label",
            "        self.info = info",
            "        if not container:",
            "            if show_label:",
            "                warnings.warn(\"show_label has no effect when container is False.\")",
            "            show_label = False",
            "        if show_label is None:",
            "            show_label = True",
            "        self.show_label = show_label",
            "        self.container = container",
            "        if scale is not None and scale != round(scale):",
            "            warnings.warn(",
            "                f\"'scale' value should be an integer. Using {scale} will cause issues.\"",
            "            )",
            "        self.scale = scale",
            "        self.min_width = min_width",
            "        self.interactive = interactive",
            "",
            "        # load_event is set in the Blocks.attach_load_events method",
            "        self.load_event: None | dict[str, Any] = None",
            "        self.load_event_to_attach: None | tuple[Callable, float | None] = None",
            "        load_fn, initial_value = self.get_load_fn_and_initial_value(value)",
            "        initial_value = self.postprocess(initial_value)",
            "        self.value = move_files_to_cache(",
            "            initial_value,",
            "            self,  # type: ignore",
            "            postprocess=True,",
            "            add_urls=True,",
            "        )",
            "",
            "        if callable(load_fn):",
            "            self.attach_load_event(load_fn, every)",
            "",
            "        self.component_class_id = self.__class__.get_component_class_id()",
            "",
            "    TEMPLATE_DIR = \"./templates/\"",
            "    FRONTEND_DIR = \"../../frontend/\"",
            "",
            "    def get_config(self):",
            "        config = super().get_config()",
            "        if self.info:",
            "            config[\"info\"] = self.info",
            "        if len(self.server_fns):",
            "            config[\"server_fns\"] = [fn.__name__ for fn in self.server_fns]",
            "        config.pop(\"render\", None)",
            "        return config",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return False",
            "",
            "    @staticmethod",
            "    def get_load_fn_and_initial_value(value):",
            "        if callable(value):",
            "            initial_value = value()",
            "            load_fn = value",
            "        else:",
            "            initial_value = value",
            "            load_fn = None",
            "        return load_fn, initial_value",
            "",
            "    def __str__(self):",
            "        return self.__repr__()",
            "",
            "    def __repr__(self):",
            "        return f\"{self.get_block_name()}\"",
            "",
            "    def attach_load_event(self, callable: Callable, every: float | None):",
            "        \"\"\"Add a load event that runs `callable`, optionally every `every` seconds.\"\"\"",
            "        self.load_event_to_attach = (callable, every)",
            "",
            "    def process_example(self, value):",
            "        \"\"\"",
            "        Process the input data in a way that can be displayed by the examples dataset component in the front-end.",
            "        By default, this calls the `.postprocess()` method of the component. However, if the `.postprocess()` method is",
            "        computationally intensive, or returns a large payload, a custom implementation may be appropriate.",
            "",
            "        For example,  the `process_example()` method of the `gr.Audio()` component only returns the name of the file, not",
            "        the processed audio file. The `.process_example()` method of the `gr.Dataframe()` returns the head of a dataframe",
            "        instead of the full dataframe.",
            "",
            "        The return value of this method must be json-serializable to put in the config.",
            "        \"\"\"",
            "        return self.postprocess(value)",
            "",
            "    def as_example(self, value):",
            "        \"\"\"Deprecated and replaced by `process_example()`.\"\"\"",
            "        return self.process_example(value)",
            "",
            "    def api_info(self) -> dict[str, Any]:",
            "        \"\"\"",
            "        The typing information for this component as a dictionary whose values are a list of 2 strings: [Python type, language-agnostic description].",
            "        Keys of the dictionary are: raw_input, raw_output, serialized_input, serialized_output",
            "        \"\"\"",
            "        if self.data_model is not None:",
            "            return self.data_model.model_json_schema()",
            "        raise NotImplementedError(",
            "            f\"The api_info method has not been implemented for {self.get_block_name()}\"",
            "        )",
            "",
            "    def flag(self, payload: Any, flag_dir: str | Path = \"\") -> str:",
            "        \"\"\"",
            "        Write the component's value to a format that can be stored in a csv or jsonl format for flagging.",
            "        \"\"\"",
            "        if self.data_model:",
            "            payload = self.data_model.from_json(payload)",
            "            Path(flag_dir).mkdir(exist_ok=True)",
            "            return payload.copy_to_dir(flag_dir).model_dump_json()",
            "        return payload",
            "",
            "    def read_from_flag(self, payload: Any):",
            "        \"\"\"",
            "        Convert the data from the csv or jsonl file into the component state.",
            "        \"\"\"",
            "        if self.data_model:",
            "            return self.data_model.from_json(json.loads(payload))",
            "        return payload",
            "",
            "",
            "class FormComponent(Component):",
            "    def get_expected_parent(self) -> type[Form] | None:",
            "        if getattr(self, \"container\", None) is False:",
            "            return None",
            "        return Form",
            "",
            "    def preprocess(self, payload: Any) -> Any:",
            "        return payload",
            "",
            "    def postprocess(self, value):",
            "        return value",
            "",
            "",
            "class StreamingOutput(metaclass=abc.ABCMeta):",
            "    def __init__(self, *args, **kwargs) -> None:",
            "        super().__init__(*args, **kwargs)",
            "        self.streaming: bool",
            "",
            "    @abc.abstractmethod",
            "    def stream_output(",
            "        self, value, output_id: str, first_chunk: bool",
            "    ) -> tuple[bytes, Any]:",
            "        pass",
            "",
            "",
            "class StreamingInput(metaclass=abc.ABCMeta):",
            "    def __init__(self, *args, **kwargs) -> None:",
            "        super().__init__(*args, **kwargs)",
            "",
            "    @abc.abstractmethod",
            "    def check_streamable(self):",
            "        \"\"\"Used to check if streaming is supported given the input.\"\"\"",
            "        pass",
            "",
            "",
            "def component(cls_name: str, render: bool) -> Component:",
            "    obj = utils.component_or_layout_class(cls_name)(render=render)",
            "    if isinstance(obj, BlockContext):",
            "        raise ValueError(f\"Invalid component: {obj.__class__}\")",
            "    assert isinstance(obj, Component)",
            "    return obj",
            "",
            "",
            "def get_component_instance(",
            "    comp: str | dict | Component, render: bool = False, unrender: bool = False",
            ") -> Component:",
            "    \"\"\"",
            "    Returns a component instance from a string, dict, or Component object.",
            "    Parameters:",
            "        comp: the component to instantiate. If a string, must be the name of a component, e.g. \"dropdown\". If a dict, must have a \"name\" key, e.g. {\"name\": \"dropdown\", \"choices\": [\"a\", \"b\"]}. If a Component object, will be returned as is.",
            "        render: whether to render the component. If True, renders the component (if not already rendered). If False, does not do anything.",
            "        unrender: whether to unrender the component. If True, unrenders the the component (if already rendered) -- this is useful when constructing an Interface or ChatInterface inside of a Blocks. If False, does not do anything.",
            "    \"\"\"",
            "    if isinstance(comp, str):",
            "        component_obj = component(comp, render=render)",
            "    elif isinstance(comp, dict):",
            "        name = comp.pop(\"name\")",
            "        component_cls = utils.component_or_layout_class(name)",
            "        component_obj = component_cls(**comp, render=render)",
            "        if isinstance(component_obj, BlockContext):",
            "            raise ValueError(f\"Invalid component: {name}\")",
            "    elif isinstance(comp, Component):",
            "        component_obj = comp",
            "    else:",
            "        raise ValueError(",
            "            f\"Component must provided as a `str` or `dict` or `Component` but is {comp}\"",
            "        )",
            "",
            "    if render and not component_obj.is_rendered:",
            "        component_obj.render()",
            "    elif unrender and component_obj.is_rendered:",
            "        component_obj.unrender()",
            "    assert isinstance(component_obj, Component)",
            "    return component_obj"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "198": [
                "Component",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "gradio/components/login_button.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 88,
                "PatchRowcode": "             request.request, \"session\", None"
            },
            "1": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "         )"
            },
            "2": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "         if session is None or \"oauth_info\" not in session:"
            },
            "3": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return LoginButton(value=self.value, interactive=True)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+            return LoginButton(value=self.value, interactive=True)  # type: ignore"
            },
            "5": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "         else:"
            },
            "6": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "             username = session[\"oauth_info\"][\"userinfo\"][\"preferred_username\"]"
            },
            "7": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "             logout_text = self.logout_value.format(username)"
            }
        },
        "frontPatchFile": [
            "\"\"\"Predefined button to sign in with Hugging Face in a Gradio Space.\"\"\"",
            "from __future__ import annotations",
            "",
            "import json",
            "import warnings",
            "from typing import Literal",
            "",
            "from gradio_client.documentation import document",
            "",
            "from gradio.components import Button",
            "from gradio.context import Context",
            "from gradio.routes import Request",
            "",
            "",
            "@document()",
            "class LoginButton(Button):",
            "    \"\"\"",
            "    Creates a button that redirects the user to Sign with Hugging Face using OAuth.",
            "    \"\"\"",
            "",
            "    is_template = True",
            "",
            "    def __init__(",
            "        self,",
            "        value: str = \"Sign in with Hugging Face\",",
            "        logout_value: str = \"Logout ({})\",",
            "        *,",
            "        every: float | None = None,",
            "        variant: Literal[\"primary\", \"secondary\", \"stop\"] = \"secondary\",",
            "        size: Literal[\"sm\", \"lg\"] | None = None,",
            "        icon: str",
            "        | None = \"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\",",
            "        link: str | None = None,",
            "        visible: bool = True,",
            "        interactive: bool = True,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        render: bool = True,",
            "        scale: int | None = 0,",
            "        min_width: int | None = None,",
            "        signed_in_value: str = \"Signed in as {}\",",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            logout_value: The text to display when the user is signed in. The string should contain a placeholder for the username with a call-to-action to logout, e.g. \"Logout ({})\".",
            "        \"\"\"",
            "        if signed_in_value != \"Signed in as {}\":",
            "            warnings.warn(",
            "                \"The `signed_in_value` parameter is deprecated. Please use `logout_value` instead.\"",
            "            )",
            "        self.logout_value = logout_value",
            "        super().__init__(",
            "            value,",
            "            every=every,",
            "            variant=variant,",
            "            size=size,",
            "            icon=icon,",
            "            link=link,",
            "            visible=visible,",
            "            interactive=interactive,",
            "            elem_id=elem_id,",
            "            elem_classes=elem_classes,",
            "            render=render,",
            "            scale=scale,",
            "            min_width=min_width,",
            "        )",
            "        if Context.root_block:",
            "            self.activate()",
            "        else:",
            "            warnings.warn(",
            "                \"LoginButton created outside of a Blocks context. May not work unless you call its `activate()` method manually.\"",
            "            )",
            "",
            "    def activate(self):",
            "        # Taken from https://cmgdo.com/external-link-in-gradio-button/",
            "        # Taking `self` as input to check if user is logged in",
            "        # ('self' value will be either \"Sign in with Hugging Face\" or \"Signed in as ...\")",
            "        _js = _js_handle_redirect.replace(",
            "            \"BUTTON_DEFAULT_VALUE\", json.dumps(self.value)",
            "        )",
            "        self.click(fn=None, inputs=[self], outputs=None, js=_js)",
            "",
            "        self.attach_load_event(self._check_login_status, None)",
            "",
            "    def _check_login_status(self, request: Request) -> LoginButton:",
            "        # Each time the page is refreshed or loaded, check if the user is logged in and adapt label",
            "        session = getattr(request, \"session\", None) or getattr(",
            "            request.request, \"session\", None",
            "        )",
            "        if session is None or \"oauth_info\" not in session:",
            "            return LoginButton(value=self.value, interactive=True)",
            "        else:",
            "            username = session[\"oauth_info\"][\"userinfo\"][\"preferred_username\"]",
            "            logout_text = self.logout_value.format(username)",
            "            return LoginButton(logout_text, interactive=True)",
            "",
            "",
            "# JS code to redirects to /login/huggingface if user is not logged in.",
            "# If the app is opened in an iframe, open the login page in a new tab.",
            "# Otherwise, redirects locally. Taken from https://stackoverflow.com/a/61596084.",
            "# If user is logged in, redirect to logout page (always in-place).",
            "_js_handle_redirect = \"\"\"",
            "(buttonValue) => {",
            "    if (buttonValue === BUTTON_DEFAULT_VALUE) {",
            "        url = '/login/huggingface' + window.location.search;",
            "        if ( window !== window.parent ) {",
            "            window.open(url, '_blank');",
            "        } else {",
            "            window.location.assign(url);",
            "        }",
            "    } else {",
            "        url = '/logout' + window.location.search",
            "        window.location.assign(url);",
            "    }",
            "}",
            "\"\"\""
        ],
        "afterPatchFile": [
            "\"\"\"Predefined button to sign in with Hugging Face in a Gradio Space.\"\"\"",
            "from __future__ import annotations",
            "",
            "import json",
            "import warnings",
            "from typing import Literal",
            "",
            "from gradio_client.documentation import document",
            "",
            "from gradio.components import Button",
            "from gradio.context import Context",
            "from gradio.routes import Request",
            "",
            "",
            "@document()",
            "class LoginButton(Button):",
            "    \"\"\"",
            "    Creates a button that redirects the user to Sign with Hugging Face using OAuth.",
            "    \"\"\"",
            "",
            "    is_template = True",
            "",
            "    def __init__(",
            "        self,",
            "        value: str = \"Sign in with Hugging Face\",",
            "        logout_value: str = \"Logout ({})\",",
            "        *,",
            "        every: float | None = None,",
            "        variant: Literal[\"primary\", \"secondary\", \"stop\"] = \"secondary\",",
            "        size: Literal[\"sm\", \"lg\"] | None = None,",
            "        icon: str",
            "        | None = \"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\",",
            "        link: str | None = None,",
            "        visible: bool = True,",
            "        interactive: bool = True,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        render: bool = True,",
            "        scale: int | None = 0,",
            "        min_width: int | None = None,",
            "        signed_in_value: str = \"Signed in as {}\",",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            logout_value: The text to display when the user is signed in. The string should contain a placeholder for the username with a call-to-action to logout, e.g. \"Logout ({})\".",
            "        \"\"\"",
            "        if signed_in_value != \"Signed in as {}\":",
            "            warnings.warn(",
            "                \"The `signed_in_value` parameter is deprecated. Please use `logout_value` instead.\"",
            "            )",
            "        self.logout_value = logout_value",
            "        super().__init__(",
            "            value,",
            "            every=every,",
            "            variant=variant,",
            "            size=size,",
            "            icon=icon,",
            "            link=link,",
            "            visible=visible,",
            "            interactive=interactive,",
            "            elem_id=elem_id,",
            "            elem_classes=elem_classes,",
            "            render=render,",
            "            scale=scale,",
            "            min_width=min_width,",
            "        )",
            "        if Context.root_block:",
            "            self.activate()",
            "        else:",
            "            warnings.warn(",
            "                \"LoginButton created outside of a Blocks context. May not work unless you call its `activate()` method manually.\"",
            "            )",
            "",
            "    def activate(self):",
            "        # Taken from https://cmgdo.com/external-link-in-gradio-button/",
            "        # Taking `self` as input to check if user is logged in",
            "        # ('self' value will be either \"Sign in with Hugging Face\" or \"Signed in as ...\")",
            "        _js = _js_handle_redirect.replace(",
            "            \"BUTTON_DEFAULT_VALUE\", json.dumps(self.value)",
            "        )",
            "        self.click(fn=None, inputs=[self], outputs=None, js=_js)",
            "",
            "        self.attach_load_event(self._check_login_status, None)",
            "",
            "    def _check_login_status(self, request: Request) -> LoginButton:",
            "        # Each time the page is refreshed or loaded, check if the user is logged in and adapt label",
            "        session = getattr(request, \"session\", None) or getattr(",
            "            request.request, \"session\", None",
            "        )",
            "        if session is None or \"oauth_info\" not in session:",
            "            return LoginButton(value=self.value, interactive=True)  # type: ignore",
            "        else:",
            "            username = session[\"oauth_info\"][\"userinfo\"][\"preferred_username\"]",
            "            logout_text = self.logout_value.format(username)",
            "            return LoginButton(logout_text, interactive=True)",
            "",
            "",
            "# JS code to redirects to /login/huggingface if user is not logged in.",
            "# If the app is opened in an iframe, open the login page in a new tab.",
            "# Otherwise, redirects locally. Taken from https://stackoverflow.com/a/61596084.",
            "# If user is logged in, redirect to logout page (always in-place).",
            "_js_handle_redirect = \"\"\"",
            "(buttonValue) => {",
            "    if (buttonValue === BUTTON_DEFAULT_VALUE) {",
            "        url = '/login/huggingface' + window.location.search;",
            "        if ( window !== window.parent ) {",
            "            window.open(url, '_blank');",
            "        } else {",
            "            window.location.assign(url);",
            "        }",
            "    } else {",
            "        url = '/logout' + window.location.search",
            "        window.location.assign(url);",
            "    }",
            "}",
            "\"\"\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "91": [
                "LoginButton",
                "_check_login_status"
            ]
        },
        "addLocation": []
    },
    "gradio/data_classes.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": 168,
                "PatchRowcode": "     size: Optional[int] = None  # size in bytes"
            },
            "1": {
                "beforePatchRowNumber": 169,
                "afterPatchRowNumber": 169,
                "PatchRowcode": "     orig_name: Optional[str] = None  # original filename"
            },
            "2": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 170,
                "PatchRowcode": "     mime_type: Optional[str] = None"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+    is_stream: bool = False"
            },
            "4": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": 172,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 173,
                "PatchRowcode": "     @property"
            },
            "6": {
                "beforePatchRowNumber": 173,
                "afterPatchRowNumber": 174,
                "PatchRowcode": "     def is_none(self):"
            }
        },
        "frontPatchFile": [
            "\"\"\"Pydantic data models and other dataclasses. This is the only file that uses Optional[]",
            "typing syntax instead of | None syntax to work with pydantic\"\"\"",
            "from __future__ import annotations",
            "",
            "import pathlib",
            "import secrets",
            "import shutil",
            "from abc import ABC, abstractmethod",
            "from enum import Enum, auto",
            "from typing import Any, List, Optional, Union",
            "",
            "from fastapi import Request",
            "from gradio_client.utils import traverse",
            "from typing_extensions import Literal",
            "",
            "from . import wasm_utils",
            "",
            "if not wasm_utils.IS_WASM:",
            "    from pydantic import BaseModel, RootModel, ValidationError  # type: ignore",
            "else:",
            "    # XXX: Currently Pyodide V2 is not available on Pyodide,",
            "    # so we install V1 for the Wasm version.",
            "    from typing import Generic, TypeVar",
            "",
            "    from pydantic import BaseModel as BaseModelV1",
            "    from pydantic import ValidationError, schema_of",
            "",
            "    # Map V2 method calls to V1 implementations.",
            "    # Ref: https://docs.pydantic.dev/latest/migration/#changes-to-pydanticbasemodel",
            "    class BaseModel(BaseModelV1):",
            "        pass",
            "",
            "    BaseModel.model_dump = BaseModel.dict  # type: ignore",
            "    BaseModel.model_json_schema = BaseModel.schema  # type: ignore",
            "",
            "    # RootModel is not available in V1, so we create a dummy class.",
            "    PydanticUndefined = object()",
            "    RootModelRootType = TypeVar(\"RootModelRootType\")",
            "",
            "    class RootModel(BaseModel, Generic[RootModelRootType]):",
            "        root: RootModelRootType",
            "",
            "        def __init__(self, root: RootModelRootType = PydanticUndefined, **data):",
            "            if data:",
            "                if root is not PydanticUndefined:",
            "                    raise ValueError(",
            "                        '\"RootModel.__init__\" accepts either a single positional argument or arbitrary keyword arguments'",
            "                    )",
            "                root = data  # type: ignore",
            "            # XXX: No runtime validation is executed.",
            "            super().__init__(root=root)  # type: ignore",
            "",
            "        def dict(self, **kwargs):",
            "            return super().dict(**kwargs)[\"root\"]",
            "",
            "        @classmethod",
            "        def schema(cls, **_kwargs):",
            "            # XXX: kwargs are ignored.",
            "            return schema_of(cls.__fields__[\"root\"].type_)  # type: ignore",
            "",
            "    RootModel.model_dump = RootModel.dict  # type: ignore",
            "    RootModel.model_json_schema = RootModel.schema  # type: ignore",
            "",
            "",
            "class PredictBody(BaseModel):",
            "    model_config = {\"arbitrary_types_allowed\": True}",
            "",
            "    session_hash: Optional[str] = None",
            "    event_id: Optional[str] = None",
            "    data: List[Any]",
            "    event_data: Optional[Any] = None",
            "    fn_index: Optional[int] = None",
            "    trigger_id: Optional[int] = None",
            "    batched: Optional[",
            "        bool",
            "    ] = False  # Whether the data is a batch of samples (i.e. called from the queue if batch=True) or a single sample (i.e. called from the UI)",
            "    request: Optional[",
            "        Request",
            "    ] = None  # dictionary of request headers, query parameters, url, etc. (used to to pass in request for queuing)",
            "",
            "",
            "class ResetBody(BaseModel):",
            "    event_id: str",
            "",
            "",
            "class ComponentServerBody(BaseModel):",
            "    session_hash: str",
            "    component_id: int",
            "    fn_name: str",
            "    data: Any",
            "",
            "",
            "class InterfaceTypes(Enum):",
            "    STANDARD = auto()",
            "    INPUT_ONLY = auto()",
            "    OUTPUT_ONLY = auto()",
            "    UNIFIED = auto()",
            "",
            "",
            "class Estimation(BaseModel):",
            "    rank: Optional[int] = None",
            "    queue_size: int",
            "    rank_eta: Optional[float] = None",
            "",
            "",
            "class ProgressUnit(BaseModel):",
            "    index: Optional[int] = None",
            "    length: Optional[int] = None",
            "    unit: Optional[str] = None",
            "    progress: Optional[float] = None",
            "    desc: Optional[str] = None",
            "",
            "",
            "class Progress(BaseModel):",
            "    progress_data: List[ProgressUnit] = []",
            "",
            "",
            "class LogMessage(BaseModel):",
            "    log: str",
            "    level: Literal[\"info\", \"warning\"]",
            "",
            "",
            "class GradioBaseModel(ABC):",
            "    def copy_to_dir(self, dir: str | pathlib.Path) -> GradioDataModel:",
            "        assert isinstance(self, (BaseModel, RootModel))",
            "        if isinstance(dir, str):",
            "            dir = pathlib.Path(dir)",
            "",
            "        # TODO: Making sure path is unique should be done in caller",
            "        def unique_copy(obj: dict):",
            "            data = FileData(**obj)",
            "            return data._copy_to_dir(",
            "                str(pathlib.Path(dir / secrets.token_hex(10)))",
            "            ).model_dump()",
            "",
            "        return self.__class__.from_json(",
            "            x=traverse(",
            "                self.model_dump(),",
            "                unique_copy,",
            "                FileData.is_file_data,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    @abstractmethod",
            "    def from_json(cls, x) -> GradioDataModel:",
            "        pass",
            "",
            "",
            "class GradioModel(GradioBaseModel, BaseModel):",
            "    @classmethod",
            "    def from_json(cls, x) -> GradioModel:",
            "        return cls(**x)",
            "",
            "",
            "class GradioRootModel(GradioBaseModel, RootModel):",
            "    @classmethod",
            "    def from_json(cls, x) -> GradioRootModel:",
            "        return cls(root=x)",
            "",
            "",
            "GradioDataModel = Union[GradioModel, GradioRootModel]",
            "",
            "",
            "class FileData(GradioModel):",
            "    path: str  # server filepath",
            "    url: Optional[str] = None  # normalised server url",
            "    size: Optional[int] = None  # size in bytes",
            "    orig_name: Optional[str] = None  # original filename",
            "    mime_type: Optional[str] = None",
            "",
            "    @property",
            "    def is_none(self):",
            "        return all(",
            "            f is None",
            "            for f in [",
            "                self.path,",
            "                self.url,",
            "                self.size,",
            "                self.orig_name,",
            "                self.mime_type,",
            "            ]",
            "        )",
            "",
            "    @classmethod",
            "    def from_path(cls, path: str) -> FileData:",
            "        return cls(path=path)",
            "",
            "    def _copy_to_dir(self, dir: str) -> FileData:",
            "        pathlib.Path(dir).mkdir(exist_ok=True)",
            "        new_obj = dict(self)",
            "",
            "        assert self.path",
            "        new_name = shutil.copy(self.path, dir)",
            "        new_obj[\"path\"] = new_name",
            "        return self.__class__(**new_obj)",
            "",
            "    @classmethod",
            "    def is_file_data(cls, obj: Any):",
            "        if isinstance(obj, dict):",
            "            try:",
            "                return not FileData(**obj).is_none",
            "            except (TypeError, ValidationError):",
            "                return False",
            "        return False",
            "",
            "",
            "class ListFiles(GradioRootModel):",
            "    root: List[FileData]",
            "",
            "    def __getitem__(self, index):",
            "        return self.root[index]",
            "",
            "    def __iter__(self):",
            "        return iter(self.root)"
        ],
        "afterPatchFile": [
            "\"\"\"Pydantic data models and other dataclasses. This is the only file that uses Optional[]",
            "typing syntax instead of | None syntax to work with pydantic\"\"\"",
            "from __future__ import annotations",
            "",
            "import pathlib",
            "import secrets",
            "import shutil",
            "from abc import ABC, abstractmethod",
            "from enum import Enum, auto",
            "from typing import Any, List, Optional, Union",
            "",
            "from fastapi import Request",
            "from gradio_client.utils import traverse",
            "from typing_extensions import Literal",
            "",
            "from . import wasm_utils",
            "",
            "if not wasm_utils.IS_WASM:",
            "    from pydantic import BaseModel, RootModel, ValidationError  # type: ignore",
            "else:",
            "    # XXX: Currently Pyodide V2 is not available on Pyodide,",
            "    # so we install V1 for the Wasm version.",
            "    from typing import Generic, TypeVar",
            "",
            "    from pydantic import BaseModel as BaseModelV1",
            "    from pydantic import ValidationError, schema_of",
            "",
            "    # Map V2 method calls to V1 implementations.",
            "    # Ref: https://docs.pydantic.dev/latest/migration/#changes-to-pydanticbasemodel",
            "    class BaseModel(BaseModelV1):",
            "        pass",
            "",
            "    BaseModel.model_dump = BaseModel.dict  # type: ignore",
            "    BaseModel.model_json_schema = BaseModel.schema  # type: ignore",
            "",
            "    # RootModel is not available in V1, so we create a dummy class.",
            "    PydanticUndefined = object()",
            "    RootModelRootType = TypeVar(\"RootModelRootType\")",
            "",
            "    class RootModel(BaseModel, Generic[RootModelRootType]):",
            "        root: RootModelRootType",
            "",
            "        def __init__(self, root: RootModelRootType = PydanticUndefined, **data):",
            "            if data:",
            "                if root is not PydanticUndefined:",
            "                    raise ValueError(",
            "                        '\"RootModel.__init__\" accepts either a single positional argument or arbitrary keyword arguments'",
            "                    )",
            "                root = data  # type: ignore",
            "            # XXX: No runtime validation is executed.",
            "            super().__init__(root=root)  # type: ignore",
            "",
            "        def dict(self, **kwargs):",
            "            return super().dict(**kwargs)[\"root\"]",
            "",
            "        @classmethod",
            "        def schema(cls, **_kwargs):",
            "            # XXX: kwargs are ignored.",
            "            return schema_of(cls.__fields__[\"root\"].type_)  # type: ignore",
            "",
            "    RootModel.model_dump = RootModel.dict  # type: ignore",
            "    RootModel.model_json_schema = RootModel.schema  # type: ignore",
            "",
            "",
            "class PredictBody(BaseModel):",
            "    model_config = {\"arbitrary_types_allowed\": True}",
            "",
            "    session_hash: Optional[str] = None",
            "    event_id: Optional[str] = None",
            "    data: List[Any]",
            "    event_data: Optional[Any] = None",
            "    fn_index: Optional[int] = None",
            "    trigger_id: Optional[int] = None",
            "    batched: Optional[",
            "        bool",
            "    ] = False  # Whether the data is a batch of samples (i.e. called from the queue if batch=True) or a single sample (i.e. called from the UI)",
            "    request: Optional[",
            "        Request",
            "    ] = None  # dictionary of request headers, query parameters, url, etc. (used to to pass in request for queuing)",
            "",
            "",
            "class ResetBody(BaseModel):",
            "    event_id: str",
            "",
            "",
            "class ComponentServerBody(BaseModel):",
            "    session_hash: str",
            "    component_id: int",
            "    fn_name: str",
            "    data: Any",
            "",
            "",
            "class InterfaceTypes(Enum):",
            "    STANDARD = auto()",
            "    INPUT_ONLY = auto()",
            "    OUTPUT_ONLY = auto()",
            "    UNIFIED = auto()",
            "",
            "",
            "class Estimation(BaseModel):",
            "    rank: Optional[int] = None",
            "    queue_size: int",
            "    rank_eta: Optional[float] = None",
            "",
            "",
            "class ProgressUnit(BaseModel):",
            "    index: Optional[int] = None",
            "    length: Optional[int] = None",
            "    unit: Optional[str] = None",
            "    progress: Optional[float] = None",
            "    desc: Optional[str] = None",
            "",
            "",
            "class Progress(BaseModel):",
            "    progress_data: List[ProgressUnit] = []",
            "",
            "",
            "class LogMessage(BaseModel):",
            "    log: str",
            "    level: Literal[\"info\", \"warning\"]",
            "",
            "",
            "class GradioBaseModel(ABC):",
            "    def copy_to_dir(self, dir: str | pathlib.Path) -> GradioDataModel:",
            "        assert isinstance(self, (BaseModel, RootModel))",
            "        if isinstance(dir, str):",
            "            dir = pathlib.Path(dir)",
            "",
            "        # TODO: Making sure path is unique should be done in caller",
            "        def unique_copy(obj: dict):",
            "            data = FileData(**obj)",
            "            return data._copy_to_dir(",
            "                str(pathlib.Path(dir / secrets.token_hex(10)))",
            "            ).model_dump()",
            "",
            "        return self.__class__.from_json(",
            "            x=traverse(",
            "                self.model_dump(),",
            "                unique_copy,",
            "                FileData.is_file_data,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    @abstractmethod",
            "    def from_json(cls, x) -> GradioDataModel:",
            "        pass",
            "",
            "",
            "class GradioModel(GradioBaseModel, BaseModel):",
            "    @classmethod",
            "    def from_json(cls, x) -> GradioModel:",
            "        return cls(**x)",
            "",
            "",
            "class GradioRootModel(GradioBaseModel, RootModel):",
            "    @classmethod",
            "    def from_json(cls, x) -> GradioRootModel:",
            "        return cls(root=x)",
            "",
            "",
            "GradioDataModel = Union[GradioModel, GradioRootModel]",
            "",
            "",
            "class FileData(GradioModel):",
            "    path: str  # server filepath",
            "    url: Optional[str] = None  # normalised server url",
            "    size: Optional[int] = None  # size in bytes",
            "    orig_name: Optional[str] = None  # original filename",
            "    mime_type: Optional[str] = None",
            "    is_stream: bool = False",
            "",
            "    @property",
            "    def is_none(self):",
            "        return all(",
            "            f is None",
            "            for f in [",
            "                self.path,",
            "                self.url,",
            "                self.size,",
            "                self.orig_name,",
            "                self.mime_type,",
            "            ]",
            "        )",
            "",
            "    @classmethod",
            "    def from_path(cls, path: str) -> FileData:",
            "        return cls(path=path)",
            "",
            "    def _copy_to_dir(self, dir: str) -> FileData:",
            "        pathlib.Path(dir).mkdir(exist_ok=True)",
            "        new_obj = dict(self)",
            "",
            "        assert self.path",
            "        new_name = shutil.copy(self.path, dir)",
            "        new_obj[\"path\"] = new_name",
            "        return self.__class__(**new_obj)",
            "",
            "    @classmethod",
            "    def is_file_data(cls, obj: Any):",
            "        if isinstance(obj, dict):",
            "            try:",
            "                return not FileData(**obj).is_none",
            "            except (TypeError, ValidationError):",
            "                return False",
            "        return False",
            "",
            "",
            "class ListFiles(GradioRootModel):",
            "    root: List[FileData]",
            "",
            "    def __getitem__(self, index):",
            "        return self.root[index]",
            "",
            "    def __iter__(self):",
            "        return iter(self.root)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "gradio.data_classes.FileData.self",
            "gradio.data_classes.FileData.from_path"
        ]
    },
    "gradio/processing_utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": 236,
                "PatchRowcode": "     return block.move_resource_to_block_cache(url_or_file_path)"
            },
            "1": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": 237,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": 238,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def move_files_to_cache(data: Any, block: Component, postprocess: bool = False):"
            },
            "4": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\"Move files to cache and replace the file path with the cache path."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 239,
                "PatchRowcode": "+def move_files_to_cache("
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 240,
                "PatchRowcode": "+    data: Any,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 241,
                "PatchRowcode": "+    block: Component,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 242,
                "PatchRowcode": "+    postprocess: bool = False,"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 243,
                "PatchRowcode": "+    add_urls=False,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 244,
                "PatchRowcode": "+) -> dict:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 245,
                "PatchRowcode": "+    \"\"\"Move any files in `data` to cache and (optionally), adds URL prefixes (/file=...) needed to access the cached file."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 246,
                "PatchRowcode": "+    Also handles the case where the file is on an external Gradio app (/proxy=...)."
            },
            "13": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 247,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Runs after .postprocess(), after .process_example(), and before .preprocess()."
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+    Runs after .postprocess() and before .preprocess()."
            },
            "16": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 249,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 250,
                "PatchRowcode": "     Args:"
            },
            "18": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": 251,
                "PatchRowcode": "         data: The input or output data for a component. Can be a dictionary or a dataclass"
            },
            "19": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        block: The component"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 252,
                "PatchRowcode": "+        block: The component whose data is being processed"
            },
            "21": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 253,
                "PatchRowcode": "         postprocess: Whether its running from postprocessing"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 254,
                "PatchRowcode": "+        root_url: The root URL of the local server, if applicable"
            },
            "23": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "     \"\"\""
            },
            "24": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 256,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": 257,
                "PatchRowcode": "     def _move_to_cache(d: dict):"
            },
            "26": {
                "beforePatchRowNumber": 259,
                "afterPatchRowNumber": 266,
                "PatchRowcode": "             temp_file_path = move_resource_to_block_cache(payload.path, block)"
            },
            "27": {
                "beforePatchRowNumber": 260,
                "afterPatchRowNumber": 267,
                "PatchRowcode": "         assert temp_file_path is not None"
            },
            "28": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "         payload.path = temp_file_path"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 269,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 270,
                "PatchRowcode": "+        if add_urls:"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 271,
                "PatchRowcode": "+            url_prefix = \"/stream/\" if payload.is_stream else \"/file=\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 272,
                "PatchRowcode": "+            if block.proxy_url:"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 273,
                "PatchRowcode": "+                url = f\"/proxy={block.proxy_url}{url_prefix}{temp_file_path}\""
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 274,
                "PatchRowcode": "+            elif client_utils.is_http_url_like("
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 275,
                "PatchRowcode": "+                temp_file_path"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 276,
                "PatchRowcode": "+            ) or temp_file_path.startswith(f\"{url_prefix}\"):"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 277,
                "PatchRowcode": "+                url = temp_file_path"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 278,
                "PatchRowcode": "+            else:"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 279,
                "PatchRowcode": "+                url = f\"{url_prefix}{temp_file_path}\""
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 280,
                "PatchRowcode": "+            payload.url = url"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 281,
                "PatchRowcode": "+"
            },
            "42": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 282,
                "PatchRowcode": "         return payload.model_dump()"
            },
            "43": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 283,
                "PatchRowcode": " "
            },
            "44": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 284,
                "PatchRowcode": "     if isinstance(data, (GradioRootModel, GradioModel)):"
            },
            "45": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 287,
                "PatchRowcode": "     return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)"
            },
            "46": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 288,
                "PatchRowcode": " "
            },
            "47": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": 289,
                "PatchRowcode": " "
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 290,
                "PatchRowcode": "+def add_root_url(data, root_url) -> dict:"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 291,
                "PatchRowcode": "+    def _add_root_url(file_dict: dict):"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 292,
                "PatchRowcode": "+        if not client_utils.is_http_url_like(file_dict[\"url\"]):"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 293,
                "PatchRowcode": "+            file_dict[\"url\"] = f'{root_url}{file_dict[\"url\"]}'"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 294,
                "PatchRowcode": "+        return file_dict"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 295,
                "PatchRowcode": "+"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 296,
                "PatchRowcode": "+    return client_utils.traverse(data, _add_root_url, client_utils.is_file_obj_with_url)"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 297,
                "PatchRowcode": "+"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 298,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": 270,
                "afterPatchRowNumber": 299,
                "PatchRowcode": " def resize_and_crop(img, size, crop_type=\"center\"):"
            },
            "58": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 300,
                "PatchRowcode": "     \"\"\""
            },
            "59": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "     Resize and crop an image to fit the specified size."
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import base64",
            "import hashlib",
            "import json",
            "import logging",
            "import os",
            "import shutil",
            "import subprocess",
            "import tempfile",
            "import warnings",
            "from io import BytesIO",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Any, Literal",
            "",
            "import httpx",
            "import numpy as np",
            "from gradio_client import utils as client_utils",
            "from PIL import Image, ImageOps, PngImagePlugin",
            "",
            "from gradio import wasm_utils",
            "from gradio.data_classes import FileData, GradioModel, GradioRootModel",
            "from gradio.utils import abspath",
            "",
            "with warnings.catch_warnings():",
            "    warnings.simplefilter(\"ignore\")  # Ignore pydub warning if ffmpeg is not installed",
            "    from pydub import AudioSegment",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.components.base import Component",
            "",
            "#########################",
            "# GENERAL",
            "#########################",
            "",
            "",
            "def to_binary(x: str | dict) -> bytes:",
            "    \"\"\"Converts a base64 string or dictionary to a binary string that can be sent in a POST.\"\"\"",
            "    if isinstance(x, dict):",
            "        if x.get(\"data\"):",
            "            base64str = x[\"data\"]",
            "        else:",
            "            base64str = client_utils.encode_url_or_file_to_base64(x[\"path\"])",
            "    else:",
            "        base64str = x",
            "    return base64.b64decode(extract_base64_data(base64str))",
            "",
            "",
            "def extract_base64_data(x: str) -> str:",
            "    \"\"\"Just extracts the base64 data from a general base64 string.\"\"\"",
            "    return x.rsplit(\",\", 1)[-1]",
            "",
            "",
            "#########################",
            "# IMAGE PRE-PROCESSING",
            "#########################",
            "",
            "",
            "def encode_plot_to_base64(plt):",
            "    with BytesIO() as output_bytes:",
            "        plt.savefig(output_bytes, format=\"png\")",
            "        bytes_data = output_bytes.getvalue()",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def get_pil_metadata(pil_image):",
            "    # Copy any text-only metadata",
            "    metadata = PngImagePlugin.PngInfo()",
            "    for key, value in pil_image.info.items():",
            "        if isinstance(key, str) and isinstance(value, str):",
            "            metadata.add_text(key, value)",
            "",
            "    return metadata",
            "",
            "",
            "def encode_pil_to_bytes(pil_image, format=\"png\"):",
            "    with BytesIO() as output_bytes:",
            "        pil_image.save(output_bytes, format, pnginfo=get_pil_metadata(pil_image))",
            "        return output_bytes.getvalue()",
            "",
            "",
            "def encode_pil_to_base64(pil_image):",
            "    bytes_data = encode_pil_to_bytes(pil_image)",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def encode_array_to_base64(image_array):",
            "    with BytesIO() as output_bytes:",
            "        pil_image = Image.fromarray(_convert(image_array, np.uint8, force_copy=False))",
            "        pil_image.save(output_bytes, \"PNG\")",
            "        bytes_data = output_bytes.getvalue()",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def hash_file(file_path: str | Path, chunk_num_blocks: int = 128) -> str:",
            "    sha1 = hashlib.sha1()",
            "    with open(file_path, \"rb\") as f:",
            "        for chunk in iter(lambda: f.read(chunk_num_blocks * sha1.block_size), b\"\"):",
            "            sha1.update(chunk)",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_url(url: str) -> str:",
            "    sha1 = hashlib.sha1()",
            "    sha1.update(url.encode(\"utf-8\"))",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_bytes(bytes: bytes):",
            "    sha1 = hashlib.sha1()",
            "    sha1.update(bytes)",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_base64(base64_encoding: str, chunk_num_blocks: int = 128) -> str:",
            "    sha1 = hashlib.sha1()",
            "    for i in range(0, len(base64_encoding), chunk_num_blocks * sha1.block_size):",
            "        data = base64_encoding[i : i + chunk_num_blocks * sha1.block_size]",
            "        sha1.update(data.encode(\"utf-8\"))",
            "    return sha1.hexdigest()",
            "",
            "",
            "def save_pil_to_cache(",
            "    img: Image.Image,",
            "    cache_dir: str,",
            "    name: str = \"image\",",
            "    format: Literal[\"png\", \"jpeg\"] = \"png\",",
            ") -> str:",
            "    bytes_data = encode_pil_to_bytes(img, format)",
            "    temp_dir = Path(cache_dir) / hash_bytes(bytes_data)",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    filename = str((temp_dir / f\"{name}.{format}\").resolve())",
            "    img.save(filename, pnginfo=get_pil_metadata(img))",
            "    return filename",
            "",
            "",
            "def save_img_array_to_cache(",
            "    arr: np.ndarray, cache_dir: str, format: Literal[\"png\", \"jpeg\"] = \"png\"",
            ") -> str:",
            "    pil_image = Image.fromarray(_convert(arr, np.uint8, force_copy=False))",
            "    return save_pil_to_cache(pil_image, cache_dir, format=format)",
            "",
            "",
            "def save_audio_to_cache(",
            "    data: np.ndarray, sample_rate: int, format: str, cache_dir: str",
            ") -> str:",
            "    temp_dir = Path(cache_dir) / hash_bytes(data.tobytes())",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    filename = str((temp_dir / f\"audio.{format}\").resolve())",
            "    audio_to_file(sample_rate, data, filename, format=format)",
            "    return filename",
            "",
            "",
            "def save_bytes_to_cache(data: bytes, file_name: str, cache_dir: str) -> str:",
            "    path = Path(cache_dir) / hash_bytes(data)",
            "    path.mkdir(exist_ok=True, parents=True)",
            "    path = path / Path(file_name).name",
            "    path.write_bytes(data)",
            "    return str(path.resolve())",
            "",
            "",
            "def save_file_to_cache(file_path: str | Path, cache_dir: str) -> str:",
            "    \"\"\"Returns a temporary file path for a copy of the given file path if it does",
            "    not already exist. Otherwise returns the path to the existing temp file.\"\"\"",
            "    temp_dir = hash_file(file_path)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    name = client_utils.strip_invalid_filename_characters(Path(file_path).name)",
            "    full_temp_file_path = str(abspath(temp_dir / name))",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        shutil.copy2(file_path, full_temp_file_path)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def save_url_to_cache(url: str, cache_dir: str) -> str:",
            "    \"\"\"Downloads a file and makes a temporary file path for a copy if does not already",
            "    exist. Otherwise returns the path to the existing temp file.\"\"\"",
            "    temp_dir = hash_url(url)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    name = client_utils.strip_invalid_filename_characters(Path(url).name)",
            "    full_temp_file_path = str(abspath(temp_dir / name))",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        with httpx.stream(\"GET\", url, follow_redirects=True) as r, open(",
            "            full_temp_file_path, \"wb\"",
            "        ) as f:",
            "            for chunk in r.iter_raw():",
            "                f.write(chunk)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def save_base64_to_cache(",
            "    base64_encoding: str, cache_dir: str, file_name: str | None = None",
            ") -> str:",
            "    \"\"\"Converts a base64 encoding to a file and returns the path to the file if",
            "    the file doesn't already exist. Otherwise returns the path to the existing file.",
            "    \"\"\"",
            "    temp_dir = hash_base64(base64_encoding)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    guess_extension = client_utils.get_extension(base64_encoding)",
            "    if file_name:",
            "        file_name = client_utils.strip_invalid_filename_characters(file_name)",
            "    elif guess_extension:",
            "        file_name = f\"file.{guess_extension}\"",
            "    else:",
            "        file_name = \"file\"",
            "",
            "    full_temp_file_path = str(abspath(temp_dir / file_name))  # type: ignore",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        data, _ = client_utils.decode_base64_to_binary(base64_encoding)",
            "        with open(full_temp_file_path, \"wb\") as fb:",
            "            fb.write(data)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def move_resource_to_block_cache(",
            "    url_or_file_path: str | Path | None, block: Component",
            ") -> str | None:",
            "    \"\"\"This method has been replaced by Block.move_resource_to_block_cache(), but is",
            "    left here for backwards compatibility for any custom components created in Gradio 4.2.0 or earlier.",
            "    \"\"\"",
            "    return block.move_resource_to_block_cache(url_or_file_path)",
            "",
            "",
            "def move_files_to_cache(data: Any, block: Component, postprocess: bool = False):",
            "    \"\"\"Move files to cache and replace the file path with the cache path.",
            "",
            "    Runs after .postprocess(), after .process_example(), and before .preprocess().",
            "",
            "    Args:",
            "        data: The input or output data for a component. Can be a dictionary or a dataclass",
            "        block: The component",
            "        postprocess: Whether its running from postprocessing",
            "    \"\"\"",
            "",
            "    def _move_to_cache(d: dict):",
            "        payload = FileData(**d)",
            "        # If the gradio app developer is returning a URL from",
            "        # postprocess, it means the component can display a URL",
            "        # without it being served from the gradio server",
            "        # This makes it so that the URL is not downloaded and speeds up event processing",
            "        if payload.url and postprocess:",
            "            temp_file_path = payload.url",
            "        else:",
            "            temp_file_path = move_resource_to_block_cache(payload.path, block)",
            "        assert temp_file_path is not None",
            "        payload.path = temp_file_path",
            "        return payload.model_dump()",
            "",
            "    if isinstance(data, (GradioRootModel, GradioModel)):",
            "        data = data.model_dump()",
            "",
            "    return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)",
            "",
            "",
            "def resize_and_crop(img, size, crop_type=\"center\"):",
            "    \"\"\"",
            "    Resize and crop an image to fit the specified size.",
            "    args:",
            "        size: `(width, height)` tuple. Pass `None` for either width or height",
            "        to only crop and resize the other.",
            "        crop_type: can be 'top', 'middle' or 'bottom', depending on this",
            "            value, the image will cropped getting the 'top/left', 'middle' or",
            "            'bottom/right' of the image to fit the size.",
            "    raises:",
            "        ValueError: if an invalid `crop_type` is provided.",
            "    \"\"\"",
            "    if crop_type == \"top\":",
            "        center = (0, 0)",
            "    elif crop_type == \"center\":",
            "        center = (0.5, 0.5)",
            "    else:",
            "        raise ValueError",
            "",
            "    resize = list(size)",
            "    if size[0] is None:",
            "        resize[0] = img.size[0]",
            "    if size[1] is None:",
            "        resize[1] = img.size[1]",
            "    return ImageOps.fit(img, resize, centering=center)  # type: ignore",
            "",
            "",
            "##################",
            "# Audio",
            "##################",
            "",
            "",
            "def audio_from_file(filename, crop_min=0, crop_max=100):",
            "    try:",
            "        audio = AudioSegment.from_file(filename)",
            "    except FileNotFoundError as e:",
            "        isfile = Path(filename).is_file()",
            "        msg = (",
            "            f\"Cannot load audio from file: `{'ffprobe' if isfile else filename}` not found.\"",
            "            + \" Please install `ffmpeg` in your system to use non-WAV audio file formats\"",
            "            \" and make sure `ffprobe` is in your PATH.\"",
            "            if isfile",
            "            else \"\"",
            "        )",
            "        raise RuntimeError(msg) from e",
            "    if crop_min != 0 or crop_max != 100:",
            "        audio_start = len(audio) * crop_min / 100",
            "        audio_end = len(audio) * crop_max / 100",
            "        audio = audio[audio_start:audio_end]",
            "    data = np.array(audio.get_array_of_samples())",
            "    if audio.channels > 1:",
            "        data = data.reshape(-1, audio.channels)",
            "    return audio.frame_rate, data",
            "",
            "",
            "def audio_to_file(sample_rate, data, filename, format=\"wav\"):",
            "    if format == \"wav\":",
            "        data = convert_to_16_bit_wav(data)",
            "    audio = AudioSegment(",
            "        data.tobytes(),",
            "        frame_rate=sample_rate,",
            "        sample_width=data.dtype.itemsize,",
            "        channels=(1 if len(data.shape) == 1 else data.shape[1]),",
            "    )",
            "    file = audio.export(filename, format=format)",
            "    file.close()  # type: ignore",
            "",
            "",
            "def convert_to_16_bit_wav(data):",
            "    # Based on: https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html",
            "    warning = \"Trying to convert audio automatically from {} to 16-bit int format.\"",
            "    if data.dtype in [np.float64, np.float32, np.float16]:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data / np.abs(data).max()",
            "        data = data * 32767",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int32:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data / 65536",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int16:",
            "        pass",
            "    elif data.dtype == np.uint16:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data - 32768",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.uint8:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data * 257 - 32768",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int8:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data * 256",
            "        data = data.astype(np.int16)",
            "    else:",
            "        raise ValueError(",
            "            \"Audio data cannot be converted automatically from \"",
            "            f\"{data.dtype} to 16-bit int format.\"",
            "        )",
            "    return data",
            "",
            "",
            "##################",
            "# OUTPUT",
            "##################",
            "",
            "",
            "def _convert(image, dtype, force_copy=False, uniform=False):",
            "    \"\"\"",
            "    Adapted from: https://github.com/scikit-image/scikit-image/blob/main/skimage/util/dtype.py#L510-L531",
            "",
            "    Convert an image to the requested data-type.",
            "    Warnings are issued in case of precision loss, or when negative values",
            "    are clipped during conversion to unsigned integer types (sign loss).",
            "    Floating point values are expected to be normalized and will be clipped",
            "    to the range [0.0, 1.0] or [-1.0, 1.0] when converting to unsigned or",
            "    signed integers respectively.",
            "    Numbers are not shifted to the negative side when converting from",
            "    unsigned to signed integer types. Negative values will be clipped when",
            "    converting to unsigned integers.",
            "    Parameters",
            "    ----------",
            "    image : ndarray",
            "        Input image.",
            "    dtype : dtype",
            "        Target data-type.",
            "    force_copy : bool, optional",
            "        Force a copy of the data, irrespective of its current dtype.",
            "    uniform : bool, optional",
            "        Uniformly quantize the floating point range to the integer range.",
            "        By default (uniform=False) floating point values are scaled and",
            "        rounded to the nearest integers, which minimizes back and forth",
            "        conversion errors.",
            "    .. versionchanged :: 0.15",
            "        ``_convert`` no longer warns about possible precision or sign",
            "        information loss. See discussions on these warnings at:",
            "        https://github.com/scikit-image/scikit-image/issues/2602",
            "        https://github.com/scikit-image/scikit-image/issues/543#issuecomment-208202228",
            "        https://github.com/scikit-image/scikit-image/pull/3575",
            "    References",
            "    ----------",
            "    .. [1] DirectX data conversion rules.",
            "           https://msdn.microsoft.com/en-us/library/windows/desktop/dd607323%28v=vs.85%29.aspx",
            "    .. [2] Data Conversions. In \"OpenGL ES 2.0 Specification v2.0.25\",",
            "           pp 7-8. Khronos Group, 2010.",
            "    .. [3] Proper treatment of pixels as integers. A.W. Paeth.",
            "           In \"Graphics Gems I\", pp 249-256. Morgan Kaufmann, 1990.",
            "    .. [4] Dirty Pixels. J. Blinn. In \"Jim Blinn's corner: Dirty Pixels\",",
            "           pp 47-57. Morgan Kaufmann, 1998.",
            "    \"\"\"",
            "    dtype_range = {",
            "        bool: (False, True),",
            "        np.bool_: (False, True),",
            "        np.bool8: (False, True),  # type: ignore",
            "        float: (-1, 1),",
            "        np.float_: (-1, 1),",
            "        np.float16: (-1, 1),",
            "        np.float32: (-1, 1),",
            "        np.float64: (-1, 1),",
            "    }",
            "",
            "    def _dtype_itemsize(itemsize, *dtypes):",
            "        \"\"\"Return first of `dtypes` with itemsize greater than `itemsize`",
            "        Parameters",
            "        ----------",
            "        itemsize: int",
            "            The data type object element size.",
            "        Other Parameters",
            "        ----------------",
            "        *dtypes:",
            "            Any Object accepted by `np.dtype` to be converted to a data",
            "            type object",
            "        Returns",
            "        -------",
            "        dtype: data type object",
            "            First of `dtypes` with itemsize greater than `itemsize`.",
            "        \"\"\"",
            "        return next(dt for dt in dtypes if np.dtype(dt).itemsize >= itemsize)",
            "",
            "    def _dtype_bits(kind, bits, itemsize=1):",
            "        \"\"\"Return dtype of `kind` that can store a `bits` wide unsigned int",
            "        Parameters:",
            "        kind: str",
            "            Data type kind.",
            "        bits: int",
            "            Desired number of bits.",
            "        itemsize: int",
            "            The data type object element size.",
            "        Returns",
            "        -------",
            "        dtype: data type object",
            "            Data type of `kind` that can store a `bits` wide unsigned int",
            "        \"\"\"",
            "",
            "        s = next(",
            "            i",
            "            for i in (itemsize,) + (2, 4, 8)",
            "            if bits < (i * 8) or (bits == (i * 8) and kind == \"u\")",
            "        )",
            "",
            "        return np.dtype(kind + str(s))",
            "",
            "    def _scale(a, n, m, copy=True):",
            "        \"\"\"Scale an array of unsigned/positive integers from `n` to `m` bits.",
            "        Numbers can be represented exactly only if `m` is a multiple of `n`.",
            "        Parameters",
            "        ----------",
            "        a : ndarray",
            "            Input image array.",
            "        n : int",
            "            Number of bits currently used to encode the values in `a`.",
            "        m : int",
            "            Desired number of bits to encode the values in `out`.",
            "        copy : bool, optional",
            "            If True, allocates and returns new array. Otherwise, modifies",
            "            `a` in place.",
            "        Returns",
            "        -------",
            "        out : array",
            "            Output image array. Has the same kind as `a`.",
            "        \"\"\"",
            "        kind = a.dtype.kind",
            "        if n > m and a.max() < 2**m:",
            "            return a.astype(_dtype_bits(kind, m))",
            "        elif n == m:",
            "            return a.copy() if copy else a",
            "        elif n > m:",
            "            # downscale with precision loss",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, m))",
            "                np.floor_divide(a, 2 ** (n - m), out=b, dtype=a.dtype, casting=\"unsafe\")",
            "                return b",
            "            else:",
            "                a //= 2 ** (n - m)",
            "                return a",
            "        elif m % n == 0:",
            "            # exact upscale to a multiple of `n` bits",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, m))",
            "                np.multiply(a, (2**m - 1) // (2**n - 1), out=b, dtype=b.dtype)",
            "                return b",
            "            else:",
            "                a = a.astype(_dtype_bits(kind, m, a.dtype.itemsize), copy=False)",
            "                a *= (2**m - 1) // (2**n - 1)",
            "                return a",
            "        else:",
            "            # upscale to a multiple of `n` bits,",
            "            # then downscale with precision loss",
            "            o = (m // n + 1) * n",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, o))",
            "                np.multiply(a, (2**o - 1) // (2**n - 1), out=b, dtype=b.dtype)",
            "                b //= 2 ** (o - m)",
            "                return b",
            "            else:",
            "                a = a.astype(_dtype_bits(kind, o, a.dtype.itemsize), copy=False)",
            "                a *= (2**o - 1) // (2**n - 1)",
            "                a //= 2 ** (o - m)",
            "                return a",
            "",
            "    image = np.asarray(image)",
            "    dtypeobj_in = image.dtype",
            "    dtypeobj_out = np.dtype(\"float64\") if dtype is np.floating else np.dtype(dtype)",
            "    dtype_in = dtypeobj_in.type",
            "    dtype_out = dtypeobj_out.type",
            "    kind_in = dtypeobj_in.kind",
            "    kind_out = dtypeobj_out.kind",
            "    itemsize_in = dtypeobj_in.itemsize",
            "    itemsize_out = dtypeobj_out.itemsize",
            "",
            "    # Below, we do an `issubdtype` check.  Its purpose is to find out",
            "    # whether we can get away without doing any image conversion.  This happens",
            "    # when:",
            "    #",
            "    # - the output and input dtypes are the same or",
            "    # - when the output is specified as a type, and the input dtype",
            "    #   is a subclass of that type (e.g. `np.floating` will allow",
            "    #   `float32` and `float64` arrays through)",
            "",
            "    if np.issubdtype(dtype_in, np.obj2sctype(dtype)):",
            "        if force_copy:",
            "            image = image.copy()",
            "        return image",
            "",
            "    if kind_in in \"ui\":",
            "        imin_in = np.iinfo(dtype_in).min",
            "        imax_in = np.iinfo(dtype_in).max",
            "    if kind_out in \"ui\":",
            "        imin_out = np.iinfo(dtype_out).min  # type: ignore",
            "        imax_out = np.iinfo(dtype_out).max  # type: ignore",
            "",
            "    # any -> binary",
            "    if kind_out == \"b\":",
            "        return image > dtype_in(dtype_range[dtype_in][1] / 2)",
            "",
            "    # binary -> any",
            "    if kind_in == \"b\":",
            "        result = image.astype(dtype_out)",
            "        if kind_out != \"f\":",
            "            result *= dtype_out(dtype_range[dtype_out][1])",
            "        return result",
            "",
            "    # float -> any",
            "    if kind_in == \"f\":",
            "        if kind_out == \"f\":",
            "            # float -> float",
            "            return image.astype(dtype_out)",
            "",
            "        if np.min(image) < -1.0 or np.max(image) > 1.0:",
            "            raise ValueError(\"Images of type float must be between -1 and 1.\")",
            "        # floating point -> integer",
            "        # use float type that can represent output integer type",
            "        computation_type = _dtype_itemsize(",
            "            itemsize_out, dtype_in, np.float32, np.float64",
            "        )",
            "",
            "        if not uniform:",
            "            if kind_out == \"u\":",
            "                image_out = np.multiply(image, imax_out, dtype=computation_type)  # type: ignore",
            "            else:",
            "                image_out = np.multiply(",
            "                    image,",
            "                    (imax_out - imin_out) / 2,  # type: ignore",
            "                    dtype=computation_type,",
            "                )",
            "                image_out -= 1.0 / 2.0",
            "            np.rint(image_out, out=image_out)",
            "            np.clip(image_out, imin_out, imax_out, out=image_out)  # type: ignore",
            "        elif kind_out == \"u\":",
            "            image_out = np.multiply(image, imax_out + 1, dtype=computation_type)  # type: ignore",
            "            np.clip(image_out, 0, imax_out, out=image_out)  # type: ignore",
            "        else:",
            "            image_out = np.multiply(",
            "                image,",
            "                (imax_out - imin_out + 1.0) / 2.0,  # type: ignore",
            "                dtype=computation_type,",
            "            )",
            "            np.floor(image_out, out=image_out)",
            "            np.clip(image_out, imin_out, imax_out, out=image_out)  # type: ignore",
            "        return image_out.astype(dtype_out)",
            "",
            "    # signed/unsigned int -> float",
            "    if kind_out == \"f\":",
            "        # use float type that can exactly represent input integers",
            "        computation_type = _dtype_itemsize(",
            "            itemsize_in, dtype_out, np.float32, np.float64",
            "        )",
            "",
            "        if kind_in == \"u\":",
            "            # using np.divide or np.multiply doesn't copy the data",
            "            # until the computation time",
            "            image = np.multiply(image, 1.0 / imax_in, dtype=computation_type)  # type: ignore",
            "            # DirectX uses this conversion also for signed ints",
            "            # if imin_in:",
            "            #     np.maximum(image, -1.0, out=image)",
            "        else:",
            "            image = np.add(image, 0.5, dtype=computation_type)",
            "            image *= 2 / (imax_in - imin_in)  # type: ignore",
            "",
            "        return np.asarray(image, dtype_out)",
            "",
            "    # unsigned int -> signed/unsigned int",
            "    if kind_in == \"u\":",
            "        if kind_out == \"i\":",
            "            # unsigned int -> signed int",
            "            image = _scale(image, 8 * itemsize_in, 8 * itemsize_out - 1)",
            "            return image.view(dtype_out)",
            "        else:",
            "            # unsigned int -> unsigned int",
            "            return _scale(image, 8 * itemsize_in, 8 * itemsize_out)",
            "",
            "    # signed int -> unsigned int",
            "    if kind_out == \"u\":",
            "        image = _scale(image, 8 * itemsize_in - 1, 8 * itemsize_out)",
            "        result = np.empty(image.shape, dtype_out)",
            "        np.maximum(image, 0, out=result, dtype=image.dtype, casting=\"unsafe\")",
            "        return result",
            "",
            "    # signed int -> signed int",
            "    if itemsize_in > itemsize_out:",
            "        return _scale(image, 8 * itemsize_in - 1, 8 * itemsize_out - 1)",
            "",
            "    image = image.astype(_dtype_bits(\"i\", itemsize_out * 8))",
            "    image -= imin_in  # type: ignore",
            "    image = _scale(image, 8 * itemsize_in, 8 * itemsize_out, copy=False)",
            "    image += imin_out  # type: ignore",
            "    return image.astype(dtype_out)",
            "",
            "",
            "def ffmpeg_installed() -> bool:",
            "    if wasm_utils.IS_WASM:",
            "        # TODO: Support ffmpeg in WASM",
            "        return False",
            "",
            "    return shutil.which(\"ffmpeg\") is not None",
            "",
            "",
            "def video_is_playable(video_filepath: str) -> bool:",
            "    \"\"\"Determines if a video is playable in the browser.",
            "",
            "    A video is playable if it has a playable container and codec.",
            "        .mp4 -> h264",
            "        .webm -> vp9",
            "        .ogg -> theora",
            "    \"\"\"",
            "    from ffmpy import FFprobe, FFRuntimeError",
            "",
            "    try:",
            "        container = Path(video_filepath).suffix.lower()",
            "        probe = FFprobe(",
            "            global_options=\"-show_format -show_streams -select_streams v -print_format json\",",
            "            inputs={video_filepath: None},",
            "        )",
            "        output = probe.run(stderr=subprocess.PIPE, stdout=subprocess.PIPE)",
            "        output = json.loads(output[0])",
            "        video_codec = output[\"streams\"][0][\"codec_name\"]",
            "        return (container, video_codec) in [",
            "            (\".mp4\", \"h264\"),",
            "            (\".ogg\", \"theora\"),",
            "            (\".webm\", \"vp9\"),",
            "        ]",
            "    # If anything goes wrong, assume the video can be played to not convert downstream",
            "    except (FFRuntimeError, IndexError, KeyError):",
            "        return True",
            "",
            "",
            "def convert_video_to_playable_mp4(video_path: str) -> str:",
            "    \"\"\"Convert the video to mp4. If something goes wrong return the original video.\"\"\"",
            "    from ffmpy import FFmpeg, FFRuntimeError",
            "",
            "    try:",
            "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:",
            "            output_path = Path(video_path).with_suffix(\".mp4\")",
            "            shutil.copy2(video_path, tmp_file.name)",
            "            # ffmpeg will automatically use h264 codec (playable in browser) when converting to mp4",
            "            ff = FFmpeg(",
            "                inputs={str(tmp_file.name): None},",
            "                outputs={str(output_path): None},",
            "                global_options=\"-y -loglevel quiet\",",
            "            )",
            "            ff.run()",
            "    except FFRuntimeError as e:",
            "        print(f\"Error converting video to browser-playable format {str(e)}\")",
            "        output_path = video_path",
            "    finally:",
            "        # Remove temp file",
            "        os.remove(tmp_file.name)  # type: ignore",
            "    return str(output_path)",
            "",
            "",
            "def get_video_length(video_path: str | Path):",
            "    if wasm_utils.IS_WASM:",
            "        raise wasm_utils.WasmUnsupportedError(",
            "            \"Video duration is not supported in the Wasm mode.\"",
            "        )",
            "    duration = subprocess.check_output(",
            "        [",
            "            \"ffprobe\",",
            "            \"-i\",",
            "            str(video_path),",
            "            \"-show_entries\",",
            "            \"format=duration\",",
            "            \"-v\",",
            "            \"quiet\",",
            "            \"-of\",",
            "            \"csv={}\".format(\"p=0\"),",
            "        ]",
            "    )",
            "    duration_str = duration.decode(\"utf-8\").strip()",
            "    duration_float = float(duration_str)",
            "",
            "    return duration_float"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import base64",
            "import hashlib",
            "import json",
            "import logging",
            "import os",
            "import shutil",
            "import subprocess",
            "import tempfile",
            "import warnings",
            "from io import BytesIO",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Any, Literal",
            "",
            "import httpx",
            "import numpy as np",
            "from gradio_client import utils as client_utils",
            "from PIL import Image, ImageOps, PngImagePlugin",
            "",
            "from gradio import wasm_utils",
            "from gradio.data_classes import FileData, GradioModel, GradioRootModel",
            "from gradio.utils import abspath",
            "",
            "with warnings.catch_warnings():",
            "    warnings.simplefilter(\"ignore\")  # Ignore pydub warning if ffmpeg is not installed",
            "    from pydub import AudioSegment",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.components.base import Component",
            "",
            "#########################",
            "# GENERAL",
            "#########################",
            "",
            "",
            "def to_binary(x: str | dict) -> bytes:",
            "    \"\"\"Converts a base64 string or dictionary to a binary string that can be sent in a POST.\"\"\"",
            "    if isinstance(x, dict):",
            "        if x.get(\"data\"):",
            "            base64str = x[\"data\"]",
            "        else:",
            "            base64str = client_utils.encode_url_or_file_to_base64(x[\"path\"])",
            "    else:",
            "        base64str = x",
            "    return base64.b64decode(extract_base64_data(base64str))",
            "",
            "",
            "def extract_base64_data(x: str) -> str:",
            "    \"\"\"Just extracts the base64 data from a general base64 string.\"\"\"",
            "    return x.rsplit(\",\", 1)[-1]",
            "",
            "",
            "#########################",
            "# IMAGE PRE-PROCESSING",
            "#########################",
            "",
            "",
            "def encode_plot_to_base64(plt):",
            "    with BytesIO() as output_bytes:",
            "        plt.savefig(output_bytes, format=\"png\")",
            "        bytes_data = output_bytes.getvalue()",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def get_pil_metadata(pil_image):",
            "    # Copy any text-only metadata",
            "    metadata = PngImagePlugin.PngInfo()",
            "    for key, value in pil_image.info.items():",
            "        if isinstance(key, str) and isinstance(value, str):",
            "            metadata.add_text(key, value)",
            "",
            "    return metadata",
            "",
            "",
            "def encode_pil_to_bytes(pil_image, format=\"png\"):",
            "    with BytesIO() as output_bytes:",
            "        pil_image.save(output_bytes, format, pnginfo=get_pil_metadata(pil_image))",
            "        return output_bytes.getvalue()",
            "",
            "",
            "def encode_pil_to_base64(pil_image):",
            "    bytes_data = encode_pil_to_bytes(pil_image)",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def encode_array_to_base64(image_array):",
            "    with BytesIO() as output_bytes:",
            "        pil_image = Image.fromarray(_convert(image_array, np.uint8, force_copy=False))",
            "        pil_image.save(output_bytes, \"PNG\")",
            "        bytes_data = output_bytes.getvalue()",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def hash_file(file_path: str | Path, chunk_num_blocks: int = 128) -> str:",
            "    sha1 = hashlib.sha1()",
            "    with open(file_path, \"rb\") as f:",
            "        for chunk in iter(lambda: f.read(chunk_num_blocks * sha1.block_size), b\"\"):",
            "            sha1.update(chunk)",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_url(url: str) -> str:",
            "    sha1 = hashlib.sha1()",
            "    sha1.update(url.encode(\"utf-8\"))",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_bytes(bytes: bytes):",
            "    sha1 = hashlib.sha1()",
            "    sha1.update(bytes)",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_base64(base64_encoding: str, chunk_num_blocks: int = 128) -> str:",
            "    sha1 = hashlib.sha1()",
            "    for i in range(0, len(base64_encoding), chunk_num_blocks * sha1.block_size):",
            "        data = base64_encoding[i : i + chunk_num_blocks * sha1.block_size]",
            "        sha1.update(data.encode(\"utf-8\"))",
            "    return sha1.hexdigest()",
            "",
            "",
            "def save_pil_to_cache(",
            "    img: Image.Image,",
            "    cache_dir: str,",
            "    name: str = \"image\",",
            "    format: Literal[\"png\", \"jpeg\"] = \"png\",",
            ") -> str:",
            "    bytes_data = encode_pil_to_bytes(img, format)",
            "    temp_dir = Path(cache_dir) / hash_bytes(bytes_data)",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    filename = str((temp_dir / f\"{name}.{format}\").resolve())",
            "    img.save(filename, pnginfo=get_pil_metadata(img))",
            "    return filename",
            "",
            "",
            "def save_img_array_to_cache(",
            "    arr: np.ndarray, cache_dir: str, format: Literal[\"png\", \"jpeg\"] = \"png\"",
            ") -> str:",
            "    pil_image = Image.fromarray(_convert(arr, np.uint8, force_copy=False))",
            "    return save_pil_to_cache(pil_image, cache_dir, format=format)",
            "",
            "",
            "def save_audio_to_cache(",
            "    data: np.ndarray, sample_rate: int, format: str, cache_dir: str",
            ") -> str:",
            "    temp_dir = Path(cache_dir) / hash_bytes(data.tobytes())",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    filename = str((temp_dir / f\"audio.{format}\").resolve())",
            "    audio_to_file(sample_rate, data, filename, format=format)",
            "    return filename",
            "",
            "",
            "def save_bytes_to_cache(data: bytes, file_name: str, cache_dir: str) -> str:",
            "    path = Path(cache_dir) / hash_bytes(data)",
            "    path.mkdir(exist_ok=True, parents=True)",
            "    path = path / Path(file_name).name",
            "    path.write_bytes(data)",
            "    return str(path.resolve())",
            "",
            "",
            "def save_file_to_cache(file_path: str | Path, cache_dir: str) -> str:",
            "    \"\"\"Returns a temporary file path for a copy of the given file path if it does",
            "    not already exist. Otherwise returns the path to the existing temp file.\"\"\"",
            "    temp_dir = hash_file(file_path)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    name = client_utils.strip_invalid_filename_characters(Path(file_path).name)",
            "    full_temp_file_path = str(abspath(temp_dir / name))",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        shutil.copy2(file_path, full_temp_file_path)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def save_url_to_cache(url: str, cache_dir: str) -> str:",
            "    \"\"\"Downloads a file and makes a temporary file path for a copy if does not already",
            "    exist. Otherwise returns the path to the existing temp file.\"\"\"",
            "    temp_dir = hash_url(url)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    name = client_utils.strip_invalid_filename_characters(Path(url).name)",
            "    full_temp_file_path = str(abspath(temp_dir / name))",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        with httpx.stream(\"GET\", url, follow_redirects=True) as r, open(",
            "            full_temp_file_path, \"wb\"",
            "        ) as f:",
            "            for chunk in r.iter_raw():",
            "                f.write(chunk)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def save_base64_to_cache(",
            "    base64_encoding: str, cache_dir: str, file_name: str | None = None",
            ") -> str:",
            "    \"\"\"Converts a base64 encoding to a file and returns the path to the file if",
            "    the file doesn't already exist. Otherwise returns the path to the existing file.",
            "    \"\"\"",
            "    temp_dir = hash_base64(base64_encoding)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    guess_extension = client_utils.get_extension(base64_encoding)",
            "    if file_name:",
            "        file_name = client_utils.strip_invalid_filename_characters(file_name)",
            "    elif guess_extension:",
            "        file_name = f\"file.{guess_extension}\"",
            "    else:",
            "        file_name = \"file\"",
            "",
            "    full_temp_file_path = str(abspath(temp_dir / file_name))  # type: ignore",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        data, _ = client_utils.decode_base64_to_binary(base64_encoding)",
            "        with open(full_temp_file_path, \"wb\") as fb:",
            "            fb.write(data)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def move_resource_to_block_cache(",
            "    url_or_file_path: str | Path | None, block: Component",
            ") -> str | None:",
            "    \"\"\"This method has been replaced by Block.move_resource_to_block_cache(), but is",
            "    left here for backwards compatibility for any custom components created in Gradio 4.2.0 or earlier.",
            "    \"\"\"",
            "    return block.move_resource_to_block_cache(url_or_file_path)",
            "",
            "",
            "def move_files_to_cache(",
            "    data: Any,",
            "    block: Component,",
            "    postprocess: bool = False,",
            "    add_urls=False,",
            ") -> dict:",
            "    \"\"\"Move any files in `data` to cache and (optionally), adds URL prefixes (/file=...) needed to access the cached file.",
            "    Also handles the case where the file is on an external Gradio app (/proxy=...).",
            "",
            "    Runs after .postprocess() and before .preprocess().",
            "",
            "    Args:",
            "        data: The input or output data for a component. Can be a dictionary or a dataclass",
            "        block: The component whose data is being processed",
            "        postprocess: Whether its running from postprocessing",
            "        root_url: The root URL of the local server, if applicable",
            "    \"\"\"",
            "",
            "    def _move_to_cache(d: dict):",
            "        payload = FileData(**d)",
            "        # If the gradio app developer is returning a URL from",
            "        # postprocess, it means the component can display a URL",
            "        # without it being served from the gradio server",
            "        # This makes it so that the URL is not downloaded and speeds up event processing",
            "        if payload.url and postprocess:",
            "            temp_file_path = payload.url",
            "        else:",
            "            temp_file_path = move_resource_to_block_cache(payload.path, block)",
            "        assert temp_file_path is not None",
            "        payload.path = temp_file_path",
            "",
            "        if add_urls:",
            "            url_prefix = \"/stream/\" if payload.is_stream else \"/file=\"",
            "            if block.proxy_url:",
            "                url = f\"/proxy={block.proxy_url}{url_prefix}{temp_file_path}\"",
            "            elif client_utils.is_http_url_like(",
            "                temp_file_path",
            "            ) or temp_file_path.startswith(f\"{url_prefix}\"):",
            "                url = temp_file_path",
            "            else:",
            "                url = f\"{url_prefix}{temp_file_path}\"",
            "            payload.url = url",
            "",
            "        return payload.model_dump()",
            "",
            "    if isinstance(data, (GradioRootModel, GradioModel)):",
            "        data = data.model_dump()",
            "",
            "    return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)",
            "",
            "",
            "def add_root_url(data, root_url) -> dict:",
            "    def _add_root_url(file_dict: dict):",
            "        if not client_utils.is_http_url_like(file_dict[\"url\"]):",
            "            file_dict[\"url\"] = f'{root_url}{file_dict[\"url\"]}'",
            "        return file_dict",
            "",
            "    return client_utils.traverse(data, _add_root_url, client_utils.is_file_obj_with_url)",
            "",
            "",
            "def resize_and_crop(img, size, crop_type=\"center\"):",
            "    \"\"\"",
            "    Resize and crop an image to fit the specified size.",
            "    args:",
            "        size: `(width, height)` tuple. Pass `None` for either width or height",
            "        to only crop and resize the other.",
            "        crop_type: can be 'top', 'middle' or 'bottom', depending on this",
            "            value, the image will cropped getting the 'top/left', 'middle' or",
            "            'bottom/right' of the image to fit the size.",
            "    raises:",
            "        ValueError: if an invalid `crop_type` is provided.",
            "    \"\"\"",
            "    if crop_type == \"top\":",
            "        center = (0, 0)",
            "    elif crop_type == \"center\":",
            "        center = (0.5, 0.5)",
            "    else:",
            "        raise ValueError",
            "",
            "    resize = list(size)",
            "    if size[0] is None:",
            "        resize[0] = img.size[0]",
            "    if size[1] is None:",
            "        resize[1] = img.size[1]",
            "    return ImageOps.fit(img, resize, centering=center)  # type: ignore",
            "",
            "",
            "##################",
            "# Audio",
            "##################",
            "",
            "",
            "def audio_from_file(filename, crop_min=0, crop_max=100):",
            "    try:",
            "        audio = AudioSegment.from_file(filename)",
            "    except FileNotFoundError as e:",
            "        isfile = Path(filename).is_file()",
            "        msg = (",
            "            f\"Cannot load audio from file: `{'ffprobe' if isfile else filename}` not found.\"",
            "            + \" Please install `ffmpeg` in your system to use non-WAV audio file formats\"",
            "            \" and make sure `ffprobe` is in your PATH.\"",
            "            if isfile",
            "            else \"\"",
            "        )",
            "        raise RuntimeError(msg) from e",
            "    if crop_min != 0 or crop_max != 100:",
            "        audio_start = len(audio) * crop_min / 100",
            "        audio_end = len(audio) * crop_max / 100",
            "        audio = audio[audio_start:audio_end]",
            "    data = np.array(audio.get_array_of_samples())",
            "    if audio.channels > 1:",
            "        data = data.reshape(-1, audio.channels)",
            "    return audio.frame_rate, data",
            "",
            "",
            "def audio_to_file(sample_rate, data, filename, format=\"wav\"):",
            "    if format == \"wav\":",
            "        data = convert_to_16_bit_wav(data)",
            "    audio = AudioSegment(",
            "        data.tobytes(),",
            "        frame_rate=sample_rate,",
            "        sample_width=data.dtype.itemsize,",
            "        channels=(1 if len(data.shape) == 1 else data.shape[1]),",
            "    )",
            "    file = audio.export(filename, format=format)",
            "    file.close()  # type: ignore",
            "",
            "",
            "def convert_to_16_bit_wav(data):",
            "    # Based on: https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html",
            "    warning = \"Trying to convert audio automatically from {} to 16-bit int format.\"",
            "    if data.dtype in [np.float64, np.float32, np.float16]:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data / np.abs(data).max()",
            "        data = data * 32767",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int32:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data / 65536",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int16:",
            "        pass",
            "    elif data.dtype == np.uint16:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data - 32768",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.uint8:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data * 257 - 32768",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int8:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data * 256",
            "        data = data.astype(np.int16)",
            "    else:",
            "        raise ValueError(",
            "            \"Audio data cannot be converted automatically from \"",
            "            f\"{data.dtype} to 16-bit int format.\"",
            "        )",
            "    return data",
            "",
            "",
            "##################",
            "# OUTPUT",
            "##################",
            "",
            "",
            "def _convert(image, dtype, force_copy=False, uniform=False):",
            "    \"\"\"",
            "    Adapted from: https://github.com/scikit-image/scikit-image/blob/main/skimage/util/dtype.py#L510-L531",
            "",
            "    Convert an image to the requested data-type.",
            "    Warnings are issued in case of precision loss, or when negative values",
            "    are clipped during conversion to unsigned integer types (sign loss).",
            "    Floating point values are expected to be normalized and will be clipped",
            "    to the range [0.0, 1.0] or [-1.0, 1.0] when converting to unsigned or",
            "    signed integers respectively.",
            "    Numbers are not shifted to the negative side when converting from",
            "    unsigned to signed integer types. Negative values will be clipped when",
            "    converting to unsigned integers.",
            "    Parameters",
            "    ----------",
            "    image : ndarray",
            "        Input image.",
            "    dtype : dtype",
            "        Target data-type.",
            "    force_copy : bool, optional",
            "        Force a copy of the data, irrespective of its current dtype.",
            "    uniform : bool, optional",
            "        Uniformly quantize the floating point range to the integer range.",
            "        By default (uniform=False) floating point values are scaled and",
            "        rounded to the nearest integers, which minimizes back and forth",
            "        conversion errors.",
            "    .. versionchanged :: 0.15",
            "        ``_convert`` no longer warns about possible precision or sign",
            "        information loss. See discussions on these warnings at:",
            "        https://github.com/scikit-image/scikit-image/issues/2602",
            "        https://github.com/scikit-image/scikit-image/issues/543#issuecomment-208202228",
            "        https://github.com/scikit-image/scikit-image/pull/3575",
            "    References",
            "    ----------",
            "    .. [1] DirectX data conversion rules.",
            "           https://msdn.microsoft.com/en-us/library/windows/desktop/dd607323%28v=vs.85%29.aspx",
            "    .. [2] Data Conversions. In \"OpenGL ES 2.0 Specification v2.0.25\",",
            "           pp 7-8. Khronos Group, 2010.",
            "    .. [3] Proper treatment of pixels as integers. A.W. Paeth.",
            "           In \"Graphics Gems I\", pp 249-256. Morgan Kaufmann, 1990.",
            "    .. [4] Dirty Pixels. J. Blinn. In \"Jim Blinn's corner: Dirty Pixels\",",
            "           pp 47-57. Morgan Kaufmann, 1998.",
            "    \"\"\"",
            "    dtype_range = {",
            "        bool: (False, True),",
            "        np.bool_: (False, True),",
            "        np.bool8: (False, True),  # type: ignore",
            "        float: (-1, 1),",
            "        np.float_: (-1, 1),",
            "        np.float16: (-1, 1),",
            "        np.float32: (-1, 1),",
            "        np.float64: (-1, 1),",
            "    }",
            "",
            "    def _dtype_itemsize(itemsize, *dtypes):",
            "        \"\"\"Return first of `dtypes` with itemsize greater than `itemsize`",
            "        Parameters",
            "        ----------",
            "        itemsize: int",
            "            The data type object element size.",
            "        Other Parameters",
            "        ----------------",
            "        *dtypes:",
            "            Any Object accepted by `np.dtype` to be converted to a data",
            "            type object",
            "        Returns",
            "        -------",
            "        dtype: data type object",
            "            First of `dtypes` with itemsize greater than `itemsize`.",
            "        \"\"\"",
            "        return next(dt for dt in dtypes if np.dtype(dt).itemsize >= itemsize)",
            "",
            "    def _dtype_bits(kind, bits, itemsize=1):",
            "        \"\"\"Return dtype of `kind` that can store a `bits` wide unsigned int",
            "        Parameters:",
            "        kind: str",
            "            Data type kind.",
            "        bits: int",
            "            Desired number of bits.",
            "        itemsize: int",
            "            The data type object element size.",
            "        Returns",
            "        -------",
            "        dtype: data type object",
            "            Data type of `kind` that can store a `bits` wide unsigned int",
            "        \"\"\"",
            "",
            "        s = next(",
            "            i",
            "            for i in (itemsize,) + (2, 4, 8)",
            "            if bits < (i * 8) or (bits == (i * 8) and kind == \"u\")",
            "        )",
            "",
            "        return np.dtype(kind + str(s))",
            "",
            "    def _scale(a, n, m, copy=True):",
            "        \"\"\"Scale an array of unsigned/positive integers from `n` to `m` bits.",
            "        Numbers can be represented exactly only if `m` is a multiple of `n`.",
            "        Parameters",
            "        ----------",
            "        a : ndarray",
            "            Input image array.",
            "        n : int",
            "            Number of bits currently used to encode the values in `a`.",
            "        m : int",
            "            Desired number of bits to encode the values in `out`.",
            "        copy : bool, optional",
            "            If True, allocates and returns new array. Otherwise, modifies",
            "            `a` in place.",
            "        Returns",
            "        -------",
            "        out : array",
            "            Output image array. Has the same kind as `a`.",
            "        \"\"\"",
            "        kind = a.dtype.kind",
            "        if n > m and a.max() < 2**m:",
            "            return a.astype(_dtype_bits(kind, m))",
            "        elif n == m:",
            "            return a.copy() if copy else a",
            "        elif n > m:",
            "            # downscale with precision loss",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, m))",
            "                np.floor_divide(a, 2 ** (n - m), out=b, dtype=a.dtype, casting=\"unsafe\")",
            "                return b",
            "            else:",
            "                a //= 2 ** (n - m)",
            "                return a",
            "        elif m % n == 0:",
            "            # exact upscale to a multiple of `n` bits",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, m))",
            "                np.multiply(a, (2**m - 1) // (2**n - 1), out=b, dtype=b.dtype)",
            "                return b",
            "            else:",
            "                a = a.astype(_dtype_bits(kind, m, a.dtype.itemsize), copy=False)",
            "                a *= (2**m - 1) // (2**n - 1)",
            "                return a",
            "        else:",
            "            # upscale to a multiple of `n` bits,",
            "            # then downscale with precision loss",
            "            o = (m // n + 1) * n",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, o))",
            "                np.multiply(a, (2**o - 1) // (2**n - 1), out=b, dtype=b.dtype)",
            "                b //= 2 ** (o - m)",
            "                return b",
            "            else:",
            "                a = a.astype(_dtype_bits(kind, o, a.dtype.itemsize), copy=False)",
            "                a *= (2**o - 1) // (2**n - 1)",
            "                a //= 2 ** (o - m)",
            "                return a",
            "",
            "    image = np.asarray(image)",
            "    dtypeobj_in = image.dtype",
            "    dtypeobj_out = np.dtype(\"float64\") if dtype is np.floating else np.dtype(dtype)",
            "    dtype_in = dtypeobj_in.type",
            "    dtype_out = dtypeobj_out.type",
            "    kind_in = dtypeobj_in.kind",
            "    kind_out = dtypeobj_out.kind",
            "    itemsize_in = dtypeobj_in.itemsize",
            "    itemsize_out = dtypeobj_out.itemsize",
            "",
            "    # Below, we do an `issubdtype` check.  Its purpose is to find out",
            "    # whether we can get away without doing any image conversion.  This happens",
            "    # when:",
            "    #",
            "    # - the output and input dtypes are the same or",
            "    # - when the output is specified as a type, and the input dtype",
            "    #   is a subclass of that type (e.g. `np.floating` will allow",
            "    #   `float32` and `float64` arrays through)",
            "",
            "    if np.issubdtype(dtype_in, np.obj2sctype(dtype)):",
            "        if force_copy:",
            "            image = image.copy()",
            "        return image",
            "",
            "    if kind_in in \"ui\":",
            "        imin_in = np.iinfo(dtype_in).min",
            "        imax_in = np.iinfo(dtype_in).max",
            "    if kind_out in \"ui\":",
            "        imin_out = np.iinfo(dtype_out).min  # type: ignore",
            "        imax_out = np.iinfo(dtype_out).max  # type: ignore",
            "",
            "    # any -> binary",
            "    if kind_out == \"b\":",
            "        return image > dtype_in(dtype_range[dtype_in][1] / 2)",
            "",
            "    # binary -> any",
            "    if kind_in == \"b\":",
            "        result = image.astype(dtype_out)",
            "        if kind_out != \"f\":",
            "            result *= dtype_out(dtype_range[dtype_out][1])",
            "        return result",
            "",
            "    # float -> any",
            "    if kind_in == \"f\":",
            "        if kind_out == \"f\":",
            "            # float -> float",
            "            return image.astype(dtype_out)",
            "",
            "        if np.min(image) < -1.0 or np.max(image) > 1.0:",
            "            raise ValueError(\"Images of type float must be between -1 and 1.\")",
            "        # floating point -> integer",
            "        # use float type that can represent output integer type",
            "        computation_type = _dtype_itemsize(",
            "            itemsize_out, dtype_in, np.float32, np.float64",
            "        )",
            "",
            "        if not uniform:",
            "            if kind_out == \"u\":",
            "                image_out = np.multiply(image, imax_out, dtype=computation_type)  # type: ignore",
            "            else:",
            "                image_out = np.multiply(",
            "                    image,",
            "                    (imax_out - imin_out) / 2,  # type: ignore",
            "                    dtype=computation_type,",
            "                )",
            "                image_out -= 1.0 / 2.0",
            "            np.rint(image_out, out=image_out)",
            "            np.clip(image_out, imin_out, imax_out, out=image_out)  # type: ignore",
            "        elif kind_out == \"u\":",
            "            image_out = np.multiply(image, imax_out + 1, dtype=computation_type)  # type: ignore",
            "            np.clip(image_out, 0, imax_out, out=image_out)  # type: ignore",
            "        else:",
            "            image_out = np.multiply(",
            "                image,",
            "                (imax_out - imin_out + 1.0) / 2.0,  # type: ignore",
            "                dtype=computation_type,",
            "            )",
            "            np.floor(image_out, out=image_out)",
            "            np.clip(image_out, imin_out, imax_out, out=image_out)  # type: ignore",
            "        return image_out.astype(dtype_out)",
            "",
            "    # signed/unsigned int -> float",
            "    if kind_out == \"f\":",
            "        # use float type that can exactly represent input integers",
            "        computation_type = _dtype_itemsize(",
            "            itemsize_in, dtype_out, np.float32, np.float64",
            "        )",
            "",
            "        if kind_in == \"u\":",
            "            # using np.divide or np.multiply doesn't copy the data",
            "            # until the computation time",
            "            image = np.multiply(image, 1.0 / imax_in, dtype=computation_type)  # type: ignore",
            "            # DirectX uses this conversion also for signed ints",
            "            # if imin_in:",
            "            #     np.maximum(image, -1.0, out=image)",
            "        else:",
            "            image = np.add(image, 0.5, dtype=computation_type)",
            "            image *= 2 / (imax_in - imin_in)  # type: ignore",
            "",
            "        return np.asarray(image, dtype_out)",
            "",
            "    # unsigned int -> signed/unsigned int",
            "    if kind_in == \"u\":",
            "        if kind_out == \"i\":",
            "            # unsigned int -> signed int",
            "            image = _scale(image, 8 * itemsize_in, 8 * itemsize_out - 1)",
            "            return image.view(dtype_out)",
            "        else:",
            "            # unsigned int -> unsigned int",
            "            return _scale(image, 8 * itemsize_in, 8 * itemsize_out)",
            "",
            "    # signed int -> unsigned int",
            "    if kind_out == \"u\":",
            "        image = _scale(image, 8 * itemsize_in - 1, 8 * itemsize_out)",
            "        result = np.empty(image.shape, dtype_out)",
            "        np.maximum(image, 0, out=result, dtype=image.dtype, casting=\"unsafe\")",
            "        return result",
            "",
            "    # signed int -> signed int",
            "    if itemsize_in > itemsize_out:",
            "        return _scale(image, 8 * itemsize_in - 1, 8 * itemsize_out - 1)",
            "",
            "    image = image.astype(_dtype_bits(\"i\", itemsize_out * 8))",
            "    image -= imin_in  # type: ignore",
            "    image = _scale(image, 8 * itemsize_in, 8 * itemsize_out, copy=False)",
            "    image += imin_out  # type: ignore",
            "    return image.astype(dtype_out)",
            "",
            "",
            "def ffmpeg_installed() -> bool:",
            "    if wasm_utils.IS_WASM:",
            "        # TODO: Support ffmpeg in WASM",
            "        return False",
            "",
            "    return shutil.which(\"ffmpeg\") is not None",
            "",
            "",
            "def video_is_playable(video_filepath: str) -> bool:",
            "    \"\"\"Determines if a video is playable in the browser.",
            "",
            "    A video is playable if it has a playable container and codec.",
            "        .mp4 -> h264",
            "        .webm -> vp9",
            "        .ogg -> theora",
            "    \"\"\"",
            "    from ffmpy import FFprobe, FFRuntimeError",
            "",
            "    try:",
            "        container = Path(video_filepath).suffix.lower()",
            "        probe = FFprobe(",
            "            global_options=\"-show_format -show_streams -select_streams v -print_format json\",",
            "            inputs={video_filepath: None},",
            "        )",
            "        output = probe.run(stderr=subprocess.PIPE, stdout=subprocess.PIPE)",
            "        output = json.loads(output[0])",
            "        video_codec = output[\"streams\"][0][\"codec_name\"]",
            "        return (container, video_codec) in [",
            "            (\".mp4\", \"h264\"),",
            "            (\".ogg\", \"theora\"),",
            "            (\".webm\", \"vp9\"),",
            "        ]",
            "    # If anything goes wrong, assume the video can be played to not convert downstream",
            "    except (FFRuntimeError, IndexError, KeyError):",
            "        return True",
            "",
            "",
            "def convert_video_to_playable_mp4(video_path: str) -> str:",
            "    \"\"\"Convert the video to mp4. If something goes wrong return the original video.\"\"\"",
            "    from ffmpy import FFmpeg, FFRuntimeError",
            "",
            "    try:",
            "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:",
            "            output_path = Path(video_path).with_suffix(\".mp4\")",
            "            shutil.copy2(video_path, tmp_file.name)",
            "            # ffmpeg will automatically use h264 codec (playable in browser) when converting to mp4",
            "            ff = FFmpeg(",
            "                inputs={str(tmp_file.name): None},",
            "                outputs={str(output_path): None},",
            "                global_options=\"-y -loglevel quiet\",",
            "            )",
            "            ff.run()",
            "    except FFRuntimeError as e:",
            "        print(f\"Error converting video to browser-playable format {str(e)}\")",
            "        output_path = video_path",
            "    finally:",
            "        # Remove temp file",
            "        os.remove(tmp_file.name)  # type: ignore",
            "    return str(output_path)",
            "",
            "",
            "def get_video_length(video_path: str | Path):",
            "    if wasm_utils.IS_WASM:",
            "        raise wasm_utils.WasmUnsupportedError(",
            "            \"Video duration is not supported in the Wasm mode.\"",
            "        )",
            "    duration = subprocess.check_output(",
            "        [",
            "            \"ffprobe\",",
            "            \"-i\",",
            "            str(video_path),",
            "            \"-show_entries\",",
            "            \"format=duration\",",
            "            \"-v\",",
            "            \"quiet\",",
            "            \"-of\",",
            "            \"csv={}\".format(\"p=0\"),",
            "        ]",
            "    )",
            "    duration_str = duration.decode(\"utf-8\").strip()",
            "    duration_float = float(duration_str)",
            "",
            "    return duration_float"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "239": [
                "move_files_to_cache"
            ],
            "240": [
                "move_files_to_cache"
            ],
            "242": [
                "move_files_to_cache"
            ],
            "246": [
                "move_files_to_cache"
            ]
        },
        "addLocation": []
    },
    "gradio/route_utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import hashlib"
            },
            "2": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import json"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 5,
                "PatchRowcode": "+import shutil"
            },
            "4": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from collections import deque"
            },
            "5": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from dataclasses import dataclass as python_dataclass"
            },
            "6": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from tempfile import NamedTemporaryFile, _TemporaryFileWrapper"
            },
            "7": {
                "beforePatchRowNumber": 545,
                "afterPatchRowNumber": 546,
                "PatchRowcode": "         if self.upload_progress is not None:"
            },
            "8": {
                "beforePatchRowNumber": 546,
                "afterPatchRowNumber": 547,
                "PatchRowcode": "             self.upload_progress.set_done(self.upload_id)  # type: ignore"
            },
            "9": {
                "beforePatchRowNumber": 547,
                "afterPatchRowNumber": 548,
                "PatchRowcode": "         return FormData(self.items)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 549,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 550,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 551,
                "PatchRowcode": "+def move_uploaded_files_to_cache(files: list[str], destinations: list[str]) -> None:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 552,
                "PatchRowcode": "+    for file, dest in zip(files, destinations):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 553,
                "PatchRowcode": "+        shutil.move(file, dest)"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import hashlib",
            "import json",
            "from collections import deque",
            "from dataclasses import dataclass as python_dataclass",
            "from tempfile import NamedTemporaryFile, _TemporaryFileWrapper",
            "from typing import TYPE_CHECKING, AsyncGenerator, BinaryIO, List, Optional, Tuple, Union",
            "",
            "import fastapi",
            "import httpx",
            "import multipart",
            "from gradio_client.documentation import document",
            "from multipart.multipart import parse_options_header",
            "from starlette.datastructures import FormData, Headers, UploadFile",
            "from starlette.formparsers import MultiPartException, MultipartPart",
            "",
            "from gradio import utils",
            "from gradio.data_classes import PredictBody",
            "from gradio.exceptions import Error",
            "from gradio.helpers import EventData",
            "from gradio.state_holder import SessionState",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import Blocks",
            "    from gradio.routes import App",
            "",
            "",
            "class Obj:",
            "    \"\"\"",
            "    Using a class to convert dictionaries into objects. Used by the `Request` class.",
            "    Credit: https://www.geeksforgeeks.org/convert-nested-python-dictionary-to-object/",
            "    \"\"\"",
            "",
            "    def __init__(self, dict_):",
            "        self.__dict__.update(dict_)",
            "        for key, value in dict_.items():",
            "            if isinstance(value, (dict, list)):",
            "                value = Obj(value)",
            "            setattr(self, key, value)",
            "",
            "    def __getitem__(self, item):",
            "        return self.__dict__[item]",
            "",
            "    def __setitem__(self, item, value):",
            "        self.__dict__[item] = value",
            "",
            "    def __iter__(self):",
            "        for key, value in self.__dict__.items():",
            "            if isinstance(value, Obj):",
            "                yield (key, dict(value))",
            "            else:",
            "                yield (key, value)",
            "",
            "    def __contains__(self, item) -> bool:",
            "        if item in self.__dict__:",
            "            return True",
            "        for value in self.__dict__.values():",
            "            if isinstance(value, Obj) and item in value:",
            "                return True",
            "        return False",
            "",
            "    def keys(self):",
            "        return self.__dict__.keys()",
            "",
            "    def values(self):",
            "        return self.__dict__.values()",
            "",
            "    def items(self):",
            "        return self.__dict__.items()",
            "",
            "    def __str__(self) -> str:",
            "        return str(self.__dict__)",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.__dict__)",
            "",
            "",
            "@document()",
            "class Request:",
            "    \"\"\"",
            "    A Gradio request object that can be used to access the request headers, cookies,",
            "    query parameters and other information about the request from within the prediction",
            "    function. The class is a thin wrapper around the fastapi.Request class. Attributes",
            "    of this class include: `headers`, `client`, `query_params`, and `path_params`. If",
            "    auth is enabled, the `username` attribute can be used to get the logged in user.",
            "    Example:",
            "        import gradio as gr",
            "        def echo(text, request: gr.Request):",
            "            if request:",
            "                print(\"Request headers dictionary:\", request.headers)",
            "                print(\"IP address:\", request.client.host)",
            "                print(\"Query parameters:\", dict(request.query_params))",
            "            return text",
            "        io = gr.Interface(echo, \"textbox\", \"textbox\").launch()",
            "    Demos: request_ip_headers",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        request: fastapi.Request | None = None,",
            "        username: str | None = None,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Can be instantiated with either a fastapi.Request or by manually passing in",
            "        attributes (needed for queueing).",
            "        Parameters:",
            "            request: A fastapi.Request",
            "        \"\"\"",
            "        self.request = request",
            "        self.username = username",
            "        self.kwargs: dict = kwargs",
            "",
            "    def dict_to_obj(self, d):",
            "        if isinstance(d, dict):",
            "            return json.loads(json.dumps(d), object_hook=Obj)",
            "        else:",
            "            return d",
            "",
            "    def __getattr__(self, name):",
            "        if self.request:",
            "            return self.dict_to_obj(getattr(self.request, name))",
            "        else:",
            "            try:",
            "                obj = self.kwargs[name]",
            "            except KeyError as ke:",
            "                raise AttributeError(",
            "                    f\"'Request' object has no attribute '{name}'\"",
            "                ) from ke",
            "            return self.dict_to_obj(obj)",
            "",
            "",
            "class FnIndexInferError(Exception):",
            "    pass",
            "",
            "",
            "def infer_fn_index(app: App, api_name: str, body: PredictBody) -> int:",
            "    if body.fn_index is None:",
            "        for i, fn in enumerate(app.get_blocks().dependencies):",
            "            if fn[\"api_name\"] == api_name:",
            "                return i",
            "",
            "        raise FnIndexInferError(f\"Could not infer fn_index for api_name {api_name}.\")",
            "    else:",
            "        return body.fn_index",
            "",
            "",
            "def compile_gr_request(",
            "    app: App,",
            "    body: PredictBody,",
            "    fn_index_inferred: int,",
            "    username: Optional[str],",
            "    request: Optional[fastapi.Request],",
            "):",
            "    # If this fn_index cancels jobs, then the only input we need is the",
            "    # current session hash",
            "    if app.get_blocks().dependencies[fn_index_inferred][\"cancels\"]:",
            "        body.data = [body.session_hash]",
            "    if body.request:",
            "        if body.batched:",
            "            gr_request = [Request(username=username, request=request)]",
            "        else:",
            "            gr_request = Request(username=username, request=body.request)",
            "    else:",
            "        if request is None:",
            "            raise ValueError(\"request must be provided if body.request is None\")",
            "        gr_request = Request(username=username, request=request)",
            "",
            "    return gr_request",
            "",
            "",
            "def restore_session_state(app: App, body: PredictBody):",
            "    event_id = body.event_id",
            "    session_hash = getattr(body, \"session_hash\", None)",
            "    if session_hash is not None:",
            "        session_state = app.state_holder[session_hash]",
            "        # The should_reset set keeps track of the fn_indices",
            "        # that have been cancelled. When a job is cancelled,",
            "        # the /reset route will mark the jobs as having been reset.",
            "        # That way if the cancel job finishes BEFORE the job being cancelled",
            "        # the job being cancelled will not overwrite the state of the iterator.",
            "        if event_id is None:",
            "            iterator = None",
            "        elif event_id in app.iterators_to_reset:",
            "            iterator = None",
            "            app.iterators_to_reset.remove(event_id)",
            "        else:",
            "            iterator = app.iterators.get(event_id)",
            "    else:",
            "        session_state = SessionState(app.get_blocks())",
            "        iterator = None",
            "",
            "    return session_state, iterator",
            "",
            "",
            "def prepare_event_data(",
            "    blocks: Blocks,",
            "    body: PredictBody,",
            ") -> EventData:",
            "    target = body.trigger_id",
            "    event_data = EventData(",
            "        blocks.blocks.get(target) if target else None,",
            "        body.event_data,",
            "    )",
            "    return event_data",
            "",
            "",
            "async def call_process_api(",
            "    app: App,",
            "    body: PredictBody,",
            "    gr_request: Union[Request, list[Request]],",
            "    fn_index_inferred: int,",
            "):",
            "    session_state, iterator = restore_session_state(app=app, body=body)",
            "",
            "    dependency = app.get_blocks().dependencies[fn_index_inferred]",
            "    event_data = prepare_event_data(app.get_blocks(), body)",
            "    event_id = body.event_id",
            "",
            "    session_hash = getattr(body, \"session_hash\", None)",
            "    inputs = body.data",
            "",
            "    batch_in_single_out = not body.batched and dependency[\"batch\"]",
            "    if batch_in_single_out:",
            "        inputs = [inputs]",
            "",
            "    try:",
            "        with utils.MatplotlibBackendMananger():",
            "            output = await app.get_blocks().process_api(",
            "                fn_index=fn_index_inferred,",
            "                inputs=inputs,",
            "                request=gr_request,",
            "                state=session_state,",
            "                iterator=iterator,",
            "                session_hash=session_hash,",
            "                event_id=event_id,",
            "                event_data=event_data,",
            "                in_event_listener=True,",
            "            )",
            "        iterator = output.pop(\"iterator\", None)",
            "        if event_id is not None:",
            "            app.iterators[event_id] = iterator  # type: ignore",
            "        if isinstance(output, Error):",
            "            raise output",
            "    except BaseException:",
            "        iterator = app.iterators.get(event_id) if event_id is not None else None",
            "        if iterator is not None:  # close off any streams that are still open",
            "            run_id = id(iterator)",
            "            pending_streams: dict[int, list] = (",
            "                app.get_blocks().pending_streams[session_hash].get(run_id, {})",
            "            )",
            "            for stream in pending_streams.values():",
            "                stream.append(None)",
            "        raise",
            "",
            "    if batch_in_single_out:",
            "        output[\"data\"] = output[\"data\"][0]",
            "",
            "    return output",
            "",
            "",
            "def strip_url(orig_url: str) -> str:",
            "    \"\"\"",
            "    Strips the query parameters and trailing slash from a URL.",
            "    \"\"\"",
            "    parsed_url = httpx.URL(orig_url)",
            "    stripped_url = parsed_url.copy_with(query=None)",
            "    stripped_url = str(stripped_url)",
            "    return stripped_url.rstrip(\"/\")",
            "",
            "",
            "def _user_safe_decode(src: bytes, codec: str) -> str:",
            "    try:",
            "        return src.decode(codec)",
            "    except (UnicodeDecodeError, LookupError):",
            "        return src.decode(\"latin-1\")",
            "",
            "",
            "class GradioUploadFile(UploadFile):",
            "    \"\"\"UploadFile with a sha attribute.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        file: BinaryIO,",
            "        *,",
            "        size: int | None = None,",
            "        filename: str | None = None,",
            "        headers: Headers | None = None,",
            "    ) -> None:",
            "        super().__init__(file, size=size, filename=filename, headers=headers)",
            "        self.sha = hashlib.sha1()",
            "",
            "",
            "@python_dataclass(frozen=True)",
            "class FileUploadProgressUnit:",
            "    filename: str",
            "    chunk_size: int",
            "",
            "",
            "@python_dataclass",
            "class FileUploadProgressTracker:",
            "    deque: deque[FileUploadProgressUnit]",
            "    is_done: bool",
            "",
            "",
            "class FileUploadProgressNotTrackedError(Exception):",
            "    pass",
            "",
            "",
            "class FileUploadProgressNotQueuedError(Exception):",
            "    pass",
            "",
            "",
            "class FileUploadProgress:",
            "    def __init__(self) -> None:",
            "        self._statuses: dict[str, FileUploadProgressTracker] = {}",
            "",
            "    def track(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            self._statuses[upload_id] = FileUploadProgressTracker(deque(), False)",
            "",
            "    def append(self, upload_id: str, filename: str, message_bytes: bytes):",
            "        if upload_id not in self._statuses:",
            "            self.track(upload_id)",
            "        queue = self._statuses[upload_id].deque",
            "",
            "        if len(queue) == 0:",
            "            queue.append(FileUploadProgressUnit(filename, len(message_bytes)))",
            "        else:",
            "            last_unit = queue.popleft()",
            "            if last_unit.filename != filename:",
            "                queue.append(FileUploadProgressUnit(filename, len(message_bytes)))",
            "            else:",
            "                queue.append(",
            "                    FileUploadProgressUnit(",
            "                        filename,",
            "                        last_unit.chunk_size + len(message_bytes),",
            "                    )",
            "                )",
            "",
            "    def set_done(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            self.track(upload_id)",
            "        self._statuses[upload_id].is_done = True",
            "",
            "    def is_done(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            raise FileUploadProgressNotTrackedError()",
            "        return self._statuses[upload_id].is_done",
            "",
            "    def stop_tracking(self, upload_id: str):",
            "        if upload_id in self._statuses:",
            "            del self._statuses[upload_id]",
            "",
            "    def pop(self, upload_id: str) -> FileUploadProgressUnit:",
            "        if upload_id not in self._statuses:",
            "            raise FileUploadProgressNotTrackedError()",
            "        try:",
            "            return self._statuses[upload_id].deque.pop()",
            "        except IndexError as e:",
            "            raise FileUploadProgressNotQueuedError() from e",
            "",
            "",
            "class GradioMultiPartParser:",
            "    \"\"\"Vendored from starlette.MultipartParser.",
            "",
            "    Thanks starlette!",
            "",
            "    Made the following modifications",
            "        - Use GradioUploadFile instead of UploadFile",
            "        - Use NamedTemporaryFile instead of SpooledTemporaryFile",
            "        - Compute hash of data as the request is streamed",
            "",
            "    \"\"\"",
            "",
            "    max_file_size = 1024 * 1024",
            "",
            "    def __init__(",
            "        self,",
            "        headers: Headers,",
            "        stream: AsyncGenerator[bytes, None],",
            "        *,",
            "        max_files: Union[int, float] = 1000,",
            "        max_fields: Union[int, float] = 1000,",
            "        upload_id: str | None = None,",
            "        upload_progress: FileUploadProgress | None = None,",
            "    ) -> None:",
            "        assert (",
            "            multipart is not None",
            "        ), \"The `python-multipart` library must be installed to use form parsing.\"",
            "        self.headers = headers",
            "        self.stream = stream",
            "        self.max_files = max_files",
            "        self.max_fields = max_fields",
            "        self.items: List[Tuple[str, Union[str, UploadFile]]] = []",
            "        self.upload_id = upload_id",
            "        self.upload_progress = upload_progress",
            "        self._current_files = 0",
            "        self._current_fields = 0",
            "        self._current_partial_header_name: bytes = b\"\"",
            "        self._current_partial_header_value: bytes = b\"\"",
            "        self._current_part = MultipartPart()",
            "        self._charset = \"\"",
            "        self._file_parts_to_write: List[Tuple[MultipartPart, bytes]] = []",
            "        self._file_parts_to_finish: List[MultipartPart] = []",
            "        self._files_to_close_on_error: List[_TemporaryFileWrapper] = []",
            "",
            "    def on_part_begin(self) -> None:",
            "        self._current_part = MultipartPart()",
            "",
            "    def on_part_data(self, data: bytes, start: int, end: int) -> None:",
            "        message_bytes = data[start:end]",
            "        if self.upload_progress is not None:",
            "            self.upload_progress.append(",
            "                self.upload_id,  # type: ignore",
            "                self._current_part.file.filename,  # type: ignore",
            "                message_bytes,",
            "            )",
            "        if self._current_part.file is None:",
            "            self._current_part.data += message_bytes",
            "        else:",
            "            self._file_parts_to_write.append((self._current_part, message_bytes))",
            "",
            "    def on_part_end(self) -> None:",
            "        if self._current_part.file is None:",
            "            self.items.append(",
            "                (",
            "                    self._current_part.field_name,",
            "                    _user_safe_decode(self._current_part.data, self._charset),",
            "                )",
            "            )",
            "        else:",
            "            self._file_parts_to_finish.append(self._current_part)",
            "            # The file can be added to the items right now even though it's not",
            "            # finished yet, because it will be finished in the `parse()` method, before",
            "            # self.items is used in the return value.",
            "            self.items.append((self._current_part.field_name, self._current_part.file))",
            "",
            "    def on_header_field(self, data: bytes, start: int, end: int) -> None:",
            "        self._current_partial_header_name += data[start:end]",
            "",
            "    def on_header_value(self, data: bytes, start: int, end: int) -> None:",
            "        self._current_partial_header_value += data[start:end]",
            "",
            "    def on_header_end(self) -> None:",
            "        field = self._current_partial_header_name.lower()",
            "        if field == b\"content-disposition\":",
            "            self._current_part.content_disposition = self._current_partial_header_value",
            "        self._current_part.item_headers.append(",
            "            (field, self._current_partial_header_value)",
            "        )",
            "        self._current_partial_header_name = b\"\"",
            "        self._current_partial_header_value = b\"\"",
            "",
            "    def on_headers_finished(self) -> None:",
            "        disposition, options = parse_options_header(",
            "            self._current_part.content_disposition",
            "        )",
            "        try:",
            "            self._current_part.field_name = _user_safe_decode(",
            "                options[b\"name\"], self._charset",
            "            )",
            "        except KeyError as e:",
            "            raise MultiPartException(",
            "                'The Content-Disposition header field \"name\" must be ' \"provided.\"",
            "            ) from e",
            "        if b\"filename\" in options:",
            "            self._current_files += 1",
            "            if self._current_files > self.max_files:",
            "                raise MultiPartException(",
            "                    f\"Too many files. Maximum number of files is {self.max_files}.\"",
            "                )",
            "            filename = _user_safe_decode(options[b\"filename\"], self._charset)",
            "            tempfile = NamedTemporaryFile(delete=False)",
            "            self._files_to_close_on_error.append(tempfile)",
            "            self._current_part.file = GradioUploadFile(",
            "                file=tempfile,  # type: ignore[arg-type]",
            "                size=0,",
            "                filename=filename,",
            "                headers=Headers(raw=self._current_part.item_headers),",
            "            )",
            "        else:",
            "            self._current_fields += 1",
            "            if self._current_fields > self.max_fields:",
            "                raise MultiPartException(",
            "                    f\"Too many fields. Maximum number of fields is {self.max_fields}.\"",
            "                )",
            "            self._current_part.file = None",
            "",
            "    def on_end(self) -> None:",
            "        pass",
            "",
            "    async def parse(self) -> FormData:",
            "        # Parse the Content-Type header to get the multipart boundary.",
            "        _, params = parse_options_header(self.headers[\"Content-Type\"])",
            "        charset = params.get(b\"charset\", \"utf-8\")",
            "        if isinstance(charset, bytes):",
            "            charset = charset.decode(\"latin-1\")",
            "        self._charset = charset",
            "        try:",
            "            boundary = params[b\"boundary\"]",
            "        except KeyError as e:",
            "            raise MultiPartException(\"Missing boundary in multipart.\") from e",
            "",
            "        # Callbacks dictionary.",
            "        callbacks = {",
            "            \"on_part_begin\": self.on_part_begin,",
            "            \"on_part_data\": self.on_part_data,",
            "            \"on_part_end\": self.on_part_end,",
            "            \"on_header_field\": self.on_header_field,",
            "            \"on_header_value\": self.on_header_value,",
            "            \"on_header_end\": self.on_header_end,",
            "            \"on_headers_finished\": self.on_headers_finished,",
            "            \"on_end\": self.on_end,",
            "        }",
            "",
            "        # Create the parser.",
            "        parser = multipart.MultipartParser(boundary, callbacks)",
            "        try:",
            "            # Feed the parser with data from the request.",
            "            async for chunk in self.stream:",
            "                parser.write(chunk)",
            "                # Write file data, it needs to use await with the UploadFile methods",
            "                # that call the corresponding file methods *in a threadpool*,",
            "                # otherwise, if they were called directly in the callback methods above",
            "                # (regular, non-async functions), that would block the event loop in",
            "                # the main thread.",
            "                for part, data in self._file_parts_to_write:",
            "                    assert part.file  # for type checkers",
            "                    await part.file.write(data)",
            "                    part.file.sha.update(data)  # type: ignore",
            "                for part in self._file_parts_to_finish:",
            "                    assert part.file  # for type checkers",
            "                    await part.file.seek(0)",
            "                self._file_parts_to_write.clear()",
            "                self._file_parts_to_finish.clear()",
            "        except MultiPartException as exc:",
            "            # Close all the files if there was an error.",
            "            for file in self._files_to_close_on_error:",
            "                file.close()",
            "            raise exc",
            "",
            "        parser.finalize()",
            "        if self.upload_progress is not None:",
            "            self.upload_progress.set_done(self.upload_id)  # type: ignore",
            "        return FormData(self.items)"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import hashlib",
            "import json",
            "import shutil",
            "from collections import deque",
            "from dataclasses import dataclass as python_dataclass",
            "from tempfile import NamedTemporaryFile, _TemporaryFileWrapper",
            "from typing import TYPE_CHECKING, AsyncGenerator, BinaryIO, List, Optional, Tuple, Union",
            "",
            "import fastapi",
            "import httpx",
            "import multipart",
            "from gradio_client.documentation import document",
            "from multipart.multipart import parse_options_header",
            "from starlette.datastructures import FormData, Headers, UploadFile",
            "from starlette.formparsers import MultiPartException, MultipartPart",
            "",
            "from gradio import utils",
            "from gradio.data_classes import PredictBody",
            "from gradio.exceptions import Error",
            "from gradio.helpers import EventData",
            "from gradio.state_holder import SessionState",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import Blocks",
            "    from gradio.routes import App",
            "",
            "",
            "class Obj:",
            "    \"\"\"",
            "    Using a class to convert dictionaries into objects. Used by the `Request` class.",
            "    Credit: https://www.geeksforgeeks.org/convert-nested-python-dictionary-to-object/",
            "    \"\"\"",
            "",
            "    def __init__(self, dict_):",
            "        self.__dict__.update(dict_)",
            "        for key, value in dict_.items():",
            "            if isinstance(value, (dict, list)):",
            "                value = Obj(value)",
            "            setattr(self, key, value)",
            "",
            "    def __getitem__(self, item):",
            "        return self.__dict__[item]",
            "",
            "    def __setitem__(self, item, value):",
            "        self.__dict__[item] = value",
            "",
            "    def __iter__(self):",
            "        for key, value in self.__dict__.items():",
            "            if isinstance(value, Obj):",
            "                yield (key, dict(value))",
            "            else:",
            "                yield (key, value)",
            "",
            "    def __contains__(self, item) -> bool:",
            "        if item in self.__dict__:",
            "            return True",
            "        for value in self.__dict__.values():",
            "            if isinstance(value, Obj) and item in value:",
            "                return True",
            "        return False",
            "",
            "    def keys(self):",
            "        return self.__dict__.keys()",
            "",
            "    def values(self):",
            "        return self.__dict__.values()",
            "",
            "    def items(self):",
            "        return self.__dict__.items()",
            "",
            "    def __str__(self) -> str:",
            "        return str(self.__dict__)",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.__dict__)",
            "",
            "",
            "@document()",
            "class Request:",
            "    \"\"\"",
            "    A Gradio request object that can be used to access the request headers, cookies,",
            "    query parameters and other information about the request from within the prediction",
            "    function. The class is a thin wrapper around the fastapi.Request class. Attributes",
            "    of this class include: `headers`, `client`, `query_params`, and `path_params`. If",
            "    auth is enabled, the `username` attribute can be used to get the logged in user.",
            "    Example:",
            "        import gradio as gr",
            "        def echo(text, request: gr.Request):",
            "            if request:",
            "                print(\"Request headers dictionary:\", request.headers)",
            "                print(\"IP address:\", request.client.host)",
            "                print(\"Query parameters:\", dict(request.query_params))",
            "            return text",
            "        io = gr.Interface(echo, \"textbox\", \"textbox\").launch()",
            "    Demos: request_ip_headers",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        request: fastapi.Request | None = None,",
            "        username: str | None = None,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Can be instantiated with either a fastapi.Request or by manually passing in",
            "        attributes (needed for queueing).",
            "        Parameters:",
            "            request: A fastapi.Request",
            "        \"\"\"",
            "        self.request = request",
            "        self.username = username",
            "        self.kwargs: dict = kwargs",
            "",
            "    def dict_to_obj(self, d):",
            "        if isinstance(d, dict):",
            "            return json.loads(json.dumps(d), object_hook=Obj)",
            "        else:",
            "            return d",
            "",
            "    def __getattr__(self, name):",
            "        if self.request:",
            "            return self.dict_to_obj(getattr(self.request, name))",
            "        else:",
            "            try:",
            "                obj = self.kwargs[name]",
            "            except KeyError as ke:",
            "                raise AttributeError(",
            "                    f\"'Request' object has no attribute '{name}'\"",
            "                ) from ke",
            "            return self.dict_to_obj(obj)",
            "",
            "",
            "class FnIndexInferError(Exception):",
            "    pass",
            "",
            "",
            "def infer_fn_index(app: App, api_name: str, body: PredictBody) -> int:",
            "    if body.fn_index is None:",
            "        for i, fn in enumerate(app.get_blocks().dependencies):",
            "            if fn[\"api_name\"] == api_name:",
            "                return i",
            "",
            "        raise FnIndexInferError(f\"Could not infer fn_index for api_name {api_name}.\")",
            "    else:",
            "        return body.fn_index",
            "",
            "",
            "def compile_gr_request(",
            "    app: App,",
            "    body: PredictBody,",
            "    fn_index_inferred: int,",
            "    username: Optional[str],",
            "    request: Optional[fastapi.Request],",
            "):",
            "    # If this fn_index cancels jobs, then the only input we need is the",
            "    # current session hash",
            "    if app.get_blocks().dependencies[fn_index_inferred][\"cancels\"]:",
            "        body.data = [body.session_hash]",
            "    if body.request:",
            "        if body.batched:",
            "            gr_request = [Request(username=username, request=request)]",
            "        else:",
            "            gr_request = Request(username=username, request=body.request)",
            "    else:",
            "        if request is None:",
            "            raise ValueError(\"request must be provided if body.request is None\")",
            "        gr_request = Request(username=username, request=request)",
            "",
            "    return gr_request",
            "",
            "",
            "def restore_session_state(app: App, body: PredictBody):",
            "    event_id = body.event_id",
            "    session_hash = getattr(body, \"session_hash\", None)",
            "    if session_hash is not None:",
            "        session_state = app.state_holder[session_hash]",
            "        # The should_reset set keeps track of the fn_indices",
            "        # that have been cancelled. When a job is cancelled,",
            "        # the /reset route will mark the jobs as having been reset.",
            "        # That way if the cancel job finishes BEFORE the job being cancelled",
            "        # the job being cancelled will not overwrite the state of the iterator.",
            "        if event_id is None:",
            "            iterator = None",
            "        elif event_id in app.iterators_to_reset:",
            "            iterator = None",
            "            app.iterators_to_reset.remove(event_id)",
            "        else:",
            "            iterator = app.iterators.get(event_id)",
            "    else:",
            "        session_state = SessionState(app.get_blocks())",
            "        iterator = None",
            "",
            "    return session_state, iterator",
            "",
            "",
            "def prepare_event_data(",
            "    blocks: Blocks,",
            "    body: PredictBody,",
            ") -> EventData:",
            "    target = body.trigger_id",
            "    event_data = EventData(",
            "        blocks.blocks.get(target) if target else None,",
            "        body.event_data,",
            "    )",
            "    return event_data",
            "",
            "",
            "async def call_process_api(",
            "    app: App,",
            "    body: PredictBody,",
            "    gr_request: Union[Request, list[Request]],",
            "    fn_index_inferred: int,",
            "):",
            "    session_state, iterator = restore_session_state(app=app, body=body)",
            "",
            "    dependency = app.get_blocks().dependencies[fn_index_inferred]",
            "    event_data = prepare_event_data(app.get_blocks(), body)",
            "    event_id = body.event_id",
            "",
            "    session_hash = getattr(body, \"session_hash\", None)",
            "    inputs = body.data",
            "",
            "    batch_in_single_out = not body.batched and dependency[\"batch\"]",
            "    if batch_in_single_out:",
            "        inputs = [inputs]",
            "",
            "    try:",
            "        with utils.MatplotlibBackendMananger():",
            "            output = await app.get_blocks().process_api(",
            "                fn_index=fn_index_inferred,",
            "                inputs=inputs,",
            "                request=gr_request,",
            "                state=session_state,",
            "                iterator=iterator,",
            "                session_hash=session_hash,",
            "                event_id=event_id,",
            "                event_data=event_data,",
            "                in_event_listener=True,",
            "            )",
            "        iterator = output.pop(\"iterator\", None)",
            "        if event_id is not None:",
            "            app.iterators[event_id] = iterator  # type: ignore",
            "        if isinstance(output, Error):",
            "            raise output",
            "    except BaseException:",
            "        iterator = app.iterators.get(event_id) if event_id is not None else None",
            "        if iterator is not None:  # close off any streams that are still open",
            "            run_id = id(iterator)",
            "            pending_streams: dict[int, list] = (",
            "                app.get_blocks().pending_streams[session_hash].get(run_id, {})",
            "            )",
            "            for stream in pending_streams.values():",
            "                stream.append(None)",
            "        raise",
            "",
            "    if batch_in_single_out:",
            "        output[\"data\"] = output[\"data\"][0]",
            "",
            "    return output",
            "",
            "",
            "def strip_url(orig_url: str) -> str:",
            "    \"\"\"",
            "    Strips the query parameters and trailing slash from a URL.",
            "    \"\"\"",
            "    parsed_url = httpx.URL(orig_url)",
            "    stripped_url = parsed_url.copy_with(query=None)",
            "    stripped_url = str(stripped_url)",
            "    return stripped_url.rstrip(\"/\")",
            "",
            "",
            "def _user_safe_decode(src: bytes, codec: str) -> str:",
            "    try:",
            "        return src.decode(codec)",
            "    except (UnicodeDecodeError, LookupError):",
            "        return src.decode(\"latin-1\")",
            "",
            "",
            "class GradioUploadFile(UploadFile):",
            "    \"\"\"UploadFile with a sha attribute.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        file: BinaryIO,",
            "        *,",
            "        size: int | None = None,",
            "        filename: str | None = None,",
            "        headers: Headers | None = None,",
            "    ) -> None:",
            "        super().__init__(file, size=size, filename=filename, headers=headers)",
            "        self.sha = hashlib.sha1()",
            "",
            "",
            "@python_dataclass(frozen=True)",
            "class FileUploadProgressUnit:",
            "    filename: str",
            "    chunk_size: int",
            "",
            "",
            "@python_dataclass",
            "class FileUploadProgressTracker:",
            "    deque: deque[FileUploadProgressUnit]",
            "    is_done: bool",
            "",
            "",
            "class FileUploadProgressNotTrackedError(Exception):",
            "    pass",
            "",
            "",
            "class FileUploadProgressNotQueuedError(Exception):",
            "    pass",
            "",
            "",
            "class FileUploadProgress:",
            "    def __init__(self) -> None:",
            "        self._statuses: dict[str, FileUploadProgressTracker] = {}",
            "",
            "    def track(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            self._statuses[upload_id] = FileUploadProgressTracker(deque(), False)",
            "",
            "    def append(self, upload_id: str, filename: str, message_bytes: bytes):",
            "        if upload_id not in self._statuses:",
            "            self.track(upload_id)",
            "        queue = self._statuses[upload_id].deque",
            "",
            "        if len(queue) == 0:",
            "            queue.append(FileUploadProgressUnit(filename, len(message_bytes)))",
            "        else:",
            "            last_unit = queue.popleft()",
            "            if last_unit.filename != filename:",
            "                queue.append(FileUploadProgressUnit(filename, len(message_bytes)))",
            "            else:",
            "                queue.append(",
            "                    FileUploadProgressUnit(",
            "                        filename,",
            "                        last_unit.chunk_size + len(message_bytes),",
            "                    )",
            "                )",
            "",
            "    def set_done(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            self.track(upload_id)",
            "        self._statuses[upload_id].is_done = True",
            "",
            "    def is_done(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            raise FileUploadProgressNotTrackedError()",
            "        return self._statuses[upload_id].is_done",
            "",
            "    def stop_tracking(self, upload_id: str):",
            "        if upload_id in self._statuses:",
            "            del self._statuses[upload_id]",
            "",
            "    def pop(self, upload_id: str) -> FileUploadProgressUnit:",
            "        if upload_id not in self._statuses:",
            "            raise FileUploadProgressNotTrackedError()",
            "        try:",
            "            return self._statuses[upload_id].deque.pop()",
            "        except IndexError as e:",
            "            raise FileUploadProgressNotQueuedError() from e",
            "",
            "",
            "class GradioMultiPartParser:",
            "    \"\"\"Vendored from starlette.MultipartParser.",
            "",
            "    Thanks starlette!",
            "",
            "    Made the following modifications",
            "        - Use GradioUploadFile instead of UploadFile",
            "        - Use NamedTemporaryFile instead of SpooledTemporaryFile",
            "        - Compute hash of data as the request is streamed",
            "",
            "    \"\"\"",
            "",
            "    max_file_size = 1024 * 1024",
            "",
            "    def __init__(",
            "        self,",
            "        headers: Headers,",
            "        stream: AsyncGenerator[bytes, None],",
            "        *,",
            "        max_files: Union[int, float] = 1000,",
            "        max_fields: Union[int, float] = 1000,",
            "        upload_id: str | None = None,",
            "        upload_progress: FileUploadProgress | None = None,",
            "    ) -> None:",
            "        assert (",
            "            multipart is not None",
            "        ), \"The `python-multipart` library must be installed to use form parsing.\"",
            "        self.headers = headers",
            "        self.stream = stream",
            "        self.max_files = max_files",
            "        self.max_fields = max_fields",
            "        self.items: List[Tuple[str, Union[str, UploadFile]]] = []",
            "        self.upload_id = upload_id",
            "        self.upload_progress = upload_progress",
            "        self._current_files = 0",
            "        self._current_fields = 0",
            "        self._current_partial_header_name: bytes = b\"\"",
            "        self._current_partial_header_value: bytes = b\"\"",
            "        self._current_part = MultipartPart()",
            "        self._charset = \"\"",
            "        self._file_parts_to_write: List[Tuple[MultipartPart, bytes]] = []",
            "        self._file_parts_to_finish: List[MultipartPart] = []",
            "        self._files_to_close_on_error: List[_TemporaryFileWrapper] = []",
            "",
            "    def on_part_begin(self) -> None:",
            "        self._current_part = MultipartPart()",
            "",
            "    def on_part_data(self, data: bytes, start: int, end: int) -> None:",
            "        message_bytes = data[start:end]",
            "        if self.upload_progress is not None:",
            "            self.upload_progress.append(",
            "                self.upload_id,  # type: ignore",
            "                self._current_part.file.filename,  # type: ignore",
            "                message_bytes,",
            "            )",
            "        if self._current_part.file is None:",
            "            self._current_part.data += message_bytes",
            "        else:",
            "            self._file_parts_to_write.append((self._current_part, message_bytes))",
            "",
            "    def on_part_end(self) -> None:",
            "        if self._current_part.file is None:",
            "            self.items.append(",
            "                (",
            "                    self._current_part.field_name,",
            "                    _user_safe_decode(self._current_part.data, self._charset),",
            "                )",
            "            )",
            "        else:",
            "            self._file_parts_to_finish.append(self._current_part)",
            "            # The file can be added to the items right now even though it's not",
            "            # finished yet, because it will be finished in the `parse()` method, before",
            "            # self.items is used in the return value.",
            "            self.items.append((self._current_part.field_name, self._current_part.file))",
            "",
            "    def on_header_field(self, data: bytes, start: int, end: int) -> None:",
            "        self._current_partial_header_name += data[start:end]",
            "",
            "    def on_header_value(self, data: bytes, start: int, end: int) -> None:",
            "        self._current_partial_header_value += data[start:end]",
            "",
            "    def on_header_end(self) -> None:",
            "        field = self._current_partial_header_name.lower()",
            "        if field == b\"content-disposition\":",
            "            self._current_part.content_disposition = self._current_partial_header_value",
            "        self._current_part.item_headers.append(",
            "            (field, self._current_partial_header_value)",
            "        )",
            "        self._current_partial_header_name = b\"\"",
            "        self._current_partial_header_value = b\"\"",
            "",
            "    def on_headers_finished(self) -> None:",
            "        disposition, options = parse_options_header(",
            "            self._current_part.content_disposition",
            "        )",
            "        try:",
            "            self._current_part.field_name = _user_safe_decode(",
            "                options[b\"name\"], self._charset",
            "            )",
            "        except KeyError as e:",
            "            raise MultiPartException(",
            "                'The Content-Disposition header field \"name\" must be ' \"provided.\"",
            "            ) from e",
            "        if b\"filename\" in options:",
            "            self._current_files += 1",
            "            if self._current_files > self.max_files:",
            "                raise MultiPartException(",
            "                    f\"Too many files. Maximum number of files is {self.max_files}.\"",
            "                )",
            "            filename = _user_safe_decode(options[b\"filename\"], self._charset)",
            "            tempfile = NamedTemporaryFile(delete=False)",
            "            self._files_to_close_on_error.append(tempfile)",
            "            self._current_part.file = GradioUploadFile(",
            "                file=tempfile,  # type: ignore[arg-type]",
            "                size=0,",
            "                filename=filename,",
            "                headers=Headers(raw=self._current_part.item_headers),",
            "            )",
            "        else:",
            "            self._current_fields += 1",
            "            if self._current_fields > self.max_fields:",
            "                raise MultiPartException(",
            "                    f\"Too many fields. Maximum number of fields is {self.max_fields}.\"",
            "                )",
            "            self._current_part.file = None",
            "",
            "    def on_end(self) -> None:",
            "        pass",
            "",
            "    async def parse(self) -> FormData:",
            "        # Parse the Content-Type header to get the multipart boundary.",
            "        _, params = parse_options_header(self.headers[\"Content-Type\"])",
            "        charset = params.get(b\"charset\", \"utf-8\")",
            "        if isinstance(charset, bytes):",
            "            charset = charset.decode(\"latin-1\")",
            "        self._charset = charset",
            "        try:",
            "            boundary = params[b\"boundary\"]",
            "        except KeyError as e:",
            "            raise MultiPartException(\"Missing boundary in multipart.\") from e",
            "",
            "        # Callbacks dictionary.",
            "        callbacks = {",
            "            \"on_part_begin\": self.on_part_begin,",
            "            \"on_part_data\": self.on_part_data,",
            "            \"on_part_end\": self.on_part_end,",
            "            \"on_header_field\": self.on_header_field,",
            "            \"on_header_value\": self.on_header_value,",
            "            \"on_header_end\": self.on_header_end,",
            "            \"on_headers_finished\": self.on_headers_finished,",
            "            \"on_end\": self.on_end,",
            "        }",
            "",
            "        # Create the parser.",
            "        parser = multipart.MultipartParser(boundary, callbacks)",
            "        try:",
            "            # Feed the parser with data from the request.",
            "            async for chunk in self.stream:",
            "                parser.write(chunk)",
            "                # Write file data, it needs to use await with the UploadFile methods",
            "                # that call the corresponding file methods *in a threadpool*,",
            "                # otherwise, if they were called directly in the callback methods above",
            "                # (regular, non-async functions), that would block the event loop in",
            "                # the main thread.",
            "                for part, data in self._file_parts_to_write:",
            "                    assert part.file  # for type checkers",
            "                    await part.file.write(data)",
            "                    part.file.sha.update(data)  # type: ignore",
            "                for part in self._file_parts_to_finish:",
            "                    assert part.file  # for type checkers",
            "                    await part.file.seek(0)",
            "                self._file_parts_to_write.clear()",
            "                self._file_parts_to_finish.clear()",
            "        except MultiPartException as exc:",
            "            # Close all the files if there was an error.",
            "            for file in self._files_to_close_on_error:",
            "                file.close()",
            "            raise exc",
            "",
            "        parser.finalize()",
            "        if self.upload_progress is not None:",
            "            self.upload_progress.set_done(self.upload_id)  # type: ignore",
            "        return FormData(self.items)",
            "",
            "",
            "def move_uploaded_files_to_cache(files: list[str], destinations: list[str]) -> None:",
            "    for file, dest in zip(files, destinations):",
            "        shutil.move(file, dest)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.urllib3.packages.rfc3986"
        ]
    }
}