{
    "superset/charts/commands/importers/v1/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from flask import g"
            },
            "1": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from sqlalchemy.orm import Session"
            },
            "2": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+from superset import security_manager"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+from superset.commands.exceptions import ImportFailedError"
            },
            "5": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from superset.models.slice import Slice"
            },
            "6": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " def import_chart("
            },
            "9": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 30,
                "PatchRowcode": "     session: Session, config: Dict[str, Any], overwrite: bool = False"
            },
            "10": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " ) -> Slice:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+    can_write = security_manager.can_access(\"can_write\", \"Chart\")"
            },
            "12": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "     existing = session.query(Slice).filter_by(uuid=config[\"uuid\"]).first()"
            },
            "13": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "     if existing:"
            },
            "14": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not overwrite:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+        if not overwrite or not can_write:"
            },
            "16": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "             return existing"
            },
            "17": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "         config[\"id\"] = existing.id"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+    elif not can_write:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+        raise ImportFailedError("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+            \"Chart doesn't exist and user doesn't have permission to create charts\""
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+        )"
            },
            "22": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "     # TODO (betodealmeida): move this logic to import_from_dict"
            },
            "24": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "     config[\"params\"] = json.dumps(config[\"params\"])"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import json",
            "from typing import Any, Dict",
            "",
            "from flask import g",
            "from sqlalchemy.orm import Session",
            "",
            "from superset.models.slice import Slice",
            "",
            "",
            "def import_chart(",
            "    session: Session, config: Dict[str, Any], overwrite: bool = False",
            ") -> Slice:",
            "    existing = session.query(Slice).filter_by(uuid=config[\"uuid\"]).first()",
            "    if existing:",
            "        if not overwrite:",
            "            return existing",
            "        config[\"id\"] = existing.id",
            "",
            "    # TODO (betodealmeida): move this logic to import_from_dict",
            "    config[\"params\"] = json.dumps(config[\"params\"])",
            "",
            "    chart = Slice.import_from_dict(session, config, recursive=False)",
            "    if chart.id is None:",
            "        session.flush()",
            "",
            "    if hasattr(g, \"user\") and g.user:",
            "        chart.owners.append(g.user)",
            "",
            "    return chart"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import json",
            "from typing import Any, Dict",
            "",
            "from flask import g",
            "from sqlalchemy.orm import Session",
            "",
            "from superset import security_manager",
            "from superset.commands.exceptions import ImportFailedError",
            "from superset.models.slice import Slice",
            "",
            "",
            "def import_chart(",
            "    session: Session, config: Dict[str, Any], overwrite: bool = False",
            ") -> Slice:",
            "    can_write = security_manager.can_access(\"can_write\", \"Chart\")",
            "    existing = session.query(Slice).filter_by(uuid=config[\"uuid\"]).first()",
            "    if existing:",
            "        if not overwrite or not can_write:",
            "            return existing",
            "        config[\"id\"] = existing.id",
            "    elif not can_write:",
            "        raise ImportFailedError(",
            "            \"Chart doesn't exist and user doesn't have permission to create charts\"",
            "        )",
            "",
            "    # TODO (betodealmeida): move this logic to import_from_dict",
            "    config[\"params\"] = json.dumps(config[\"params\"])",
            "",
            "    chart = Slice.import_from_dict(session, config, recursive=False)",
            "    if chart.id is None:",
            "        session.flush()",
            "",
            "    if hasattr(g, \"user\") and g.user:",
            "        chart.owners.append(g.user)",
            "",
            "    return chart"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "32": [
                "import_chart"
            ]
        },
        "addLocation": [
            "src.requests.adapters"
        ]
    },
    "superset/commands/importers/v1/examples.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from sqlalchemy.orm.exc import MultipleResultsFound"
            },
            "1": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from sqlalchemy.sql import select"
            },
            "2": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from superset import db"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+from superset import db, security_manager"
            },
            "5": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from superset.charts.commands.importers.v1 import ImportChartsCommand"
            },
            "6": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from superset.charts.commands.importers.v1.utils import import_chart"
            },
            "7": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from superset.charts.schemas import ImportV1ChartSchema"
            },
            "8": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " from superset.datasets.commands.importers.v1.utils import import_dataset"
            },
            "9": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 43,
                "PatchRowcode": " from superset.datasets.schemas import ImportV1DatasetSchema"
            },
            "10": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " from superset.models.dashboard import dashboard_slices"
            },
            "11": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from superset.utils.core import get_example_default_schema"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+from superset.utils.core import get_example_default_schema, override_user"
            },
            "13": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 46,
                "PatchRowcode": " from superset.utils.database import get_example_database"
            },
            "14": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 69,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "         # rollback to prevent partial imports"
            },
            "18": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "         try:"
            },
            "19": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._import(db.session, self._configs, self.overwrite, self.force_data)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+            with override_user(security_manager.find_user(username=\"admin\")):"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+                self._import("
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+                    db.session,"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+                    self._configs,"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+                    self.overwrite,"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+                    self.force_data,"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+                )"
            },
            "27": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "             db.session.commit()"
            },
            "28": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "         except Exception as ex:"
            },
            "29": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "             db.session.rollback()"
            },
            "30": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "                 if config[\"schema\"] is None:"
            },
            "31": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 126,
                "PatchRowcode": "                     config[\"schema\"] = get_example_default_schema()"
            },
            "32": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 127,
                "PatchRowcode": " "
            },
            "33": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                dataset = import_dataset("
            },
            "34": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    session, config, overwrite=overwrite, force_data=force_data"
            },
            "35": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                )"
            },
            "36": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "37": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 128,
                "PatchRowcode": "                 try:"
            },
            "38": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 129,
                "PatchRowcode": "                     dataset = import_dataset("
            },
            "39": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        session, config, overwrite=overwrite, force_data=force_data"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+                        session,"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+                        config,"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+                        overwrite=overwrite,"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+                        force_data=force_data,"
            },
            "44": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "                     )"
            },
            "45": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "                 except MultipleResultsFound:"
            },
            "46": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "                     # Multiple result can be found for datasets. There was a bug in"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from typing import Any, Dict, List, Set, Tuple",
            "",
            "from marshmallow import Schema",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.orm.exc import MultipleResultsFound",
            "from sqlalchemy.sql import select",
            "",
            "from superset import db",
            "from superset.charts.commands.importers.v1 import ImportChartsCommand",
            "from superset.charts.commands.importers.v1.utils import import_chart",
            "from superset.charts.schemas import ImportV1ChartSchema",
            "from superset.commands.exceptions import CommandException",
            "from superset.commands.importers.v1 import ImportModelsCommand",
            "from superset.dao.base import BaseDAO",
            "from superset.dashboards.commands.importers.v1 import ImportDashboardsCommand",
            "from superset.dashboards.commands.importers.v1.utils import (",
            "    find_chart_uuids,",
            "    import_dashboard,",
            "    update_id_refs,",
            ")",
            "from superset.dashboards.schemas import ImportV1DashboardSchema",
            "from superset.databases.commands.importers.v1 import ImportDatabasesCommand",
            "from superset.databases.commands.importers.v1.utils import import_database",
            "from superset.databases.schemas import ImportV1DatabaseSchema",
            "from superset.datasets.commands.importers.v1 import ImportDatasetsCommand",
            "from superset.datasets.commands.importers.v1.utils import import_dataset",
            "from superset.datasets.schemas import ImportV1DatasetSchema",
            "from superset.models.dashboard import dashboard_slices",
            "from superset.utils.core import get_example_default_schema",
            "from superset.utils.database import get_example_database",
            "",
            "",
            "class ImportExamplesCommand(ImportModelsCommand):",
            "",
            "    \"\"\"Import examples\"\"\"",
            "",
            "    dao = BaseDAO",
            "    model_name = \"model\"",
            "    schemas: Dict[str, Schema] = {",
            "        \"charts/\": ImportV1ChartSchema(),",
            "        \"dashboards/\": ImportV1DashboardSchema(),",
            "        \"datasets/\": ImportV1DatasetSchema(),",
            "        \"databases/\": ImportV1DatabaseSchema(),",
            "    }",
            "    import_error = CommandException",
            "",
            "    def __init__(self, contents: Dict[str, str], *args: Any, **kwargs: Any):",
            "        super().__init__(contents, *args, **kwargs)",
            "        self.force_data = kwargs.get(\"force_data\", False)",
            "",
            "    def run(self) -> None:",
            "        self.validate()",
            "",
            "        # rollback to prevent partial imports",
            "        try:",
            "            self._import(db.session, self._configs, self.overwrite, self.force_data)",
            "            db.session.commit()",
            "        except Exception as ex:",
            "            db.session.rollback()",
            "            raise self.import_error() from ex",
            "",
            "    @classmethod",
            "    def _get_uuids(cls) -> Set[str]:",
            "        # pylint: disable=protected-access",
            "        return (",
            "            ImportDatabasesCommand._get_uuids()",
            "            | ImportDatasetsCommand._get_uuids()",
            "            | ImportChartsCommand._get_uuids()",
            "            | ImportDashboardsCommand._get_uuids()",
            "        )",
            "",
            "    @staticmethod",
            "    def _import(  # pylint: disable=arguments-differ, too-many-locals, too-many-branches",
            "        session: Session,",
            "        configs: Dict[str, Any],",
            "        overwrite: bool = False,",
            "        force_data: bool = False,",
            "    ) -> None:",
            "        # import databases",
            "        database_ids: Dict[str, int] = {}",
            "        for file_name, config in configs.items():",
            "            if file_name.startswith(\"databases/\"):",
            "                database = import_database(session, config, overwrite=overwrite)",
            "                database_ids[str(database.uuid)] = database.id",
            "",
            "        # import datasets",
            "        # If database_uuid is not in the list of UUIDs it means that the examples",
            "        # database was created before its UUID was frozen, so it has a random UUID.",
            "        # We need to determine its ID so we can point the dataset to it.",
            "        examples_db = get_example_database()",
            "        dataset_info: Dict[str, Dict[str, Any]] = {}",
            "        for file_name, config in configs.items():",
            "            if file_name.startswith(\"datasets/\"):",
            "                # find the ID of the corresponding database",
            "                if config[\"database_uuid\"] not in database_ids:",
            "                    if examples_db is None:",
            "                        raise Exception(\"Cannot find examples database\")",
            "                    config[\"database_id\"] = examples_db.id",
            "                else:",
            "                    config[\"database_id\"] = database_ids[config[\"database_uuid\"]]",
            "",
            "                # set schema",
            "                if config[\"schema\"] is None:",
            "                    config[\"schema\"] = get_example_default_schema()",
            "",
            "                dataset = import_dataset(",
            "                    session, config, overwrite=overwrite, force_data=force_data",
            "                )",
            "",
            "                try:",
            "                    dataset = import_dataset(",
            "                        session, config, overwrite=overwrite, force_data=force_data",
            "                    )",
            "                except MultipleResultsFound:",
            "                    # Multiple result can be found for datasets. There was a bug in",
            "                    # load-examples that resulted in datasets being loaded with a NULL",
            "                    # schema. Users could then add a new dataset with the same name in",
            "                    # the correct schema, resulting in duplicates, since the uniqueness",
            "                    # constraint was not enforced correctly in the application logic.",
            "                    # See https://github.com/apache/superset/issues/16051.",
            "                    continue",
            "",
            "                dataset_info[str(dataset.uuid)] = {",
            "                    \"datasource_id\": dataset.id,",
            "                    \"datasource_type\": \"table\",",
            "                    \"datasource_name\": dataset.table_name,",
            "                }",
            "",
            "        # import charts",
            "        chart_ids: Dict[str, int] = {}",
            "        for file_name, config in configs.items():",
            "            if (",
            "                file_name.startswith(\"charts/\")",
            "                and config[\"dataset_uuid\"] in dataset_info",
            "            ):",
            "                # update datasource id, type, and name",
            "                config.update(dataset_info[config[\"dataset_uuid\"]])",
            "                chart = import_chart(session, config, overwrite=overwrite)",
            "                chart_ids[str(chart.uuid)] = chart.id",
            "",
            "        # store the existing relationship between dashboards and charts",
            "        existing_relationships = session.execute(",
            "            select([dashboard_slices.c.dashboard_id, dashboard_slices.c.slice_id])",
            "        ).fetchall()",
            "",
            "        # import dashboards",
            "        dashboard_chart_ids: List[Tuple[int, int]] = []",
            "        for file_name, config in configs.items():",
            "            if file_name.startswith(\"dashboards/\"):",
            "                try:",
            "                    config = update_id_refs(config, chart_ids, dataset_info)",
            "                except KeyError:",
            "                    continue",
            "",
            "                dashboard = import_dashboard(session, config, overwrite=overwrite)",
            "                dashboard.published = True",
            "",
            "                for uuid in find_chart_uuids(config[\"position\"]):",
            "                    chart_id = chart_ids[uuid]",
            "                    if (dashboard.id, chart_id) not in existing_relationships:",
            "                        dashboard_chart_ids.append((dashboard.id, chart_id))",
            "",
            "        # set ref in the dashboard_slices table",
            "        values = [",
            "            {\"dashboard_id\": dashboard_id, \"slice_id\": chart_id}",
            "            for (dashboard_id, chart_id) in dashboard_chart_ids",
            "        ]",
            "        session.execute(dashboard_slices.insert(), values)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from typing import Any, Dict, List, Set, Tuple",
            "",
            "from marshmallow import Schema",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.orm.exc import MultipleResultsFound",
            "from sqlalchemy.sql import select",
            "",
            "from superset import db, security_manager",
            "from superset.charts.commands.importers.v1 import ImportChartsCommand",
            "from superset.charts.commands.importers.v1.utils import import_chart",
            "from superset.charts.schemas import ImportV1ChartSchema",
            "from superset.commands.exceptions import CommandException",
            "from superset.commands.importers.v1 import ImportModelsCommand",
            "from superset.dao.base import BaseDAO",
            "from superset.dashboards.commands.importers.v1 import ImportDashboardsCommand",
            "from superset.dashboards.commands.importers.v1.utils import (",
            "    find_chart_uuids,",
            "    import_dashboard,",
            "    update_id_refs,",
            ")",
            "from superset.dashboards.schemas import ImportV1DashboardSchema",
            "from superset.databases.commands.importers.v1 import ImportDatabasesCommand",
            "from superset.databases.commands.importers.v1.utils import import_database",
            "from superset.databases.schemas import ImportV1DatabaseSchema",
            "from superset.datasets.commands.importers.v1 import ImportDatasetsCommand",
            "from superset.datasets.commands.importers.v1.utils import import_dataset",
            "from superset.datasets.schemas import ImportV1DatasetSchema",
            "from superset.models.dashboard import dashboard_slices",
            "from superset.utils.core import get_example_default_schema, override_user",
            "from superset.utils.database import get_example_database",
            "",
            "",
            "class ImportExamplesCommand(ImportModelsCommand):",
            "",
            "    \"\"\"Import examples\"\"\"",
            "",
            "    dao = BaseDAO",
            "    model_name = \"model\"",
            "    schemas: Dict[str, Schema] = {",
            "        \"charts/\": ImportV1ChartSchema(),",
            "        \"dashboards/\": ImportV1DashboardSchema(),",
            "        \"datasets/\": ImportV1DatasetSchema(),",
            "        \"databases/\": ImportV1DatabaseSchema(),",
            "    }",
            "    import_error = CommandException",
            "",
            "    def __init__(self, contents: Dict[str, str], *args: Any, **kwargs: Any):",
            "        super().__init__(contents, *args, **kwargs)",
            "        self.force_data = kwargs.get(\"force_data\", False)",
            "",
            "    def run(self) -> None:",
            "        self.validate()",
            "",
            "        # rollback to prevent partial imports",
            "        try:",
            "            with override_user(security_manager.find_user(username=\"admin\")):",
            "                self._import(",
            "                    db.session,",
            "                    self._configs,",
            "                    self.overwrite,",
            "                    self.force_data,",
            "                )",
            "            db.session.commit()",
            "        except Exception as ex:",
            "            db.session.rollback()",
            "            raise self.import_error() from ex",
            "",
            "    @classmethod",
            "    def _get_uuids(cls) -> Set[str]:",
            "        # pylint: disable=protected-access",
            "        return (",
            "            ImportDatabasesCommand._get_uuids()",
            "            | ImportDatasetsCommand._get_uuids()",
            "            | ImportChartsCommand._get_uuids()",
            "            | ImportDashboardsCommand._get_uuids()",
            "        )",
            "",
            "    @staticmethod",
            "    def _import(  # pylint: disable=arguments-differ, too-many-locals, too-many-branches",
            "        session: Session,",
            "        configs: Dict[str, Any],",
            "        overwrite: bool = False,",
            "        force_data: bool = False,",
            "    ) -> None:",
            "        # import databases",
            "        database_ids: Dict[str, int] = {}",
            "        for file_name, config in configs.items():",
            "            if file_name.startswith(\"databases/\"):",
            "                database = import_database(session, config, overwrite=overwrite)",
            "                database_ids[str(database.uuid)] = database.id",
            "",
            "        # import datasets",
            "        # If database_uuid is not in the list of UUIDs it means that the examples",
            "        # database was created before its UUID was frozen, so it has a random UUID.",
            "        # We need to determine its ID so we can point the dataset to it.",
            "        examples_db = get_example_database()",
            "        dataset_info: Dict[str, Dict[str, Any]] = {}",
            "        for file_name, config in configs.items():",
            "            if file_name.startswith(\"datasets/\"):",
            "                # find the ID of the corresponding database",
            "                if config[\"database_uuid\"] not in database_ids:",
            "                    if examples_db is None:",
            "                        raise Exception(\"Cannot find examples database\")",
            "                    config[\"database_id\"] = examples_db.id",
            "                else:",
            "                    config[\"database_id\"] = database_ids[config[\"database_uuid\"]]",
            "",
            "                # set schema",
            "                if config[\"schema\"] is None:",
            "                    config[\"schema\"] = get_example_default_schema()",
            "",
            "                try:",
            "                    dataset = import_dataset(",
            "                        session,",
            "                        config,",
            "                        overwrite=overwrite,",
            "                        force_data=force_data,",
            "                    )",
            "                except MultipleResultsFound:",
            "                    # Multiple result can be found for datasets. There was a bug in",
            "                    # load-examples that resulted in datasets being loaded with a NULL",
            "                    # schema. Users could then add a new dataset with the same name in",
            "                    # the correct schema, resulting in duplicates, since the uniqueness",
            "                    # constraint was not enforced correctly in the application logic.",
            "                    # See https://github.com/apache/superset/issues/16051.",
            "                    continue",
            "",
            "                dataset_info[str(dataset.uuid)] = {",
            "                    \"datasource_id\": dataset.id,",
            "                    \"datasource_type\": \"table\",",
            "                    \"datasource_name\": dataset.table_name,",
            "                }",
            "",
            "        # import charts",
            "        chart_ids: Dict[str, int] = {}",
            "        for file_name, config in configs.items():",
            "            if (",
            "                file_name.startswith(\"charts/\")",
            "                and config[\"dataset_uuid\"] in dataset_info",
            "            ):",
            "                # update datasource id, type, and name",
            "                config.update(dataset_info[config[\"dataset_uuid\"]])",
            "                chart = import_chart(session, config, overwrite=overwrite)",
            "                chart_ids[str(chart.uuid)] = chart.id",
            "",
            "        # store the existing relationship between dashboards and charts",
            "        existing_relationships = session.execute(",
            "            select([dashboard_slices.c.dashboard_id, dashboard_slices.c.slice_id])",
            "        ).fetchall()",
            "",
            "        # import dashboards",
            "        dashboard_chart_ids: List[Tuple[int, int]] = []",
            "        for file_name, config in configs.items():",
            "            if file_name.startswith(\"dashboards/\"):",
            "                try:",
            "                    config = update_id_refs(config, chart_ids, dataset_info)",
            "                except KeyError:",
            "                    continue",
            "",
            "                dashboard = import_dashboard(session, config, overwrite=overwrite)",
            "                dashboard.published = True",
            "",
            "                for uuid in find_chart_uuids(config[\"position\"]):",
            "                    chart_id = chart_ids[uuid]",
            "                    if (dashboard.id, chart_id) not in existing_relationships:",
            "                        dashboard_chart_ids.append((dashboard.id, chart_id))",
            "",
            "        # set ref in the dashboard_slices table",
            "        values = [",
            "            {\"dashboard_id\": dashboard_id, \"slice_id\": chart_id}",
            "            for (dashboard_id, chart_id) in dashboard_chart_ids",
            "        ]",
            "        session.execute(dashboard_slices.insert(), values)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "24": [],
            "45": [],
            "72": [
                "ImportExamplesCommand",
                "run"
            ],
            "122": [
                "ImportExamplesCommand",
                "_import"
            ],
            "123": [
                "ImportExamplesCommand",
                "_import"
            ],
            "124": [
                "ImportExamplesCommand",
                "_import"
            ],
            "125": [
                "ImportExamplesCommand",
                "_import"
            ],
            "128": [
                "ImportExamplesCommand",
                "_import"
            ]
        },
        "addLocation": []
    },
    "superset/dashboards/commands/importers/v1/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from flask import g"
            },
            "1": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from sqlalchemy.orm import Session"
            },
            "2": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+from superset import security_manager"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+from superset.commands.exceptions import ImportFailedError"
            },
            "5": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from superset.models.dashboard import Dashboard"
            },
            "6": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " logger = logging.getLogger(__name__)"
            },
            "8": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 148,
                "PatchRowcode": " def import_dashboard("
            },
            "9": {
                "beforePatchRowNumber": 147,
                "afterPatchRowNumber": 149,
                "PatchRowcode": "     session: Session, config: Dict[str, Any], overwrite: bool = False"
            },
            "10": {
                "beforePatchRowNumber": 148,
                "afterPatchRowNumber": 150,
                "PatchRowcode": " ) -> Dashboard:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+    can_write = security_manager.can_access(\"can_write\", \"Dashboard\")"
            },
            "12": {
                "beforePatchRowNumber": 149,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "     existing = session.query(Dashboard).filter_by(uuid=config[\"uuid\"]).first()"
            },
            "13": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "     if existing:"
            },
            "14": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not overwrite:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+        if not overwrite or not can_write:"
            },
            "16": {
                "beforePatchRowNumber": 152,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "             return existing"
            },
            "17": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "         config[\"id\"] = existing.id"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 157,
                "PatchRowcode": "+    elif not can_write:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 158,
                "PatchRowcode": "+        raise ImportFailedError("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 159,
                "PatchRowcode": "+            \"Dashboard doesn't exist and user doesn't \""
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 160,
                "PatchRowcode": "+            \"have permission to create dashboards\""
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 161,
                "PatchRowcode": "+        )"
            },
            "23": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 162,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 163,
                "PatchRowcode": "     # TODO (betodealmeida): move this logic to import_from_dict"
            },
            "25": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 164,
                "PatchRowcode": "     config = config.copy()"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import json",
            "import logging",
            "from typing import Any, Dict, Set",
            "",
            "from flask import g",
            "from sqlalchemy.orm import Session",
            "",
            "from superset.models.dashboard import Dashboard",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "JSON_KEYS = {\"position\": \"position_json\", \"metadata\": \"json_metadata\"}",
            "",
            "",
            "def find_chart_uuids(position: Dict[str, Any]) -> Set[str]:",
            "    return set(build_uuid_to_id_map(position))",
            "",
            "",
            "def find_native_filter_datasets(metadata: Dict[str, Any]) -> Set[str]:",
            "    uuids: Set[str] = set()",
            "    for native_filter in metadata.get(\"native_filter_configuration\", []):",
            "        targets = native_filter.get(\"targets\", [])",
            "        for target in targets:",
            "            dataset_uuid = target.get(\"datasetUuid\")",
            "            if dataset_uuid:",
            "                uuids.add(dataset_uuid)",
            "    return uuids",
            "",
            "",
            "def build_uuid_to_id_map(position: Dict[str, Any]) -> Dict[str, int]:",
            "    return {",
            "        child[\"meta\"][\"uuid\"]: child[\"meta\"][\"chartId\"]",
            "        for child in position.values()",
            "        if (",
            "            isinstance(child, dict)",
            "            and child[\"type\"] == \"CHART\"",
            "            and \"uuid\" in child[\"meta\"]",
            "        )",
            "    }",
            "",
            "",
            "def update_id_refs(  # pylint: disable=too-many-locals",
            "    config: Dict[str, Any],",
            "    chart_ids: Dict[str, int],",
            "    dataset_info: Dict[str, Dict[str, Any]],",
            ") -> Dict[str, Any]:",
            "    \"\"\"Update dashboard metadata to use new IDs\"\"\"",
            "    fixed = config.copy()",
            "",
            "    # build map old_id => new_id",
            "    old_ids = build_uuid_to_id_map(fixed[\"position\"])",
            "    id_map = {",
            "        old_id: chart_ids[uuid] for uuid, old_id in old_ids.items() if uuid in chart_ids",
            "    }",
            "",
            "    # fix metadata",
            "    metadata = fixed.get(\"metadata\", {})",
            "    if \"timed_refresh_immune_slices\" in metadata:",
            "        metadata[\"timed_refresh_immune_slices\"] = [",
            "            id_map[old_id] for old_id in metadata[\"timed_refresh_immune_slices\"]",
            "        ]",
            "",
            "    if \"filter_scopes\" in metadata:",
            "        # in filter_scopes the key is the chart ID as a string; we need to udpate",
            "        # them to be the new ID as a string:",
            "        metadata[\"filter_scopes\"] = {",
            "            str(id_map[int(old_id)]): columns",
            "            for old_id, columns in metadata[\"filter_scopes\"].items()",
            "            if int(old_id) in id_map",
            "        }",
            "",
            "        # now update columns to use new IDs:",
            "        for columns in metadata[\"filter_scopes\"].values():",
            "            for attributes in columns.values():",
            "                attributes[\"immune\"] = [",
            "                    id_map[old_id]",
            "                    for old_id in attributes[\"immune\"]",
            "                    if old_id in id_map",
            "                ]",
            "",
            "    if \"expanded_slices\" in metadata:",
            "        metadata[\"expanded_slices\"] = {",
            "            str(id_map[int(old_id)]): value",
            "            for old_id, value in metadata[\"expanded_slices\"].items()",
            "        }",
            "",
            "    if \"default_filters\" in metadata:",
            "        default_filters = json.loads(metadata[\"default_filters\"])",
            "        metadata[\"default_filters\"] = json.dumps(",
            "            {",
            "                str(id_map[int(old_id)]): value",
            "                for old_id, value in default_filters.items()",
            "                if int(old_id) in id_map",
            "            }",
            "        )",
            "",
            "    # fix position",
            "    position = fixed.get(\"position\", {})",
            "    for child in position.values():",
            "        if (",
            "            isinstance(child, dict)",
            "            and child[\"type\"] == \"CHART\"",
            "            and \"uuid\" in child[\"meta\"]",
            "            and child[\"meta\"][\"uuid\"] in chart_ids",
            "        ):",
            "            child[\"meta\"][\"chartId\"] = chart_ids[child[\"meta\"][\"uuid\"]]",
            "",
            "    # fix native filter references",
            "    native_filter_configuration = fixed.get(\"metadata\", {}).get(",
            "        \"native_filter_configuration\", []",
            "    )",
            "    for native_filter in native_filter_configuration:",
            "        targets = native_filter.get(\"targets\", [])",
            "        for target in targets:",
            "            dataset_uuid = target.pop(\"datasetUuid\", None)",
            "            if dataset_uuid:",
            "                target[\"datasetId\"] = dataset_info[dataset_uuid][\"datasource_id\"]",
            "",
            "        scope_excluded = native_filter.get(\"scope\", {}).get(\"excluded\", [])",
            "        if scope_excluded:",
            "            native_filter[\"scope\"][\"excluded\"] = [",
            "                id_map[old_id] for old_id in scope_excluded if old_id in id_map",
            "            ]",
            "",
            "    return fixed",
            "",
            "",
            "def import_dashboard(",
            "    session: Session, config: Dict[str, Any], overwrite: bool = False",
            ") -> Dashboard:",
            "    existing = session.query(Dashboard).filter_by(uuid=config[\"uuid\"]).first()",
            "    if existing:",
            "        if not overwrite:",
            "            return existing",
            "        config[\"id\"] = existing.id",
            "",
            "    # TODO (betodealmeida): move this logic to import_from_dict",
            "    config = config.copy()",
            "    for key, new_name in JSON_KEYS.items():",
            "        if config.get(key) is not None:",
            "            value = config.pop(key)",
            "            try:",
            "                config[new_name] = json.dumps(value)",
            "            except TypeError:",
            "                logger.info(\"Unable to encode `%s` field: %s\", key, value)",
            "",
            "    dashboard = Dashboard.import_from_dict(session, config, recursive=False)",
            "    if dashboard.id is None:",
            "        session.flush()",
            "",
            "    if hasattr(g, \"user\") and g.user:",
            "        dashboard.owners.append(g.user)",
            "",
            "    return dashboard"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import json",
            "import logging",
            "from typing import Any, Dict, Set",
            "",
            "from flask import g",
            "from sqlalchemy.orm import Session",
            "",
            "from superset import security_manager",
            "from superset.commands.exceptions import ImportFailedError",
            "from superset.models.dashboard import Dashboard",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "JSON_KEYS = {\"position\": \"position_json\", \"metadata\": \"json_metadata\"}",
            "",
            "",
            "def find_chart_uuids(position: Dict[str, Any]) -> Set[str]:",
            "    return set(build_uuid_to_id_map(position))",
            "",
            "",
            "def find_native_filter_datasets(metadata: Dict[str, Any]) -> Set[str]:",
            "    uuids: Set[str] = set()",
            "    for native_filter in metadata.get(\"native_filter_configuration\", []):",
            "        targets = native_filter.get(\"targets\", [])",
            "        for target in targets:",
            "            dataset_uuid = target.get(\"datasetUuid\")",
            "            if dataset_uuid:",
            "                uuids.add(dataset_uuid)",
            "    return uuids",
            "",
            "",
            "def build_uuid_to_id_map(position: Dict[str, Any]) -> Dict[str, int]:",
            "    return {",
            "        child[\"meta\"][\"uuid\"]: child[\"meta\"][\"chartId\"]",
            "        for child in position.values()",
            "        if (",
            "            isinstance(child, dict)",
            "            and child[\"type\"] == \"CHART\"",
            "            and \"uuid\" in child[\"meta\"]",
            "        )",
            "    }",
            "",
            "",
            "def update_id_refs(  # pylint: disable=too-many-locals",
            "    config: Dict[str, Any],",
            "    chart_ids: Dict[str, int],",
            "    dataset_info: Dict[str, Dict[str, Any]],",
            ") -> Dict[str, Any]:",
            "    \"\"\"Update dashboard metadata to use new IDs\"\"\"",
            "    fixed = config.copy()",
            "",
            "    # build map old_id => new_id",
            "    old_ids = build_uuid_to_id_map(fixed[\"position\"])",
            "    id_map = {",
            "        old_id: chart_ids[uuid] for uuid, old_id in old_ids.items() if uuid in chart_ids",
            "    }",
            "",
            "    # fix metadata",
            "    metadata = fixed.get(\"metadata\", {})",
            "    if \"timed_refresh_immune_slices\" in metadata:",
            "        metadata[\"timed_refresh_immune_slices\"] = [",
            "            id_map[old_id] for old_id in metadata[\"timed_refresh_immune_slices\"]",
            "        ]",
            "",
            "    if \"filter_scopes\" in metadata:",
            "        # in filter_scopes the key is the chart ID as a string; we need to udpate",
            "        # them to be the new ID as a string:",
            "        metadata[\"filter_scopes\"] = {",
            "            str(id_map[int(old_id)]): columns",
            "            for old_id, columns in metadata[\"filter_scopes\"].items()",
            "            if int(old_id) in id_map",
            "        }",
            "",
            "        # now update columns to use new IDs:",
            "        for columns in metadata[\"filter_scopes\"].values():",
            "            for attributes in columns.values():",
            "                attributes[\"immune\"] = [",
            "                    id_map[old_id]",
            "                    for old_id in attributes[\"immune\"]",
            "                    if old_id in id_map",
            "                ]",
            "",
            "    if \"expanded_slices\" in metadata:",
            "        metadata[\"expanded_slices\"] = {",
            "            str(id_map[int(old_id)]): value",
            "            for old_id, value in metadata[\"expanded_slices\"].items()",
            "        }",
            "",
            "    if \"default_filters\" in metadata:",
            "        default_filters = json.loads(metadata[\"default_filters\"])",
            "        metadata[\"default_filters\"] = json.dumps(",
            "            {",
            "                str(id_map[int(old_id)]): value",
            "                for old_id, value in default_filters.items()",
            "                if int(old_id) in id_map",
            "            }",
            "        )",
            "",
            "    # fix position",
            "    position = fixed.get(\"position\", {})",
            "    for child in position.values():",
            "        if (",
            "            isinstance(child, dict)",
            "            and child[\"type\"] == \"CHART\"",
            "            and \"uuid\" in child[\"meta\"]",
            "            and child[\"meta\"][\"uuid\"] in chart_ids",
            "        ):",
            "            child[\"meta\"][\"chartId\"] = chart_ids[child[\"meta\"][\"uuid\"]]",
            "",
            "    # fix native filter references",
            "    native_filter_configuration = fixed.get(\"metadata\", {}).get(",
            "        \"native_filter_configuration\", []",
            "    )",
            "    for native_filter in native_filter_configuration:",
            "        targets = native_filter.get(\"targets\", [])",
            "        for target in targets:",
            "            dataset_uuid = target.pop(\"datasetUuid\", None)",
            "            if dataset_uuid:",
            "                target[\"datasetId\"] = dataset_info[dataset_uuid][\"datasource_id\"]",
            "",
            "        scope_excluded = native_filter.get(\"scope\", {}).get(\"excluded\", [])",
            "        if scope_excluded:",
            "            native_filter[\"scope\"][\"excluded\"] = [",
            "                id_map[old_id] for old_id in scope_excluded if old_id in id_map",
            "            ]",
            "",
            "    return fixed",
            "",
            "",
            "def import_dashboard(",
            "    session: Session, config: Dict[str, Any], overwrite: bool = False",
            ") -> Dashboard:",
            "    can_write = security_manager.can_access(\"can_write\", \"Dashboard\")",
            "    existing = session.query(Dashboard).filter_by(uuid=config[\"uuid\"]).first()",
            "    if existing:",
            "        if not overwrite or not can_write:",
            "            return existing",
            "        config[\"id\"] = existing.id",
            "    elif not can_write:",
            "        raise ImportFailedError(",
            "            \"Dashboard doesn't exist and user doesn't \"",
            "            \"have permission to create dashboards\"",
            "        )",
            "",
            "    # TODO (betodealmeida): move this logic to import_from_dict",
            "    config = config.copy()",
            "    for key, new_name in JSON_KEYS.items():",
            "        if config.get(key) is not None:",
            "            value = config.pop(key)",
            "            try:",
            "                config[new_name] = json.dumps(value)",
            "            except TypeError:",
            "                logger.info(\"Unable to encode `%s` field: %s\", key, value)",
            "",
            "    dashboard = Dashboard.import_from_dict(session, config, recursive=False)",
            "    if dashboard.id is None:",
            "        session.flush()",
            "",
            "    if hasattr(g, \"user\") and g.user:",
            "        dashboard.owners.append(g.user)",
            "",
            "    return dashboard"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "151": [
                "import_dashboard"
            ]
        },
        "addLocation": [
            "src.requests.adapters"
        ]
    },
    "superset/databases/commands/importers/v1/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from sqlalchemy.orm import Session"
            },
            "2": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+from superset import security_manager"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+from superset.commands.exceptions import ImportFailedError"
            },
            "5": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from superset.models.core import Database"
            },
            "6": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " def import_database("
            },
            "9": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    session: Session, config: Dict[str, Any], overwrite: bool = False"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+    session: Session,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+    config: Dict[str, Any],"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+    overwrite: bool = False,"
            },
            "13": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " ) -> Database:"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+    can_write = security_manager.can_access(\"can_write\", \"Database\")"
            },
            "15": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "     existing = session.query(Database).filter_by(uuid=config[\"uuid\"]).first()"
            },
            "16": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "     if existing:"
            },
            "17": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not overwrite:"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+        if not overwrite or not can_write:"
            },
            "19": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "             return existing"
            },
            "20": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "         config[\"id\"] = existing.id"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+    elif not can_write:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+        raise ImportFailedError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+            \"Database doesn't exist and user doesn't have permission to create databases\""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+        )"
            },
            "25": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 43,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "     # https://github.com/apache/superset/pull/16756 renamed ``csv`` to ``file``."
            },
            "27": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 45,
                "PatchRowcode": "     config[\"allow_file_upload\"] = config.pop(\"allow_csv_upload\")"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import json",
            "from typing import Any, Dict",
            "",
            "from sqlalchemy.orm import Session",
            "",
            "from superset.models.core import Database",
            "",
            "",
            "def import_database(",
            "    session: Session, config: Dict[str, Any], overwrite: bool = False",
            ") -> Database:",
            "    existing = session.query(Database).filter_by(uuid=config[\"uuid\"]).first()",
            "    if existing:",
            "        if not overwrite:",
            "            return existing",
            "        config[\"id\"] = existing.id",
            "",
            "    # https://github.com/apache/superset/pull/16756 renamed ``csv`` to ``file``.",
            "    config[\"allow_file_upload\"] = config.pop(\"allow_csv_upload\")",
            "    if \"schemas_allowed_for_csv_upload\" in config[\"extra\"]:",
            "        config[\"extra\"][\"schemas_allowed_for_file_upload\"] = config[\"extra\"].pop(",
            "            \"schemas_allowed_for_csv_upload\"",
            "        )",
            "",
            "    # TODO (betodealmeida): move this logic to import_from_dict",
            "    config[\"extra\"] = json.dumps(config[\"extra\"])",
            "",
            "    database = Database.import_from_dict(session, config, recursive=False)",
            "    if database.id is None:",
            "        session.flush()",
            "",
            "    return database"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import json",
            "from typing import Any, Dict",
            "",
            "from sqlalchemy.orm import Session",
            "",
            "from superset import security_manager",
            "from superset.commands.exceptions import ImportFailedError",
            "from superset.models.core import Database",
            "",
            "",
            "def import_database(",
            "    session: Session,",
            "    config: Dict[str, Any],",
            "    overwrite: bool = False,",
            ") -> Database:",
            "    can_write = security_manager.can_access(\"can_write\", \"Database\")",
            "    existing = session.query(Database).filter_by(uuid=config[\"uuid\"]).first()",
            "    if existing:",
            "        if not overwrite or not can_write:",
            "            return existing",
            "        config[\"id\"] = existing.id",
            "    elif not can_write:",
            "        raise ImportFailedError(",
            "            \"Database doesn't exist and user doesn't have permission to create databases\"",
            "        )",
            "",
            "    # https://github.com/apache/superset/pull/16756 renamed ``csv`` to ``file``.",
            "    config[\"allow_file_upload\"] = config.pop(\"allow_csv_upload\")",
            "    if \"schemas_allowed_for_csv_upload\" in config[\"extra\"]:",
            "        config[\"extra\"][\"schemas_allowed_for_file_upload\"] = config[\"extra\"].pop(",
            "            \"schemas_allowed_for_csv_upload\"",
            "        )",
            "",
            "    # TODO (betodealmeida): move this logic to import_from_dict",
            "    config[\"extra\"] = json.dumps(config[\"extra\"])",
            "",
            "    database = Database.import_from_dict(session, config, recursive=False)",
            "    if database.id is None:",
            "        session.flush()",
            "",
            "    return database"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "27": [
                "import_database"
            ],
            "31": [
                "import_database"
            ]
        },
        "addLocation": []
    },
    "superset/datasets/commands/importers/v1/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from sqlalchemy.orm.exc import MultipleResultsFound"
            },
            "1": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " from sqlalchemy.sql.visitors import VisitableType"
            },
            "2": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+from superset import security_manager"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+from superset.commands.exceptions import ImportFailedError"
            },
            "5": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from superset.connectors.sqla.models import SqlaTable"
            },
            "6": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " from superset.datasets.commands.exceptions import DatasetForbiddenDataURI"
            },
            "7": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " from superset.models.core import Database"
            },
            "8": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "     overwrite: bool = False,"
            },
            "9": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "     force_data: bool = False,"
            },
            "10": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 108,
                "PatchRowcode": " ) -> SqlaTable:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+    can_write = security_manager.can_access(\"can_write\", \"Dataset\")"
            },
            "12": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "     existing = session.query(SqlaTable).filter_by(uuid=config[\"uuid\"]).first()"
            },
            "13": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "     if existing:"
            },
            "14": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not overwrite:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+        if not overwrite or not can_write:"
            },
            "16": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "             return existing"
            },
            "17": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "         config[\"id\"] = existing.id"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+    elif not can_write:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        raise ImportFailedError("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+            \"Dataset doesn't exist and user doesn't have permission to create datasets\""
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+        )"
            },
            "22": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 119,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "     # TODO (betodealmeida): move this logic to import_from_dict"
            },
            "24": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "     config = config.copy()"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import gzip",
            "import json",
            "import logging",
            "import re",
            "from typing import Any, Dict",
            "from urllib import request",
            "",
            "import pandas as pd",
            "from flask import current_app, g",
            "from sqlalchemy import BigInteger, Boolean, Date, DateTime, Float, String, Text",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.orm.exc import MultipleResultsFound",
            "from sqlalchemy.sql.visitors import VisitableType",
            "",
            "from superset.connectors.sqla.models import SqlaTable",
            "from superset.datasets.commands.exceptions import DatasetForbiddenDataURI",
            "from superset.models.core import Database",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "CHUNKSIZE = 512",
            "VARCHAR = re.compile(r\"VARCHAR\\((\\d+)\\)\", re.IGNORECASE)",
            "",
            "JSON_KEYS = {\"params\", \"template_params\", \"extra\"}",
            "",
            "",
            "type_map = {",
            "    \"BOOLEAN\": Boolean(),",
            "    \"VARCHAR\": String(255),",
            "    \"STRING\": String(255),",
            "    \"TEXT\": Text(),",
            "    \"BIGINT\": BigInteger(),",
            "    \"FLOAT\": Float(),",
            "    \"FLOAT64\": Float(),",
            "    \"DOUBLE PRECISION\": Float(),",
            "    \"DATE\": Date(),",
            "    \"DATETIME\": DateTime(),",
            "    \"TIMESTAMP WITHOUT TIME ZONE\": DateTime(timezone=False),",
            "    \"TIMESTAMP WITH TIME ZONE\": DateTime(timezone=True),",
            "}",
            "",
            "",
            "def get_sqla_type(native_type: str) -> VisitableType:",
            "    if native_type.upper() in type_map:",
            "        return type_map[native_type.upper()]",
            "",
            "    match = VARCHAR.match(native_type)",
            "    if match:",
            "        size = int(match.group(1))",
            "        return String(size)",
            "",
            "    raise Exception(f\"Unknown type: {native_type}\")",
            "",
            "",
            "def get_dtype(df: pd.DataFrame, dataset: SqlaTable) -> Dict[str, VisitableType]:",
            "    return {",
            "        column.column_name: get_sqla_type(column.type)",
            "        for column in dataset.columns",
            "        if column.column_name in df.keys()",
            "    }",
            "",
            "",
            "def validate_data_uri(data_uri: str) -> None:",
            "    \"\"\"",
            "    Validate that the data URI is configured on DATASET_IMPORT_ALLOWED_URLS",
            "    has a valid URL.",
            "",
            "    :param data_uri:",
            "    :return:",
            "    \"\"\"",
            "    allowed_urls = current_app.config[\"DATASET_IMPORT_ALLOWED_DATA_URLS\"]",
            "    for allowed_url in allowed_urls:",
            "        try:",
            "            match = re.match(allowed_url, data_uri)",
            "        except re.error:",
            "            logger.exception(",
            "                \"Invalid regular expression on DATASET_IMPORT_ALLOWED_URLS\"",
            "            )",
            "            raise",
            "        if match:",
            "            return",
            "    raise DatasetForbiddenDataURI()",
            "",
            "",
            "def import_dataset(",
            "    session: Session,",
            "    config: Dict[str, Any],",
            "    overwrite: bool = False,",
            "    force_data: bool = False,",
            ") -> SqlaTable:",
            "    existing = session.query(SqlaTable).filter_by(uuid=config[\"uuid\"]).first()",
            "    if existing:",
            "        if not overwrite:",
            "            return existing",
            "        config[\"id\"] = existing.id",
            "",
            "    # TODO (betodealmeida): move this logic to import_from_dict",
            "    config = config.copy()",
            "    for key in JSON_KEYS:",
            "        if config.get(key) is not None:",
            "            try:",
            "                config[key] = json.dumps(config[key])",
            "            except TypeError:",
            "                logger.info(\"Unable to encode `%s` field: %s\", key, config[key])",
            "    for key in (\"metrics\", \"columns\"):",
            "        for attributes in config.get(key, []):",
            "            if attributes.get(\"extra\") is not None:",
            "                try:",
            "                    attributes[\"extra\"] = json.dumps(attributes[\"extra\"])",
            "                except TypeError:",
            "                    logger.info(",
            "                        \"Unable to encode `extra` field: %s\", attributes[\"extra\"]",
            "                    )",
            "                    attributes[\"extra\"] = None",
            "",
            "    # should we delete columns and metrics not present in the current import?",
            "    sync = [\"columns\", \"metrics\"] if overwrite else []",
            "",
            "    # should we also load data into the dataset?",
            "    data_uri = config.get(\"data\")",
            "",
            "    # import recursively to include columns and metrics",
            "    try:",
            "        dataset = SqlaTable.import_from_dict(session, config, recursive=True, sync=sync)",
            "    except MultipleResultsFound:",
            "        # Finding multiple results when importing a dataset only happens because initially",
            "        # datasets were imported without schemas (eg, `examples.NULL.users`), and later",
            "        # they were fixed to have the default schema (eg, `examples.public.users`). If a",
            "        # user created `examples.public.users` during that time the second import will",
            "        # fail because the UUID match will try to update `examples.NULL.users` to",
            "        # `examples.public.users`, resulting in a conflict.",
            "        #",
            "        # When that happens, we return the original dataset, unmodified.",
            "        dataset = session.query(SqlaTable).filter_by(uuid=config[\"uuid\"]).one()",
            "",
            "    if dataset.id is None:",
            "        session.flush()",
            "",
            "    try:",
            "        table_exists = dataset.database.has_table_by_name(dataset.table_name)",
            "    except Exception:  # pylint: disable=broad-except",
            "        # MySQL doesn't play nice with GSheets table names",
            "        logger.warning(",
            "            \"Couldn't check if table %s exists, assuming it does\", dataset.table_name",
            "        )",
            "        table_exists = True",
            "",
            "    if data_uri and (not table_exists or force_data):",
            "        load_data(data_uri, dataset, dataset.database, session)",
            "",
            "    if hasattr(g, \"user\") and g.user:",
            "        dataset.owners.append(g.user)",
            "",
            "    return dataset",
            "",
            "",
            "def load_data(",
            "    data_uri: str, dataset: SqlaTable, database: Database, session: Session",
            ") -> None:",
            "    \"\"\"",
            "    Load data from a data URI into a dataset.",
            "",
            "    :raises DatasetUnAllowedDataURI: If a dataset is trying",
            "    to load data from a URI that is not allowed.",
            "    \"\"\"",
            "    validate_data_uri(data_uri)",
            "    logger.info(\"Downloading data from %s\", data_uri)",
            "    data = request.urlopen(data_uri)  # pylint: disable=consider-using-with",
            "    if data_uri.endswith(\".gz\"):",
            "        data = gzip.open(data)",
            "    df = pd.read_csv(data, encoding=\"utf-8\")",
            "    dtype = get_dtype(df, dataset)",
            "",
            "    # convert temporal columns",
            "    for column_name, sqla_type in dtype.items():",
            "        if isinstance(sqla_type, (Date, DateTime)):",
            "            df[column_name] = pd.to_datetime(df[column_name])",
            "",
            "    # reuse session when loading data if possible, to make import atomic",
            "    if database.sqlalchemy_uri == current_app.config.get(\"SQLALCHEMY_DATABASE_URI\"):",
            "        logger.info(\"Loading data inside the import transaction\")",
            "        connection = session.connection()",
            "        df.to_sql(",
            "            dataset.table_name,",
            "            con=connection,",
            "            schema=dataset.schema,",
            "            if_exists=\"replace\",",
            "            chunksize=CHUNKSIZE,",
            "            dtype=dtype,",
            "            index=False,",
            "            method=\"multi\",",
            "        )",
            "    else:",
            "        logger.warning(\"Loading data outside the import transaction\")",
            "        with database.get_sqla_engine_with_context() as engine:",
            "            df.to_sql(",
            "                dataset.table_name,",
            "                con=engine,",
            "                schema=dataset.schema,",
            "                if_exists=\"replace\",",
            "                chunksize=CHUNKSIZE,",
            "                dtype=dtype,",
            "                index=False,",
            "                method=\"multi\",",
            "            )"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import gzip",
            "import json",
            "import logging",
            "import re",
            "from typing import Any, Dict",
            "from urllib import request",
            "",
            "import pandas as pd",
            "from flask import current_app, g",
            "from sqlalchemy import BigInteger, Boolean, Date, DateTime, Float, String, Text",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.orm.exc import MultipleResultsFound",
            "from sqlalchemy.sql.visitors import VisitableType",
            "",
            "from superset import security_manager",
            "from superset.commands.exceptions import ImportFailedError",
            "from superset.connectors.sqla.models import SqlaTable",
            "from superset.datasets.commands.exceptions import DatasetForbiddenDataURI",
            "from superset.models.core import Database",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "CHUNKSIZE = 512",
            "VARCHAR = re.compile(r\"VARCHAR\\((\\d+)\\)\", re.IGNORECASE)",
            "",
            "JSON_KEYS = {\"params\", \"template_params\", \"extra\"}",
            "",
            "",
            "type_map = {",
            "    \"BOOLEAN\": Boolean(),",
            "    \"VARCHAR\": String(255),",
            "    \"STRING\": String(255),",
            "    \"TEXT\": Text(),",
            "    \"BIGINT\": BigInteger(),",
            "    \"FLOAT\": Float(),",
            "    \"FLOAT64\": Float(),",
            "    \"DOUBLE PRECISION\": Float(),",
            "    \"DATE\": Date(),",
            "    \"DATETIME\": DateTime(),",
            "    \"TIMESTAMP WITHOUT TIME ZONE\": DateTime(timezone=False),",
            "    \"TIMESTAMP WITH TIME ZONE\": DateTime(timezone=True),",
            "}",
            "",
            "",
            "def get_sqla_type(native_type: str) -> VisitableType:",
            "    if native_type.upper() in type_map:",
            "        return type_map[native_type.upper()]",
            "",
            "    match = VARCHAR.match(native_type)",
            "    if match:",
            "        size = int(match.group(1))",
            "        return String(size)",
            "",
            "    raise Exception(f\"Unknown type: {native_type}\")",
            "",
            "",
            "def get_dtype(df: pd.DataFrame, dataset: SqlaTable) -> Dict[str, VisitableType]:",
            "    return {",
            "        column.column_name: get_sqla_type(column.type)",
            "        for column in dataset.columns",
            "        if column.column_name in df.keys()",
            "    }",
            "",
            "",
            "def validate_data_uri(data_uri: str) -> None:",
            "    \"\"\"",
            "    Validate that the data URI is configured on DATASET_IMPORT_ALLOWED_URLS",
            "    has a valid URL.",
            "",
            "    :param data_uri:",
            "    :return:",
            "    \"\"\"",
            "    allowed_urls = current_app.config[\"DATASET_IMPORT_ALLOWED_DATA_URLS\"]",
            "    for allowed_url in allowed_urls:",
            "        try:",
            "            match = re.match(allowed_url, data_uri)",
            "        except re.error:",
            "            logger.exception(",
            "                \"Invalid regular expression on DATASET_IMPORT_ALLOWED_URLS\"",
            "            )",
            "            raise",
            "        if match:",
            "            return",
            "    raise DatasetForbiddenDataURI()",
            "",
            "",
            "def import_dataset(",
            "    session: Session,",
            "    config: Dict[str, Any],",
            "    overwrite: bool = False,",
            "    force_data: bool = False,",
            ") -> SqlaTable:",
            "    can_write = security_manager.can_access(\"can_write\", \"Dataset\")",
            "    existing = session.query(SqlaTable).filter_by(uuid=config[\"uuid\"]).first()",
            "    if existing:",
            "        if not overwrite or not can_write:",
            "            return existing",
            "        config[\"id\"] = existing.id",
            "    elif not can_write:",
            "        raise ImportFailedError(",
            "            \"Dataset doesn't exist and user doesn't have permission to create datasets\"",
            "        )",
            "",
            "    # TODO (betodealmeida): move this logic to import_from_dict",
            "    config = config.copy()",
            "    for key in JSON_KEYS:",
            "        if config.get(key) is not None:",
            "            try:",
            "                config[key] = json.dumps(config[key])",
            "            except TypeError:",
            "                logger.info(\"Unable to encode `%s` field: %s\", key, config[key])",
            "    for key in (\"metrics\", \"columns\"):",
            "        for attributes in config.get(key, []):",
            "            if attributes.get(\"extra\") is not None:",
            "                try:",
            "                    attributes[\"extra\"] = json.dumps(attributes[\"extra\"])",
            "                except TypeError:",
            "                    logger.info(",
            "                        \"Unable to encode `extra` field: %s\", attributes[\"extra\"]",
            "                    )",
            "                    attributes[\"extra\"] = None",
            "",
            "    # should we delete columns and metrics not present in the current import?",
            "    sync = [\"columns\", \"metrics\"] if overwrite else []",
            "",
            "    # should we also load data into the dataset?",
            "    data_uri = config.get(\"data\")",
            "",
            "    # import recursively to include columns and metrics",
            "    try:",
            "        dataset = SqlaTable.import_from_dict(session, config, recursive=True, sync=sync)",
            "    except MultipleResultsFound:",
            "        # Finding multiple results when importing a dataset only happens because initially",
            "        # datasets were imported without schemas (eg, `examples.NULL.users`), and later",
            "        # they were fixed to have the default schema (eg, `examples.public.users`). If a",
            "        # user created `examples.public.users` during that time the second import will",
            "        # fail because the UUID match will try to update `examples.NULL.users` to",
            "        # `examples.public.users`, resulting in a conflict.",
            "        #",
            "        # When that happens, we return the original dataset, unmodified.",
            "        dataset = session.query(SqlaTable).filter_by(uuid=config[\"uuid\"]).one()",
            "",
            "    if dataset.id is None:",
            "        session.flush()",
            "",
            "    try:",
            "        table_exists = dataset.database.has_table_by_name(dataset.table_name)",
            "    except Exception:  # pylint: disable=broad-except",
            "        # MySQL doesn't play nice with GSheets table names",
            "        logger.warning(",
            "            \"Couldn't check if table %s exists, assuming it does\", dataset.table_name",
            "        )",
            "        table_exists = True",
            "",
            "    if data_uri and (not table_exists or force_data):",
            "        load_data(data_uri, dataset, dataset.database, session)",
            "",
            "    if hasattr(g, \"user\") and g.user:",
            "        dataset.owners.append(g.user)",
            "",
            "    return dataset",
            "",
            "",
            "def load_data(",
            "    data_uri: str, dataset: SqlaTable, database: Database, session: Session",
            ") -> None:",
            "    \"\"\"",
            "    Load data from a data URI into a dataset.",
            "",
            "    :raises DatasetUnAllowedDataURI: If a dataset is trying",
            "    to load data from a URI that is not allowed.",
            "    \"\"\"",
            "    validate_data_uri(data_uri)",
            "    logger.info(\"Downloading data from %s\", data_uri)",
            "    data = request.urlopen(data_uri)  # pylint: disable=consider-using-with",
            "    if data_uri.endswith(\".gz\"):",
            "        data = gzip.open(data)",
            "    df = pd.read_csv(data, encoding=\"utf-8\")",
            "    dtype = get_dtype(df, dataset)",
            "",
            "    # convert temporal columns",
            "    for column_name, sqla_type in dtype.items():",
            "        if isinstance(sqla_type, (Date, DateTime)):",
            "            df[column_name] = pd.to_datetime(df[column_name])",
            "",
            "    # reuse session when loading data if possible, to make import atomic",
            "    if database.sqlalchemy_uri == current_app.config.get(\"SQLALCHEMY_DATABASE_URI\"):",
            "        logger.info(\"Loading data inside the import transaction\")",
            "        connection = session.connection()",
            "        df.to_sql(",
            "            dataset.table_name,",
            "            con=connection,",
            "            schema=dataset.schema,",
            "            if_exists=\"replace\",",
            "            chunksize=CHUNKSIZE,",
            "            dtype=dtype,",
            "            index=False,",
            "            method=\"multi\",",
            "        )",
            "    else:",
            "        logger.warning(\"Loading data outside the import transaction\")",
            "        with database.get_sqla_engine_with_context() as engine:",
            "            df.to_sql(",
            "                dataset.table_name,",
            "                con=engine,",
            "                schema=dataset.schema,",
            "                if_exists=\"replace\",",
            "                chunksize=CHUNKSIZE,",
            "                dtype=dtype,",
            "                index=False,",
            "                method=\"multi\",",
            "            )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "109": [
                "import_dataset"
            ]
        },
        "addLocation": [
            "src.requests.adapters"
        ]
    },
    "superset/examples/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "     contents[METADATA_FILE_NAME] = yaml.dump(metadata)"
            },
            "1": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 93,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "     command = ImportExamplesCommand("
            },
            "3": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        contents, overwrite=overwrite, force_data=force_data"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+        contents,"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+        overwrite=overwrite,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+        force_data=force_data,"
            },
            "7": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "     )"
            },
            "8": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "     try:"
            },
            "9": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "         command.run()"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import logging",
            "import re",
            "from pathlib import Path",
            "from typing import Any, Dict",
            "",
            "import yaml",
            "from pkg_resources import resource_isdir, resource_listdir, resource_stream",
            "",
            "from superset.commands.exceptions import CommandInvalidError",
            "from superset.commands.importers.v1.examples import ImportExamplesCommand",
            "from superset.commands.importers.v1.utils import METADATA_FILE_NAME",
            "",
            "_logger = logging.getLogger(__name__)",
            "",
            "YAML_EXTENSIONS = {\".yaml\", \".yml\"}",
            "",
            "",
            "def load_examples_from_configs(",
            "    force_data: bool = False, load_test_data: bool = False",
            ") -> None:",
            "    \"\"\"",
            "    Load all the examples inside superset/examples/configs/.",
            "    \"\"\"",
            "    contents = load_contents(load_test_data)",
            "    command = ImportExamplesCommand(contents, overwrite=True, force_data=force_data)",
            "    command.run()",
            "",
            "",
            "def load_contents(load_test_data: bool = False) -> Dict[str, Any]:",
            "    \"\"\"Traverse configs directory and load contents\"\"\"",
            "    root = Path(\"examples/configs\")",
            "    resource_names = resource_listdir(\"superset\", str(root))",
            "    queue = [root / resource_name for resource_name in resource_names]",
            "",
            "    contents: Dict[Path, str] = {}",
            "    while queue:",
            "        path_name = queue.pop()",
            "        test_re = re.compile(r\"\\.test\\.|metadata\\.yaml$\")",
            "",
            "        if resource_isdir(\"superset\", str(path_name)):",
            "            queue.extend(",
            "                path_name / child_name",
            "                for child_name in resource_listdir(\"superset\", str(path_name))",
            "            )",
            "        elif path_name.suffix.lower() in YAML_EXTENSIONS:",
            "            if load_test_data and test_re.search(str(path_name)) is None:",
            "                continue",
            "            contents[path_name] = (",
            "                resource_stream(\"superset\", str(path_name)).read().decode(\"utf-8\")",
            "            )",
            "",
            "    return {str(path.relative_to(root)): content for path, content in contents.items()}",
            "",
            "",
            "def load_configs_from_directory(",
            "    root: Path, overwrite: bool = True, force_data: bool = False",
            ") -> None:",
            "    \"\"\"",
            "    Load all the examples from a given directory.",
            "    \"\"\"",
            "    contents: Dict[str, str] = {}",
            "    queue = [root]",
            "    while queue:",
            "        path_name = queue.pop()",
            "        if path_name.is_dir():",
            "            queue.extend(path_name.glob(\"*\"))",
            "        elif path_name.suffix.lower() in YAML_EXTENSIONS:",
            "            with open(path_name) as fp:",
            "                contents[str(path_name.relative_to(root))] = fp.read()",
            "",
            "    # removing \"type\" from the metadata allows us to import any exported model",
            "    # from the unzipped directory directly",
            "    metadata = yaml.load(contents.get(METADATA_FILE_NAME, \"{}\"), Loader=None)",
            "    if \"type\" in metadata:",
            "        del metadata[\"type\"]",
            "    contents[METADATA_FILE_NAME] = yaml.dump(metadata)",
            "",
            "    command = ImportExamplesCommand(",
            "        contents, overwrite=overwrite, force_data=force_data",
            "    )",
            "    try:",
            "        command.run()",
            "    except CommandInvalidError as ex:",
            "        _logger.error(\"An error occurred: %s\", ex.normalized_messages())"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import logging",
            "import re",
            "from pathlib import Path",
            "from typing import Any, Dict",
            "",
            "import yaml",
            "from pkg_resources import resource_isdir, resource_listdir, resource_stream",
            "",
            "from superset.commands.exceptions import CommandInvalidError",
            "from superset.commands.importers.v1.examples import ImportExamplesCommand",
            "from superset.commands.importers.v1.utils import METADATA_FILE_NAME",
            "",
            "_logger = logging.getLogger(__name__)",
            "",
            "YAML_EXTENSIONS = {\".yaml\", \".yml\"}",
            "",
            "",
            "def load_examples_from_configs(",
            "    force_data: bool = False, load_test_data: bool = False",
            ") -> None:",
            "    \"\"\"",
            "    Load all the examples inside superset/examples/configs/.",
            "    \"\"\"",
            "    contents = load_contents(load_test_data)",
            "    command = ImportExamplesCommand(contents, overwrite=True, force_data=force_data)",
            "    command.run()",
            "",
            "",
            "def load_contents(load_test_data: bool = False) -> Dict[str, Any]:",
            "    \"\"\"Traverse configs directory and load contents\"\"\"",
            "    root = Path(\"examples/configs\")",
            "    resource_names = resource_listdir(\"superset\", str(root))",
            "    queue = [root / resource_name for resource_name in resource_names]",
            "",
            "    contents: Dict[Path, str] = {}",
            "    while queue:",
            "        path_name = queue.pop()",
            "        test_re = re.compile(r\"\\.test\\.|metadata\\.yaml$\")",
            "",
            "        if resource_isdir(\"superset\", str(path_name)):",
            "            queue.extend(",
            "                path_name / child_name",
            "                for child_name in resource_listdir(\"superset\", str(path_name))",
            "            )",
            "        elif path_name.suffix.lower() in YAML_EXTENSIONS:",
            "            if load_test_data and test_re.search(str(path_name)) is None:",
            "                continue",
            "            contents[path_name] = (",
            "                resource_stream(\"superset\", str(path_name)).read().decode(\"utf-8\")",
            "            )",
            "",
            "    return {str(path.relative_to(root)): content for path, content in contents.items()}",
            "",
            "",
            "def load_configs_from_directory(",
            "    root: Path, overwrite: bool = True, force_data: bool = False",
            ") -> None:",
            "    \"\"\"",
            "    Load all the examples from a given directory.",
            "    \"\"\"",
            "    contents: Dict[str, str] = {}",
            "    queue = [root]",
            "    while queue:",
            "        path_name = queue.pop()",
            "        if path_name.is_dir():",
            "            queue.extend(path_name.glob(\"*\"))",
            "        elif path_name.suffix.lower() in YAML_EXTENSIONS:",
            "            with open(path_name) as fp:",
            "                contents[str(path_name.relative_to(root))] = fp.read()",
            "",
            "    # removing \"type\" from the metadata allows us to import any exported model",
            "    # from the unzipped directory directly",
            "    metadata = yaml.load(contents.get(METADATA_FILE_NAME, \"{}\"), Loader=None)",
            "    if \"type\" in metadata:",
            "        del metadata[\"type\"]",
            "    contents[METADATA_FILE_NAME] = yaml.dump(metadata)",
            "",
            "    command = ImportExamplesCommand(",
            "        contents,",
            "        overwrite=overwrite,",
            "        force_data=force_data,",
            "    )",
            "    try:",
            "        command.run()",
            "    except CommandInvalidError as ex:",
            "        _logger.error(\"An error occurred: %s\", ex.normalized_messages())"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "95": [
                "load_configs_from_directory"
            ]
        },
        "addLocation": []
    }
}