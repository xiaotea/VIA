{
    "airflow/executors/celery_executor.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 71,
                "PatchRowcode": " @app.task"
            },
            "1": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": " def execute_command(command_to_exec: CommandType) -> None:"
            },
            "2": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "     \"\"\"Executes command.\"\"\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+    if command_to_exec[0:3] != [\"airflow\", \"tasks\", \"run\"]:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+        raise ValueError('The command must start with [\"airflow\", \"tasks\", \"run\"].')"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "     log.info(\"Executing command in Celery: %s\", command_to_exec)"
            },
            "7": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "     env = os.environ.copy()"
            },
            "8": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "     try:"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"CeleryExecutor",
            "",
            ".. seealso::",
            "    For more information on how the CeleryExecutor works, take a look at the guide:",
            "    :ref:`executor:CeleryExecutor`",
            "\"\"\"",
            "import logging",
            "import math",
            "import os",
            "import subprocess",
            "import time",
            "import traceback",
            "from multiprocessing import Pool, cpu_count",
            "from typing import Any, List, Mapping, MutableMapping, Optional, Set, Tuple, Union",
            "",
            "from celery import Celery, Task, states as celery_states",
            "from celery.backends.base import BaseKeyValueStoreBackend",
            "from celery.backends.database import DatabaseBackend, Task as TaskDb, session_cleanup",
            "from celery.result import AsyncResult",
            "",
            "from airflow.config_templates.default_celery import DEFAULT_CELERY_CONFIG",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowException",
            "from airflow.executors.base_executor import BaseExecutor, CommandType, EventBufferValueType",
            "from airflow.models.taskinstance import SimpleTaskInstance, TaskInstanceKeyType",
            "from airflow.utils.log.logging_mixin import LoggingMixin",
            "from airflow.utils.net import get_hostname",
            "from airflow.utils.timeout import timeout",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "# Make it constant for unit test.",
            "CELERY_FETCH_ERR_MSG_HEADER = 'Error fetching Celery task state'",
            "",
            "CELERY_SEND_ERR_MSG_HEADER = 'Error sending Celery task'",
            "",
            "OPERATION_TIMEOUT = conf.getint('celery', 'operation_timeout', fallback=2)",
            "",
            "'''",
            "To start the celery worker, run the command:",
            "airflow celery worker",
            "'''",
            "",
            "if conf.has_option('celery', 'celery_config_options'):",
            "    celery_configuration = conf.getimport('celery', 'celery_config_options')",
            "else:",
            "    celery_configuration = DEFAULT_CELERY_CONFIG",
            "",
            "app = Celery(",
            "    conf.get('celery', 'CELERY_APP_NAME'),",
            "    config_source=celery_configuration)",
            "",
            "",
            "@app.task",
            "def execute_command(command_to_exec: CommandType) -> None:",
            "    \"\"\"Executes command.\"\"\"",
            "    log.info(\"Executing command in Celery: %s\", command_to_exec)",
            "    env = os.environ.copy()",
            "    try:",
            "        subprocess.check_call(command_to_exec, stderr=subprocess.STDOUT,",
            "                              close_fds=True, env=env)",
            "    except subprocess.CalledProcessError as e:",
            "        log.exception('execute_command encountered a CalledProcessError')",
            "        log.error(e.output)",
            "        msg = 'Celery command failed on host: ' + get_hostname()",
            "        raise AirflowException(msg)",
            "",
            "",
            "class ExceptionWithTraceback:",
            "    \"\"\"",
            "    Wrapper class used to propagate exceptions to parent processes from subprocesses.",
            "",
            "    :param exception: The exception to wrap",
            "    :type exception: Exception",
            "    :param exception_traceback: The stacktrace to wrap",
            "    :type exception_traceback: str",
            "    \"\"\"",
            "",
            "    def __init__(self, exception: Exception, exception_traceback: str):",
            "        self.exception = exception",
            "        self.traceback = exception_traceback",
            "",
            "",
            "# Task instance that is sent over Celery queues",
            "# TaskInstanceKeyType, SimpleTaskInstance, Command, queue_name, CallableTask",
            "TaskInstanceInCelery = Tuple[TaskInstanceKeyType, SimpleTaskInstance, CommandType, Optional[str], Task]",
            "",
            "",
            "def send_task_to_executor(task_tuple: TaskInstanceInCelery) \\",
            "        -> Tuple[TaskInstanceKeyType, CommandType, Union[AsyncResult, ExceptionWithTraceback]]:",
            "    \"\"\"Sends task to executor.\"\"\"",
            "    key, _, command, queue, task_to_run = task_tuple",
            "    try:",
            "        with timeout(seconds=OPERATION_TIMEOUT):",
            "            result = task_to_run.apply_async(args=[command], queue=queue)",
            "    except Exception as e:  # pylint: disable=broad-except",
            "        exception_traceback = \"Celery Task ID: {}\\n{}\".format(key, traceback.format_exc())",
            "        result = ExceptionWithTraceback(e, exception_traceback)",
            "",
            "    return key, command, result",
            "",
            "",
            "class CeleryExecutor(BaseExecutor):",
            "    \"\"\"",
            "    CeleryExecutor is recommended for production use of Airflow. It allows",
            "    distributing the execution of task instances to multiple worker nodes.",
            "",
            "    Celery is a simple, flexible and reliable distributed system to process",
            "    vast amounts of messages, while providing operations with the tools",
            "    required to maintain such a system.",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        super().__init__()",
            "",
            "        # Celery doesn't support bulk sending the tasks (which can become a bottleneck on bigger clusters)",
            "        # so we use a multiprocessing pool to speed this up.",
            "        # How many worker processes are created for checking celery task state.",
            "        self._sync_parallelism = conf.getint('celery', 'SYNC_PARALLELISM')",
            "        if self._sync_parallelism == 0:",
            "            self._sync_parallelism = max(1, cpu_count() - 1)",
            "        self.bulk_state_fetcher = BulkStateFetcher(self._sync_parallelism)",
            "        self.tasks = {}",
            "        self.last_state = {}",
            "",
            "    def start(self) -> None:",
            "        self.log.debug(",
            "            'Starting Celery Executor using %s processes for syncing',",
            "            self._sync_parallelism",
            "        )",
            "",
            "    def _num_tasks_per_send_process(self, to_send_count: int) -> int:",
            "        \"\"\"",
            "        How many Celery tasks should each worker process send.",
            "",
            "        :return: Number of tasks that should be sent per process",
            "        :rtype: int",
            "        \"\"\"",
            "        return max(1,",
            "                   int(math.ceil(1.0 * to_send_count / self._sync_parallelism)))",
            "",
            "    def trigger_tasks(self, open_slots: int) -> None:",
            "        \"\"\"",
            "        Overwrite trigger_tasks function from BaseExecutor",
            "",
            "        :param open_slots: Number of open slots",
            "        :return:",
            "        \"\"\"",
            "        sorted_queue = self.order_queued_tasks_by_priority()",
            "",
            "        task_tuples_to_send: List[TaskInstanceInCelery] = []",
            "",
            "        for _ in range(min((open_slots, len(self.queued_tasks)))):",
            "            key, (command, _, queue, simple_ti) = sorted_queue.pop(0)",
            "            task_tuples_to_send.append((key, simple_ti, command, queue, execute_command))",
            "",
            "        if task_tuples_to_send:",
            "            first_task = next(t[4] for t in task_tuples_to_send)",
            "",
            "            # Celery state queries will stuck if we do not use one same backend",
            "            # for all tasks.",
            "            cached_celery_backend = first_task.backend",
            "",
            "            key_and_async_results = self._send_tasks_to_celery(task_tuples_to_send)",
            "            self.log.debug('Sent all tasks.')",
            "",
            "            for key, command, result in key_and_async_results:",
            "                if isinstance(result, ExceptionWithTraceback):",
            "                    self.log.error(  # pylint: disable=logging-not-lazy",
            "                        CELERY_SEND_ERR_MSG_HEADER + \":%s\\n%s\\n\", result.exception, result.traceback",
            "                    )",
            "                elif result is not None:",
            "                    # Only pops when enqueued successfully, otherwise keep it",
            "                    # and expect scheduler loop to deal with it.",
            "                    self.queued_tasks.pop(key)",
            "                    result.backend = cached_celery_backend",
            "                    self.running.add(key)",
            "                    self.tasks[key] = result",
            "                    self.last_state[key] = celery_states.PENDING",
            "",
            "    def _send_tasks_to_celery(self, task_tuples_to_send):",
            "        if len(task_tuples_to_send) == 1:",
            "            # One tuple, so send it in the main thread.",
            "            return [",
            "                send_task_to_executor(task_tuples_to_send[0])",
            "            ]",
            "        # Use chunks instead of a work queue to reduce context switching",
            "        # since tasks are roughly uniform in size",
            "        chunksize = self._num_tasks_per_send_process(len(task_tuples_to_send))",
            "        num_processes = min(len(task_tuples_to_send), self._sync_parallelism)",
            "        with Pool(processes=num_processes) as send_pool:",
            "            key_and_async_results = send_pool.map(",
            "                send_task_to_executor,",
            "                task_tuples_to_send,",
            "                chunksize=chunksize)",
            "        return key_and_async_results",
            "",
            "    def sync(self) -> None:",
            "        if not self.tasks:",
            "            self.log.debug(\"No task to query celery, skipping sync\")",
            "            return",
            "        self.update_all_task_states()",
            "",
            "    def update_all_task_states(self) -> None:",
            "        \"\"\"Updates states of the tasks.\"\"\"",
            "",
            "        self.log.debug(\"Inquiring about %s celery task(s)\", len(self.tasks))",
            "        state_and_info_by_celery_task_id = self.bulk_state_fetcher.get_many(self.tasks.values())",
            "",
            "        self.log.debug(\"Inquiries completed.\")",
            "        for key, async_result in list(self.tasks.items()):",
            "            state, info = state_and_info_by_celery_task_id.get(async_result.task_id)",
            "            if state:",
            "                self.update_task_state(key, state, info)",
            "",
            "    def update_task_state(self, key: TaskInstanceKeyType, state: str, info: Any) -> None:",
            "        \"\"\"Updates state of a single task.\"\"\"",
            "        # noinspection PyBroadException",
            "        try:",
            "            if self.last_state[key] != state:",
            "                if state == celery_states.SUCCESS:",
            "                    self.success(key, info)",
            "                    del self.tasks[key]",
            "                    del self.last_state[key]",
            "                elif state == celery_states.FAILURE:",
            "                    self.fail(key, info)",
            "                    del self.tasks[key]",
            "                    del self.last_state[key]",
            "                elif state == celery_states.REVOKED:",
            "                    self.fail(key, info)",
            "                    del self.tasks[key]",
            "                    del self.last_state[key]",
            "                else:",
            "                    self.log.info(\"Unexpected state: %s\", state)",
            "                    self.last_state[key] = state",
            "        except Exception:  # pylint: disable=broad-except",
            "            self.log.exception(\"Error syncing the Celery executor, ignoring it.\")",
            "",
            "    def end(self, synchronous: bool = False) -> None:",
            "        if synchronous:",
            "            while any([task.state not in celery_states.READY_STATES for task in self.tasks.values()]):",
            "                time.sleep(5)",
            "        self.sync()",
            "",
            "    def execute_async(self,",
            "                      key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None):",
            "        \"\"\"Do not allow async execution for Celery executor.\"\"\"",
            "        raise AirflowException(\"No Async execution for Celery executor.\")",
            "",
            "    def terminate(self):",
            "        pass",
            "",
            "",
            "def fetch_celery_task_state(async_result: AsyncResult) -> \\",
            "        Tuple[str, Union[str, ExceptionWithTraceback], Any]:",
            "    \"\"\"",
            "    Fetch and return the state of the given celery task. The scope of this function is",
            "    global so that it can be called by subprocesses in the pool.",
            "",
            "    :param async_result: a tuple of the Celery task key and the async Celery object used",
            "        to fetch the task's state",
            "    :type async_result: tuple(str, celery.result.AsyncResult)",
            "    :return: a tuple of the Celery task key and the Celery state and the celery info",
            "        of the task",
            "    :rtype: tuple[str, str, str]",
            "    \"\"\"",
            "",
            "    try:",
            "        with timeout(seconds=OPERATION_TIMEOUT):",
            "            # Accessing state property of celery task will make actual network request",
            "            # to get the current state of the task",
            "            info = async_result.info if hasattr(async_result, 'info') else None",
            "            return async_result.task_id, async_result.state, info",
            "    except Exception as e:  # pylint: disable=broad-except",
            "        exception_traceback = f\"Celery Task ID: {async_result}\\n{traceback.format_exc()}\"",
            "        return async_result.task_id, ExceptionWithTraceback(e, exception_traceback), None",
            "",
            "",
            "def _tasks_list_to_task_ids(async_tasks) -> Set[str]:",
            "    return {a.task_id for a in async_tasks}",
            "",
            "",
            "class BulkStateFetcher(LoggingMixin):",
            "    \"\"\"",
            "    Gets status for many Celery tasks using the best method available",
            "",
            "    If BaseKeyValueStoreBackend is used as result backend, the mget method is used.",
            "    If DatabaseBackend is used as result backend, the SELECT ...WHER task_id IN (...) query is used",
            "    Otherwise, multiprocessing.Pool will be used. Each task status will be downloaded individually.",
            "    \"\"\"",
            "    def __init__(self, sync_parralelism=None):",
            "        super().__init__()",
            "        self._sync_parallelism = sync_parralelism",
            "",
            "    def get_many(self, async_results) -> Mapping[str, EventBufferValueType]:",
            "        \"\"\"",
            "        Gets status for many Celery tasks using the best method available.",
            "        \"\"\"",
            "        if isinstance(app.backend, BaseKeyValueStoreBackend):",
            "            result = self._get_many_from_kv_backend(async_results)",
            "            return result",
            "        if isinstance(app.backend, DatabaseBackend):",
            "            result = self._get_many_from_db_backend(async_results)",
            "            return result",
            "        result = self._get_many_using_multiprocessing(async_results)",
            "        self.log.debug(\"Fetched %d states for %d task\", len(result), len(async_results))",
            "        return result",
            "",
            "    def _get_many_from_kv_backend(self, async_tasks) -> Mapping[str, EventBufferValueType]:",
            "        task_ids = _tasks_list_to_task_ids(async_tasks)",
            "        keys = [app.backend.get_key_for_task(k) for k in task_ids]",
            "        values = app.backend.mget(keys)",
            "        task_results = [app.backend.decode_result(v) for v in values if v]",
            "        task_results_by_task_id = {task_result[\"task_id\"]: task_result for task_result in task_results}",
            "",
            "        return self._prepare_state_and_info_by_task_dict(task_ids, task_results_by_task_id)",
            "",
            "    def _get_many_from_db_backend(self, async_tasks) -> Mapping[str, EventBufferValueType]:",
            "        task_ids = _tasks_list_to_task_ids(async_tasks)",
            "        session = app.backend.ResultSession()",
            "        with session_cleanup(session):",
            "            tasks = session.query(TaskDb).filter(TaskDb.task_id.in_(task_ids)).all()",
            "",
            "        task_results = [app.backend.meta_from_decoded(task.to_dict()) for task in tasks]",
            "        task_results_by_task_id = {task_result[\"task_id\"]: task_result for task_result in task_results}",
            "        return self._prepare_state_and_info_by_task_dict(task_ids, task_results_by_task_id)",
            "",
            "    @staticmethod",
            "    def _prepare_state_and_info_by_task_dict(task_ids,",
            "                                             task_results_by_task_id) -> Mapping[str, EventBufferValueType]:",
            "        state_info: MutableMapping[str, EventBufferValueType] = {}",
            "        for task_id in task_ids:",
            "            task_result = task_results_by_task_id.get(task_id)",
            "            if task_result:",
            "                state = task_result[\"status\"]",
            "                info = None if not hasattr(task_result, \"info\") else task_result[\"info\"]",
            "            else:",
            "                state = celery_states.PENDING",
            "                info = None",
            "            state_info[task_id] = state, info",
            "        return state_info",
            "",
            "    def _get_many_using_multiprocessing(self, async_results) -> Mapping[str, EventBufferValueType]:",
            "        num_process = min(len(async_results), self._sync_parallelism)",
            "",
            "        with Pool(processes=num_process) as sync_pool:",
            "            chunksize = max(1, math.floor(math.ceil(1.0 * len(async_results) / self._sync_parallelism)))",
            "",
            "            task_id_to_states_and_info = sync_pool.map(",
            "                fetch_celery_task_state,",
            "                async_results,",
            "                chunksize=chunksize)",
            "",
            "            states_and_info_by_task_id: MutableMapping[str, EventBufferValueType] = {}",
            "            for task_id, state_or_exception, info in task_id_to_states_and_info:",
            "                if isinstance(state_or_exception, ExceptionWithTraceback):",
            "                    self.log.error(  # pylint: disable=logging-not-lazy",
            "                        CELERY_FETCH_ERR_MSG_HEADER + \":%s\\n%s\\n\",",
            "                        state_or_exception.exception, state_or_exception.traceback",
            "                    )",
            "                else:",
            "                    states_and_info_by_task_id[task_id] = state_or_exception, info",
            "        return states_and_info_by_task_id"
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"CeleryExecutor",
            "",
            ".. seealso::",
            "    For more information on how the CeleryExecutor works, take a look at the guide:",
            "    :ref:`executor:CeleryExecutor`",
            "\"\"\"",
            "import logging",
            "import math",
            "import os",
            "import subprocess",
            "import time",
            "import traceback",
            "from multiprocessing import Pool, cpu_count",
            "from typing import Any, List, Mapping, MutableMapping, Optional, Set, Tuple, Union",
            "",
            "from celery import Celery, Task, states as celery_states",
            "from celery.backends.base import BaseKeyValueStoreBackend",
            "from celery.backends.database import DatabaseBackend, Task as TaskDb, session_cleanup",
            "from celery.result import AsyncResult",
            "",
            "from airflow.config_templates.default_celery import DEFAULT_CELERY_CONFIG",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowException",
            "from airflow.executors.base_executor import BaseExecutor, CommandType, EventBufferValueType",
            "from airflow.models.taskinstance import SimpleTaskInstance, TaskInstanceKeyType",
            "from airflow.utils.log.logging_mixin import LoggingMixin",
            "from airflow.utils.net import get_hostname",
            "from airflow.utils.timeout import timeout",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "# Make it constant for unit test.",
            "CELERY_FETCH_ERR_MSG_HEADER = 'Error fetching Celery task state'",
            "",
            "CELERY_SEND_ERR_MSG_HEADER = 'Error sending Celery task'",
            "",
            "OPERATION_TIMEOUT = conf.getint('celery', 'operation_timeout', fallback=2)",
            "",
            "'''",
            "To start the celery worker, run the command:",
            "airflow celery worker",
            "'''",
            "",
            "if conf.has_option('celery', 'celery_config_options'):",
            "    celery_configuration = conf.getimport('celery', 'celery_config_options')",
            "else:",
            "    celery_configuration = DEFAULT_CELERY_CONFIG",
            "",
            "app = Celery(",
            "    conf.get('celery', 'CELERY_APP_NAME'),",
            "    config_source=celery_configuration)",
            "",
            "",
            "@app.task",
            "def execute_command(command_to_exec: CommandType) -> None:",
            "    \"\"\"Executes command.\"\"\"",
            "    if command_to_exec[0:3] != [\"airflow\", \"tasks\", \"run\"]:",
            "        raise ValueError('The command must start with [\"airflow\", \"tasks\", \"run\"].')",
            "",
            "    log.info(\"Executing command in Celery: %s\", command_to_exec)",
            "    env = os.environ.copy()",
            "    try:",
            "        subprocess.check_call(command_to_exec, stderr=subprocess.STDOUT,",
            "                              close_fds=True, env=env)",
            "    except subprocess.CalledProcessError as e:",
            "        log.exception('execute_command encountered a CalledProcessError')",
            "        log.error(e.output)",
            "        msg = 'Celery command failed on host: ' + get_hostname()",
            "        raise AirflowException(msg)",
            "",
            "",
            "class ExceptionWithTraceback:",
            "    \"\"\"",
            "    Wrapper class used to propagate exceptions to parent processes from subprocesses.",
            "",
            "    :param exception: The exception to wrap",
            "    :type exception: Exception",
            "    :param exception_traceback: The stacktrace to wrap",
            "    :type exception_traceback: str",
            "    \"\"\"",
            "",
            "    def __init__(self, exception: Exception, exception_traceback: str):",
            "        self.exception = exception",
            "        self.traceback = exception_traceback",
            "",
            "",
            "# Task instance that is sent over Celery queues",
            "# TaskInstanceKeyType, SimpleTaskInstance, Command, queue_name, CallableTask",
            "TaskInstanceInCelery = Tuple[TaskInstanceKeyType, SimpleTaskInstance, CommandType, Optional[str], Task]",
            "",
            "",
            "def send_task_to_executor(task_tuple: TaskInstanceInCelery) \\",
            "        -> Tuple[TaskInstanceKeyType, CommandType, Union[AsyncResult, ExceptionWithTraceback]]:",
            "    \"\"\"Sends task to executor.\"\"\"",
            "    key, _, command, queue, task_to_run = task_tuple",
            "    try:",
            "        with timeout(seconds=OPERATION_TIMEOUT):",
            "            result = task_to_run.apply_async(args=[command], queue=queue)",
            "    except Exception as e:  # pylint: disable=broad-except",
            "        exception_traceback = \"Celery Task ID: {}\\n{}\".format(key, traceback.format_exc())",
            "        result = ExceptionWithTraceback(e, exception_traceback)",
            "",
            "    return key, command, result",
            "",
            "",
            "class CeleryExecutor(BaseExecutor):",
            "    \"\"\"",
            "    CeleryExecutor is recommended for production use of Airflow. It allows",
            "    distributing the execution of task instances to multiple worker nodes.",
            "",
            "    Celery is a simple, flexible and reliable distributed system to process",
            "    vast amounts of messages, while providing operations with the tools",
            "    required to maintain such a system.",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        super().__init__()",
            "",
            "        # Celery doesn't support bulk sending the tasks (which can become a bottleneck on bigger clusters)",
            "        # so we use a multiprocessing pool to speed this up.",
            "        # How many worker processes are created for checking celery task state.",
            "        self._sync_parallelism = conf.getint('celery', 'SYNC_PARALLELISM')",
            "        if self._sync_parallelism == 0:",
            "            self._sync_parallelism = max(1, cpu_count() - 1)",
            "        self.bulk_state_fetcher = BulkStateFetcher(self._sync_parallelism)",
            "        self.tasks = {}",
            "        self.last_state = {}",
            "",
            "    def start(self) -> None:",
            "        self.log.debug(",
            "            'Starting Celery Executor using %s processes for syncing',",
            "            self._sync_parallelism",
            "        )",
            "",
            "    def _num_tasks_per_send_process(self, to_send_count: int) -> int:",
            "        \"\"\"",
            "        How many Celery tasks should each worker process send.",
            "",
            "        :return: Number of tasks that should be sent per process",
            "        :rtype: int",
            "        \"\"\"",
            "        return max(1,",
            "                   int(math.ceil(1.0 * to_send_count / self._sync_parallelism)))",
            "",
            "    def trigger_tasks(self, open_slots: int) -> None:",
            "        \"\"\"",
            "        Overwrite trigger_tasks function from BaseExecutor",
            "",
            "        :param open_slots: Number of open slots",
            "        :return:",
            "        \"\"\"",
            "        sorted_queue = self.order_queued_tasks_by_priority()",
            "",
            "        task_tuples_to_send: List[TaskInstanceInCelery] = []",
            "",
            "        for _ in range(min((open_slots, len(self.queued_tasks)))):",
            "            key, (command, _, queue, simple_ti) = sorted_queue.pop(0)",
            "            task_tuples_to_send.append((key, simple_ti, command, queue, execute_command))",
            "",
            "        if task_tuples_to_send:",
            "            first_task = next(t[4] for t in task_tuples_to_send)",
            "",
            "            # Celery state queries will stuck if we do not use one same backend",
            "            # for all tasks.",
            "            cached_celery_backend = first_task.backend",
            "",
            "            key_and_async_results = self._send_tasks_to_celery(task_tuples_to_send)",
            "            self.log.debug('Sent all tasks.')",
            "",
            "            for key, command, result in key_and_async_results:",
            "                if isinstance(result, ExceptionWithTraceback):",
            "                    self.log.error(  # pylint: disable=logging-not-lazy",
            "                        CELERY_SEND_ERR_MSG_HEADER + \":%s\\n%s\\n\", result.exception, result.traceback",
            "                    )",
            "                elif result is not None:",
            "                    # Only pops when enqueued successfully, otherwise keep it",
            "                    # and expect scheduler loop to deal with it.",
            "                    self.queued_tasks.pop(key)",
            "                    result.backend = cached_celery_backend",
            "                    self.running.add(key)",
            "                    self.tasks[key] = result",
            "                    self.last_state[key] = celery_states.PENDING",
            "",
            "    def _send_tasks_to_celery(self, task_tuples_to_send):",
            "        if len(task_tuples_to_send) == 1:",
            "            # One tuple, so send it in the main thread.",
            "            return [",
            "                send_task_to_executor(task_tuples_to_send[0])",
            "            ]",
            "        # Use chunks instead of a work queue to reduce context switching",
            "        # since tasks are roughly uniform in size",
            "        chunksize = self._num_tasks_per_send_process(len(task_tuples_to_send))",
            "        num_processes = min(len(task_tuples_to_send), self._sync_parallelism)",
            "        with Pool(processes=num_processes) as send_pool:",
            "            key_and_async_results = send_pool.map(",
            "                send_task_to_executor,",
            "                task_tuples_to_send,",
            "                chunksize=chunksize)",
            "        return key_and_async_results",
            "",
            "    def sync(self) -> None:",
            "        if not self.tasks:",
            "            self.log.debug(\"No task to query celery, skipping sync\")",
            "            return",
            "        self.update_all_task_states()",
            "",
            "    def update_all_task_states(self) -> None:",
            "        \"\"\"Updates states of the tasks.\"\"\"",
            "",
            "        self.log.debug(\"Inquiring about %s celery task(s)\", len(self.tasks))",
            "        state_and_info_by_celery_task_id = self.bulk_state_fetcher.get_many(self.tasks.values())",
            "",
            "        self.log.debug(\"Inquiries completed.\")",
            "        for key, async_result in list(self.tasks.items()):",
            "            state, info = state_and_info_by_celery_task_id.get(async_result.task_id)",
            "            if state:",
            "                self.update_task_state(key, state, info)",
            "",
            "    def update_task_state(self, key: TaskInstanceKeyType, state: str, info: Any) -> None:",
            "        \"\"\"Updates state of a single task.\"\"\"",
            "        # noinspection PyBroadException",
            "        try:",
            "            if self.last_state[key] != state:",
            "                if state == celery_states.SUCCESS:",
            "                    self.success(key, info)",
            "                    del self.tasks[key]",
            "                    del self.last_state[key]",
            "                elif state == celery_states.FAILURE:",
            "                    self.fail(key, info)",
            "                    del self.tasks[key]",
            "                    del self.last_state[key]",
            "                elif state == celery_states.REVOKED:",
            "                    self.fail(key, info)",
            "                    del self.tasks[key]",
            "                    del self.last_state[key]",
            "                else:",
            "                    self.log.info(\"Unexpected state: %s\", state)",
            "                    self.last_state[key] = state",
            "        except Exception:  # pylint: disable=broad-except",
            "            self.log.exception(\"Error syncing the Celery executor, ignoring it.\")",
            "",
            "    def end(self, synchronous: bool = False) -> None:",
            "        if synchronous:",
            "            while any([task.state not in celery_states.READY_STATES for task in self.tasks.values()]):",
            "                time.sleep(5)",
            "        self.sync()",
            "",
            "    def execute_async(self,",
            "                      key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None):",
            "        \"\"\"Do not allow async execution for Celery executor.\"\"\"",
            "        raise AirflowException(\"No Async execution for Celery executor.\")",
            "",
            "    def terminate(self):",
            "        pass",
            "",
            "",
            "def fetch_celery_task_state(async_result: AsyncResult) -> \\",
            "        Tuple[str, Union[str, ExceptionWithTraceback], Any]:",
            "    \"\"\"",
            "    Fetch and return the state of the given celery task. The scope of this function is",
            "    global so that it can be called by subprocesses in the pool.",
            "",
            "    :param async_result: a tuple of the Celery task key and the async Celery object used",
            "        to fetch the task's state",
            "    :type async_result: tuple(str, celery.result.AsyncResult)",
            "    :return: a tuple of the Celery task key and the Celery state and the celery info",
            "        of the task",
            "    :rtype: tuple[str, str, str]",
            "    \"\"\"",
            "",
            "    try:",
            "        with timeout(seconds=OPERATION_TIMEOUT):",
            "            # Accessing state property of celery task will make actual network request",
            "            # to get the current state of the task",
            "            info = async_result.info if hasattr(async_result, 'info') else None",
            "            return async_result.task_id, async_result.state, info",
            "    except Exception as e:  # pylint: disable=broad-except",
            "        exception_traceback = f\"Celery Task ID: {async_result}\\n{traceback.format_exc()}\"",
            "        return async_result.task_id, ExceptionWithTraceback(e, exception_traceback), None",
            "",
            "",
            "def _tasks_list_to_task_ids(async_tasks) -> Set[str]:",
            "    return {a.task_id for a in async_tasks}",
            "",
            "",
            "class BulkStateFetcher(LoggingMixin):",
            "    \"\"\"",
            "    Gets status for many Celery tasks using the best method available",
            "",
            "    If BaseKeyValueStoreBackend is used as result backend, the mget method is used.",
            "    If DatabaseBackend is used as result backend, the SELECT ...WHER task_id IN (...) query is used",
            "    Otherwise, multiprocessing.Pool will be used. Each task status will be downloaded individually.",
            "    \"\"\"",
            "    def __init__(self, sync_parralelism=None):",
            "        super().__init__()",
            "        self._sync_parallelism = sync_parralelism",
            "",
            "    def get_many(self, async_results) -> Mapping[str, EventBufferValueType]:",
            "        \"\"\"",
            "        Gets status for many Celery tasks using the best method available.",
            "        \"\"\"",
            "        if isinstance(app.backend, BaseKeyValueStoreBackend):",
            "            result = self._get_many_from_kv_backend(async_results)",
            "            return result",
            "        if isinstance(app.backend, DatabaseBackend):",
            "            result = self._get_many_from_db_backend(async_results)",
            "            return result",
            "        result = self._get_many_using_multiprocessing(async_results)",
            "        self.log.debug(\"Fetched %d states for %d task\", len(result), len(async_results))",
            "        return result",
            "",
            "    def _get_many_from_kv_backend(self, async_tasks) -> Mapping[str, EventBufferValueType]:",
            "        task_ids = _tasks_list_to_task_ids(async_tasks)",
            "        keys = [app.backend.get_key_for_task(k) for k in task_ids]",
            "        values = app.backend.mget(keys)",
            "        task_results = [app.backend.decode_result(v) for v in values if v]",
            "        task_results_by_task_id = {task_result[\"task_id\"]: task_result for task_result in task_results}",
            "",
            "        return self._prepare_state_and_info_by_task_dict(task_ids, task_results_by_task_id)",
            "",
            "    def _get_many_from_db_backend(self, async_tasks) -> Mapping[str, EventBufferValueType]:",
            "        task_ids = _tasks_list_to_task_ids(async_tasks)",
            "        session = app.backend.ResultSession()",
            "        with session_cleanup(session):",
            "            tasks = session.query(TaskDb).filter(TaskDb.task_id.in_(task_ids)).all()",
            "",
            "        task_results = [app.backend.meta_from_decoded(task.to_dict()) for task in tasks]",
            "        task_results_by_task_id = {task_result[\"task_id\"]: task_result for task_result in task_results}",
            "        return self._prepare_state_and_info_by_task_dict(task_ids, task_results_by_task_id)",
            "",
            "    @staticmethod",
            "    def _prepare_state_and_info_by_task_dict(task_ids,",
            "                                             task_results_by_task_id) -> Mapping[str, EventBufferValueType]:",
            "        state_info: MutableMapping[str, EventBufferValueType] = {}",
            "        for task_id in task_ids:",
            "            task_result = task_results_by_task_id.get(task_id)",
            "            if task_result:",
            "                state = task_result[\"status\"]",
            "                info = None if not hasattr(task_result, \"info\") else task_result[\"info\"]",
            "            else:",
            "                state = celery_states.PENDING",
            "                info = None",
            "            state_info[task_id] = state, info",
            "        return state_info",
            "",
            "    def _get_many_using_multiprocessing(self, async_results) -> Mapping[str, EventBufferValueType]:",
            "        num_process = min(len(async_results), self._sync_parallelism)",
            "",
            "        with Pool(processes=num_process) as sync_pool:",
            "            chunksize = max(1, math.floor(math.ceil(1.0 * len(async_results) / self._sync_parallelism)))",
            "",
            "            task_id_to_states_and_info = sync_pool.map(",
            "                fetch_celery_task_state,",
            "                async_results,",
            "                chunksize=chunksize)",
            "",
            "            states_and_info_by_task_id: MutableMapping[str, EventBufferValueType] = {}",
            "            for task_id, state_or_exception, info in task_id_to_states_and_info:",
            "                if isinstance(state_or_exception, ExceptionWithTraceback):",
            "                    self.log.error(  # pylint: disable=logging-not-lazy",
            "                        CELERY_FETCH_ERR_MSG_HEADER + \":%s\\n%s\\n\",",
            "                        state_or_exception.exception, state_or_exception.traceback",
            "                    )",
            "                else:",
            "                    states_and_info_by_task_id[task_id] = state_or_exception, info",
            "        return states_and_info_by_task_id"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "tensorflow.python.kernel_tests.nn_ops.lrn_op_test.LRNOpTest._LRN"
        ]
    },
    "airflow/executors/dask_executor.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "                       queue: Optional[str] = None,"
            },
            "1": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "                       executor_config: Optional[Any] = None) -> None:"
            },
            "2": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+        if command[0:3] != [\"airflow\", \"tasks\", \"run\"]:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+            raise ValueError('The command must start with [\"airflow\", \"tasks\", \"run\"].')"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "         def airflow_run():"
            },
            "7": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "             return subprocess.check_call(command, close_fds=True)"
            },
            "8": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"",
            "DaskExecutor",
            "",
            ".. seealso::",
            "    For more information on how the DaskExecutor works, take a look at the guide:",
            "    :ref:`executor:DaskExecutor`",
            "\"\"\"",
            "import subprocess",
            "from typing import Any, Dict, Optional",
            "",
            "from distributed import Client, Future, as_completed",
            "from distributed.security import Security",
            "",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowException",
            "from airflow.executors.base_executor import NOT_STARTED_MESSAGE, BaseExecutor, CommandType",
            "from airflow.models.taskinstance import TaskInstanceKeyType",
            "",
            "",
            "class DaskExecutor(BaseExecutor):",
            "    \"\"\"",
            "    DaskExecutor submits tasks to a Dask Distributed cluster.",
            "    \"\"\"",
            "    def __init__(self, cluster_address=None):",
            "        super().__init__(parallelism=0)",
            "        if cluster_address is None:",
            "            cluster_address = conf.get('dask', 'cluster_address')",
            "        if not cluster_address:",
            "            raise ValueError('Please provide a Dask cluster address in airflow.cfg')",
            "        self.cluster_address = cluster_address",
            "        # ssl / tls parameters",
            "        self.tls_ca = conf.get('dask', 'tls_ca')",
            "        self.tls_key = conf.get('dask', 'tls_key')",
            "        self.tls_cert = conf.get('dask', 'tls_cert')",
            "        self.client: Optional[Client] = None",
            "        self.futures: Optional[Dict[Future, TaskInstanceKeyType]] = None",
            "",
            "    def start(self) -> None:",
            "        if self.tls_ca or self.tls_key or self.tls_cert:",
            "            security = Security(",
            "                tls_client_key=self.tls_key,",
            "                tls_client_cert=self.tls_cert,",
            "                tls_ca_file=self.tls_ca,",
            "                require_encryption=True,",
            "            )",
            "        else:",
            "            security = None",
            "",
            "        self.client = Client(self.cluster_address, security=security)",
            "        self.futures = {}",
            "",
            "    def execute_async(self,",
            "                      key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None) -> None:",
            "",
            "        def airflow_run():",
            "            return subprocess.check_call(command, close_fds=True)",
            "",
            "        if not self.client:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "",
            "        future = self.client.submit(airflow_run, pure=False)",
            "        self.futures[future] = key  # type: ignore",
            "",
            "    def _process_future(self, future: Future) -> None:",
            "        if not self.futures:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if future.done():",
            "            key = self.futures[future]",
            "            if future.exception():",
            "                self.log.error(\"Failed to execute task: %s\", repr(future.exception()))",
            "                self.fail(key)",
            "            elif future.cancelled():",
            "                self.log.error(\"Failed to execute task\")",
            "                self.fail(key)",
            "            else:",
            "                self.success(key)",
            "            self.futures.pop(future)",
            "",
            "    def sync(self) -> None:",
            "        if not self.futures:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        # make a copy so futures can be popped during iteration",
            "        for future in self.futures.copy():",
            "            self._process_future(future)",
            "",
            "    def end(self) -> None:",
            "        if not self.client:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.futures:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.client.cancel(list(self.futures.keys()))",
            "        for future in as_completed(self.futures.copy()):",
            "            self._process_future(future)",
            "",
            "    def terminate(self):",
            "        if not self.futures:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.client.cancel(self.futures.keys())",
            "        self.end()"
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"",
            "DaskExecutor",
            "",
            ".. seealso::",
            "    For more information on how the DaskExecutor works, take a look at the guide:",
            "    :ref:`executor:DaskExecutor`",
            "\"\"\"",
            "import subprocess",
            "from typing import Any, Dict, Optional",
            "",
            "from distributed import Client, Future, as_completed",
            "from distributed.security import Security",
            "",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowException",
            "from airflow.executors.base_executor import NOT_STARTED_MESSAGE, BaseExecutor, CommandType",
            "from airflow.models.taskinstance import TaskInstanceKeyType",
            "",
            "",
            "class DaskExecutor(BaseExecutor):",
            "    \"\"\"",
            "    DaskExecutor submits tasks to a Dask Distributed cluster.",
            "    \"\"\"",
            "    def __init__(self, cluster_address=None):",
            "        super().__init__(parallelism=0)",
            "        if cluster_address is None:",
            "            cluster_address = conf.get('dask', 'cluster_address')",
            "        if not cluster_address:",
            "            raise ValueError('Please provide a Dask cluster address in airflow.cfg')",
            "        self.cluster_address = cluster_address",
            "        # ssl / tls parameters",
            "        self.tls_ca = conf.get('dask', 'tls_ca')",
            "        self.tls_key = conf.get('dask', 'tls_key')",
            "        self.tls_cert = conf.get('dask', 'tls_cert')",
            "        self.client: Optional[Client] = None",
            "        self.futures: Optional[Dict[Future, TaskInstanceKeyType]] = None",
            "",
            "    def start(self) -> None:",
            "        if self.tls_ca or self.tls_key or self.tls_cert:",
            "            security = Security(",
            "                tls_client_key=self.tls_key,",
            "                tls_client_cert=self.tls_cert,",
            "                tls_ca_file=self.tls_ca,",
            "                require_encryption=True,",
            "            )",
            "        else:",
            "            security = None",
            "",
            "        self.client = Client(self.cluster_address, security=security)",
            "        self.futures = {}",
            "",
            "    def execute_async(self,",
            "                      key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None) -> None:",
            "",
            "        if command[0:3] != [\"airflow\", \"tasks\", \"run\"]:",
            "            raise ValueError('The command must start with [\"airflow\", \"tasks\", \"run\"].')",
            "",
            "        def airflow_run():",
            "            return subprocess.check_call(command, close_fds=True)",
            "",
            "        if not self.client:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "",
            "        future = self.client.submit(airflow_run, pure=False)",
            "        self.futures[future] = key  # type: ignore",
            "",
            "    def _process_future(self, future: Future) -> None:",
            "        if not self.futures:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if future.done():",
            "            key = self.futures[future]",
            "            if future.exception():",
            "                self.log.error(\"Failed to execute task: %s\", repr(future.exception()))",
            "                self.fail(key)",
            "            elif future.cancelled():",
            "                self.log.error(\"Failed to execute task\")",
            "                self.fail(key)",
            "            else:",
            "                self.success(key)",
            "            self.futures.pop(future)",
            "",
            "    def sync(self) -> None:",
            "        if not self.futures:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        # make a copy so futures can be popped during iteration",
            "        for future in self.futures.copy():",
            "            self._process_future(future)",
            "",
            "    def end(self) -> None:",
            "        if not self.client:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.futures:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.client.cancel(list(self.futures.keys()))",
            "        for future in as_completed(self.futures.copy()):",
            "            self._process_future(future)",
            "",
            "    def terminate(self):",
            "        if not self.futures:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.client.cancel(self.futures.keys())",
            "        self.end()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "tensorflow.python.kernel_tests.nn_ops.lrn_op_test.LRNOpTest._LRN"
        ]
    },
    "airflow/executors/kubernetes_executor.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 459,
                "afterPatchRowNumber": 459,
                "PatchRowcode": "         if isinstance(command, str):"
            },
            "1": {
                "beforePatchRowNumber": 460,
                "afterPatchRowNumber": 460,
                "PatchRowcode": "             command = [command]"
            },
            "2": {
                "beforePatchRowNumber": 461,
                "afterPatchRowNumber": 461,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 462,
                "PatchRowcode": "+        if command[0] != \"airflow\":"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 463,
                "PatchRowcode": "+            raise ValueError('The first element of command must be equal to \"airflow\".')"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 464,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 462,
                "afterPatchRowNumber": 465,
                "PatchRowcode": "         pod = PodGenerator.construct_pod("
            },
            "7": {
                "beforePatchRowNumber": 463,
                "afterPatchRowNumber": 466,
                "PatchRowcode": "             namespace=self.namespace,"
            },
            "8": {
                "beforePatchRowNumber": 464,
                "afterPatchRowNumber": 467,
                "PatchRowcode": "             worker_uuid=self.worker_uuid,"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"",
            "KubernetesExecutor",
            "",
            ".. seealso::",
            "    For more information on how the KubernetesExecutor works, take a look at the guide:",
            "    :ref:`executor:KubernetesExecutor`",
            "\"\"\"",
            "import base64",
            "import datetime",
            "import json",
            "import multiprocessing",
            "import time",
            "from queue import Empty, Queue  # pylint: disable=unused-import",
            "from typing import Any, Dict, Optional, Tuple, Union",
            "",
            "import kubernetes",
            "from dateutil import parser",
            "from kubernetes import client, watch",
            "from kubernetes.client import Configuration",
            "from kubernetes.client.rest import ApiException",
            "from urllib3.exceptions import ReadTimeoutError",
            "",
            "from airflow import settings",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowConfigException, AirflowException",
            "from airflow.executors.base_executor import NOT_STARTED_MESSAGE, BaseExecutor, CommandType",
            "from airflow.kubernetes import pod_generator",
            "from airflow.kubernetes.kube_client import get_kube_client",
            "from airflow.kubernetes.pod_generator import MAX_POD_ID_LEN, PodGenerator",
            "from airflow.kubernetes.pod_launcher import PodLauncher",
            "from airflow.kubernetes.worker_configuration import WorkerConfiguration",
            "from airflow.models import KubeResourceVersion, KubeWorkerIdentifier, TaskInstance",
            "from airflow.models.taskinstance import TaskInstanceKeyType",
            "from airflow.utils.log.logging_mixin import LoggingMixin",
            "from airflow.utils.session import create_session, provide_session",
            "from airflow.utils.state import State",
            "",
            "MAX_LABEL_LEN = 63",
            "",
            "# TaskInstance key, command, configuration",
            "KubernetesJobType = Tuple[TaskInstanceKeyType, CommandType, Any]",
            "",
            "# key, state, pod_id, namespace, resource_version",
            "KubernetesResultsType = Tuple[TaskInstanceKeyType, Optional[str], str, str, str]",
            "",
            "# pod_id, namespace, state, labels, resource_version",
            "KubernetesWatchType = Tuple[str, str, Optional[str], Dict[str, str], str]",
            "",
            "",
            "class KubeConfig:  # pylint: disable=too-many-instance-attributes",
            "    \"\"\"Configuration for Kubernetes\"\"\"",
            "    core_section = 'core'",
            "    kubernetes_section = 'kubernetes'",
            "    logging_section = 'logging'",
            "",
            "    def __init__(self):  # pylint: disable=too-many-statements",
            "        configuration_dict = conf.as_dict(display_sensitive=True)",
            "        self.core_configuration = configuration_dict['core']",
            "        self.kube_secrets = configuration_dict.get('kubernetes_secrets', {})",
            "        self.kube_env_vars = configuration_dict.get('kubernetes_environment_variables', {})",
            "        self.env_from_configmap_ref = conf.get(self.kubernetes_section,",
            "                                               'env_from_configmap_ref')",
            "        self.env_from_secret_ref = conf.get(self.kubernetes_section,",
            "                                            'env_from_secret_ref')",
            "        self.airflow_home = settings.AIRFLOW_HOME",
            "        self.dags_folder = conf.get(self.core_section, 'dags_folder')",
            "        self.parallelism = conf.getint(self.core_section, 'parallelism')",
            "        self.worker_container_repository = conf.get(",
            "            self.kubernetes_section, 'worker_container_repository')",
            "        self.worker_container_tag = conf.get(",
            "            self.kubernetes_section, 'worker_container_tag')",
            "        self.kube_image = '{}:{}'.format(",
            "            self.worker_container_repository, self.worker_container_tag)",
            "        self.kube_image_pull_policy = conf.get(",
            "            self.kubernetes_section, \"worker_container_image_pull_policy\"",
            "        )",
            "        self.kube_node_selectors = configuration_dict.get('kubernetes_node_selectors', {})",
            "        self.pod_template_file = conf.get(self.kubernetes_section, 'pod_template_file',",
            "                                          fallback=None)",
            "",
            "        kube_worker_annotations = conf.get(self.kubernetes_section, 'worker_annotations')",
            "        if kube_worker_annotations:",
            "            self.kube_annotations = json.loads(kube_worker_annotations)",
            "        else:",
            "            self.kube_annotations = None",
            "",
            "        self.kube_labels = configuration_dict.get('kubernetes_labels', {})",
            "        self.delete_worker_pods = conf.getboolean(",
            "            self.kubernetes_section, 'delete_worker_pods')",
            "        self.delete_worker_pods_on_failure = conf.getboolean(",
            "            self.kubernetes_section, 'delete_worker_pods_on_failure')",
            "        self.worker_pods_creation_batch_size = conf.getint(",
            "            self.kubernetes_section, 'worker_pods_creation_batch_size')",
            "        self.worker_service_account_name = conf.get(",
            "            self.kubernetes_section, 'worker_service_account_name')",
            "        self.image_pull_secrets = conf.get(self.kubernetes_section, 'image_pull_secrets')",
            "",
            "        # NOTE: user can build the dags into the docker image directly,",
            "        # this will set to True if so",
            "        self.dags_in_image = conf.getboolean(self.kubernetes_section, 'dags_in_image')",
            "",
            "        # Run as user for pod security context",
            "        self.worker_run_as_user = self._get_security_context_val('run_as_user')",
            "        self.worker_fs_group = self._get_security_context_val('fs_group')",
            "",
            "        kube_worker_resources = conf.get(self.kubernetes_section, 'worker_resources')",
            "        if kube_worker_resources:",
            "            self.worker_resources = json.loads(kube_worker_resources)",
            "        else:",
            "            self.worker_resources = None",
            "",
            "        # NOTE: `git_repo` and `git_branch` must be specified together as a pair",
            "        # The http URL of the git repository to clone from",
            "        self.git_repo = conf.get(self.kubernetes_section, 'git_repo')",
            "        # The branch of the repository to be checked out",
            "        self.git_branch = conf.get(self.kubernetes_section, 'git_branch')",
            "        # Clone depth for git sync",
            "        self.git_sync_depth = conf.get(self.kubernetes_section, 'git_sync_depth')",
            "        # Optionally, the directory in the git repository containing the dags",
            "        self.git_subpath = conf.get(self.kubernetes_section, 'git_subpath')",
            "        # Optionally, the root directory for git operations",
            "        self.git_sync_root = conf.get(self.kubernetes_section, 'git_sync_root')",
            "        # Optionally, the name at which to publish the checked-out files under --root",
            "        self.git_sync_dest = conf.get(self.kubernetes_section, 'git_sync_dest')",
            "        # Optionally, the tag or hash to checkout",
            "        self.git_sync_rev = conf.get(self.kubernetes_section, 'git_sync_rev')",
            "        # Optionally, if git_dags_folder_mount_point is set the worker will use",
            "        # {git_dags_folder_mount_point}/{git_sync_dest}/{git_subpath} as dags_folder",
            "        self.git_dags_folder_mount_point = conf.get(self.kubernetes_section,",
            "                                                    'git_dags_folder_mount_point')",
            "",
            "        # Optionally a user may supply a (`git_user` AND `git_password`) OR",
            "        # (`git_ssh_key_secret_name` AND `git_ssh_key_secret_key`) for private repositories",
            "        self.git_user = conf.get(self.kubernetes_section, 'git_user')",
            "        self.git_password = conf.get(self.kubernetes_section, 'git_password')",
            "        self.git_ssh_key_secret_name = conf.get(self.kubernetes_section, 'git_ssh_key_secret_name')",
            "        self.git_ssh_known_hosts_configmap_name = conf.get(self.kubernetes_section,",
            "                                                           'git_ssh_known_hosts_configmap_name')",
            "        self.git_sync_credentials_secret = conf.get(self.kubernetes_section,",
            "                                                    'git_sync_credentials_secret')",
            "",
            "        # NOTE: The user may optionally use a volume claim to mount a PV containing",
            "        # DAGs directly",
            "        self.dags_volume_claim = conf.get(self.kubernetes_section, 'dags_volume_claim')",
            "",
            "        self.dags_volume_mount_point = conf.get(self.kubernetes_section, 'dags_volume_mount_point')",
            "",
            "        # This prop may optionally be set for PV Claims and is used to write logs",
            "        self.logs_volume_claim = conf.get(self.kubernetes_section, 'logs_volume_claim')",
            "",
            "        # This prop may optionally be set for PV Claims and is used to locate DAGs",
            "        # on a SubPath",
            "        self.dags_volume_subpath = conf.get(",
            "            self.kubernetes_section, 'dags_volume_subpath')",
            "",
            "        # This prop may optionally be set for PV Claims and is used to locate logs",
            "        # on a SubPath",
            "        self.logs_volume_subpath = conf.get(",
            "            self.kubernetes_section, 'logs_volume_subpath')",
            "",
            "        # Optionally, hostPath volume containing DAGs",
            "        self.dags_volume_host = conf.get(self.kubernetes_section, 'dags_volume_host')",
            "",
            "        # Optionally, write logs to a hostPath Volume",
            "        self.logs_volume_host = conf.get(self.kubernetes_section, 'logs_volume_host')",
            "",
            "        # This prop may optionally be set for PV Claims and is used to write logs",
            "        self.base_log_folder = conf.get(self.logging_section, 'base_log_folder')",
            "",
            "        # The Kubernetes Namespace in which the Scheduler and Webserver reside. Note",
            "        # that if your",
            "        # cluster has RBAC enabled, your scheduler may need service account permissions to",
            "        # create, watch, get, and delete pods in this namespace.",
            "        self.kube_namespace = conf.get(self.kubernetes_section, 'namespace')",
            "        # The Kubernetes Namespace in which pods will be created by the executor. Note",
            "        # that if your",
            "        # cluster has RBAC enabled, your workers may need service account permissions to",
            "        # interact with cluster components.",
            "        self.executor_namespace = conf.get(self.kubernetes_section, 'namespace')",
            "",
            "        # If the user is using the git-sync container to clone their repository via git,",
            "        # allow them to specify repository, tag, and pod name for the init container.",
            "        self.git_sync_container_repository = conf.get(",
            "            self.kubernetes_section, 'git_sync_container_repository')",
            "",
            "        self.git_sync_container_tag = conf.get(",
            "            self.kubernetes_section, 'git_sync_container_tag')",
            "        self.git_sync_container = '{}:{}'.format(",
            "            self.git_sync_container_repository, self.git_sync_container_tag)",
            "",
            "        self.git_sync_init_container_name = conf.get(",
            "            self.kubernetes_section, 'git_sync_init_container_name')",
            "",
            "        self.git_sync_run_as_user = self._get_security_context_val('git_sync_run_as_user')",
            "",
            "        # The worker pod may optionally have a  valid Airflow config loaded via a",
            "        # configmap",
            "        self.airflow_configmap = conf.get(self.kubernetes_section, 'airflow_configmap')",
            "",
            "        # The worker pod may optionally have a valid Airflow local settings loaded via a",
            "        # configmap",
            "        self.airflow_local_settings_configmap = conf.get(",
            "            self.kubernetes_section, 'airflow_local_settings_configmap')",
            "",
            "        affinity_json = conf.get(self.kubernetes_section, 'affinity')",
            "        if affinity_json:",
            "            self.kube_affinity = json.loads(affinity_json)",
            "        else:",
            "            self.kube_affinity = None",
            "",
            "        tolerations_json = conf.get(self.kubernetes_section, 'tolerations')",
            "        if tolerations_json:",
            "            self.kube_tolerations = json.loads(tolerations_json)",
            "        else:",
            "            self.kube_tolerations = None",
            "",
            "        kube_client_request_args = conf.get(self.kubernetes_section, 'kube_client_request_args')",
            "        if kube_client_request_args:",
            "            self.kube_client_request_args = json.loads(kube_client_request_args)",
            "            if self.kube_client_request_args['_request_timeout'] and \\",
            "                    isinstance(self.kube_client_request_args['_request_timeout'], list):",
            "                self.kube_client_request_args['_request_timeout'] = \\",
            "                    tuple(self.kube_client_request_args['_request_timeout'])",
            "        else:",
            "            self.kube_client_request_args = {}",
            "        self._validate()",
            "",
            "        delete_option_kwargs = conf.get(self.kubernetes_section, 'delete_option_kwargs')",
            "        if delete_option_kwargs:",
            "            self.delete_option_kwargs = json.loads(delete_option_kwargs)",
            "        else:",
            "            self.delete_option_kwargs = {}",
            "",
            "    # pod security context items should return integers",
            "    # and only return a blank string if contexts are not set.",
            "    def _get_security_context_val(self, scontext: str) -> Union[str, int]:",
            "        val = conf.get(self.kubernetes_section, scontext)",
            "        if not val:",
            "            return \"\"",
            "        else:",
            "            return int(val)",
            "",
            "    def _validate(self):",
            "        if self.pod_template_file:",
            "            return",
            "        # TODO: use XOR for dags_volume_claim and git_dags_folder_mount_point",
            "        # pylint: disable=too-many-boolean-expressions",
            "        if not self.dags_volume_claim \\",
            "            and not self.dags_volume_host \\",
            "            and not self.dags_in_image \\",
            "                and (not self.git_repo or not self.git_branch or not self.git_dags_folder_mount_point):",
            "            raise AirflowConfigException(",
            "                'In kubernetes mode the following must be set in the `kubernetes` '",
            "                'config section: `dags_volume_claim` '",
            "                'or `dags_volume_host` '",
            "                'or `dags_in_image` '",
            "                'or `git_repo and git_branch and git_dags_folder_mount_point`')",
            "        if self.git_repo \\",
            "            and (self.git_user or self.git_password) \\",
            "                and self.git_ssh_key_secret_name:",
            "            raise AirflowConfigException(",
            "                'In kubernetes mode, using `git_repo` to pull the DAGs: '",
            "                'for private repositories, either `git_user` and `git_password` '",
            "                'must be set for authentication through user credentials; '",
            "                'or `git_ssh_key_secret_name` must be set for authentication '",
            "                'through ssh key, but not both')",
            "        # pylint: enable=too-many-boolean-expressions",
            "",
            "",
            "class KubernetesJobWatcher(multiprocessing.Process, LoggingMixin):",
            "    \"\"\"Watches for Kubernetes jobs\"\"\"",
            "",
            "    def __init__(self,",
            "                 watcher_queue: 'Queue[KubernetesWatchType]',",
            "                 resource_version: Optional[str],",
            "                 worker_uuid: Optional[str],",
            "                 kube_config: Configuration):",
            "        super().__init__()",
            "        self.worker_uuid = worker_uuid",
            "        self.watcher_queue = watcher_queue",
            "        self.resource_version = resource_version",
            "        self.kube_config = kube_config",
            "",
            "    def run(self) -> None:",
            "        \"\"\"Performs watching\"\"\"",
            "        kube_client: client.CoreV1Api = get_kube_client()",
            "        if not self.worker_uuid:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        while True:",
            "            try:",
            "                self.resource_version = self._run(kube_client, self.resource_version,",
            "                                                  self.worker_uuid, self.kube_config)",
            "            except ReadTimeoutError:",
            "                self.log.warning(\"There was a timeout error accessing the Kube API. \"",
            "                                 \"Retrying request.\", exc_info=True)",
            "                time.sleep(1)",
            "            except Exception:",
            "                self.log.exception('Unknown error in KubernetesJobWatcher. Failing')",
            "                raise",
            "            else:",
            "                self.log.warning('Watch died gracefully, starting back up with: '",
            "                                 'last resource_version: %s', self.resource_version)",
            "",
            "    def _run(self,",
            "             kube_client: client.CoreV1Api,",
            "             resource_version: Optional[str],",
            "             worker_uuid: str,",
            "             kube_config: Any) -> Optional[str]:",
            "        self.log.info(",
            "            'Event: and now my watch begins starting at resource_version: %s',",
            "            resource_version",
            "        )",
            "        watcher = watch.Watch()",
            "",
            "        kwargs = {'label_selector': 'airflow-worker={}'.format(worker_uuid)}",
            "        if resource_version:",
            "            kwargs['resource_version'] = resource_version",
            "        if kube_config.kube_client_request_args:",
            "            for key, value in kube_config.kube_client_request_args.items():",
            "                kwargs[key] = value",
            "",
            "        last_resource_version: Optional[str] = None",
            "        for event in watcher.stream(kube_client.list_pod_for_all_namespaces, **kwargs):",
            "            task = event['object']",
            "            self.log.info(",
            "                'Event: %s had an event of type %s',",
            "                task.metadata.name, event['type']",
            "            )",
            "            if event['type'] == 'ERROR':",
            "                return self.process_error(event)",
            "            self.process_status(",
            "                pod_id=task.metadata.name,",
            "                namespace=task.metadata.namespace,",
            "                status=task.status.phase,",
            "                labels=task.metadata.labels,",
            "                resource_version=task.metadata.resource_version,",
            "                event=event,",
            "            )",
            "            last_resource_version = task.metadata.resource_version",
            "",
            "        return last_resource_version",
            "",
            "    def process_error(self, event: Any) -> str:",
            "        \"\"\"Process error response\"\"\"",
            "        self.log.error(",
            "            'Encountered Error response from k8s list namespaced pod stream => %s',",
            "            event",
            "        )",
            "        raw_object = event['raw_object']",
            "        if raw_object['code'] == 410:",
            "            self.log.info(",
            "                'Kubernetes resource version is too old, must reset to 0 => %s',",
            "                (raw_object['message'],)",
            "            )",
            "            # Return resource version 0",
            "            return '0'",
            "        raise AirflowException(",
            "            'Kubernetes failure for %s with code %s and message: %s' %",
            "            (raw_object['reason'], raw_object['code'], raw_object['message'])",
            "        )",
            "",
            "    def process_status(self, pod_id: str,",
            "                       namespace: str,",
            "                       status: str,",
            "                       labels: Dict[str, str],",
            "                       resource_version: str,",
            "                       event: Any) -> None:",
            "        \"\"\"Process status response\"\"\"",
            "        if status == 'Pending':",
            "            if event['type'] == 'DELETED':",
            "                self.log.info('Event: Failed to start pod %s, will reschedule', pod_id)",
            "                self.watcher_queue.put((pod_id, namespace, State.UP_FOR_RESCHEDULE, labels, resource_version))",
            "            else:",
            "                self.log.info('Event: %s Pending', pod_id)",
            "        elif status == 'Failed':",
            "            self.log.error('Event: %s Failed', pod_id)",
            "            self.watcher_queue.put((pod_id, namespace, State.FAILED, labels, resource_version))",
            "        elif status == 'Succeeded':",
            "            self.log.info('Event: %s Succeeded', pod_id)",
            "            self.watcher_queue.put((pod_id, namespace, None, labels, resource_version))",
            "        elif status == 'Running':",
            "            self.log.info('Event: %s is Running', pod_id)",
            "        else:",
            "            self.log.warning(",
            "                'Event: Invalid state: %s on pod: %s in namespace %s with labels: %s with '",
            "                'resource_version: %s', status, pod_id, namespace, labels, resource_version",
            "            )",
            "",
            "",
            "class AirflowKubernetesScheduler(LoggingMixin):",
            "    \"\"\"Airflow Scheduler for Kubernetes\"\"\"",
            "",
            "    def __init__(self,",
            "                 kube_config: Any,",
            "                 task_queue: 'Queue[KubernetesJobType]',",
            "                 result_queue: 'Queue[KubernetesResultsType]',",
            "                 kube_client: client.CoreV1Api,",
            "                 worker_uuid: str):",
            "        super().__init__()",
            "        self.log.debug(\"Creating Kubernetes executor\")",
            "        self.kube_config = kube_config",
            "        self.task_queue = task_queue",
            "        self.result_queue = result_queue",
            "        self.namespace = self.kube_config.kube_namespace",
            "        self.log.debug(\"Kubernetes using namespace %s\", self.namespace)",
            "        self.kube_client = kube_client",
            "        self.launcher = PodLauncher(kube_client=self.kube_client)",
            "        self.worker_configuration_pod = WorkerConfiguration(kube_config=self.kube_config).as_pod()",
            "        self._manager = multiprocessing.Manager()",
            "        self.watcher_queue = self._manager.Queue()",
            "        self.worker_uuid = worker_uuid",
            "        self.kube_watcher = self._make_kube_watcher()",
            "",
            "    def _make_kube_watcher(self) -> KubernetesJobWatcher:",
            "        resource_version = KubeResourceVersion.get_current_resource_version()",
            "        watcher = KubernetesJobWatcher(watcher_queue=self.watcher_queue,",
            "                                       resource_version=resource_version,",
            "                                       worker_uuid=self.worker_uuid,",
            "                                       kube_config=self.kube_config)",
            "        watcher.start()",
            "        return watcher",
            "",
            "    def _health_check_kube_watcher(self):",
            "        if self.kube_watcher.is_alive():",
            "            pass",
            "        else:",
            "            self.log.error(",
            "                'Error while health checking kube watcher process. '",
            "                'Process died for unknown reasons')",
            "            self.kube_watcher = self._make_kube_watcher()",
            "",
            "    def run_next(self, next_job: KubernetesJobType) -> None:",
            "        \"\"\"",
            "        The run_next command will check the task_queue for any un-run jobs.",
            "        It will then create a unique job-id, launch that job in the cluster,",
            "        and store relevant info in the current_jobs map so we can track the job's",
            "        status",
            "        \"\"\"",
            "        self.log.info('Kubernetes job is %s', str(next_job))",
            "        key, command, kube_executor_config = next_job",
            "        dag_id, task_id, execution_date, try_number = key",
            "",
            "        if isinstance(command, str):",
            "            command = [command]",
            "",
            "        pod = PodGenerator.construct_pod(",
            "            namespace=self.namespace,",
            "            worker_uuid=self.worker_uuid,",
            "            pod_id=self._create_pod_id(dag_id, task_id),",
            "            dag_id=pod_generator.make_safe_label_value(dag_id),",
            "            task_id=pod_generator.make_safe_label_value(task_id),",
            "            try_number=try_number,",
            "            date=self._datetime_to_label_safe_datestring(execution_date),",
            "            command=command,",
            "            kube_executor_config=kube_executor_config,",
            "            worker_config=self.worker_configuration_pod",
            "        )",
            "        # Reconcile the pod generated by the Operator and the Pod",
            "        # generated by the .cfg file",
            "        self.log.debug(\"Kubernetes running for command %s\", command)",
            "        self.log.debug(\"Kubernetes launching image %s\", pod.spec.containers[0].image)",
            "",
            "        # the watcher will monitor pods, so we do not block.",
            "        self.launcher.run_pod_async(pod, **self.kube_config.kube_client_request_args)",
            "        self.log.debug(\"Kubernetes Job created!\")",
            "",
            "    def delete_pod(self, pod_id: str, namespace: str) -> None:",
            "        \"\"\"Deletes POD\"\"\"",
            "        try:",
            "            self.kube_client.delete_namespaced_pod(",
            "                pod_id, namespace, body=client.V1DeleteOptions(**self.kube_config.delete_option_kwargs),",
            "                **self.kube_config.kube_client_request_args)",
            "        except ApiException as e:",
            "            # If the pod is already deleted",
            "            if e.status != 404:",
            "                raise",
            "",
            "    def sync(self) -> None:",
            "        \"\"\"",
            "        The sync function checks the status of all currently running kubernetes jobs.",
            "        If a job is completed, its status is placed in the result queue to",
            "        be sent back to the scheduler.",
            "",
            "        :return:",
            "",
            "        \"\"\"",
            "        self._health_check_kube_watcher()",
            "        while True:",
            "            try:",
            "                task = self.watcher_queue.get_nowait()",
            "                try:",
            "                    self.process_watcher_task(task)",
            "                finally:",
            "                    self.watcher_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "    def process_watcher_task(self, task: KubernetesWatchType) -> None:",
            "        \"\"\"Process the task by watcher.\"\"\"",
            "        pod_id, namespace, state, labels, resource_version = task",
            "        self.log.info(",
            "            'Attempting to finish pod; pod_id: %s; state: %s; labels: %s',",
            "            pod_id, state, labels",
            "        )",
            "        key = self._labels_to_key(labels=labels)",
            "        if key:",
            "            self.log.debug('finishing job %s - %s (%s)', key, state, pod_id)",
            "            self.result_queue.put((key, state, pod_id, namespace, resource_version))",
            "",
            "    @staticmethod",
            "    def _strip_unsafe_kubernetes_special_chars(string: str) -> str:",
            "        \"\"\"",
            "        Kubernetes only supports lowercase alphanumeric characters and \"-\" and \".\" in",
            "        the pod name",
            "        However, there are special rules about how \"-\" and \".\" can be used so let's",
            "        only keep",
            "        alphanumeric chars  see here for detail:",
            "        https://kubernetes.io/docs/concepts/overview/working-with-objects/names/",
            "",
            "        :param string: The requested Pod name",
            "        :return: ``str`` Pod name stripped of any unsafe characters",
            "        \"\"\"",
            "        return ''.join(ch.lower() for ind, ch in enumerate(string) if ch.isalnum())",
            "",
            "    @staticmethod",
            "    def _make_safe_pod_id(safe_dag_id: str, safe_task_id: str, safe_uuid: str) -> str:",
            "        \"\"\"",
            "        Kubernetes pod names must be <= 253 chars and must pass the following regex for",
            "        validation",
            "        ``^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$``",
            "",
            "        :param safe_dag_id: a dag_id with only alphanumeric characters",
            "        :param safe_task_id: a task_id with only alphanumeric characters",
            "        :param safe_uuid: a uuid",
            "        :return: ``str`` valid Pod name of appropriate length",
            "        \"\"\"",
            "        safe_key = safe_dag_id + safe_task_id",
            "",
            "        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + \"-\" + safe_uuid",
            "",
            "        return safe_pod_id",
            "",
            "    @staticmethod",
            "    def _create_pod_id(dag_id: str, task_id: str) -> str:",
            "        safe_dag_id = AirflowKubernetesScheduler._strip_unsafe_kubernetes_special_chars(",
            "            dag_id)",
            "        safe_task_id = AirflowKubernetesScheduler._strip_unsafe_kubernetes_special_chars(",
            "            task_id)",
            "        return safe_dag_id + safe_task_id",
            "",
            "    @staticmethod",
            "    def _label_safe_datestring_to_datetime(string: str) -> datetime.datetime:",
            "        \"\"\"",
            "        Kubernetes doesn't permit \":\" in labels. ISO datetime format uses \":\" but not",
            "        \"_\", let's",
            "        replace \":\" with \"_\"",
            "",
            "        :param string: str",
            "        :return: datetime.datetime object",
            "        \"\"\"",
            "        return parser.parse(string.replace('_plus_', '+').replace(\"_\", \":\"))",
            "",
            "    @staticmethod",
            "    def _datetime_to_label_safe_datestring(datetime_obj: datetime.datetime) -> str:",
            "        \"\"\"",
            "        Kubernetes doesn't like \":\" in labels, since ISO datetime format uses \":\" but",
            "        not \"_\" let's",
            "        replace \":\" with \"_\"",
            "",
            "        :param datetime_obj: datetime.datetime object",
            "        :return: ISO-like string representing the datetime",
            "        \"\"\"",
            "        return datetime_obj.isoformat().replace(\":\", \"_\").replace('+', '_plus_')",
            "",
            "    def _labels_to_key(self, labels: Dict[str, str]) -> Optional[TaskInstanceKeyType]:",
            "        try_num = 1",
            "        try:",
            "            try_num = int(labels.get('try_number', '1'))",
            "        except ValueError:",
            "            self.log.warning(\"could not get try_number as an int: %s\", labels.get('try_number', '1'))",
            "",
            "        try:",
            "            dag_id = labels['dag_id']",
            "            task_id = labels['task_id']",
            "            ex_time = self._label_safe_datestring_to_datetime(labels['execution_date'])",
            "        except Exception as e:  # pylint: disable=broad-except",
            "            self.log.warning(",
            "                'Error while retrieving labels; labels: %s; exception: %s',",
            "                labels, e",
            "            )",
            "            return None",
            "",
            "        with create_session() as session:",
            "            task = (",
            "                session",
            "                .query(TaskInstance)",
            "                .filter_by(task_id=task_id, dag_id=dag_id, execution_date=ex_time)",
            "                .one_or_none()",
            "            )",
            "            if task:",
            "                self.log.info(",
            "                    'Found matching task %s-%s (%s) with current state of %s',",
            "                    task.dag_id, task.task_id, task.execution_date, task.state",
            "                )",
            "                return (dag_id, task_id, ex_time, try_num)",
            "            else:",
            "                self.log.warning(",
            "                    'task_id/dag_id are not safe to use as Kubernetes labels. This can cause '",
            "                    'severe performance regressions. Please see '",
            "                    '<https://kubernetes.io/docs/concepts/overview/working-with-objects'",
            "                    '/labels/#syntax-and-character-set>. '",
            "                    'Given dag_id: %s, task_id: %s', task_id, dag_id",
            "                )",
            "",
            "            tasks = (",
            "                session",
            "                .query(TaskInstance)",
            "                .filter_by(execution_date=ex_time).all()",
            "            )",
            "            self.log.info(",
            "                'Checking %s task instances.',",
            "                len(tasks)",
            "            )",
            "            for task in tasks:",
            "                if (",
            "                    pod_generator.make_safe_label_value(task.dag_id) == dag_id and",
            "                    pod_generator.make_safe_label_value(task.task_id) == task_id and",
            "                    task.execution_date == ex_time",
            "                ):",
            "                    self.log.info(",
            "                        'Found matching task %s-%s (%s) with current state of %s',",
            "                        task.dag_id, task.task_id, task.execution_date, task.state",
            "                    )",
            "                    dag_id = task.dag_id",
            "                    task_id = task.task_id",
            "                    return dag_id, task_id, ex_time, try_num",
            "        self.log.warning(",
            "            'Failed to find and match task details to a pod; labels: %s',",
            "            labels",
            "        )",
            "        return None",
            "",
            "    def _flush_watcher_queue(self) -> None:",
            "        self.log.debug('Executor shutting down, watcher_queue approx. size=%d', self.watcher_queue.qsize())",
            "        while True:",
            "            try:",
            "                task = self.watcher_queue.get_nowait()",
            "                # Ignoring it since it can only have either FAILED or SUCCEEDED pods",
            "                self.log.warning('Executor shutting down, IGNORING watcher task=%s', task)",
            "                self.watcher_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "    def terminate(self) -> None:",
            "        \"\"\"Terminates the watcher.\"\"\"",
            "        self.log.debug(\"Terminating kube_watcher...\")",
            "        self.kube_watcher.terminate()",
            "        self.kube_watcher.join()",
            "        self.log.debug(\"kube_watcher=%s\", self.kube_watcher)",
            "        self.log.debug(\"Flushing watcher_queue...\")",
            "        self._flush_watcher_queue()",
            "        # Queue should be empty...",
            "        self.watcher_queue.join()",
            "        self.log.debug(\"Shutting down manager...\")",
            "        self._manager.shutdown()",
            "",
            "",
            "class KubernetesExecutor(BaseExecutor, LoggingMixin):",
            "    \"\"\"Executor for Kubernetes\"\"\"",
            "",
            "    def __init__(self):",
            "        self.kube_config = KubeConfig()",
            "        self._manager = multiprocessing.Manager()",
            "        self.task_queue: 'Queue[KubernetesJobType]' = self._manager.Queue()",
            "        self.result_queue: 'Queue[KubernetesResultsType]' = self._manager.Queue()",
            "        self.kube_scheduler: Optional[AirflowKubernetesScheduler] = None",
            "        self.kube_client: Optional[client.CoreV1Api] = None",
            "        self.worker_uuid: Optional[str] = None",
            "        super().__init__(parallelism=self.kube_config.parallelism)",
            "",
            "    @provide_session",
            "    def clear_not_launched_queued_tasks(self, session=None) -> None:",
            "        \"\"\"",
            "        If the airflow scheduler restarts with pending \"Queued\" tasks, the tasks may or",
            "        may not",
            "        have been launched. Thus on starting up the scheduler let's check every",
            "        \"Queued\" task to",
            "        see if it has been launched (ie: if there is a corresponding pod on kubernetes)",
            "",
            "        If it has been launched then do nothing, otherwise reset the state to \"None\" so",
            "        the task",
            "        will be rescheduled",
            "",
            "        This will not be necessary in a future version of airflow in which there is",
            "        proper support",
            "        for State.LAUNCHED",
            "        \"\"\"",
            "        if not self.kube_client:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        queued_tasks = session \\",
            "            .query(TaskInstance) \\",
            "            .filter(TaskInstance.state == State.QUEUED).all()",
            "        self.log.info(",
            "            'When executor started up, found %s queued task instances',",
            "            len(queued_tasks)",
            "        )",
            "",
            "        for task in queued_tasks:",
            "            # noinspection PyProtectedMember",
            "            # pylint: disable=protected-access",
            "            dict_string = (",
            "                \"dag_id={},task_id={},execution_date={},airflow-worker={}\".format(",
            "                    pod_generator.make_safe_label_value(task.dag_id),",
            "                    pod_generator.make_safe_label_value(task.task_id),",
            "                    AirflowKubernetesScheduler._datetime_to_label_safe_datestring(",
            "                        task.execution_date",
            "                    ),",
            "                    self.worker_uuid",
            "                )",
            "            )",
            "            # pylint: enable=protected-access",
            "            kwargs = dict(label_selector=dict_string)",
            "            if self.kube_config.kube_client_request_args:",
            "                for key, value in self.kube_config.kube_client_request_args.items():",
            "                    kwargs[key] = value",
            "            pod_list = self.kube_client.list_namespaced_pod(",
            "                self.kube_config.kube_namespace, **kwargs)",
            "            if not pod_list.items:",
            "                self.log.info(",
            "                    'TaskInstance: %s found in queued state but was not launched, '",
            "                    'rescheduling', task",
            "                )",
            "                session.query(TaskInstance).filter(",
            "                    TaskInstance.dag_id == task.dag_id,",
            "                    TaskInstance.task_id == task.task_id,",
            "                    TaskInstance.execution_date == task.execution_date",
            "                ).update({TaskInstance.state: State.NONE})",
            "",
            "    def _inject_secrets(self) -> None:",
            "        def _create_or_update_secret(secret_name, secret_path):",
            "            try:",
            "                return self.kube_client.create_namespaced_secret(",
            "                    self.kube_config.executor_namespace, kubernetes.client.V1Secret(",
            "                        data={",
            "                            'key.json': base64.b64encode(open(secret_path, 'r').read())},",
            "                        metadata=kubernetes.client.V1ObjectMeta(name=secret_name)),",
            "                    **self.kube_config.kube_client_request_args)",
            "            except ApiException as e:",
            "                if e.status == 409:",
            "                    return self.kube_client.replace_namespaced_secret(",
            "                        secret_name, self.kube_config.executor_namespace,",
            "                        kubernetes.client.V1Secret(",
            "                            data={'key.json': base64.b64encode(",
            "                                open(secret_path, 'r').read())},",
            "                            metadata=kubernetes.client.V1ObjectMeta(name=secret_name)),",
            "                        **self.kube_config.kube_client_request_args)",
            "                self.log.exception(",
            "                    'Exception while trying to inject secret. '",
            "                    'Secret name: %s, error details: %s',",
            "                    secret_name, e",
            "                )",
            "                raise",
            "",
            "    def start(self) -> None:",
            "        \"\"\"Starts the executor\"\"\"",
            "        self.log.info('Start Kubernetes executor')",
            "        self.worker_uuid = KubeWorkerIdentifier.get_or_create_current_kube_worker_uuid()",
            "        if not self.worker_uuid:",
            "            raise AirflowException(\"Could not get worker uuid\")",
            "        self.log.debug('Start with worker_uuid: %s', self.worker_uuid)",
            "        # always need to reset resource version since we don't know",
            "        # when we last started, note for behavior below",
            "        # https://github.com/kubernetes-client/python/blob/master/kubernetes/docs",
            "        # /CoreV1Api.md#list_namespaced_pod",
            "        KubeResourceVersion.reset_resource_version()",
            "        self.kube_client = get_kube_client()",
            "        self.kube_scheduler = AirflowKubernetesScheduler(",
            "            self.kube_config, self.task_queue, self.result_queue,",
            "            self.kube_client, self.worker_uuid",
            "        )",
            "        self._inject_secrets()",
            "        self.clear_not_launched_queued_tasks()",
            "",
            "    def execute_async(self,",
            "                      key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None) -> None:",
            "        \"\"\"Executes task asynchronously\"\"\"",
            "        self.log.info(",
            "            'Add task %s with command %s with executor_config %s',",
            "            key, command, executor_config",
            "        )",
            "",
            "        kube_executor_config = PodGenerator.from_obj(executor_config)",
            "        if not self.task_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.task_queue.put((key, command, kube_executor_config))",
            "",
            "    def sync(self) -> None:",
            "        \"\"\"Synchronize task state.\"\"\"",
            "        if self.running:",
            "            self.log.debug('self.running: %s', self.running)",
            "        if self.queued_tasks:",
            "            self.log.debug('self.queued: %s', self.queued_tasks)",
            "        if not self.worker_uuid:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.kube_scheduler:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.kube_config:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.result_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.task_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.kube_scheduler.sync()",
            "",
            "        last_resource_version = None",
            "        while True:  # pylint: disable=too-many-nested-blocks",
            "            try:",
            "                results = self.result_queue.get_nowait()",
            "                try:",
            "                    key, state, pod_id, namespace, resource_version = results",
            "                    last_resource_version = resource_version",
            "                    self.log.info('Changing state of %s to %s', results, state)",
            "                    try:",
            "                        self._change_state(key, state, pod_id, namespace)",
            "                    except Exception as e:  # pylint: disable=broad-except",
            "                        self.log.exception(",
            "                            \"Exception: %s when attempting to change state of %s to %s, re-queueing.\",",
            "                            e, results, state",
            "                        )",
            "                        self.result_queue.put(results)",
            "                finally:",
            "                    self.result_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "        KubeResourceVersion.checkpoint_resource_version(last_resource_version)",
            "",
            "        # pylint: disable=too-many-nested-blocks",
            "        for _ in range(self.kube_config.worker_pods_creation_batch_size):",
            "            try:",
            "                task = self.task_queue.get_nowait()",
            "                try:",
            "                    self.kube_scheduler.run_next(task)",
            "                except ApiException as e:",
            "                    self.log.warning('ApiException when attempting to run task, re-queueing. '",
            "                                     'Message: %s', json.loads(e.body)['message'])",
            "                    self.task_queue.put(task)",
            "                finally:",
            "                    self.task_queue.task_done()",
            "            except Empty:",
            "                break",
            "        # pylint: enable=too-many-nested-blocks",
            "",
            "    def _change_state(self,",
            "                      key: TaskInstanceKeyType,",
            "                      state: Optional[str],",
            "                      pod_id: str,",
            "                      namespace: str) -> None:",
            "        if state != State.RUNNING:",
            "            if self.kube_config.delete_worker_pods:",
            "                if not self.kube_scheduler:",
            "                    raise AirflowException(NOT_STARTED_MESSAGE)",
            "                if state is not State.FAILED or self.kube_config.delete_worker_pods_on_failure:",
            "                    self.kube_scheduler.delete_pod(pod_id, namespace)",
            "                    self.log.info('Deleted pod: %s in namespace %s', str(key), str(namespace))",
            "            try:",
            "                self.running.remove(key)",
            "            except KeyError:",
            "                self.log.debug('Could not find key: %s', str(key))",
            "        self.event_buffer[key] = state, None",
            "",
            "    def _flush_task_queue(self) -> None:",
            "        if not self.task_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.log.debug('Executor shutting down, task_queue approximate size=%d', self.task_queue.qsize())",
            "        while True:",
            "            try:",
            "                task = self.task_queue.get_nowait()",
            "                # This is a new task to run thus ok to ignore.",
            "                self.log.warning('Executor shutting down, will NOT run task=%s', task)",
            "                self.task_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "    def _flush_result_queue(self) -> None:",
            "        if not self.result_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.log.debug('Executor shutting down, result_queue approximate size=%d', self.result_queue.qsize())",
            "        while True:  # pylint: disable=too-many-nested-blocks",
            "            try:",
            "                results = self.result_queue.get_nowait()",
            "                self.log.warning('Executor shutting down, flushing results=%s', results)",
            "                try:",
            "                    key, state, pod_id, namespace, resource_version = results",
            "                    self.log.info('Changing state of %s to %s : resource_version=%d', results, state,",
            "                                  resource_version)",
            "                    try:",
            "                        self._change_state(key, state, pod_id, namespace)",
            "                    except Exception as e:  # pylint: disable=broad-except",
            "                        self.log.exception('Ignoring exception: %s when attempting to change state of %s '",
            "                                           'to %s.', e, results, state)",
            "                finally:",
            "                    self.result_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "    def end(self) -> None:",
            "        \"\"\"Called when the executor shuts down\"\"\"",
            "        if not self.task_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.result_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.kube_scheduler:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.log.info('Shutting down Kubernetes executor')",
            "        self.log.debug('Flushing task_queue...')",
            "        self._flush_task_queue()",
            "        self.log.debug('Flushing result_queue...')",
            "        self._flush_result_queue()",
            "        # Both queues should be empty...",
            "        self.task_queue.join()",
            "        self.result_queue.join()",
            "        if self.kube_scheduler:",
            "            self.kube_scheduler.terminate()",
            "        self._manager.shutdown()",
            "",
            "    def terminate(self):",
            "        \"\"\"Terminate the executor is not doing anything.\"\"\""
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"",
            "KubernetesExecutor",
            "",
            ".. seealso::",
            "    For more information on how the KubernetesExecutor works, take a look at the guide:",
            "    :ref:`executor:KubernetesExecutor`",
            "\"\"\"",
            "import base64",
            "import datetime",
            "import json",
            "import multiprocessing",
            "import time",
            "from queue import Empty, Queue  # pylint: disable=unused-import",
            "from typing import Any, Dict, Optional, Tuple, Union",
            "",
            "import kubernetes",
            "from dateutil import parser",
            "from kubernetes import client, watch",
            "from kubernetes.client import Configuration",
            "from kubernetes.client.rest import ApiException",
            "from urllib3.exceptions import ReadTimeoutError",
            "",
            "from airflow import settings",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowConfigException, AirflowException",
            "from airflow.executors.base_executor import NOT_STARTED_MESSAGE, BaseExecutor, CommandType",
            "from airflow.kubernetes import pod_generator",
            "from airflow.kubernetes.kube_client import get_kube_client",
            "from airflow.kubernetes.pod_generator import MAX_POD_ID_LEN, PodGenerator",
            "from airflow.kubernetes.pod_launcher import PodLauncher",
            "from airflow.kubernetes.worker_configuration import WorkerConfiguration",
            "from airflow.models import KubeResourceVersion, KubeWorkerIdentifier, TaskInstance",
            "from airflow.models.taskinstance import TaskInstanceKeyType",
            "from airflow.utils.log.logging_mixin import LoggingMixin",
            "from airflow.utils.session import create_session, provide_session",
            "from airflow.utils.state import State",
            "",
            "MAX_LABEL_LEN = 63",
            "",
            "# TaskInstance key, command, configuration",
            "KubernetesJobType = Tuple[TaskInstanceKeyType, CommandType, Any]",
            "",
            "# key, state, pod_id, namespace, resource_version",
            "KubernetesResultsType = Tuple[TaskInstanceKeyType, Optional[str], str, str, str]",
            "",
            "# pod_id, namespace, state, labels, resource_version",
            "KubernetesWatchType = Tuple[str, str, Optional[str], Dict[str, str], str]",
            "",
            "",
            "class KubeConfig:  # pylint: disable=too-many-instance-attributes",
            "    \"\"\"Configuration for Kubernetes\"\"\"",
            "    core_section = 'core'",
            "    kubernetes_section = 'kubernetes'",
            "    logging_section = 'logging'",
            "",
            "    def __init__(self):  # pylint: disable=too-many-statements",
            "        configuration_dict = conf.as_dict(display_sensitive=True)",
            "        self.core_configuration = configuration_dict['core']",
            "        self.kube_secrets = configuration_dict.get('kubernetes_secrets', {})",
            "        self.kube_env_vars = configuration_dict.get('kubernetes_environment_variables', {})",
            "        self.env_from_configmap_ref = conf.get(self.kubernetes_section,",
            "                                               'env_from_configmap_ref')",
            "        self.env_from_secret_ref = conf.get(self.kubernetes_section,",
            "                                            'env_from_secret_ref')",
            "        self.airflow_home = settings.AIRFLOW_HOME",
            "        self.dags_folder = conf.get(self.core_section, 'dags_folder')",
            "        self.parallelism = conf.getint(self.core_section, 'parallelism')",
            "        self.worker_container_repository = conf.get(",
            "            self.kubernetes_section, 'worker_container_repository')",
            "        self.worker_container_tag = conf.get(",
            "            self.kubernetes_section, 'worker_container_tag')",
            "        self.kube_image = '{}:{}'.format(",
            "            self.worker_container_repository, self.worker_container_tag)",
            "        self.kube_image_pull_policy = conf.get(",
            "            self.kubernetes_section, \"worker_container_image_pull_policy\"",
            "        )",
            "        self.kube_node_selectors = configuration_dict.get('kubernetes_node_selectors', {})",
            "        self.pod_template_file = conf.get(self.kubernetes_section, 'pod_template_file',",
            "                                          fallback=None)",
            "",
            "        kube_worker_annotations = conf.get(self.kubernetes_section, 'worker_annotations')",
            "        if kube_worker_annotations:",
            "            self.kube_annotations = json.loads(kube_worker_annotations)",
            "        else:",
            "            self.kube_annotations = None",
            "",
            "        self.kube_labels = configuration_dict.get('kubernetes_labels', {})",
            "        self.delete_worker_pods = conf.getboolean(",
            "            self.kubernetes_section, 'delete_worker_pods')",
            "        self.delete_worker_pods_on_failure = conf.getboolean(",
            "            self.kubernetes_section, 'delete_worker_pods_on_failure')",
            "        self.worker_pods_creation_batch_size = conf.getint(",
            "            self.kubernetes_section, 'worker_pods_creation_batch_size')",
            "        self.worker_service_account_name = conf.get(",
            "            self.kubernetes_section, 'worker_service_account_name')",
            "        self.image_pull_secrets = conf.get(self.kubernetes_section, 'image_pull_secrets')",
            "",
            "        # NOTE: user can build the dags into the docker image directly,",
            "        # this will set to True if so",
            "        self.dags_in_image = conf.getboolean(self.kubernetes_section, 'dags_in_image')",
            "",
            "        # Run as user for pod security context",
            "        self.worker_run_as_user = self._get_security_context_val('run_as_user')",
            "        self.worker_fs_group = self._get_security_context_val('fs_group')",
            "",
            "        kube_worker_resources = conf.get(self.kubernetes_section, 'worker_resources')",
            "        if kube_worker_resources:",
            "            self.worker_resources = json.loads(kube_worker_resources)",
            "        else:",
            "            self.worker_resources = None",
            "",
            "        # NOTE: `git_repo` and `git_branch` must be specified together as a pair",
            "        # The http URL of the git repository to clone from",
            "        self.git_repo = conf.get(self.kubernetes_section, 'git_repo')",
            "        # The branch of the repository to be checked out",
            "        self.git_branch = conf.get(self.kubernetes_section, 'git_branch')",
            "        # Clone depth for git sync",
            "        self.git_sync_depth = conf.get(self.kubernetes_section, 'git_sync_depth')",
            "        # Optionally, the directory in the git repository containing the dags",
            "        self.git_subpath = conf.get(self.kubernetes_section, 'git_subpath')",
            "        # Optionally, the root directory for git operations",
            "        self.git_sync_root = conf.get(self.kubernetes_section, 'git_sync_root')",
            "        # Optionally, the name at which to publish the checked-out files under --root",
            "        self.git_sync_dest = conf.get(self.kubernetes_section, 'git_sync_dest')",
            "        # Optionally, the tag or hash to checkout",
            "        self.git_sync_rev = conf.get(self.kubernetes_section, 'git_sync_rev')",
            "        # Optionally, if git_dags_folder_mount_point is set the worker will use",
            "        # {git_dags_folder_mount_point}/{git_sync_dest}/{git_subpath} as dags_folder",
            "        self.git_dags_folder_mount_point = conf.get(self.kubernetes_section,",
            "                                                    'git_dags_folder_mount_point')",
            "",
            "        # Optionally a user may supply a (`git_user` AND `git_password`) OR",
            "        # (`git_ssh_key_secret_name` AND `git_ssh_key_secret_key`) for private repositories",
            "        self.git_user = conf.get(self.kubernetes_section, 'git_user')",
            "        self.git_password = conf.get(self.kubernetes_section, 'git_password')",
            "        self.git_ssh_key_secret_name = conf.get(self.kubernetes_section, 'git_ssh_key_secret_name')",
            "        self.git_ssh_known_hosts_configmap_name = conf.get(self.kubernetes_section,",
            "                                                           'git_ssh_known_hosts_configmap_name')",
            "        self.git_sync_credentials_secret = conf.get(self.kubernetes_section,",
            "                                                    'git_sync_credentials_secret')",
            "",
            "        # NOTE: The user may optionally use a volume claim to mount a PV containing",
            "        # DAGs directly",
            "        self.dags_volume_claim = conf.get(self.kubernetes_section, 'dags_volume_claim')",
            "",
            "        self.dags_volume_mount_point = conf.get(self.kubernetes_section, 'dags_volume_mount_point')",
            "",
            "        # This prop may optionally be set for PV Claims and is used to write logs",
            "        self.logs_volume_claim = conf.get(self.kubernetes_section, 'logs_volume_claim')",
            "",
            "        # This prop may optionally be set for PV Claims and is used to locate DAGs",
            "        # on a SubPath",
            "        self.dags_volume_subpath = conf.get(",
            "            self.kubernetes_section, 'dags_volume_subpath')",
            "",
            "        # This prop may optionally be set for PV Claims and is used to locate logs",
            "        # on a SubPath",
            "        self.logs_volume_subpath = conf.get(",
            "            self.kubernetes_section, 'logs_volume_subpath')",
            "",
            "        # Optionally, hostPath volume containing DAGs",
            "        self.dags_volume_host = conf.get(self.kubernetes_section, 'dags_volume_host')",
            "",
            "        # Optionally, write logs to a hostPath Volume",
            "        self.logs_volume_host = conf.get(self.kubernetes_section, 'logs_volume_host')",
            "",
            "        # This prop may optionally be set for PV Claims and is used to write logs",
            "        self.base_log_folder = conf.get(self.logging_section, 'base_log_folder')",
            "",
            "        # The Kubernetes Namespace in which the Scheduler and Webserver reside. Note",
            "        # that if your",
            "        # cluster has RBAC enabled, your scheduler may need service account permissions to",
            "        # create, watch, get, and delete pods in this namespace.",
            "        self.kube_namespace = conf.get(self.kubernetes_section, 'namespace')",
            "        # The Kubernetes Namespace in which pods will be created by the executor. Note",
            "        # that if your",
            "        # cluster has RBAC enabled, your workers may need service account permissions to",
            "        # interact with cluster components.",
            "        self.executor_namespace = conf.get(self.kubernetes_section, 'namespace')",
            "",
            "        # If the user is using the git-sync container to clone their repository via git,",
            "        # allow them to specify repository, tag, and pod name for the init container.",
            "        self.git_sync_container_repository = conf.get(",
            "            self.kubernetes_section, 'git_sync_container_repository')",
            "",
            "        self.git_sync_container_tag = conf.get(",
            "            self.kubernetes_section, 'git_sync_container_tag')",
            "        self.git_sync_container = '{}:{}'.format(",
            "            self.git_sync_container_repository, self.git_sync_container_tag)",
            "",
            "        self.git_sync_init_container_name = conf.get(",
            "            self.kubernetes_section, 'git_sync_init_container_name')",
            "",
            "        self.git_sync_run_as_user = self._get_security_context_val('git_sync_run_as_user')",
            "",
            "        # The worker pod may optionally have a  valid Airflow config loaded via a",
            "        # configmap",
            "        self.airflow_configmap = conf.get(self.kubernetes_section, 'airflow_configmap')",
            "",
            "        # The worker pod may optionally have a valid Airflow local settings loaded via a",
            "        # configmap",
            "        self.airflow_local_settings_configmap = conf.get(",
            "            self.kubernetes_section, 'airflow_local_settings_configmap')",
            "",
            "        affinity_json = conf.get(self.kubernetes_section, 'affinity')",
            "        if affinity_json:",
            "            self.kube_affinity = json.loads(affinity_json)",
            "        else:",
            "            self.kube_affinity = None",
            "",
            "        tolerations_json = conf.get(self.kubernetes_section, 'tolerations')",
            "        if tolerations_json:",
            "            self.kube_tolerations = json.loads(tolerations_json)",
            "        else:",
            "            self.kube_tolerations = None",
            "",
            "        kube_client_request_args = conf.get(self.kubernetes_section, 'kube_client_request_args')",
            "        if kube_client_request_args:",
            "            self.kube_client_request_args = json.loads(kube_client_request_args)",
            "            if self.kube_client_request_args['_request_timeout'] and \\",
            "                    isinstance(self.kube_client_request_args['_request_timeout'], list):",
            "                self.kube_client_request_args['_request_timeout'] = \\",
            "                    tuple(self.kube_client_request_args['_request_timeout'])",
            "        else:",
            "            self.kube_client_request_args = {}",
            "        self._validate()",
            "",
            "        delete_option_kwargs = conf.get(self.kubernetes_section, 'delete_option_kwargs')",
            "        if delete_option_kwargs:",
            "            self.delete_option_kwargs = json.loads(delete_option_kwargs)",
            "        else:",
            "            self.delete_option_kwargs = {}",
            "",
            "    # pod security context items should return integers",
            "    # and only return a blank string if contexts are not set.",
            "    def _get_security_context_val(self, scontext: str) -> Union[str, int]:",
            "        val = conf.get(self.kubernetes_section, scontext)",
            "        if not val:",
            "            return \"\"",
            "        else:",
            "            return int(val)",
            "",
            "    def _validate(self):",
            "        if self.pod_template_file:",
            "            return",
            "        # TODO: use XOR for dags_volume_claim and git_dags_folder_mount_point",
            "        # pylint: disable=too-many-boolean-expressions",
            "        if not self.dags_volume_claim \\",
            "            and not self.dags_volume_host \\",
            "            and not self.dags_in_image \\",
            "                and (not self.git_repo or not self.git_branch or not self.git_dags_folder_mount_point):",
            "            raise AirflowConfigException(",
            "                'In kubernetes mode the following must be set in the `kubernetes` '",
            "                'config section: `dags_volume_claim` '",
            "                'or `dags_volume_host` '",
            "                'or `dags_in_image` '",
            "                'or `git_repo and git_branch and git_dags_folder_mount_point`')",
            "        if self.git_repo \\",
            "            and (self.git_user or self.git_password) \\",
            "                and self.git_ssh_key_secret_name:",
            "            raise AirflowConfigException(",
            "                'In kubernetes mode, using `git_repo` to pull the DAGs: '",
            "                'for private repositories, either `git_user` and `git_password` '",
            "                'must be set for authentication through user credentials; '",
            "                'or `git_ssh_key_secret_name` must be set for authentication '",
            "                'through ssh key, but not both')",
            "        # pylint: enable=too-many-boolean-expressions",
            "",
            "",
            "class KubernetesJobWatcher(multiprocessing.Process, LoggingMixin):",
            "    \"\"\"Watches for Kubernetes jobs\"\"\"",
            "",
            "    def __init__(self,",
            "                 watcher_queue: 'Queue[KubernetesWatchType]',",
            "                 resource_version: Optional[str],",
            "                 worker_uuid: Optional[str],",
            "                 kube_config: Configuration):",
            "        super().__init__()",
            "        self.worker_uuid = worker_uuid",
            "        self.watcher_queue = watcher_queue",
            "        self.resource_version = resource_version",
            "        self.kube_config = kube_config",
            "",
            "    def run(self) -> None:",
            "        \"\"\"Performs watching\"\"\"",
            "        kube_client: client.CoreV1Api = get_kube_client()",
            "        if not self.worker_uuid:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        while True:",
            "            try:",
            "                self.resource_version = self._run(kube_client, self.resource_version,",
            "                                                  self.worker_uuid, self.kube_config)",
            "            except ReadTimeoutError:",
            "                self.log.warning(\"There was a timeout error accessing the Kube API. \"",
            "                                 \"Retrying request.\", exc_info=True)",
            "                time.sleep(1)",
            "            except Exception:",
            "                self.log.exception('Unknown error in KubernetesJobWatcher. Failing')",
            "                raise",
            "            else:",
            "                self.log.warning('Watch died gracefully, starting back up with: '",
            "                                 'last resource_version: %s', self.resource_version)",
            "",
            "    def _run(self,",
            "             kube_client: client.CoreV1Api,",
            "             resource_version: Optional[str],",
            "             worker_uuid: str,",
            "             kube_config: Any) -> Optional[str]:",
            "        self.log.info(",
            "            'Event: and now my watch begins starting at resource_version: %s',",
            "            resource_version",
            "        )",
            "        watcher = watch.Watch()",
            "",
            "        kwargs = {'label_selector': 'airflow-worker={}'.format(worker_uuid)}",
            "        if resource_version:",
            "            kwargs['resource_version'] = resource_version",
            "        if kube_config.kube_client_request_args:",
            "            for key, value in kube_config.kube_client_request_args.items():",
            "                kwargs[key] = value",
            "",
            "        last_resource_version: Optional[str] = None",
            "        for event in watcher.stream(kube_client.list_pod_for_all_namespaces, **kwargs):",
            "            task = event['object']",
            "            self.log.info(",
            "                'Event: %s had an event of type %s',",
            "                task.metadata.name, event['type']",
            "            )",
            "            if event['type'] == 'ERROR':",
            "                return self.process_error(event)",
            "            self.process_status(",
            "                pod_id=task.metadata.name,",
            "                namespace=task.metadata.namespace,",
            "                status=task.status.phase,",
            "                labels=task.metadata.labels,",
            "                resource_version=task.metadata.resource_version,",
            "                event=event,",
            "            )",
            "            last_resource_version = task.metadata.resource_version",
            "",
            "        return last_resource_version",
            "",
            "    def process_error(self, event: Any) -> str:",
            "        \"\"\"Process error response\"\"\"",
            "        self.log.error(",
            "            'Encountered Error response from k8s list namespaced pod stream => %s',",
            "            event",
            "        )",
            "        raw_object = event['raw_object']",
            "        if raw_object['code'] == 410:",
            "            self.log.info(",
            "                'Kubernetes resource version is too old, must reset to 0 => %s',",
            "                (raw_object['message'],)",
            "            )",
            "            # Return resource version 0",
            "            return '0'",
            "        raise AirflowException(",
            "            'Kubernetes failure for %s with code %s and message: %s' %",
            "            (raw_object['reason'], raw_object['code'], raw_object['message'])",
            "        )",
            "",
            "    def process_status(self, pod_id: str,",
            "                       namespace: str,",
            "                       status: str,",
            "                       labels: Dict[str, str],",
            "                       resource_version: str,",
            "                       event: Any) -> None:",
            "        \"\"\"Process status response\"\"\"",
            "        if status == 'Pending':",
            "            if event['type'] == 'DELETED':",
            "                self.log.info('Event: Failed to start pod %s, will reschedule', pod_id)",
            "                self.watcher_queue.put((pod_id, namespace, State.UP_FOR_RESCHEDULE, labels, resource_version))",
            "            else:",
            "                self.log.info('Event: %s Pending', pod_id)",
            "        elif status == 'Failed':",
            "            self.log.error('Event: %s Failed', pod_id)",
            "            self.watcher_queue.put((pod_id, namespace, State.FAILED, labels, resource_version))",
            "        elif status == 'Succeeded':",
            "            self.log.info('Event: %s Succeeded', pod_id)",
            "            self.watcher_queue.put((pod_id, namespace, None, labels, resource_version))",
            "        elif status == 'Running':",
            "            self.log.info('Event: %s is Running', pod_id)",
            "        else:",
            "            self.log.warning(",
            "                'Event: Invalid state: %s on pod: %s in namespace %s with labels: %s with '",
            "                'resource_version: %s', status, pod_id, namespace, labels, resource_version",
            "            )",
            "",
            "",
            "class AirflowKubernetesScheduler(LoggingMixin):",
            "    \"\"\"Airflow Scheduler for Kubernetes\"\"\"",
            "",
            "    def __init__(self,",
            "                 kube_config: Any,",
            "                 task_queue: 'Queue[KubernetesJobType]',",
            "                 result_queue: 'Queue[KubernetesResultsType]',",
            "                 kube_client: client.CoreV1Api,",
            "                 worker_uuid: str):",
            "        super().__init__()",
            "        self.log.debug(\"Creating Kubernetes executor\")",
            "        self.kube_config = kube_config",
            "        self.task_queue = task_queue",
            "        self.result_queue = result_queue",
            "        self.namespace = self.kube_config.kube_namespace",
            "        self.log.debug(\"Kubernetes using namespace %s\", self.namespace)",
            "        self.kube_client = kube_client",
            "        self.launcher = PodLauncher(kube_client=self.kube_client)",
            "        self.worker_configuration_pod = WorkerConfiguration(kube_config=self.kube_config).as_pod()",
            "        self._manager = multiprocessing.Manager()",
            "        self.watcher_queue = self._manager.Queue()",
            "        self.worker_uuid = worker_uuid",
            "        self.kube_watcher = self._make_kube_watcher()",
            "",
            "    def _make_kube_watcher(self) -> KubernetesJobWatcher:",
            "        resource_version = KubeResourceVersion.get_current_resource_version()",
            "        watcher = KubernetesJobWatcher(watcher_queue=self.watcher_queue,",
            "                                       resource_version=resource_version,",
            "                                       worker_uuid=self.worker_uuid,",
            "                                       kube_config=self.kube_config)",
            "        watcher.start()",
            "        return watcher",
            "",
            "    def _health_check_kube_watcher(self):",
            "        if self.kube_watcher.is_alive():",
            "            pass",
            "        else:",
            "            self.log.error(",
            "                'Error while health checking kube watcher process. '",
            "                'Process died for unknown reasons')",
            "            self.kube_watcher = self._make_kube_watcher()",
            "",
            "    def run_next(self, next_job: KubernetesJobType) -> None:",
            "        \"\"\"",
            "        The run_next command will check the task_queue for any un-run jobs.",
            "        It will then create a unique job-id, launch that job in the cluster,",
            "        and store relevant info in the current_jobs map so we can track the job's",
            "        status",
            "        \"\"\"",
            "        self.log.info('Kubernetes job is %s', str(next_job))",
            "        key, command, kube_executor_config = next_job",
            "        dag_id, task_id, execution_date, try_number = key",
            "",
            "        if isinstance(command, str):",
            "            command = [command]",
            "",
            "        if command[0] != \"airflow\":",
            "            raise ValueError('The first element of command must be equal to \"airflow\".')",
            "",
            "        pod = PodGenerator.construct_pod(",
            "            namespace=self.namespace,",
            "            worker_uuid=self.worker_uuid,",
            "            pod_id=self._create_pod_id(dag_id, task_id),",
            "            dag_id=pod_generator.make_safe_label_value(dag_id),",
            "            task_id=pod_generator.make_safe_label_value(task_id),",
            "            try_number=try_number,",
            "            date=self._datetime_to_label_safe_datestring(execution_date),",
            "            command=command,",
            "            kube_executor_config=kube_executor_config,",
            "            worker_config=self.worker_configuration_pod",
            "        )",
            "        # Reconcile the pod generated by the Operator and the Pod",
            "        # generated by the .cfg file",
            "        self.log.debug(\"Kubernetes running for command %s\", command)",
            "        self.log.debug(\"Kubernetes launching image %s\", pod.spec.containers[0].image)",
            "",
            "        # the watcher will monitor pods, so we do not block.",
            "        self.launcher.run_pod_async(pod, **self.kube_config.kube_client_request_args)",
            "        self.log.debug(\"Kubernetes Job created!\")",
            "",
            "    def delete_pod(self, pod_id: str, namespace: str) -> None:",
            "        \"\"\"Deletes POD\"\"\"",
            "        try:",
            "            self.kube_client.delete_namespaced_pod(",
            "                pod_id, namespace, body=client.V1DeleteOptions(**self.kube_config.delete_option_kwargs),",
            "                **self.kube_config.kube_client_request_args)",
            "        except ApiException as e:",
            "            # If the pod is already deleted",
            "            if e.status != 404:",
            "                raise",
            "",
            "    def sync(self) -> None:",
            "        \"\"\"",
            "        The sync function checks the status of all currently running kubernetes jobs.",
            "        If a job is completed, its status is placed in the result queue to",
            "        be sent back to the scheduler.",
            "",
            "        :return:",
            "",
            "        \"\"\"",
            "        self._health_check_kube_watcher()",
            "        while True:",
            "            try:",
            "                task = self.watcher_queue.get_nowait()",
            "                try:",
            "                    self.process_watcher_task(task)",
            "                finally:",
            "                    self.watcher_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "    def process_watcher_task(self, task: KubernetesWatchType) -> None:",
            "        \"\"\"Process the task by watcher.\"\"\"",
            "        pod_id, namespace, state, labels, resource_version = task",
            "        self.log.info(",
            "            'Attempting to finish pod; pod_id: %s; state: %s; labels: %s',",
            "            pod_id, state, labels",
            "        )",
            "        key = self._labels_to_key(labels=labels)",
            "        if key:",
            "            self.log.debug('finishing job %s - %s (%s)', key, state, pod_id)",
            "            self.result_queue.put((key, state, pod_id, namespace, resource_version))",
            "",
            "    @staticmethod",
            "    def _strip_unsafe_kubernetes_special_chars(string: str) -> str:",
            "        \"\"\"",
            "        Kubernetes only supports lowercase alphanumeric characters and \"-\" and \".\" in",
            "        the pod name",
            "        However, there are special rules about how \"-\" and \".\" can be used so let's",
            "        only keep",
            "        alphanumeric chars  see here for detail:",
            "        https://kubernetes.io/docs/concepts/overview/working-with-objects/names/",
            "",
            "        :param string: The requested Pod name",
            "        :return: ``str`` Pod name stripped of any unsafe characters",
            "        \"\"\"",
            "        return ''.join(ch.lower() for ind, ch in enumerate(string) if ch.isalnum())",
            "",
            "    @staticmethod",
            "    def _make_safe_pod_id(safe_dag_id: str, safe_task_id: str, safe_uuid: str) -> str:",
            "        \"\"\"",
            "        Kubernetes pod names must be <= 253 chars and must pass the following regex for",
            "        validation",
            "        ``^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$``",
            "",
            "        :param safe_dag_id: a dag_id with only alphanumeric characters",
            "        :param safe_task_id: a task_id with only alphanumeric characters",
            "        :param safe_uuid: a uuid",
            "        :return: ``str`` valid Pod name of appropriate length",
            "        \"\"\"",
            "        safe_key = safe_dag_id + safe_task_id",
            "",
            "        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + \"-\" + safe_uuid",
            "",
            "        return safe_pod_id",
            "",
            "    @staticmethod",
            "    def _create_pod_id(dag_id: str, task_id: str) -> str:",
            "        safe_dag_id = AirflowKubernetesScheduler._strip_unsafe_kubernetes_special_chars(",
            "            dag_id)",
            "        safe_task_id = AirflowKubernetesScheduler._strip_unsafe_kubernetes_special_chars(",
            "            task_id)",
            "        return safe_dag_id + safe_task_id",
            "",
            "    @staticmethod",
            "    def _label_safe_datestring_to_datetime(string: str) -> datetime.datetime:",
            "        \"\"\"",
            "        Kubernetes doesn't permit \":\" in labels. ISO datetime format uses \":\" but not",
            "        \"_\", let's",
            "        replace \":\" with \"_\"",
            "",
            "        :param string: str",
            "        :return: datetime.datetime object",
            "        \"\"\"",
            "        return parser.parse(string.replace('_plus_', '+').replace(\"_\", \":\"))",
            "",
            "    @staticmethod",
            "    def _datetime_to_label_safe_datestring(datetime_obj: datetime.datetime) -> str:",
            "        \"\"\"",
            "        Kubernetes doesn't like \":\" in labels, since ISO datetime format uses \":\" but",
            "        not \"_\" let's",
            "        replace \":\" with \"_\"",
            "",
            "        :param datetime_obj: datetime.datetime object",
            "        :return: ISO-like string representing the datetime",
            "        \"\"\"",
            "        return datetime_obj.isoformat().replace(\":\", \"_\").replace('+', '_plus_')",
            "",
            "    def _labels_to_key(self, labels: Dict[str, str]) -> Optional[TaskInstanceKeyType]:",
            "        try_num = 1",
            "        try:",
            "            try_num = int(labels.get('try_number', '1'))",
            "        except ValueError:",
            "            self.log.warning(\"could not get try_number as an int: %s\", labels.get('try_number', '1'))",
            "",
            "        try:",
            "            dag_id = labels['dag_id']",
            "            task_id = labels['task_id']",
            "            ex_time = self._label_safe_datestring_to_datetime(labels['execution_date'])",
            "        except Exception as e:  # pylint: disable=broad-except",
            "            self.log.warning(",
            "                'Error while retrieving labels; labels: %s; exception: %s',",
            "                labels, e",
            "            )",
            "            return None",
            "",
            "        with create_session() as session:",
            "            task = (",
            "                session",
            "                .query(TaskInstance)",
            "                .filter_by(task_id=task_id, dag_id=dag_id, execution_date=ex_time)",
            "                .one_or_none()",
            "            )",
            "            if task:",
            "                self.log.info(",
            "                    'Found matching task %s-%s (%s) with current state of %s',",
            "                    task.dag_id, task.task_id, task.execution_date, task.state",
            "                )",
            "                return (dag_id, task_id, ex_time, try_num)",
            "            else:",
            "                self.log.warning(",
            "                    'task_id/dag_id are not safe to use as Kubernetes labels. This can cause '",
            "                    'severe performance regressions. Please see '",
            "                    '<https://kubernetes.io/docs/concepts/overview/working-with-objects'",
            "                    '/labels/#syntax-and-character-set>. '",
            "                    'Given dag_id: %s, task_id: %s', task_id, dag_id",
            "                )",
            "",
            "            tasks = (",
            "                session",
            "                .query(TaskInstance)",
            "                .filter_by(execution_date=ex_time).all()",
            "            )",
            "            self.log.info(",
            "                'Checking %s task instances.',",
            "                len(tasks)",
            "            )",
            "            for task in tasks:",
            "                if (",
            "                    pod_generator.make_safe_label_value(task.dag_id) == dag_id and",
            "                    pod_generator.make_safe_label_value(task.task_id) == task_id and",
            "                    task.execution_date == ex_time",
            "                ):",
            "                    self.log.info(",
            "                        'Found matching task %s-%s (%s) with current state of %s',",
            "                        task.dag_id, task.task_id, task.execution_date, task.state",
            "                    )",
            "                    dag_id = task.dag_id",
            "                    task_id = task.task_id",
            "                    return dag_id, task_id, ex_time, try_num",
            "        self.log.warning(",
            "            'Failed to find and match task details to a pod; labels: %s',",
            "            labels",
            "        )",
            "        return None",
            "",
            "    def _flush_watcher_queue(self) -> None:",
            "        self.log.debug('Executor shutting down, watcher_queue approx. size=%d', self.watcher_queue.qsize())",
            "        while True:",
            "            try:",
            "                task = self.watcher_queue.get_nowait()",
            "                # Ignoring it since it can only have either FAILED or SUCCEEDED pods",
            "                self.log.warning('Executor shutting down, IGNORING watcher task=%s', task)",
            "                self.watcher_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "    def terminate(self) -> None:",
            "        \"\"\"Terminates the watcher.\"\"\"",
            "        self.log.debug(\"Terminating kube_watcher...\")",
            "        self.kube_watcher.terminate()",
            "        self.kube_watcher.join()",
            "        self.log.debug(\"kube_watcher=%s\", self.kube_watcher)",
            "        self.log.debug(\"Flushing watcher_queue...\")",
            "        self._flush_watcher_queue()",
            "        # Queue should be empty...",
            "        self.watcher_queue.join()",
            "        self.log.debug(\"Shutting down manager...\")",
            "        self._manager.shutdown()",
            "",
            "",
            "class KubernetesExecutor(BaseExecutor, LoggingMixin):",
            "    \"\"\"Executor for Kubernetes\"\"\"",
            "",
            "    def __init__(self):",
            "        self.kube_config = KubeConfig()",
            "        self._manager = multiprocessing.Manager()",
            "        self.task_queue: 'Queue[KubernetesJobType]' = self._manager.Queue()",
            "        self.result_queue: 'Queue[KubernetesResultsType]' = self._manager.Queue()",
            "        self.kube_scheduler: Optional[AirflowKubernetesScheduler] = None",
            "        self.kube_client: Optional[client.CoreV1Api] = None",
            "        self.worker_uuid: Optional[str] = None",
            "        super().__init__(parallelism=self.kube_config.parallelism)",
            "",
            "    @provide_session",
            "    def clear_not_launched_queued_tasks(self, session=None) -> None:",
            "        \"\"\"",
            "        If the airflow scheduler restarts with pending \"Queued\" tasks, the tasks may or",
            "        may not",
            "        have been launched. Thus on starting up the scheduler let's check every",
            "        \"Queued\" task to",
            "        see if it has been launched (ie: if there is a corresponding pod on kubernetes)",
            "",
            "        If it has been launched then do nothing, otherwise reset the state to \"None\" so",
            "        the task",
            "        will be rescheduled",
            "",
            "        This will not be necessary in a future version of airflow in which there is",
            "        proper support",
            "        for State.LAUNCHED",
            "        \"\"\"",
            "        if not self.kube_client:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        queued_tasks = session \\",
            "            .query(TaskInstance) \\",
            "            .filter(TaskInstance.state == State.QUEUED).all()",
            "        self.log.info(",
            "            'When executor started up, found %s queued task instances',",
            "            len(queued_tasks)",
            "        )",
            "",
            "        for task in queued_tasks:",
            "            # noinspection PyProtectedMember",
            "            # pylint: disable=protected-access",
            "            dict_string = (",
            "                \"dag_id={},task_id={},execution_date={},airflow-worker={}\".format(",
            "                    pod_generator.make_safe_label_value(task.dag_id),",
            "                    pod_generator.make_safe_label_value(task.task_id),",
            "                    AirflowKubernetesScheduler._datetime_to_label_safe_datestring(",
            "                        task.execution_date",
            "                    ),",
            "                    self.worker_uuid",
            "                )",
            "            )",
            "            # pylint: enable=protected-access",
            "            kwargs = dict(label_selector=dict_string)",
            "            if self.kube_config.kube_client_request_args:",
            "                for key, value in self.kube_config.kube_client_request_args.items():",
            "                    kwargs[key] = value",
            "            pod_list = self.kube_client.list_namespaced_pod(",
            "                self.kube_config.kube_namespace, **kwargs)",
            "            if not pod_list.items:",
            "                self.log.info(",
            "                    'TaskInstance: %s found in queued state but was not launched, '",
            "                    'rescheduling', task",
            "                )",
            "                session.query(TaskInstance).filter(",
            "                    TaskInstance.dag_id == task.dag_id,",
            "                    TaskInstance.task_id == task.task_id,",
            "                    TaskInstance.execution_date == task.execution_date",
            "                ).update({TaskInstance.state: State.NONE})",
            "",
            "    def _inject_secrets(self) -> None:",
            "        def _create_or_update_secret(secret_name, secret_path):",
            "            try:",
            "                return self.kube_client.create_namespaced_secret(",
            "                    self.kube_config.executor_namespace, kubernetes.client.V1Secret(",
            "                        data={",
            "                            'key.json': base64.b64encode(open(secret_path, 'r').read())},",
            "                        metadata=kubernetes.client.V1ObjectMeta(name=secret_name)),",
            "                    **self.kube_config.kube_client_request_args)",
            "            except ApiException as e:",
            "                if e.status == 409:",
            "                    return self.kube_client.replace_namespaced_secret(",
            "                        secret_name, self.kube_config.executor_namespace,",
            "                        kubernetes.client.V1Secret(",
            "                            data={'key.json': base64.b64encode(",
            "                                open(secret_path, 'r').read())},",
            "                            metadata=kubernetes.client.V1ObjectMeta(name=secret_name)),",
            "                        **self.kube_config.kube_client_request_args)",
            "                self.log.exception(",
            "                    'Exception while trying to inject secret. '",
            "                    'Secret name: %s, error details: %s',",
            "                    secret_name, e",
            "                )",
            "                raise",
            "",
            "    def start(self) -> None:",
            "        \"\"\"Starts the executor\"\"\"",
            "        self.log.info('Start Kubernetes executor')",
            "        self.worker_uuid = KubeWorkerIdentifier.get_or_create_current_kube_worker_uuid()",
            "        if not self.worker_uuid:",
            "            raise AirflowException(\"Could not get worker uuid\")",
            "        self.log.debug('Start with worker_uuid: %s', self.worker_uuid)",
            "        # always need to reset resource version since we don't know",
            "        # when we last started, note for behavior below",
            "        # https://github.com/kubernetes-client/python/blob/master/kubernetes/docs",
            "        # /CoreV1Api.md#list_namespaced_pod",
            "        KubeResourceVersion.reset_resource_version()",
            "        self.kube_client = get_kube_client()",
            "        self.kube_scheduler = AirflowKubernetesScheduler(",
            "            self.kube_config, self.task_queue, self.result_queue,",
            "            self.kube_client, self.worker_uuid",
            "        )",
            "        self._inject_secrets()",
            "        self.clear_not_launched_queued_tasks()",
            "",
            "    def execute_async(self,",
            "                      key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None) -> None:",
            "        \"\"\"Executes task asynchronously\"\"\"",
            "        self.log.info(",
            "            'Add task %s with command %s with executor_config %s',",
            "            key, command, executor_config",
            "        )",
            "",
            "        kube_executor_config = PodGenerator.from_obj(executor_config)",
            "        if not self.task_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.task_queue.put((key, command, kube_executor_config))",
            "",
            "    def sync(self) -> None:",
            "        \"\"\"Synchronize task state.\"\"\"",
            "        if self.running:",
            "            self.log.debug('self.running: %s', self.running)",
            "        if self.queued_tasks:",
            "            self.log.debug('self.queued: %s', self.queued_tasks)",
            "        if not self.worker_uuid:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.kube_scheduler:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.kube_config:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.result_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.task_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.kube_scheduler.sync()",
            "",
            "        last_resource_version = None",
            "        while True:  # pylint: disable=too-many-nested-blocks",
            "            try:",
            "                results = self.result_queue.get_nowait()",
            "                try:",
            "                    key, state, pod_id, namespace, resource_version = results",
            "                    last_resource_version = resource_version",
            "                    self.log.info('Changing state of %s to %s', results, state)",
            "                    try:",
            "                        self._change_state(key, state, pod_id, namespace)",
            "                    except Exception as e:  # pylint: disable=broad-except",
            "                        self.log.exception(",
            "                            \"Exception: %s when attempting to change state of %s to %s, re-queueing.\",",
            "                            e, results, state",
            "                        )",
            "                        self.result_queue.put(results)",
            "                finally:",
            "                    self.result_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "        KubeResourceVersion.checkpoint_resource_version(last_resource_version)",
            "",
            "        # pylint: disable=too-many-nested-blocks",
            "        for _ in range(self.kube_config.worker_pods_creation_batch_size):",
            "            try:",
            "                task = self.task_queue.get_nowait()",
            "                try:",
            "                    self.kube_scheduler.run_next(task)",
            "                except ApiException as e:",
            "                    self.log.warning('ApiException when attempting to run task, re-queueing. '",
            "                                     'Message: %s', json.loads(e.body)['message'])",
            "                    self.task_queue.put(task)",
            "                finally:",
            "                    self.task_queue.task_done()",
            "            except Empty:",
            "                break",
            "        # pylint: enable=too-many-nested-blocks",
            "",
            "    def _change_state(self,",
            "                      key: TaskInstanceKeyType,",
            "                      state: Optional[str],",
            "                      pod_id: str,",
            "                      namespace: str) -> None:",
            "        if state != State.RUNNING:",
            "            if self.kube_config.delete_worker_pods:",
            "                if not self.kube_scheduler:",
            "                    raise AirflowException(NOT_STARTED_MESSAGE)",
            "                if state is not State.FAILED or self.kube_config.delete_worker_pods_on_failure:",
            "                    self.kube_scheduler.delete_pod(pod_id, namespace)",
            "                    self.log.info('Deleted pod: %s in namespace %s', str(key), str(namespace))",
            "            try:",
            "                self.running.remove(key)",
            "            except KeyError:",
            "                self.log.debug('Could not find key: %s', str(key))",
            "        self.event_buffer[key] = state, None",
            "",
            "    def _flush_task_queue(self) -> None:",
            "        if not self.task_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.log.debug('Executor shutting down, task_queue approximate size=%d', self.task_queue.qsize())",
            "        while True:",
            "            try:",
            "                task = self.task_queue.get_nowait()",
            "                # This is a new task to run thus ok to ignore.",
            "                self.log.warning('Executor shutting down, will NOT run task=%s', task)",
            "                self.task_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "    def _flush_result_queue(self) -> None:",
            "        if not self.result_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.log.debug('Executor shutting down, result_queue approximate size=%d', self.result_queue.qsize())",
            "        while True:  # pylint: disable=too-many-nested-blocks",
            "            try:",
            "                results = self.result_queue.get_nowait()",
            "                self.log.warning('Executor shutting down, flushing results=%s', results)",
            "                try:",
            "                    key, state, pod_id, namespace, resource_version = results",
            "                    self.log.info('Changing state of %s to %s : resource_version=%d', results, state,",
            "                                  resource_version)",
            "                    try:",
            "                        self._change_state(key, state, pod_id, namespace)",
            "                    except Exception as e:  # pylint: disable=broad-except",
            "                        self.log.exception('Ignoring exception: %s when attempting to change state of %s '",
            "                                           'to %s.', e, results, state)",
            "                finally:",
            "                    self.result_queue.task_done()",
            "            except Empty:",
            "                break",
            "",
            "    def end(self) -> None:",
            "        \"\"\"Called when the executor shuts down\"\"\"",
            "        if not self.task_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.result_queue:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.kube_scheduler:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.log.info('Shutting down Kubernetes executor')",
            "        self.log.debug('Flushing task_queue...')",
            "        self._flush_task_queue()",
            "        self.log.debug('Flushing result_queue...')",
            "        self._flush_result_queue()",
            "        # Both queues should be empty...",
            "        self.task_queue.join()",
            "        self.result_queue.join()",
            "        if self.kube_scheduler:",
            "            self.kube_scheduler.terminate()",
            "        self._manager.shutdown()",
            "",
            "    def terminate(self):",
            "        \"\"\"Terminate the executor is not doing anything.\"\"\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.executors.kubernetes_executor.AirflowKubernetesScheduler.run_next.command",
            "tensorflow.python.kernel_tests.nn_ops.lrn_op_test.LRNOpTest._LRN"
        ]
    },
    "airflow/executors/local_executor.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 284,
                "PatchRowcode": "         \"\"\"Execute asynchronously.\"\"\""
            },
            "1": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": 285,
                "PatchRowcode": "         if not self.impl:"
            },
            "2": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 286,
                "PatchRowcode": "             raise AirflowException(NOT_STARTED_MESSAGE)"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 287,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 288,
                "PatchRowcode": "+        if command[0:3] != [\"airflow\", \"tasks\", \"run\"]:"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 289,
                "PatchRowcode": "+            raise ValueError('The command must start with [\"airflow\", \"tasks\", \"run\"].')"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 290,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": 291,
                "PatchRowcode": "         self.impl.execute_async(key=key, command=command, queue=queue, executor_config=executor_config)"
            },
            "8": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": 292,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 289,
                "afterPatchRowNumber": 293,
                "PatchRowcode": "     def sync(self) -> None:"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"",
            "LocalExecutor",
            "",
            ".. seealso::",
            "    For more information on how the LocalExecutor works, take a look at the guide:",
            "    :ref:`executor:LocalExecutor`",
            "\"\"\"",
            "import subprocess",
            "from multiprocessing import Manager, Process",
            "from multiprocessing.managers import SyncManager",
            "from queue import Empty, Queue  # pylint: disable=unused-import  # noqa: F401",
            "from typing import Any, List, Optional, Tuple, Union  # pylint: disable=unused-import # noqa: F401",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.executors.base_executor import NOT_STARTED_MESSAGE, PARALLELISM, BaseExecutor, CommandType",
            "from airflow.models.taskinstance import (  # pylint: disable=unused-import # noqa: F401",
            "    TaskInstanceKeyType, TaskInstanceStateType,",
            ")",
            "from airflow.utils.log.logging_mixin import LoggingMixin",
            "from airflow.utils.state import State",
            "",
            "# This is a work to be executed by a worker.",
            "# It can Key and Command - but it can also be None, None which is actually a",
            "# \"Poison Pill\" - worker seeing Poison Pill should take the pill and ... die instantly.",
            "ExecutorWorkType = Tuple[Optional[TaskInstanceKeyType], Optional[CommandType]]",
            "",
            "",
            "class LocalWorkerBase(Process, LoggingMixin):",
            "    \"\"\"",
            "    LocalWorkerBase implementation to run airflow commands. Executes the given",
            "    command and puts the result into a result queue when done, terminating execution.",
            "",
            "    :param result_queue: the queue to store result state",
            "    \"\"\"",
            "    def __init__(self, result_queue: 'Queue[TaskInstanceStateType]'):",
            "        super().__init__()",
            "        self.daemon: bool = True",
            "        self.result_queue: 'Queue[TaskInstanceStateType]' = result_queue",
            "",
            "    def execute_work(self, key: TaskInstanceKeyType, command: CommandType) -> None:",
            "        \"\"\"",
            "        Executes command received and stores result state in queue.",
            "",
            "        :param key: the key to identify the task instance",
            "        :param command: the command to execute",
            "        \"\"\"",
            "        if key is None:",
            "            return",
            "        self.log.info(\"%s running %s\", self.__class__.__name__, command)",
            "        try:",
            "            subprocess.check_call(command, close_fds=True)",
            "            state = State.SUCCESS",
            "        except subprocess.CalledProcessError as e:",
            "            state = State.FAILED",
            "            self.log.error(\"Failed to execute task %s.\", str(e))",
            "        self.result_queue.put((key, state))",
            "",
            "",
            "class LocalWorker(LocalWorkerBase):",
            "    \"\"\"",
            "    Local worker that executes the task.",
            "",
            "    :param result_queue: queue where results of the tasks are put.",
            "    :param key: key identifying task instance",
            "    :param command: Command to execute",
            "    \"\"\"",
            "    def __init__(self,",
            "                 result_queue: 'Queue[TaskInstanceStateType]',",
            "                 key: TaskInstanceKeyType,",
            "                 command: CommandType):",
            "        super().__init__(result_queue)",
            "        self.key: TaskInstanceKeyType = key",
            "        self.command: CommandType = command",
            "",
            "    def run(self) -> None:",
            "        self.execute_work(key=self.key, command=self.command)",
            "",
            "",
            "class QueuedLocalWorker(LocalWorkerBase):",
            "    \"\"\"",
            "    LocalWorker implementation that is waiting for tasks from a queue and will",
            "    continue executing commands as they become available in the queue.",
            "    It will terminate execution once the poison token is found.",
            "",
            "    :param task_queue: queue from which worker reads tasks",
            "    :param result_queue: queue where worker puts results after finishing tasks",
            "    \"\"\"",
            "    def __init__(self,",
            "                 task_queue: 'Queue[ExecutorWorkType]',",
            "                 result_queue: 'Queue[TaskInstanceStateType]'):",
            "        super().__init__(result_queue=result_queue)",
            "        self.task_queue = task_queue",
            "",
            "    def run(self) -> None:",
            "        while True:",
            "            key, command = self.task_queue.get()",
            "            try:",
            "                if key is None or command is None:",
            "                    # Received poison pill, no more tasks to run",
            "                    break",
            "                self.execute_work(key=key, command=command)",
            "            finally:",
            "                self.task_queue.task_done()",
            "",
            "",
            "class LocalExecutor(BaseExecutor):",
            "    \"\"\"",
            "    LocalExecutor executes tasks locally in parallel.",
            "    It uses the multiprocessing Python library and queues to parallelize the execution",
            "    of tasks.",
            "",
            "    :param parallelism: how many parallel processes are run in the executor",
            "    \"\"\"",
            "    def __init__(self, parallelism: int = PARALLELISM):",
            "        super().__init__(parallelism=parallelism)",
            "        self.manager: Optional[SyncManager] = None",
            "        self.result_queue: Optional['Queue[TaskInstanceStateType]'] = None",
            "        self.workers: List[QueuedLocalWorker] = []",
            "        self.workers_used: int = 0",
            "        self.workers_active: int = 0",
            "        self.impl: Optional[Union['LocalExecutor.UnlimitedParallelism',",
            "                                  'LocalExecutor.LimitedParallelism']] = None",
            "",
            "    class UnlimitedParallelism:",
            "        \"\"\"",
            "        Implements LocalExecutor with unlimited parallelism, starting one process",
            "        per each command to execute.",
            "",
            "        :param executor: the executor instance to implement.",
            "        \"\"\"",
            "        def __init__(self, executor: 'LocalExecutor'):",
            "            self.executor: 'LocalExecutor' = executor",
            "",
            "        def start(self) -> None:",
            "            \"\"\"Starts the executor.\"\"\"",
            "            self.executor.workers_used = 0",
            "            self.executor.workers_active = 0",
            "",
            "        # noinspection PyUnusedLocal",
            "        def execute_async(self,",
            "                          key: TaskInstanceKeyType,",
            "                          command: CommandType,",
            "                          queue: Optional[str] = None,",
            "                          executor_config: Optional[Any] = None) -> None:  \\",
            "                # pylint: disable=unused-argument # pragma: no cover",
            "            \"\"\"",
            "            Executes task asynchronously.",
            "",
            "            :param key: the key to identify the task instance",
            "            :param command: the command to execute",
            "            :param queue: Name of the queue",
            "            :param executor_config: configuration for the executor",
            "            \"\"\"",
            "            if not self.executor.result_queue:",
            "                raise AirflowException(NOT_STARTED_MESSAGE)",
            "            local_worker = LocalWorker(self.executor.result_queue, key=key, command=command)",
            "            self.executor.workers_used += 1",
            "            self.executor.workers_active += 1",
            "            local_worker.start()",
            "",
            "        def sync(self) -> None:",
            "            \"\"\"",
            "            Sync will get called periodically by the heartbeat method.",
            "            \"\"\"",
            "            if not self.executor.result_queue:",
            "                raise AirflowException(\"Executor should be started first\")",
            "            while not self.executor.result_queue.empty():",
            "                results = self.executor.result_queue.get()",
            "                self.executor.change_state(*results)",
            "                self.executor.workers_active -= 1",
            "",
            "        def end(self) -> None:",
            "            \"\"\"",
            "            This method is called when the caller is done submitting job and",
            "            wants to wait synchronously for the job submitted previously to be",
            "            all done.",
            "            \"\"\"",
            "            while self.executor.workers_active > 0:",
            "                self.executor.sync()",
            "",
            "    class LimitedParallelism:",
            "        \"\"\"",
            "        Implements LocalExecutor with limited parallelism using a task queue to",
            "        coordinate work distribution.",
            "",
            "        :param executor: the executor instance to implement.",
            "        \"\"\"",
            "        def __init__(self, executor: 'LocalExecutor'):",
            "            self.executor: 'LocalExecutor' = executor",
            "            self.queue: Optional['Queue[ExecutorWorkType]'] = None",
            "",
            "        def start(self) -> None:",
            "            \"\"\"Starts limited parallelism implementation.\"\"\"",
            "            if not self.executor.manager:",
            "                raise AirflowException(NOT_STARTED_MESSAGE)",
            "            self.queue = self.executor.manager.Queue()",
            "            if not self.executor.result_queue:",
            "                raise AirflowException(NOT_STARTED_MESSAGE)",
            "            self.executor.workers = [",
            "                QueuedLocalWorker(self.queue, self.executor.result_queue)",
            "                for _ in range(self.executor.parallelism)",
            "            ]",
            "",
            "            self.executor.workers_used = len(self.executor.workers)",
            "",
            "            for worker in self.executor.workers:",
            "                worker.start()",
            "",
            "        # noinspection PyUnusedLocal",
            "        def execute_async(self,",
            "                          key: TaskInstanceKeyType,",
            "                          command: CommandType,",
            "                          queue: Optional[str] = None,",
            "                          executor_config: Optional[Any] = None) -> None: \\",
            "                # pylint: disable=unused-argument # pragma: no cover",
            "            \"\"\"",
            "            Executes task asynchronously.",
            "",
            "            :param key: the key to identify the task instance",
            "            :param command: the command to execute",
            "            :param queue: name of the queue",
            "            :param executor_config: configuration for the executor",
            "           \"\"\"",
            "            if not self.queue:",
            "                raise AirflowException(NOT_STARTED_MESSAGE)",
            "            self.queue.put((key, command))",
            "",
            "        def sync(self):",
            "            \"\"\"",
            "            Sync will get called periodically by the heartbeat method.",
            "            \"\"\"",
            "            while True:",
            "                try:",
            "                    results = self.executor.result_queue.get_nowait()",
            "                    try:",
            "                        self.executor.change_state(*results)",
            "                    finally:",
            "                        self.executor.result_queue.task_done()",
            "                except Empty:",
            "                    break",
            "",
            "        def end(self):",
            "            \"\"\"Ends the executor. Sends the poison pill to all workers.\"\"\"",
            "            for _ in self.executor.workers:",
            "                self.queue.put((None, None))",
            "",
            "            # Wait for commands to finish",
            "            self.queue.join()",
            "            self.executor.sync()",
            "",
            "    def start(self) -> None:",
            "        \"\"\"Starts the executor\"\"\"",
            "        self.manager = Manager()",
            "        self.result_queue = self.manager.Queue()",
            "        self.workers = []",
            "        self.workers_used = 0",
            "        self.workers_active = 0",
            "        self.impl = (LocalExecutor.UnlimitedParallelism(self) if self.parallelism == 0",
            "                     else LocalExecutor.LimitedParallelism(self))",
            "",
            "        self.impl.start()",
            "",
            "    def execute_async(self, key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None) -> None:",
            "        \"\"\"Execute asynchronously.\"\"\"",
            "        if not self.impl:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.impl.execute_async(key=key, command=command, queue=queue, executor_config=executor_config)",
            "",
            "    def sync(self) -> None:",
            "        \"\"\"",
            "        Sync will get called periodically by the heartbeat method.",
            "        \"\"\"",
            "        if not self.impl:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.impl.sync()",
            "",
            "    def end(self) -> None:",
            "        \"\"\"",
            "        Ends the executor.",
            "        :return:",
            "        \"\"\"",
            "        if not self.impl:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.manager:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.impl.end()",
            "        self.manager.shutdown()",
            "",
            "    def terminate(self):",
            "        \"\"\"Terminate the executor is not doing anything.\"\"\""
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"",
            "LocalExecutor",
            "",
            ".. seealso::",
            "    For more information on how the LocalExecutor works, take a look at the guide:",
            "    :ref:`executor:LocalExecutor`",
            "\"\"\"",
            "import subprocess",
            "from multiprocessing import Manager, Process",
            "from multiprocessing.managers import SyncManager",
            "from queue import Empty, Queue  # pylint: disable=unused-import  # noqa: F401",
            "from typing import Any, List, Optional, Tuple, Union  # pylint: disable=unused-import # noqa: F401",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.executors.base_executor import NOT_STARTED_MESSAGE, PARALLELISM, BaseExecutor, CommandType",
            "from airflow.models.taskinstance import (  # pylint: disable=unused-import # noqa: F401",
            "    TaskInstanceKeyType, TaskInstanceStateType,",
            ")",
            "from airflow.utils.log.logging_mixin import LoggingMixin",
            "from airflow.utils.state import State",
            "",
            "# This is a work to be executed by a worker.",
            "# It can Key and Command - but it can also be None, None which is actually a",
            "# \"Poison Pill\" - worker seeing Poison Pill should take the pill and ... die instantly.",
            "ExecutorWorkType = Tuple[Optional[TaskInstanceKeyType], Optional[CommandType]]",
            "",
            "",
            "class LocalWorkerBase(Process, LoggingMixin):",
            "    \"\"\"",
            "    LocalWorkerBase implementation to run airflow commands. Executes the given",
            "    command and puts the result into a result queue when done, terminating execution.",
            "",
            "    :param result_queue: the queue to store result state",
            "    \"\"\"",
            "    def __init__(self, result_queue: 'Queue[TaskInstanceStateType]'):",
            "        super().__init__()",
            "        self.daemon: bool = True",
            "        self.result_queue: 'Queue[TaskInstanceStateType]' = result_queue",
            "",
            "    def execute_work(self, key: TaskInstanceKeyType, command: CommandType) -> None:",
            "        \"\"\"",
            "        Executes command received and stores result state in queue.",
            "",
            "        :param key: the key to identify the task instance",
            "        :param command: the command to execute",
            "        \"\"\"",
            "        if key is None:",
            "            return",
            "        self.log.info(\"%s running %s\", self.__class__.__name__, command)",
            "        try:",
            "            subprocess.check_call(command, close_fds=True)",
            "            state = State.SUCCESS",
            "        except subprocess.CalledProcessError as e:",
            "            state = State.FAILED",
            "            self.log.error(\"Failed to execute task %s.\", str(e))",
            "        self.result_queue.put((key, state))",
            "",
            "",
            "class LocalWorker(LocalWorkerBase):",
            "    \"\"\"",
            "    Local worker that executes the task.",
            "",
            "    :param result_queue: queue where results of the tasks are put.",
            "    :param key: key identifying task instance",
            "    :param command: Command to execute",
            "    \"\"\"",
            "    def __init__(self,",
            "                 result_queue: 'Queue[TaskInstanceStateType]',",
            "                 key: TaskInstanceKeyType,",
            "                 command: CommandType):",
            "        super().__init__(result_queue)",
            "        self.key: TaskInstanceKeyType = key",
            "        self.command: CommandType = command",
            "",
            "    def run(self) -> None:",
            "        self.execute_work(key=self.key, command=self.command)",
            "",
            "",
            "class QueuedLocalWorker(LocalWorkerBase):",
            "    \"\"\"",
            "    LocalWorker implementation that is waiting for tasks from a queue and will",
            "    continue executing commands as they become available in the queue.",
            "    It will terminate execution once the poison token is found.",
            "",
            "    :param task_queue: queue from which worker reads tasks",
            "    :param result_queue: queue where worker puts results after finishing tasks",
            "    \"\"\"",
            "    def __init__(self,",
            "                 task_queue: 'Queue[ExecutorWorkType]',",
            "                 result_queue: 'Queue[TaskInstanceStateType]'):",
            "        super().__init__(result_queue=result_queue)",
            "        self.task_queue = task_queue",
            "",
            "    def run(self) -> None:",
            "        while True:",
            "            key, command = self.task_queue.get()",
            "            try:",
            "                if key is None or command is None:",
            "                    # Received poison pill, no more tasks to run",
            "                    break",
            "                self.execute_work(key=key, command=command)",
            "            finally:",
            "                self.task_queue.task_done()",
            "",
            "",
            "class LocalExecutor(BaseExecutor):",
            "    \"\"\"",
            "    LocalExecutor executes tasks locally in parallel.",
            "    It uses the multiprocessing Python library and queues to parallelize the execution",
            "    of tasks.",
            "",
            "    :param parallelism: how many parallel processes are run in the executor",
            "    \"\"\"",
            "    def __init__(self, parallelism: int = PARALLELISM):",
            "        super().__init__(parallelism=parallelism)",
            "        self.manager: Optional[SyncManager] = None",
            "        self.result_queue: Optional['Queue[TaskInstanceStateType]'] = None",
            "        self.workers: List[QueuedLocalWorker] = []",
            "        self.workers_used: int = 0",
            "        self.workers_active: int = 0",
            "        self.impl: Optional[Union['LocalExecutor.UnlimitedParallelism',",
            "                                  'LocalExecutor.LimitedParallelism']] = None",
            "",
            "    class UnlimitedParallelism:",
            "        \"\"\"",
            "        Implements LocalExecutor with unlimited parallelism, starting one process",
            "        per each command to execute.",
            "",
            "        :param executor: the executor instance to implement.",
            "        \"\"\"",
            "        def __init__(self, executor: 'LocalExecutor'):",
            "            self.executor: 'LocalExecutor' = executor",
            "",
            "        def start(self) -> None:",
            "            \"\"\"Starts the executor.\"\"\"",
            "            self.executor.workers_used = 0",
            "            self.executor.workers_active = 0",
            "",
            "        # noinspection PyUnusedLocal",
            "        def execute_async(self,",
            "                          key: TaskInstanceKeyType,",
            "                          command: CommandType,",
            "                          queue: Optional[str] = None,",
            "                          executor_config: Optional[Any] = None) -> None:  \\",
            "                # pylint: disable=unused-argument # pragma: no cover",
            "            \"\"\"",
            "            Executes task asynchronously.",
            "",
            "            :param key: the key to identify the task instance",
            "            :param command: the command to execute",
            "            :param queue: Name of the queue",
            "            :param executor_config: configuration for the executor",
            "            \"\"\"",
            "            if not self.executor.result_queue:",
            "                raise AirflowException(NOT_STARTED_MESSAGE)",
            "            local_worker = LocalWorker(self.executor.result_queue, key=key, command=command)",
            "            self.executor.workers_used += 1",
            "            self.executor.workers_active += 1",
            "            local_worker.start()",
            "",
            "        def sync(self) -> None:",
            "            \"\"\"",
            "            Sync will get called periodically by the heartbeat method.",
            "            \"\"\"",
            "            if not self.executor.result_queue:",
            "                raise AirflowException(\"Executor should be started first\")",
            "            while not self.executor.result_queue.empty():",
            "                results = self.executor.result_queue.get()",
            "                self.executor.change_state(*results)",
            "                self.executor.workers_active -= 1",
            "",
            "        def end(self) -> None:",
            "            \"\"\"",
            "            This method is called when the caller is done submitting job and",
            "            wants to wait synchronously for the job submitted previously to be",
            "            all done.",
            "            \"\"\"",
            "            while self.executor.workers_active > 0:",
            "                self.executor.sync()",
            "",
            "    class LimitedParallelism:",
            "        \"\"\"",
            "        Implements LocalExecutor with limited parallelism using a task queue to",
            "        coordinate work distribution.",
            "",
            "        :param executor: the executor instance to implement.",
            "        \"\"\"",
            "        def __init__(self, executor: 'LocalExecutor'):",
            "            self.executor: 'LocalExecutor' = executor",
            "            self.queue: Optional['Queue[ExecutorWorkType]'] = None",
            "",
            "        def start(self) -> None:",
            "            \"\"\"Starts limited parallelism implementation.\"\"\"",
            "            if not self.executor.manager:",
            "                raise AirflowException(NOT_STARTED_MESSAGE)",
            "            self.queue = self.executor.manager.Queue()",
            "            if not self.executor.result_queue:",
            "                raise AirflowException(NOT_STARTED_MESSAGE)",
            "            self.executor.workers = [",
            "                QueuedLocalWorker(self.queue, self.executor.result_queue)",
            "                for _ in range(self.executor.parallelism)",
            "            ]",
            "",
            "            self.executor.workers_used = len(self.executor.workers)",
            "",
            "            for worker in self.executor.workers:",
            "                worker.start()",
            "",
            "        # noinspection PyUnusedLocal",
            "        def execute_async(self,",
            "                          key: TaskInstanceKeyType,",
            "                          command: CommandType,",
            "                          queue: Optional[str] = None,",
            "                          executor_config: Optional[Any] = None) -> None: \\",
            "                # pylint: disable=unused-argument # pragma: no cover",
            "            \"\"\"",
            "            Executes task asynchronously.",
            "",
            "            :param key: the key to identify the task instance",
            "            :param command: the command to execute",
            "            :param queue: name of the queue",
            "            :param executor_config: configuration for the executor",
            "           \"\"\"",
            "            if not self.queue:",
            "                raise AirflowException(NOT_STARTED_MESSAGE)",
            "            self.queue.put((key, command))",
            "",
            "        def sync(self):",
            "            \"\"\"",
            "            Sync will get called periodically by the heartbeat method.",
            "            \"\"\"",
            "            while True:",
            "                try:",
            "                    results = self.executor.result_queue.get_nowait()",
            "                    try:",
            "                        self.executor.change_state(*results)",
            "                    finally:",
            "                        self.executor.result_queue.task_done()",
            "                except Empty:",
            "                    break",
            "",
            "        def end(self):",
            "            \"\"\"Ends the executor. Sends the poison pill to all workers.\"\"\"",
            "            for _ in self.executor.workers:",
            "                self.queue.put((None, None))",
            "",
            "            # Wait for commands to finish",
            "            self.queue.join()",
            "            self.executor.sync()",
            "",
            "    def start(self) -> None:",
            "        \"\"\"Starts the executor\"\"\"",
            "        self.manager = Manager()",
            "        self.result_queue = self.manager.Queue()",
            "        self.workers = []",
            "        self.workers_used = 0",
            "        self.workers_active = 0",
            "        self.impl = (LocalExecutor.UnlimitedParallelism(self) if self.parallelism == 0",
            "                     else LocalExecutor.LimitedParallelism(self))",
            "",
            "        self.impl.start()",
            "",
            "    def execute_async(self, key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None) -> None:",
            "        \"\"\"Execute asynchronously.\"\"\"",
            "        if not self.impl:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "",
            "        if command[0:3] != [\"airflow\", \"tasks\", \"run\"]:",
            "            raise ValueError('The command must start with [\"airflow\", \"tasks\", \"run\"].')",
            "",
            "        self.impl.execute_async(key=key, command=command, queue=queue, executor_config=executor_config)",
            "",
            "    def sync(self) -> None:",
            "        \"\"\"",
            "        Sync will get called periodically by the heartbeat method.",
            "        \"\"\"",
            "        if not self.impl:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.impl.sync()",
            "",
            "    def end(self) -> None:",
            "        \"\"\"",
            "        Ends the executor.",
            "        :return:",
            "        \"\"\"",
            "        if not self.impl:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        if not self.manager:",
            "            raise AirflowException(NOT_STARTED_MESSAGE)",
            "        self.impl.end()",
            "        self.manager.shutdown()",
            "",
            "    def terminate(self):",
            "        \"\"\"Terminate the executor is not doing anything.\"\"\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "tensorflow.python.kernel_tests.nn_ops.lrn_op_test.LRNOpTest._LRN"
        ]
    },
    "airflow/executors/sequential_executor.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "                       command: CommandType,"
            },
            "1": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "                       queue: Optional[str] = None,"
            },
            "2": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "                       executor_config: Optional[Any] = None) -> None:"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+        if command[0:3] != [\"airflow\", \"tasks\", \"run\"]:"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+            raise ValueError('The command must start with [\"airflow\", \"tasks\", \"run\"].')"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "         self.commands_to_run.append((key, command))"
            },
            "8": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 57,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "     def sync(self) -> None:"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"",
            "SequentialExecutor",
            "",
            ".. seealso::",
            "    For more information on how the SequentialExecutor works, take a look at the guide:",
            "    :ref:`executor:SequentialExecutor`",
            "\"\"\"",
            "import subprocess",
            "from typing import Any, Optional",
            "",
            "from airflow.executors.base_executor import BaseExecutor, CommandType",
            "from airflow.models.taskinstance import TaskInstanceKeyType",
            "from airflow.utils.state import State",
            "",
            "",
            "class SequentialExecutor(BaseExecutor):",
            "    \"\"\"",
            "    This executor will only run one task instance at a time, can be used",
            "    for debugging. It is also the only executor that can be used with sqlite",
            "    since sqlite doesn't support multiple connections.",
            "",
            "    Since we want airflow to work out of the box, it defaults to this",
            "    SequentialExecutor alongside sqlite as you first install it.",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        super().__init__()",
            "        self.commands_to_run = []",
            "",
            "    def execute_async(self,",
            "                      key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None) -> None:",
            "        self.commands_to_run.append((key, command))",
            "",
            "    def sync(self) -> None:",
            "        for key, command in self.commands_to_run:",
            "            self.log.info(\"Executing command: %s\", command)",
            "",
            "            try:",
            "                subprocess.check_call(command, close_fds=True)",
            "                self.change_state(key, State.SUCCESS)",
            "            except subprocess.CalledProcessError as e:",
            "                self.change_state(key, State.FAILED)",
            "                self.log.error(\"Failed to execute task %s.\", str(e))",
            "",
            "        self.commands_to_run = []",
            "",
            "    def end(self):",
            "        \"\"\"End the executor.\"\"\"",
            "        self.heartbeat()",
            "",
            "    def terminate(self):",
            "        \"\"\"Terminate the executor is not doing anything.\"\"\""
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"",
            "SequentialExecutor",
            "",
            ".. seealso::",
            "    For more information on how the SequentialExecutor works, take a look at the guide:",
            "    :ref:`executor:SequentialExecutor`",
            "\"\"\"",
            "import subprocess",
            "from typing import Any, Optional",
            "",
            "from airflow.executors.base_executor import BaseExecutor, CommandType",
            "from airflow.models.taskinstance import TaskInstanceKeyType",
            "from airflow.utils.state import State",
            "",
            "",
            "class SequentialExecutor(BaseExecutor):",
            "    \"\"\"",
            "    This executor will only run one task instance at a time, can be used",
            "    for debugging. It is also the only executor that can be used with sqlite",
            "    since sqlite doesn't support multiple connections.",
            "",
            "    Since we want airflow to work out of the box, it defaults to this",
            "    SequentialExecutor alongside sqlite as you first install it.",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        super().__init__()",
            "        self.commands_to_run = []",
            "",
            "    def execute_async(self,",
            "                      key: TaskInstanceKeyType,",
            "                      command: CommandType,",
            "                      queue: Optional[str] = None,",
            "                      executor_config: Optional[Any] = None) -> None:",
            "",
            "        if command[0:3] != [\"airflow\", \"tasks\", \"run\"]:",
            "            raise ValueError('The command must start with [\"airflow\", \"tasks\", \"run\"].')",
            "",
            "        self.commands_to_run.append((key, command))",
            "",
            "    def sync(self) -> None:",
            "        for key, command in self.commands_to_run:",
            "            self.log.info(\"Executing command: %s\", command)",
            "",
            "            try:",
            "                subprocess.check_call(command, close_fds=True)",
            "                self.change_state(key, State.SUCCESS)",
            "            except subprocess.CalledProcessError as e:",
            "                self.change_state(key, State.FAILED)",
            "                self.log.error(\"Failed to execute task %s.\", str(e))",
            "",
            "        self.commands_to_run = []",
            "",
            "    def end(self):",
            "        \"\"\"End the executor.\"\"\"",
            "        self.heartbeat()",
            "",
            "    def terminate(self):",
            "        \"\"\"Terminate the executor is not doing anything.\"\"\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "tensorflow.python.kernel_tests.nn_ops.lrn_op_test.LRNOpTest._LRN"
        ]
    }
}