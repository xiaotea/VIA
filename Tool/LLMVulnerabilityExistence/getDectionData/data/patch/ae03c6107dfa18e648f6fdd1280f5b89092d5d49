{
    "markdown_it/renderer.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 83,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "         for i, token in enumerate(tokens):"
            },
            "2": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "             if token.type == \"inline\":"
            },
            "3": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                assert token.children is not None"
            },
            "4": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                result += self.renderInline(token.children, options, env)"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+                if token.children:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+                    result += self.renderInline(token.children, options, env)"
            },
            "7": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 88,
                "PatchRowcode": "             elif token.type in self.rules:"
            },
            "8": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "                 result += self.rules[token.type](tokens, i, options, env)"
            },
            "9": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "             else:"
            },
            "10": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "             if token.type == \"text\":"
            },
            "11": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "                 result += token.content"
            },
            "12": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "             elif token.type == \"image\":"
            },
            "13": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                assert token.children is not None"
            },
            "14": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                result += self.renderInlineAsText(token.children, options, env)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+                if token.children:"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+                    result += self.renderInlineAsText(token.children, options, env)"
            },
            "17": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": 211,
                "PatchRowcode": "             elif token.type == \"softbreak\":"
            },
            "18": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": 212,
                "PatchRowcode": "                 result += \"\\n\""
            },
            "19": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 213,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": 305,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": 306,
                "PatchRowcode": "         # \"alt\" attr MUST be set, even if empty. Because it's mandatory and"
            },
            "22": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": 307,
                "PatchRowcode": "         # should be placed on proper position for tests."
            },
            "23": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "24": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        assert ("
            },
            "25": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            token.attrs and \"alt\" in token.attrs"
            },
            "26": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ), '\"image\" token\\'s attrs must contain `alt`'"
            },
            "27": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "28": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # Replace content with actual value"
            },
            "29": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "30": {
                "beforePatchRowNumber": 315,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        token.attrSet(\"alt\", self.renderInlineAsText(token.children, options, env))"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 308,
                "PatchRowcode": "+        if token.children:"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 309,
                "PatchRowcode": "+            token.attrSet(\"alt\", self.renderInlineAsText(token.children, options, env))"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 310,
                "PatchRowcode": "+        else:"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 311,
                "PatchRowcode": "+            token.attrSet(\"alt\", \"\")"
            },
            "35": {
                "beforePatchRowNumber": 316,
                "afterPatchRowNumber": 312,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 317,
                "afterPatchRowNumber": 313,
                "PatchRowcode": "         return self.renderToken(tokens, idx, options, env)"
            },
            "37": {
                "beforePatchRowNumber": 318,
                "afterPatchRowNumber": 314,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "class Renderer",
            "",
            "Generates HTML from parsed token stream. Each instance has independent",
            "copy of rules. Those can be rewritten with ease. Also, you can add new",
            "rules if you create plugin and adds new token types.",
            "\"\"\"",
            "from __future__ import annotations",
            "",
            "from collections.abc import MutableMapping, Sequence",
            "import inspect",
            "from typing import Any, ClassVar",
            "",
            "from .common.utils import escapeHtml, unescapeAll",
            "from .token import Token",
            "from .utils import OptionsDict",
            "",
            "try:",
            "    from typing import Protocol",
            "except ImportError:  # Python <3.8 doesn't have `Protocol` in the stdlib",
            "    from typing_extensions import Protocol  # type: ignore",
            "",
            "",
            "class RendererProtocol(Protocol):",
            "    __output__: ClassVar[str]",
            "",
            "    def render(",
            "        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping",
            "    ) -> Any:",
            "        ...",
            "",
            "",
            "class RendererHTML(RendererProtocol):",
            "    \"\"\"Contains render rules for tokens. Can be updated and extended.",
            "",
            "    Example:",
            "",
            "    Each rule is called as independent static function with fixed signature:",
            "",
            "    ::",
            "",
            "        class Renderer:",
            "            def token_type_name(self, tokens, idx, options, env) {",
            "                # ...",
            "                return renderedHTML",
            "",
            "    ::",
            "",
            "        class CustomRenderer(RendererHTML):",
            "            def strong_open(self, tokens, idx, options, env):",
            "                return '<b>'",
            "            def strong_close(self, tokens, idx, options, env):",
            "                return '</b>'",
            "",
            "        md = MarkdownIt(renderer_cls=CustomRenderer)",
            "",
            "        result = md.render(...)",
            "",
            "    See https://github.com/markdown-it/markdown-it/blob/master/lib/renderer.js",
            "    for more details and examples.",
            "    \"\"\"",
            "",
            "    __output__ = \"html\"",
            "",
            "    def __init__(self, parser=None):",
            "        self.rules = {",
            "            k: v",
            "            for k, v in inspect.getmembers(self, predicate=inspect.ismethod)",
            "            if not (k.startswith(\"render\") or k.startswith(\"_\"))",
            "        }",
            "",
            "    def render(",
            "        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping",
            "    ) -> str:",
            "        \"\"\"Takes token stream and generates HTML.",
            "",
            "        :param tokens: list on block tokens to render",
            "        :param options: params of parser instance",
            "        :param env: additional data from parsed input",
            "",
            "        \"\"\"",
            "        result = \"\"",
            "",
            "        for i, token in enumerate(tokens):",
            "            if token.type == \"inline\":",
            "                assert token.children is not None",
            "                result += self.renderInline(token.children, options, env)",
            "            elif token.type in self.rules:",
            "                result += self.rules[token.type](tokens, i, options, env)",
            "            else:",
            "                result += self.renderToken(tokens, i, options, env)",
            "",
            "        return result",
            "",
            "    def renderInline(",
            "        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping",
            "    ) -> str:",
            "        \"\"\"The same as ``render``, but for single token of `inline` type.",
            "",
            "        :param tokens: list on block tokens to render",
            "        :param options: params of parser instance",
            "        :param env: additional data from parsed input (references, for example)",
            "        \"\"\"",
            "        result = \"\"",
            "",
            "        for i, token in enumerate(tokens):",
            "            if token.type in self.rules:",
            "                result += self.rules[token.type](tokens, i, options, env)",
            "            else:",
            "                result += self.renderToken(tokens, i, options, env)",
            "",
            "        return result",
            "",
            "    def renderToken(",
            "        self,",
            "        tokens: Sequence[Token],",
            "        idx: int,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        \"\"\"Default token renderer.",
            "",
            "        Can be overridden by custom function",
            "",
            "        :param idx: token index to render",
            "        :param options: params of parser instance",
            "        \"\"\"",
            "        result = \"\"",
            "        needLf = False",
            "        token = tokens[idx]",
            "",
            "        # Tight list paragraphs",
            "        if token.hidden:",
            "            return \"\"",
            "",
            "        # Insert a newline between hidden paragraph and subsequent opening",
            "        # block-level tag.",
            "        #",
            "        # For example, here we should insert a newline before blockquote:",
            "        #  - a",
            "        #    >",
            "        #",
            "        if token.block and token.nesting != -1 and idx and tokens[idx - 1].hidden:",
            "            result += \"\\n\"",
            "",
            "        # Add token name, e.g. `<img`",
            "        result += (\"</\" if token.nesting == -1 else \"<\") + token.tag",
            "",
            "        # Encode attributes, e.g. `<img src=\"foo\"`",
            "        result += self.renderAttrs(token)",
            "",
            "        # Add a slash for self-closing tags, e.g. `<img src=\"foo\" /`",
            "        if token.nesting == 0 and options[\"xhtmlOut\"]:",
            "            result += \" /\"",
            "",
            "        # Check if we need to add a newline after this tag",
            "        if token.block:",
            "            needLf = True",
            "",
            "            if token.nesting == 1:",
            "                if idx + 1 < len(tokens):",
            "                    nextToken = tokens[idx + 1]",
            "",
            "                    if nextToken.type == \"inline\" or nextToken.hidden:",
            "                        # Block-level tag containing an inline tag.",
            "                        #",
            "                        needLf = False",
            "",
            "                    elif nextToken.nesting == -1 and nextToken.tag == token.tag:",
            "                        # Opening tag + closing tag of the same type. E.g. `<li></li>`.",
            "                        #",
            "                        needLf = False",
            "",
            "        result += \">\\n\" if needLf else \">\"",
            "",
            "        return result",
            "",
            "    @staticmethod",
            "    def renderAttrs(token: Token) -> str:",
            "        \"\"\"Render token attributes to string.\"\"\"",
            "        result = \"\"",
            "",
            "        for key, value in token.attrItems():",
            "            result += \" \" + escapeHtml(key) + '=\"' + escapeHtml(str(value)) + '\"'",
            "",
            "        return result",
            "",
            "    def renderInlineAsText(",
            "        self,",
            "        tokens: Sequence[Token] | None,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        \"\"\"Special kludge for image `alt` attributes to conform CommonMark spec.",
            "",
            "        Don't try to use it! Spec requires to show `alt` content with stripped markup,",
            "        instead of simple escaping.",
            "",
            "        :param tokens: list on block tokens to render",
            "        :param options: params of parser instance",
            "        :param env: additional data from parsed input",
            "        \"\"\"",
            "        result = \"\"",
            "",
            "        for token in tokens or []:",
            "            if token.type == \"text\":",
            "                result += token.content",
            "            elif token.type == \"image\":",
            "                assert token.children is not None",
            "                result += self.renderInlineAsText(token.children, options, env)",
            "            elif token.type == \"softbreak\":",
            "                result += \"\\n\"",
            "",
            "        return result",
            "",
            "    ###################################################",
            "",
            "    def code_inline(self, tokens: Sequence[Token], idx: int, options, env) -> str:",
            "        token = tokens[idx]",
            "        return (",
            "            \"<code\"",
            "            + self.renderAttrs(token)",
            "            + \">\"",
            "            + escapeHtml(tokens[idx].content)",
            "            + \"</code>\"",
            "        )",
            "",
            "    def code_block(",
            "        self,",
            "        tokens: Sequence[Token],",
            "        idx: int,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        token = tokens[idx]",
            "",
            "        return (",
            "            \"<pre\"",
            "            + self.renderAttrs(token)",
            "            + \"><code>\"",
            "            + escapeHtml(tokens[idx].content)",
            "            + \"</code></pre>\\n\"",
            "        )",
            "",
            "    def fence(",
            "        self,",
            "        tokens: Sequence[Token],",
            "        idx: int,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        token = tokens[idx]",
            "        info = unescapeAll(token.info).strip() if token.info else \"\"",
            "        langName = \"\"",
            "        langAttrs = \"\"",
            "",
            "        if info:",
            "            arr = info.split(maxsplit=1)",
            "            langName = arr[0]",
            "            if len(arr) == 2:",
            "                langAttrs = arr[1]",
            "",
            "        if options.highlight:",
            "            highlighted = options.highlight(",
            "                token.content, langName, langAttrs",
            "            ) or escapeHtml(token.content)",
            "        else:",
            "            highlighted = escapeHtml(token.content)",
            "",
            "        if highlighted.startswith(\"<pre\"):",
            "            return highlighted + \"\\n\"",
            "",
            "        # If language exists, inject class gently, without modifying original token.",
            "        # May be, one day we will add .deepClone() for token and simplify this part, but",
            "        # now we prefer to keep things local.",
            "        if info:",
            "            # Fake token just to render attributes",
            "            tmpToken = Token(type=\"\", tag=\"\", nesting=0, attrs=token.attrs.copy())",
            "            tmpToken.attrJoin(\"class\", options.langPrefix + langName)",
            "",
            "            return (",
            "                \"<pre><code\"",
            "                + self.renderAttrs(tmpToken)",
            "                + \">\"",
            "                + highlighted",
            "                + \"</code></pre>\\n\"",
            "            )",
            "",
            "        return (",
            "            \"<pre><code\"",
            "            + self.renderAttrs(token)",
            "            + \">\"",
            "            + highlighted",
            "            + \"</code></pre>\\n\"",
            "        )",
            "",
            "    def image(",
            "        self,",
            "        tokens: Sequence[Token],",
            "        idx: int,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        token = tokens[idx]",
            "",
            "        # \"alt\" attr MUST be set, even if empty. Because it's mandatory and",
            "        # should be placed on proper position for tests.",
            "",
            "        assert (",
            "            token.attrs and \"alt\" in token.attrs",
            "        ), '\"image\" token\\'s attrs must contain `alt`'",
            "",
            "        # Replace content with actual value",
            "",
            "        token.attrSet(\"alt\", self.renderInlineAsText(token.children, options, env))",
            "",
            "        return self.renderToken(tokens, idx, options, env)",
            "",
            "    def hardbreak(",
            "        self, tokens: Sequence[Token], idx: int, options: OptionsDict, *args",
            "    ) -> str:",
            "        return \"<br />\\n\" if options.xhtmlOut else \"<br>\\n\"",
            "",
            "    def softbreak(",
            "        self, tokens: Sequence[Token], idx: int, options: OptionsDict, *args",
            "    ) -> str:",
            "        return (",
            "            (\"<br />\\n\" if options.xhtmlOut else \"<br>\\n\") if options.breaks else \"\\n\"",
            "        )",
            "",
            "    def text(self, tokens: Sequence[Token], idx: int, *args) -> str:",
            "        return escapeHtml(tokens[idx].content)",
            "",
            "    def html_block(self, tokens: Sequence[Token], idx: int, *args) -> str:",
            "        return tokens[idx].content",
            "",
            "    def html_inline(self, tokens: Sequence[Token], idx: int, *args) -> str:",
            "        return tokens[idx].content"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "class Renderer",
            "",
            "Generates HTML from parsed token stream. Each instance has independent",
            "copy of rules. Those can be rewritten with ease. Also, you can add new",
            "rules if you create plugin and adds new token types.",
            "\"\"\"",
            "from __future__ import annotations",
            "",
            "from collections.abc import MutableMapping, Sequence",
            "import inspect",
            "from typing import Any, ClassVar",
            "",
            "from .common.utils import escapeHtml, unescapeAll",
            "from .token import Token",
            "from .utils import OptionsDict",
            "",
            "try:",
            "    from typing import Protocol",
            "except ImportError:  # Python <3.8 doesn't have `Protocol` in the stdlib",
            "    from typing_extensions import Protocol  # type: ignore",
            "",
            "",
            "class RendererProtocol(Protocol):",
            "    __output__: ClassVar[str]",
            "",
            "    def render(",
            "        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping",
            "    ) -> Any:",
            "        ...",
            "",
            "",
            "class RendererHTML(RendererProtocol):",
            "    \"\"\"Contains render rules for tokens. Can be updated and extended.",
            "",
            "    Example:",
            "",
            "    Each rule is called as independent static function with fixed signature:",
            "",
            "    ::",
            "",
            "        class Renderer:",
            "            def token_type_name(self, tokens, idx, options, env) {",
            "                # ...",
            "                return renderedHTML",
            "",
            "    ::",
            "",
            "        class CustomRenderer(RendererHTML):",
            "            def strong_open(self, tokens, idx, options, env):",
            "                return '<b>'",
            "            def strong_close(self, tokens, idx, options, env):",
            "                return '</b>'",
            "",
            "        md = MarkdownIt(renderer_cls=CustomRenderer)",
            "",
            "        result = md.render(...)",
            "",
            "    See https://github.com/markdown-it/markdown-it/blob/master/lib/renderer.js",
            "    for more details and examples.",
            "    \"\"\"",
            "",
            "    __output__ = \"html\"",
            "",
            "    def __init__(self, parser=None):",
            "        self.rules = {",
            "            k: v",
            "            for k, v in inspect.getmembers(self, predicate=inspect.ismethod)",
            "            if not (k.startswith(\"render\") or k.startswith(\"_\"))",
            "        }",
            "",
            "    def render(",
            "        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping",
            "    ) -> str:",
            "        \"\"\"Takes token stream and generates HTML.",
            "",
            "        :param tokens: list on block tokens to render",
            "        :param options: params of parser instance",
            "        :param env: additional data from parsed input",
            "",
            "        \"\"\"",
            "        result = \"\"",
            "",
            "        for i, token in enumerate(tokens):",
            "            if token.type == \"inline\":",
            "                if token.children:",
            "                    result += self.renderInline(token.children, options, env)",
            "            elif token.type in self.rules:",
            "                result += self.rules[token.type](tokens, i, options, env)",
            "            else:",
            "                result += self.renderToken(tokens, i, options, env)",
            "",
            "        return result",
            "",
            "    def renderInline(",
            "        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping",
            "    ) -> str:",
            "        \"\"\"The same as ``render``, but for single token of `inline` type.",
            "",
            "        :param tokens: list on block tokens to render",
            "        :param options: params of parser instance",
            "        :param env: additional data from parsed input (references, for example)",
            "        \"\"\"",
            "        result = \"\"",
            "",
            "        for i, token in enumerate(tokens):",
            "            if token.type in self.rules:",
            "                result += self.rules[token.type](tokens, i, options, env)",
            "            else:",
            "                result += self.renderToken(tokens, i, options, env)",
            "",
            "        return result",
            "",
            "    def renderToken(",
            "        self,",
            "        tokens: Sequence[Token],",
            "        idx: int,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        \"\"\"Default token renderer.",
            "",
            "        Can be overridden by custom function",
            "",
            "        :param idx: token index to render",
            "        :param options: params of parser instance",
            "        \"\"\"",
            "        result = \"\"",
            "        needLf = False",
            "        token = tokens[idx]",
            "",
            "        # Tight list paragraphs",
            "        if token.hidden:",
            "            return \"\"",
            "",
            "        # Insert a newline between hidden paragraph and subsequent opening",
            "        # block-level tag.",
            "        #",
            "        # For example, here we should insert a newline before blockquote:",
            "        #  - a",
            "        #    >",
            "        #",
            "        if token.block and token.nesting != -1 and idx and tokens[idx - 1].hidden:",
            "            result += \"\\n\"",
            "",
            "        # Add token name, e.g. `<img`",
            "        result += (\"</\" if token.nesting == -1 else \"<\") + token.tag",
            "",
            "        # Encode attributes, e.g. `<img src=\"foo\"`",
            "        result += self.renderAttrs(token)",
            "",
            "        # Add a slash for self-closing tags, e.g. `<img src=\"foo\" /`",
            "        if token.nesting == 0 and options[\"xhtmlOut\"]:",
            "            result += \" /\"",
            "",
            "        # Check if we need to add a newline after this tag",
            "        if token.block:",
            "            needLf = True",
            "",
            "            if token.nesting == 1:",
            "                if idx + 1 < len(tokens):",
            "                    nextToken = tokens[idx + 1]",
            "",
            "                    if nextToken.type == \"inline\" or nextToken.hidden:",
            "                        # Block-level tag containing an inline tag.",
            "                        #",
            "                        needLf = False",
            "",
            "                    elif nextToken.nesting == -1 and nextToken.tag == token.tag:",
            "                        # Opening tag + closing tag of the same type. E.g. `<li></li>`.",
            "                        #",
            "                        needLf = False",
            "",
            "        result += \">\\n\" if needLf else \">\"",
            "",
            "        return result",
            "",
            "    @staticmethod",
            "    def renderAttrs(token: Token) -> str:",
            "        \"\"\"Render token attributes to string.\"\"\"",
            "        result = \"\"",
            "",
            "        for key, value in token.attrItems():",
            "            result += \" \" + escapeHtml(key) + '=\"' + escapeHtml(str(value)) + '\"'",
            "",
            "        return result",
            "",
            "    def renderInlineAsText(",
            "        self,",
            "        tokens: Sequence[Token] | None,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        \"\"\"Special kludge for image `alt` attributes to conform CommonMark spec.",
            "",
            "        Don't try to use it! Spec requires to show `alt` content with stripped markup,",
            "        instead of simple escaping.",
            "",
            "        :param tokens: list on block tokens to render",
            "        :param options: params of parser instance",
            "        :param env: additional data from parsed input",
            "        \"\"\"",
            "        result = \"\"",
            "",
            "        for token in tokens or []:",
            "            if token.type == \"text\":",
            "                result += token.content",
            "            elif token.type == \"image\":",
            "                if token.children:",
            "                    result += self.renderInlineAsText(token.children, options, env)",
            "            elif token.type == \"softbreak\":",
            "                result += \"\\n\"",
            "",
            "        return result",
            "",
            "    ###################################################",
            "",
            "    def code_inline(self, tokens: Sequence[Token], idx: int, options, env) -> str:",
            "        token = tokens[idx]",
            "        return (",
            "            \"<code\"",
            "            + self.renderAttrs(token)",
            "            + \">\"",
            "            + escapeHtml(tokens[idx].content)",
            "            + \"</code>\"",
            "        )",
            "",
            "    def code_block(",
            "        self,",
            "        tokens: Sequence[Token],",
            "        idx: int,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        token = tokens[idx]",
            "",
            "        return (",
            "            \"<pre\"",
            "            + self.renderAttrs(token)",
            "            + \"><code>\"",
            "            + escapeHtml(tokens[idx].content)",
            "            + \"</code></pre>\\n\"",
            "        )",
            "",
            "    def fence(",
            "        self,",
            "        tokens: Sequence[Token],",
            "        idx: int,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        token = tokens[idx]",
            "        info = unescapeAll(token.info).strip() if token.info else \"\"",
            "        langName = \"\"",
            "        langAttrs = \"\"",
            "",
            "        if info:",
            "            arr = info.split(maxsplit=1)",
            "            langName = arr[0]",
            "            if len(arr) == 2:",
            "                langAttrs = arr[1]",
            "",
            "        if options.highlight:",
            "            highlighted = options.highlight(",
            "                token.content, langName, langAttrs",
            "            ) or escapeHtml(token.content)",
            "        else:",
            "            highlighted = escapeHtml(token.content)",
            "",
            "        if highlighted.startswith(\"<pre\"):",
            "            return highlighted + \"\\n\"",
            "",
            "        # If language exists, inject class gently, without modifying original token.",
            "        # May be, one day we will add .deepClone() for token and simplify this part, but",
            "        # now we prefer to keep things local.",
            "        if info:",
            "            # Fake token just to render attributes",
            "            tmpToken = Token(type=\"\", tag=\"\", nesting=0, attrs=token.attrs.copy())",
            "            tmpToken.attrJoin(\"class\", options.langPrefix + langName)",
            "",
            "            return (",
            "                \"<pre><code\"",
            "                + self.renderAttrs(tmpToken)",
            "                + \">\"",
            "                + highlighted",
            "                + \"</code></pre>\\n\"",
            "            )",
            "",
            "        return (",
            "            \"<pre><code\"",
            "            + self.renderAttrs(token)",
            "            + \">\"",
            "            + highlighted",
            "            + \"</code></pre>\\n\"",
            "        )",
            "",
            "    def image(",
            "        self,",
            "        tokens: Sequence[Token],",
            "        idx: int,",
            "        options: OptionsDict,",
            "        env: MutableMapping,",
            "    ) -> str:",
            "        token = tokens[idx]",
            "",
            "        # \"alt\" attr MUST be set, even if empty. Because it's mandatory and",
            "        # should be placed on proper position for tests.",
            "        if token.children:",
            "            token.attrSet(\"alt\", self.renderInlineAsText(token.children, options, env))",
            "        else:",
            "            token.attrSet(\"alt\", \"\")",
            "",
            "        return self.renderToken(tokens, idx, options, env)",
            "",
            "    def hardbreak(",
            "        self, tokens: Sequence[Token], idx: int, options: OptionsDict, *args",
            "    ) -> str:",
            "        return \"<br />\\n\" if options.xhtmlOut else \"<br>\\n\"",
            "",
            "    def softbreak(",
            "        self, tokens: Sequence[Token], idx: int, options: OptionsDict, *args",
            "    ) -> str:",
            "        return (",
            "            (\"<br />\\n\" if options.xhtmlOut else \"<br>\\n\") if options.breaks else \"\\n\"",
            "        )",
            "",
            "    def text(self, tokens: Sequence[Token], idx: int, *args) -> str:",
            "        return escapeHtml(tokens[idx].content)",
            "",
            "    def html_block(self, tokens: Sequence[Token], idx: int, *args) -> str:",
            "        return tokens[idx].content",
            "",
            "    def html_inline(self, tokens: Sequence[Token], idx: int, *args) -> str:",
            "        return tokens[idx].content"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "86": [
                "RendererHTML",
                "render"
            ],
            "87": [
                "RendererHTML",
                "render"
            ],
            "209": [
                "RendererHTML",
                "renderInlineAsText"
            ],
            "210": [
                "RendererHTML",
                "renderInlineAsText"
            ],
            "308": [
                "RendererHTML",
                "image"
            ],
            "309": [
                "RendererHTML",
                "image"
            ],
            "310": [
                "RendererHTML",
                "image"
            ],
            "311": [
                "RendererHTML",
                "image"
            ],
            "312": [
                "RendererHTML",
                "image"
            ],
            "313": [
                "RendererHTML",
                "image"
            ],
            "314": [
                "RendererHTML",
                "image"
            ],
            "315": [
                "RendererHTML",
                "image"
            ]
        },
        "addLocation": []
    },
    "markdown_it/rules_core/replacements.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "     for token in state.tokens:"
            },
            "1": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "         if token.type != \"inline\":"
            },
            "2": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "             continue"
            },
            "3": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        assert token.children is not None"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+        if token.children is None:"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+            continue"
            },
            "6": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 121,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 122,
                "PatchRowcode": "         if SCOPED_ABBR_RE.search(token.content):"
            },
            "8": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 123,
                "PatchRowcode": "             replace_scoped(token.children)"
            }
        },
        "frontPatchFile": [
            "\"\"\"Simple typographic replacements",
            "",
            "* ``(c)``, ``(C)`` \u2192 \u00a9",
            "* ``(tm)``, ``(TM)`` \u2192 \u2122",
            "* ``(r)``, ``(R)`` \u2192 \u00ae",
            "* ``(p)``, ``(P)`` \u2192 \u00a7",
            "* ``+-`` \u2192 \u00b1",
            "* ``...`` \u2192 \u2026",
            "* ``?....`` \u2192 ?..",
            "* ``!....`` \u2192 !..",
            "* ``????????`` \u2192 ???",
            "* ``!!!!!`` \u2192 !!!",
            "* ``,,,`` \u2192 ,",
            "* ``--`` \u2192 &ndash",
            "* ``---`` \u2192 &mdash",
            "\"\"\"",
            "from __future__ import annotations",
            "",
            "import logging",
            "import re",
            "",
            "from ..token import Token",
            "from .state_core import StateCore",
            "",
            "LOGGER = logging.getLogger(__name__)",
            "",
            "# TODO:",
            "# - fractionals 1/2, 1/4, 3/4 -> \u00bd, \u00bc, \u00be",
            "# - miltiplication 2 x 4 -> 2 \u00d7 4",
            "",
            "RARE_RE = re.compile(r\"\\+-|\\.\\.|\\?\\?\\?\\?|!!!!|,,|--\")",
            "",
            "# Workaround for phantomjs - need regex without /g flag,",
            "# or root check will fail every second time",
            "# SCOPED_ABBR_TEST_RE = r\"\\((c|tm|r|p)\\)\"",
            "",
            "SCOPED_ABBR_RE = re.compile(r\"\\((c|tm|r|p)\\)\", flags=re.IGNORECASE)",
            "",
            "PLUS_MINUS_RE = re.compile(r\"\\+-\")",
            "",
            "ELLIPSIS_RE = re.compile(r\"\\.{2,}\")",
            "",
            "ELLIPSIS_QUESTION_EXCLAMATION_RE = re.compile(r\"([?!])\u2026\")",
            "",
            "QUESTION_EXCLAMATION_RE = re.compile(r\"([?!]){4,}\")",
            "",
            "COMMA_RE = re.compile(r\",{2,}\")",
            "",
            "EM_DASH_RE = re.compile(r\"(^|[^-])---(?=[^-]|$)\", flags=re.MULTILINE)",
            "",
            "EN_DASH_RE = re.compile(r\"(^|\\s)--(?=\\s|$)\", flags=re.MULTILINE)",
            "",
            "EN_DASH_INDENT_RE = re.compile(r\"(^|[^-\\s])--(?=[^-\\s]|$)\", flags=re.MULTILINE)",
            "",
            "",
            "SCOPED_ABBR = {\"c\": \"\u00a9\", \"r\": \"\u00ae\", \"p\": \"\u00a7\", \"tm\": \"\u2122\"}",
            "",
            "",
            "def replaceFn(match: re.Match[str]):",
            "    return SCOPED_ABBR[match.group(1).lower()]",
            "",
            "",
            "def replace_scoped(inlineTokens: list[Token]) -> None:",
            "    inside_autolink = 0",
            "",
            "    for token in inlineTokens:",
            "        if token.type == \"text\" and not inside_autolink:",
            "            token.content = SCOPED_ABBR_RE.sub(replaceFn, token.content)",
            "",
            "        if token.type == \"link_open\" and token.info == \"auto\":",
            "            inside_autolink -= 1",
            "",
            "        if token.type == \"link_close\" and token.info == \"auto\":",
            "            inside_autolink += 1",
            "",
            "",
            "def replace_rare(inlineTokens: list[Token]) -> None:",
            "    inside_autolink = 0",
            "",
            "    for token in inlineTokens:",
            "        if token.type == \"text\" and not inside_autolink:",
            "            if RARE_RE.search(token.content):",
            "                # +- -> \u00b1",
            "                token.content = PLUS_MINUS_RE.sub(\"\u00b1\", token.content)",
            "",
            "                # .., ..., ....... -> \u2026",
            "                token.content = ELLIPSIS_RE.sub(\"\u2026\", token.content)",
            "",
            "                # but ?..... & !..... -> ?.. & !..",
            "                token.content = ELLIPSIS_QUESTION_EXCLAMATION_RE.sub(",
            "                    \"\\\\1..\", token.content",
            "                )",
            "                token.content = QUESTION_EXCLAMATION_RE.sub(\"\\\\1\\\\1\\\\1\", token.content)",
            "",
            "                # ,,  ,,,  ,,,, -> ,",
            "                token.content = COMMA_RE.sub(\",\", token.content)",
            "",
            "                # em-dash",
            "                token.content = EM_DASH_RE.sub(\"\\\\1\\u2014\", token.content)",
            "",
            "                # en-dash",
            "                token.content = EN_DASH_RE.sub(\"\\\\1\\u2013\", token.content)",
            "                token.content = EN_DASH_INDENT_RE.sub(\"\\\\1\\u2013\", token.content)",
            "",
            "        if token.type == \"link_open\" and token.info == \"auto\":",
            "            inside_autolink -= 1",
            "",
            "        if token.type == \"link_close\" and token.info == \"auto\":",
            "            inside_autolink += 1",
            "",
            "",
            "def replace(state: StateCore) -> None:",
            "    if not state.md.options.typographer:",
            "        return",
            "",
            "    for token in state.tokens:",
            "        if token.type != \"inline\":",
            "            continue",
            "        assert token.children is not None",
            "",
            "        if SCOPED_ABBR_RE.search(token.content):",
            "            replace_scoped(token.children)",
            "",
            "        if RARE_RE.search(token.content):",
            "            replace_rare(token.children)"
        ],
        "afterPatchFile": [
            "\"\"\"Simple typographic replacements",
            "",
            "* ``(c)``, ``(C)`` \u2192 \u00a9",
            "* ``(tm)``, ``(TM)`` \u2192 \u2122",
            "* ``(r)``, ``(R)`` \u2192 \u00ae",
            "* ``(p)``, ``(P)`` \u2192 \u00a7",
            "* ``+-`` \u2192 \u00b1",
            "* ``...`` \u2192 \u2026",
            "* ``?....`` \u2192 ?..",
            "* ``!....`` \u2192 !..",
            "* ``????????`` \u2192 ???",
            "* ``!!!!!`` \u2192 !!!",
            "* ``,,,`` \u2192 ,",
            "* ``--`` \u2192 &ndash",
            "* ``---`` \u2192 &mdash",
            "\"\"\"",
            "from __future__ import annotations",
            "",
            "import logging",
            "import re",
            "",
            "from ..token import Token",
            "from .state_core import StateCore",
            "",
            "LOGGER = logging.getLogger(__name__)",
            "",
            "# TODO:",
            "# - fractionals 1/2, 1/4, 3/4 -> \u00bd, \u00bc, \u00be",
            "# - miltiplication 2 x 4 -> 2 \u00d7 4",
            "",
            "RARE_RE = re.compile(r\"\\+-|\\.\\.|\\?\\?\\?\\?|!!!!|,,|--\")",
            "",
            "# Workaround for phantomjs - need regex without /g flag,",
            "# or root check will fail every second time",
            "# SCOPED_ABBR_TEST_RE = r\"\\((c|tm|r|p)\\)\"",
            "",
            "SCOPED_ABBR_RE = re.compile(r\"\\((c|tm|r|p)\\)\", flags=re.IGNORECASE)",
            "",
            "PLUS_MINUS_RE = re.compile(r\"\\+-\")",
            "",
            "ELLIPSIS_RE = re.compile(r\"\\.{2,}\")",
            "",
            "ELLIPSIS_QUESTION_EXCLAMATION_RE = re.compile(r\"([?!])\u2026\")",
            "",
            "QUESTION_EXCLAMATION_RE = re.compile(r\"([?!]){4,}\")",
            "",
            "COMMA_RE = re.compile(r\",{2,}\")",
            "",
            "EM_DASH_RE = re.compile(r\"(^|[^-])---(?=[^-]|$)\", flags=re.MULTILINE)",
            "",
            "EN_DASH_RE = re.compile(r\"(^|\\s)--(?=\\s|$)\", flags=re.MULTILINE)",
            "",
            "EN_DASH_INDENT_RE = re.compile(r\"(^|[^-\\s])--(?=[^-\\s]|$)\", flags=re.MULTILINE)",
            "",
            "",
            "SCOPED_ABBR = {\"c\": \"\u00a9\", \"r\": \"\u00ae\", \"p\": \"\u00a7\", \"tm\": \"\u2122\"}",
            "",
            "",
            "def replaceFn(match: re.Match[str]):",
            "    return SCOPED_ABBR[match.group(1).lower()]",
            "",
            "",
            "def replace_scoped(inlineTokens: list[Token]) -> None:",
            "    inside_autolink = 0",
            "",
            "    for token in inlineTokens:",
            "        if token.type == \"text\" and not inside_autolink:",
            "            token.content = SCOPED_ABBR_RE.sub(replaceFn, token.content)",
            "",
            "        if token.type == \"link_open\" and token.info == \"auto\":",
            "            inside_autolink -= 1",
            "",
            "        if token.type == \"link_close\" and token.info == \"auto\":",
            "            inside_autolink += 1",
            "",
            "",
            "def replace_rare(inlineTokens: list[Token]) -> None:",
            "    inside_autolink = 0",
            "",
            "    for token in inlineTokens:",
            "        if token.type == \"text\" and not inside_autolink:",
            "            if RARE_RE.search(token.content):",
            "                # +- -> \u00b1",
            "                token.content = PLUS_MINUS_RE.sub(\"\u00b1\", token.content)",
            "",
            "                # .., ..., ....... -> \u2026",
            "                token.content = ELLIPSIS_RE.sub(\"\u2026\", token.content)",
            "",
            "                # but ?..... & !..... -> ?.. & !..",
            "                token.content = ELLIPSIS_QUESTION_EXCLAMATION_RE.sub(",
            "                    \"\\\\1..\", token.content",
            "                )",
            "                token.content = QUESTION_EXCLAMATION_RE.sub(\"\\\\1\\\\1\\\\1\", token.content)",
            "",
            "                # ,,  ,,,  ,,,, -> ,",
            "                token.content = COMMA_RE.sub(\",\", token.content)",
            "",
            "                # em-dash",
            "                token.content = EM_DASH_RE.sub(\"\\\\1\\u2014\", token.content)",
            "",
            "                # en-dash",
            "                token.content = EN_DASH_RE.sub(\"\\\\1\\u2013\", token.content)",
            "                token.content = EN_DASH_INDENT_RE.sub(\"\\\\1\\u2013\", token.content)",
            "",
            "        if token.type == \"link_open\" and token.info == \"auto\":",
            "            inside_autolink -= 1",
            "",
            "        if token.type == \"link_close\" and token.info == \"auto\":",
            "            inside_autolink += 1",
            "",
            "",
            "def replace(state: StateCore) -> None:",
            "    if not state.md.options.typographer:",
            "        return",
            "",
            "    for token in state.tokens:",
            "        if token.type != \"inline\":",
            "            continue",
            "        if token.children is None:",
            "            continue",
            "",
            "        if SCOPED_ABBR_RE.search(token.content):",
            "            replace_scoped(token.children)",
            "",
            "        if RARE_RE.search(token.content):",
            "            replace_rare(token.children)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "119": [
                "replace"
            ]
        },
        "addLocation": []
    },
    "markdown_it/rules_core/smartquotes.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 197,
                "afterPatchRowNumber": 197,
                "PatchRowcode": "     for token in state.tokens:"
            },
            "1": {
                "beforePatchRowNumber": 198,
                "afterPatchRowNumber": 198,
                "PatchRowcode": "         if token.type != \"inline\" or not QUOTE_RE.search(token.content):"
            },
            "2": {
                "beforePatchRowNumber": 199,
                "afterPatchRowNumber": 199,
                "PatchRowcode": "             continue"
            },
            "3": {
                "beforePatchRowNumber": 200,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        assert token.children is not None"
            },
            "4": {
                "beforePatchRowNumber": 201,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        process_inlines(token.children, state)"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+        if token.children is not None:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+            process_inlines(token.children, state)"
            }
        },
        "frontPatchFile": [
            "\"\"\"Convert straight quotation marks to typographic ones",
            "\"\"\"",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any",
            "",
            "from ..common.utils import charCodeAt, isMdAsciiPunct, isPunctChar, isWhiteSpace",
            "from ..token import Token",
            "from .state_core import StateCore",
            "",
            "QUOTE_TEST_RE = re.compile(r\"['\\\"]\")",
            "QUOTE_RE = re.compile(r\"['\\\"]\")",
            "APOSTROPHE = \"\\u2019\"  # \u2019",
            "",
            "",
            "def replaceAt(string: str, index: int, ch: str) -> str:",
            "    # When the index is negative, the behavior is different from the js version.",
            "    # But basically, the index will not be negative.",
            "    assert index >= 0",
            "    return string[:index] + ch + string[index + 1 :]",
            "",
            "",
            "def process_inlines(tokens: list[Token], state: StateCore) -> None:",
            "    stack: list[dict[str, Any]] = []",
            "",
            "    for i in range(len(tokens)):",
            "        token = tokens[i]",
            "",
            "        thisLevel = token.level",
            "",
            "        j = 0",
            "        for j in range(len(stack))[::-1]:",
            "            if stack[j][\"level\"] <= thisLevel:",
            "                break",
            "        else:",
            "            # When the loop is terminated without a \"break\".",
            "            # Subtract 1 to get the same index as the js version.",
            "            j -= 1",
            "",
            "        stack = stack[: j + 1]",
            "",
            "        if token.type != \"text\":",
            "            continue",
            "",
            "        text = token.content",
            "        pos = 0",
            "        maximum = len(text)",
            "",
            "        while pos < maximum:",
            "            goto_outer = False",
            "            lastIndex = pos",
            "            t = QUOTE_RE.search(text[lastIndex:])",
            "            if not t:",
            "                break",
            "",
            "            canOpen = canClose = True",
            "            pos = t.start(0) + lastIndex + 1",
            "            isSingle = t.group(0) == \"'\"",
            "",
            "            # Find previous character,",
            "            # default to space if it's the beginning of the line",
            "            lastChar = 0x20",
            "",
            "            if t.start(0) + lastIndex - 1 >= 0:",
            "                lastChar = charCodeAt(text, t.start(0) + lastIndex - 1)",
            "            else:",
            "                for j in range(i)[::-1]:",
            "                    # lastChar defaults to 0x20",
            "                    if tokens[j].type == \"softbreak\" or tokens[j].type == \"hardbreak\":",
            "                        break",
            "                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'",
            "                    if not tokens[j].content:",
            "                        continue",
            "",
            "                    lastChar = charCodeAt(tokens[j].content, len(tokens[j].content) - 1)",
            "                    break",
            "",
            "            # Find next character,",
            "            # default to space if it's the end of the line",
            "            nextChar = 0x20",
            "",
            "            if pos < maximum:",
            "                nextChar = charCodeAt(text, pos)",
            "            else:",
            "                for j in range(i + 1, len(tokens)):",
            "                    # nextChar defaults to 0x20",
            "                    if tokens[j].type == \"softbreak\" or tokens[j].type == \"hardbreak\":",
            "                        break",
            "                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'",
            "                    if not tokens[j].content:",
            "                        continue",
            "",
            "                    nextChar = charCodeAt(tokens[j].content, 0)",
            "                    break",
            "",
            "            isLastPunctChar = isMdAsciiPunct(lastChar) or isPunctChar(chr(lastChar))",
            "            isNextPunctChar = isMdAsciiPunct(nextChar) or isPunctChar(chr(nextChar))",
            "",
            "            isLastWhiteSpace = isWhiteSpace(lastChar)",
            "            isNextWhiteSpace = isWhiteSpace(nextChar)",
            "",
            "            if isNextWhiteSpace:",
            "                canOpen = False",
            "            elif isNextPunctChar:",
            "                if not (isLastWhiteSpace or isLastPunctChar):",
            "                    canOpen = False",
            "",
            "            if isLastWhiteSpace:",
            "                canClose = False",
            "            elif isLastPunctChar:",
            "                if not (isNextWhiteSpace or isNextPunctChar):",
            "                    canClose = False",
            "",
            "            if nextChar == 0x22 and t.group(0) == '\"':  # 0x22: \"",
            "                if lastChar >= 0x30 and lastChar <= 0x39:  # 0x30: 0, 0x39: 9",
            "                    # special case: 1\"\" - count first quote as an inch",
            "                    canClose = canOpen = False",
            "",
            "            if canOpen and canClose:",
            "                # Replace quotes in the middle of punctuation sequence, but not",
            "                # in the middle of the words, i.e.:",
            "                #",
            "                # 1. foo \" bar \" baz - not replaced",
            "                # 2. foo-\"-bar-\"-baz - replaced",
            "                # 3. foo\"bar\"baz     - not replaced",
            "                canOpen = isLastPunctChar",
            "                canClose = isNextPunctChar",
            "",
            "            if not canOpen and not canClose:",
            "                # middle of word",
            "                if isSingle:",
            "                    token.content = replaceAt(",
            "                        token.content, t.start(0) + lastIndex, APOSTROPHE",
            "                    )",
            "                continue",
            "",
            "            if canClose:",
            "                # this could be a closing quote, rewind the stack to get a match",
            "                for j in range(len(stack))[::-1]:",
            "                    item = stack[j]",
            "                    if stack[j][\"level\"] < thisLevel:",
            "                        break",
            "                    if item[\"single\"] == isSingle and stack[j][\"level\"] == thisLevel:",
            "                        item = stack[j]",
            "",
            "                        if isSingle:",
            "                            openQuote = state.md.options.quotes[2]",
            "                            closeQuote = state.md.options.quotes[3]",
            "                        else:",
            "                            openQuote = state.md.options.quotes[0]",
            "                            closeQuote = state.md.options.quotes[1]",
            "",
            "                        # replace token.content *before* tokens[item.token].content,",
            "                        # because, if they are pointing at the same token, replaceAt",
            "                        # could mess up indices when quote length != 1",
            "                        token.content = replaceAt(",
            "                            token.content, t.start(0) + lastIndex, closeQuote",
            "                        )",
            "                        tokens[item[\"token\"]].content = replaceAt(",
            "                            tokens[item[\"token\"]].content, item[\"pos\"], openQuote",
            "                        )",
            "",
            "                        pos += len(closeQuote) - 1",
            "                        if item[\"token\"] == i:",
            "                            pos += len(openQuote) - 1",
            "",
            "                        text = token.content",
            "                        maximum = len(text)",
            "",
            "                        stack = stack[:j]",
            "                        goto_outer = True",
            "                        break",
            "                if goto_outer:",
            "                    goto_outer = False",
            "                    continue",
            "",
            "            if canOpen:",
            "                stack.append(",
            "                    {",
            "                        \"token\": i,",
            "                        \"pos\": t.start(0) + lastIndex,",
            "                        \"single\": isSingle,",
            "                        \"level\": thisLevel,",
            "                    }",
            "                )",
            "            elif canClose and isSingle:",
            "                token.content = replaceAt(",
            "                    token.content, t.start(0) + lastIndex, APOSTROPHE",
            "                )",
            "",
            "",
            "def smartquotes(state: StateCore) -> None:",
            "    if not state.md.options.typographer:",
            "        return",
            "",
            "    for token in state.tokens:",
            "        if token.type != \"inline\" or not QUOTE_RE.search(token.content):",
            "            continue",
            "        assert token.children is not None",
            "        process_inlines(token.children, state)"
        ],
        "afterPatchFile": [
            "\"\"\"Convert straight quotation marks to typographic ones",
            "\"\"\"",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any",
            "",
            "from ..common.utils import charCodeAt, isMdAsciiPunct, isPunctChar, isWhiteSpace",
            "from ..token import Token",
            "from .state_core import StateCore",
            "",
            "QUOTE_TEST_RE = re.compile(r\"['\\\"]\")",
            "QUOTE_RE = re.compile(r\"['\\\"]\")",
            "APOSTROPHE = \"\\u2019\"  # \u2019",
            "",
            "",
            "def replaceAt(string: str, index: int, ch: str) -> str:",
            "    # When the index is negative, the behavior is different from the js version.",
            "    # But basically, the index will not be negative.",
            "    assert index >= 0",
            "    return string[:index] + ch + string[index + 1 :]",
            "",
            "",
            "def process_inlines(tokens: list[Token], state: StateCore) -> None:",
            "    stack: list[dict[str, Any]] = []",
            "",
            "    for i in range(len(tokens)):",
            "        token = tokens[i]",
            "",
            "        thisLevel = token.level",
            "",
            "        j = 0",
            "        for j in range(len(stack))[::-1]:",
            "            if stack[j][\"level\"] <= thisLevel:",
            "                break",
            "        else:",
            "            # When the loop is terminated without a \"break\".",
            "            # Subtract 1 to get the same index as the js version.",
            "            j -= 1",
            "",
            "        stack = stack[: j + 1]",
            "",
            "        if token.type != \"text\":",
            "            continue",
            "",
            "        text = token.content",
            "        pos = 0",
            "        maximum = len(text)",
            "",
            "        while pos < maximum:",
            "            goto_outer = False",
            "            lastIndex = pos",
            "            t = QUOTE_RE.search(text[lastIndex:])",
            "            if not t:",
            "                break",
            "",
            "            canOpen = canClose = True",
            "            pos = t.start(0) + lastIndex + 1",
            "            isSingle = t.group(0) == \"'\"",
            "",
            "            # Find previous character,",
            "            # default to space if it's the beginning of the line",
            "            lastChar = 0x20",
            "",
            "            if t.start(0) + lastIndex - 1 >= 0:",
            "                lastChar = charCodeAt(text, t.start(0) + lastIndex - 1)",
            "            else:",
            "                for j in range(i)[::-1]:",
            "                    # lastChar defaults to 0x20",
            "                    if tokens[j].type == \"softbreak\" or tokens[j].type == \"hardbreak\":",
            "                        break",
            "                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'",
            "                    if not tokens[j].content:",
            "                        continue",
            "",
            "                    lastChar = charCodeAt(tokens[j].content, len(tokens[j].content) - 1)",
            "                    break",
            "",
            "            # Find next character,",
            "            # default to space if it's the end of the line",
            "            nextChar = 0x20",
            "",
            "            if pos < maximum:",
            "                nextChar = charCodeAt(text, pos)",
            "            else:",
            "                for j in range(i + 1, len(tokens)):",
            "                    # nextChar defaults to 0x20",
            "                    if tokens[j].type == \"softbreak\" or tokens[j].type == \"hardbreak\":",
            "                        break",
            "                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'",
            "                    if not tokens[j].content:",
            "                        continue",
            "",
            "                    nextChar = charCodeAt(tokens[j].content, 0)",
            "                    break",
            "",
            "            isLastPunctChar = isMdAsciiPunct(lastChar) or isPunctChar(chr(lastChar))",
            "            isNextPunctChar = isMdAsciiPunct(nextChar) or isPunctChar(chr(nextChar))",
            "",
            "            isLastWhiteSpace = isWhiteSpace(lastChar)",
            "            isNextWhiteSpace = isWhiteSpace(nextChar)",
            "",
            "            if isNextWhiteSpace:",
            "                canOpen = False",
            "            elif isNextPunctChar:",
            "                if not (isLastWhiteSpace or isLastPunctChar):",
            "                    canOpen = False",
            "",
            "            if isLastWhiteSpace:",
            "                canClose = False",
            "            elif isLastPunctChar:",
            "                if not (isNextWhiteSpace or isNextPunctChar):",
            "                    canClose = False",
            "",
            "            if nextChar == 0x22 and t.group(0) == '\"':  # 0x22: \"",
            "                if lastChar >= 0x30 and lastChar <= 0x39:  # 0x30: 0, 0x39: 9",
            "                    # special case: 1\"\" - count first quote as an inch",
            "                    canClose = canOpen = False",
            "",
            "            if canOpen and canClose:",
            "                # Replace quotes in the middle of punctuation sequence, but not",
            "                # in the middle of the words, i.e.:",
            "                #",
            "                # 1. foo \" bar \" baz - not replaced",
            "                # 2. foo-\"-bar-\"-baz - replaced",
            "                # 3. foo\"bar\"baz     - not replaced",
            "                canOpen = isLastPunctChar",
            "                canClose = isNextPunctChar",
            "",
            "            if not canOpen and not canClose:",
            "                # middle of word",
            "                if isSingle:",
            "                    token.content = replaceAt(",
            "                        token.content, t.start(0) + lastIndex, APOSTROPHE",
            "                    )",
            "                continue",
            "",
            "            if canClose:",
            "                # this could be a closing quote, rewind the stack to get a match",
            "                for j in range(len(stack))[::-1]:",
            "                    item = stack[j]",
            "                    if stack[j][\"level\"] < thisLevel:",
            "                        break",
            "                    if item[\"single\"] == isSingle and stack[j][\"level\"] == thisLevel:",
            "                        item = stack[j]",
            "",
            "                        if isSingle:",
            "                            openQuote = state.md.options.quotes[2]",
            "                            closeQuote = state.md.options.quotes[3]",
            "                        else:",
            "                            openQuote = state.md.options.quotes[0]",
            "                            closeQuote = state.md.options.quotes[1]",
            "",
            "                        # replace token.content *before* tokens[item.token].content,",
            "                        # because, if they are pointing at the same token, replaceAt",
            "                        # could mess up indices when quote length != 1",
            "                        token.content = replaceAt(",
            "                            token.content, t.start(0) + lastIndex, closeQuote",
            "                        )",
            "                        tokens[item[\"token\"]].content = replaceAt(",
            "                            tokens[item[\"token\"]].content, item[\"pos\"], openQuote",
            "                        )",
            "",
            "                        pos += len(closeQuote) - 1",
            "                        if item[\"token\"] == i:",
            "                            pos += len(openQuote) - 1",
            "",
            "                        text = token.content",
            "                        maximum = len(text)",
            "",
            "                        stack = stack[:j]",
            "                        goto_outer = True",
            "                        break",
            "                if goto_outer:",
            "                    goto_outer = False",
            "                    continue",
            "",
            "            if canOpen:",
            "                stack.append(",
            "                    {",
            "                        \"token\": i,",
            "                        \"pos\": t.start(0) + lastIndex,",
            "                        \"single\": isSingle,",
            "                        \"level\": thisLevel,",
            "                    }",
            "                )",
            "            elif canClose and isSingle:",
            "                token.content = replaceAt(",
            "                    token.content, t.start(0) + lastIndex, APOSTROPHE",
            "                )",
            "",
            "",
            "def smartquotes(state: StateCore) -> None:",
            "    if not state.md.options.typographer:",
            "        return",
            "",
            "    for token in state.tokens:",
            "        if token.type != \"inline\" or not QUOTE_RE.search(token.content):",
            "            continue",
            "        if token.children is not None:",
            "            process_inlines(token.children, state)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "200": [
                "smartquotes"
            ],
            "201": [
                "smartquotes"
            ]
        },
        "addLocation": []
    }
}