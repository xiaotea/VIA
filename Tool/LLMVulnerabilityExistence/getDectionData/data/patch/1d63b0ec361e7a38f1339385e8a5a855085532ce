{
    "src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "     is_torch_available,"
            },
            "1": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "     logging,"
            },
            "2": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "     requires_backends,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+    strtobool,"
            },
            "4": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "     torch_only_method,"
            },
            "5": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " )"
            },
            "6": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "             vocab_dict = None"
            },
            "8": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "             if pretrained_vocab_file is not None:"
            },
            "9": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "                 # Priority on pickle files (support PyTorch and TF)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+                if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 217,
                "PatchRowcode": "+                    raise ValueError("
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 218,
                "PatchRowcode": "+                        \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is \""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 219,
                "PatchRowcode": "+                        \"potentially malicious. It's recommended to never unpickle data that could have come from an \""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 220,
                "PatchRowcode": "+                        \"untrusted source, or that could have been tampered with. If you already verified the pickle \""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 221,
                "PatchRowcode": "+                        \"data and decided to use it, you can set the environment variable \""
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+                        \"`TRUST_REMOTE_CODE` to `True` to allow it.\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 223,
                "PatchRowcode": "+                    )"
            },
            "18": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 224,
                "PatchRowcode": "                 with open(pretrained_vocab_file, \"rb\") as f:"
            },
            "19": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 225,
                "PatchRowcode": "                     vocab_dict = pickle.load(f)"
            },
            "20": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 226,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 790,
                "afterPatchRowNumber": 799,
                "PatchRowcode": "         corpus = torch.load(fn_pickle)"
            },
            "22": {
                "beforePatchRowNumber": 791,
                "afterPatchRowNumber": 800,
                "PatchRowcode": "     elif os.path.exists(fn):"
            },
            "23": {
                "beforePatchRowNumber": 792,
                "afterPatchRowNumber": 801,
                "PatchRowcode": "         logger.info(\"Loading cached dataset from pickle...\")"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 802,
                "PatchRowcode": "+        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 803,
                "PatchRowcode": "+            raise ValueError("
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 804,
                "PatchRowcode": "+                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 805,
                "PatchRowcode": "+                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 806,
                "PatchRowcode": "+                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 807,
                "PatchRowcode": "+                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 808,
                "PatchRowcode": "+            )"
            },
            "31": {
                "beforePatchRowNumber": 793,
                "afterPatchRowNumber": 809,
                "PatchRowcode": "         with open(fn, \"rb\") as fp:"
            },
            "32": {
                "beforePatchRowNumber": 794,
                "afterPatchRowNumber": 810,
                "PatchRowcode": "             corpus = pickle.load(fp)"
            },
            "33": {
                "beforePatchRowNumber": 795,
                "afterPatchRowNumber": 811,
                "PatchRowcode": "     else:"
            }
        },
        "frontPatchFile": [
            "# coding=utf-8",
            "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
            "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "\"\"\"",
            " Tokenization classes for Transformer XL model. Adapted from https://github.com/kimiyoung/transformer-xl.",
            "\"\"\"",
            "",
            "",
            "import glob",
            "import os",
            "import pickle",
            "import re",
            "from collections import Counter, OrderedDict",
            "from typing import List, Optional, Tuple",
            "",
            "import numpy as np",
            "",
            "from ....tokenization_utils import PreTrainedTokenizer",
            "from ....utils import (",
            "    cached_file,",
            "    is_sacremoses_available,",
            "    is_torch_available,",
            "    logging,",
            "    requires_backends,",
            "    torch_only_method,",
            ")",
            "",
            "",
            "if is_sacremoses_available():",
            "    import sacremoses as sm",
            "",
            "",
            "if is_torch_available():",
            "    import torch",
            "",
            "",
            "logger = logging.get_logger(__name__)",
            "",
            "VOCAB_FILES_NAMES = {",
            "    \"pretrained_vocab_file\": \"vocab.pkl\",",
            "    \"pretrained_vocab_file_torch\": \"vocab.bin\",",
            "    \"vocab_file\": \"vocab.txt\",",
            "}",
            "",
            "PRETRAINED_VOCAB_FILES_MAP = {",
            "    \"pretrained_vocab_file\": {",
            "        \"transfo-xl-wt103\": \"https://huggingface.co/transfo-xl-wt103/resolve/main/vocab.pkl\",",
            "    }",
            "}",
            "",
            "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {",
            "    \"transfo-xl-wt103\": None,",
            "}",
            "",
            "PRETRAINED_CORPUS_ARCHIVE_MAP = {",
            "    \"transfo-xl-wt103\": \"https://huggingface.co/transfo-xl-wt103/resolve/main/corpus.bin\",",
            "}",
            "CORPUS_NAME = \"corpus.bin\"",
            "",
            "MATCH_NUMBERS = r\"(?<=\\d)[,.](?=\\d)\", r\" @\\g<0>@ \"",
            "DETOKENIZE_NUMBERS = [(r\" @\\,@ \", r\",\"), (r\" @\\.@ \", r\".\")]",
            "",
            "",
            "def tokenize_numbers(text_array: List[str]) -> List[str]:",
            "    \"\"\"",
            "    Splits large comma-separated numbers and floating point values. This is done by replacing commas with ' @,@ ' and",
            "    dots with ' @.@ '.",
            "",
            "    Args:",
            "        text_array: An already tokenized text as list.",
            "",
            "    Returns:",
            "        A list of strings with tokenized numbers.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    >>> tokenize_numbers([\"$\", \"5,000\", \"1.73\", \"m\"])",
            "    ['$', '5', '@,@', '000', '1', '@.@', '73', 'm']",
            "    ```\"\"\"",
            "    tokenized = []",
            "    for i in range(len(text_array)):",
            "        reg, sub = MATCH_NUMBERS",
            "        replaced = re.sub(reg, sub, text_array[i]).split()",
            "        tokenized.extend(replaced)",
            "",
            "    return tokenized",
            "",
            "",
            "def detokenize_numbers(text: str) -> str:",
            "    \"\"\"",
            "    Inverts the operation of *tokenize_numbers*. This is replacing ' @,@ ' and ' @.@' by ',' and '.'.",
            "",
            "    Args:",
            "        text: A string where the number should be detokenized.",
            "",
            "    Returns:",
            "        A detokenized string.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    >>> detokenize_numbers(\"$ 5 @,@ 000 1 @.@ 73 m\")",
            "    '$ 5,000 1.73 m'",
            "    ```\"\"\"",
            "    for reg, sub in DETOKENIZE_NUMBERS:",
            "        text = re.sub(reg, sub, text)",
            "    return text",
            "",
            "",
            "class TransfoXLTokenizer(PreTrainedTokenizer):",
            "    \"\"\"",
            "    Construct a Transformer-XL tokenizer adapted from Vocab class in [the original",
            "    code](https://github.com/kimiyoung/transformer-xl). The Transformer-XL tokenizer is a word-level tokenizer (no",
            "    sub-word tokenization).",
            "",
            "    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to",
            "    this superclass for more information regarding those methods.",
            "",
            "    Args:",
            "        special (`List[str]`, *optional*):",
            "            A list of special tokens (to be treated by the original implementation of this tokenizer).",
            "        min_freq (`int`, *optional*, defaults to 0):",
            "            The minimum number of times a token has to be present in order to be kept in the vocabulary (otherwise it",
            "            will be mapped to `unk_token`).",
            "        max_size (`int`, *optional*):",
            "            The maximum size of the vocabulary. If left unset, it will default to the size of the vocabulary found",
            "            after excluding the tokens according to the `min_freq` rule.",
            "        lower_case (`bool`, *optional*, defaults to `False`):",
            "            Whether or not to lowercase the input when tokenizing.",
            "        delimiter (`str`, *optional*):",
            "            The delimiter used between tokens.",
            "        vocab_file (`str`, *optional*):",
            "            File containing the vocabulary (from the original implementation).",
            "        pretrained_vocab_file (`str`, *optional*):",
            "            File containing the vocabulary as saved with the `save_pretrained()` method.",
            "        never_split (`List[str]`, *optional*):",
            "            List of tokens that should never be split. If no list is specified, will simply use the existing special",
            "            tokens.",
            "        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):",
            "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this",
            "            token instead.",
            "        eos_token (`str`, *optional*, defaults to `\"<eos>\"`):",
            "            The end of sequence token.",
            "        additional_special_tokens (`List[str]`, *optional*, defaults to `['<formula>']`):",
            "            A list of additional special tokens (for the HuggingFace functionality).",
            "        language (`str`, *optional*, defaults to `\"en\"`):",
            "            The language of this tokenizer (used for mose preprocessing).",
            "    \"\"\"",
            "",
            "    vocab_files_names = VOCAB_FILES_NAMES",
            "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP",
            "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES",
            "    model_input_names = [\"input_ids\"]",
            "",
            "    def __init__(",
            "        self,",
            "        special=None,",
            "        min_freq=0,",
            "        max_size=None,",
            "        lower_case=False,",
            "        delimiter=None,",
            "        vocab_file=None,",
            "        pretrained_vocab_file: str = None,",
            "        never_split=None,",
            "        unk_token=\"<unk>\",",
            "        eos_token=\"<eos>\",",
            "        additional_special_tokens=[\"<formula>\"],",
            "        language=\"en\",",
            "        **kwargs,",
            "    ):",
            "        logger.error(",
            "            \"`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. \"",
            "            \"See more details on this model's documentation page: \"",
            "            \"`https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\"",
            "        )",
            "",
            "        requires_backends(self, \"sacremoses\")",
            "        if special is None:",
            "            special = []",
            "        self.counter = Counter()",
            "        self.special = special",
            "        self.min_freq = min_freq",
            "        self.max_size = max_size",
            "        self.lower_case = lower_case",
            "        self.delimiter = delimiter",
            "        self.vocab_file = vocab_file",
            "        self.punctuation_symbols = '!\"#$%&()*+,-./\\\\:;<=>?@[\\\\]^_`{|}~'",
            "        self.punction_without_space_before_pattern = re.compile(rf\"[^\\s][{self.punctuation_symbols}]\")",
            "        self.punctuation_with_space_around_pattern = self._compile_space_around_punctuation_pattern()",
            "        self.language = language",
            "        self.moses_punct_normalizer = sm.MosesPunctNormalizer(language)",
            "        self.moses_tokenizer = sm.MosesTokenizer(language)",
            "        self.moses_detokenizer = sm.MosesDetokenizer(language)",
            "        self.idx2sym = []",
            "        self.sym2idx = OrderedDict()",
            "        # This try... catch... is not beautiful but honestly this tokenizer was not made to be used",
            "        # in a library like ours, at all.",
            "        try:",
            "            vocab_dict = None",
            "            if pretrained_vocab_file is not None:",
            "                # Priority on pickle files (support PyTorch and TF)",
            "                with open(pretrained_vocab_file, \"rb\") as f:",
            "                    vocab_dict = pickle.load(f)",
            "",
            "                # Loading a torch-saved transfo-xl vocab dict with pickle results in an integer",
            "                # Entering this if statement means that we tried to load a torch-saved file with pickle, and we failed.",
            "                # We therefore load it with torch, if it's available.",
            "                if isinstance(vocab_dict, int):",
            "                    if not is_torch_available():",
            "                        raise ImportError(",
            "                            \"Not trying to load dict with PyTorch as you need to install pytorch to load \"",
            "                            \"from a PyTorch pretrained vocabulary, \"",
            "                            \"or activate it with environment variables USE_TORCH=1 and USE_TF=0.\"",
            "                        )",
            "                    vocab_dict = torch.load(pretrained_vocab_file)",
            "",
            "            if vocab_dict is not None:",
            "                for key, value in vocab_dict.items():",
            "                    if key not in self.__dict__ or key in [\"sym2idx\", \"idx2sym\"]:",
            "                        self.__dict__[key] = value",
            "            elif vocab_file is not None:",
            "                self.build_vocab()",
            "",
            "        except Exception as e:",
            "            raise ValueError(",
            "                f\"Unable to parse file {pretrained_vocab_file}. Unknown format. \"",
            "                \"If you tried to load a model saved through TransfoXLTokenizerFast, \"",
            "                \"please note they are not compatible.\"",
            "            ) from e",
            "",
            "        if vocab_file is not None:",
            "            self.build_vocab()",
            "",
            "        super().__init__(",
            "            special=special,",
            "            min_freq=min_freq,",
            "            max_size=max_size,",
            "            lower_case=lower_case,",
            "            delimiter=delimiter,",
            "            vocab_file=vocab_file,",
            "            pretrained_vocab_file=pretrained_vocab_file,",
            "            never_split=never_split,",
            "            unk_token=unk_token,",
            "            eos_token=eos_token,",
            "            additional_special_tokens=additional_special_tokens,",
            "            language=language,",
            "            **kwargs,",
            "        )",
            "",
            "        # these are not required to initialize the parent class as only used when tokenizing.",
            "        if never_split is None:",
            "            never_split = self.all_special_tokens",
            "        self.never_split = never_split",
            "",
            "    @property",
            "    def do_lower_case(self):",
            "        return self.lower_case",
            "",
            "    def _compile_space_around_punctuation_pattern(self):",
            "        look_ahead_for_special_token = f\"(?=[{self.punctuation_symbols}])\"",
            "        look_ahead_to_match_all_except_space = r\"(?=[^\\s])\"",
            "        return re.compile(r\"\" + look_ahead_for_special_token + look_ahead_to_match_all_except_space)",
            "",
            "    def count_file(self, path, verbose=False, add_eos=False):",
            "        if verbose:",
            "            logger.info(f\"counting file {path} ...\")",
            "        assert os.path.exists(path), f\"Input file {path} not found\"",
            "",
            "        sents = []",
            "        with open(path, \"r\", encoding=\"utf-8\") as f:",
            "            for idx, line in enumerate(f):",
            "                if verbose and idx > 0 and idx % 500000 == 0:",
            "                    logger.info(f\"    line {idx}\")",
            "                symbols = self.tokenize(line, add_eos=add_eos)",
            "                self.counter.update(symbols)",
            "                sents.append(symbols)",
            "",
            "        return sents",
            "",
            "    def count_sents(self, sents, verbose=False):",
            "        \"\"\"",
            "        sents : a list of sentences, each a list of tokenized symbols",
            "        \"\"\"",
            "        if verbose:",
            "            logger.info(f\"counting {len(sents)} sents ...\")",
            "        for idx, symbols in enumerate(sents):",
            "            if verbose and idx > 0 and idx % 500000 == 0:",
            "                logger.info(f\"    line {idx}\")",
            "            self.counter.update(symbols)",
            "",
            "    def _build_from_file(self, vocab_file):",
            "        self.idx2sym = []",
            "        self.sym2idx = OrderedDict()",
            "",
            "        with open(vocab_file, \"r\", encoding=\"utf-8\") as f:",
            "            for line in f:",
            "                symb = line.strip().split()[0]",
            "                self.add_symbol(symb)",
            "        if \"<UNK>\" in self.sym2idx:",
            "            self.unk_idx = self.sym2idx[\"<UNK>\"]",
            "        elif \"<unk>\" in self.sym2idx:",
            "            self.unk_idx = self.sym2idx[\"<unk>\"]",
            "        else:",
            "            raise ValueError(\"Token not in vocabulary and no <unk> token in vocabulary for replacement.\")",
            "",
            "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:",
            "        if os.path.isdir(save_directory):",
            "            vocab_file = os.path.join(",
            "                save_directory,",
            "                (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"pretrained_vocab_file\"],",
            "            )",
            "        else:",
            "            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory",
            "        with open(vocab_file, \"wb\") as f:",
            "            pickle.dump(self.__dict__, f)",
            "        return (vocab_file,)",
            "",
            "    def build_vocab(self):",
            "        if self.vocab_file:",
            "            logger.info(f\"building vocab from {self.vocab_file}\")",
            "            self._build_from_file(self.vocab_file)",
            "            logger.info(f\"Final vocab size {len(self.sym2idx)}\")",
            "        else:",
            "            logger.info(f\"building vocab with min_freq={self.min_freq}, max_size={self.max_size}\")",
            "            self.idx2sym = []",
            "            self.sym2idx = OrderedDict()",
            "",
            "            for sym in self.special:",
            "                self.add_special(sym)",
            "",
            "            for sym, cnt in self.counter.most_common(self.max_size):",
            "                if cnt < self.min_freq:",
            "                    break",
            "                self.add_symbol(sym)",
            "",
            "            logger.info(f\"Final vocab size {len(self.sym2idx)} from {len(self.counter)} unique tokens\")",
            "",
            "    @torch_only_method",
            "    def encode_file(self, path, ordered=False, verbose=False, add_eos=True, add_double_eos=False):",
            "        if verbose:",
            "            logger.info(f\"encoding file {path} ...\")",
            "        assert os.path.exists(path), f\"Output file {path} not found\"",
            "        encoded = []",
            "        with open(path, \"r\", encoding=\"utf-8\") as f:",
            "            for idx, line in enumerate(f):",
            "                if verbose and idx > 0 and idx % 500000 == 0:",
            "                    logger.info(f\"    line {idx}\")",
            "                symbols = self.tokenize(line, add_eos=add_eos, add_double_eos=add_double_eos)",
            "                encoded.append(self.convert_to_tensor(symbols))",
            "",
            "        if ordered:",
            "            encoded = torch.cat(encoded)",
            "",
            "        return encoded",
            "",
            "    @torch_only_method",
            "    def encode_sents(self, sents, ordered=False, verbose=False):",
            "        if verbose:",
            "            logger.info(f\"encoding {len(sents)} sents ...\")",
            "        encoded = []",
            "        for idx, symbols in enumerate(sents):",
            "            if verbose and idx > 0 and idx % 500000 == 0:",
            "                logger.info(f\"    line {idx}\")",
            "            encoded.append(self.convert_to_tensor(symbols))",
            "",
            "        if ordered:",
            "            encoded = torch.cat(encoded)",
            "",
            "        return encoded",
            "",
            "    def add_special(self, sym):",
            "        if sym not in self.sym2idx:",
            "            self.idx2sym.append(sym)",
            "            self.sym2idx[sym] = len(self.idx2sym) - 1",
            "            setattr(self, f\"{sym.strip('<>')}_idx\", self.sym2idx[sym])",
            "",
            "    def add_symbol(self, sym):",
            "        if sym not in self.sym2idx:",
            "            self.idx2sym.append(sym)",
            "            self.sym2idx[sym] = len(self.idx2sym) - 1",
            "",
            "    def move_added_token(self, token: str, target_idx: int):",
            "        \"\"\"",
            "        Moves an added token to a specific position in the vocab. This method should be used when resizing an embedding",
            "        layer other than the last one in the `AdaptiveEmbedding` in order to move the token in the tokenizer from the",
            "        default position (at the very end) to the desired one.",
            "",
            "        Args:",
            "            token: The token to move to a specific position in the vocab.",
            "            target_idx: The position where the token should be moved to.",
            "        \"\"\"",
            "        assert token in self.added_tokens_encoder, \"Token which should be moved has to be an added token\"",
            "        assert token not in self.idx2sym, \"Token which should be moved is already in vocab\"",
            "",
            "        # Insert sym into vocab",
            "        self.idx2sym.insert(target_idx, token)",
            "        self.sym2idx[token] = target_idx",
            "",
            "        # Shift following indices in sym2idx",
            "        for idx in range(target_idx + 1, len(self.idx2sym)):",
            "            current_sym = self.idx2sym[idx]",
            "            self.sym2idx[current_sym] = idx",
            "",
            "        # Delete token from added_tokens",
            "        old_index = self._added_tokens_encoder.pop(token)",
            "        self._added_tokens_decoder.pop(old_index)",
            "",
            "    def moses_punct_norm(self, text):",
            "        return self.moses_punct_normalizer.normalize(text)",
            "",
            "    def moses_tokenize(self, text):",
            "        return self.moses_tokenizer.tokenize(",
            "            text, aggressive_dash_splits=True, return_str=False, escape=False, protected_patterns=self.never_split",
            "        )",
            "",
            "    def moses_pipeline(self, text: str) -> List[str]:",
            "        \"\"\"",
            "        Does basic tokenization using [`sacremoses.MosesPunctNormalizer`] and [`sacremoses.MosesTokenizer`] with",
            "        *aggressive_dash_splits=True* (see [`sacremoses.tokenize.MosesTokenizer.tokenize`]). Additionally, large",
            "        comma-separated numbers and floating point values are split. E.g. \"23,000 people are 1.80m tall\" -> \"23 @,@ 000",
            "        people are 1 @.@ 80m tall\"",
            "",
            "        Args:",
            "            text: Text to be tokenize",
            "",
            "        Returns:",
            "            A list of tokenized string",
            "",
            "        Example:",
            "",
            "        ```python",
            "        >>> tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\")",
            "        >>> tokenizer.moses_pipeline(\"23,000 people are 1.80 m tall\")",
            "        ['23', '@,@', '000', 'people', 'are', '1', '@.@', '80', 'm', 'tall']",
            "        ```\"\"\"",
            "        text = self.moses_punct_norm(text)",
            "        text = self.moses_tokenize(text)",
            "        text = tokenize_numbers(text)",
            "        return text",
            "",
            "    def _convert_id_to_token(self, idx):",
            "        \"\"\"Converts an id in a token (BPE) using the vocab.\"\"\"",
            "        assert 0 <= idx < len(self), f\"Index {idx} out of vocabulary range\"",
            "        return self.idx2sym[idx]",
            "",
            "    def _convert_token_to_id(self, sym):",
            "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"",
            "        if sym in self.sym2idx:",
            "            return self.sym2idx[sym]",
            "        else:",
            "            # logger.info(f'encounter unk {sym}')",
            "            # assert '<eos>' not in sym",
            "            if hasattr(self, \"unk_idx\"):",
            "                return self.sym2idx.get(sym, self.unk_idx)",
            "            # Backward compatibility with pre-trained models",
            "            elif \"<unk>\" in self.sym2idx:",
            "                return self.sym2idx[\"<unk>\"]",
            "            elif \"<UNK>\" in self.sym2idx:",
            "                return self.sym2idx[\"<UNK>\"]",
            "            else:",
            "                raise ValueError(\"Token not in vocabulary and no <unk> token in vocabulary for replacement.\")",
            "",
            "    def convert_tokens_to_string(self, tokens):",
            "        \"\"\"",
            "        Converts a sequence of tokens (string) in a single string. Additionally, the split numbers are converted back",
            "        into it's original form.",
            "        \"\"\"",
            "        out_string = self.moses_detokenizer.detokenize(tokens)",
            "        return detokenize_numbers(out_string).strip()",
            "",
            "    @torch_only_method",
            "    def convert_to_tensor(self, symbols):",
            "        return torch.LongTensor(self.convert_tokens_to_ids(symbols))",
            "",
            "    @property",
            "    def vocab_size(self):",
            "        return len(self.idx2sym)",
            "",
            "    def get_vocab(self):",
            "        vocab = self.sym2idx.copy()",
            "        vocab.update(self.added_tokens_encoder)",
            "        return vocab",
            "",
            "    def _tokenize(self, line, add_eos=False, add_double_eos=False):",
            "        line = line.strip()",
            "        # convert to lower case",
            "        if self.lower_case:",
            "            line = line.lower()",
            "",
            "        # empty delimiter '' will evaluate False",
            "        if self.delimiter == \"\":",
            "            symbols = line",
            "        else:",
            "            symbols = self.moses_pipeline(line)",
            "",
            "        if add_double_eos:  # lm1b",
            "            return [\"<S>\"] + symbols + [\"<S>\"]",
            "        elif add_eos:",
            "            return symbols + [\"<eos>\"]",
            "        else:",
            "            return symbols",
            "",
            "",
            "class LMOrderedIterator(object):",
            "    def __init__(self, data, bsz, bptt, device=\"cpu\", ext_len=None):",
            "        \"\"\"",
            "        data -- LongTensor -- the LongTensor is strictly ordered",
            "        \"\"\"",
            "        self.bsz = bsz",
            "        self.bptt = bptt",
            "        self.ext_len = ext_len if ext_len is not None else 0",
            "",
            "        self.device = device",
            "",
            "        # Work out how cleanly we can divide the dataset into bsz parts.",
            "        self.n_step = data.size(0) // bsz",
            "",
            "        # Trim off any extra elements that wouldn't cleanly fit (remainders).",
            "        data = data.narrow(0, 0, self.n_step * bsz)",
            "",
            "        # Evenly divide the data across the bsz batches.",
            "        self.data = data.view(bsz, -1).t().contiguous().to(device)",
            "",
            "        # Number of mini-batches",
            "        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt",
            "",
            "    def get_batch(self, i, bptt=None):",
            "        if bptt is None:",
            "            bptt = self.bptt",
            "        seq_len = min(bptt, self.data.size(0) - 1 - i)",
            "",
            "        end_idx = i + seq_len",
            "        beg_idx = max(0, i - self.ext_len)",
            "",
            "        data = self.data[beg_idx:end_idx]",
            "        target = self.data[i + 1 : i + 1 + seq_len]",
            "",
            "        data_out = data.transpose(0, 1).contiguous().to(self.device)",
            "        target_out = target.transpose(0, 1).contiguous().to(self.device)",
            "",
            "        return data_out, target_out, seq_len",
            "",
            "    def get_fixlen_iter(self, start=0):",
            "        for i in range(start, self.data.size(0) - 1, self.bptt):",
            "            yield self.get_batch(i)",
            "",
            "    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):",
            "        max_len = self.bptt + max_deviation * std",
            "        i = start",
            "        while True:",
            "            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.0",
            "            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))",
            "            data, target, seq_len = self.get_batch(i, bptt)",
            "            i += seq_len",
            "            yield data, target, seq_len",
            "            if i >= self.data.size(0) - 2:",
            "                break",
            "",
            "    def __iter__(self):",
            "        return self.get_fixlen_iter()",
            "",
            "",
            "class LMShuffledIterator(object):",
            "    def __init__(self, data, bsz, bptt, device=\"cpu\", ext_len=None, shuffle=False):",
            "        \"\"\"",
            "        data -- list[LongTensor] -- there is no order among the LongTensors",
            "        \"\"\"",
            "        self.data = data",
            "",
            "        self.bsz = bsz",
            "        self.bptt = bptt",
            "        self.ext_len = ext_len if ext_len is not None else 0",
            "",
            "        self.device = device",
            "        self.shuffle = shuffle",
            "",
            "    def get_sent_stream(self):",
            "        # index iterator",
            "        epoch_indices = np.random.permutation(len(self.data)) if self.shuffle else np.array(range(len(self.data)))",
            "",
            "        # sentence iterator",
            "        for idx in epoch_indices:",
            "            yield self.data[idx]",
            "",
            "    @torch_only_method",
            "    def stream_iterator(self, sent_stream):",
            "        # streams for each data in the batch",
            "        streams = [None] * self.bsz",
            "",
            "        data = torch.LongTensor(self.bptt, self.bsz)",
            "        target = torch.LongTensor(self.bptt, self.bsz)",
            "",
            "        n_retain = 0",
            "",
            "        while True:",
            "            # data   : [n_retain+bptt x bsz]",
            "            # target : [bptt x bsz]",
            "            data[n_retain:].fill_(-1)",
            "            target.fill_(-1)",
            "",
            "            valid_batch = True",
            "",
            "            for i in range(self.bsz):",
            "                n_filled = 0",
            "                try:",
            "                    while n_filled < self.bptt:",
            "                        if streams[i] is None or len(streams[i]) <= 1:",
            "                            streams[i] = next(sent_stream)",
            "                        # number of new tokens to fill in",
            "                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)",
            "                        # first n_retain tokens are retained from last batch",
            "                        data[n_retain + n_filled : n_retain + n_filled + n_new, i] = streams[i][:n_new]",
            "                        target[n_filled : n_filled + n_new, i] = streams[i][1 : n_new + 1]",
            "                        streams[i] = streams[i][n_new:]",
            "                        n_filled += n_new",
            "                except StopIteration:",
            "                    valid_batch = False",
            "                    break",
            "",
            "            if not valid_batch:",
            "                return",
            "",
            "            data_out = data.transpose(0, 1).contiguous().to(self.device)",
            "            target_out = target.transpose(0, 1).contiguous().to(self.device)",
            "",
            "            yield data_out, target_out, self.bptt",
            "",
            "            n_retain = min(data.size(0), self.ext_len)",
            "            if n_retain > 0:",
            "                data[:n_retain] = data[-n_retain:]",
            "            data.resize_(n_retain + self.bptt, data.size(1))",
            "",
            "    def __iter__(self):",
            "        # sent_stream is an iterator",
            "        sent_stream = self.get_sent_stream()",
            "",
            "        for batch in self.stream_iterator(sent_stream):",
            "            yield batch",
            "",
            "",
            "class LMMultiFileIterator(LMShuffledIterator):",
            "    def __init__(self, paths, vocab, bsz, bptt, device=\"cpu\", ext_len=None, shuffle=False):",
            "        self.paths = paths",
            "        self.vocab = vocab",
            "",
            "        self.bsz = bsz",
            "        self.bptt = bptt",
            "        self.ext_len = ext_len if ext_len is not None else 0",
            "",
            "        self.device = device",
            "        self.shuffle = shuffle",
            "",
            "    def get_sent_stream(self, path):",
            "        sents = self.vocab.encode_file(path, add_double_eos=True)",
            "        if self.shuffle:",
            "            np.random.shuffle(sents)",
            "        sent_stream = iter(sents)",
            "",
            "        return sent_stream",
            "",
            "    def __iter__(self):",
            "        if self.shuffle:",
            "            np.random.shuffle(self.paths)",
            "",
            "        for path in self.paths:",
            "            # sent_stream is an iterator",
            "            sent_stream = self.get_sent_stream(path)",
            "            for batch in self.stream_iterator(sent_stream):",
            "                yield batch",
            "",
            "",
            "class TransfoXLCorpus(object):",
            "    @classmethod",
            "    @torch_only_method",
            "    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):",
            "        \"\"\"",
            "        Instantiate a pre-processed corpus.",
            "        \"\"\"",
            "        vocab = TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)",
            "        is_local = os.path.isdir(pretrained_model_name_or_path)",
            "        # redirect to the cache, if necessary",
            "        try:",
            "            resolved_corpus_file = cached_file(pretrained_model_name_or_path, CORPUS_NAME, cache_dir=cache_dir)",
            "        except EnvironmentError:",
            "            logger.error(",
            "                f\"Corpus '{pretrained_model_name_or_path}' was not found in corpus list\"",
            "                f\" ({', '.join(PRETRAINED_CORPUS_ARCHIVE_MAP.keys())}. We assumed '{pretrained_model_name_or_path}'\"",
            "                f\" was a path or url but couldn't find files {CORPUS_NAME} at this path or url.\"",
            "            )",
            "            return None",
            "        if is_local:",
            "            logger.info(f\"loading corpus file {resolved_corpus_file}\")",
            "        else:",
            "            logger.info(f\"loading corpus file {CORPUS_NAME} from cache at {resolved_corpus_file}\")",
            "",
            "        # Instantiate tokenizer.",
            "        corpus = cls(*inputs, **kwargs)",
            "        corpus_dict = torch.load(resolved_corpus_file)",
            "        for key, value in corpus_dict.items():",
            "            corpus.__dict__[key] = value",
            "        corpus.vocab = vocab",
            "        if corpus.train is not None:",
            "            corpus.train = torch.tensor(corpus.train, dtype=torch.long)",
            "        if corpus.valid is not None:",
            "            corpus.valid = torch.tensor(corpus.valid, dtype=torch.long)",
            "        if corpus.test is not None:",
            "            corpus.test = torch.tensor(corpus.test, dtype=torch.long)",
            "        return corpus",
            "",
            "    def __init__(self, *args, **kwargs):",
            "        self.vocab = TransfoXLTokenizer(*args, **kwargs)",
            "        self.dataset = None",
            "        self.train = None",
            "        self.valid = None",
            "        self.test = None",
            "",
            "    def build_corpus(self, path, dataset):",
            "        self.dataset = dataset",
            "",
            "        if self.dataset in [\"ptb\", \"wt2\", \"enwik8\", \"text8\"]:",
            "            self.vocab.count_file(os.path.join(path, \"train.txt\"))",
            "            self.vocab.count_file(os.path.join(path, \"valid.txt\"))",
            "            self.vocab.count_file(os.path.join(path, \"test.txt\"))",
            "        elif self.dataset == \"wt103\":",
            "            self.vocab.count_file(os.path.join(path, \"train.txt\"))",
            "        elif self.dataset == \"lm1b\":",
            "            train_path_pattern = os.path.join(",
            "                path,",
            "                \"1-billion-word-language-modeling-benchmark-r13output\",",
            "                \"training-monolingual.tokenized.shuffled\",",
            "                \"news.en-*\",",
            "            )",
            "            train_paths = glob.glob(train_path_pattern)",
            "            # the vocab will load from file when build_vocab() is called",
            "",
            "        self.vocab.build_vocab()",
            "",
            "        if self.dataset in [\"ptb\", \"wt2\", \"wt103\"]:",
            "            self.train = self.vocab.encode_file(os.path.join(path, \"train.txt\"), ordered=True)",
            "            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=True)",
            "            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=True)",
            "        elif self.dataset in [\"enwik8\", \"text8\"]:",
            "            self.train = self.vocab.encode_file(os.path.join(path, \"train.txt\"), ordered=True, add_eos=False)",
            "            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=True, add_eos=False)",
            "            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=True, add_eos=False)",
            "        elif self.dataset == \"lm1b\":",
            "            self.train = train_paths",
            "            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=False, add_double_eos=True)",
            "            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=False, add_double_eos=True)",
            "",
            "    def get_iterator(self, split, *args, **kwargs):",
            "        if split == \"train\":",
            "            if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:",
            "                data_iter = LMOrderedIterator(self.train, *args, **kwargs)",
            "            elif self.dataset == \"lm1b\":",
            "                kwargs[\"shuffle\"] = True",
            "                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)",
            "        elif split in [\"valid\", \"test\"]:",
            "            data = self.valid if split == \"valid\" else self.test",
            "            if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:",
            "                data_iter = LMOrderedIterator(data, *args, **kwargs)",
            "            elif self.dataset == \"lm1b\":",
            "                data_iter = LMShuffledIterator(data, *args, **kwargs)",
            "        else:",
            "            data_iter = None",
            "            raise ValueError(f\"Split not recognized: {split}\")",
            "",
            "        return data_iter",
            "",
            "",
            "@torch_only_method",
            "def get_lm_corpus(datadir, dataset):",
            "    fn = os.path.join(datadir, \"cache.pt\")",
            "    fn_pickle = os.path.join(datadir, \"cache.pkl\")",
            "    if os.path.exists(fn):",
            "        logger.info(\"Loading cached dataset...\")",
            "        corpus = torch.load(fn_pickle)",
            "    elif os.path.exists(fn):",
            "        logger.info(\"Loading cached dataset from pickle...\")",
            "        with open(fn, \"rb\") as fp:",
            "            corpus = pickle.load(fp)",
            "    else:",
            "        logger.info(f\"Producing dataset {dataset}...\")",
            "        kwargs = {}",
            "        if dataset in [\"wt103\", \"wt2\"]:",
            "            kwargs[\"special\"] = [\"<eos>\"]",
            "            kwargs[\"lower_case\"] = False",
            "        elif dataset == \"ptb\":",
            "            kwargs[\"special\"] = [\"<eos>\"]",
            "            kwargs[\"lower_case\"] = True",
            "        elif dataset == \"lm1b\":",
            "            kwargs[\"special\"] = []",
            "            kwargs[\"lower_case\"] = False",
            "            kwargs[\"vocab_file\"] = os.path.join(datadir, \"1b_word_vocab.txt\")",
            "        elif dataset in [\"enwik8\", \"text8\"]:",
            "            pass",
            "",
            "        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)",
            "        torch.save(corpus, fn)",
            "",
            "    return corpus"
        ],
        "afterPatchFile": [
            "# coding=utf-8",
            "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
            "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "\"\"\"",
            " Tokenization classes for Transformer XL model. Adapted from https://github.com/kimiyoung/transformer-xl.",
            "\"\"\"",
            "",
            "",
            "import glob",
            "import os",
            "import pickle",
            "import re",
            "from collections import Counter, OrderedDict",
            "from typing import List, Optional, Tuple",
            "",
            "import numpy as np",
            "",
            "from ....tokenization_utils import PreTrainedTokenizer",
            "from ....utils import (",
            "    cached_file,",
            "    is_sacremoses_available,",
            "    is_torch_available,",
            "    logging,",
            "    requires_backends,",
            "    strtobool,",
            "    torch_only_method,",
            ")",
            "",
            "",
            "if is_sacremoses_available():",
            "    import sacremoses as sm",
            "",
            "",
            "if is_torch_available():",
            "    import torch",
            "",
            "",
            "logger = logging.get_logger(__name__)",
            "",
            "VOCAB_FILES_NAMES = {",
            "    \"pretrained_vocab_file\": \"vocab.pkl\",",
            "    \"pretrained_vocab_file_torch\": \"vocab.bin\",",
            "    \"vocab_file\": \"vocab.txt\",",
            "}",
            "",
            "PRETRAINED_VOCAB_FILES_MAP = {",
            "    \"pretrained_vocab_file\": {",
            "        \"transfo-xl-wt103\": \"https://huggingface.co/transfo-xl-wt103/resolve/main/vocab.pkl\",",
            "    }",
            "}",
            "",
            "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {",
            "    \"transfo-xl-wt103\": None,",
            "}",
            "",
            "PRETRAINED_CORPUS_ARCHIVE_MAP = {",
            "    \"transfo-xl-wt103\": \"https://huggingface.co/transfo-xl-wt103/resolve/main/corpus.bin\",",
            "}",
            "CORPUS_NAME = \"corpus.bin\"",
            "",
            "MATCH_NUMBERS = r\"(?<=\\d)[,.](?=\\d)\", r\" @\\g<0>@ \"",
            "DETOKENIZE_NUMBERS = [(r\" @\\,@ \", r\",\"), (r\" @\\.@ \", r\".\")]",
            "",
            "",
            "def tokenize_numbers(text_array: List[str]) -> List[str]:",
            "    \"\"\"",
            "    Splits large comma-separated numbers and floating point values. This is done by replacing commas with ' @,@ ' and",
            "    dots with ' @.@ '.",
            "",
            "    Args:",
            "        text_array: An already tokenized text as list.",
            "",
            "    Returns:",
            "        A list of strings with tokenized numbers.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    >>> tokenize_numbers([\"$\", \"5,000\", \"1.73\", \"m\"])",
            "    ['$', '5', '@,@', '000', '1', '@.@', '73', 'm']",
            "    ```\"\"\"",
            "    tokenized = []",
            "    for i in range(len(text_array)):",
            "        reg, sub = MATCH_NUMBERS",
            "        replaced = re.sub(reg, sub, text_array[i]).split()",
            "        tokenized.extend(replaced)",
            "",
            "    return tokenized",
            "",
            "",
            "def detokenize_numbers(text: str) -> str:",
            "    \"\"\"",
            "    Inverts the operation of *tokenize_numbers*. This is replacing ' @,@ ' and ' @.@' by ',' and '.'.",
            "",
            "    Args:",
            "        text: A string where the number should be detokenized.",
            "",
            "    Returns:",
            "        A detokenized string.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    >>> detokenize_numbers(\"$ 5 @,@ 000 1 @.@ 73 m\")",
            "    '$ 5,000 1.73 m'",
            "    ```\"\"\"",
            "    for reg, sub in DETOKENIZE_NUMBERS:",
            "        text = re.sub(reg, sub, text)",
            "    return text",
            "",
            "",
            "class TransfoXLTokenizer(PreTrainedTokenizer):",
            "    \"\"\"",
            "    Construct a Transformer-XL tokenizer adapted from Vocab class in [the original",
            "    code](https://github.com/kimiyoung/transformer-xl). The Transformer-XL tokenizer is a word-level tokenizer (no",
            "    sub-word tokenization).",
            "",
            "    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to",
            "    this superclass for more information regarding those methods.",
            "",
            "    Args:",
            "        special (`List[str]`, *optional*):",
            "            A list of special tokens (to be treated by the original implementation of this tokenizer).",
            "        min_freq (`int`, *optional*, defaults to 0):",
            "            The minimum number of times a token has to be present in order to be kept in the vocabulary (otherwise it",
            "            will be mapped to `unk_token`).",
            "        max_size (`int`, *optional*):",
            "            The maximum size of the vocabulary. If left unset, it will default to the size of the vocabulary found",
            "            after excluding the tokens according to the `min_freq` rule.",
            "        lower_case (`bool`, *optional*, defaults to `False`):",
            "            Whether or not to lowercase the input when tokenizing.",
            "        delimiter (`str`, *optional*):",
            "            The delimiter used between tokens.",
            "        vocab_file (`str`, *optional*):",
            "            File containing the vocabulary (from the original implementation).",
            "        pretrained_vocab_file (`str`, *optional*):",
            "            File containing the vocabulary as saved with the `save_pretrained()` method.",
            "        never_split (`List[str]`, *optional*):",
            "            List of tokens that should never be split. If no list is specified, will simply use the existing special",
            "            tokens.",
            "        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):",
            "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this",
            "            token instead.",
            "        eos_token (`str`, *optional*, defaults to `\"<eos>\"`):",
            "            The end of sequence token.",
            "        additional_special_tokens (`List[str]`, *optional*, defaults to `['<formula>']`):",
            "            A list of additional special tokens (for the HuggingFace functionality).",
            "        language (`str`, *optional*, defaults to `\"en\"`):",
            "            The language of this tokenizer (used for mose preprocessing).",
            "    \"\"\"",
            "",
            "    vocab_files_names = VOCAB_FILES_NAMES",
            "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP",
            "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES",
            "    model_input_names = [\"input_ids\"]",
            "",
            "    def __init__(",
            "        self,",
            "        special=None,",
            "        min_freq=0,",
            "        max_size=None,",
            "        lower_case=False,",
            "        delimiter=None,",
            "        vocab_file=None,",
            "        pretrained_vocab_file: str = None,",
            "        never_split=None,",
            "        unk_token=\"<unk>\",",
            "        eos_token=\"<eos>\",",
            "        additional_special_tokens=[\"<formula>\"],",
            "        language=\"en\",",
            "        **kwargs,",
            "    ):",
            "        logger.error(",
            "            \"`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. \"",
            "            \"See more details on this model's documentation page: \"",
            "            \"`https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\"",
            "        )",
            "",
            "        requires_backends(self, \"sacremoses\")",
            "        if special is None:",
            "            special = []",
            "        self.counter = Counter()",
            "        self.special = special",
            "        self.min_freq = min_freq",
            "        self.max_size = max_size",
            "        self.lower_case = lower_case",
            "        self.delimiter = delimiter",
            "        self.vocab_file = vocab_file",
            "        self.punctuation_symbols = '!\"#$%&()*+,-./\\\\:;<=>?@[\\\\]^_`{|}~'",
            "        self.punction_without_space_before_pattern = re.compile(rf\"[^\\s][{self.punctuation_symbols}]\")",
            "        self.punctuation_with_space_around_pattern = self._compile_space_around_punctuation_pattern()",
            "        self.language = language",
            "        self.moses_punct_normalizer = sm.MosesPunctNormalizer(language)",
            "        self.moses_tokenizer = sm.MosesTokenizer(language)",
            "        self.moses_detokenizer = sm.MosesDetokenizer(language)",
            "        self.idx2sym = []",
            "        self.sym2idx = OrderedDict()",
            "        # This try... catch... is not beautiful but honestly this tokenizer was not made to be used",
            "        # in a library like ours, at all.",
            "        try:",
            "            vocab_dict = None",
            "            if pretrained_vocab_file is not None:",
            "                # Priority on pickle files (support PyTorch and TF)",
            "                if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):",
            "                    raise ValueError(",
            "                        \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is \"",
            "                        \"potentially malicious. It's recommended to never unpickle data that could have come from an \"",
            "                        \"untrusted source, or that could have been tampered with. If you already verified the pickle \"",
            "                        \"data and decided to use it, you can set the environment variable \"",
            "                        \"`TRUST_REMOTE_CODE` to `True` to allow it.\"",
            "                    )",
            "                with open(pretrained_vocab_file, \"rb\") as f:",
            "                    vocab_dict = pickle.load(f)",
            "",
            "                # Loading a torch-saved transfo-xl vocab dict with pickle results in an integer",
            "                # Entering this if statement means that we tried to load a torch-saved file with pickle, and we failed.",
            "                # We therefore load it with torch, if it's available.",
            "                if isinstance(vocab_dict, int):",
            "                    if not is_torch_available():",
            "                        raise ImportError(",
            "                            \"Not trying to load dict with PyTorch as you need to install pytorch to load \"",
            "                            \"from a PyTorch pretrained vocabulary, \"",
            "                            \"or activate it with environment variables USE_TORCH=1 and USE_TF=0.\"",
            "                        )",
            "                    vocab_dict = torch.load(pretrained_vocab_file)",
            "",
            "            if vocab_dict is not None:",
            "                for key, value in vocab_dict.items():",
            "                    if key not in self.__dict__ or key in [\"sym2idx\", \"idx2sym\"]:",
            "                        self.__dict__[key] = value",
            "            elif vocab_file is not None:",
            "                self.build_vocab()",
            "",
            "        except Exception as e:",
            "            raise ValueError(",
            "                f\"Unable to parse file {pretrained_vocab_file}. Unknown format. \"",
            "                \"If you tried to load a model saved through TransfoXLTokenizerFast, \"",
            "                \"please note they are not compatible.\"",
            "            ) from e",
            "",
            "        if vocab_file is not None:",
            "            self.build_vocab()",
            "",
            "        super().__init__(",
            "            special=special,",
            "            min_freq=min_freq,",
            "            max_size=max_size,",
            "            lower_case=lower_case,",
            "            delimiter=delimiter,",
            "            vocab_file=vocab_file,",
            "            pretrained_vocab_file=pretrained_vocab_file,",
            "            never_split=never_split,",
            "            unk_token=unk_token,",
            "            eos_token=eos_token,",
            "            additional_special_tokens=additional_special_tokens,",
            "            language=language,",
            "            **kwargs,",
            "        )",
            "",
            "        # these are not required to initialize the parent class as only used when tokenizing.",
            "        if never_split is None:",
            "            never_split = self.all_special_tokens",
            "        self.never_split = never_split",
            "",
            "    @property",
            "    def do_lower_case(self):",
            "        return self.lower_case",
            "",
            "    def _compile_space_around_punctuation_pattern(self):",
            "        look_ahead_for_special_token = f\"(?=[{self.punctuation_symbols}])\"",
            "        look_ahead_to_match_all_except_space = r\"(?=[^\\s])\"",
            "        return re.compile(r\"\" + look_ahead_for_special_token + look_ahead_to_match_all_except_space)",
            "",
            "    def count_file(self, path, verbose=False, add_eos=False):",
            "        if verbose:",
            "            logger.info(f\"counting file {path} ...\")",
            "        assert os.path.exists(path), f\"Input file {path} not found\"",
            "",
            "        sents = []",
            "        with open(path, \"r\", encoding=\"utf-8\") as f:",
            "            for idx, line in enumerate(f):",
            "                if verbose and idx > 0 and idx % 500000 == 0:",
            "                    logger.info(f\"    line {idx}\")",
            "                symbols = self.tokenize(line, add_eos=add_eos)",
            "                self.counter.update(symbols)",
            "                sents.append(symbols)",
            "",
            "        return sents",
            "",
            "    def count_sents(self, sents, verbose=False):",
            "        \"\"\"",
            "        sents : a list of sentences, each a list of tokenized symbols",
            "        \"\"\"",
            "        if verbose:",
            "            logger.info(f\"counting {len(sents)} sents ...\")",
            "        for idx, symbols in enumerate(sents):",
            "            if verbose and idx > 0 and idx % 500000 == 0:",
            "                logger.info(f\"    line {idx}\")",
            "            self.counter.update(symbols)",
            "",
            "    def _build_from_file(self, vocab_file):",
            "        self.idx2sym = []",
            "        self.sym2idx = OrderedDict()",
            "",
            "        with open(vocab_file, \"r\", encoding=\"utf-8\") as f:",
            "            for line in f:",
            "                symb = line.strip().split()[0]",
            "                self.add_symbol(symb)",
            "        if \"<UNK>\" in self.sym2idx:",
            "            self.unk_idx = self.sym2idx[\"<UNK>\"]",
            "        elif \"<unk>\" in self.sym2idx:",
            "            self.unk_idx = self.sym2idx[\"<unk>\"]",
            "        else:",
            "            raise ValueError(\"Token not in vocabulary and no <unk> token in vocabulary for replacement.\")",
            "",
            "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:",
            "        if os.path.isdir(save_directory):",
            "            vocab_file = os.path.join(",
            "                save_directory,",
            "                (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"pretrained_vocab_file\"],",
            "            )",
            "        else:",
            "            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory",
            "        with open(vocab_file, \"wb\") as f:",
            "            pickle.dump(self.__dict__, f)",
            "        return (vocab_file,)",
            "",
            "    def build_vocab(self):",
            "        if self.vocab_file:",
            "            logger.info(f\"building vocab from {self.vocab_file}\")",
            "            self._build_from_file(self.vocab_file)",
            "            logger.info(f\"Final vocab size {len(self.sym2idx)}\")",
            "        else:",
            "            logger.info(f\"building vocab with min_freq={self.min_freq}, max_size={self.max_size}\")",
            "            self.idx2sym = []",
            "            self.sym2idx = OrderedDict()",
            "",
            "            for sym in self.special:",
            "                self.add_special(sym)",
            "",
            "            for sym, cnt in self.counter.most_common(self.max_size):",
            "                if cnt < self.min_freq:",
            "                    break",
            "                self.add_symbol(sym)",
            "",
            "            logger.info(f\"Final vocab size {len(self.sym2idx)} from {len(self.counter)} unique tokens\")",
            "",
            "    @torch_only_method",
            "    def encode_file(self, path, ordered=False, verbose=False, add_eos=True, add_double_eos=False):",
            "        if verbose:",
            "            logger.info(f\"encoding file {path} ...\")",
            "        assert os.path.exists(path), f\"Output file {path} not found\"",
            "        encoded = []",
            "        with open(path, \"r\", encoding=\"utf-8\") as f:",
            "            for idx, line in enumerate(f):",
            "                if verbose and idx > 0 and idx % 500000 == 0:",
            "                    logger.info(f\"    line {idx}\")",
            "                symbols = self.tokenize(line, add_eos=add_eos, add_double_eos=add_double_eos)",
            "                encoded.append(self.convert_to_tensor(symbols))",
            "",
            "        if ordered:",
            "            encoded = torch.cat(encoded)",
            "",
            "        return encoded",
            "",
            "    @torch_only_method",
            "    def encode_sents(self, sents, ordered=False, verbose=False):",
            "        if verbose:",
            "            logger.info(f\"encoding {len(sents)} sents ...\")",
            "        encoded = []",
            "        for idx, symbols in enumerate(sents):",
            "            if verbose and idx > 0 and idx % 500000 == 0:",
            "                logger.info(f\"    line {idx}\")",
            "            encoded.append(self.convert_to_tensor(symbols))",
            "",
            "        if ordered:",
            "            encoded = torch.cat(encoded)",
            "",
            "        return encoded",
            "",
            "    def add_special(self, sym):",
            "        if sym not in self.sym2idx:",
            "            self.idx2sym.append(sym)",
            "            self.sym2idx[sym] = len(self.idx2sym) - 1",
            "            setattr(self, f\"{sym.strip('<>')}_idx\", self.sym2idx[sym])",
            "",
            "    def add_symbol(self, sym):",
            "        if sym not in self.sym2idx:",
            "            self.idx2sym.append(sym)",
            "            self.sym2idx[sym] = len(self.idx2sym) - 1",
            "",
            "    def move_added_token(self, token: str, target_idx: int):",
            "        \"\"\"",
            "        Moves an added token to a specific position in the vocab. This method should be used when resizing an embedding",
            "        layer other than the last one in the `AdaptiveEmbedding` in order to move the token in the tokenizer from the",
            "        default position (at the very end) to the desired one.",
            "",
            "        Args:",
            "            token: The token to move to a specific position in the vocab.",
            "            target_idx: The position where the token should be moved to.",
            "        \"\"\"",
            "        assert token in self.added_tokens_encoder, \"Token which should be moved has to be an added token\"",
            "        assert token not in self.idx2sym, \"Token which should be moved is already in vocab\"",
            "",
            "        # Insert sym into vocab",
            "        self.idx2sym.insert(target_idx, token)",
            "        self.sym2idx[token] = target_idx",
            "",
            "        # Shift following indices in sym2idx",
            "        for idx in range(target_idx + 1, len(self.idx2sym)):",
            "            current_sym = self.idx2sym[idx]",
            "            self.sym2idx[current_sym] = idx",
            "",
            "        # Delete token from added_tokens",
            "        old_index = self._added_tokens_encoder.pop(token)",
            "        self._added_tokens_decoder.pop(old_index)",
            "",
            "    def moses_punct_norm(self, text):",
            "        return self.moses_punct_normalizer.normalize(text)",
            "",
            "    def moses_tokenize(self, text):",
            "        return self.moses_tokenizer.tokenize(",
            "            text, aggressive_dash_splits=True, return_str=False, escape=False, protected_patterns=self.never_split",
            "        )",
            "",
            "    def moses_pipeline(self, text: str) -> List[str]:",
            "        \"\"\"",
            "        Does basic tokenization using [`sacremoses.MosesPunctNormalizer`] and [`sacremoses.MosesTokenizer`] with",
            "        *aggressive_dash_splits=True* (see [`sacremoses.tokenize.MosesTokenizer.tokenize`]). Additionally, large",
            "        comma-separated numbers and floating point values are split. E.g. \"23,000 people are 1.80m tall\" -> \"23 @,@ 000",
            "        people are 1 @.@ 80m tall\"",
            "",
            "        Args:",
            "            text: Text to be tokenize",
            "",
            "        Returns:",
            "            A list of tokenized string",
            "",
            "        Example:",
            "",
            "        ```python",
            "        >>> tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\")",
            "        >>> tokenizer.moses_pipeline(\"23,000 people are 1.80 m tall\")",
            "        ['23', '@,@', '000', 'people', 'are', '1', '@.@', '80', 'm', 'tall']",
            "        ```\"\"\"",
            "        text = self.moses_punct_norm(text)",
            "        text = self.moses_tokenize(text)",
            "        text = tokenize_numbers(text)",
            "        return text",
            "",
            "    def _convert_id_to_token(self, idx):",
            "        \"\"\"Converts an id in a token (BPE) using the vocab.\"\"\"",
            "        assert 0 <= idx < len(self), f\"Index {idx} out of vocabulary range\"",
            "        return self.idx2sym[idx]",
            "",
            "    def _convert_token_to_id(self, sym):",
            "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"",
            "        if sym in self.sym2idx:",
            "            return self.sym2idx[sym]",
            "        else:",
            "            # logger.info(f'encounter unk {sym}')",
            "            # assert '<eos>' not in sym",
            "            if hasattr(self, \"unk_idx\"):",
            "                return self.sym2idx.get(sym, self.unk_idx)",
            "            # Backward compatibility with pre-trained models",
            "            elif \"<unk>\" in self.sym2idx:",
            "                return self.sym2idx[\"<unk>\"]",
            "            elif \"<UNK>\" in self.sym2idx:",
            "                return self.sym2idx[\"<UNK>\"]",
            "            else:",
            "                raise ValueError(\"Token not in vocabulary and no <unk> token in vocabulary for replacement.\")",
            "",
            "    def convert_tokens_to_string(self, tokens):",
            "        \"\"\"",
            "        Converts a sequence of tokens (string) in a single string. Additionally, the split numbers are converted back",
            "        into it's original form.",
            "        \"\"\"",
            "        out_string = self.moses_detokenizer.detokenize(tokens)",
            "        return detokenize_numbers(out_string).strip()",
            "",
            "    @torch_only_method",
            "    def convert_to_tensor(self, symbols):",
            "        return torch.LongTensor(self.convert_tokens_to_ids(symbols))",
            "",
            "    @property",
            "    def vocab_size(self):",
            "        return len(self.idx2sym)",
            "",
            "    def get_vocab(self):",
            "        vocab = self.sym2idx.copy()",
            "        vocab.update(self.added_tokens_encoder)",
            "        return vocab",
            "",
            "    def _tokenize(self, line, add_eos=False, add_double_eos=False):",
            "        line = line.strip()",
            "        # convert to lower case",
            "        if self.lower_case:",
            "            line = line.lower()",
            "",
            "        # empty delimiter '' will evaluate False",
            "        if self.delimiter == \"\":",
            "            symbols = line",
            "        else:",
            "            symbols = self.moses_pipeline(line)",
            "",
            "        if add_double_eos:  # lm1b",
            "            return [\"<S>\"] + symbols + [\"<S>\"]",
            "        elif add_eos:",
            "            return symbols + [\"<eos>\"]",
            "        else:",
            "            return symbols",
            "",
            "",
            "class LMOrderedIterator(object):",
            "    def __init__(self, data, bsz, bptt, device=\"cpu\", ext_len=None):",
            "        \"\"\"",
            "        data -- LongTensor -- the LongTensor is strictly ordered",
            "        \"\"\"",
            "        self.bsz = bsz",
            "        self.bptt = bptt",
            "        self.ext_len = ext_len if ext_len is not None else 0",
            "",
            "        self.device = device",
            "",
            "        # Work out how cleanly we can divide the dataset into bsz parts.",
            "        self.n_step = data.size(0) // bsz",
            "",
            "        # Trim off any extra elements that wouldn't cleanly fit (remainders).",
            "        data = data.narrow(0, 0, self.n_step * bsz)",
            "",
            "        # Evenly divide the data across the bsz batches.",
            "        self.data = data.view(bsz, -1).t().contiguous().to(device)",
            "",
            "        # Number of mini-batches",
            "        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt",
            "",
            "    def get_batch(self, i, bptt=None):",
            "        if bptt is None:",
            "            bptt = self.bptt",
            "        seq_len = min(bptt, self.data.size(0) - 1 - i)",
            "",
            "        end_idx = i + seq_len",
            "        beg_idx = max(0, i - self.ext_len)",
            "",
            "        data = self.data[beg_idx:end_idx]",
            "        target = self.data[i + 1 : i + 1 + seq_len]",
            "",
            "        data_out = data.transpose(0, 1).contiguous().to(self.device)",
            "        target_out = target.transpose(0, 1).contiguous().to(self.device)",
            "",
            "        return data_out, target_out, seq_len",
            "",
            "    def get_fixlen_iter(self, start=0):",
            "        for i in range(start, self.data.size(0) - 1, self.bptt):",
            "            yield self.get_batch(i)",
            "",
            "    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):",
            "        max_len = self.bptt + max_deviation * std",
            "        i = start",
            "        while True:",
            "            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.0",
            "            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))",
            "            data, target, seq_len = self.get_batch(i, bptt)",
            "            i += seq_len",
            "            yield data, target, seq_len",
            "            if i >= self.data.size(0) - 2:",
            "                break",
            "",
            "    def __iter__(self):",
            "        return self.get_fixlen_iter()",
            "",
            "",
            "class LMShuffledIterator(object):",
            "    def __init__(self, data, bsz, bptt, device=\"cpu\", ext_len=None, shuffle=False):",
            "        \"\"\"",
            "        data -- list[LongTensor] -- there is no order among the LongTensors",
            "        \"\"\"",
            "        self.data = data",
            "",
            "        self.bsz = bsz",
            "        self.bptt = bptt",
            "        self.ext_len = ext_len if ext_len is not None else 0",
            "",
            "        self.device = device",
            "        self.shuffle = shuffle",
            "",
            "    def get_sent_stream(self):",
            "        # index iterator",
            "        epoch_indices = np.random.permutation(len(self.data)) if self.shuffle else np.array(range(len(self.data)))",
            "",
            "        # sentence iterator",
            "        for idx in epoch_indices:",
            "            yield self.data[idx]",
            "",
            "    @torch_only_method",
            "    def stream_iterator(self, sent_stream):",
            "        # streams for each data in the batch",
            "        streams = [None] * self.bsz",
            "",
            "        data = torch.LongTensor(self.bptt, self.bsz)",
            "        target = torch.LongTensor(self.bptt, self.bsz)",
            "",
            "        n_retain = 0",
            "",
            "        while True:",
            "            # data   : [n_retain+bptt x bsz]",
            "            # target : [bptt x bsz]",
            "            data[n_retain:].fill_(-1)",
            "            target.fill_(-1)",
            "",
            "            valid_batch = True",
            "",
            "            for i in range(self.bsz):",
            "                n_filled = 0",
            "                try:",
            "                    while n_filled < self.bptt:",
            "                        if streams[i] is None or len(streams[i]) <= 1:",
            "                            streams[i] = next(sent_stream)",
            "                        # number of new tokens to fill in",
            "                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)",
            "                        # first n_retain tokens are retained from last batch",
            "                        data[n_retain + n_filled : n_retain + n_filled + n_new, i] = streams[i][:n_new]",
            "                        target[n_filled : n_filled + n_new, i] = streams[i][1 : n_new + 1]",
            "                        streams[i] = streams[i][n_new:]",
            "                        n_filled += n_new",
            "                except StopIteration:",
            "                    valid_batch = False",
            "                    break",
            "",
            "            if not valid_batch:",
            "                return",
            "",
            "            data_out = data.transpose(0, 1).contiguous().to(self.device)",
            "            target_out = target.transpose(0, 1).contiguous().to(self.device)",
            "",
            "            yield data_out, target_out, self.bptt",
            "",
            "            n_retain = min(data.size(0), self.ext_len)",
            "            if n_retain > 0:",
            "                data[:n_retain] = data[-n_retain:]",
            "            data.resize_(n_retain + self.bptt, data.size(1))",
            "",
            "    def __iter__(self):",
            "        # sent_stream is an iterator",
            "        sent_stream = self.get_sent_stream()",
            "",
            "        for batch in self.stream_iterator(sent_stream):",
            "            yield batch",
            "",
            "",
            "class LMMultiFileIterator(LMShuffledIterator):",
            "    def __init__(self, paths, vocab, bsz, bptt, device=\"cpu\", ext_len=None, shuffle=False):",
            "        self.paths = paths",
            "        self.vocab = vocab",
            "",
            "        self.bsz = bsz",
            "        self.bptt = bptt",
            "        self.ext_len = ext_len if ext_len is not None else 0",
            "",
            "        self.device = device",
            "        self.shuffle = shuffle",
            "",
            "    def get_sent_stream(self, path):",
            "        sents = self.vocab.encode_file(path, add_double_eos=True)",
            "        if self.shuffle:",
            "            np.random.shuffle(sents)",
            "        sent_stream = iter(sents)",
            "",
            "        return sent_stream",
            "",
            "    def __iter__(self):",
            "        if self.shuffle:",
            "            np.random.shuffle(self.paths)",
            "",
            "        for path in self.paths:",
            "            # sent_stream is an iterator",
            "            sent_stream = self.get_sent_stream(path)",
            "            for batch in self.stream_iterator(sent_stream):",
            "                yield batch",
            "",
            "",
            "class TransfoXLCorpus(object):",
            "    @classmethod",
            "    @torch_only_method",
            "    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):",
            "        \"\"\"",
            "        Instantiate a pre-processed corpus.",
            "        \"\"\"",
            "        vocab = TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)",
            "        is_local = os.path.isdir(pretrained_model_name_or_path)",
            "        # redirect to the cache, if necessary",
            "        try:",
            "            resolved_corpus_file = cached_file(pretrained_model_name_or_path, CORPUS_NAME, cache_dir=cache_dir)",
            "        except EnvironmentError:",
            "            logger.error(",
            "                f\"Corpus '{pretrained_model_name_or_path}' was not found in corpus list\"",
            "                f\" ({', '.join(PRETRAINED_CORPUS_ARCHIVE_MAP.keys())}. We assumed '{pretrained_model_name_or_path}'\"",
            "                f\" was a path or url but couldn't find files {CORPUS_NAME} at this path or url.\"",
            "            )",
            "            return None",
            "        if is_local:",
            "            logger.info(f\"loading corpus file {resolved_corpus_file}\")",
            "        else:",
            "            logger.info(f\"loading corpus file {CORPUS_NAME} from cache at {resolved_corpus_file}\")",
            "",
            "        # Instantiate tokenizer.",
            "        corpus = cls(*inputs, **kwargs)",
            "        corpus_dict = torch.load(resolved_corpus_file)",
            "        for key, value in corpus_dict.items():",
            "            corpus.__dict__[key] = value",
            "        corpus.vocab = vocab",
            "        if corpus.train is not None:",
            "            corpus.train = torch.tensor(corpus.train, dtype=torch.long)",
            "        if corpus.valid is not None:",
            "            corpus.valid = torch.tensor(corpus.valid, dtype=torch.long)",
            "        if corpus.test is not None:",
            "            corpus.test = torch.tensor(corpus.test, dtype=torch.long)",
            "        return corpus",
            "",
            "    def __init__(self, *args, **kwargs):",
            "        self.vocab = TransfoXLTokenizer(*args, **kwargs)",
            "        self.dataset = None",
            "        self.train = None",
            "        self.valid = None",
            "        self.test = None",
            "",
            "    def build_corpus(self, path, dataset):",
            "        self.dataset = dataset",
            "",
            "        if self.dataset in [\"ptb\", \"wt2\", \"enwik8\", \"text8\"]:",
            "            self.vocab.count_file(os.path.join(path, \"train.txt\"))",
            "            self.vocab.count_file(os.path.join(path, \"valid.txt\"))",
            "            self.vocab.count_file(os.path.join(path, \"test.txt\"))",
            "        elif self.dataset == \"wt103\":",
            "            self.vocab.count_file(os.path.join(path, \"train.txt\"))",
            "        elif self.dataset == \"lm1b\":",
            "            train_path_pattern = os.path.join(",
            "                path,",
            "                \"1-billion-word-language-modeling-benchmark-r13output\",",
            "                \"training-monolingual.tokenized.shuffled\",",
            "                \"news.en-*\",",
            "            )",
            "            train_paths = glob.glob(train_path_pattern)",
            "            # the vocab will load from file when build_vocab() is called",
            "",
            "        self.vocab.build_vocab()",
            "",
            "        if self.dataset in [\"ptb\", \"wt2\", \"wt103\"]:",
            "            self.train = self.vocab.encode_file(os.path.join(path, \"train.txt\"), ordered=True)",
            "            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=True)",
            "            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=True)",
            "        elif self.dataset in [\"enwik8\", \"text8\"]:",
            "            self.train = self.vocab.encode_file(os.path.join(path, \"train.txt\"), ordered=True, add_eos=False)",
            "            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=True, add_eos=False)",
            "            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=True, add_eos=False)",
            "        elif self.dataset == \"lm1b\":",
            "            self.train = train_paths",
            "            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=False, add_double_eos=True)",
            "            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=False, add_double_eos=True)",
            "",
            "    def get_iterator(self, split, *args, **kwargs):",
            "        if split == \"train\":",
            "            if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:",
            "                data_iter = LMOrderedIterator(self.train, *args, **kwargs)",
            "            elif self.dataset == \"lm1b\":",
            "                kwargs[\"shuffle\"] = True",
            "                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)",
            "        elif split in [\"valid\", \"test\"]:",
            "            data = self.valid if split == \"valid\" else self.test",
            "            if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:",
            "                data_iter = LMOrderedIterator(data, *args, **kwargs)",
            "            elif self.dataset == \"lm1b\":",
            "                data_iter = LMShuffledIterator(data, *args, **kwargs)",
            "        else:",
            "            data_iter = None",
            "            raise ValueError(f\"Split not recognized: {split}\")",
            "",
            "        return data_iter",
            "",
            "",
            "@torch_only_method",
            "def get_lm_corpus(datadir, dataset):",
            "    fn = os.path.join(datadir, \"cache.pt\")",
            "    fn_pickle = os.path.join(datadir, \"cache.pkl\")",
            "    if os.path.exists(fn):",
            "        logger.info(\"Loading cached dataset...\")",
            "        corpus = torch.load(fn_pickle)",
            "    elif os.path.exists(fn):",
            "        logger.info(\"Loading cached dataset from pickle...\")",
            "        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):",
            "            raise ValueError(",
            "                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"",
            "                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"",
            "                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"",
            "                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"",
            "            )",
            "        with open(fn, \"rb\") as fp:",
            "            corpus = pickle.load(fp)",
            "    else:",
            "        logger.info(f\"Producing dataset {dataset}...\")",
            "        kwargs = {}",
            "        if dataset in [\"wt103\", \"wt2\"]:",
            "            kwargs[\"special\"] = [\"<eos>\"]",
            "            kwargs[\"lower_case\"] = False",
            "        elif dataset == \"ptb\":",
            "            kwargs[\"special\"] = [\"<eos>\"]",
            "            kwargs[\"lower_case\"] = True",
            "        elif dataset == \"lm1b\":",
            "            kwargs[\"special\"] = []",
            "            kwargs[\"lower_case\"] = False",
            "            kwargs[\"vocab_file\"] = os.path.join(datadir, \"1b_word_vocab.txt\")",
            "        elif dataset in [\"enwik8\", \"text8\"]:",
            "            pass",
            "",
            "        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)",
            "        torch.save(corpus, fn)",
            "",
            "    return corpus"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.transformers.models.deprecated.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.__init__.special",
            "src.transformers.models.deprecated.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.special",
            "src.transformers.models.deprecated.transfo_xl.tokenization_transfo_xl.TransfoXLCorpus.__init__",
            "src.pyload.core.database.user_database",
            "src.transformers.models.deprecated.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer",
            "src.transformers.models.deprecated.transfo_xl.tokenization_transfo_xl.get_lm_corpus.kwargs",
            "src.transformers.models.deprecated.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.idx2sym"
        ]
    },
    "src/transformers/models/rag/retrieval_rag.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from ...tokenization_utils import PreTrainedTokenizer"
            },
            "2": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from ...tokenization_utils_base import BatchEncoding"
            },
            "3": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from ...utils import cached_file, is_datasets_available, is_faiss_available, logging, requires_backends"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+from ...utils import cached_file, is_datasets_available, is_faiss_available, logging, requires_backends, strtobool"
            },
            "5": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from .configuration_rag import RagConfig"
            },
            "6": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from .tokenization_rag import RagTokenizer"
            },
            "7": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 131,
                "PatchRowcode": "     def _load_passages(self):"
            },
            "9": {
                "beforePatchRowNumber": 132,
                "afterPatchRowNumber": 132,
                "PatchRowcode": "         logger.info(f\"Loading passages from {self.index_path}\")"
            },
            "10": {
                "beforePatchRowNumber": 133,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "         passages_path = self._resolve_path(self.index_path, self.PASSAGE_FILENAME)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+            raise ValueError("
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \""
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+            )"
            },
            "18": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "         with open(passages_path, \"rb\") as passages_file:"
            },
            "19": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": 142,
                "PatchRowcode": "             passages = pickle.load(passages_file)"
            },
            "20": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 143,
                "PatchRowcode": "         return passages"
            },
            "21": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": 147,
                "PatchRowcode": "         resolved_index_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index.dpr\")"
            },
            "22": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": 148,
                "PatchRowcode": "         self.index = faiss.read_index(resolved_index_path)"
            },
            "23": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 149,
                "PatchRowcode": "         resolved_meta_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index_meta.dpr\")"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 150,
                "PatchRowcode": "+        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+            raise ValueError("
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 152,
                "PatchRowcode": "+                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 155,
                "PatchRowcode": "+                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 156,
                "PatchRowcode": "+            )"
            },
            "31": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": 157,
                "PatchRowcode": "         with open(resolved_meta_path, \"rb\") as metadata_file:"
            },
            "32": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 158,
                "PatchRowcode": "             self.index_id_to_db_id = pickle.load(metadata_file)"
            },
            "33": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": 159,
                "PatchRowcode": "         assert ("
            }
        },
        "frontPatchFile": [
            "# coding=utf-8",
            "# Copyright 2020, The RAG Authors and The HuggingFace Inc. team.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "\"\"\"RAG Retriever model implementation.\"\"\"",
            "",
            "import os",
            "import pickle",
            "import time",
            "from typing import Iterable, List, Optional, Tuple",
            "",
            "import numpy as np",
            "",
            "from ...tokenization_utils import PreTrainedTokenizer",
            "from ...tokenization_utils_base import BatchEncoding",
            "from ...utils import cached_file, is_datasets_available, is_faiss_available, logging, requires_backends",
            "from .configuration_rag import RagConfig",
            "from .tokenization_rag import RagTokenizer",
            "",
            "",
            "if is_datasets_available():",
            "    from datasets import Dataset, load_dataset, load_from_disk",
            "",
            "if is_faiss_available():",
            "    import faiss",
            "",
            "",
            "logger = logging.get_logger(__name__)",
            "",
            "",
            "LEGACY_INDEX_PATH = \"https://storage.googleapis.com/huggingface-nlp/datasets/wiki_dpr/\"",
            "",
            "",
            "class Index:",
            "    \"\"\"",
            "    A base class for the Indices encapsulated by the [`RagRetriever`].",
            "    \"\"\"",
            "",
            "    def get_doc_dicts(self, doc_ids: np.ndarray) -> List[dict]:",
            "        \"\"\"",
            "        Returns a list of dictionaries, containing titles and text of the retrieved documents.",
            "",
            "        Args:",
            "            doc_ids (`np.ndarray` of shape `(batch_size, n_docs)`):",
            "                A tensor of document indices.",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:",
            "        \"\"\"",
            "        For each query in the batch, retrieves `n_docs` documents.",
            "",
            "        Args:",
            "            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`):",
            "                An array of query vectors.",
            "            n_docs (`int`):",
            "                The number of docs retrieved per query.",
            "",
            "        Returns:",
            "            `np.ndarray` of shape `(batch_size, n_docs)`: A tensor of indices of retrieved documents. `np.ndarray` of",
            "            shape `(batch_size, vector_size)`: A tensor of vector representations of retrieved documents.",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "    def is_initialized(self):",
            "        \"\"\"",
            "        Returns `True` if index is already initialized.",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "    def init_index(self):",
            "        \"\"\"",
            "        A function responsible for loading the index into memory. Should be called only once per training run of a RAG",
            "        model. E.g. if the model is trained on multiple GPUs in a distributed setup, only one of the workers will load",
            "        the index.",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "",
            "class LegacyIndex(Index):",
            "    \"\"\"",
            "    An index which can be deserialized from the files built using https://github.com/facebookresearch/DPR. We use",
            "    default faiss index parameters as specified in that repository.",
            "",
            "    Args:",
            "        vector_size (`int`):",
            "            The dimension of indexed vectors.",
            "        index_path (`str`):",
            "            A path to a *directory* containing index files compatible with [`~models.rag.retrieval_rag.LegacyIndex`]",
            "    \"\"\"",
            "",
            "    INDEX_FILENAME = \"hf_bert_base.hnswSQ8_correct_phi_128.c_index\"",
            "    PASSAGE_FILENAME = \"psgs_w100.tsv.pkl\"",
            "",
            "    def __init__(self, vector_size, index_path):",
            "        self.index_id_to_db_id = []",
            "        self.index_path = index_path",
            "        self.passages = self._load_passages()",
            "        self.vector_size = vector_size",
            "        self.index = None",
            "        self._index_initialized = False",
            "",
            "    def _resolve_path(self, index_path, filename):",
            "        is_local = os.path.isdir(index_path)",
            "        try:",
            "            # Load from URL or cache if already cached",
            "            resolved_archive_file = cached_file(index_path, filename)",
            "        except EnvironmentError:",
            "            msg = (",
            "                f\"Can't load '{filename}'. Make sure that:\\n\\n\"",
            "                f\"- '{index_path}' is a correct remote path to a directory containing a file named {filename}\\n\\n\"",
            "                f\"- or '{index_path}' is the correct path to a directory containing a file named {filename}.\\n\\n\"",
            "            )",
            "            raise EnvironmentError(msg)",
            "        if is_local:",
            "            logger.info(f\"loading file {resolved_archive_file}\")",
            "        else:",
            "            logger.info(f\"loading file {filename} from cache at {resolved_archive_file}\")",
            "        return resolved_archive_file",
            "",
            "    def _load_passages(self):",
            "        logger.info(f\"Loading passages from {self.index_path}\")",
            "        passages_path = self._resolve_path(self.index_path, self.PASSAGE_FILENAME)",
            "        with open(passages_path, \"rb\") as passages_file:",
            "            passages = pickle.load(passages_file)",
            "        return passages",
            "",
            "    def _deserialize_index(self):",
            "        logger.info(f\"Loading index from {self.index_path}\")",
            "        resolved_index_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index.dpr\")",
            "        self.index = faiss.read_index(resolved_index_path)",
            "        resolved_meta_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index_meta.dpr\")",
            "        with open(resolved_meta_path, \"rb\") as metadata_file:",
            "            self.index_id_to_db_id = pickle.load(metadata_file)",
            "        assert (",
            "            len(self.index_id_to_db_id) == self.index.ntotal",
            "        ), \"Deserialized index_id_to_db_id should match faiss index size\"",
            "",
            "    def is_initialized(self):",
            "        return self._index_initialized",
            "",
            "    def init_index(self):",
            "        index = faiss.IndexHNSWFlat(self.vector_size + 1, 512)",
            "        index.hnsw.efSearch = 128",
            "        index.hnsw.efConstruction = 200",
            "        self.index = index",
            "        self._deserialize_index()",
            "        self._index_initialized = True",
            "",
            "    def get_doc_dicts(self, doc_ids: np.array):",
            "        doc_list = []",
            "        for doc_ids_i in doc_ids:",
            "            ids = [str(int(doc_id)) for doc_id in doc_ids_i]",
            "            docs = [self.passages[doc_id] for doc_id in ids]",
            "            doc_list.append(docs)",
            "        doc_dicts = []",
            "        for docs in doc_list:",
            "            doc_dict = {}",
            "            doc_dict[\"title\"] = [doc[1] for doc in docs]",
            "            doc_dict[\"text\"] = [doc[0] for doc in docs]",
            "            doc_dicts.append(doc_dict)",
            "        return doc_dicts",
            "",
            "    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:",
            "        aux_dim = np.zeros(len(question_hidden_states), dtype=\"float32\").reshape(-1, 1)",
            "        query_nhsw_vectors = np.hstack((question_hidden_states, aux_dim))",
            "        _, docs_ids = self.index.search(query_nhsw_vectors, n_docs)",
            "        vectors = [[self.index.reconstruct(int(doc_id))[:-1] for doc_id in doc_ids] for doc_ids in docs_ids]",
            "        ids = [[int(self.index_id_to_db_id[doc_id]) for doc_id in doc_ids] for doc_ids in docs_ids]",
            "        return np.array(ids), np.array(vectors)",
            "",
            "",
            "class HFIndexBase(Index):",
            "    def __init__(self, vector_size, dataset, index_initialized=False):",
            "        self.vector_size = vector_size",
            "        self.dataset = dataset",
            "        self._index_initialized = index_initialized",
            "        self._check_dataset_format(with_index=index_initialized)",
            "        dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True, dtype=\"float32\")",
            "",
            "    def _check_dataset_format(self, with_index: bool):",
            "        if not isinstance(self.dataset, Dataset):",
            "            raise ValueError(f\"Dataset should be a datasets.Dataset object, but got {type(self.dataset)}\")",
            "        if len({\"title\", \"text\", \"embeddings\"} - set(self.dataset.column_names)) > 0:",
            "            raise ValueError(",
            "                \"Dataset should be a dataset with the following columns: \"",
            "                \"title (str), text (str) and embeddings (arrays of dimension vector_size), \"",
            "                f\"but got columns {self.dataset.column_names}\"",
            "            )",
            "        if with_index and \"embeddings\" not in self.dataset.list_indexes():",
            "            raise ValueError(",
            "                \"Missing faiss index in the dataset. Make sure you called `dataset.add_faiss_index` to compute it \"",
            "                \"or `dataset.load_faiss_index` to load one from the disk.\"",
            "            )",
            "",
            "    def init_index(self):",
            "        raise NotImplementedError()",
            "",
            "    def is_initialized(self):",
            "        return self._index_initialized",
            "",
            "    def get_doc_dicts(self, doc_ids: np.ndarray) -> List[dict]:",
            "        return [self.dataset[doc_ids[i].tolist()] for i in range(doc_ids.shape[0])]",
            "",
            "    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:",
            "        _, ids = self.dataset.search_batch(\"embeddings\", question_hidden_states, n_docs)",
            "        docs = [self.dataset[[i for i in indices if i >= 0]] for indices in ids]",
            "        vectors = [doc[\"embeddings\"] for doc in docs]",
            "        for i in range(len(vectors)):",
            "            if len(vectors[i]) < n_docs:",
            "                vectors[i] = np.vstack([vectors[i], np.zeros((n_docs - len(vectors[i]), self.vector_size))])",
            "        return np.array(ids), np.array(vectors)  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)",
            "",
            "",
            "class CanonicalHFIndex(HFIndexBase):",
            "    \"\"\"",
            "    A wrapper around an instance of [`~datasets.Datasets`]. If `index_path` is set to `None`, we load the pre-computed",
            "    index available with the [`~datasets.arrow_dataset.Dataset`], otherwise, we load the index from the indicated path",
            "    on disk.",
            "",
            "    Args:",
            "        vector_size (`int`): the dimension of the passages embeddings used by the index",
            "        dataset_name (`str`, optional, defaults to `wiki_dpr`):",
            "            A dataset identifier of the indexed dataset on HuggingFace AWS bucket (list all available datasets and ids",
            "            with `datasets.list_datasets()`).",
            "        dataset_split (`str`, optional, defaults to `train`)",
            "            Which split of the `dataset` to load.",
            "        index_name (`str`, optional, defaults to `train`)",
            "            The index_name of the index associated with the `dataset`. The index loaded from `index_path` will be saved",
            "            under this name.",
            "        index_path (`str`, optional, defaults to `None`)",
            "            The path to the serialized faiss index on disk.",
            "        use_dummy_dataset (`bool`, optional, defaults to `False`):",
            "            If True, use the dummy configuration of the dataset for tests.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        vector_size: int,",
            "        dataset_name: str = \"wiki_dpr\",",
            "        dataset_split: str = \"train\",",
            "        index_name: Optional[str] = None,",
            "        index_path: Optional[str] = None,",
            "        use_dummy_dataset=False,",
            "    ):",
            "        if int(index_path is None) + int(index_name is None) != 1:",
            "            raise ValueError(\"Please provide `index_name` or `index_path`.\")",
            "        self.dataset_name = dataset_name",
            "        self.dataset_split = dataset_split",
            "        self.index_name = index_name",
            "        self.index_path = index_path",
            "        self.use_dummy_dataset = use_dummy_dataset",
            "        logger.info(f\"Loading passages from {self.dataset_name}\")",
            "        dataset = load_dataset(",
            "            self.dataset_name, with_index=False, split=self.dataset_split, dummy=self.use_dummy_dataset",
            "        )",
            "        super().__init__(vector_size, dataset, index_initialized=False)",
            "",
            "    def init_index(self):",
            "        if self.index_path is not None:",
            "            logger.info(f\"Loading index from {self.index_path}\")",
            "            self.dataset.load_faiss_index(\"embeddings\", file=self.index_path)",
            "        else:",
            "            logger.info(f\"Loading index from {self.dataset_name} with index name {self.index_name}\")",
            "            self.dataset = load_dataset(",
            "                self.dataset_name,",
            "                with_embeddings=True,",
            "                with_index=True,",
            "                split=self.dataset_split,",
            "                index_name=self.index_name,",
            "                dummy=self.use_dummy_dataset,",
            "            )",
            "            self.dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True)",
            "        self._index_initialized = True",
            "",
            "",
            "class CustomHFIndex(HFIndexBase):",
            "    \"\"\"",
            "    A wrapper around an instance of [`~datasets.Datasets`]. The dataset and the index are both loaded from the",
            "    indicated paths on disk.",
            "",
            "    Args:",
            "        vector_size (`int`): the dimension of the passages embeddings used by the index",
            "        dataset_path (`str`):",
            "            The path to the serialized dataset on disk. The dataset should have 3 columns: title (str), text (str) and",
            "            embeddings (arrays of dimension vector_size)",
            "        index_path (`str`)",
            "            The path to the serialized faiss index on disk.",
            "    \"\"\"",
            "",
            "    def __init__(self, vector_size: int, dataset, index_path=None):",
            "        super().__init__(vector_size, dataset, index_initialized=index_path is None)",
            "        self.index_path = index_path",
            "",
            "    @classmethod",
            "    def load_from_disk(cls, vector_size, dataset_path, index_path):",
            "        logger.info(f\"Loading passages from {dataset_path}\")",
            "        if dataset_path is None or index_path is None:",
            "            raise ValueError(",
            "                \"Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` \"",
            "                \"and `dataset.get_index('embeddings').save(index_path)`.\"",
            "            )",
            "        dataset = load_from_disk(dataset_path)",
            "        return cls(vector_size=vector_size, dataset=dataset, index_path=index_path)",
            "",
            "    def init_index(self):",
            "        if not self.is_initialized():",
            "            logger.info(f\"Loading index from {self.index_path}\")",
            "            self.dataset.load_faiss_index(\"embeddings\", file=self.index_path)",
            "            self._index_initialized = True",
            "",
            "",
            "class RagRetriever:",
            "    \"\"\"",
            "    Retriever used to get documents from vector queries. It retrieves the documents embeddings as well as the documents",
            "    contents, and it formats them to be used with a RagModel.",
            "",
            "    Args:",
            "        config ([`RagConfig`]):",
            "            The configuration of the RAG model this Retriever is used with. Contains parameters indicating which",
            "            `Index` to build. You can load your own custom dataset with `config.index_name=\"custom\"` or use a canonical",
            "            one (default) from the datasets library with `config.index_name=\"wiki_dpr\"` for example.",
            "        question_encoder_tokenizer ([`PreTrainedTokenizer`]):",
            "            The tokenizer that was used to tokenize the question. It is used to decode the question and then use the",
            "            generator_tokenizer.",
            "        generator_tokenizer ([`PreTrainedTokenizer`]):",
            "            The tokenizer used for the generator part of the RagModel.",
            "        index ([`~models.rag.retrieval_rag.Index`], optional, defaults to the one defined by the configuration):",
            "            If specified, use this index instead of the one built using the configuration",
            "",
            "    Examples:",
            "",
            "    ```python",
            "    >>> # To load the default \"wiki_dpr\" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')",
            "    >>> from transformers import RagRetriever",
            "",
            "    >>> retriever = RagRetriever.from_pretrained(",
            "    ...     \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"",
            "    ... )",
            "",
            "    >>> # To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py",
            "    >>> from transformers import RagRetriever",
            "",
            "    >>> dataset = (",
            "    ...     ...",
            "    ... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index",
            "    >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)",
            "",
            "    >>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py",
            "    >>> from transformers import RagRetriever",
            "",
            "    >>> dataset_path = \"path/to/my/dataset\"  # dataset saved via *dataset.save_to_disk(...)*",
            "    >>> index_path = \"path/to/my/index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*",
            "    >>> retriever = RagRetriever.from_pretrained(",
            "    ...     \"facebook/dpr-ctx_encoder-single-nq-base\",",
            "    ...     index_name=\"custom\",",
            "    ...     passages_path=dataset_path,",
            "    ...     index_path=index_path,",
            "    ... )",
            "",
            "    >>> # To load the legacy index built originally for Rag's paper",
            "    >>> from transformers import RagRetriever",
            "",
            "    >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", index_name=\"legacy\")",
            "    ```\"\"\"",
            "",
            "    def __init__(self, config, question_encoder_tokenizer, generator_tokenizer, index=None, init_retrieval=True):",
            "        self._init_retrieval = init_retrieval",
            "        requires_backends(self, [\"datasets\", \"faiss\"])",
            "        super().__init__()",
            "        self.index = index or self._build_index(config)",
            "        self.generator_tokenizer = generator_tokenizer",
            "        self.question_encoder_tokenizer = question_encoder_tokenizer",
            "",
            "        self.n_docs = config.n_docs",
            "        self.batch_size = config.retrieval_batch_size",
            "",
            "        self.config = config",
            "        if self._init_retrieval:",
            "            self.init_retrieval()",
            "",
            "        self.ctx_encoder_tokenizer = None",
            "        self.return_tokenized_docs = False",
            "",
            "    @staticmethod",
            "    def _build_index(config):",
            "        if config.index_name == \"legacy\":",
            "            return LegacyIndex(",
            "                config.retrieval_vector_size,",
            "                config.index_path or LEGACY_INDEX_PATH,",
            "            )",
            "        elif config.index_name == \"custom\":",
            "            return CustomHFIndex.load_from_disk(",
            "                vector_size=config.retrieval_vector_size,",
            "                dataset_path=config.passages_path,",
            "                index_path=config.index_path,",
            "            )",
            "        else:",
            "            return CanonicalHFIndex(",
            "                vector_size=config.retrieval_vector_size,",
            "                dataset_name=config.dataset,",
            "                dataset_split=config.dataset_split,",
            "                index_name=config.index_name,",
            "                index_path=config.index_path,",
            "                use_dummy_dataset=config.use_dummy_dataset,",
            "            )",
            "",
            "    @classmethod",
            "    def from_pretrained(cls, retriever_name_or_path, indexed_dataset=None, **kwargs):",
            "        requires_backends(cls, [\"datasets\", \"faiss\"])",
            "        config = kwargs.pop(\"config\", None) or RagConfig.from_pretrained(retriever_name_or_path, **kwargs)",
            "        rag_tokenizer = RagTokenizer.from_pretrained(retriever_name_or_path, config=config)",
            "        question_encoder_tokenizer = rag_tokenizer.question_encoder",
            "        generator_tokenizer = rag_tokenizer.generator",
            "        if indexed_dataset is not None:",
            "            config.index_name = \"custom\"",
            "            index = CustomHFIndex(config.retrieval_vector_size, indexed_dataset)",
            "        else:",
            "            index = cls._build_index(config)",
            "        return cls(",
            "            config,",
            "            question_encoder_tokenizer=question_encoder_tokenizer,",
            "            generator_tokenizer=generator_tokenizer,",
            "            index=index,",
            "        )",
            "",
            "    def save_pretrained(self, save_directory):",
            "        if isinstance(self.index, CustomHFIndex):",
            "            if self.config.index_path is None:",
            "                index_path = os.path.join(save_directory, \"hf_dataset_index.faiss\")",
            "                self.index.dataset.get_index(\"embeddings\").save(index_path)",
            "                self.config.index_path = index_path",
            "            if self.config.passages_path is None:",
            "                passages_path = os.path.join(save_directory, \"hf_dataset\")",
            "                # datasets don't support save_to_disk with indexes right now",
            "                faiss_index = self.index.dataset._indexes.pop(\"embeddings\")",
            "                self.index.dataset.save_to_disk(passages_path)",
            "                self.index.dataset._indexes[\"embeddings\"] = faiss_index",
            "                self.config.passages_path = passages_path",
            "        self.config.save_pretrained(save_directory)",
            "        rag_tokenizer = RagTokenizer(",
            "            question_encoder=self.question_encoder_tokenizer,",
            "            generator=self.generator_tokenizer,",
            "        )",
            "        rag_tokenizer.save_pretrained(save_directory)",
            "",
            "    def init_retrieval(self):",
            "        \"\"\"",
            "        Retriever initialization function. It loads the index into memory.",
            "        \"\"\"",
            "",
            "        logger.info(\"initializing retrieval\")",
            "        self.index.init_index()",
            "",
            "    def postprocess_docs(self, docs, input_strings, prefix, n_docs, return_tensors=None):",
            "        r\"\"\"",
            "        Postprocessing retrieved `docs` and combining them with `input_strings`.",
            "",
            "        Args:",
            "            docs  (`dict`):",
            "                Retrieved documents.",
            "            input_strings (`str`):",
            "                Input strings decoded by `preprocess_query`.",
            "            prefix (`str`):",
            "                Prefix added at the beginning of each input, typically used with T5-based models.",
            "",
            "        Return:",
            "            `tuple(tensors)`: a tuple consisting of two elements: contextualized `input_ids` and a compatible",
            "            `attention_mask`.",
            "        \"\"\"",
            "",
            "        def cat_input_and_doc(doc_title, doc_text, input_string, prefix):",
            "            # TODO(Patrick): if we train more RAG models, I want to put the input first to take advantage of effortless truncation",
            "            # TODO(piktus): better handling of truncation",
            "            if doc_title.startswith('\"'):",
            "                doc_title = doc_title[1:]",
            "            if doc_title.endswith('\"'):",
            "                doc_title = doc_title[:-1]",
            "            if prefix is None:",
            "                prefix = \"\"",
            "            out = (prefix + doc_title + self.config.title_sep + doc_text + self.config.doc_sep + input_string).replace(",
            "                \"  \", \" \"",
            "            )",
            "            return out",
            "",
            "        rag_input_strings = [",
            "            cat_input_and_doc(",
            "                docs[i][\"title\"][j],",
            "                docs[i][\"text\"][j],",
            "                input_strings[i],",
            "                prefix,",
            "            )",
            "            for i in range(len(docs))",
            "            for j in range(n_docs)",
            "        ]",
            "",
            "        contextualized_inputs = self.generator_tokenizer.batch_encode_plus(",
            "            rag_input_strings,",
            "            max_length=self.config.max_combined_length,",
            "            return_tensors=return_tensors,",
            "            padding=\"max_length\",",
            "            truncation=True,",
            "        )",
            "",
            "        return contextualized_inputs[\"input_ids\"], contextualized_inputs[\"attention_mask\"]",
            "",
            "    def _chunk_tensor(self, t: Iterable, chunk_size: int) -> List[Iterable]:",
            "        return [t[i : i + chunk_size] for i in range(0, len(t), chunk_size)]",
            "",
            "    def _main_retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, np.ndarray]:",
            "        question_hidden_states_batched = self._chunk_tensor(question_hidden_states, self.batch_size)",
            "        ids_batched = []",
            "        vectors_batched = []",
            "        for question_hidden_states in question_hidden_states_batched:",
            "            start_time = time.time()",
            "            ids, vectors = self.index.get_top_docs(question_hidden_states, n_docs)",
            "            logger.debug(",
            "                f\"index search time: {time.time() - start_time} sec, batch size {question_hidden_states.shape}\"",
            "            )",
            "            ids_batched.extend(ids)",
            "            vectors_batched.extend(vectors)",
            "        return (",
            "            np.array(ids_batched),",
            "            np.array(vectors_batched),",
            "        )  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)",
            "",
            "    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, List[dict]]:",
            "        \"\"\"",
            "        Retrieves documents for specified `question_hidden_states`.",
            "",
            "        Args:",
            "            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`):",
            "                A batch of query vectors to retrieve with.",
            "            n_docs (`int`):",
            "                The number of docs retrieved per query.",
            "",
            "        Return:",
            "            `Tuple[np.ndarray, np.ndarray, List[dict]]`: A tuple with the following objects:",
            "",
            "            - **retrieved_doc_embeds** (`np.ndarray` of shape `(batch_size, n_docs, dim)`) -- The retrieval embeddings",
            "              of the retrieved docs per query.",
            "            - **doc_ids** (`np.ndarray` of shape `(batch_size, n_docs)`) -- The ids of the documents in the index",
            "            - **doc_dicts** (`List[dict]`): The `retrieved_doc_embeds` examples per query.",
            "        \"\"\"",
            "",
            "        doc_ids, retrieved_doc_embeds = self._main_retrieve(question_hidden_states, n_docs)",
            "        return retrieved_doc_embeds, doc_ids, self.index.get_doc_dicts(doc_ids)",
            "",
            "    def set_ctx_encoder_tokenizer(self, ctx_encoder_tokenizer: PreTrainedTokenizer):",
            "        # used in end2end retriever training",
            "        self.ctx_encoder_tokenizer = ctx_encoder_tokenizer",
            "        self.return_tokenized_docs = True",
            "",
            "    def __call__(",
            "        self,",
            "        question_input_ids: List[List[int]],",
            "        question_hidden_states: np.ndarray,",
            "        prefix=None,",
            "        n_docs=None,",
            "        return_tensors=None,",
            "    ) -> BatchEncoding:",
            "        \"\"\"",
            "        Retrieves documents for specified `question_hidden_states`.",
            "",
            "        Args:",
            "            question_input_ids (`List[List[int]]`) batch of input ids",
            "            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`:",
            "                A batch of query vectors to retrieve with.",
            "            prefix (`str`, *optional*):",
            "                The prefix used by the generator's tokenizer.",
            "            n_docs (`int`, *optional*):",
            "                The number of docs retrieved per query.",
            "            return_tensors (`str` or [`~utils.TensorType`], *optional*, defaults to \"pt\"):",
            "                If set, will return tensors instead of list of python integers. Acceptable values are:",
            "",
            "                - `'tf'`: Return TensorFlow `tf.constant` objects.",
            "                - `'pt'`: Return PyTorch `torch.Tensor` objects.",
            "                - `'np'`: Return Numpy `np.ndarray` objects.",
            "",
            "        Returns: [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:",
            "",
            "            - **context_input_ids** -- List of token ids to be fed to a model.",
            "",
            "              [What are input IDs?](../glossary#input-ids)",
            "",
            "            - **context_attention_mask** -- List of indices specifying which tokens should be attended to by the model",
            "            (when `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).",
            "",
            "              [What are attention masks?](../glossary#attention-mask)",
            "",
            "            - **retrieved_doc_embeds** -- List of embeddings of the retrieved documents",
            "            - **doc_ids** -- List of ids of the retrieved documents",
            "        \"\"\"",
            "",
            "        n_docs = n_docs if n_docs is not None else self.n_docs",
            "        prefix = prefix if prefix is not None else self.config.generator.prefix",
            "        retrieved_doc_embeds, doc_ids, docs = self.retrieve(question_hidden_states, n_docs)",
            "",
            "        input_strings = self.question_encoder_tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)",
            "        context_input_ids, context_attention_mask = self.postprocess_docs(",
            "            docs, input_strings, prefix, n_docs, return_tensors=return_tensors",
            "        )",
            "",
            "        if self.return_tokenized_docs:",
            "            retrieved_doc_text = []",
            "            retrieved_doc_title = []",
            "",
            "            for b_idx in range(len(docs)):",
            "                for doc_idx in range(n_docs):",
            "                    retrieved_doc_text.append(docs[b_idx][\"text\"][doc_idx])",
            "                    retrieved_doc_title.append(docs[b_idx][\"title\"][doc_idx])",
            "",
            "            tokenized_docs = self.ctx_encoder_tokenizer(",
            "                retrieved_doc_title,",
            "                retrieved_doc_text,",
            "                truncation=True,",
            "                padding=\"longest\",",
            "                return_tensors=return_tensors,",
            "            )",
            "",
            "            return BatchEncoding(",
            "                {",
            "                    \"context_input_ids\": context_input_ids,",
            "                    \"context_attention_mask\": context_attention_mask,",
            "                    \"retrieved_doc_embeds\": retrieved_doc_embeds,",
            "                    \"doc_ids\": doc_ids,",
            "                    \"tokenized_doc_ids\": tokenized_docs[\"input_ids\"],",
            "                    \"tokenized_doc_attention_mask\": tokenized_docs[\"attention_mask\"],",
            "                },",
            "                tensor_type=return_tensors,",
            "            )",
            "",
            "        else:",
            "            return BatchEncoding(",
            "                {",
            "                    \"context_input_ids\": context_input_ids,",
            "                    \"context_attention_mask\": context_attention_mask,",
            "                    \"retrieved_doc_embeds\": retrieved_doc_embeds,",
            "                    \"doc_ids\": doc_ids,",
            "                },",
            "                tensor_type=return_tensors,",
            "            )"
        ],
        "afterPatchFile": [
            "# coding=utf-8",
            "# Copyright 2020, The RAG Authors and The HuggingFace Inc. team.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "\"\"\"RAG Retriever model implementation.\"\"\"",
            "",
            "import os",
            "import pickle",
            "import time",
            "from typing import Iterable, List, Optional, Tuple",
            "",
            "import numpy as np",
            "",
            "from ...tokenization_utils import PreTrainedTokenizer",
            "from ...tokenization_utils_base import BatchEncoding",
            "from ...utils import cached_file, is_datasets_available, is_faiss_available, logging, requires_backends, strtobool",
            "from .configuration_rag import RagConfig",
            "from .tokenization_rag import RagTokenizer",
            "",
            "",
            "if is_datasets_available():",
            "    from datasets import Dataset, load_dataset, load_from_disk",
            "",
            "if is_faiss_available():",
            "    import faiss",
            "",
            "",
            "logger = logging.get_logger(__name__)",
            "",
            "",
            "LEGACY_INDEX_PATH = \"https://storage.googleapis.com/huggingface-nlp/datasets/wiki_dpr/\"",
            "",
            "",
            "class Index:",
            "    \"\"\"",
            "    A base class for the Indices encapsulated by the [`RagRetriever`].",
            "    \"\"\"",
            "",
            "    def get_doc_dicts(self, doc_ids: np.ndarray) -> List[dict]:",
            "        \"\"\"",
            "        Returns a list of dictionaries, containing titles and text of the retrieved documents.",
            "",
            "        Args:",
            "            doc_ids (`np.ndarray` of shape `(batch_size, n_docs)`):",
            "                A tensor of document indices.",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:",
            "        \"\"\"",
            "        For each query in the batch, retrieves `n_docs` documents.",
            "",
            "        Args:",
            "            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`):",
            "                An array of query vectors.",
            "            n_docs (`int`):",
            "                The number of docs retrieved per query.",
            "",
            "        Returns:",
            "            `np.ndarray` of shape `(batch_size, n_docs)`: A tensor of indices of retrieved documents. `np.ndarray` of",
            "            shape `(batch_size, vector_size)`: A tensor of vector representations of retrieved documents.",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "    def is_initialized(self):",
            "        \"\"\"",
            "        Returns `True` if index is already initialized.",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "    def init_index(self):",
            "        \"\"\"",
            "        A function responsible for loading the index into memory. Should be called only once per training run of a RAG",
            "        model. E.g. if the model is trained on multiple GPUs in a distributed setup, only one of the workers will load",
            "        the index.",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "",
            "class LegacyIndex(Index):",
            "    \"\"\"",
            "    An index which can be deserialized from the files built using https://github.com/facebookresearch/DPR. We use",
            "    default faiss index parameters as specified in that repository.",
            "",
            "    Args:",
            "        vector_size (`int`):",
            "            The dimension of indexed vectors.",
            "        index_path (`str`):",
            "            A path to a *directory* containing index files compatible with [`~models.rag.retrieval_rag.LegacyIndex`]",
            "    \"\"\"",
            "",
            "    INDEX_FILENAME = \"hf_bert_base.hnswSQ8_correct_phi_128.c_index\"",
            "    PASSAGE_FILENAME = \"psgs_w100.tsv.pkl\"",
            "",
            "    def __init__(self, vector_size, index_path):",
            "        self.index_id_to_db_id = []",
            "        self.index_path = index_path",
            "        self.passages = self._load_passages()",
            "        self.vector_size = vector_size",
            "        self.index = None",
            "        self._index_initialized = False",
            "",
            "    def _resolve_path(self, index_path, filename):",
            "        is_local = os.path.isdir(index_path)",
            "        try:",
            "            # Load from URL or cache if already cached",
            "            resolved_archive_file = cached_file(index_path, filename)",
            "        except EnvironmentError:",
            "            msg = (",
            "                f\"Can't load '{filename}'. Make sure that:\\n\\n\"",
            "                f\"- '{index_path}' is a correct remote path to a directory containing a file named {filename}\\n\\n\"",
            "                f\"- or '{index_path}' is the correct path to a directory containing a file named {filename}.\\n\\n\"",
            "            )",
            "            raise EnvironmentError(msg)",
            "        if is_local:",
            "            logger.info(f\"loading file {resolved_archive_file}\")",
            "        else:",
            "            logger.info(f\"loading file {filename} from cache at {resolved_archive_file}\")",
            "        return resolved_archive_file",
            "",
            "    def _load_passages(self):",
            "        logger.info(f\"Loading passages from {self.index_path}\")",
            "        passages_path = self._resolve_path(self.index_path, self.PASSAGE_FILENAME)",
            "        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):",
            "            raise ValueError(",
            "                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"",
            "                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"",
            "                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"",
            "                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"",
            "            )",
            "        with open(passages_path, \"rb\") as passages_file:",
            "            passages = pickle.load(passages_file)",
            "        return passages",
            "",
            "    def _deserialize_index(self):",
            "        logger.info(f\"Loading index from {self.index_path}\")",
            "        resolved_index_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index.dpr\")",
            "        self.index = faiss.read_index(resolved_index_path)",
            "        resolved_meta_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index_meta.dpr\")",
            "        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):",
            "            raise ValueError(",
            "                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"",
            "                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"",
            "                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"",
            "                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"",
            "            )",
            "        with open(resolved_meta_path, \"rb\") as metadata_file:",
            "            self.index_id_to_db_id = pickle.load(metadata_file)",
            "        assert (",
            "            len(self.index_id_to_db_id) == self.index.ntotal",
            "        ), \"Deserialized index_id_to_db_id should match faiss index size\"",
            "",
            "    def is_initialized(self):",
            "        return self._index_initialized",
            "",
            "    def init_index(self):",
            "        index = faiss.IndexHNSWFlat(self.vector_size + 1, 512)",
            "        index.hnsw.efSearch = 128",
            "        index.hnsw.efConstruction = 200",
            "        self.index = index",
            "        self._deserialize_index()",
            "        self._index_initialized = True",
            "",
            "    def get_doc_dicts(self, doc_ids: np.array):",
            "        doc_list = []",
            "        for doc_ids_i in doc_ids:",
            "            ids = [str(int(doc_id)) for doc_id in doc_ids_i]",
            "            docs = [self.passages[doc_id] for doc_id in ids]",
            "            doc_list.append(docs)",
            "        doc_dicts = []",
            "        for docs in doc_list:",
            "            doc_dict = {}",
            "            doc_dict[\"title\"] = [doc[1] for doc in docs]",
            "            doc_dict[\"text\"] = [doc[0] for doc in docs]",
            "            doc_dicts.append(doc_dict)",
            "        return doc_dicts",
            "",
            "    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:",
            "        aux_dim = np.zeros(len(question_hidden_states), dtype=\"float32\").reshape(-1, 1)",
            "        query_nhsw_vectors = np.hstack((question_hidden_states, aux_dim))",
            "        _, docs_ids = self.index.search(query_nhsw_vectors, n_docs)",
            "        vectors = [[self.index.reconstruct(int(doc_id))[:-1] for doc_id in doc_ids] for doc_ids in docs_ids]",
            "        ids = [[int(self.index_id_to_db_id[doc_id]) for doc_id in doc_ids] for doc_ids in docs_ids]",
            "        return np.array(ids), np.array(vectors)",
            "",
            "",
            "class HFIndexBase(Index):",
            "    def __init__(self, vector_size, dataset, index_initialized=False):",
            "        self.vector_size = vector_size",
            "        self.dataset = dataset",
            "        self._index_initialized = index_initialized",
            "        self._check_dataset_format(with_index=index_initialized)",
            "        dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True, dtype=\"float32\")",
            "",
            "    def _check_dataset_format(self, with_index: bool):",
            "        if not isinstance(self.dataset, Dataset):",
            "            raise ValueError(f\"Dataset should be a datasets.Dataset object, but got {type(self.dataset)}\")",
            "        if len({\"title\", \"text\", \"embeddings\"} - set(self.dataset.column_names)) > 0:",
            "            raise ValueError(",
            "                \"Dataset should be a dataset with the following columns: \"",
            "                \"title (str), text (str) and embeddings (arrays of dimension vector_size), \"",
            "                f\"but got columns {self.dataset.column_names}\"",
            "            )",
            "        if with_index and \"embeddings\" not in self.dataset.list_indexes():",
            "            raise ValueError(",
            "                \"Missing faiss index in the dataset. Make sure you called `dataset.add_faiss_index` to compute it \"",
            "                \"or `dataset.load_faiss_index` to load one from the disk.\"",
            "            )",
            "",
            "    def init_index(self):",
            "        raise NotImplementedError()",
            "",
            "    def is_initialized(self):",
            "        return self._index_initialized",
            "",
            "    def get_doc_dicts(self, doc_ids: np.ndarray) -> List[dict]:",
            "        return [self.dataset[doc_ids[i].tolist()] for i in range(doc_ids.shape[0])]",
            "",
            "    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:",
            "        _, ids = self.dataset.search_batch(\"embeddings\", question_hidden_states, n_docs)",
            "        docs = [self.dataset[[i for i in indices if i >= 0]] for indices in ids]",
            "        vectors = [doc[\"embeddings\"] for doc in docs]",
            "        for i in range(len(vectors)):",
            "            if len(vectors[i]) < n_docs:",
            "                vectors[i] = np.vstack([vectors[i], np.zeros((n_docs - len(vectors[i]), self.vector_size))])",
            "        return np.array(ids), np.array(vectors)  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)",
            "",
            "",
            "class CanonicalHFIndex(HFIndexBase):",
            "    \"\"\"",
            "    A wrapper around an instance of [`~datasets.Datasets`]. If `index_path` is set to `None`, we load the pre-computed",
            "    index available with the [`~datasets.arrow_dataset.Dataset`], otherwise, we load the index from the indicated path",
            "    on disk.",
            "",
            "    Args:",
            "        vector_size (`int`): the dimension of the passages embeddings used by the index",
            "        dataset_name (`str`, optional, defaults to `wiki_dpr`):",
            "            A dataset identifier of the indexed dataset on HuggingFace AWS bucket (list all available datasets and ids",
            "            with `datasets.list_datasets()`).",
            "        dataset_split (`str`, optional, defaults to `train`)",
            "            Which split of the `dataset` to load.",
            "        index_name (`str`, optional, defaults to `train`)",
            "            The index_name of the index associated with the `dataset`. The index loaded from `index_path` will be saved",
            "            under this name.",
            "        index_path (`str`, optional, defaults to `None`)",
            "            The path to the serialized faiss index on disk.",
            "        use_dummy_dataset (`bool`, optional, defaults to `False`):",
            "            If True, use the dummy configuration of the dataset for tests.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        vector_size: int,",
            "        dataset_name: str = \"wiki_dpr\",",
            "        dataset_split: str = \"train\",",
            "        index_name: Optional[str] = None,",
            "        index_path: Optional[str] = None,",
            "        use_dummy_dataset=False,",
            "    ):",
            "        if int(index_path is None) + int(index_name is None) != 1:",
            "            raise ValueError(\"Please provide `index_name` or `index_path`.\")",
            "        self.dataset_name = dataset_name",
            "        self.dataset_split = dataset_split",
            "        self.index_name = index_name",
            "        self.index_path = index_path",
            "        self.use_dummy_dataset = use_dummy_dataset",
            "        logger.info(f\"Loading passages from {self.dataset_name}\")",
            "        dataset = load_dataset(",
            "            self.dataset_name, with_index=False, split=self.dataset_split, dummy=self.use_dummy_dataset",
            "        )",
            "        super().__init__(vector_size, dataset, index_initialized=False)",
            "",
            "    def init_index(self):",
            "        if self.index_path is not None:",
            "            logger.info(f\"Loading index from {self.index_path}\")",
            "            self.dataset.load_faiss_index(\"embeddings\", file=self.index_path)",
            "        else:",
            "            logger.info(f\"Loading index from {self.dataset_name} with index name {self.index_name}\")",
            "            self.dataset = load_dataset(",
            "                self.dataset_name,",
            "                with_embeddings=True,",
            "                with_index=True,",
            "                split=self.dataset_split,",
            "                index_name=self.index_name,",
            "                dummy=self.use_dummy_dataset,",
            "            )",
            "            self.dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True)",
            "        self._index_initialized = True",
            "",
            "",
            "class CustomHFIndex(HFIndexBase):",
            "    \"\"\"",
            "    A wrapper around an instance of [`~datasets.Datasets`]. The dataset and the index are both loaded from the",
            "    indicated paths on disk.",
            "",
            "    Args:",
            "        vector_size (`int`): the dimension of the passages embeddings used by the index",
            "        dataset_path (`str`):",
            "            The path to the serialized dataset on disk. The dataset should have 3 columns: title (str), text (str) and",
            "            embeddings (arrays of dimension vector_size)",
            "        index_path (`str`)",
            "            The path to the serialized faiss index on disk.",
            "    \"\"\"",
            "",
            "    def __init__(self, vector_size: int, dataset, index_path=None):",
            "        super().__init__(vector_size, dataset, index_initialized=index_path is None)",
            "        self.index_path = index_path",
            "",
            "    @classmethod",
            "    def load_from_disk(cls, vector_size, dataset_path, index_path):",
            "        logger.info(f\"Loading passages from {dataset_path}\")",
            "        if dataset_path is None or index_path is None:",
            "            raise ValueError(",
            "                \"Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` \"",
            "                \"and `dataset.get_index('embeddings').save(index_path)`.\"",
            "            )",
            "        dataset = load_from_disk(dataset_path)",
            "        return cls(vector_size=vector_size, dataset=dataset, index_path=index_path)",
            "",
            "    def init_index(self):",
            "        if not self.is_initialized():",
            "            logger.info(f\"Loading index from {self.index_path}\")",
            "            self.dataset.load_faiss_index(\"embeddings\", file=self.index_path)",
            "            self._index_initialized = True",
            "",
            "",
            "class RagRetriever:",
            "    \"\"\"",
            "    Retriever used to get documents from vector queries. It retrieves the documents embeddings as well as the documents",
            "    contents, and it formats them to be used with a RagModel.",
            "",
            "    Args:",
            "        config ([`RagConfig`]):",
            "            The configuration of the RAG model this Retriever is used with. Contains parameters indicating which",
            "            `Index` to build. You can load your own custom dataset with `config.index_name=\"custom\"` or use a canonical",
            "            one (default) from the datasets library with `config.index_name=\"wiki_dpr\"` for example.",
            "        question_encoder_tokenizer ([`PreTrainedTokenizer`]):",
            "            The tokenizer that was used to tokenize the question. It is used to decode the question and then use the",
            "            generator_tokenizer.",
            "        generator_tokenizer ([`PreTrainedTokenizer`]):",
            "            The tokenizer used for the generator part of the RagModel.",
            "        index ([`~models.rag.retrieval_rag.Index`], optional, defaults to the one defined by the configuration):",
            "            If specified, use this index instead of the one built using the configuration",
            "",
            "    Examples:",
            "",
            "    ```python",
            "    >>> # To load the default \"wiki_dpr\" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')",
            "    >>> from transformers import RagRetriever",
            "",
            "    >>> retriever = RagRetriever.from_pretrained(",
            "    ...     \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"",
            "    ... )",
            "",
            "    >>> # To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py",
            "    >>> from transformers import RagRetriever",
            "",
            "    >>> dataset = (",
            "    ...     ...",
            "    ... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index",
            "    >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)",
            "",
            "    >>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py",
            "    >>> from transformers import RagRetriever",
            "",
            "    >>> dataset_path = \"path/to/my/dataset\"  # dataset saved via *dataset.save_to_disk(...)*",
            "    >>> index_path = \"path/to/my/index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*",
            "    >>> retriever = RagRetriever.from_pretrained(",
            "    ...     \"facebook/dpr-ctx_encoder-single-nq-base\",",
            "    ...     index_name=\"custom\",",
            "    ...     passages_path=dataset_path,",
            "    ...     index_path=index_path,",
            "    ... )",
            "",
            "    >>> # To load the legacy index built originally for Rag's paper",
            "    >>> from transformers import RagRetriever",
            "",
            "    >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", index_name=\"legacy\")",
            "    ```\"\"\"",
            "",
            "    def __init__(self, config, question_encoder_tokenizer, generator_tokenizer, index=None, init_retrieval=True):",
            "        self._init_retrieval = init_retrieval",
            "        requires_backends(self, [\"datasets\", \"faiss\"])",
            "        super().__init__()",
            "        self.index = index or self._build_index(config)",
            "        self.generator_tokenizer = generator_tokenizer",
            "        self.question_encoder_tokenizer = question_encoder_tokenizer",
            "",
            "        self.n_docs = config.n_docs",
            "        self.batch_size = config.retrieval_batch_size",
            "",
            "        self.config = config",
            "        if self._init_retrieval:",
            "            self.init_retrieval()",
            "",
            "        self.ctx_encoder_tokenizer = None",
            "        self.return_tokenized_docs = False",
            "",
            "    @staticmethod",
            "    def _build_index(config):",
            "        if config.index_name == \"legacy\":",
            "            return LegacyIndex(",
            "                config.retrieval_vector_size,",
            "                config.index_path or LEGACY_INDEX_PATH,",
            "            )",
            "        elif config.index_name == \"custom\":",
            "            return CustomHFIndex.load_from_disk(",
            "                vector_size=config.retrieval_vector_size,",
            "                dataset_path=config.passages_path,",
            "                index_path=config.index_path,",
            "            )",
            "        else:",
            "            return CanonicalHFIndex(",
            "                vector_size=config.retrieval_vector_size,",
            "                dataset_name=config.dataset,",
            "                dataset_split=config.dataset_split,",
            "                index_name=config.index_name,",
            "                index_path=config.index_path,",
            "                use_dummy_dataset=config.use_dummy_dataset,",
            "            )",
            "",
            "    @classmethod",
            "    def from_pretrained(cls, retriever_name_or_path, indexed_dataset=None, **kwargs):",
            "        requires_backends(cls, [\"datasets\", \"faiss\"])",
            "        config = kwargs.pop(\"config\", None) or RagConfig.from_pretrained(retriever_name_or_path, **kwargs)",
            "        rag_tokenizer = RagTokenizer.from_pretrained(retriever_name_or_path, config=config)",
            "        question_encoder_tokenizer = rag_tokenizer.question_encoder",
            "        generator_tokenizer = rag_tokenizer.generator",
            "        if indexed_dataset is not None:",
            "            config.index_name = \"custom\"",
            "            index = CustomHFIndex(config.retrieval_vector_size, indexed_dataset)",
            "        else:",
            "            index = cls._build_index(config)",
            "        return cls(",
            "            config,",
            "            question_encoder_tokenizer=question_encoder_tokenizer,",
            "            generator_tokenizer=generator_tokenizer,",
            "            index=index,",
            "        )",
            "",
            "    def save_pretrained(self, save_directory):",
            "        if isinstance(self.index, CustomHFIndex):",
            "            if self.config.index_path is None:",
            "                index_path = os.path.join(save_directory, \"hf_dataset_index.faiss\")",
            "                self.index.dataset.get_index(\"embeddings\").save(index_path)",
            "                self.config.index_path = index_path",
            "            if self.config.passages_path is None:",
            "                passages_path = os.path.join(save_directory, \"hf_dataset\")",
            "                # datasets don't support save_to_disk with indexes right now",
            "                faiss_index = self.index.dataset._indexes.pop(\"embeddings\")",
            "                self.index.dataset.save_to_disk(passages_path)",
            "                self.index.dataset._indexes[\"embeddings\"] = faiss_index",
            "                self.config.passages_path = passages_path",
            "        self.config.save_pretrained(save_directory)",
            "        rag_tokenizer = RagTokenizer(",
            "            question_encoder=self.question_encoder_tokenizer,",
            "            generator=self.generator_tokenizer,",
            "        )",
            "        rag_tokenizer.save_pretrained(save_directory)",
            "",
            "    def init_retrieval(self):",
            "        \"\"\"",
            "        Retriever initialization function. It loads the index into memory.",
            "        \"\"\"",
            "",
            "        logger.info(\"initializing retrieval\")",
            "        self.index.init_index()",
            "",
            "    def postprocess_docs(self, docs, input_strings, prefix, n_docs, return_tensors=None):",
            "        r\"\"\"",
            "        Postprocessing retrieved `docs` and combining them with `input_strings`.",
            "",
            "        Args:",
            "            docs  (`dict`):",
            "                Retrieved documents.",
            "            input_strings (`str`):",
            "                Input strings decoded by `preprocess_query`.",
            "            prefix (`str`):",
            "                Prefix added at the beginning of each input, typically used with T5-based models.",
            "",
            "        Return:",
            "            `tuple(tensors)`: a tuple consisting of two elements: contextualized `input_ids` and a compatible",
            "            `attention_mask`.",
            "        \"\"\"",
            "",
            "        def cat_input_and_doc(doc_title, doc_text, input_string, prefix):",
            "            # TODO(Patrick): if we train more RAG models, I want to put the input first to take advantage of effortless truncation",
            "            # TODO(piktus): better handling of truncation",
            "            if doc_title.startswith('\"'):",
            "                doc_title = doc_title[1:]",
            "            if doc_title.endswith('\"'):",
            "                doc_title = doc_title[:-1]",
            "            if prefix is None:",
            "                prefix = \"\"",
            "            out = (prefix + doc_title + self.config.title_sep + doc_text + self.config.doc_sep + input_string).replace(",
            "                \"  \", \" \"",
            "            )",
            "            return out",
            "",
            "        rag_input_strings = [",
            "            cat_input_and_doc(",
            "                docs[i][\"title\"][j],",
            "                docs[i][\"text\"][j],",
            "                input_strings[i],",
            "                prefix,",
            "            )",
            "            for i in range(len(docs))",
            "            for j in range(n_docs)",
            "        ]",
            "",
            "        contextualized_inputs = self.generator_tokenizer.batch_encode_plus(",
            "            rag_input_strings,",
            "            max_length=self.config.max_combined_length,",
            "            return_tensors=return_tensors,",
            "            padding=\"max_length\",",
            "            truncation=True,",
            "        )",
            "",
            "        return contextualized_inputs[\"input_ids\"], contextualized_inputs[\"attention_mask\"]",
            "",
            "    def _chunk_tensor(self, t: Iterable, chunk_size: int) -> List[Iterable]:",
            "        return [t[i : i + chunk_size] for i in range(0, len(t), chunk_size)]",
            "",
            "    def _main_retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, np.ndarray]:",
            "        question_hidden_states_batched = self._chunk_tensor(question_hidden_states, self.batch_size)",
            "        ids_batched = []",
            "        vectors_batched = []",
            "        for question_hidden_states in question_hidden_states_batched:",
            "            start_time = time.time()",
            "            ids, vectors = self.index.get_top_docs(question_hidden_states, n_docs)",
            "            logger.debug(",
            "                f\"index search time: {time.time() - start_time} sec, batch size {question_hidden_states.shape}\"",
            "            )",
            "            ids_batched.extend(ids)",
            "            vectors_batched.extend(vectors)",
            "        return (",
            "            np.array(ids_batched),",
            "            np.array(vectors_batched),",
            "        )  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)",
            "",
            "    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, List[dict]]:",
            "        \"\"\"",
            "        Retrieves documents for specified `question_hidden_states`.",
            "",
            "        Args:",
            "            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`):",
            "                A batch of query vectors to retrieve with.",
            "            n_docs (`int`):",
            "                The number of docs retrieved per query.",
            "",
            "        Return:",
            "            `Tuple[np.ndarray, np.ndarray, List[dict]]`: A tuple with the following objects:",
            "",
            "            - **retrieved_doc_embeds** (`np.ndarray` of shape `(batch_size, n_docs, dim)`) -- The retrieval embeddings",
            "              of the retrieved docs per query.",
            "            - **doc_ids** (`np.ndarray` of shape `(batch_size, n_docs)`) -- The ids of the documents in the index",
            "            - **doc_dicts** (`List[dict]`): The `retrieved_doc_embeds` examples per query.",
            "        \"\"\"",
            "",
            "        doc_ids, retrieved_doc_embeds = self._main_retrieve(question_hidden_states, n_docs)",
            "        return retrieved_doc_embeds, doc_ids, self.index.get_doc_dicts(doc_ids)",
            "",
            "    def set_ctx_encoder_tokenizer(self, ctx_encoder_tokenizer: PreTrainedTokenizer):",
            "        # used in end2end retriever training",
            "        self.ctx_encoder_tokenizer = ctx_encoder_tokenizer",
            "        self.return_tokenized_docs = True",
            "",
            "    def __call__(",
            "        self,",
            "        question_input_ids: List[List[int]],",
            "        question_hidden_states: np.ndarray,",
            "        prefix=None,",
            "        n_docs=None,",
            "        return_tensors=None,",
            "    ) -> BatchEncoding:",
            "        \"\"\"",
            "        Retrieves documents for specified `question_hidden_states`.",
            "",
            "        Args:",
            "            question_input_ids (`List[List[int]]`) batch of input ids",
            "            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`:",
            "                A batch of query vectors to retrieve with.",
            "            prefix (`str`, *optional*):",
            "                The prefix used by the generator's tokenizer.",
            "            n_docs (`int`, *optional*):",
            "                The number of docs retrieved per query.",
            "            return_tensors (`str` or [`~utils.TensorType`], *optional*, defaults to \"pt\"):",
            "                If set, will return tensors instead of list of python integers. Acceptable values are:",
            "",
            "                - `'tf'`: Return TensorFlow `tf.constant` objects.",
            "                - `'pt'`: Return PyTorch `torch.Tensor` objects.",
            "                - `'np'`: Return Numpy `np.ndarray` objects.",
            "",
            "        Returns: [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:",
            "",
            "            - **context_input_ids** -- List of token ids to be fed to a model.",
            "",
            "              [What are input IDs?](../glossary#input-ids)",
            "",
            "            - **context_attention_mask** -- List of indices specifying which tokens should be attended to by the model",
            "            (when `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).",
            "",
            "              [What are attention masks?](../glossary#attention-mask)",
            "",
            "            - **retrieved_doc_embeds** -- List of embeddings of the retrieved documents",
            "            - **doc_ids** -- List of ids of the retrieved documents",
            "        \"\"\"",
            "",
            "        n_docs = n_docs if n_docs is not None else self.n_docs",
            "        prefix = prefix if prefix is not None else self.config.generator.prefix",
            "        retrieved_doc_embeds, doc_ids, docs = self.retrieve(question_hidden_states, n_docs)",
            "",
            "        input_strings = self.question_encoder_tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)",
            "        context_input_ids, context_attention_mask = self.postprocess_docs(",
            "            docs, input_strings, prefix, n_docs, return_tensors=return_tensors",
            "        )",
            "",
            "        if self.return_tokenized_docs:",
            "            retrieved_doc_text = []",
            "            retrieved_doc_title = []",
            "",
            "            for b_idx in range(len(docs)):",
            "                for doc_idx in range(n_docs):",
            "                    retrieved_doc_text.append(docs[b_idx][\"text\"][doc_idx])",
            "                    retrieved_doc_title.append(docs[b_idx][\"title\"][doc_idx])",
            "",
            "            tokenized_docs = self.ctx_encoder_tokenizer(",
            "                retrieved_doc_title,",
            "                retrieved_doc_text,",
            "                truncation=True,",
            "                padding=\"longest\",",
            "                return_tensors=return_tensors,",
            "            )",
            "",
            "            return BatchEncoding(",
            "                {",
            "                    \"context_input_ids\": context_input_ids,",
            "                    \"context_attention_mask\": context_attention_mask,",
            "                    \"retrieved_doc_embeds\": retrieved_doc_embeds,",
            "                    \"doc_ids\": doc_ids,",
            "                    \"tokenized_doc_ids\": tokenized_docs[\"input_ids\"],",
            "                    \"tokenized_doc_attention_mask\": tokenized_docs[\"attention_mask\"],",
            "                },",
            "                tensor_type=return_tensors,",
            "            )",
            "",
            "        else:",
            "            return BatchEncoding(",
            "                {",
            "                    \"context_input_ids\": context_input_ids,",
            "                    \"context_attention_mask\": context_attention_mask,",
            "                    \"retrieved_doc_embeds\": retrieved_doc_embeds,",
            "                    \"doc_ids\": doc_ids,",
            "                },",
            "                tensor_type=return_tensors,",
            "            )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "26": []
        },
        "addLocation": []
    }
}