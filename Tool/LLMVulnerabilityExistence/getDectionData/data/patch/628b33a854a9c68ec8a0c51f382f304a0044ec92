{
    "django/conf/global_settings.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 313,
                "PatchRowcode": " # SuspiciousOperation (TooManyFieldsSent) is raised."
            },
            "1": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": 314,
                "PatchRowcode": " DATA_UPLOAD_MAX_NUMBER_FIELDS = 1000"
            },
            "2": {
                "beforePatchRowNumber": 315,
                "afterPatchRowNumber": 315,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 316,
                "PatchRowcode": "+# Maximum number of files encoded in a multipart upload that will be read"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 317,
                "PatchRowcode": "+# before a SuspiciousOperation (TooManyFilesSent) is raised."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 318,
                "PatchRowcode": "+DATA_UPLOAD_MAX_NUMBER_FILES = 100"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 319,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": 316,
                "afterPatchRowNumber": 320,
                "PatchRowcode": " # Directory in which upload streamed files will be temporarily saved. A value of"
            },
            "8": {
                "beforePatchRowNumber": 317,
                "afterPatchRowNumber": 321,
                "PatchRowcode": " # `None` will make Django use the operating system's default temporary directory"
            },
            "9": {
                "beforePatchRowNumber": 318,
                "afterPatchRowNumber": 322,
                "PatchRowcode": " # (i.e. \"/tmp\" on *nix systems)."
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Default Django settings. Override these with settings in the module pointed to",
            "by the DJANGO_SETTINGS_MODULE environment variable.",
            "\"\"\"",
            "",
            "",
            "# This is defined here as a do-nothing function because we can't import",
            "# django.utils.translation -- that module depends on the settings.",
            "def gettext_noop(s):",
            "    return s",
            "",
            "",
            "####################",
            "# CORE             #",
            "####################",
            "",
            "DEBUG = False",
            "",
            "# Whether the framework should propagate raw exceptions rather than catching",
            "# them. This is useful under some testing situations and should never be used",
            "# on a live site.",
            "DEBUG_PROPAGATE_EXCEPTIONS = False",
            "",
            "# People who get code error notifications. In the format",
            "# [('Full Name', 'email@example.com'), ('Full Name', 'anotheremail@example.com')]",
            "ADMINS = []",
            "",
            "# List of IP addresses, as strings, that:",
            "#   * See debug comments, when DEBUG is true",
            "#   * Receive x-headers",
            "INTERNAL_IPS = []",
            "",
            "# Hosts/domain names that are valid for this site.",
            "# \"*\" matches anything, \".example.com\" matches example.com and all subdomains",
            "ALLOWED_HOSTS = []",
            "",
            "# Local time zone for this installation. All choices can be found here:",
            "# https://en.wikipedia.org/wiki/List_of_tz_zones_by_name (although not all",
            "# systems may support all possibilities). When USE_TZ is True, this is",
            "# interpreted as the default user time zone.",
            "TIME_ZONE = \"America/Chicago\"",
            "",
            "# If you set this to True, Django will use timezone-aware datetimes.",
            "USE_TZ = False",
            "",
            "# RemovedInDjango50Warning: It's a transitional setting helpful in migrating",
            "# from pytz tzinfo to ZoneInfo(). Set True to continue using pytz tzinfo",
            "# objects during the Django 4.x release cycle.",
            "USE_DEPRECATED_PYTZ = False",
            "",
            "# Language code for this installation. All choices can be found here:",
            "# http://www.i18nguy.com/unicode/language-identifiers.html",
            "LANGUAGE_CODE = \"en-us\"",
            "",
            "# Languages we provide translations for, out of the box.",
            "LANGUAGES = [",
            "    (\"af\", gettext_noop(\"Afrikaans\")),",
            "    (\"ar\", gettext_noop(\"Arabic\")),",
            "    (\"ar-dz\", gettext_noop(\"Algerian Arabic\")),",
            "    (\"ast\", gettext_noop(\"Asturian\")),",
            "    (\"az\", gettext_noop(\"Azerbaijani\")),",
            "    (\"bg\", gettext_noop(\"Bulgarian\")),",
            "    (\"be\", gettext_noop(\"Belarusian\")),",
            "    (\"bn\", gettext_noop(\"Bengali\")),",
            "    (\"br\", gettext_noop(\"Breton\")),",
            "    (\"bs\", gettext_noop(\"Bosnian\")),",
            "    (\"ca\", gettext_noop(\"Catalan\")),",
            "    (\"cs\", gettext_noop(\"Czech\")),",
            "    (\"cy\", gettext_noop(\"Welsh\")),",
            "    (\"da\", gettext_noop(\"Danish\")),",
            "    (\"de\", gettext_noop(\"German\")),",
            "    (\"dsb\", gettext_noop(\"Lower Sorbian\")),",
            "    (\"el\", gettext_noop(\"Greek\")),",
            "    (\"en\", gettext_noop(\"English\")),",
            "    (\"en-au\", gettext_noop(\"Australian English\")),",
            "    (\"en-gb\", gettext_noop(\"British English\")),",
            "    (\"eo\", gettext_noop(\"Esperanto\")),",
            "    (\"es\", gettext_noop(\"Spanish\")),",
            "    (\"es-ar\", gettext_noop(\"Argentinian Spanish\")),",
            "    (\"es-co\", gettext_noop(\"Colombian Spanish\")),",
            "    (\"es-mx\", gettext_noop(\"Mexican Spanish\")),",
            "    (\"es-ni\", gettext_noop(\"Nicaraguan Spanish\")),",
            "    (\"es-ve\", gettext_noop(\"Venezuelan Spanish\")),",
            "    (\"et\", gettext_noop(\"Estonian\")),",
            "    (\"eu\", gettext_noop(\"Basque\")),",
            "    (\"fa\", gettext_noop(\"Persian\")),",
            "    (\"fi\", gettext_noop(\"Finnish\")),",
            "    (\"fr\", gettext_noop(\"French\")),",
            "    (\"fy\", gettext_noop(\"Frisian\")),",
            "    (\"ga\", gettext_noop(\"Irish\")),",
            "    (\"gd\", gettext_noop(\"Scottish Gaelic\")),",
            "    (\"gl\", gettext_noop(\"Galician\")),",
            "    (\"he\", gettext_noop(\"Hebrew\")),",
            "    (\"hi\", gettext_noop(\"Hindi\")),",
            "    (\"hr\", gettext_noop(\"Croatian\")),",
            "    (\"hsb\", gettext_noop(\"Upper Sorbian\")),",
            "    (\"hu\", gettext_noop(\"Hungarian\")),",
            "    (\"hy\", gettext_noop(\"Armenian\")),",
            "    (\"ia\", gettext_noop(\"Interlingua\")),",
            "    (\"id\", gettext_noop(\"Indonesian\")),",
            "    (\"ig\", gettext_noop(\"Igbo\")),",
            "    (\"io\", gettext_noop(\"Ido\")),",
            "    (\"is\", gettext_noop(\"Icelandic\")),",
            "    (\"it\", gettext_noop(\"Italian\")),",
            "    (\"ja\", gettext_noop(\"Japanese\")),",
            "    (\"ka\", gettext_noop(\"Georgian\")),",
            "    (\"kab\", gettext_noop(\"Kabyle\")),",
            "    (\"kk\", gettext_noop(\"Kazakh\")),",
            "    (\"km\", gettext_noop(\"Khmer\")),",
            "    (\"kn\", gettext_noop(\"Kannada\")),",
            "    (\"ko\", gettext_noop(\"Korean\")),",
            "    (\"ky\", gettext_noop(\"Kyrgyz\")),",
            "    (\"lb\", gettext_noop(\"Luxembourgish\")),",
            "    (\"lt\", gettext_noop(\"Lithuanian\")),",
            "    (\"lv\", gettext_noop(\"Latvian\")),",
            "    (\"mk\", gettext_noop(\"Macedonian\")),",
            "    (\"ml\", gettext_noop(\"Malayalam\")),",
            "    (\"mn\", gettext_noop(\"Mongolian\")),",
            "    (\"mr\", gettext_noop(\"Marathi\")),",
            "    (\"ms\", gettext_noop(\"Malay\")),",
            "    (\"my\", gettext_noop(\"Burmese\")),",
            "    (\"nb\", gettext_noop(\"Norwegian Bokm\u00e5l\")),",
            "    (\"ne\", gettext_noop(\"Nepali\")),",
            "    (\"nl\", gettext_noop(\"Dutch\")),",
            "    (\"nn\", gettext_noop(\"Norwegian Nynorsk\")),",
            "    (\"os\", gettext_noop(\"Ossetic\")),",
            "    (\"pa\", gettext_noop(\"Punjabi\")),",
            "    (\"pl\", gettext_noop(\"Polish\")),",
            "    (\"pt\", gettext_noop(\"Portuguese\")),",
            "    (\"pt-br\", gettext_noop(\"Brazilian Portuguese\")),",
            "    (\"ro\", gettext_noop(\"Romanian\")),",
            "    (\"ru\", gettext_noop(\"Russian\")),",
            "    (\"sk\", gettext_noop(\"Slovak\")),",
            "    (\"sl\", gettext_noop(\"Slovenian\")),",
            "    (\"sq\", gettext_noop(\"Albanian\")),",
            "    (\"sr\", gettext_noop(\"Serbian\")),",
            "    (\"sr-latn\", gettext_noop(\"Serbian Latin\")),",
            "    (\"sv\", gettext_noop(\"Swedish\")),",
            "    (\"sw\", gettext_noop(\"Swahili\")),",
            "    (\"ta\", gettext_noop(\"Tamil\")),",
            "    (\"te\", gettext_noop(\"Telugu\")),",
            "    (\"tg\", gettext_noop(\"Tajik\")),",
            "    (\"th\", gettext_noop(\"Thai\")),",
            "    (\"tk\", gettext_noop(\"Turkmen\")),",
            "    (\"tr\", gettext_noop(\"Turkish\")),",
            "    (\"tt\", gettext_noop(\"Tatar\")),",
            "    (\"udm\", gettext_noop(\"Udmurt\")),",
            "    (\"uk\", gettext_noop(\"Ukrainian\")),",
            "    (\"ur\", gettext_noop(\"Urdu\")),",
            "    (\"uz\", gettext_noop(\"Uzbek\")),",
            "    (\"vi\", gettext_noop(\"Vietnamese\")),",
            "    (\"zh-hans\", gettext_noop(\"Simplified Chinese\")),",
            "    (\"zh-hant\", gettext_noop(\"Traditional Chinese\")),",
            "]",
            "",
            "# Languages using BiDi (right-to-left) layout",
            "LANGUAGES_BIDI = [\"he\", \"ar\", \"ar-dz\", \"fa\", \"ur\"]",
            "",
            "# If you set this to False, Django will make some optimizations so as not",
            "# to load the internationalization machinery.",
            "USE_I18N = True",
            "LOCALE_PATHS = []",
            "",
            "# Settings for language cookie",
            "LANGUAGE_COOKIE_NAME = \"django_language\"",
            "LANGUAGE_COOKIE_AGE = None",
            "LANGUAGE_COOKIE_DOMAIN = None",
            "LANGUAGE_COOKIE_PATH = \"/\"",
            "LANGUAGE_COOKIE_SECURE = False",
            "LANGUAGE_COOKIE_HTTPONLY = False",
            "LANGUAGE_COOKIE_SAMESITE = None",
            "",
            "",
            "# If you set this to True, Django will format dates, numbers and calendars",
            "# according to user current locale.",
            "USE_L10N = True",
            "",
            "# Not-necessarily-technical managers of the site. They get broken link",
            "# notifications and other various emails.",
            "MANAGERS = ADMINS",
            "",
            "# Default charset to use for all HttpResponse objects, if a MIME type isn't",
            "# manually specified. It's used to construct the Content-Type header.",
            "DEFAULT_CHARSET = \"utf-8\"",
            "",
            "# Email address that error messages come from.",
            "SERVER_EMAIL = \"root@localhost\"",
            "",
            "# Database connection info. If left empty, will default to the dummy backend.",
            "DATABASES = {}",
            "",
            "# Classes used to implement DB routing behavior.",
            "DATABASE_ROUTERS = []",
            "",
            "# The email backend to use. For possible shortcuts see django.core.mail.",
            "# The default is to use the SMTP backend.",
            "# Third-party backends can be specified by providing a Python path",
            "# to a module that defines an EmailBackend class.",
            "EMAIL_BACKEND = \"django.core.mail.backends.smtp.EmailBackend\"",
            "",
            "# Host for sending email.",
            "EMAIL_HOST = \"localhost\"",
            "",
            "# Port for sending email.",
            "EMAIL_PORT = 25",
            "",
            "# Whether to send SMTP 'Date' header in the local time zone or in UTC.",
            "EMAIL_USE_LOCALTIME = False",
            "",
            "# Optional SMTP authentication information for EMAIL_HOST.",
            "EMAIL_HOST_USER = \"\"",
            "EMAIL_HOST_PASSWORD = \"\"",
            "EMAIL_USE_TLS = False",
            "EMAIL_USE_SSL = False",
            "EMAIL_SSL_CERTFILE = None",
            "EMAIL_SSL_KEYFILE = None",
            "EMAIL_TIMEOUT = None",
            "",
            "# List of strings representing installed apps.",
            "INSTALLED_APPS = []",
            "",
            "TEMPLATES = []",
            "",
            "# Default form rendering class.",
            "FORM_RENDERER = \"django.forms.renderers.DjangoTemplates\"",
            "",
            "# Default email address to use for various automated correspondence from",
            "# the site managers.",
            "DEFAULT_FROM_EMAIL = \"webmaster@localhost\"",
            "",
            "# Subject-line prefix for email messages send with django.core.mail.mail_admins",
            "# or ...mail_managers.  Make sure to include the trailing space.",
            "EMAIL_SUBJECT_PREFIX = \"[Django] \"",
            "",
            "# Whether to append trailing slashes to URLs.",
            "APPEND_SLASH = True",
            "",
            "# Whether to prepend the \"www.\" subdomain to URLs that don't have it.",
            "PREPEND_WWW = False",
            "",
            "# Override the server-derived value of SCRIPT_NAME",
            "FORCE_SCRIPT_NAME = None",
            "",
            "# List of compiled regular expression objects representing User-Agent strings",
            "# that are not allowed to visit any page, systemwide. Use this for bad",
            "# robots/crawlers. Here are a few examples:",
            "#     import re",
            "#     DISALLOWED_USER_AGENTS = [",
            "#         re.compile(r'^NaverBot.*'),",
            "#         re.compile(r'^EmailSiphon.*'),",
            "#         re.compile(r'^SiteSucker.*'),",
            "#         re.compile(r'^sohu-search'),",
            "#     ]",
            "DISALLOWED_USER_AGENTS = []",
            "",
            "ABSOLUTE_URL_OVERRIDES = {}",
            "",
            "# List of compiled regular expression objects representing URLs that need not",
            "# be reported by BrokenLinkEmailsMiddleware. Here are a few examples:",
            "#    import re",
            "#    IGNORABLE_404_URLS = [",
            "#        re.compile(r'^/apple-touch-icon.*\\.png$'),",
            "#        re.compile(r'^/favicon.ico$'),",
            "#        re.compile(r'^/robots.txt$'),",
            "#        re.compile(r'^/phpmyadmin/'),",
            "#        re.compile(r'\\.(cgi|php|pl)$'),",
            "#    ]",
            "IGNORABLE_404_URLS = []",
            "",
            "# A secret key for this particular Django installation. Used in secret-key",
            "# hashing algorithms. Set this in your settings, or Django will complain",
            "# loudly.",
            "SECRET_KEY = \"\"",
            "",
            "# List of secret keys used to verify the validity of signatures. This allows",
            "# secret key rotation.",
            "SECRET_KEY_FALLBACKS = []",
            "",
            "# Default file storage mechanism that holds media.",
            "DEFAULT_FILE_STORAGE = \"django.core.files.storage.FileSystemStorage\"",
            "",
            "# Absolute filesystem path to the directory that will hold user-uploaded files.",
            "# Example: \"/var/www/example.com/media/\"",
            "MEDIA_ROOT = \"\"",
            "",
            "# URL that handles the media served from MEDIA_ROOT.",
            "# Examples: \"http://example.com/media/\", \"http://media.example.com/\"",
            "MEDIA_URL = \"\"",
            "",
            "# Absolute path to the directory static files should be collected to.",
            "# Example: \"/var/www/example.com/static/\"",
            "STATIC_ROOT = None",
            "",
            "# URL that handles the static files served from STATIC_ROOT.",
            "# Example: \"http://example.com/static/\", \"http://static.example.com/\"",
            "STATIC_URL = None",
            "",
            "# List of upload handler classes to be applied in order.",
            "FILE_UPLOAD_HANDLERS = [",
            "    \"django.core.files.uploadhandler.MemoryFileUploadHandler\",",
            "    \"django.core.files.uploadhandler.TemporaryFileUploadHandler\",",
            "]",
            "",
            "# Maximum size, in bytes, of a request before it will be streamed to the",
            "# file system instead of into memory.",
            "FILE_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB",
            "",
            "# Maximum size in bytes of request data (excluding file uploads) that will be",
            "# read before a SuspiciousOperation (RequestDataTooBig) is raised.",
            "DATA_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB",
            "",
            "# Maximum number of GET/POST parameters that will be read before a",
            "# SuspiciousOperation (TooManyFieldsSent) is raised.",
            "DATA_UPLOAD_MAX_NUMBER_FIELDS = 1000",
            "",
            "# Directory in which upload streamed files will be temporarily saved. A value of",
            "# `None` will make Django use the operating system's default temporary directory",
            "# (i.e. \"/tmp\" on *nix systems).",
            "FILE_UPLOAD_TEMP_DIR = None",
            "",
            "# The numeric mode to set newly-uploaded files to. The value should be a mode",
            "# you'd pass directly to os.chmod; see",
            "# https://docs.python.org/library/os.html#files-and-directories.",
            "FILE_UPLOAD_PERMISSIONS = 0o644",
            "",
            "# The numeric mode to assign to newly-created directories, when uploading files.",
            "# The value should be a mode as you'd pass to os.chmod;",
            "# see https://docs.python.org/library/os.html#files-and-directories.",
            "FILE_UPLOAD_DIRECTORY_PERMISSIONS = None",
            "",
            "# Python module path where user will place custom format definition.",
            "# The directory where this setting is pointing should contain subdirectories",
            "# named as the locales, containing a formats.py file",
            "# (i.e. \"myproject.locale\" for myproject/locale/en/formats.py etc. use)",
            "FORMAT_MODULE_PATH = None",
            "",
            "# Default formatting for date objects. See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "DATE_FORMAT = \"N j, Y\"",
            "",
            "# Default formatting for datetime objects. See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "DATETIME_FORMAT = \"N j, Y, P\"",
            "",
            "# Default formatting for time objects. See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "TIME_FORMAT = \"P\"",
            "",
            "# Default formatting for date objects when only the year and month are relevant.",
            "# See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "YEAR_MONTH_FORMAT = \"F Y\"",
            "",
            "# Default formatting for date objects when only the month and day are relevant.",
            "# See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "MONTH_DAY_FORMAT = \"F j\"",
            "",
            "# Default short formatting for date objects. See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "SHORT_DATE_FORMAT = \"m/d/Y\"",
            "",
            "# Default short formatting for datetime objects.",
            "# See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "SHORT_DATETIME_FORMAT = \"m/d/Y P\"",
            "",
            "# Default formats to be used when parsing dates from input boxes, in order",
            "# See all available format string here:",
            "# https://docs.python.org/library/datetime.html#strftime-behavior",
            "# * Note that these format strings are different from the ones to display dates",
            "DATE_INPUT_FORMATS = [",
            "    \"%Y-%m-%d\",  # '2006-10-25'",
            "    \"%m/%d/%Y\",  # '10/25/2006'",
            "    \"%m/%d/%y\",  # '10/25/06'",
            "    \"%b %d %Y\",  # 'Oct 25 2006'",
            "    \"%b %d, %Y\",  # 'Oct 25, 2006'",
            "    \"%d %b %Y\",  # '25 Oct 2006'",
            "    \"%d %b, %Y\",  # '25 Oct, 2006'",
            "    \"%B %d %Y\",  # 'October 25 2006'",
            "    \"%B %d, %Y\",  # 'October 25, 2006'",
            "    \"%d %B %Y\",  # '25 October 2006'",
            "    \"%d %B, %Y\",  # '25 October, 2006'",
            "]",
            "",
            "# Default formats to be used when parsing times from input boxes, in order",
            "# See all available format string here:",
            "# https://docs.python.org/library/datetime.html#strftime-behavior",
            "# * Note that these format strings are different from the ones to display dates",
            "TIME_INPUT_FORMATS = [",
            "    \"%H:%M:%S\",  # '14:30:59'",
            "    \"%H:%M:%S.%f\",  # '14:30:59.000200'",
            "    \"%H:%M\",  # '14:30'",
            "]",
            "",
            "# Default formats to be used when parsing dates and times from input boxes,",
            "# in order",
            "# See all available format string here:",
            "# https://docs.python.org/library/datetime.html#strftime-behavior",
            "# * Note that these format strings are different from the ones to display dates",
            "DATETIME_INPUT_FORMATS = [",
            "    \"%Y-%m-%d %H:%M:%S\",  # '2006-10-25 14:30:59'",
            "    \"%Y-%m-%d %H:%M:%S.%f\",  # '2006-10-25 14:30:59.000200'",
            "    \"%Y-%m-%d %H:%M\",  # '2006-10-25 14:30'",
            "    \"%m/%d/%Y %H:%M:%S\",  # '10/25/2006 14:30:59'",
            "    \"%m/%d/%Y %H:%M:%S.%f\",  # '10/25/2006 14:30:59.000200'",
            "    \"%m/%d/%Y %H:%M\",  # '10/25/2006 14:30'",
            "    \"%m/%d/%y %H:%M:%S\",  # '10/25/06 14:30:59'",
            "    \"%m/%d/%y %H:%M:%S.%f\",  # '10/25/06 14:30:59.000200'",
            "    \"%m/%d/%y %H:%M\",  # '10/25/06 14:30'",
            "]",
            "",
            "# First day of week, to be used on calendars",
            "# 0 means Sunday, 1 means Monday...",
            "FIRST_DAY_OF_WEEK = 0",
            "",
            "# Decimal separator symbol",
            "DECIMAL_SEPARATOR = \".\"",
            "",
            "# Boolean that sets whether to add thousand separator when formatting numbers",
            "USE_THOUSAND_SEPARATOR = False",
            "",
            "# Number of digits that will be together, when splitting them by",
            "# THOUSAND_SEPARATOR. 0 means no grouping, 3 means splitting by thousands...",
            "NUMBER_GROUPING = 0",
            "",
            "# Thousand separator symbol",
            "THOUSAND_SEPARATOR = \",\"",
            "",
            "# The tablespaces to use for each model when not specified otherwise.",
            "DEFAULT_TABLESPACE = \"\"",
            "DEFAULT_INDEX_TABLESPACE = \"\"",
            "",
            "# Default primary key field type.",
            "DEFAULT_AUTO_FIELD = \"django.db.models.AutoField\"",
            "",
            "# Default X-Frame-Options header value",
            "X_FRAME_OPTIONS = \"DENY\"",
            "",
            "USE_X_FORWARDED_HOST = False",
            "USE_X_FORWARDED_PORT = False",
            "",
            "# The Python dotted path to the WSGI application that Django's internal server",
            "# (runserver) will use. If `None`, the return value of",
            "# 'django.core.wsgi.get_wsgi_application' is used, thus preserving the same",
            "# behavior as previous versions of Django. Otherwise this should point to an",
            "# actual WSGI application object.",
            "WSGI_APPLICATION = None",
            "",
            "# If your Django app is behind a proxy that sets a header to specify secure",
            "# connections, AND that proxy ensures that user-submitted headers with the",
            "# same name are ignored (so that people can't spoof it), set this value to",
            "# a tuple of (header_name, header_value). For any requests that come in with",
            "# that header/value, request.is_secure() will return True.",
            "# WARNING! Only set this if you fully understand what you're doing. Otherwise,",
            "# you may be opening yourself up to a security risk.",
            "SECURE_PROXY_SSL_HEADER = None",
            "",
            "##############",
            "# MIDDLEWARE #",
            "##############",
            "",
            "# List of middleware to use. Order is important; in the request phase, these",
            "# middleware will be applied in the order given, and in the response",
            "# phase the middleware will be applied in reverse order.",
            "MIDDLEWARE = []",
            "",
            "############",
            "# SESSIONS #",
            "############",
            "",
            "# Cache to store session data if using the cache session backend.",
            "SESSION_CACHE_ALIAS = \"default\"",
            "# Cookie name. This can be whatever you want.",
            "SESSION_COOKIE_NAME = \"sessionid\"",
            "# Age of cookie, in seconds (default: 2 weeks).",
            "SESSION_COOKIE_AGE = 60 * 60 * 24 * 7 * 2",
            "# A string like \"example.com\", or None for standard domain cookie.",
            "SESSION_COOKIE_DOMAIN = None",
            "# Whether the session cookie should be secure (https:// only).",
            "SESSION_COOKIE_SECURE = False",
            "# The path of the session cookie.",
            "SESSION_COOKIE_PATH = \"/\"",
            "# Whether to use the HttpOnly flag.",
            "SESSION_COOKIE_HTTPONLY = True",
            "# Whether to set the flag restricting cookie leaks on cross-site requests.",
            "# This can be 'Lax', 'Strict', 'None', or False to disable the flag.",
            "SESSION_COOKIE_SAMESITE = \"Lax\"",
            "# Whether to save the session data on every request.",
            "SESSION_SAVE_EVERY_REQUEST = False",
            "# Whether a user's session cookie expires when the web browser is closed.",
            "SESSION_EXPIRE_AT_BROWSER_CLOSE = False",
            "# The module to store session data",
            "SESSION_ENGINE = \"django.contrib.sessions.backends.db\"",
            "# Directory to store session files if using the file session module. If None,",
            "# the backend will use a sensible default.",
            "SESSION_FILE_PATH = None",
            "# class to serialize session data",
            "SESSION_SERIALIZER = \"django.contrib.sessions.serializers.JSONSerializer\"",
            "",
            "#########",
            "# CACHE #",
            "#########",
            "",
            "# The cache backends to use.",
            "CACHES = {",
            "    \"default\": {",
            "        \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",",
            "    }",
            "}",
            "CACHE_MIDDLEWARE_KEY_PREFIX = \"\"",
            "CACHE_MIDDLEWARE_SECONDS = 600",
            "CACHE_MIDDLEWARE_ALIAS = \"default\"",
            "",
            "##################",
            "# AUTHENTICATION #",
            "##################",
            "",
            "AUTH_USER_MODEL = \"auth.User\"",
            "",
            "AUTHENTICATION_BACKENDS = [\"django.contrib.auth.backends.ModelBackend\"]",
            "",
            "LOGIN_URL = \"/accounts/login/\"",
            "",
            "LOGIN_REDIRECT_URL = \"/accounts/profile/\"",
            "",
            "LOGOUT_REDIRECT_URL = None",
            "",
            "# The number of seconds a password reset link is valid for (default: 3 days).",
            "PASSWORD_RESET_TIMEOUT = 60 * 60 * 24 * 3",
            "",
            "# the first hasher in this list is the preferred algorithm.  any",
            "# password using different algorithms will be converted automatically",
            "# upon login",
            "PASSWORD_HASHERS = [",
            "    \"django.contrib.auth.hashers.PBKDF2PasswordHasher\",",
            "    \"django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher\",",
            "    \"django.contrib.auth.hashers.Argon2PasswordHasher\",",
            "    \"django.contrib.auth.hashers.BCryptSHA256PasswordHasher\",",
            "    \"django.contrib.auth.hashers.ScryptPasswordHasher\",",
            "]",
            "",
            "AUTH_PASSWORD_VALIDATORS = []",
            "",
            "###########",
            "# SIGNING #",
            "###########",
            "",
            "SIGNING_BACKEND = \"django.core.signing.TimestampSigner\"",
            "",
            "########",
            "# CSRF #",
            "########",
            "",
            "# Dotted path to callable to be used as view when a request is",
            "# rejected by the CSRF middleware.",
            "CSRF_FAILURE_VIEW = \"django.views.csrf.csrf_failure\"",
            "",
            "# Settings for CSRF cookie.",
            "CSRF_COOKIE_NAME = \"csrftoken\"",
            "CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52",
            "CSRF_COOKIE_DOMAIN = None",
            "CSRF_COOKIE_PATH = \"/\"",
            "CSRF_COOKIE_SECURE = False",
            "CSRF_COOKIE_HTTPONLY = False",
            "CSRF_COOKIE_SAMESITE = \"Lax\"",
            "CSRF_HEADER_NAME = \"HTTP_X_CSRFTOKEN\"",
            "CSRF_TRUSTED_ORIGINS = []",
            "CSRF_USE_SESSIONS = False",
            "",
            "# Whether to mask CSRF cookie value. It's a transitional setting helpful in",
            "# migrating multiple instance of the same project to Django 4.1+.",
            "CSRF_COOKIE_MASKED = False",
            "",
            "############",
            "# MESSAGES #",
            "############",
            "",
            "# Class to use as messages backend",
            "MESSAGE_STORAGE = \"django.contrib.messages.storage.fallback.FallbackStorage\"",
            "",
            "# Default values of MESSAGE_LEVEL and MESSAGE_TAGS are defined within",
            "# django.contrib.messages to avoid imports in this settings file.",
            "",
            "###########",
            "# LOGGING #",
            "###########",
            "",
            "# The callable to use to configure logging",
            "LOGGING_CONFIG = \"logging.config.dictConfig\"",
            "",
            "# Custom logging configuration.",
            "LOGGING = {}",
            "",
            "# Default exception reporter class used in case none has been",
            "# specifically assigned to the HttpRequest instance.",
            "DEFAULT_EXCEPTION_REPORTER = \"django.views.debug.ExceptionReporter\"",
            "",
            "# Default exception reporter filter class used in case none has been",
            "# specifically assigned to the HttpRequest instance.",
            "DEFAULT_EXCEPTION_REPORTER_FILTER = \"django.views.debug.SafeExceptionReporterFilter\"",
            "",
            "###########",
            "# TESTING #",
            "###########",
            "",
            "# The name of the class to use to run the test suite",
            "TEST_RUNNER = \"django.test.runner.DiscoverRunner\"",
            "",
            "# Apps that don't need to be serialized at test database creation time",
            "# (only apps with migrations are to start with)",
            "TEST_NON_SERIALIZED_APPS = []",
            "",
            "############",
            "# FIXTURES #",
            "############",
            "",
            "# The list of directories to search for fixtures",
            "FIXTURE_DIRS = []",
            "",
            "###############",
            "# STATICFILES #",
            "###############",
            "",
            "# A list of locations of additional static files",
            "STATICFILES_DIRS = []",
            "",
            "# The default file storage backend used during the build process",
            "STATICFILES_STORAGE = \"django.contrib.staticfiles.storage.StaticFilesStorage\"",
            "",
            "# List of finder classes that know how to find static files in",
            "# various locations.",
            "STATICFILES_FINDERS = [",
            "    \"django.contrib.staticfiles.finders.FileSystemFinder\",",
            "    \"django.contrib.staticfiles.finders.AppDirectoriesFinder\",",
            "    # 'django.contrib.staticfiles.finders.DefaultStorageFinder',",
            "]",
            "",
            "##############",
            "# MIGRATIONS #",
            "##############",
            "",
            "# Migration module overrides for apps, by app label.",
            "MIGRATION_MODULES = {}",
            "",
            "#################",
            "# SYSTEM CHECKS #",
            "#################",
            "",
            "# List of all issues generated by system checks that should be silenced. Light",
            "# issues like warnings, infos or debugs will not generate a message. Silencing",
            "# serious issues like errors and criticals does not result in hiding the",
            "# message, but Django will not stop you from e.g. running server.",
            "SILENCED_SYSTEM_CHECKS = []",
            "",
            "#######################",
            "# SECURITY MIDDLEWARE #",
            "#######################",
            "SECURE_CONTENT_TYPE_NOSNIFF = True",
            "SECURE_CROSS_ORIGIN_OPENER_POLICY = \"same-origin\"",
            "SECURE_HSTS_INCLUDE_SUBDOMAINS = False",
            "SECURE_HSTS_PRELOAD = False",
            "SECURE_HSTS_SECONDS = 0",
            "SECURE_REDIRECT_EXEMPT = []",
            "SECURE_REFERRER_POLICY = \"same-origin\"",
            "SECURE_SSL_HOST = None",
            "SECURE_SSL_REDIRECT = False"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Default Django settings. Override these with settings in the module pointed to",
            "by the DJANGO_SETTINGS_MODULE environment variable.",
            "\"\"\"",
            "",
            "",
            "# This is defined here as a do-nothing function because we can't import",
            "# django.utils.translation -- that module depends on the settings.",
            "def gettext_noop(s):",
            "    return s",
            "",
            "",
            "####################",
            "# CORE             #",
            "####################",
            "",
            "DEBUG = False",
            "",
            "# Whether the framework should propagate raw exceptions rather than catching",
            "# them. This is useful under some testing situations and should never be used",
            "# on a live site.",
            "DEBUG_PROPAGATE_EXCEPTIONS = False",
            "",
            "# People who get code error notifications. In the format",
            "# [('Full Name', 'email@example.com'), ('Full Name', 'anotheremail@example.com')]",
            "ADMINS = []",
            "",
            "# List of IP addresses, as strings, that:",
            "#   * See debug comments, when DEBUG is true",
            "#   * Receive x-headers",
            "INTERNAL_IPS = []",
            "",
            "# Hosts/domain names that are valid for this site.",
            "# \"*\" matches anything, \".example.com\" matches example.com and all subdomains",
            "ALLOWED_HOSTS = []",
            "",
            "# Local time zone for this installation. All choices can be found here:",
            "# https://en.wikipedia.org/wiki/List_of_tz_zones_by_name (although not all",
            "# systems may support all possibilities). When USE_TZ is True, this is",
            "# interpreted as the default user time zone.",
            "TIME_ZONE = \"America/Chicago\"",
            "",
            "# If you set this to True, Django will use timezone-aware datetimes.",
            "USE_TZ = False",
            "",
            "# RemovedInDjango50Warning: It's a transitional setting helpful in migrating",
            "# from pytz tzinfo to ZoneInfo(). Set True to continue using pytz tzinfo",
            "# objects during the Django 4.x release cycle.",
            "USE_DEPRECATED_PYTZ = False",
            "",
            "# Language code for this installation. All choices can be found here:",
            "# http://www.i18nguy.com/unicode/language-identifiers.html",
            "LANGUAGE_CODE = \"en-us\"",
            "",
            "# Languages we provide translations for, out of the box.",
            "LANGUAGES = [",
            "    (\"af\", gettext_noop(\"Afrikaans\")),",
            "    (\"ar\", gettext_noop(\"Arabic\")),",
            "    (\"ar-dz\", gettext_noop(\"Algerian Arabic\")),",
            "    (\"ast\", gettext_noop(\"Asturian\")),",
            "    (\"az\", gettext_noop(\"Azerbaijani\")),",
            "    (\"bg\", gettext_noop(\"Bulgarian\")),",
            "    (\"be\", gettext_noop(\"Belarusian\")),",
            "    (\"bn\", gettext_noop(\"Bengali\")),",
            "    (\"br\", gettext_noop(\"Breton\")),",
            "    (\"bs\", gettext_noop(\"Bosnian\")),",
            "    (\"ca\", gettext_noop(\"Catalan\")),",
            "    (\"cs\", gettext_noop(\"Czech\")),",
            "    (\"cy\", gettext_noop(\"Welsh\")),",
            "    (\"da\", gettext_noop(\"Danish\")),",
            "    (\"de\", gettext_noop(\"German\")),",
            "    (\"dsb\", gettext_noop(\"Lower Sorbian\")),",
            "    (\"el\", gettext_noop(\"Greek\")),",
            "    (\"en\", gettext_noop(\"English\")),",
            "    (\"en-au\", gettext_noop(\"Australian English\")),",
            "    (\"en-gb\", gettext_noop(\"British English\")),",
            "    (\"eo\", gettext_noop(\"Esperanto\")),",
            "    (\"es\", gettext_noop(\"Spanish\")),",
            "    (\"es-ar\", gettext_noop(\"Argentinian Spanish\")),",
            "    (\"es-co\", gettext_noop(\"Colombian Spanish\")),",
            "    (\"es-mx\", gettext_noop(\"Mexican Spanish\")),",
            "    (\"es-ni\", gettext_noop(\"Nicaraguan Spanish\")),",
            "    (\"es-ve\", gettext_noop(\"Venezuelan Spanish\")),",
            "    (\"et\", gettext_noop(\"Estonian\")),",
            "    (\"eu\", gettext_noop(\"Basque\")),",
            "    (\"fa\", gettext_noop(\"Persian\")),",
            "    (\"fi\", gettext_noop(\"Finnish\")),",
            "    (\"fr\", gettext_noop(\"French\")),",
            "    (\"fy\", gettext_noop(\"Frisian\")),",
            "    (\"ga\", gettext_noop(\"Irish\")),",
            "    (\"gd\", gettext_noop(\"Scottish Gaelic\")),",
            "    (\"gl\", gettext_noop(\"Galician\")),",
            "    (\"he\", gettext_noop(\"Hebrew\")),",
            "    (\"hi\", gettext_noop(\"Hindi\")),",
            "    (\"hr\", gettext_noop(\"Croatian\")),",
            "    (\"hsb\", gettext_noop(\"Upper Sorbian\")),",
            "    (\"hu\", gettext_noop(\"Hungarian\")),",
            "    (\"hy\", gettext_noop(\"Armenian\")),",
            "    (\"ia\", gettext_noop(\"Interlingua\")),",
            "    (\"id\", gettext_noop(\"Indonesian\")),",
            "    (\"ig\", gettext_noop(\"Igbo\")),",
            "    (\"io\", gettext_noop(\"Ido\")),",
            "    (\"is\", gettext_noop(\"Icelandic\")),",
            "    (\"it\", gettext_noop(\"Italian\")),",
            "    (\"ja\", gettext_noop(\"Japanese\")),",
            "    (\"ka\", gettext_noop(\"Georgian\")),",
            "    (\"kab\", gettext_noop(\"Kabyle\")),",
            "    (\"kk\", gettext_noop(\"Kazakh\")),",
            "    (\"km\", gettext_noop(\"Khmer\")),",
            "    (\"kn\", gettext_noop(\"Kannada\")),",
            "    (\"ko\", gettext_noop(\"Korean\")),",
            "    (\"ky\", gettext_noop(\"Kyrgyz\")),",
            "    (\"lb\", gettext_noop(\"Luxembourgish\")),",
            "    (\"lt\", gettext_noop(\"Lithuanian\")),",
            "    (\"lv\", gettext_noop(\"Latvian\")),",
            "    (\"mk\", gettext_noop(\"Macedonian\")),",
            "    (\"ml\", gettext_noop(\"Malayalam\")),",
            "    (\"mn\", gettext_noop(\"Mongolian\")),",
            "    (\"mr\", gettext_noop(\"Marathi\")),",
            "    (\"ms\", gettext_noop(\"Malay\")),",
            "    (\"my\", gettext_noop(\"Burmese\")),",
            "    (\"nb\", gettext_noop(\"Norwegian Bokm\u00e5l\")),",
            "    (\"ne\", gettext_noop(\"Nepali\")),",
            "    (\"nl\", gettext_noop(\"Dutch\")),",
            "    (\"nn\", gettext_noop(\"Norwegian Nynorsk\")),",
            "    (\"os\", gettext_noop(\"Ossetic\")),",
            "    (\"pa\", gettext_noop(\"Punjabi\")),",
            "    (\"pl\", gettext_noop(\"Polish\")),",
            "    (\"pt\", gettext_noop(\"Portuguese\")),",
            "    (\"pt-br\", gettext_noop(\"Brazilian Portuguese\")),",
            "    (\"ro\", gettext_noop(\"Romanian\")),",
            "    (\"ru\", gettext_noop(\"Russian\")),",
            "    (\"sk\", gettext_noop(\"Slovak\")),",
            "    (\"sl\", gettext_noop(\"Slovenian\")),",
            "    (\"sq\", gettext_noop(\"Albanian\")),",
            "    (\"sr\", gettext_noop(\"Serbian\")),",
            "    (\"sr-latn\", gettext_noop(\"Serbian Latin\")),",
            "    (\"sv\", gettext_noop(\"Swedish\")),",
            "    (\"sw\", gettext_noop(\"Swahili\")),",
            "    (\"ta\", gettext_noop(\"Tamil\")),",
            "    (\"te\", gettext_noop(\"Telugu\")),",
            "    (\"tg\", gettext_noop(\"Tajik\")),",
            "    (\"th\", gettext_noop(\"Thai\")),",
            "    (\"tk\", gettext_noop(\"Turkmen\")),",
            "    (\"tr\", gettext_noop(\"Turkish\")),",
            "    (\"tt\", gettext_noop(\"Tatar\")),",
            "    (\"udm\", gettext_noop(\"Udmurt\")),",
            "    (\"uk\", gettext_noop(\"Ukrainian\")),",
            "    (\"ur\", gettext_noop(\"Urdu\")),",
            "    (\"uz\", gettext_noop(\"Uzbek\")),",
            "    (\"vi\", gettext_noop(\"Vietnamese\")),",
            "    (\"zh-hans\", gettext_noop(\"Simplified Chinese\")),",
            "    (\"zh-hant\", gettext_noop(\"Traditional Chinese\")),",
            "]",
            "",
            "# Languages using BiDi (right-to-left) layout",
            "LANGUAGES_BIDI = [\"he\", \"ar\", \"ar-dz\", \"fa\", \"ur\"]",
            "",
            "# If you set this to False, Django will make some optimizations so as not",
            "# to load the internationalization machinery.",
            "USE_I18N = True",
            "LOCALE_PATHS = []",
            "",
            "# Settings for language cookie",
            "LANGUAGE_COOKIE_NAME = \"django_language\"",
            "LANGUAGE_COOKIE_AGE = None",
            "LANGUAGE_COOKIE_DOMAIN = None",
            "LANGUAGE_COOKIE_PATH = \"/\"",
            "LANGUAGE_COOKIE_SECURE = False",
            "LANGUAGE_COOKIE_HTTPONLY = False",
            "LANGUAGE_COOKIE_SAMESITE = None",
            "",
            "",
            "# If you set this to True, Django will format dates, numbers and calendars",
            "# according to user current locale.",
            "USE_L10N = True",
            "",
            "# Not-necessarily-technical managers of the site. They get broken link",
            "# notifications and other various emails.",
            "MANAGERS = ADMINS",
            "",
            "# Default charset to use for all HttpResponse objects, if a MIME type isn't",
            "# manually specified. It's used to construct the Content-Type header.",
            "DEFAULT_CHARSET = \"utf-8\"",
            "",
            "# Email address that error messages come from.",
            "SERVER_EMAIL = \"root@localhost\"",
            "",
            "# Database connection info. If left empty, will default to the dummy backend.",
            "DATABASES = {}",
            "",
            "# Classes used to implement DB routing behavior.",
            "DATABASE_ROUTERS = []",
            "",
            "# The email backend to use. For possible shortcuts see django.core.mail.",
            "# The default is to use the SMTP backend.",
            "# Third-party backends can be specified by providing a Python path",
            "# to a module that defines an EmailBackend class.",
            "EMAIL_BACKEND = \"django.core.mail.backends.smtp.EmailBackend\"",
            "",
            "# Host for sending email.",
            "EMAIL_HOST = \"localhost\"",
            "",
            "# Port for sending email.",
            "EMAIL_PORT = 25",
            "",
            "# Whether to send SMTP 'Date' header in the local time zone or in UTC.",
            "EMAIL_USE_LOCALTIME = False",
            "",
            "# Optional SMTP authentication information for EMAIL_HOST.",
            "EMAIL_HOST_USER = \"\"",
            "EMAIL_HOST_PASSWORD = \"\"",
            "EMAIL_USE_TLS = False",
            "EMAIL_USE_SSL = False",
            "EMAIL_SSL_CERTFILE = None",
            "EMAIL_SSL_KEYFILE = None",
            "EMAIL_TIMEOUT = None",
            "",
            "# List of strings representing installed apps.",
            "INSTALLED_APPS = []",
            "",
            "TEMPLATES = []",
            "",
            "# Default form rendering class.",
            "FORM_RENDERER = \"django.forms.renderers.DjangoTemplates\"",
            "",
            "# Default email address to use for various automated correspondence from",
            "# the site managers.",
            "DEFAULT_FROM_EMAIL = \"webmaster@localhost\"",
            "",
            "# Subject-line prefix for email messages send with django.core.mail.mail_admins",
            "# or ...mail_managers.  Make sure to include the trailing space.",
            "EMAIL_SUBJECT_PREFIX = \"[Django] \"",
            "",
            "# Whether to append trailing slashes to URLs.",
            "APPEND_SLASH = True",
            "",
            "# Whether to prepend the \"www.\" subdomain to URLs that don't have it.",
            "PREPEND_WWW = False",
            "",
            "# Override the server-derived value of SCRIPT_NAME",
            "FORCE_SCRIPT_NAME = None",
            "",
            "# List of compiled regular expression objects representing User-Agent strings",
            "# that are not allowed to visit any page, systemwide. Use this for bad",
            "# robots/crawlers. Here are a few examples:",
            "#     import re",
            "#     DISALLOWED_USER_AGENTS = [",
            "#         re.compile(r'^NaverBot.*'),",
            "#         re.compile(r'^EmailSiphon.*'),",
            "#         re.compile(r'^SiteSucker.*'),",
            "#         re.compile(r'^sohu-search'),",
            "#     ]",
            "DISALLOWED_USER_AGENTS = []",
            "",
            "ABSOLUTE_URL_OVERRIDES = {}",
            "",
            "# List of compiled regular expression objects representing URLs that need not",
            "# be reported by BrokenLinkEmailsMiddleware. Here are a few examples:",
            "#    import re",
            "#    IGNORABLE_404_URLS = [",
            "#        re.compile(r'^/apple-touch-icon.*\\.png$'),",
            "#        re.compile(r'^/favicon.ico$'),",
            "#        re.compile(r'^/robots.txt$'),",
            "#        re.compile(r'^/phpmyadmin/'),",
            "#        re.compile(r'\\.(cgi|php|pl)$'),",
            "#    ]",
            "IGNORABLE_404_URLS = []",
            "",
            "# A secret key for this particular Django installation. Used in secret-key",
            "# hashing algorithms. Set this in your settings, or Django will complain",
            "# loudly.",
            "SECRET_KEY = \"\"",
            "",
            "# List of secret keys used to verify the validity of signatures. This allows",
            "# secret key rotation.",
            "SECRET_KEY_FALLBACKS = []",
            "",
            "# Default file storage mechanism that holds media.",
            "DEFAULT_FILE_STORAGE = \"django.core.files.storage.FileSystemStorage\"",
            "",
            "# Absolute filesystem path to the directory that will hold user-uploaded files.",
            "# Example: \"/var/www/example.com/media/\"",
            "MEDIA_ROOT = \"\"",
            "",
            "# URL that handles the media served from MEDIA_ROOT.",
            "# Examples: \"http://example.com/media/\", \"http://media.example.com/\"",
            "MEDIA_URL = \"\"",
            "",
            "# Absolute path to the directory static files should be collected to.",
            "# Example: \"/var/www/example.com/static/\"",
            "STATIC_ROOT = None",
            "",
            "# URL that handles the static files served from STATIC_ROOT.",
            "# Example: \"http://example.com/static/\", \"http://static.example.com/\"",
            "STATIC_URL = None",
            "",
            "# List of upload handler classes to be applied in order.",
            "FILE_UPLOAD_HANDLERS = [",
            "    \"django.core.files.uploadhandler.MemoryFileUploadHandler\",",
            "    \"django.core.files.uploadhandler.TemporaryFileUploadHandler\",",
            "]",
            "",
            "# Maximum size, in bytes, of a request before it will be streamed to the",
            "# file system instead of into memory.",
            "FILE_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB",
            "",
            "# Maximum size in bytes of request data (excluding file uploads) that will be",
            "# read before a SuspiciousOperation (RequestDataTooBig) is raised.",
            "DATA_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB",
            "",
            "# Maximum number of GET/POST parameters that will be read before a",
            "# SuspiciousOperation (TooManyFieldsSent) is raised.",
            "DATA_UPLOAD_MAX_NUMBER_FIELDS = 1000",
            "",
            "# Maximum number of files encoded in a multipart upload that will be read",
            "# before a SuspiciousOperation (TooManyFilesSent) is raised.",
            "DATA_UPLOAD_MAX_NUMBER_FILES = 100",
            "",
            "# Directory in which upload streamed files will be temporarily saved. A value of",
            "# `None` will make Django use the operating system's default temporary directory",
            "# (i.e. \"/tmp\" on *nix systems).",
            "FILE_UPLOAD_TEMP_DIR = None",
            "",
            "# The numeric mode to set newly-uploaded files to. The value should be a mode",
            "# you'd pass directly to os.chmod; see",
            "# https://docs.python.org/library/os.html#files-and-directories.",
            "FILE_UPLOAD_PERMISSIONS = 0o644",
            "",
            "# The numeric mode to assign to newly-created directories, when uploading files.",
            "# The value should be a mode as you'd pass to os.chmod;",
            "# see https://docs.python.org/library/os.html#files-and-directories.",
            "FILE_UPLOAD_DIRECTORY_PERMISSIONS = None",
            "",
            "# Python module path where user will place custom format definition.",
            "# The directory where this setting is pointing should contain subdirectories",
            "# named as the locales, containing a formats.py file",
            "# (i.e. \"myproject.locale\" for myproject/locale/en/formats.py etc. use)",
            "FORMAT_MODULE_PATH = None",
            "",
            "# Default formatting for date objects. See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "DATE_FORMAT = \"N j, Y\"",
            "",
            "# Default formatting for datetime objects. See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "DATETIME_FORMAT = \"N j, Y, P\"",
            "",
            "# Default formatting for time objects. See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "TIME_FORMAT = \"P\"",
            "",
            "# Default formatting for date objects when only the year and month are relevant.",
            "# See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "YEAR_MONTH_FORMAT = \"F Y\"",
            "",
            "# Default formatting for date objects when only the month and day are relevant.",
            "# See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "MONTH_DAY_FORMAT = \"F j\"",
            "",
            "# Default short formatting for date objects. See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "SHORT_DATE_FORMAT = \"m/d/Y\"",
            "",
            "# Default short formatting for datetime objects.",
            "# See all available format strings here:",
            "# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date",
            "SHORT_DATETIME_FORMAT = \"m/d/Y P\"",
            "",
            "# Default formats to be used when parsing dates from input boxes, in order",
            "# See all available format string here:",
            "# https://docs.python.org/library/datetime.html#strftime-behavior",
            "# * Note that these format strings are different from the ones to display dates",
            "DATE_INPUT_FORMATS = [",
            "    \"%Y-%m-%d\",  # '2006-10-25'",
            "    \"%m/%d/%Y\",  # '10/25/2006'",
            "    \"%m/%d/%y\",  # '10/25/06'",
            "    \"%b %d %Y\",  # 'Oct 25 2006'",
            "    \"%b %d, %Y\",  # 'Oct 25, 2006'",
            "    \"%d %b %Y\",  # '25 Oct 2006'",
            "    \"%d %b, %Y\",  # '25 Oct, 2006'",
            "    \"%B %d %Y\",  # 'October 25 2006'",
            "    \"%B %d, %Y\",  # 'October 25, 2006'",
            "    \"%d %B %Y\",  # '25 October 2006'",
            "    \"%d %B, %Y\",  # '25 October, 2006'",
            "]",
            "",
            "# Default formats to be used when parsing times from input boxes, in order",
            "# See all available format string here:",
            "# https://docs.python.org/library/datetime.html#strftime-behavior",
            "# * Note that these format strings are different from the ones to display dates",
            "TIME_INPUT_FORMATS = [",
            "    \"%H:%M:%S\",  # '14:30:59'",
            "    \"%H:%M:%S.%f\",  # '14:30:59.000200'",
            "    \"%H:%M\",  # '14:30'",
            "]",
            "",
            "# Default formats to be used when parsing dates and times from input boxes,",
            "# in order",
            "# See all available format string here:",
            "# https://docs.python.org/library/datetime.html#strftime-behavior",
            "# * Note that these format strings are different from the ones to display dates",
            "DATETIME_INPUT_FORMATS = [",
            "    \"%Y-%m-%d %H:%M:%S\",  # '2006-10-25 14:30:59'",
            "    \"%Y-%m-%d %H:%M:%S.%f\",  # '2006-10-25 14:30:59.000200'",
            "    \"%Y-%m-%d %H:%M\",  # '2006-10-25 14:30'",
            "    \"%m/%d/%Y %H:%M:%S\",  # '10/25/2006 14:30:59'",
            "    \"%m/%d/%Y %H:%M:%S.%f\",  # '10/25/2006 14:30:59.000200'",
            "    \"%m/%d/%Y %H:%M\",  # '10/25/2006 14:30'",
            "    \"%m/%d/%y %H:%M:%S\",  # '10/25/06 14:30:59'",
            "    \"%m/%d/%y %H:%M:%S.%f\",  # '10/25/06 14:30:59.000200'",
            "    \"%m/%d/%y %H:%M\",  # '10/25/06 14:30'",
            "]",
            "",
            "# First day of week, to be used on calendars",
            "# 0 means Sunday, 1 means Monday...",
            "FIRST_DAY_OF_WEEK = 0",
            "",
            "# Decimal separator symbol",
            "DECIMAL_SEPARATOR = \".\"",
            "",
            "# Boolean that sets whether to add thousand separator when formatting numbers",
            "USE_THOUSAND_SEPARATOR = False",
            "",
            "# Number of digits that will be together, when splitting them by",
            "# THOUSAND_SEPARATOR. 0 means no grouping, 3 means splitting by thousands...",
            "NUMBER_GROUPING = 0",
            "",
            "# Thousand separator symbol",
            "THOUSAND_SEPARATOR = \",\"",
            "",
            "# The tablespaces to use for each model when not specified otherwise.",
            "DEFAULT_TABLESPACE = \"\"",
            "DEFAULT_INDEX_TABLESPACE = \"\"",
            "",
            "# Default primary key field type.",
            "DEFAULT_AUTO_FIELD = \"django.db.models.AutoField\"",
            "",
            "# Default X-Frame-Options header value",
            "X_FRAME_OPTIONS = \"DENY\"",
            "",
            "USE_X_FORWARDED_HOST = False",
            "USE_X_FORWARDED_PORT = False",
            "",
            "# The Python dotted path to the WSGI application that Django's internal server",
            "# (runserver) will use. If `None`, the return value of",
            "# 'django.core.wsgi.get_wsgi_application' is used, thus preserving the same",
            "# behavior as previous versions of Django. Otherwise this should point to an",
            "# actual WSGI application object.",
            "WSGI_APPLICATION = None",
            "",
            "# If your Django app is behind a proxy that sets a header to specify secure",
            "# connections, AND that proxy ensures that user-submitted headers with the",
            "# same name are ignored (so that people can't spoof it), set this value to",
            "# a tuple of (header_name, header_value). For any requests that come in with",
            "# that header/value, request.is_secure() will return True.",
            "# WARNING! Only set this if you fully understand what you're doing. Otherwise,",
            "# you may be opening yourself up to a security risk.",
            "SECURE_PROXY_SSL_HEADER = None",
            "",
            "##############",
            "# MIDDLEWARE #",
            "##############",
            "",
            "# List of middleware to use. Order is important; in the request phase, these",
            "# middleware will be applied in the order given, and in the response",
            "# phase the middleware will be applied in reverse order.",
            "MIDDLEWARE = []",
            "",
            "############",
            "# SESSIONS #",
            "############",
            "",
            "# Cache to store session data if using the cache session backend.",
            "SESSION_CACHE_ALIAS = \"default\"",
            "# Cookie name. This can be whatever you want.",
            "SESSION_COOKIE_NAME = \"sessionid\"",
            "# Age of cookie, in seconds (default: 2 weeks).",
            "SESSION_COOKIE_AGE = 60 * 60 * 24 * 7 * 2",
            "# A string like \"example.com\", or None for standard domain cookie.",
            "SESSION_COOKIE_DOMAIN = None",
            "# Whether the session cookie should be secure (https:// only).",
            "SESSION_COOKIE_SECURE = False",
            "# The path of the session cookie.",
            "SESSION_COOKIE_PATH = \"/\"",
            "# Whether to use the HttpOnly flag.",
            "SESSION_COOKIE_HTTPONLY = True",
            "# Whether to set the flag restricting cookie leaks on cross-site requests.",
            "# This can be 'Lax', 'Strict', 'None', or False to disable the flag.",
            "SESSION_COOKIE_SAMESITE = \"Lax\"",
            "# Whether to save the session data on every request.",
            "SESSION_SAVE_EVERY_REQUEST = False",
            "# Whether a user's session cookie expires when the web browser is closed.",
            "SESSION_EXPIRE_AT_BROWSER_CLOSE = False",
            "# The module to store session data",
            "SESSION_ENGINE = \"django.contrib.sessions.backends.db\"",
            "# Directory to store session files if using the file session module. If None,",
            "# the backend will use a sensible default.",
            "SESSION_FILE_PATH = None",
            "# class to serialize session data",
            "SESSION_SERIALIZER = \"django.contrib.sessions.serializers.JSONSerializer\"",
            "",
            "#########",
            "# CACHE #",
            "#########",
            "",
            "# The cache backends to use.",
            "CACHES = {",
            "    \"default\": {",
            "        \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",",
            "    }",
            "}",
            "CACHE_MIDDLEWARE_KEY_PREFIX = \"\"",
            "CACHE_MIDDLEWARE_SECONDS = 600",
            "CACHE_MIDDLEWARE_ALIAS = \"default\"",
            "",
            "##################",
            "# AUTHENTICATION #",
            "##################",
            "",
            "AUTH_USER_MODEL = \"auth.User\"",
            "",
            "AUTHENTICATION_BACKENDS = [\"django.contrib.auth.backends.ModelBackend\"]",
            "",
            "LOGIN_URL = \"/accounts/login/\"",
            "",
            "LOGIN_REDIRECT_URL = \"/accounts/profile/\"",
            "",
            "LOGOUT_REDIRECT_URL = None",
            "",
            "# The number of seconds a password reset link is valid for (default: 3 days).",
            "PASSWORD_RESET_TIMEOUT = 60 * 60 * 24 * 3",
            "",
            "# the first hasher in this list is the preferred algorithm.  any",
            "# password using different algorithms will be converted automatically",
            "# upon login",
            "PASSWORD_HASHERS = [",
            "    \"django.contrib.auth.hashers.PBKDF2PasswordHasher\",",
            "    \"django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher\",",
            "    \"django.contrib.auth.hashers.Argon2PasswordHasher\",",
            "    \"django.contrib.auth.hashers.BCryptSHA256PasswordHasher\",",
            "    \"django.contrib.auth.hashers.ScryptPasswordHasher\",",
            "]",
            "",
            "AUTH_PASSWORD_VALIDATORS = []",
            "",
            "###########",
            "# SIGNING #",
            "###########",
            "",
            "SIGNING_BACKEND = \"django.core.signing.TimestampSigner\"",
            "",
            "########",
            "# CSRF #",
            "########",
            "",
            "# Dotted path to callable to be used as view when a request is",
            "# rejected by the CSRF middleware.",
            "CSRF_FAILURE_VIEW = \"django.views.csrf.csrf_failure\"",
            "",
            "# Settings for CSRF cookie.",
            "CSRF_COOKIE_NAME = \"csrftoken\"",
            "CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52",
            "CSRF_COOKIE_DOMAIN = None",
            "CSRF_COOKIE_PATH = \"/\"",
            "CSRF_COOKIE_SECURE = False",
            "CSRF_COOKIE_HTTPONLY = False",
            "CSRF_COOKIE_SAMESITE = \"Lax\"",
            "CSRF_HEADER_NAME = \"HTTP_X_CSRFTOKEN\"",
            "CSRF_TRUSTED_ORIGINS = []",
            "CSRF_USE_SESSIONS = False",
            "",
            "# Whether to mask CSRF cookie value. It's a transitional setting helpful in",
            "# migrating multiple instance of the same project to Django 4.1+.",
            "CSRF_COOKIE_MASKED = False",
            "",
            "############",
            "# MESSAGES #",
            "############",
            "",
            "# Class to use as messages backend",
            "MESSAGE_STORAGE = \"django.contrib.messages.storage.fallback.FallbackStorage\"",
            "",
            "# Default values of MESSAGE_LEVEL and MESSAGE_TAGS are defined within",
            "# django.contrib.messages to avoid imports in this settings file.",
            "",
            "###########",
            "# LOGGING #",
            "###########",
            "",
            "# The callable to use to configure logging",
            "LOGGING_CONFIG = \"logging.config.dictConfig\"",
            "",
            "# Custom logging configuration.",
            "LOGGING = {}",
            "",
            "# Default exception reporter class used in case none has been",
            "# specifically assigned to the HttpRequest instance.",
            "DEFAULT_EXCEPTION_REPORTER = \"django.views.debug.ExceptionReporter\"",
            "",
            "# Default exception reporter filter class used in case none has been",
            "# specifically assigned to the HttpRequest instance.",
            "DEFAULT_EXCEPTION_REPORTER_FILTER = \"django.views.debug.SafeExceptionReporterFilter\"",
            "",
            "###########",
            "# TESTING #",
            "###########",
            "",
            "# The name of the class to use to run the test suite",
            "TEST_RUNNER = \"django.test.runner.DiscoverRunner\"",
            "",
            "# Apps that don't need to be serialized at test database creation time",
            "# (only apps with migrations are to start with)",
            "TEST_NON_SERIALIZED_APPS = []",
            "",
            "############",
            "# FIXTURES #",
            "############",
            "",
            "# The list of directories to search for fixtures",
            "FIXTURE_DIRS = []",
            "",
            "###############",
            "# STATICFILES #",
            "###############",
            "",
            "# A list of locations of additional static files",
            "STATICFILES_DIRS = []",
            "",
            "# The default file storage backend used during the build process",
            "STATICFILES_STORAGE = \"django.contrib.staticfiles.storage.StaticFilesStorage\"",
            "",
            "# List of finder classes that know how to find static files in",
            "# various locations.",
            "STATICFILES_FINDERS = [",
            "    \"django.contrib.staticfiles.finders.FileSystemFinder\",",
            "    \"django.contrib.staticfiles.finders.AppDirectoriesFinder\",",
            "    # 'django.contrib.staticfiles.finders.DefaultStorageFinder',",
            "]",
            "",
            "##############",
            "# MIGRATIONS #",
            "##############",
            "",
            "# Migration module overrides for apps, by app label.",
            "MIGRATION_MODULES = {}",
            "",
            "#################",
            "# SYSTEM CHECKS #",
            "#################",
            "",
            "# List of all issues generated by system checks that should be silenced. Light",
            "# issues like warnings, infos or debugs will not generate a message. Silencing",
            "# serious issues like errors and criticals does not result in hiding the",
            "# message, but Django will not stop you from e.g. running server.",
            "SILENCED_SYSTEM_CHECKS = []",
            "",
            "#######################",
            "# SECURITY MIDDLEWARE #",
            "#######################",
            "SECURE_CONTENT_TYPE_NOSNIFF = True",
            "SECURE_CROSS_ORIGIN_OPENER_POLICY = \"same-origin\"",
            "SECURE_HSTS_INCLUDE_SUBDOMAINS = False",
            "SECURE_HSTS_PRELOAD = False",
            "SECURE_HSTS_SECONDS = 0",
            "SECURE_REDIRECT_EXEMPT = []",
            "SECURE_REFERRER_POLICY = \"same-origin\"",
            "SECURE_SSL_HOST = None",
            "SECURE_SSL_REDIRECT = False"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "django/core/exceptions.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "     pass"
            },
            "1": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 68,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 69,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+class TooManyFilesSent(SuspiciousOperation):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+    \"\"\""
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+    The number of fields in a GET or POST request exceeded"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+    settings.DATA_UPLOAD_MAX_NUMBER_FILES."
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+    \"\"\""
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+    pass"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 79,
                "PatchRowcode": " class RequestDataTooBig(SuspiciousOperation):"
            },
            "13": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "     \"\"\""
            },
            "14": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "     The size of the request (excluding any file uploads) exceeded"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Global Django exception and warning classes.",
            "\"\"\"",
            "import operator",
            "",
            "from django.utils.hashable import make_hashable",
            "",
            "",
            "class FieldDoesNotExist(Exception):",
            "    \"\"\"The requested model field does not exist\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class AppRegistryNotReady(Exception):",
            "    \"\"\"The django.apps registry is not populated yet\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class ObjectDoesNotExist(Exception):",
            "    \"\"\"The requested object does not exist\"\"\"",
            "",
            "    silent_variable_failure = True",
            "",
            "",
            "class MultipleObjectsReturned(Exception):",
            "    \"\"\"The query returned multiple objects when only one was expected.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class SuspiciousOperation(Exception):",
            "    \"\"\"The user did something suspicious\"\"\"",
            "",
            "",
            "class SuspiciousMultipartForm(SuspiciousOperation):",
            "    \"\"\"Suspect MIME request in multipart form data\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class SuspiciousFileOperation(SuspiciousOperation):",
            "    \"\"\"A Suspicious filesystem operation was attempted\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class DisallowedHost(SuspiciousOperation):",
            "    \"\"\"HTTP_HOST header contains invalid value\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class DisallowedRedirect(SuspiciousOperation):",
            "    \"\"\"Redirect to scheme not in allowed list\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class TooManyFieldsSent(SuspiciousOperation):",
            "    \"\"\"",
            "    The number of fields in a GET or POST request exceeded",
            "    settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class RequestDataTooBig(SuspiciousOperation):",
            "    \"\"\"",
            "    The size of the request (excluding any file uploads) exceeded",
            "    settings.DATA_UPLOAD_MAX_MEMORY_SIZE.",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class RequestAborted(Exception):",
            "    \"\"\"The request was closed before it was completed, or timed out.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class BadRequest(Exception):",
            "    \"\"\"The request is malformed and cannot be processed.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class PermissionDenied(Exception):",
            "    \"\"\"The user did not have permission to do that\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class ViewDoesNotExist(Exception):",
            "    \"\"\"The requested view does not exist\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class MiddlewareNotUsed(Exception):",
            "    \"\"\"This middleware is not used in this server configuration\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class ImproperlyConfigured(Exception):",
            "    \"\"\"Django is somehow improperly configured\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class FieldError(Exception):",
            "    \"\"\"Some kind of problem with a model field.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "NON_FIELD_ERRORS = \"__all__\"",
            "",
            "",
            "class ValidationError(Exception):",
            "    \"\"\"An error while validating data.\"\"\"",
            "",
            "    def __init__(self, message, code=None, params=None):",
            "        \"\"\"",
            "        The `message` argument can be a single error, a list of errors, or a",
            "        dictionary that maps field names to lists of errors. What we define as",
            "        an \"error\" can be either a simple string or an instance of",
            "        ValidationError with its message attribute set, and what we define as",
            "        list or dictionary can be an actual `list` or `dict` or an instance",
            "        of ValidationError with its `error_list` or `error_dict` attribute set.",
            "        \"\"\"",
            "        super().__init__(message, code, params)",
            "",
            "        if isinstance(message, ValidationError):",
            "            if hasattr(message, \"error_dict\"):",
            "                message = message.error_dict",
            "            elif not hasattr(message, \"message\"):",
            "                message = message.error_list",
            "            else:",
            "                message, code, params = message.message, message.code, message.params",
            "",
            "        if isinstance(message, dict):",
            "            self.error_dict = {}",
            "            for field, messages in message.items():",
            "                if not isinstance(messages, ValidationError):",
            "                    messages = ValidationError(messages)",
            "                self.error_dict[field] = messages.error_list",
            "",
            "        elif isinstance(message, list):",
            "            self.error_list = []",
            "            for message in message:",
            "                # Normalize plain strings to instances of ValidationError.",
            "                if not isinstance(message, ValidationError):",
            "                    message = ValidationError(message)",
            "                if hasattr(message, \"error_dict\"):",
            "                    self.error_list.extend(sum(message.error_dict.values(), []))",
            "                else:",
            "                    self.error_list.extend(message.error_list)",
            "",
            "        else:",
            "            self.message = message",
            "            self.code = code",
            "            self.params = params",
            "            self.error_list = [self]",
            "",
            "    @property",
            "    def message_dict(self):",
            "        # Trigger an AttributeError if this ValidationError",
            "        # doesn't have an error_dict.",
            "        getattr(self, \"error_dict\")",
            "",
            "        return dict(self)",
            "",
            "    @property",
            "    def messages(self):",
            "        if hasattr(self, \"error_dict\"):",
            "            return sum(dict(self).values(), [])",
            "        return list(self)",
            "",
            "    def update_error_dict(self, error_dict):",
            "        if hasattr(self, \"error_dict\"):",
            "            for field, error_list in self.error_dict.items():",
            "                error_dict.setdefault(field, []).extend(error_list)",
            "        else:",
            "            error_dict.setdefault(NON_FIELD_ERRORS, []).extend(self.error_list)",
            "        return error_dict",
            "",
            "    def __iter__(self):",
            "        if hasattr(self, \"error_dict\"):",
            "            for field, errors in self.error_dict.items():",
            "                yield field, list(ValidationError(errors))",
            "        else:",
            "            for error in self.error_list:",
            "                message = error.message",
            "                if error.params:",
            "                    message %= error.params",
            "                yield str(message)",
            "",
            "    def __str__(self):",
            "        if hasattr(self, \"error_dict\"):",
            "            return repr(dict(self))",
            "        return repr(list(self))",
            "",
            "    def __repr__(self):",
            "        return \"ValidationError(%s)\" % self",
            "",
            "    def __eq__(self, other):",
            "        if not isinstance(other, ValidationError):",
            "            return NotImplemented",
            "        return hash(self) == hash(other)",
            "",
            "    def __hash__(self):",
            "        if hasattr(self, \"message\"):",
            "            return hash(",
            "                (",
            "                    self.message,",
            "                    self.code,",
            "                    make_hashable(self.params),",
            "                )",
            "            )",
            "        if hasattr(self, \"error_dict\"):",
            "            return hash(make_hashable(self.error_dict))",
            "        return hash(tuple(sorted(self.error_list, key=operator.attrgetter(\"message\"))))",
            "",
            "",
            "class EmptyResultSet(Exception):",
            "    \"\"\"A database query predicate is impossible.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class SynchronousOnlyOperation(Exception):",
            "    \"\"\"The user tried to call a sync-only function from an async context.\"\"\"",
            "",
            "    pass"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Global Django exception and warning classes.",
            "\"\"\"",
            "import operator",
            "",
            "from django.utils.hashable import make_hashable",
            "",
            "",
            "class FieldDoesNotExist(Exception):",
            "    \"\"\"The requested model field does not exist\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class AppRegistryNotReady(Exception):",
            "    \"\"\"The django.apps registry is not populated yet\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class ObjectDoesNotExist(Exception):",
            "    \"\"\"The requested object does not exist\"\"\"",
            "",
            "    silent_variable_failure = True",
            "",
            "",
            "class MultipleObjectsReturned(Exception):",
            "    \"\"\"The query returned multiple objects when only one was expected.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class SuspiciousOperation(Exception):",
            "    \"\"\"The user did something suspicious\"\"\"",
            "",
            "",
            "class SuspiciousMultipartForm(SuspiciousOperation):",
            "    \"\"\"Suspect MIME request in multipart form data\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class SuspiciousFileOperation(SuspiciousOperation):",
            "    \"\"\"A Suspicious filesystem operation was attempted\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class DisallowedHost(SuspiciousOperation):",
            "    \"\"\"HTTP_HOST header contains invalid value\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class DisallowedRedirect(SuspiciousOperation):",
            "    \"\"\"Redirect to scheme not in allowed list\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class TooManyFieldsSent(SuspiciousOperation):",
            "    \"\"\"",
            "    The number of fields in a GET or POST request exceeded",
            "    settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class TooManyFilesSent(SuspiciousOperation):",
            "    \"\"\"",
            "    The number of fields in a GET or POST request exceeded",
            "    settings.DATA_UPLOAD_MAX_NUMBER_FILES.",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class RequestDataTooBig(SuspiciousOperation):",
            "    \"\"\"",
            "    The size of the request (excluding any file uploads) exceeded",
            "    settings.DATA_UPLOAD_MAX_MEMORY_SIZE.",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class RequestAborted(Exception):",
            "    \"\"\"The request was closed before it was completed, or timed out.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class BadRequest(Exception):",
            "    \"\"\"The request is malformed and cannot be processed.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class PermissionDenied(Exception):",
            "    \"\"\"The user did not have permission to do that\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class ViewDoesNotExist(Exception):",
            "    \"\"\"The requested view does not exist\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class MiddlewareNotUsed(Exception):",
            "    \"\"\"This middleware is not used in this server configuration\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class ImproperlyConfigured(Exception):",
            "    \"\"\"Django is somehow improperly configured\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class FieldError(Exception):",
            "    \"\"\"Some kind of problem with a model field.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "NON_FIELD_ERRORS = \"__all__\"",
            "",
            "",
            "class ValidationError(Exception):",
            "    \"\"\"An error while validating data.\"\"\"",
            "",
            "    def __init__(self, message, code=None, params=None):",
            "        \"\"\"",
            "        The `message` argument can be a single error, a list of errors, or a",
            "        dictionary that maps field names to lists of errors. What we define as",
            "        an \"error\" can be either a simple string or an instance of",
            "        ValidationError with its message attribute set, and what we define as",
            "        list or dictionary can be an actual `list` or `dict` or an instance",
            "        of ValidationError with its `error_list` or `error_dict` attribute set.",
            "        \"\"\"",
            "        super().__init__(message, code, params)",
            "",
            "        if isinstance(message, ValidationError):",
            "            if hasattr(message, \"error_dict\"):",
            "                message = message.error_dict",
            "            elif not hasattr(message, \"message\"):",
            "                message = message.error_list",
            "            else:",
            "                message, code, params = message.message, message.code, message.params",
            "",
            "        if isinstance(message, dict):",
            "            self.error_dict = {}",
            "            for field, messages in message.items():",
            "                if not isinstance(messages, ValidationError):",
            "                    messages = ValidationError(messages)",
            "                self.error_dict[field] = messages.error_list",
            "",
            "        elif isinstance(message, list):",
            "            self.error_list = []",
            "            for message in message:",
            "                # Normalize plain strings to instances of ValidationError.",
            "                if not isinstance(message, ValidationError):",
            "                    message = ValidationError(message)",
            "                if hasattr(message, \"error_dict\"):",
            "                    self.error_list.extend(sum(message.error_dict.values(), []))",
            "                else:",
            "                    self.error_list.extend(message.error_list)",
            "",
            "        else:",
            "            self.message = message",
            "            self.code = code",
            "            self.params = params",
            "            self.error_list = [self]",
            "",
            "    @property",
            "    def message_dict(self):",
            "        # Trigger an AttributeError if this ValidationError",
            "        # doesn't have an error_dict.",
            "        getattr(self, \"error_dict\")",
            "",
            "        return dict(self)",
            "",
            "    @property",
            "    def messages(self):",
            "        if hasattr(self, \"error_dict\"):",
            "            return sum(dict(self).values(), [])",
            "        return list(self)",
            "",
            "    def update_error_dict(self, error_dict):",
            "        if hasattr(self, \"error_dict\"):",
            "            for field, error_list in self.error_dict.items():",
            "                error_dict.setdefault(field, []).extend(error_list)",
            "        else:",
            "            error_dict.setdefault(NON_FIELD_ERRORS, []).extend(self.error_list)",
            "        return error_dict",
            "",
            "    def __iter__(self):",
            "        if hasattr(self, \"error_dict\"):",
            "            for field, errors in self.error_dict.items():",
            "                yield field, list(ValidationError(errors))",
            "        else:",
            "            for error in self.error_list:",
            "                message = error.message",
            "                if error.params:",
            "                    message %= error.params",
            "                yield str(message)",
            "",
            "    def __str__(self):",
            "        if hasattr(self, \"error_dict\"):",
            "            return repr(dict(self))",
            "        return repr(list(self))",
            "",
            "    def __repr__(self):",
            "        return \"ValidationError(%s)\" % self",
            "",
            "    def __eq__(self, other):",
            "        if not isinstance(other, ValidationError):",
            "            return NotImplemented",
            "        return hash(self) == hash(other)",
            "",
            "    def __hash__(self):",
            "        if hasattr(self, \"message\"):",
            "            return hash(",
            "                (",
            "                    self.message,",
            "                    self.code,",
            "                    make_hashable(self.params),",
            "                )",
            "            )",
            "        if hasattr(self, \"error_dict\"):",
            "            return hash(make_hashable(self.error_dict))",
            "        return hash(tuple(sorted(self.error_list, key=operator.attrgetter(\"message\"))))",
            "",
            "",
            "class EmptyResultSet(Exception):",
            "    \"\"\"A database query predicate is impossible.\"\"\"",
            "",
            "    pass",
            "",
            "",
            "class SynchronousOnlyOperation(Exception):",
            "    \"\"\"The user tried to call a sync-only function from an async context.\"\"\"",
            "",
            "    pass"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "django/core/handlers/exception.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": "     RequestDataTooBig,"
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": "     SuspiciousOperation,"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": "     TooManyFieldsSent,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+    TooManyFilesSent,"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " )"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from django.http import Http404"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from django.http.multipartparser import MultiPartParserError"
            },
            "7": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "             exception=exc,"
            },
            "8": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "         )"
            },
            "9": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "     elif isinstance(exc, SuspiciousOperation):"
            },
            "10": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent)):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent, TooManyFilesSent)):"
            },
            "12": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "             # POST data can't be accessed again, otherwise the original"
            },
            "13": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "             # exception would be raised."
            },
            "14": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "             request._mark_post_parse_error()"
            }
        },
        "frontPatchFile": [
            "import asyncio",
            "import logging",
            "import sys",
            "from functools import wraps",
            "",
            "from asgiref.sync import sync_to_async",
            "",
            "from django.conf import settings",
            "from django.core import signals",
            "from django.core.exceptions import (",
            "    BadRequest,",
            "    PermissionDenied,",
            "    RequestDataTooBig,",
            "    SuspiciousOperation,",
            "    TooManyFieldsSent,",
            ")",
            "from django.http import Http404",
            "from django.http.multipartparser import MultiPartParserError",
            "from django.urls import get_resolver, get_urlconf",
            "from django.utils.log import log_response",
            "from django.views import debug",
            "",
            "",
            "def convert_exception_to_response(get_response):",
            "    \"\"\"",
            "    Wrap the given get_response callable in exception-to-response conversion.",
            "",
            "    All exceptions will be converted. All known 4xx exceptions (Http404,",
            "    PermissionDenied, MultiPartParserError, SuspiciousOperation) will be",
            "    converted to the appropriate response, and all other exceptions will be",
            "    converted to 500 responses.",
            "",
            "    This decorator is automatically applied to all middleware to ensure that",
            "    no middleware leaks an exception and that the next middleware in the stack",
            "    can rely on getting a response instead of an exception.",
            "    \"\"\"",
            "    if asyncio.iscoroutinefunction(get_response):",
            "",
            "        @wraps(get_response)",
            "        async def inner(request):",
            "            try:",
            "                response = await get_response(request)",
            "            except Exception as exc:",
            "                response = await sync_to_async(",
            "                    response_for_exception, thread_sensitive=False",
            "                )(request, exc)",
            "            return response",
            "",
            "        return inner",
            "    else:",
            "",
            "        @wraps(get_response)",
            "        def inner(request):",
            "            try:",
            "                response = get_response(request)",
            "            except Exception as exc:",
            "                response = response_for_exception(request, exc)",
            "            return response",
            "",
            "        return inner",
            "",
            "",
            "def response_for_exception(request, exc):",
            "    if isinstance(exc, Http404):",
            "        if settings.DEBUG:",
            "            response = debug.technical_404_response(request, exc)",
            "        else:",
            "            response = get_exception_response(",
            "                request, get_resolver(get_urlconf()), 404, exc",
            "            )",
            "",
            "    elif isinstance(exc, PermissionDenied):",
            "        response = get_exception_response(",
            "            request, get_resolver(get_urlconf()), 403, exc",
            "        )",
            "        log_response(",
            "            \"Forbidden (Permission denied): %s\",",
            "            request.path,",
            "            response=response,",
            "            request=request,",
            "            exception=exc,",
            "        )",
            "",
            "    elif isinstance(exc, MultiPartParserError):",
            "        response = get_exception_response(",
            "            request, get_resolver(get_urlconf()), 400, exc",
            "        )",
            "        log_response(",
            "            \"Bad request (Unable to parse request body): %s\",",
            "            request.path,",
            "            response=response,",
            "            request=request,",
            "            exception=exc,",
            "        )",
            "",
            "    elif isinstance(exc, BadRequest):",
            "        if settings.DEBUG:",
            "            response = debug.technical_500_response(",
            "                request, *sys.exc_info(), status_code=400",
            "            )",
            "        else:",
            "            response = get_exception_response(",
            "                request, get_resolver(get_urlconf()), 400, exc",
            "            )",
            "        log_response(",
            "            \"%s: %s\",",
            "            str(exc),",
            "            request.path,",
            "            response=response,",
            "            request=request,",
            "            exception=exc,",
            "        )",
            "    elif isinstance(exc, SuspiciousOperation):",
            "        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent)):",
            "            # POST data can't be accessed again, otherwise the original",
            "            # exception would be raised.",
            "            request._mark_post_parse_error()",
            "",
            "        # The request logger receives events for any problematic request",
            "        # The security logger receives events for all SuspiciousOperations",
            "        security_logger = logging.getLogger(",
            "            \"django.security.%s\" % exc.__class__.__name__",
            "        )",
            "        security_logger.error(",
            "            str(exc),",
            "            exc_info=exc,",
            "            extra={\"status_code\": 400, \"request\": request},",
            "        )",
            "        if settings.DEBUG:",
            "            response = debug.technical_500_response(",
            "                request, *sys.exc_info(), status_code=400",
            "            )",
            "        else:",
            "            response = get_exception_response(",
            "                request, get_resolver(get_urlconf()), 400, exc",
            "            )",
            "",
            "    else:",
            "        signals.got_request_exception.send(sender=None, request=request)",
            "        response = handle_uncaught_exception(",
            "            request, get_resolver(get_urlconf()), sys.exc_info()",
            "        )",
            "        log_response(",
            "            \"%s: %s\",",
            "            response.reason_phrase,",
            "            request.path,",
            "            response=response,",
            "            request=request,",
            "            exception=exc,",
            "        )",
            "",
            "    # Force a TemplateResponse to be rendered.",
            "    if not getattr(response, \"is_rendered\", True) and callable(",
            "        getattr(response, \"render\", None)",
            "    ):",
            "        response = response.render()",
            "",
            "    return response",
            "",
            "",
            "def get_exception_response(request, resolver, status_code, exception):",
            "    try:",
            "        callback = resolver.resolve_error_handler(status_code)",
            "        response = callback(request, exception=exception)",
            "    except Exception:",
            "        signals.got_request_exception.send(sender=None, request=request)",
            "        response = handle_uncaught_exception(request, resolver, sys.exc_info())",
            "",
            "    return response",
            "",
            "",
            "def handle_uncaught_exception(request, resolver, exc_info):",
            "    \"\"\"",
            "    Processing for any otherwise uncaught exceptions (those that will",
            "    generate HTTP 500 responses).",
            "    \"\"\"",
            "    if settings.DEBUG_PROPAGATE_EXCEPTIONS:",
            "        raise",
            "",
            "    if settings.DEBUG:",
            "        return debug.technical_500_response(request, *exc_info)",
            "",
            "    # Return an HttpResponse that displays a friendly error message.",
            "    callback = resolver.resolve_error_handler(500)",
            "    return callback(request)"
        ],
        "afterPatchFile": [
            "import asyncio",
            "import logging",
            "import sys",
            "from functools import wraps",
            "",
            "from asgiref.sync import sync_to_async",
            "",
            "from django.conf import settings",
            "from django.core import signals",
            "from django.core.exceptions import (",
            "    BadRequest,",
            "    PermissionDenied,",
            "    RequestDataTooBig,",
            "    SuspiciousOperation,",
            "    TooManyFieldsSent,",
            "    TooManyFilesSent,",
            ")",
            "from django.http import Http404",
            "from django.http.multipartparser import MultiPartParserError",
            "from django.urls import get_resolver, get_urlconf",
            "from django.utils.log import log_response",
            "from django.views import debug",
            "",
            "",
            "def convert_exception_to_response(get_response):",
            "    \"\"\"",
            "    Wrap the given get_response callable in exception-to-response conversion.",
            "",
            "    All exceptions will be converted. All known 4xx exceptions (Http404,",
            "    PermissionDenied, MultiPartParserError, SuspiciousOperation) will be",
            "    converted to the appropriate response, and all other exceptions will be",
            "    converted to 500 responses.",
            "",
            "    This decorator is automatically applied to all middleware to ensure that",
            "    no middleware leaks an exception and that the next middleware in the stack",
            "    can rely on getting a response instead of an exception.",
            "    \"\"\"",
            "    if asyncio.iscoroutinefunction(get_response):",
            "",
            "        @wraps(get_response)",
            "        async def inner(request):",
            "            try:",
            "                response = await get_response(request)",
            "            except Exception as exc:",
            "                response = await sync_to_async(",
            "                    response_for_exception, thread_sensitive=False",
            "                )(request, exc)",
            "            return response",
            "",
            "        return inner",
            "    else:",
            "",
            "        @wraps(get_response)",
            "        def inner(request):",
            "            try:",
            "                response = get_response(request)",
            "            except Exception as exc:",
            "                response = response_for_exception(request, exc)",
            "            return response",
            "",
            "        return inner",
            "",
            "",
            "def response_for_exception(request, exc):",
            "    if isinstance(exc, Http404):",
            "        if settings.DEBUG:",
            "            response = debug.technical_404_response(request, exc)",
            "        else:",
            "            response = get_exception_response(",
            "                request, get_resolver(get_urlconf()), 404, exc",
            "            )",
            "",
            "    elif isinstance(exc, PermissionDenied):",
            "        response = get_exception_response(",
            "            request, get_resolver(get_urlconf()), 403, exc",
            "        )",
            "        log_response(",
            "            \"Forbidden (Permission denied): %s\",",
            "            request.path,",
            "            response=response,",
            "            request=request,",
            "            exception=exc,",
            "        )",
            "",
            "    elif isinstance(exc, MultiPartParserError):",
            "        response = get_exception_response(",
            "            request, get_resolver(get_urlconf()), 400, exc",
            "        )",
            "        log_response(",
            "            \"Bad request (Unable to parse request body): %s\",",
            "            request.path,",
            "            response=response,",
            "            request=request,",
            "            exception=exc,",
            "        )",
            "",
            "    elif isinstance(exc, BadRequest):",
            "        if settings.DEBUG:",
            "            response = debug.technical_500_response(",
            "                request, *sys.exc_info(), status_code=400",
            "            )",
            "        else:",
            "            response = get_exception_response(",
            "                request, get_resolver(get_urlconf()), 400, exc",
            "            )",
            "        log_response(",
            "            \"%s: %s\",",
            "            str(exc),",
            "            request.path,",
            "            response=response,",
            "            request=request,",
            "            exception=exc,",
            "        )",
            "    elif isinstance(exc, SuspiciousOperation):",
            "        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent, TooManyFilesSent)):",
            "            # POST data can't be accessed again, otherwise the original",
            "            # exception would be raised.",
            "            request._mark_post_parse_error()",
            "",
            "        # The request logger receives events for any problematic request",
            "        # The security logger receives events for all SuspiciousOperations",
            "        security_logger = logging.getLogger(",
            "            \"django.security.%s\" % exc.__class__.__name__",
            "        )",
            "        security_logger.error(",
            "            str(exc),",
            "            exc_info=exc,",
            "            extra={\"status_code\": 400, \"request\": request},",
            "        )",
            "        if settings.DEBUG:",
            "            response = debug.technical_500_response(",
            "                request, *sys.exc_info(), status_code=400",
            "            )",
            "        else:",
            "            response = get_exception_response(",
            "                request, get_resolver(get_urlconf()), 400, exc",
            "            )",
            "",
            "    else:",
            "        signals.got_request_exception.send(sender=None, request=request)",
            "        response = handle_uncaught_exception(",
            "            request, get_resolver(get_urlconf()), sys.exc_info()",
            "        )",
            "        log_response(",
            "            \"%s: %s\",",
            "            response.reason_phrase,",
            "            request.path,",
            "            response=response,",
            "            request=request,",
            "            exception=exc,",
            "        )",
            "",
            "    # Force a TemplateResponse to be rendered.",
            "    if not getattr(response, \"is_rendered\", True) and callable(",
            "        getattr(response, \"render\", None)",
            "    ):",
            "        response = response.render()",
            "",
            "    return response",
            "",
            "",
            "def get_exception_response(request, resolver, status_code, exception):",
            "    try:",
            "        callback = resolver.resolve_error_handler(status_code)",
            "        response = callback(request, exception=exception)",
            "    except Exception:",
            "        signals.got_request_exception.send(sender=None, request=request)",
            "        response = handle_uncaught_exception(request, resolver, sys.exc_info())",
            "",
            "    return response",
            "",
            "",
            "def handle_uncaught_exception(request, resolver, exc_info):",
            "    \"\"\"",
            "    Processing for any otherwise uncaught exceptions (those that will",
            "    generate HTTP 500 responses).",
            "    \"\"\"",
            "    if settings.DEBUG_PROPAGATE_EXCEPTIONS:",
            "        raise",
            "",
            "    if settings.DEBUG:",
            "        return debug.technical_500_response(request, *exc_info)",
            "",
            "    # Return an HttpResponse that displays a friendly error message.",
            "    callback = resolver.resolve_error_handler(500)",
            "    return callback(request)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "114": [
                "response_for_exception"
            ]
        },
        "addLocation": []
    },
    "django/http/multipartparser.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": "     RequestDataTooBig,"
            },
            "1": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": "     SuspiciousMultipartForm,"
            },
            "2": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": "     TooManyFieldsSent,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 18,
                "PatchRowcode": "+    TooManyFilesSent,"
            },
            "4": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " )"
            },
            "5": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload"
            },
            "6": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from django.utils.datastructures import MultiValueDict"
            },
            "7": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " RAW = \"raw\""
            },
            "8": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " FILE = \"file\""
            },
            "9": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " FIELD = \"field\""
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+FIELD_TYPES = frozenset([FIELD, RAW])"
            },
            "11": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 46,
                "PatchRowcode": " class MultiPartParser:"
            },
            "14": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "         self._upload_handlers = upload_handlers"
            },
            "15": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 114,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "     def parse(self):"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        # Call the actual parse routine and close all open files in case of"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+        # errors. This is needed because if exceptions are thrown the"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+        # MultiPartParser will not be garbage collected immediately and"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+        # resources would be kept alive. This is only needed for errors because"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+        # the Request object closes all uploaded files at the end of the"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+        # request."
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+        try:"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+            return self._parse()"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+        except Exception:"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+            if hasattr(self, \"_files\"):"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+                for _, files in self._files.lists():"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+                    for fileobj in files:"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+                        fileobj.close()"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+            raise"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+    def _parse(self):"
            },
            "33": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 132,
                "PatchRowcode": "         \"\"\""
            },
            "34": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "         Parse the POST data and break it into a FILES MultiValueDict and a POST"
            },
            "35": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "         MultiValueDict."
            },
            "36": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 174,
                "PatchRowcode": "         num_bytes_read = 0"
            },
            "37": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 175,
                "PatchRowcode": "         # To count the number of keys in the request."
            },
            "38": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 176,
                "PatchRowcode": "         num_post_keys = 0"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+        # To count the number of files in the request."
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+        num_files = 0"
            },
            "41": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 179,
                "PatchRowcode": "         # To limit the amount of data read from the request."
            },
            "42": {
                "beforePatchRowNumber": 160,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "         read_size = None"
            },
            "43": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "         # Whether a file upload is finished."
            },
            "44": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "                     old_field_name = None"
            },
            "45": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 192,
                "PatchRowcode": "                     uploaded_file = True"
            },
            "46": {
                "beforePatchRowNumber": 173,
                "afterPatchRowNumber": 193,
                "PatchRowcode": " "
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+                if ("
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+                    item_type in FIELD_TYPES"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+                    and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+                ):"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS."
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+                    num_post_keys += 1"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+                    # 2 accounts for empty raw fields before and after the"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+                    # last boundary."
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+                    if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+                        raise TooManyFieldsSent("
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+                            \"The number of GET/POST parameters exceeded \""
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+                            \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\""
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+                        )"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+"
            },
            "61": {
                "beforePatchRowNumber": 174,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "                 try:"
            },
            "62": {
                "beforePatchRowNumber": 175,
                "afterPatchRowNumber": 209,
                "PatchRowcode": "                     disposition = meta_data[\"content-disposition\"][1]"
            },
            "63": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 210,
                "PatchRowcode": "                     field_name = disposition[\"name\"].strip()"
            },
            "64": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 217,
                "PatchRowcode": "                 field_name = force_str(field_name, encoding, errors=\"replace\")"
            },
            "65": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 218,
                "PatchRowcode": " "
            },
            "66": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 219,
                "PatchRowcode": "                 if item_type == FIELD:"
            },
            "67": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS."
            },
            "68": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    num_post_keys += 1"
            },
            "69": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    if ("
            },
            "70": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None"
            },
            "71": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys"
            },
            "72": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    ):"
            },
            "73": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        raise TooManyFieldsSent("
            },
            "74": {
                "beforePatchRowNumber": 193,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            \"The number of GET/POST parameters exceeded \""
            },
            "75": {
                "beforePatchRowNumber": 194,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\""
            },
            "76": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        )"
            },
            "77": {
                "beforePatchRowNumber": 196,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "78": {
                "beforePatchRowNumber": 197,
                "afterPatchRowNumber": 220,
                "PatchRowcode": "                     # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE."
            },
            "79": {
                "beforePatchRowNumber": 198,
                "afterPatchRowNumber": 221,
                "PatchRowcode": "                     if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:"
            },
            "80": {
                "beforePatchRowNumber": 199,
                "afterPatchRowNumber": 222,
                "PatchRowcode": "                         read_size = ("
            },
            "81": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 251,
                "PatchRowcode": "                         field_name, force_str(data, encoding, errors=\"replace\")"
            },
            "82": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": 252,
                "PatchRowcode": "                     )"
            },
            "83": {
                "beforePatchRowNumber": 230,
                "afterPatchRowNumber": 253,
                "PatchRowcode": "                 elif item_type == FILE:"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 254,
                "PatchRowcode": "+                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES."
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 255,
                "PatchRowcode": "+                    num_files += 1"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 256,
                "PatchRowcode": "+                    if ("
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 257,
                "PatchRowcode": "+                        settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 258,
                "PatchRowcode": "+                        and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 259,
                "PatchRowcode": "+                    ):"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 260,
                "PatchRowcode": "+                        raise TooManyFilesSent("
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 261,
                "PatchRowcode": "+                            \"The number of files exceeded \""
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 262,
                "PatchRowcode": "+                            \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\""
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 263,
                "PatchRowcode": "+                        )"
            },
            "94": {
                "beforePatchRowNumber": 231,
                "afterPatchRowNumber": 264,
                "PatchRowcode": "                     # This is a file, use the handler..."
            },
            "95": {
                "beforePatchRowNumber": 232,
                "afterPatchRowNumber": 265,
                "PatchRowcode": "                     file_name = disposition.get(\"filename\")"
            },
            "96": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": 266,
                "PatchRowcode": "                     if file_name:"
            },
            "97": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": 338,
                "PatchRowcode": "                         # Handle file upload completions on next iteration."
            },
            "98": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "                         old_field_name = field_name"
            },
            "99": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": 340,
                "PatchRowcode": "                 else:"
            },
            "100": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    # If this is neither a FIELD or a FILE, just exhaust the stream."
            },
            "101": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    exhaust(stream)"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 341,
                "PatchRowcode": "+                    # If this is neither a FIELD nor a FILE, exhaust the field"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 342,
                "PatchRowcode": "+                    # stream. Note: There could be an error here at some point,"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+                    # but there will be at least two RAW types (before and"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 344,
                "PatchRowcode": "+                    # after the other boundaries). This branch is usually not"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 345,
                "PatchRowcode": "+                    # reached at all, because a missing content-disposition"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 346,
                "PatchRowcode": "+                    # header will skip the whole boundary."
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 347,
                "PatchRowcode": "+                    exhaust(field_stream)"
            },
            "109": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": 348,
                "PatchRowcode": "         except StopUpload as e:"
            },
            "110": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": 349,
                "PatchRowcode": "             self._close_files()"
            },
            "111": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 350,
                "PatchRowcode": "             if not e.connection_reset:"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Multi-part parsing for file uploads.",
            "",
            "Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to",
            "file upload handlers for processing.",
            "\"\"\"",
            "import base64",
            "import binascii",
            "import collections",
            "import html",
            "from urllib.parse import unquote",
            "",
            "from django.conf import settings",
            "from django.core.exceptions import (",
            "    RequestDataTooBig,",
            "    SuspiciousMultipartForm,",
            "    TooManyFieldsSent,",
            ")",
            "from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload",
            "from django.utils.datastructures import MultiValueDict",
            "from django.utils.encoding import force_str",
            "from django.utils.regex_helper import _lazy_re_compile",
            "",
            "__all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")",
            "",
            "",
            "class MultiPartParserError(Exception):",
            "    pass",
            "",
            "",
            "class InputStreamExhausted(Exception):",
            "    \"\"\"",
            "    No more reads are allowed from this device.",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "RAW = \"raw\"",
            "FILE = \"file\"",
            "FIELD = \"field\"",
            "",
            "",
            "class MultiPartParser:",
            "    \"\"\"",
            "    A rfc2388 multipart/form-data parser.",
            "",
            "    ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks",
            "    and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.",
            "    \"\"\"",
            "",
            "    boundary_re = _lazy_re_compile(rb\"[ -~]{0,200}[!-~]\")",
            "",
            "    def __init__(self, META, input_data, upload_handlers, encoding=None):",
            "        \"\"\"",
            "        Initialize the MultiPartParser object.",
            "",
            "        :META:",
            "            The standard ``META`` dictionary in Django request objects.",
            "        :input_data:",
            "            The raw post data, as a file-like object.",
            "        :upload_handlers:",
            "            A list of UploadHandler instances that perform operations on the",
            "            uploaded data.",
            "        :encoding:",
            "            The encoding with which to treat the incoming data.",
            "        \"\"\"",
            "        # Content-Type should contain multipart and the boundary information.",
            "        content_type = META.get(\"CONTENT_TYPE\", \"\")",
            "        if not content_type.startswith(\"multipart/\"):",
            "            raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)",
            "",
            "        # Parse the header to get the boundary to split the parts.",
            "        try:",
            "            ctypes, opts = parse_header(content_type.encode(\"ascii\"))",
            "        except UnicodeEncodeError:",
            "            raise MultiPartParserError(",
            "                \"Invalid non-ASCII Content-Type in multipart: %s\"",
            "                % force_str(content_type)",
            "            )",
            "        boundary = opts.get(\"boundary\")",
            "        if not boundary or not self.boundary_re.fullmatch(boundary):",
            "            raise MultiPartParserError(",
            "                \"Invalid boundary in multipart: %s\" % force_str(boundary)",
            "            )",
            "",
            "        # Content-Length should contain the length of the body we are about",
            "        # to receive.",
            "        try:",
            "            content_length = int(META.get(\"CONTENT_LENGTH\", 0))",
            "        except (ValueError, TypeError):",
            "            content_length = 0",
            "",
            "        if content_length < 0:",
            "            # This means we shouldn't continue...raise an error.",
            "            raise MultiPartParserError(\"Invalid content length: %r\" % content_length)",
            "",
            "        if isinstance(boundary, str):",
            "            boundary = boundary.encode(\"ascii\")",
            "        self._boundary = boundary",
            "        self._input_data = input_data",
            "",
            "        # For compatibility with low-level network APIs (with 32-bit integers),",
            "        # the chunk size should be < 2^31, but still divisible by 4.",
            "        possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]",
            "        self._chunk_size = min([2**31 - 4] + possible_sizes)",
            "",
            "        self._meta = META",
            "        self._encoding = encoding or settings.DEFAULT_CHARSET",
            "        self._content_length = content_length",
            "        self._upload_handlers = upload_handlers",
            "",
            "    def parse(self):",
            "        \"\"\"",
            "        Parse the POST data and break it into a FILES MultiValueDict and a POST",
            "        MultiValueDict.",
            "",
            "        Return a tuple containing the POST and FILES dictionary, respectively.",
            "        \"\"\"",
            "        from django.http import QueryDict",
            "",
            "        encoding = self._encoding",
            "        handlers = self._upload_handlers",
            "",
            "        # HTTP spec says that Content-Length >= 0 is valid",
            "        # handling content-length == 0 before continuing",
            "        if self._content_length == 0:",
            "            return QueryDict(encoding=self._encoding), MultiValueDict()",
            "",
            "        # See if any of the handlers take care of the parsing.",
            "        # This allows overriding everything if need be.",
            "        for handler in handlers:",
            "            result = handler.handle_raw_input(",
            "                self._input_data,",
            "                self._meta,",
            "                self._content_length,",
            "                self._boundary,",
            "                encoding,",
            "            )",
            "            # Check to see if it was handled",
            "            if result is not None:",
            "                return result[0], result[1]",
            "",
            "        # Create the data structures to be used later.",
            "        self._post = QueryDict(mutable=True)",
            "        self._files = MultiValueDict()",
            "",
            "        # Instantiate the parser and stream:",
            "        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))",
            "",
            "        # Whether or not to signal a file-completion at the beginning of the loop.",
            "        old_field_name = None",
            "        counters = [0] * len(handlers)",
            "",
            "        # Number of bytes that have been read.",
            "        num_bytes_read = 0",
            "        # To count the number of keys in the request.",
            "        num_post_keys = 0",
            "        # To limit the amount of data read from the request.",
            "        read_size = None",
            "        # Whether a file upload is finished.",
            "        uploaded_file = True",
            "",
            "        try:",
            "            for item_type, meta_data, field_stream in Parser(stream, self._boundary):",
            "                if old_field_name:",
            "                    # We run this at the beginning of the next loop",
            "                    # since we cannot be sure a file is complete until",
            "                    # we hit the next boundary/part of the multipart content.",
            "                    self.handle_file_complete(old_field_name, counters)",
            "                    old_field_name = None",
            "                    uploaded_file = True",
            "",
            "                try:",
            "                    disposition = meta_data[\"content-disposition\"][1]",
            "                    field_name = disposition[\"name\"].strip()",
            "                except (KeyError, IndexError, AttributeError):",
            "                    continue",
            "",
            "                transfer_encoding = meta_data.get(\"content-transfer-encoding\")",
            "                if transfer_encoding is not None:",
            "                    transfer_encoding = transfer_encoding[0].strip()",
            "                field_name = force_str(field_name, encoding, errors=\"replace\")",
            "",
            "                if item_type == FIELD:",
            "                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.",
            "                    num_post_keys += 1",
            "                    if (",
            "                        settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None",
            "                        and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys",
            "                    ):",
            "                        raise TooManyFieldsSent(",
            "                            \"The number of GET/POST parameters exceeded \"",
            "                            \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"",
            "                        )",
            "",
            "                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.",
            "                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:",
            "                        read_size = (",
            "                            settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read",
            "                        )",
            "",
            "                    # This is a post field, we can just set it in the post",
            "                    if transfer_encoding == \"base64\":",
            "                        raw_data = field_stream.read(size=read_size)",
            "                        num_bytes_read += len(raw_data)",
            "                        try:",
            "                            data = base64.b64decode(raw_data)",
            "                        except binascii.Error:",
            "                            data = raw_data",
            "                    else:",
            "                        data = field_stream.read(size=read_size)",
            "                        num_bytes_read += len(data)",
            "",
            "                    # Add two here to make the check consistent with the",
            "                    # x-www-form-urlencoded check that includes '&='.",
            "                    num_bytes_read += len(field_name) + 2",
            "                    if (",
            "                        settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None",
            "                        and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE",
            "                    ):",
            "                        raise RequestDataTooBig(",
            "                            \"Request body exceeded \"",
            "                            \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"",
            "                        )",
            "",
            "                    self._post.appendlist(",
            "                        field_name, force_str(data, encoding, errors=\"replace\")",
            "                    )",
            "                elif item_type == FILE:",
            "                    # This is a file, use the handler...",
            "                    file_name = disposition.get(\"filename\")",
            "                    if file_name:",
            "                        file_name = force_str(file_name, encoding, errors=\"replace\")",
            "                        file_name = self.sanitize_file_name(file_name)",
            "                    if not file_name:",
            "                        continue",
            "",
            "                    content_type, content_type_extra = meta_data.get(",
            "                        \"content-type\", (\"\", {})",
            "                    )",
            "                    content_type = content_type.strip()",
            "                    charset = content_type_extra.get(\"charset\")",
            "",
            "                    try:",
            "                        content_length = int(meta_data.get(\"content-length\")[0])",
            "                    except (IndexError, TypeError, ValueError):",
            "                        content_length = None",
            "",
            "                    counters = [0] * len(handlers)",
            "                    uploaded_file = False",
            "                    try:",
            "                        for handler in handlers:",
            "                            try:",
            "                                handler.new_file(",
            "                                    field_name,",
            "                                    file_name,",
            "                                    content_type,",
            "                                    content_length,",
            "                                    charset,",
            "                                    content_type_extra,",
            "                                )",
            "                            except StopFutureHandlers:",
            "                                break",
            "",
            "                        for chunk in field_stream:",
            "                            if transfer_encoding == \"base64\":",
            "                                # We only special-case base64 transfer encoding",
            "                                # We should always decode base64 chunks by",
            "                                # multiple of 4, ignoring whitespace.",
            "",
            "                                stripped_chunk = b\"\".join(chunk.split())",
            "",
            "                                remaining = len(stripped_chunk) % 4",
            "                                while remaining != 0:",
            "                                    over_chunk = field_stream.read(4 - remaining)",
            "                                    if not over_chunk:",
            "                                        break",
            "                                    stripped_chunk += b\"\".join(over_chunk.split())",
            "                                    remaining = len(stripped_chunk) % 4",
            "",
            "                                try:",
            "                                    chunk = base64.b64decode(stripped_chunk)",
            "                                except Exception as exc:",
            "                                    # Since this is only a chunk, any error is",
            "                                    # an unfixable error.",
            "                                    raise MultiPartParserError(",
            "                                        \"Could not decode base64 data.\"",
            "                                    ) from exc",
            "",
            "                            for i, handler in enumerate(handlers):",
            "                                chunk_length = len(chunk)",
            "                                chunk = handler.receive_data_chunk(chunk, counters[i])",
            "                                counters[i] += chunk_length",
            "                                if chunk is None:",
            "                                    # Don't continue if the chunk received by",
            "                                    # the handler is None.",
            "                                    break",
            "",
            "                    except SkipFile:",
            "                        self._close_files()",
            "                        # Just use up the rest of this file...",
            "                        exhaust(field_stream)",
            "                    else:",
            "                        # Handle file upload completions on next iteration.",
            "                        old_field_name = field_name",
            "                else:",
            "                    # If this is neither a FIELD or a FILE, just exhaust the stream.",
            "                    exhaust(stream)",
            "        except StopUpload as e:",
            "            self._close_files()",
            "            if not e.connection_reset:",
            "                exhaust(self._input_data)",
            "        else:",
            "            if not uploaded_file:",
            "                for handler in handlers:",
            "                    handler.upload_interrupted()",
            "            # Make sure that the request data is all fed",
            "            exhaust(self._input_data)",
            "",
            "        # Signal that the upload has completed.",
            "        # any() shortcircuits if a handler's upload_complete() returns a value.",
            "        any(handler.upload_complete() for handler in handlers)",
            "        self._post._mutable = False",
            "        return self._post, self._files",
            "",
            "    def handle_file_complete(self, old_field_name, counters):",
            "        \"\"\"",
            "        Handle all the signaling that takes place when a file is complete.",
            "        \"\"\"",
            "        for i, handler in enumerate(self._upload_handlers):",
            "            file_obj = handler.file_complete(counters[i])",
            "            if file_obj:",
            "                # If it returns a file object, then set the files dict.",
            "                self._files.appendlist(",
            "                    force_str(old_field_name, self._encoding, errors=\"replace\"),",
            "                    file_obj,",
            "                )",
            "                break",
            "",
            "    def sanitize_file_name(self, file_name):",
            "        \"\"\"",
            "        Sanitize the filename of an upload.",
            "",
            "        Remove all possible path separators, even though that might remove more",
            "        than actually required by the target system. Filenames that could",
            "        potentially cause problems (current/parent dir) are also discarded.",
            "",
            "        It should be noted that this function could still return a \"filepath\"",
            "        like \"C:some_file.txt\" which is handled later on by the storage layer.",
            "        So while this function does sanitize filenames to some extent, the",
            "        resulting filename should still be considered as untrusted user input.",
            "        \"\"\"",
            "        file_name = html.unescape(file_name)",
            "        file_name = file_name.rsplit(\"/\")[-1]",
            "        file_name = file_name.rsplit(\"\\\\\")[-1]",
            "        # Remove non-printable characters.",
            "        file_name = \"\".join([char for char in file_name if char.isprintable()])",
            "",
            "        if file_name in {\"\", \".\", \"..\"}:",
            "            return None",
            "        return file_name",
            "",
            "    IE_sanitize = sanitize_file_name",
            "",
            "    def _close_files(self):",
            "        # Free up all file handles.",
            "        # FIXME: this currently assumes that upload handlers store the file as 'file'",
            "        # We should document that...",
            "        # (Maybe add handler.free_file to complement new_file)",
            "        for handler in self._upload_handlers:",
            "            if hasattr(handler, \"file\"):",
            "                handler.file.close()",
            "",
            "",
            "class LazyStream:",
            "    \"\"\"",
            "    The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.",
            "",
            "    Given a producer object (an iterator that yields bytestrings), the",
            "    LazyStream object will support iteration, reading, and keeping a \"look-back\"",
            "    variable in case you need to \"unget\" some bytes.",
            "    \"\"\"",
            "",
            "    def __init__(self, producer, length=None):",
            "        \"\"\"",
            "        Every LazyStream must have a producer when instantiated.",
            "",
            "        A producer is an iterable that returns a string each time it",
            "        is called.",
            "        \"\"\"",
            "        self._producer = producer",
            "        self._empty = False",
            "        self._leftover = b\"\"",
            "        self.length = length",
            "        self.position = 0",
            "        self._remaining = length",
            "        self._unget_history = []",
            "",
            "    def tell(self):",
            "        return self.position",
            "",
            "    def read(self, size=None):",
            "        def parts():",
            "            remaining = self._remaining if size is None else size",
            "            # do the whole thing in one shot if no limit was provided.",
            "            if remaining is None:",
            "                yield b\"\".join(self)",
            "                return",
            "",
            "            # otherwise do some bookkeeping to return exactly enough",
            "            # of the stream and stashing any extra content we get from",
            "            # the producer",
            "            while remaining != 0:",
            "                assert remaining > 0, \"remaining bytes to read should never go negative\"",
            "",
            "                try:",
            "                    chunk = next(self)",
            "                except StopIteration:",
            "                    return",
            "                else:",
            "                    emitting = chunk[:remaining]",
            "                    self.unget(chunk[remaining:])",
            "                    remaining -= len(emitting)",
            "                    yield emitting",
            "",
            "        return b\"\".join(parts())",
            "",
            "    def __next__(self):",
            "        \"\"\"",
            "        Used when the exact number of bytes to read is unimportant.",
            "",
            "        Return whatever chunk is conveniently returned from the iterator.",
            "        Useful to avoid unnecessary bookkeeping if performance is an issue.",
            "        \"\"\"",
            "        if self._leftover:",
            "            output = self._leftover",
            "            self._leftover = b\"\"",
            "        else:",
            "            output = next(self._producer)",
            "            self._unget_history = []",
            "        self.position += len(output)",
            "        return output",
            "",
            "    def close(self):",
            "        \"\"\"",
            "        Used to invalidate/disable this lazy stream.",
            "",
            "        Replace the producer with an empty list. Any leftover bytes that have",
            "        already been read will still be reported upon read() and/or next().",
            "        \"\"\"",
            "        self._producer = []",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "    def unget(self, bytes):",
            "        \"\"\"",
            "        Place bytes back onto the front of the lazy stream.",
            "",
            "        Future calls to read() will return those bytes first. The",
            "        stream position and thus tell() will be rewound.",
            "        \"\"\"",
            "        if not bytes:",
            "            return",
            "        self._update_unget_history(len(bytes))",
            "        self.position -= len(bytes)",
            "        self._leftover = bytes + self._leftover",
            "",
            "    def _update_unget_history(self, num_bytes):",
            "        \"\"\"",
            "        Update the unget history as a sanity check to see if we've pushed",
            "        back the same number of bytes in one chunk. If we keep ungetting the",
            "        same number of bytes many times (here, 50), we're mostly likely in an",
            "        infinite loop of some sort. This is usually caused by a",
            "        maliciously-malformed MIME request.",
            "        \"\"\"",
            "        self._unget_history = [num_bytes] + self._unget_history[:49]",
            "        number_equal = len(",
            "            [",
            "                current_number",
            "                for current_number in self._unget_history",
            "                if current_number == num_bytes",
            "            ]",
            "        )",
            "",
            "        if number_equal > 40:",
            "            raise SuspiciousMultipartForm(",
            "                \"The multipart parser got stuck, which shouldn't happen with\"",
            "                \" normal uploaded files. Check for malicious upload activity;\"",
            "                \" if there is none, report this to the Django developers.\"",
            "            )",
            "",
            "",
            "class ChunkIter:",
            "    \"\"\"",
            "    An iterable that will yield chunks of data. Given a file-like object as the",
            "    constructor, yield chunks of read operations from that object.",
            "    \"\"\"",
            "",
            "    def __init__(self, flo, chunk_size=64 * 1024):",
            "        self.flo = flo",
            "        self.chunk_size = chunk_size",
            "",
            "    def __next__(self):",
            "        try:",
            "            data = self.flo.read(self.chunk_size)",
            "        except InputStreamExhausted:",
            "            raise StopIteration()",
            "        if data:",
            "            return data",
            "        else:",
            "            raise StopIteration()",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "",
            "class InterBoundaryIter:",
            "    \"\"\"",
            "    A Producer that will iterate over boundaries.",
            "    \"\"\"",
            "",
            "    def __init__(self, stream, boundary):",
            "        self._stream = stream",
            "        self._boundary = boundary",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "    def __next__(self):",
            "        try:",
            "            return LazyStream(BoundaryIter(self._stream, self._boundary))",
            "        except InputStreamExhausted:",
            "            raise StopIteration()",
            "",
            "",
            "class BoundaryIter:",
            "    \"\"\"",
            "    A Producer that is sensitive to boundaries.",
            "",
            "    Will happily yield bytes until a boundary is found. Will yield the bytes",
            "    before the boundary, throw away the boundary bytes themselves, and push the",
            "    post-boundary bytes back on the stream.",
            "",
            "    The future calls to next() after locating the boundary will raise a",
            "    StopIteration exception.",
            "    \"\"\"",
            "",
            "    def __init__(self, stream, boundary):",
            "        self._stream = stream",
            "        self._boundary = boundary",
            "        self._done = False",
            "        # rollback an additional six bytes because the format is like",
            "        # this: CRLF<boundary>[--CRLF]",
            "        self._rollback = len(boundary) + 6",
            "",
            "        # Try to use mx fast string search if available. Otherwise",
            "        # use Python find. Wrap the latter for consistency.",
            "        unused_char = self._stream.read(1)",
            "        if not unused_char:",
            "            raise InputStreamExhausted()",
            "        self._stream.unget(unused_char)",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "    def __next__(self):",
            "        if self._done:",
            "            raise StopIteration()",
            "",
            "        stream = self._stream",
            "        rollback = self._rollback",
            "",
            "        bytes_read = 0",
            "        chunks = []",
            "        for bytes in stream:",
            "            bytes_read += len(bytes)",
            "            chunks.append(bytes)",
            "            if bytes_read > rollback:",
            "                break",
            "            if not bytes:",
            "                break",
            "        else:",
            "            self._done = True",
            "",
            "        if not chunks:",
            "            raise StopIteration()",
            "",
            "        chunk = b\"\".join(chunks)",
            "        boundary = self._find_boundary(chunk)",
            "",
            "        if boundary:",
            "            end, next = boundary",
            "            stream.unget(chunk[next:])",
            "            self._done = True",
            "            return chunk[:end]",
            "        else:",
            "            # make sure we don't treat a partial boundary (and",
            "            # its separators) as data",
            "            if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):",
            "                # There's nothing left, we should just return and mark as done.",
            "                self._done = True",
            "                return chunk",
            "            else:",
            "                stream.unget(chunk[-rollback:])",
            "                return chunk[:-rollback]",
            "",
            "    def _find_boundary(self, data):",
            "        \"\"\"",
            "        Find a multipart boundary in data.",
            "",
            "        Should no boundary exist in the data, return None. Otherwise, return",
            "        a tuple containing the indices of the following:",
            "         * the end of current encapsulation",
            "         * the start of the next encapsulation",
            "        \"\"\"",
            "        index = data.find(self._boundary)",
            "        if index < 0:",
            "            return None",
            "        else:",
            "            end = index",
            "            next = index + len(self._boundary)",
            "            # backup over CRLF",
            "            last = max(0, end - 1)",
            "            if data[last : last + 1] == b\"\\n\":",
            "                end -= 1",
            "            last = max(0, end - 1)",
            "            if data[last : last + 1] == b\"\\r\":",
            "                end -= 1",
            "            return end, next",
            "",
            "",
            "def exhaust(stream_or_iterable):",
            "    \"\"\"Exhaust an iterator or stream.\"\"\"",
            "    try:",
            "        iterator = iter(stream_or_iterable)",
            "    except TypeError:",
            "        iterator = ChunkIter(stream_or_iterable, 16384)",
            "    collections.deque(iterator, maxlen=0)  # consume iterator quickly.",
            "",
            "",
            "def parse_boundary_stream(stream, max_header_size):",
            "    \"\"\"",
            "    Parse one and exactly one stream that encapsulates a boundary.",
            "    \"\"\"",
            "    # Stream at beginning of header, look for end of header",
            "    # and parse it if found. The header must fit within one",
            "    # chunk.",
            "    chunk = stream.read(max_header_size)",
            "",
            "    # 'find' returns the top of these four bytes, so we'll",
            "    # need to munch them later to prevent them from polluting",
            "    # the payload.",
            "    header_end = chunk.find(b\"\\r\\n\\r\\n\")",
            "",
            "    def _parse_header(line):",
            "        main_value_pair, params = parse_header(line)",
            "        try:",
            "            name, value = main_value_pair.split(\":\", 1)",
            "        except ValueError:",
            "            raise ValueError(\"Invalid header: %r\" % line)",
            "        return name, (value, params)",
            "",
            "    if header_end == -1:",
            "        # we find no header, so we just mark this fact and pass on",
            "        # the stream verbatim",
            "        stream.unget(chunk)",
            "        return (RAW, {}, stream)",
            "",
            "    header = chunk[:header_end]",
            "",
            "    # here we place any excess chunk back onto the stream, as",
            "    # well as throwing away the CRLFCRLF bytes from above.",
            "    stream.unget(chunk[header_end + 4 :])",
            "",
            "    TYPE = RAW",
            "    outdict = {}",
            "",
            "    # Eliminate blank lines",
            "    for line in header.split(b\"\\r\\n\"):",
            "        # This terminology (\"main value\" and \"dictionary of",
            "        # parameters\") is from the Python docs.",
            "        try:",
            "            name, (value, params) = _parse_header(line)",
            "        except ValueError:",
            "            continue",
            "",
            "        if name == \"content-disposition\":",
            "            TYPE = FIELD",
            "            if params.get(\"filename\"):",
            "                TYPE = FILE",
            "",
            "        outdict[name] = value, params",
            "",
            "    if TYPE == RAW:",
            "        stream.unget(chunk)",
            "",
            "    return (TYPE, outdict, stream)",
            "",
            "",
            "class Parser:",
            "    def __init__(self, stream, boundary):",
            "        self._stream = stream",
            "        self._separator = b\"--\" + boundary",
            "",
            "    def __iter__(self):",
            "        boundarystream = InterBoundaryIter(self._stream, self._separator)",
            "        for sub_stream in boundarystream:",
            "            # Iterate over each part",
            "            yield parse_boundary_stream(sub_stream, 1024)",
            "",
            "",
            "def parse_header(line):",
            "    \"\"\"",
            "    Parse the header into a key-value.",
            "",
            "    Input (line): bytes, output: str for key/name, bytes for values which",
            "    will be decoded later.",
            "    \"\"\"",
            "    plist = _parse_header_params(b\";\" + line)",
            "    key = plist.pop(0).lower().decode(\"ascii\")",
            "    pdict = {}",
            "    for p in plist:",
            "        i = p.find(b\"=\")",
            "        if i >= 0:",
            "            has_encoding = False",
            "            name = p[:i].strip().lower().decode(\"ascii\")",
            "            if name.endswith(\"*\"):",
            "                # Lang/encoding embedded in the value (like \"filename*=UTF-8''file.ext\")",
            "                # https://tools.ietf.org/html/rfc2231#section-4",
            "                name = name[:-1]",
            "                if p.count(b\"'\") == 2:",
            "                    has_encoding = True",
            "            value = p[i + 1 :].strip()",
            "            if len(value) >= 2 and value[:1] == value[-1:] == b'\"':",
            "                value = value[1:-1]",
            "                value = value.replace(b\"\\\\\\\\\", b\"\\\\\").replace(b'\\\\\"', b'\"')",
            "            if has_encoding:",
            "                encoding, lang, value = value.split(b\"'\")",
            "                value = unquote(value.decode(), encoding=encoding.decode())",
            "            pdict[name] = value",
            "    return key, pdict",
            "",
            "",
            "def _parse_header_params(s):",
            "    plist = []",
            "    while s[:1] == b\";\":",
            "        s = s[1:]",
            "        end = s.find(b\";\")",
            "        while end > 0 and (s.count(b'\"', 0, end) - s.count(b'\\\\\"', 0, end)) % 2:",
            "            end = s.find(b\";\", end + 1)",
            "        if end < 0:",
            "            end = len(s)",
            "        f = s[:end]",
            "        plist.append(f.strip())",
            "        s = s[end:]",
            "    return plist"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Multi-part parsing for file uploads.",
            "",
            "Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to",
            "file upload handlers for processing.",
            "\"\"\"",
            "import base64",
            "import binascii",
            "import collections",
            "import html",
            "from urllib.parse import unquote",
            "",
            "from django.conf import settings",
            "from django.core.exceptions import (",
            "    RequestDataTooBig,",
            "    SuspiciousMultipartForm,",
            "    TooManyFieldsSent,",
            "    TooManyFilesSent,",
            ")",
            "from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload",
            "from django.utils.datastructures import MultiValueDict",
            "from django.utils.encoding import force_str",
            "from django.utils.regex_helper import _lazy_re_compile",
            "",
            "__all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")",
            "",
            "",
            "class MultiPartParserError(Exception):",
            "    pass",
            "",
            "",
            "class InputStreamExhausted(Exception):",
            "    \"\"\"",
            "    No more reads are allowed from this device.",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "RAW = \"raw\"",
            "FILE = \"file\"",
            "FIELD = \"field\"",
            "FIELD_TYPES = frozenset([FIELD, RAW])",
            "",
            "",
            "class MultiPartParser:",
            "    \"\"\"",
            "    A rfc2388 multipart/form-data parser.",
            "",
            "    ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks",
            "    and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.",
            "    \"\"\"",
            "",
            "    boundary_re = _lazy_re_compile(rb\"[ -~]{0,200}[!-~]\")",
            "",
            "    def __init__(self, META, input_data, upload_handlers, encoding=None):",
            "        \"\"\"",
            "        Initialize the MultiPartParser object.",
            "",
            "        :META:",
            "            The standard ``META`` dictionary in Django request objects.",
            "        :input_data:",
            "            The raw post data, as a file-like object.",
            "        :upload_handlers:",
            "            A list of UploadHandler instances that perform operations on the",
            "            uploaded data.",
            "        :encoding:",
            "            The encoding with which to treat the incoming data.",
            "        \"\"\"",
            "        # Content-Type should contain multipart and the boundary information.",
            "        content_type = META.get(\"CONTENT_TYPE\", \"\")",
            "        if not content_type.startswith(\"multipart/\"):",
            "            raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)",
            "",
            "        # Parse the header to get the boundary to split the parts.",
            "        try:",
            "            ctypes, opts = parse_header(content_type.encode(\"ascii\"))",
            "        except UnicodeEncodeError:",
            "            raise MultiPartParserError(",
            "                \"Invalid non-ASCII Content-Type in multipart: %s\"",
            "                % force_str(content_type)",
            "            )",
            "        boundary = opts.get(\"boundary\")",
            "        if not boundary or not self.boundary_re.fullmatch(boundary):",
            "            raise MultiPartParserError(",
            "                \"Invalid boundary in multipart: %s\" % force_str(boundary)",
            "            )",
            "",
            "        # Content-Length should contain the length of the body we are about",
            "        # to receive.",
            "        try:",
            "            content_length = int(META.get(\"CONTENT_LENGTH\", 0))",
            "        except (ValueError, TypeError):",
            "            content_length = 0",
            "",
            "        if content_length < 0:",
            "            # This means we shouldn't continue...raise an error.",
            "            raise MultiPartParserError(\"Invalid content length: %r\" % content_length)",
            "",
            "        if isinstance(boundary, str):",
            "            boundary = boundary.encode(\"ascii\")",
            "        self._boundary = boundary",
            "        self._input_data = input_data",
            "",
            "        # For compatibility with low-level network APIs (with 32-bit integers),",
            "        # the chunk size should be < 2^31, but still divisible by 4.",
            "        possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]",
            "        self._chunk_size = min([2**31 - 4] + possible_sizes)",
            "",
            "        self._meta = META",
            "        self._encoding = encoding or settings.DEFAULT_CHARSET",
            "        self._content_length = content_length",
            "        self._upload_handlers = upload_handlers",
            "",
            "    def parse(self):",
            "        # Call the actual parse routine and close all open files in case of",
            "        # errors. This is needed because if exceptions are thrown the",
            "        # MultiPartParser will not be garbage collected immediately and",
            "        # resources would be kept alive. This is only needed for errors because",
            "        # the Request object closes all uploaded files at the end of the",
            "        # request.",
            "        try:",
            "            return self._parse()",
            "        except Exception:",
            "            if hasattr(self, \"_files\"):",
            "                for _, files in self._files.lists():",
            "                    for fileobj in files:",
            "                        fileobj.close()",
            "            raise",
            "",
            "    def _parse(self):",
            "        \"\"\"",
            "        Parse the POST data and break it into a FILES MultiValueDict and a POST",
            "        MultiValueDict.",
            "",
            "        Return a tuple containing the POST and FILES dictionary, respectively.",
            "        \"\"\"",
            "        from django.http import QueryDict",
            "",
            "        encoding = self._encoding",
            "        handlers = self._upload_handlers",
            "",
            "        # HTTP spec says that Content-Length >= 0 is valid",
            "        # handling content-length == 0 before continuing",
            "        if self._content_length == 0:",
            "            return QueryDict(encoding=self._encoding), MultiValueDict()",
            "",
            "        # See if any of the handlers take care of the parsing.",
            "        # This allows overriding everything if need be.",
            "        for handler in handlers:",
            "            result = handler.handle_raw_input(",
            "                self._input_data,",
            "                self._meta,",
            "                self._content_length,",
            "                self._boundary,",
            "                encoding,",
            "            )",
            "            # Check to see if it was handled",
            "            if result is not None:",
            "                return result[0], result[1]",
            "",
            "        # Create the data structures to be used later.",
            "        self._post = QueryDict(mutable=True)",
            "        self._files = MultiValueDict()",
            "",
            "        # Instantiate the parser and stream:",
            "        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))",
            "",
            "        # Whether or not to signal a file-completion at the beginning of the loop.",
            "        old_field_name = None",
            "        counters = [0] * len(handlers)",
            "",
            "        # Number of bytes that have been read.",
            "        num_bytes_read = 0",
            "        # To count the number of keys in the request.",
            "        num_post_keys = 0",
            "        # To count the number of files in the request.",
            "        num_files = 0",
            "        # To limit the amount of data read from the request.",
            "        read_size = None",
            "        # Whether a file upload is finished.",
            "        uploaded_file = True",
            "",
            "        try:",
            "            for item_type, meta_data, field_stream in Parser(stream, self._boundary):",
            "                if old_field_name:",
            "                    # We run this at the beginning of the next loop",
            "                    # since we cannot be sure a file is complete until",
            "                    # we hit the next boundary/part of the multipart content.",
            "                    self.handle_file_complete(old_field_name, counters)",
            "                    old_field_name = None",
            "                    uploaded_file = True",
            "",
            "                if (",
            "                    item_type in FIELD_TYPES",
            "                    and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None",
            "                ):",
            "                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.",
            "                    num_post_keys += 1",
            "                    # 2 accounts for empty raw fields before and after the",
            "                    # last boundary.",
            "                    if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:",
            "                        raise TooManyFieldsSent(",
            "                            \"The number of GET/POST parameters exceeded \"",
            "                            \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"",
            "                        )",
            "",
            "                try:",
            "                    disposition = meta_data[\"content-disposition\"][1]",
            "                    field_name = disposition[\"name\"].strip()",
            "                except (KeyError, IndexError, AttributeError):",
            "                    continue",
            "",
            "                transfer_encoding = meta_data.get(\"content-transfer-encoding\")",
            "                if transfer_encoding is not None:",
            "                    transfer_encoding = transfer_encoding[0].strip()",
            "                field_name = force_str(field_name, encoding, errors=\"replace\")",
            "",
            "                if item_type == FIELD:",
            "                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.",
            "                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:",
            "                        read_size = (",
            "                            settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read",
            "                        )",
            "",
            "                    # This is a post field, we can just set it in the post",
            "                    if transfer_encoding == \"base64\":",
            "                        raw_data = field_stream.read(size=read_size)",
            "                        num_bytes_read += len(raw_data)",
            "                        try:",
            "                            data = base64.b64decode(raw_data)",
            "                        except binascii.Error:",
            "                            data = raw_data",
            "                    else:",
            "                        data = field_stream.read(size=read_size)",
            "                        num_bytes_read += len(data)",
            "",
            "                    # Add two here to make the check consistent with the",
            "                    # x-www-form-urlencoded check that includes '&='.",
            "                    num_bytes_read += len(field_name) + 2",
            "                    if (",
            "                        settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None",
            "                        and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE",
            "                    ):",
            "                        raise RequestDataTooBig(",
            "                            \"Request body exceeded \"",
            "                            \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"",
            "                        )",
            "",
            "                    self._post.appendlist(",
            "                        field_name, force_str(data, encoding, errors=\"replace\")",
            "                    )",
            "                elif item_type == FILE:",
            "                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.",
            "                    num_files += 1",
            "                    if (",
            "                        settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None",
            "                        and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES",
            "                    ):",
            "                        raise TooManyFilesSent(",
            "                            \"The number of files exceeded \"",
            "                            \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"",
            "                        )",
            "                    # This is a file, use the handler...",
            "                    file_name = disposition.get(\"filename\")",
            "                    if file_name:",
            "                        file_name = force_str(file_name, encoding, errors=\"replace\")",
            "                        file_name = self.sanitize_file_name(file_name)",
            "                    if not file_name:",
            "                        continue",
            "",
            "                    content_type, content_type_extra = meta_data.get(",
            "                        \"content-type\", (\"\", {})",
            "                    )",
            "                    content_type = content_type.strip()",
            "                    charset = content_type_extra.get(\"charset\")",
            "",
            "                    try:",
            "                        content_length = int(meta_data.get(\"content-length\")[0])",
            "                    except (IndexError, TypeError, ValueError):",
            "                        content_length = None",
            "",
            "                    counters = [0] * len(handlers)",
            "                    uploaded_file = False",
            "                    try:",
            "                        for handler in handlers:",
            "                            try:",
            "                                handler.new_file(",
            "                                    field_name,",
            "                                    file_name,",
            "                                    content_type,",
            "                                    content_length,",
            "                                    charset,",
            "                                    content_type_extra,",
            "                                )",
            "                            except StopFutureHandlers:",
            "                                break",
            "",
            "                        for chunk in field_stream:",
            "                            if transfer_encoding == \"base64\":",
            "                                # We only special-case base64 transfer encoding",
            "                                # We should always decode base64 chunks by",
            "                                # multiple of 4, ignoring whitespace.",
            "",
            "                                stripped_chunk = b\"\".join(chunk.split())",
            "",
            "                                remaining = len(stripped_chunk) % 4",
            "                                while remaining != 0:",
            "                                    over_chunk = field_stream.read(4 - remaining)",
            "                                    if not over_chunk:",
            "                                        break",
            "                                    stripped_chunk += b\"\".join(over_chunk.split())",
            "                                    remaining = len(stripped_chunk) % 4",
            "",
            "                                try:",
            "                                    chunk = base64.b64decode(stripped_chunk)",
            "                                except Exception as exc:",
            "                                    # Since this is only a chunk, any error is",
            "                                    # an unfixable error.",
            "                                    raise MultiPartParserError(",
            "                                        \"Could not decode base64 data.\"",
            "                                    ) from exc",
            "",
            "                            for i, handler in enumerate(handlers):",
            "                                chunk_length = len(chunk)",
            "                                chunk = handler.receive_data_chunk(chunk, counters[i])",
            "                                counters[i] += chunk_length",
            "                                if chunk is None:",
            "                                    # Don't continue if the chunk received by",
            "                                    # the handler is None.",
            "                                    break",
            "",
            "                    except SkipFile:",
            "                        self._close_files()",
            "                        # Just use up the rest of this file...",
            "                        exhaust(field_stream)",
            "                    else:",
            "                        # Handle file upload completions on next iteration.",
            "                        old_field_name = field_name",
            "                else:",
            "                    # If this is neither a FIELD nor a FILE, exhaust the field",
            "                    # stream. Note: There could be an error here at some point,",
            "                    # but there will be at least two RAW types (before and",
            "                    # after the other boundaries). This branch is usually not",
            "                    # reached at all, because a missing content-disposition",
            "                    # header will skip the whole boundary.",
            "                    exhaust(field_stream)",
            "        except StopUpload as e:",
            "            self._close_files()",
            "            if not e.connection_reset:",
            "                exhaust(self._input_data)",
            "        else:",
            "            if not uploaded_file:",
            "                for handler in handlers:",
            "                    handler.upload_interrupted()",
            "            # Make sure that the request data is all fed",
            "            exhaust(self._input_data)",
            "",
            "        # Signal that the upload has completed.",
            "        # any() shortcircuits if a handler's upload_complete() returns a value.",
            "        any(handler.upload_complete() for handler in handlers)",
            "        self._post._mutable = False",
            "        return self._post, self._files",
            "",
            "    def handle_file_complete(self, old_field_name, counters):",
            "        \"\"\"",
            "        Handle all the signaling that takes place when a file is complete.",
            "        \"\"\"",
            "        for i, handler in enumerate(self._upload_handlers):",
            "            file_obj = handler.file_complete(counters[i])",
            "            if file_obj:",
            "                # If it returns a file object, then set the files dict.",
            "                self._files.appendlist(",
            "                    force_str(old_field_name, self._encoding, errors=\"replace\"),",
            "                    file_obj,",
            "                )",
            "                break",
            "",
            "    def sanitize_file_name(self, file_name):",
            "        \"\"\"",
            "        Sanitize the filename of an upload.",
            "",
            "        Remove all possible path separators, even though that might remove more",
            "        than actually required by the target system. Filenames that could",
            "        potentially cause problems (current/parent dir) are also discarded.",
            "",
            "        It should be noted that this function could still return a \"filepath\"",
            "        like \"C:some_file.txt\" which is handled later on by the storage layer.",
            "        So while this function does sanitize filenames to some extent, the",
            "        resulting filename should still be considered as untrusted user input.",
            "        \"\"\"",
            "        file_name = html.unescape(file_name)",
            "        file_name = file_name.rsplit(\"/\")[-1]",
            "        file_name = file_name.rsplit(\"\\\\\")[-1]",
            "        # Remove non-printable characters.",
            "        file_name = \"\".join([char for char in file_name if char.isprintable()])",
            "",
            "        if file_name in {\"\", \".\", \"..\"}:",
            "            return None",
            "        return file_name",
            "",
            "    IE_sanitize = sanitize_file_name",
            "",
            "    def _close_files(self):",
            "        # Free up all file handles.",
            "        # FIXME: this currently assumes that upload handlers store the file as 'file'",
            "        # We should document that...",
            "        # (Maybe add handler.free_file to complement new_file)",
            "        for handler in self._upload_handlers:",
            "            if hasattr(handler, \"file\"):",
            "                handler.file.close()",
            "",
            "",
            "class LazyStream:",
            "    \"\"\"",
            "    The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.",
            "",
            "    Given a producer object (an iterator that yields bytestrings), the",
            "    LazyStream object will support iteration, reading, and keeping a \"look-back\"",
            "    variable in case you need to \"unget\" some bytes.",
            "    \"\"\"",
            "",
            "    def __init__(self, producer, length=None):",
            "        \"\"\"",
            "        Every LazyStream must have a producer when instantiated.",
            "",
            "        A producer is an iterable that returns a string each time it",
            "        is called.",
            "        \"\"\"",
            "        self._producer = producer",
            "        self._empty = False",
            "        self._leftover = b\"\"",
            "        self.length = length",
            "        self.position = 0",
            "        self._remaining = length",
            "        self._unget_history = []",
            "",
            "    def tell(self):",
            "        return self.position",
            "",
            "    def read(self, size=None):",
            "        def parts():",
            "            remaining = self._remaining if size is None else size",
            "            # do the whole thing in one shot if no limit was provided.",
            "            if remaining is None:",
            "                yield b\"\".join(self)",
            "                return",
            "",
            "            # otherwise do some bookkeeping to return exactly enough",
            "            # of the stream and stashing any extra content we get from",
            "            # the producer",
            "            while remaining != 0:",
            "                assert remaining > 0, \"remaining bytes to read should never go negative\"",
            "",
            "                try:",
            "                    chunk = next(self)",
            "                except StopIteration:",
            "                    return",
            "                else:",
            "                    emitting = chunk[:remaining]",
            "                    self.unget(chunk[remaining:])",
            "                    remaining -= len(emitting)",
            "                    yield emitting",
            "",
            "        return b\"\".join(parts())",
            "",
            "    def __next__(self):",
            "        \"\"\"",
            "        Used when the exact number of bytes to read is unimportant.",
            "",
            "        Return whatever chunk is conveniently returned from the iterator.",
            "        Useful to avoid unnecessary bookkeeping if performance is an issue.",
            "        \"\"\"",
            "        if self._leftover:",
            "            output = self._leftover",
            "            self._leftover = b\"\"",
            "        else:",
            "            output = next(self._producer)",
            "            self._unget_history = []",
            "        self.position += len(output)",
            "        return output",
            "",
            "    def close(self):",
            "        \"\"\"",
            "        Used to invalidate/disable this lazy stream.",
            "",
            "        Replace the producer with an empty list. Any leftover bytes that have",
            "        already been read will still be reported upon read() and/or next().",
            "        \"\"\"",
            "        self._producer = []",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "    def unget(self, bytes):",
            "        \"\"\"",
            "        Place bytes back onto the front of the lazy stream.",
            "",
            "        Future calls to read() will return those bytes first. The",
            "        stream position and thus tell() will be rewound.",
            "        \"\"\"",
            "        if not bytes:",
            "            return",
            "        self._update_unget_history(len(bytes))",
            "        self.position -= len(bytes)",
            "        self._leftover = bytes + self._leftover",
            "",
            "    def _update_unget_history(self, num_bytes):",
            "        \"\"\"",
            "        Update the unget history as a sanity check to see if we've pushed",
            "        back the same number of bytes in one chunk. If we keep ungetting the",
            "        same number of bytes many times (here, 50), we're mostly likely in an",
            "        infinite loop of some sort. This is usually caused by a",
            "        maliciously-malformed MIME request.",
            "        \"\"\"",
            "        self._unget_history = [num_bytes] + self._unget_history[:49]",
            "        number_equal = len(",
            "            [",
            "                current_number",
            "                for current_number in self._unget_history",
            "                if current_number == num_bytes",
            "            ]",
            "        )",
            "",
            "        if number_equal > 40:",
            "            raise SuspiciousMultipartForm(",
            "                \"The multipart parser got stuck, which shouldn't happen with\"",
            "                \" normal uploaded files. Check for malicious upload activity;\"",
            "                \" if there is none, report this to the Django developers.\"",
            "            )",
            "",
            "",
            "class ChunkIter:",
            "    \"\"\"",
            "    An iterable that will yield chunks of data. Given a file-like object as the",
            "    constructor, yield chunks of read operations from that object.",
            "    \"\"\"",
            "",
            "    def __init__(self, flo, chunk_size=64 * 1024):",
            "        self.flo = flo",
            "        self.chunk_size = chunk_size",
            "",
            "    def __next__(self):",
            "        try:",
            "            data = self.flo.read(self.chunk_size)",
            "        except InputStreamExhausted:",
            "            raise StopIteration()",
            "        if data:",
            "            return data",
            "        else:",
            "            raise StopIteration()",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "",
            "class InterBoundaryIter:",
            "    \"\"\"",
            "    A Producer that will iterate over boundaries.",
            "    \"\"\"",
            "",
            "    def __init__(self, stream, boundary):",
            "        self._stream = stream",
            "        self._boundary = boundary",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "    def __next__(self):",
            "        try:",
            "            return LazyStream(BoundaryIter(self._stream, self._boundary))",
            "        except InputStreamExhausted:",
            "            raise StopIteration()",
            "",
            "",
            "class BoundaryIter:",
            "    \"\"\"",
            "    A Producer that is sensitive to boundaries.",
            "",
            "    Will happily yield bytes until a boundary is found. Will yield the bytes",
            "    before the boundary, throw away the boundary bytes themselves, and push the",
            "    post-boundary bytes back on the stream.",
            "",
            "    The future calls to next() after locating the boundary will raise a",
            "    StopIteration exception.",
            "    \"\"\"",
            "",
            "    def __init__(self, stream, boundary):",
            "        self._stream = stream",
            "        self._boundary = boundary",
            "        self._done = False",
            "        # rollback an additional six bytes because the format is like",
            "        # this: CRLF<boundary>[--CRLF]",
            "        self._rollback = len(boundary) + 6",
            "",
            "        # Try to use mx fast string search if available. Otherwise",
            "        # use Python find. Wrap the latter for consistency.",
            "        unused_char = self._stream.read(1)",
            "        if not unused_char:",
            "            raise InputStreamExhausted()",
            "        self._stream.unget(unused_char)",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "    def __next__(self):",
            "        if self._done:",
            "            raise StopIteration()",
            "",
            "        stream = self._stream",
            "        rollback = self._rollback",
            "",
            "        bytes_read = 0",
            "        chunks = []",
            "        for bytes in stream:",
            "            bytes_read += len(bytes)",
            "            chunks.append(bytes)",
            "            if bytes_read > rollback:",
            "                break",
            "            if not bytes:",
            "                break",
            "        else:",
            "            self._done = True",
            "",
            "        if not chunks:",
            "            raise StopIteration()",
            "",
            "        chunk = b\"\".join(chunks)",
            "        boundary = self._find_boundary(chunk)",
            "",
            "        if boundary:",
            "            end, next = boundary",
            "            stream.unget(chunk[next:])",
            "            self._done = True",
            "            return chunk[:end]",
            "        else:",
            "            # make sure we don't treat a partial boundary (and",
            "            # its separators) as data",
            "            if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):",
            "                # There's nothing left, we should just return and mark as done.",
            "                self._done = True",
            "                return chunk",
            "            else:",
            "                stream.unget(chunk[-rollback:])",
            "                return chunk[:-rollback]",
            "",
            "    def _find_boundary(self, data):",
            "        \"\"\"",
            "        Find a multipart boundary in data.",
            "",
            "        Should no boundary exist in the data, return None. Otherwise, return",
            "        a tuple containing the indices of the following:",
            "         * the end of current encapsulation",
            "         * the start of the next encapsulation",
            "        \"\"\"",
            "        index = data.find(self._boundary)",
            "        if index < 0:",
            "            return None",
            "        else:",
            "            end = index",
            "            next = index + len(self._boundary)",
            "            # backup over CRLF",
            "            last = max(0, end - 1)",
            "            if data[last : last + 1] == b\"\\n\":",
            "                end -= 1",
            "            last = max(0, end - 1)",
            "            if data[last : last + 1] == b\"\\r\":",
            "                end -= 1",
            "            return end, next",
            "",
            "",
            "def exhaust(stream_or_iterable):",
            "    \"\"\"Exhaust an iterator or stream.\"\"\"",
            "    try:",
            "        iterator = iter(stream_or_iterable)",
            "    except TypeError:",
            "        iterator = ChunkIter(stream_or_iterable, 16384)",
            "    collections.deque(iterator, maxlen=0)  # consume iterator quickly.",
            "",
            "",
            "def parse_boundary_stream(stream, max_header_size):",
            "    \"\"\"",
            "    Parse one and exactly one stream that encapsulates a boundary.",
            "    \"\"\"",
            "    # Stream at beginning of header, look for end of header",
            "    # and parse it if found. The header must fit within one",
            "    # chunk.",
            "    chunk = stream.read(max_header_size)",
            "",
            "    # 'find' returns the top of these four bytes, so we'll",
            "    # need to munch them later to prevent them from polluting",
            "    # the payload.",
            "    header_end = chunk.find(b\"\\r\\n\\r\\n\")",
            "",
            "    def _parse_header(line):",
            "        main_value_pair, params = parse_header(line)",
            "        try:",
            "            name, value = main_value_pair.split(\":\", 1)",
            "        except ValueError:",
            "            raise ValueError(\"Invalid header: %r\" % line)",
            "        return name, (value, params)",
            "",
            "    if header_end == -1:",
            "        # we find no header, so we just mark this fact and pass on",
            "        # the stream verbatim",
            "        stream.unget(chunk)",
            "        return (RAW, {}, stream)",
            "",
            "    header = chunk[:header_end]",
            "",
            "    # here we place any excess chunk back onto the stream, as",
            "    # well as throwing away the CRLFCRLF bytes from above.",
            "    stream.unget(chunk[header_end + 4 :])",
            "",
            "    TYPE = RAW",
            "    outdict = {}",
            "",
            "    # Eliminate blank lines",
            "    for line in header.split(b\"\\r\\n\"):",
            "        # This terminology (\"main value\" and \"dictionary of",
            "        # parameters\") is from the Python docs.",
            "        try:",
            "            name, (value, params) = _parse_header(line)",
            "        except ValueError:",
            "            continue",
            "",
            "        if name == \"content-disposition\":",
            "            TYPE = FIELD",
            "            if params.get(\"filename\"):",
            "                TYPE = FILE",
            "",
            "        outdict[name] = value, params",
            "",
            "    if TYPE == RAW:",
            "        stream.unget(chunk)",
            "",
            "    return (TYPE, outdict, stream)",
            "",
            "",
            "class Parser:",
            "    def __init__(self, stream, boundary):",
            "        self._stream = stream",
            "        self._separator = b\"--\" + boundary",
            "",
            "    def __iter__(self):",
            "        boundarystream = InterBoundaryIter(self._stream, self._separator)",
            "        for sub_stream in boundarystream:",
            "            # Iterate over each part",
            "            yield parse_boundary_stream(sub_stream, 1024)",
            "",
            "",
            "def parse_header(line):",
            "    \"\"\"",
            "    Parse the header into a key-value.",
            "",
            "    Input (line): bytes, output: str for key/name, bytes for values which",
            "    will be decoded later.",
            "    \"\"\"",
            "    plist = _parse_header_params(b\";\" + line)",
            "    key = plist.pop(0).lower().decode(\"ascii\")",
            "    pdict = {}",
            "    for p in plist:",
            "        i = p.find(b\"=\")",
            "        if i >= 0:",
            "            has_encoding = False",
            "            name = p[:i].strip().lower().decode(\"ascii\")",
            "            if name.endswith(\"*\"):",
            "                # Lang/encoding embedded in the value (like \"filename*=UTF-8''file.ext\")",
            "                # https://tools.ietf.org/html/rfc2231#section-4",
            "                name = name[:-1]",
            "                if p.count(b\"'\") == 2:",
            "                    has_encoding = True",
            "            value = p[i + 1 :].strip()",
            "            if len(value) >= 2 and value[:1] == value[-1:] == b'\"':",
            "                value = value[1:-1]",
            "                value = value.replace(b\"\\\\\\\\\", b\"\\\\\").replace(b'\\\\\"', b'\"')",
            "            if has_encoding:",
            "                encoding, lang, value = value.split(b\"'\")",
            "                value = unquote(value.decode(), encoding=encoding.decode())",
            "            pdict[name] = value",
            "    return key, pdict",
            "",
            "",
            "def _parse_header_params(s):",
            "    plist = []",
            "    while s[:1] == b\";\":",
            "        s = s[1:]",
            "        end = s.find(b\";\")",
            "        while end > 0 and (s.count(b'\"', 0, end) - s.count(b'\\\\\"', 0, end)) % 2:",
            "            end = s.find(b\";\", end + 1)",
            "        if end < 0:",
            "            end = len(s)",
            "        f = s[:end]",
            "        plist.append(f.strip())",
            "        s = s[end:]",
            "    return plist"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "186": [
                "MultiPartParser",
                "parse"
            ],
            "187": [
                "MultiPartParser",
                "parse"
            ],
            "188": [
                "MultiPartParser",
                "parse"
            ],
            "189": [
                "MultiPartParser",
                "parse"
            ],
            "190": [
                "MultiPartParser",
                "parse"
            ],
            "191": [
                "MultiPartParser",
                "parse"
            ],
            "192": [
                "MultiPartParser",
                "parse"
            ],
            "193": [
                "MultiPartParser",
                "parse"
            ],
            "194": [
                "MultiPartParser",
                "parse"
            ],
            "195": [
                "MultiPartParser",
                "parse"
            ],
            "196": [
                "MultiPartParser",
                "parse"
            ],
            "308": [
                "MultiPartParser",
                "parse"
            ],
            "309": [
                "MultiPartParser",
                "parse"
            ]
        },
        "addLocation": [
            "django.http.multipartparser.MultiPartParser.parse",
            "django.http.multipartparser.MultiPartParser.self",
            "django.http.multipartparser.MultiPartParser.handle_file_complete.counters",
            "vyper.semantics.analysis.local.FunctionAnalyzer"
        ]
    },
    "django/http/request.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": "     TooManyFieldsSent,"
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " )"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from django.core.files import uploadhandler"
            },
            "3": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from django.http.multipartparser import MultiPartParser, MultiPartParserError"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from django.http.multipartparser import ("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 17,
                "PatchRowcode": "+    MultiPartParser,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 18,
                "PatchRowcode": "+    MultiPartParserError,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+    TooManyFilesSent,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+)"
            },
            "9": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from django.utils.datastructures import ("
            },
            "10": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 22,
                "PatchRowcode": "     CaseInsensitiveMapping,"
            },
            "11": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 23,
                "PatchRowcode": "     ImmutableList,"
            },
            "12": {
                "beforePatchRowNumber": 367,
                "afterPatchRowNumber": 371,
                "PatchRowcode": "                 data = self"
            },
            "13": {
                "beforePatchRowNumber": 368,
                "afterPatchRowNumber": 372,
                "PatchRowcode": "             try:"
            },
            "14": {
                "beforePatchRowNumber": 369,
                "afterPatchRowNumber": 373,
                "PatchRowcode": "                 self._post, self._files = self.parse_file_upload(self.META, data)"
            },
            "15": {
                "beforePatchRowNumber": 370,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            except MultiPartParserError:"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 374,
                "PatchRowcode": "+            except (MultiPartParserError, TooManyFilesSent):"
            },
            "17": {
                "beforePatchRowNumber": 371,
                "afterPatchRowNumber": 375,
                "PatchRowcode": "                 # An error occurred while parsing POST data. Since when"
            },
            "18": {
                "beforePatchRowNumber": 372,
                "afterPatchRowNumber": 376,
                "PatchRowcode": "                 # formatting the error the request handler might access"
            },
            "19": {
                "beforePatchRowNumber": 373,
                "afterPatchRowNumber": 377,
                "PatchRowcode": "                 # self.POST, set self._post and self._file to prevent"
            }
        },
        "frontPatchFile": [
            "import codecs",
            "import copy",
            "from io import BytesIO",
            "from itertools import chain",
            "from urllib.parse import parse_qsl, quote, urlencode, urljoin, urlsplit",
            "",
            "from django.conf import settings",
            "from django.core import signing",
            "from django.core.exceptions import (",
            "    DisallowedHost,",
            "    ImproperlyConfigured,",
            "    RequestDataTooBig,",
            "    TooManyFieldsSent,",
            ")",
            "from django.core.files import uploadhandler",
            "from django.http.multipartparser import MultiPartParser, MultiPartParserError",
            "from django.utils.datastructures import (",
            "    CaseInsensitiveMapping,",
            "    ImmutableList,",
            "    MultiValueDict,",
            ")",
            "from django.utils.encoding import escape_uri_path, iri_to_uri",
            "from django.utils.functional import cached_property",
            "from django.utils.http import is_same_domain, parse_header_parameters",
            "from django.utils.regex_helper import _lazy_re_compile",
            "",
            "from .multipartparser import parse_header",
            "",
            "RAISE_ERROR = object()",
            "host_validation_re = _lazy_re_compile(",
            "    r\"^([a-z0-9.-]+|\\[[a-f0-9]*:[a-f0-9\\.:]+\\])(:[0-9]+)?$\"",
            ")",
            "",
            "",
            "class UnreadablePostError(OSError):",
            "    pass",
            "",
            "",
            "class RawPostDataException(Exception):",
            "    \"\"\"",
            "    You cannot access raw_post_data from a request that has",
            "    multipart/* POST data if it has been accessed via POST,",
            "    FILES, etc..",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class HttpRequest:",
            "    \"\"\"A basic HTTP request.\"\"\"",
            "",
            "    # The encoding used in GET/POST dicts. None means use default setting.",
            "    _encoding = None",
            "    _upload_handlers = []",
            "",
            "    def __init__(self):",
            "        # WARNING: The `WSGIRequest` subclass doesn't call `super`.",
            "        # Any variable assignment made here should also happen in",
            "        # `WSGIRequest.__init__()`.",
            "",
            "        self.GET = QueryDict(mutable=True)",
            "        self.POST = QueryDict(mutable=True)",
            "        self.COOKIES = {}",
            "        self.META = {}",
            "        self.FILES = MultiValueDict()",
            "",
            "        self.path = \"\"",
            "        self.path_info = \"\"",
            "        self.method = None",
            "        self.resolver_match = None",
            "        self.content_type = None",
            "        self.content_params = None",
            "",
            "    def __repr__(self):",
            "        if self.method is None or not self.get_full_path():",
            "            return \"<%s>\" % self.__class__.__name__",
            "        return \"<%s: %s %r>\" % (",
            "            self.__class__.__name__,",
            "            self.method,",
            "            self.get_full_path(),",
            "        )",
            "",
            "    @cached_property",
            "    def headers(self):",
            "        return HttpHeaders(self.META)",
            "",
            "    @cached_property",
            "    def accepted_types(self):",
            "        \"\"\"Return a list of MediaType instances.\"\"\"",
            "        return parse_accept_header(self.headers.get(\"Accept\", \"*/*\"))",
            "",
            "    def accepts(self, media_type):",
            "        return any(",
            "            accepted_type.match(media_type) for accepted_type in self.accepted_types",
            "        )",
            "",
            "    def _set_content_type_params(self, meta):",
            "        \"\"\"Set content_type, content_params, and encoding.\"\"\"",
            "        self.content_type, self.content_params = parse_header_parameters(",
            "            meta.get(\"CONTENT_TYPE\", \"\")",
            "        )",
            "        if \"charset\" in self.content_params:",
            "            try:",
            "                codecs.lookup(self.content_params[\"charset\"])",
            "            except LookupError:",
            "                pass",
            "            else:",
            "                self.encoding = self.content_params[\"charset\"]",
            "",
            "    def _get_raw_host(self):",
            "        \"\"\"",
            "        Return the HTTP host using the environment or request headers. Skip",
            "        allowed hosts protection, so may return an insecure host.",
            "        \"\"\"",
            "        # We try three options, in order of decreasing preference.",
            "        if settings.USE_X_FORWARDED_HOST and (\"HTTP_X_FORWARDED_HOST\" in self.META):",
            "            host = self.META[\"HTTP_X_FORWARDED_HOST\"]",
            "        elif \"HTTP_HOST\" in self.META:",
            "            host = self.META[\"HTTP_HOST\"]",
            "        else:",
            "            # Reconstruct the host using the algorithm from PEP 333.",
            "            host = self.META[\"SERVER_NAME\"]",
            "            server_port = self.get_port()",
            "            if server_port != (\"443\" if self.is_secure() else \"80\"):",
            "                host = \"%s:%s\" % (host, server_port)",
            "        return host",
            "",
            "    def get_host(self):",
            "        \"\"\"Return the HTTP host using the environment or request headers.\"\"\"",
            "        host = self._get_raw_host()",
            "",
            "        # Allow variants of localhost if ALLOWED_HOSTS is empty and DEBUG=True.",
            "        allowed_hosts = settings.ALLOWED_HOSTS",
            "        if settings.DEBUG and not allowed_hosts:",
            "            allowed_hosts = [\".localhost\", \"127.0.0.1\", \"[::1]\"]",
            "",
            "        domain, port = split_domain_port(host)",
            "        if domain and validate_host(domain, allowed_hosts):",
            "            return host",
            "        else:",
            "            msg = \"Invalid HTTP_HOST header: %r.\" % host",
            "            if domain:",
            "                msg += \" You may need to add %r to ALLOWED_HOSTS.\" % domain",
            "            else:",
            "                msg += (",
            "                    \" The domain name provided is not valid according to RFC 1034/1035.\"",
            "                )",
            "            raise DisallowedHost(msg)",
            "",
            "    def get_port(self):",
            "        \"\"\"Return the port number for the request as a string.\"\"\"",
            "        if settings.USE_X_FORWARDED_PORT and \"HTTP_X_FORWARDED_PORT\" in self.META:",
            "            port = self.META[\"HTTP_X_FORWARDED_PORT\"]",
            "        else:",
            "            port = self.META[\"SERVER_PORT\"]",
            "        return str(port)",
            "",
            "    def get_full_path(self, force_append_slash=False):",
            "        return self._get_full_path(self.path, force_append_slash)",
            "",
            "    def get_full_path_info(self, force_append_slash=False):",
            "        return self._get_full_path(self.path_info, force_append_slash)",
            "",
            "    def _get_full_path(self, path, force_append_slash):",
            "        # RFC 3986 requires query string arguments to be in the ASCII range.",
            "        # Rather than crash if this doesn't happen, we encode defensively.",
            "        return \"%s%s%s\" % (",
            "            escape_uri_path(path),",
            "            \"/\" if force_append_slash and not path.endswith(\"/\") else \"\",",
            "            (\"?\" + iri_to_uri(self.META.get(\"QUERY_STRING\", \"\")))",
            "            if self.META.get(\"QUERY_STRING\", \"\")",
            "            else \"\",",
            "        )",
            "",
            "    def get_signed_cookie(self, key, default=RAISE_ERROR, salt=\"\", max_age=None):",
            "        \"\"\"",
            "        Attempt to return a signed cookie. If the signature fails or the",
            "        cookie has expired, raise an exception, unless the `default` argument",
            "        is provided,  in which case return that value.",
            "        \"\"\"",
            "        try:",
            "            cookie_value = self.COOKIES[key]",
            "        except KeyError:",
            "            if default is not RAISE_ERROR:",
            "                return default",
            "            else:",
            "                raise",
            "        try:",
            "            value = signing.get_cookie_signer(salt=key + salt).unsign(",
            "                cookie_value, max_age=max_age",
            "            )",
            "        except signing.BadSignature:",
            "            if default is not RAISE_ERROR:",
            "                return default",
            "            else:",
            "                raise",
            "        return value",
            "",
            "    def build_absolute_uri(self, location=None):",
            "        \"\"\"",
            "        Build an absolute URI from the location and the variables available in",
            "        this request. If no ``location`` is specified, build the absolute URI",
            "        using request.get_full_path(). If the location is absolute, convert it",
            "        to an RFC 3987 compliant URI and return it. If location is relative or",
            "        is scheme-relative (i.e., ``//example.com/``), urljoin() it to a base",
            "        URL constructed from the request variables.",
            "        \"\"\"",
            "        if location is None:",
            "            # Make it an absolute url (but schemeless and domainless) for the",
            "            # edge case that the path starts with '//'.",
            "            location = \"//%s\" % self.get_full_path()",
            "        else:",
            "            # Coerce lazy locations.",
            "            location = str(location)",
            "        bits = urlsplit(location)",
            "        if not (bits.scheme and bits.netloc):",
            "            # Handle the simple, most common case. If the location is absolute",
            "            # and a scheme or host (netloc) isn't provided, skip an expensive",
            "            # urljoin() as long as no path segments are '.' or '..'.",
            "            if (",
            "                bits.path.startswith(\"/\")",
            "                and not bits.scheme",
            "                and not bits.netloc",
            "                and \"/./\" not in bits.path",
            "                and \"/../\" not in bits.path",
            "            ):",
            "                # If location starts with '//' but has no netloc, reuse the",
            "                # schema and netloc from the current request. Strip the double",
            "                # slashes and continue as if it wasn't specified.",
            "                if location.startswith(\"//\"):",
            "                    location = location[2:]",
            "                location = self._current_scheme_host + location",
            "            else:",
            "                # Join the constructed URL with the provided location, which",
            "                # allows the provided location to apply query strings to the",
            "                # base path.",
            "                location = urljoin(self._current_scheme_host + self.path, location)",
            "        return iri_to_uri(location)",
            "",
            "    @cached_property",
            "    def _current_scheme_host(self):",
            "        return \"{}://{}\".format(self.scheme, self.get_host())",
            "",
            "    def _get_scheme(self):",
            "        \"\"\"",
            "        Hook for subclasses like WSGIRequest to implement. Return 'http' by",
            "        default.",
            "        \"\"\"",
            "        return \"http\"",
            "",
            "    @property",
            "    def scheme(self):",
            "        if settings.SECURE_PROXY_SSL_HEADER:",
            "            try:",
            "                header, secure_value = settings.SECURE_PROXY_SSL_HEADER",
            "            except ValueError:",
            "                raise ImproperlyConfigured(",
            "                    \"The SECURE_PROXY_SSL_HEADER setting must be a tuple containing \"",
            "                    \"two values.\"",
            "                )",
            "            header_value = self.META.get(header)",
            "            if header_value is not None:",
            "                header_value, *_ = header_value.split(\",\", 1)",
            "                return \"https\" if header_value.strip() == secure_value else \"http\"",
            "        return self._get_scheme()",
            "",
            "    def is_secure(self):",
            "        return self.scheme == \"https\"",
            "",
            "    @property",
            "    def encoding(self):",
            "        return self._encoding",
            "",
            "    @encoding.setter",
            "    def encoding(self, val):",
            "        \"\"\"",
            "        Set the encoding used for GET/POST accesses. If the GET or POST",
            "        dictionary has already been created, remove and recreate it on the",
            "        next access (so that it is decoded correctly).",
            "        \"\"\"",
            "        self._encoding = val",
            "        if hasattr(self, \"GET\"):",
            "            del self.GET",
            "        if hasattr(self, \"_post\"):",
            "            del self._post",
            "",
            "    def _initialize_handlers(self):",
            "        self._upload_handlers = [",
            "            uploadhandler.load_handler(handler, self)",
            "            for handler in settings.FILE_UPLOAD_HANDLERS",
            "        ]",
            "",
            "    @property",
            "    def upload_handlers(self):",
            "        if not self._upload_handlers:",
            "            # If there are no upload handlers defined, initialize them from settings.",
            "            self._initialize_handlers()",
            "        return self._upload_handlers",
            "",
            "    @upload_handlers.setter",
            "    def upload_handlers(self, upload_handlers):",
            "        if hasattr(self, \"_files\"):",
            "            raise AttributeError(",
            "                \"You cannot set the upload handlers after the upload has been \"",
            "                \"processed.\"",
            "            )",
            "        self._upload_handlers = upload_handlers",
            "",
            "    def parse_file_upload(self, META, post_data):",
            "        \"\"\"Return a tuple of (POST QueryDict, FILES MultiValueDict).\"\"\"",
            "        self.upload_handlers = ImmutableList(",
            "            self.upload_handlers,",
            "            warning=(",
            "                \"You cannot alter upload handlers after the upload has been \"",
            "                \"processed.\"",
            "            ),",
            "        )",
            "        parser = MultiPartParser(META, post_data, self.upload_handlers, self.encoding)",
            "        return parser.parse()",
            "",
            "    @property",
            "    def body(self):",
            "        if not hasattr(self, \"_body\"):",
            "            if self._read_started:",
            "                raise RawPostDataException(",
            "                    \"You cannot access body after reading from request's data stream\"",
            "                )",
            "",
            "            # Limit the maximum request data size that will be handled in-memory.",
            "            if (",
            "                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None",
            "                and int(self.META.get(\"CONTENT_LENGTH\") or 0)",
            "                > settings.DATA_UPLOAD_MAX_MEMORY_SIZE",
            "            ):",
            "                raise RequestDataTooBig(",
            "                    \"Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"",
            "                )",
            "",
            "            try:",
            "                self._body = self.read()",
            "            except OSError as e:",
            "                raise UnreadablePostError(*e.args) from e",
            "            self._stream = BytesIO(self._body)",
            "        return self._body",
            "",
            "    def _mark_post_parse_error(self):",
            "        self._post = QueryDict()",
            "        self._files = MultiValueDict()",
            "",
            "    def _load_post_and_files(self):",
            "        \"\"\"Populate self._post and self._files if the content-type is a form type\"\"\"",
            "        if self.method != \"POST\":",
            "            self._post, self._files = (",
            "                QueryDict(encoding=self._encoding),",
            "                MultiValueDict(),",
            "            )",
            "            return",
            "        if self._read_started and not hasattr(self, \"_body\"):",
            "            self._mark_post_parse_error()",
            "            return",
            "",
            "        if self.content_type == \"multipart/form-data\":",
            "            if hasattr(self, \"_body\"):",
            "                # Use already read data",
            "                data = BytesIO(self._body)",
            "            else:",
            "                data = self",
            "            try:",
            "                self._post, self._files = self.parse_file_upload(self.META, data)",
            "            except MultiPartParserError:",
            "                # An error occurred while parsing POST data. Since when",
            "                # formatting the error the request handler might access",
            "                # self.POST, set self._post and self._file to prevent",
            "                # attempts to parse POST data again.",
            "                self._mark_post_parse_error()",
            "                raise",
            "        elif self.content_type == \"application/x-www-form-urlencoded\":",
            "            self._post, self._files = (",
            "                QueryDict(self.body, encoding=self._encoding),",
            "                MultiValueDict(),",
            "            )",
            "        else:",
            "            self._post, self._files = (",
            "                QueryDict(encoding=self._encoding),",
            "                MultiValueDict(),",
            "            )",
            "",
            "    def close(self):",
            "        if hasattr(self, \"_files\"):",
            "            for f in chain.from_iterable(list_[1] for list_ in self._files.lists()):",
            "                f.close()",
            "",
            "    # File-like and iterator interface.",
            "    #",
            "    # Expects self._stream to be set to an appropriate source of bytes by",
            "    # a corresponding request subclass (e.g. WSGIRequest).",
            "    # Also when request data has already been read by request.POST or",
            "    # request.body, self._stream points to a BytesIO instance",
            "    # containing that data.",
            "",
            "    def read(self, *args, **kwargs):",
            "        self._read_started = True",
            "        try:",
            "            return self._stream.read(*args, **kwargs)",
            "        except OSError as e:",
            "            raise UnreadablePostError(*e.args) from e",
            "",
            "    def readline(self, *args, **kwargs):",
            "        self._read_started = True",
            "        try:",
            "            return self._stream.readline(*args, **kwargs)",
            "        except OSError as e:",
            "            raise UnreadablePostError(*e.args) from e",
            "",
            "    def __iter__(self):",
            "        return iter(self.readline, b\"\")",
            "",
            "    def readlines(self):",
            "        return list(self)",
            "",
            "",
            "class HttpHeaders(CaseInsensitiveMapping):",
            "    HTTP_PREFIX = \"HTTP_\"",
            "    # PEP 333 gives two headers which aren't prepended with HTTP_.",
            "    UNPREFIXED_HEADERS = {\"CONTENT_TYPE\", \"CONTENT_LENGTH\"}",
            "",
            "    def __init__(self, environ):",
            "        headers = {}",
            "        for header, value in environ.items():",
            "            name = self.parse_header_name(header)",
            "            if name:",
            "                headers[name] = value",
            "        super().__init__(headers)",
            "",
            "    def __getitem__(self, key):",
            "        \"\"\"Allow header lookup using underscores in place of hyphens.\"\"\"",
            "        return super().__getitem__(key.replace(\"_\", \"-\"))",
            "",
            "    @classmethod",
            "    def parse_header_name(cls, header):",
            "        if header.startswith(cls.HTTP_PREFIX):",
            "            header = header[len(cls.HTTP_PREFIX) :]",
            "        elif header not in cls.UNPREFIXED_HEADERS:",
            "            return None",
            "        return header.replace(\"_\", \"-\").title()",
            "",
            "",
            "class QueryDict(MultiValueDict):",
            "    \"\"\"",
            "    A specialized MultiValueDict which represents a query string.",
            "",
            "    A QueryDict can be used to represent GET or POST data. It subclasses",
            "    MultiValueDict since keys in such data can be repeated, for instance",
            "    in the data from a form with a <select multiple> field.",
            "",
            "    By default QueryDicts are immutable, though the copy() method",
            "    will always return a mutable copy.",
            "",
            "    Both keys and values set on this class are converted from the given encoding",
            "    (DEFAULT_CHARSET by default) to str.",
            "    \"\"\"",
            "",
            "    # These are both reset in __init__, but is specified here at the class",
            "    # level so that unpickling will have valid values",
            "    _mutable = True",
            "    _encoding = None",
            "",
            "    def __init__(self, query_string=None, mutable=False, encoding=None):",
            "        super().__init__()",
            "        self.encoding = encoding or settings.DEFAULT_CHARSET",
            "        query_string = query_string or \"\"",
            "        parse_qsl_kwargs = {",
            "            \"keep_blank_values\": True,",
            "            \"encoding\": self.encoding,",
            "            \"max_num_fields\": settings.DATA_UPLOAD_MAX_NUMBER_FIELDS,",
            "        }",
            "        if isinstance(query_string, bytes):",
            "            # query_string normally contains URL-encoded data, a subset of ASCII.",
            "            try:",
            "                query_string = query_string.decode(self.encoding)",
            "            except UnicodeDecodeError:",
            "                # ... but some user agents are misbehaving :-(",
            "                query_string = query_string.decode(\"iso-8859-1\")",
            "        try:",
            "            for key, value in parse_qsl(query_string, **parse_qsl_kwargs):",
            "                self.appendlist(key, value)",
            "        except ValueError as e:",
            "            # ValueError can also be raised if the strict_parsing argument to",
            "            # parse_qsl() is True. As that is not used by Django, assume that",
            "            # the exception was raised by exceeding the value of max_num_fields",
            "            # instead of fragile checks of exception message strings.",
            "            raise TooManyFieldsSent(",
            "                \"The number of GET/POST parameters exceeded \"",
            "                \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"",
            "            ) from e",
            "        self._mutable = mutable",
            "",
            "    @classmethod",
            "    def fromkeys(cls, iterable, value=\"\", mutable=False, encoding=None):",
            "        \"\"\"",
            "        Return a new QueryDict with keys (may be repeated) from an iterable and",
            "        values from value.",
            "        \"\"\"",
            "        q = cls(\"\", mutable=True, encoding=encoding)",
            "        for key in iterable:",
            "            q.appendlist(key, value)",
            "        if not mutable:",
            "            q._mutable = False",
            "        return q",
            "",
            "    @property",
            "    def encoding(self):",
            "        if self._encoding is None:",
            "            self._encoding = settings.DEFAULT_CHARSET",
            "        return self._encoding",
            "",
            "    @encoding.setter",
            "    def encoding(self, value):",
            "        self._encoding = value",
            "",
            "    def _assert_mutable(self):",
            "        if not self._mutable:",
            "            raise AttributeError(\"This QueryDict instance is immutable\")",
            "",
            "    def __setitem__(self, key, value):",
            "        self._assert_mutable()",
            "        key = bytes_to_text(key, self.encoding)",
            "        value = bytes_to_text(value, self.encoding)",
            "        super().__setitem__(key, value)",
            "",
            "    def __delitem__(self, key):",
            "        self._assert_mutable()",
            "        super().__delitem__(key)",
            "",
            "    def __copy__(self):",
            "        result = self.__class__(\"\", mutable=True, encoding=self.encoding)",
            "        for key, value in self.lists():",
            "            result.setlist(key, value)",
            "        return result",
            "",
            "    def __deepcopy__(self, memo):",
            "        result = self.__class__(\"\", mutable=True, encoding=self.encoding)",
            "        memo[id(self)] = result",
            "        for key, value in self.lists():",
            "            result.setlist(copy.deepcopy(key, memo), copy.deepcopy(value, memo))",
            "        return result",
            "",
            "    def setlist(self, key, list_):",
            "        self._assert_mutable()",
            "        key = bytes_to_text(key, self.encoding)",
            "        list_ = [bytes_to_text(elt, self.encoding) for elt in list_]",
            "        super().setlist(key, list_)",
            "",
            "    def setlistdefault(self, key, default_list=None):",
            "        self._assert_mutable()",
            "        return super().setlistdefault(key, default_list)",
            "",
            "    def appendlist(self, key, value):",
            "        self._assert_mutable()",
            "        key = bytes_to_text(key, self.encoding)",
            "        value = bytes_to_text(value, self.encoding)",
            "        super().appendlist(key, value)",
            "",
            "    def pop(self, key, *args):",
            "        self._assert_mutable()",
            "        return super().pop(key, *args)",
            "",
            "    def popitem(self):",
            "        self._assert_mutable()",
            "        return super().popitem()",
            "",
            "    def clear(self):",
            "        self._assert_mutable()",
            "        super().clear()",
            "",
            "    def setdefault(self, key, default=None):",
            "        self._assert_mutable()",
            "        key = bytes_to_text(key, self.encoding)",
            "        default = bytes_to_text(default, self.encoding)",
            "        return super().setdefault(key, default)",
            "",
            "    def copy(self):",
            "        \"\"\"Return a mutable copy of this object.\"\"\"",
            "        return self.__deepcopy__({})",
            "",
            "    def urlencode(self, safe=None):",
            "        \"\"\"",
            "        Return an encoded string of all query string arguments.",
            "",
            "        `safe` specifies characters which don't require quoting, for example::",
            "",
            "            >>> q = QueryDict(mutable=True)",
            "            >>> q['next'] = '/a&b/'",
            "            >>> q.urlencode()",
            "            'next=%2Fa%26b%2F'",
            "            >>> q.urlencode(safe='/')",
            "            'next=/a%26b/'",
            "        \"\"\"",
            "        output = []",
            "        if safe:",
            "            safe = safe.encode(self.encoding)",
            "",
            "            def encode(k, v):",
            "                return \"%s=%s\" % ((quote(k, safe), quote(v, safe)))",
            "",
            "        else:",
            "",
            "            def encode(k, v):",
            "                return urlencode({k: v})",
            "",
            "        for k, list_ in self.lists():",
            "            output.extend(",
            "                encode(k.encode(self.encoding), str(v).encode(self.encoding))",
            "                for v in list_",
            "            )",
            "        return \"&\".join(output)",
            "",
            "",
            "class MediaType:",
            "    def __init__(self, media_type_raw_line):",
            "        full_type, self.params = parse_header(",
            "            media_type_raw_line.encode(\"ascii\") if media_type_raw_line else b\"\"",
            "        )",
            "        self.main_type, _, self.sub_type = full_type.partition(\"/\")",
            "",
            "    def __str__(self):",
            "        params_str = \"\".join(",
            "            \"; %s=%s\" % (k, v.decode(\"ascii\")) for k, v in self.params.items()",
            "        )",
            "        return \"%s%s%s\" % (",
            "            self.main_type,",
            "            (\"/%s\" % self.sub_type) if self.sub_type else \"\",",
            "            params_str,",
            "        )",
            "",
            "    def __repr__(self):",
            "        return \"<%s: %s>\" % (self.__class__.__qualname__, self)",
            "",
            "    @property",
            "    def is_all_types(self):",
            "        return self.main_type == \"*\" and self.sub_type == \"*\"",
            "",
            "    def match(self, other):",
            "        if self.is_all_types:",
            "            return True",
            "        other = MediaType(other)",
            "        if self.main_type == other.main_type and self.sub_type in {\"*\", other.sub_type}:",
            "            return True",
            "        return False",
            "",
            "",
            "# It's neither necessary nor appropriate to use",
            "# django.utils.encoding.force_str() for parsing URLs and form inputs. Thus,",
            "# this slightly more restricted function, used by QueryDict.",
            "def bytes_to_text(s, encoding):",
            "    \"\"\"",
            "    Convert bytes objects to strings, using the given encoding. Illegally",
            "    encoded input characters are replaced with Unicode \"unknown\" codepoint",
            "    (\\ufffd).",
            "",
            "    Return any non-bytes objects without change.",
            "    \"\"\"",
            "    if isinstance(s, bytes):",
            "        return str(s, encoding, \"replace\")",
            "    else:",
            "        return s",
            "",
            "",
            "def split_domain_port(host):",
            "    \"\"\"",
            "    Return a (domain, port) tuple from a given host.",
            "",
            "    Returned domain is lowercased. If the host is invalid, the domain will be",
            "    empty.",
            "    \"\"\"",
            "    host = host.lower()",
            "",
            "    if not host_validation_re.match(host):",
            "        return \"\", \"\"",
            "",
            "    if host[-1] == \"]\":",
            "        # It's an IPv6 address without a port.",
            "        return host, \"\"",
            "    bits = host.rsplit(\":\", 1)",
            "    domain, port = bits if len(bits) == 2 else (bits[0], \"\")",
            "    # Remove a trailing dot (if present) from the domain.",
            "    domain = domain[:-1] if domain.endswith(\".\") else domain",
            "    return domain, port",
            "",
            "",
            "def validate_host(host, allowed_hosts):",
            "    \"\"\"",
            "    Validate the given host for this site.",
            "",
            "    Check that the host looks valid and matches a host or host pattern in the",
            "    given list of ``allowed_hosts``. Any pattern beginning with a period",
            "    matches a domain and all its subdomains (e.g. ``.example.com`` matches",
            "    ``example.com`` and any subdomain), ``*`` matches anything, and anything",
            "    else must match exactly.",
            "",
            "    Note: This function assumes that the given host is lowercased and has",
            "    already had the port, if any, stripped off.",
            "",
            "    Return ``True`` for a valid host, ``False`` otherwise.",
            "    \"\"\"",
            "    return any(",
            "        pattern == \"*\" or is_same_domain(host, pattern) for pattern in allowed_hosts",
            "    )",
            "",
            "",
            "def parse_accept_header(header):",
            "    return [MediaType(token) for token in header.split(\",\") if token.strip()]"
        ],
        "afterPatchFile": [
            "import codecs",
            "import copy",
            "from io import BytesIO",
            "from itertools import chain",
            "from urllib.parse import parse_qsl, quote, urlencode, urljoin, urlsplit",
            "",
            "from django.conf import settings",
            "from django.core import signing",
            "from django.core.exceptions import (",
            "    DisallowedHost,",
            "    ImproperlyConfigured,",
            "    RequestDataTooBig,",
            "    TooManyFieldsSent,",
            ")",
            "from django.core.files import uploadhandler",
            "from django.http.multipartparser import (",
            "    MultiPartParser,",
            "    MultiPartParserError,",
            "    TooManyFilesSent,",
            ")",
            "from django.utils.datastructures import (",
            "    CaseInsensitiveMapping,",
            "    ImmutableList,",
            "    MultiValueDict,",
            ")",
            "from django.utils.encoding import escape_uri_path, iri_to_uri",
            "from django.utils.functional import cached_property",
            "from django.utils.http import is_same_domain, parse_header_parameters",
            "from django.utils.regex_helper import _lazy_re_compile",
            "",
            "from .multipartparser import parse_header",
            "",
            "RAISE_ERROR = object()",
            "host_validation_re = _lazy_re_compile(",
            "    r\"^([a-z0-9.-]+|\\[[a-f0-9]*:[a-f0-9\\.:]+\\])(:[0-9]+)?$\"",
            ")",
            "",
            "",
            "class UnreadablePostError(OSError):",
            "    pass",
            "",
            "",
            "class RawPostDataException(Exception):",
            "    \"\"\"",
            "    You cannot access raw_post_data from a request that has",
            "    multipart/* POST data if it has been accessed via POST,",
            "    FILES, etc..",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class HttpRequest:",
            "    \"\"\"A basic HTTP request.\"\"\"",
            "",
            "    # The encoding used in GET/POST dicts. None means use default setting.",
            "    _encoding = None",
            "    _upload_handlers = []",
            "",
            "    def __init__(self):",
            "        # WARNING: The `WSGIRequest` subclass doesn't call `super`.",
            "        # Any variable assignment made here should also happen in",
            "        # `WSGIRequest.__init__()`.",
            "",
            "        self.GET = QueryDict(mutable=True)",
            "        self.POST = QueryDict(mutable=True)",
            "        self.COOKIES = {}",
            "        self.META = {}",
            "        self.FILES = MultiValueDict()",
            "",
            "        self.path = \"\"",
            "        self.path_info = \"\"",
            "        self.method = None",
            "        self.resolver_match = None",
            "        self.content_type = None",
            "        self.content_params = None",
            "",
            "    def __repr__(self):",
            "        if self.method is None or not self.get_full_path():",
            "            return \"<%s>\" % self.__class__.__name__",
            "        return \"<%s: %s %r>\" % (",
            "            self.__class__.__name__,",
            "            self.method,",
            "            self.get_full_path(),",
            "        )",
            "",
            "    @cached_property",
            "    def headers(self):",
            "        return HttpHeaders(self.META)",
            "",
            "    @cached_property",
            "    def accepted_types(self):",
            "        \"\"\"Return a list of MediaType instances.\"\"\"",
            "        return parse_accept_header(self.headers.get(\"Accept\", \"*/*\"))",
            "",
            "    def accepts(self, media_type):",
            "        return any(",
            "            accepted_type.match(media_type) for accepted_type in self.accepted_types",
            "        )",
            "",
            "    def _set_content_type_params(self, meta):",
            "        \"\"\"Set content_type, content_params, and encoding.\"\"\"",
            "        self.content_type, self.content_params = parse_header_parameters(",
            "            meta.get(\"CONTENT_TYPE\", \"\")",
            "        )",
            "        if \"charset\" in self.content_params:",
            "            try:",
            "                codecs.lookup(self.content_params[\"charset\"])",
            "            except LookupError:",
            "                pass",
            "            else:",
            "                self.encoding = self.content_params[\"charset\"]",
            "",
            "    def _get_raw_host(self):",
            "        \"\"\"",
            "        Return the HTTP host using the environment or request headers. Skip",
            "        allowed hosts protection, so may return an insecure host.",
            "        \"\"\"",
            "        # We try three options, in order of decreasing preference.",
            "        if settings.USE_X_FORWARDED_HOST and (\"HTTP_X_FORWARDED_HOST\" in self.META):",
            "            host = self.META[\"HTTP_X_FORWARDED_HOST\"]",
            "        elif \"HTTP_HOST\" in self.META:",
            "            host = self.META[\"HTTP_HOST\"]",
            "        else:",
            "            # Reconstruct the host using the algorithm from PEP 333.",
            "            host = self.META[\"SERVER_NAME\"]",
            "            server_port = self.get_port()",
            "            if server_port != (\"443\" if self.is_secure() else \"80\"):",
            "                host = \"%s:%s\" % (host, server_port)",
            "        return host",
            "",
            "    def get_host(self):",
            "        \"\"\"Return the HTTP host using the environment or request headers.\"\"\"",
            "        host = self._get_raw_host()",
            "",
            "        # Allow variants of localhost if ALLOWED_HOSTS is empty and DEBUG=True.",
            "        allowed_hosts = settings.ALLOWED_HOSTS",
            "        if settings.DEBUG and not allowed_hosts:",
            "            allowed_hosts = [\".localhost\", \"127.0.0.1\", \"[::1]\"]",
            "",
            "        domain, port = split_domain_port(host)",
            "        if domain and validate_host(domain, allowed_hosts):",
            "            return host",
            "        else:",
            "            msg = \"Invalid HTTP_HOST header: %r.\" % host",
            "            if domain:",
            "                msg += \" You may need to add %r to ALLOWED_HOSTS.\" % domain",
            "            else:",
            "                msg += (",
            "                    \" The domain name provided is not valid according to RFC 1034/1035.\"",
            "                )",
            "            raise DisallowedHost(msg)",
            "",
            "    def get_port(self):",
            "        \"\"\"Return the port number for the request as a string.\"\"\"",
            "        if settings.USE_X_FORWARDED_PORT and \"HTTP_X_FORWARDED_PORT\" in self.META:",
            "            port = self.META[\"HTTP_X_FORWARDED_PORT\"]",
            "        else:",
            "            port = self.META[\"SERVER_PORT\"]",
            "        return str(port)",
            "",
            "    def get_full_path(self, force_append_slash=False):",
            "        return self._get_full_path(self.path, force_append_slash)",
            "",
            "    def get_full_path_info(self, force_append_slash=False):",
            "        return self._get_full_path(self.path_info, force_append_slash)",
            "",
            "    def _get_full_path(self, path, force_append_slash):",
            "        # RFC 3986 requires query string arguments to be in the ASCII range.",
            "        # Rather than crash if this doesn't happen, we encode defensively.",
            "        return \"%s%s%s\" % (",
            "            escape_uri_path(path),",
            "            \"/\" if force_append_slash and not path.endswith(\"/\") else \"\",",
            "            (\"?\" + iri_to_uri(self.META.get(\"QUERY_STRING\", \"\")))",
            "            if self.META.get(\"QUERY_STRING\", \"\")",
            "            else \"\",",
            "        )",
            "",
            "    def get_signed_cookie(self, key, default=RAISE_ERROR, salt=\"\", max_age=None):",
            "        \"\"\"",
            "        Attempt to return a signed cookie. If the signature fails or the",
            "        cookie has expired, raise an exception, unless the `default` argument",
            "        is provided,  in which case return that value.",
            "        \"\"\"",
            "        try:",
            "            cookie_value = self.COOKIES[key]",
            "        except KeyError:",
            "            if default is not RAISE_ERROR:",
            "                return default",
            "            else:",
            "                raise",
            "        try:",
            "            value = signing.get_cookie_signer(salt=key + salt).unsign(",
            "                cookie_value, max_age=max_age",
            "            )",
            "        except signing.BadSignature:",
            "            if default is not RAISE_ERROR:",
            "                return default",
            "            else:",
            "                raise",
            "        return value",
            "",
            "    def build_absolute_uri(self, location=None):",
            "        \"\"\"",
            "        Build an absolute URI from the location and the variables available in",
            "        this request. If no ``location`` is specified, build the absolute URI",
            "        using request.get_full_path(). If the location is absolute, convert it",
            "        to an RFC 3987 compliant URI and return it. If location is relative or",
            "        is scheme-relative (i.e., ``//example.com/``), urljoin() it to a base",
            "        URL constructed from the request variables.",
            "        \"\"\"",
            "        if location is None:",
            "            # Make it an absolute url (but schemeless and domainless) for the",
            "            # edge case that the path starts with '//'.",
            "            location = \"//%s\" % self.get_full_path()",
            "        else:",
            "            # Coerce lazy locations.",
            "            location = str(location)",
            "        bits = urlsplit(location)",
            "        if not (bits.scheme and bits.netloc):",
            "            # Handle the simple, most common case. If the location is absolute",
            "            # and a scheme or host (netloc) isn't provided, skip an expensive",
            "            # urljoin() as long as no path segments are '.' or '..'.",
            "            if (",
            "                bits.path.startswith(\"/\")",
            "                and not bits.scheme",
            "                and not bits.netloc",
            "                and \"/./\" not in bits.path",
            "                and \"/../\" not in bits.path",
            "            ):",
            "                # If location starts with '//' but has no netloc, reuse the",
            "                # schema and netloc from the current request. Strip the double",
            "                # slashes and continue as if it wasn't specified.",
            "                if location.startswith(\"//\"):",
            "                    location = location[2:]",
            "                location = self._current_scheme_host + location",
            "            else:",
            "                # Join the constructed URL with the provided location, which",
            "                # allows the provided location to apply query strings to the",
            "                # base path.",
            "                location = urljoin(self._current_scheme_host + self.path, location)",
            "        return iri_to_uri(location)",
            "",
            "    @cached_property",
            "    def _current_scheme_host(self):",
            "        return \"{}://{}\".format(self.scheme, self.get_host())",
            "",
            "    def _get_scheme(self):",
            "        \"\"\"",
            "        Hook for subclasses like WSGIRequest to implement. Return 'http' by",
            "        default.",
            "        \"\"\"",
            "        return \"http\"",
            "",
            "    @property",
            "    def scheme(self):",
            "        if settings.SECURE_PROXY_SSL_HEADER:",
            "            try:",
            "                header, secure_value = settings.SECURE_PROXY_SSL_HEADER",
            "            except ValueError:",
            "                raise ImproperlyConfigured(",
            "                    \"The SECURE_PROXY_SSL_HEADER setting must be a tuple containing \"",
            "                    \"two values.\"",
            "                )",
            "            header_value = self.META.get(header)",
            "            if header_value is not None:",
            "                header_value, *_ = header_value.split(\",\", 1)",
            "                return \"https\" if header_value.strip() == secure_value else \"http\"",
            "        return self._get_scheme()",
            "",
            "    def is_secure(self):",
            "        return self.scheme == \"https\"",
            "",
            "    @property",
            "    def encoding(self):",
            "        return self._encoding",
            "",
            "    @encoding.setter",
            "    def encoding(self, val):",
            "        \"\"\"",
            "        Set the encoding used for GET/POST accesses. If the GET or POST",
            "        dictionary has already been created, remove and recreate it on the",
            "        next access (so that it is decoded correctly).",
            "        \"\"\"",
            "        self._encoding = val",
            "        if hasattr(self, \"GET\"):",
            "            del self.GET",
            "        if hasattr(self, \"_post\"):",
            "            del self._post",
            "",
            "    def _initialize_handlers(self):",
            "        self._upload_handlers = [",
            "            uploadhandler.load_handler(handler, self)",
            "            for handler in settings.FILE_UPLOAD_HANDLERS",
            "        ]",
            "",
            "    @property",
            "    def upload_handlers(self):",
            "        if not self._upload_handlers:",
            "            # If there are no upload handlers defined, initialize them from settings.",
            "            self._initialize_handlers()",
            "        return self._upload_handlers",
            "",
            "    @upload_handlers.setter",
            "    def upload_handlers(self, upload_handlers):",
            "        if hasattr(self, \"_files\"):",
            "            raise AttributeError(",
            "                \"You cannot set the upload handlers after the upload has been \"",
            "                \"processed.\"",
            "            )",
            "        self._upload_handlers = upload_handlers",
            "",
            "    def parse_file_upload(self, META, post_data):",
            "        \"\"\"Return a tuple of (POST QueryDict, FILES MultiValueDict).\"\"\"",
            "        self.upload_handlers = ImmutableList(",
            "            self.upload_handlers,",
            "            warning=(",
            "                \"You cannot alter upload handlers after the upload has been \"",
            "                \"processed.\"",
            "            ),",
            "        )",
            "        parser = MultiPartParser(META, post_data, self.upload_handlers, self.encoding)",
            "        return parser.parse()",
            "",
            "    @property",
            "    def body(self):",
            "        if not hasattr(self, \"_body\"):",
            "            if self._read_started:",
            "                raise RawPostDataException(",
            "                    \"You cannot access body after reading from request's data stream\"",
            "                )",
            "",
            "            # Limit the maximum request data size that will be handled in-memory.",
            "            if (",
            "                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None",
            "                and int(self.META.get(\"CONTENT_LENGTH\") or 0)",
            "                > settings.DATA_UPLOAD_MAX_MEMORY_SIZE",
            "            ):",
            "                raise RequestDataTooBig(",
            "                    \"Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"",
            "                )",
            "",
            "            try:",
            "                self._body = self.read()",
            "            except OSError as e:",
            "                raise UnreadablePostError(*e.args) from e",
            "            self._stream = BytesIO(self._body)",
            "        return self._body",
            "",
            "    def _mark_post_parse_error(self):",
            "        self._post = QueryDict()",
            "        self._files = MultiValueDict()",
            "",
            "    def _load_post_and_files(self):",
            "        \"\"\"Populate self._post and self._files if the content-type is a form type\"\"\"",
            "        if self.method != \"POST\":",
            "            self._post, self._files = (",
            "                QueryDict(encoding=self._encoding),",
            "                MultiValueDict(),",
            "            )",
            "            return",
            "        if self._read_started and not hasattr(self, \"_body\"):",
            "            self._mark_post_parse_error()",
            "            return",
            "",
            "        if self.content_type == \"multipart/form-data\":",
            "            if hasattr(self, \"_body\"):",
            "                # Use already read data",
            "                data = BytesIO(self._body)",
            "            else:",
            "                data = self",
            "            try:",
            "                self._post, self._files = self.parse_file_upload(self.META, data)",
            "            except (MultiPartParserError, TooManyFilesSent):",
            "                # An error occurred while parsing POST data. Since when",
            "                # formatting the error the request handler might access",
            "                # self.POST, set self._post and self._file to prevent",
            "                # attempts to parse POST data again.",
            "                self._mark_post_parse_error()",
            "                raise",
            "        elif self.content_type == \"application/x-www-form-urlencoded\":",
            "            self._post, self._files = (",
            "                QueryDict(self.body, encoding=self._encoding),",
            "                MultiValueDict(),",
            "            )",
            "        else:",
            "            self._post, self._files = (",
            "                QueryDict(encoding=self._encoding),",
            "                MultiValueDict(),",
            "            )",
            "",
            "    def close(self):",
            "        if hasattr(self, \"_files\"):",
            "            for f in chain.from_iterable(list_[1] for list_ in self._files.lists()):",
            "                f.close()",
            "",
            "    # File-like and iterator interface.",
            "    #",
            "    # Expects self._stream to be set to an appropriate source of bytes by",
            "    # a corresponding request subclass (e.g. WSGIRequest).",
            "    # Also when request data has already been read by request.POST or",
            "    # request.body, self._stream points to a BytesIO instance",
            "    # containing that data.",
            "",
            "    def read(self, *args, **kwargs):",
            "        self._read_started = True",
            "        try:",
            "            return self._stream.read(*args, **kwargs)",
            "        except OSError as e:",
            "            raise UnreadablePostError(*e.args) from e",
            "",
            "    def readline(self, *args, **kwargs):",
            "        self._read_started = True",
            "        try:",
            "            return self._stream.readline(*args, **kwargs)",
            "        except OSError as e:",
            "            raise UnreadablePostError(*e.args) from e",
            "",
            "    def __iter__(self):",
            "        return iter(self.readline, b\"\")",
            "",
            "    def readlines(self):",
            "        return list(self)",
            "",
            "",
            "class HttpHeaders(CaseInsensitiveMapping):",
            "    HTTP_PREFIX = \"HTTP_\"",
            "    # PEP 333 gives two headers which aren't prepended with HTTP_.",
            "    UNPREFIXED_HEADERS = {\"CONTENT_TYPE\", \"CONTENT_LENGTH\"}",
            "",
            "    def __init__(self, environ):",
            "        headers = {}",
            "        for header, value in environ.items():",
            "            name = self.parse_header_name(header)",
            "            if name:",
            "                headers[name] = value",
            "        super().__init__(headers)",
            "",
            "    def __getitem__(self, key):",
            "        \"\"\"Allow header lookup using underscores in place of hyphens.\"\"\"",
            "        return super().__getitem__(key.replace(\"_\", \"-\"))",
            "",
            "    @classmethod",
            "    def parse_header_name(cls, header):",
            "        if header.startswith(cls.HTTP_PREFIX):",
            "            header = header[len(cls.HTTP_PREFIX) :]",
            "        elif header not in cls.UNPREFIXED_HEADERS:",
            "            return None",
            "        return header.replace(\"_\", \"-\").title()",
            "",
            "",
            "class QueryDict(MultiValueDict):",
            "    \"\"\"",
            "    A specialized MultiValueDict which represents a query string.",
            "",
            "    A QueryDict can be used to represent GET or POST data. It subclasses",
            "    MultiValueDict since keys in such data can be repeated, for instance",
            "    in the data from a form with a <select multiple> field.",
            "",
            "    By default QueryDicts are immutable, though the copy() method",
            "    will always return a mutable copy.",
            "",
            "    Both keys and values set on this class are converted from the given encoding",
            "    (DEFAULT_CHARSET by default) to str.",
            "    \"\"\"",
            "",
            "    # These are both reset in __init__, but is specified here at the class",
            "    # level so that unpickling will have valid values",
            "    _mutable = True",
            "    _encoding = None",
            "",
            "    def __init__(self, query_string=None, mutable=False, encoding=None):",
            "        super().__init__()",
            "        self.encoding = encoding or settings.DEFAULT_CHARSET",
            "        query_string = query_string or \"\"",
            "        parse_qsl_kwargs = {",
            "            \"keep_blank_values\": True,",
            "            \"encoding\": self.encoding,",
            "            \"max_num_fields\": settings.DATA_UPLOAD_MAX_NUMBER_FIELDS,",
            "        }",
            "        if isinstance(query_string, bytes):",
            "            # query_string normally contains URL-encoded data, a subset of ASCII.",
            "            try:",
            "                query_string = query_string.decode(self.encoding)",
            "            except UnicodeDecodeError:",
            "                # ... but some user agents are misbehaving :-(",
            "                query_string = query_string.decode(\"iso-8859-1\")",
            "        try:",
            "            for key, value in parse_qsl(query_string, **parse_qsl_kwargs):",
            "                self.appendlist(key, value)",
            "        except ValueError as e:",
            "            # ValueError can also be raised if the strict_parsing argument to",
            "            # parse_qsl() is True. As that is not used by Django, assume that",
            "            # the exception was raised by exceeding the value of max_num_fields",
            "            # instead of fragile checks of exception message strings.",
            "            raise TooManyFieldsSent(",
            "                \"The number of GET/POST parameters exceeded \"",
            "                \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"",
            "            ) from e",
            "        self._mutable = mutable",
            "",
            "    @classmethod",
            "    def fromkeys(cls, iterable, value=\"\", mutable=False, encoding=None):",
            "        \"\"\"",
            "        Return a new QueryDict with keys (may be repeated) from an iterable and",
            "        values from value.",
            "        \"\"\"",
            "        q = cls(\"\", mutable=True, encoding=encoding)",
            "        for key in iterable:",
            "            q.appendlist(key, value)",
            "        if not mutable:",
            "            q._mutable = False",
            "        return q",
            "",
            "    @property",
            "    def encoding(self):",
            "        if self._encoding is None:",
            "            self._encoding = settings.DEFAULT_CHARSET",
            "        return self._encoding",
            "",
            "    @encoding.setter",
            "    def encoding(self, value):",
            "        self._encoding = value",
            "",
            "    def _assert_mutable(self):",
            "        if not self._mutable:",
            "            raise AttributeError(\"This QueryDict instance is immutable\")",
            "",
            "    def __setitem__(self, key, value):",
            "        self._assert_mutable()",
            "        key = bytes_to_text(key, self.encoding)",
            "        value = bytes_to_text(value, self.encoding)",
            "        super().__setitem__(key, value)",
            "",
            "    def __delitem__(self, key):",
            "        self._assert_mutable()",
            "        super().__delitem__(key)",
            "",
            "    def __copy__(self):",
            "        result = self.__class__(\"\", mutable=True, encoding=self.encoding)",
            "        for key, value in self.lists():",
            "            result.setlist(key, value)",
            "        return result",
            "",
            "    def __deepcopy__(self, memo):",
            "        result = self.__class__(\"\", mutable=True, encoding=self.encoding)",
            "        memo[id(self)] = result",
            "        for key, value in self.lists():",
            "            result.setlist(copy.deepcopy(key, memo), copy.deepcopy(value, memo))",
            "        return result",
            "",
            "    def setlist(self, key, list_):",
            "        self._assert_mutable()",
            "        key = bytes_to_text(key, self.encoding)",
            "        list_ = [bytes_to_text(elt, self.encoding) for elt in list_]",
            "        super().setlist(key, list_)",
            "",
            "    def setlistdefault(self, key, default_list=None):",
            "        self._assert_mutable()",
            "        return super().setlistdefault(key, default_list)",
            "",
            "    def appendlist(self, key, value):",
            "        self._assert_mutable()",
            "        key = bytes_to_text(key, self.encoding)",
            "        value = bytes_to_text(value, self.encoding)",
            "        super().appendlist(key, value)",
            "",
            "    def pop(self, key, *args):",
            "        self._assert_mutable()",
            "        return super().pop(key, *args)",
            "",
            "    def popitem(self):",
            "        self._assert_mutable()",
            "        return super().popitem()",
            "",
            "    def clear(self):",
            "        self._assert_mutable()",
            "        super().clear()",
            "",
            "    def setdefault(self, key, default=None):",
            "        self._assert_mutable()",
            "        key = bytes_to_text(key, self.encoding)",
            "        default = bytes_to_text(default, self.encoding)",
            "        return super().setdefault(key, default)",
            "",
            "    def copy(self):",
            "        \"\"\"Return a mutable copy of this object.\"\"\"",
            "        return self.__deepcopy__({})",
            "",
            "    def urlencode(self, safe=None):",
            "        \"\"\"",
            "        Return an encoded string of all query string arguments.",
            "",
            "        `safe` specifies characters which don't require quoting, for example::",
            "",
            "            >>> q = QueryDict(mutable=True)",
            "            >>> q['next'] = '/a&b/'",
            "            >>> q.urlencode()",
            "            'next=%2Fa%26b%2F'",
            "            >>> q.urlencode(safe='/')",
            "            'next=/a%26b/'",
            "        \"\"\"",
            "        output = []",
            "        if safe:",
            "            safe = safe.encode(self.encoding)",
            "",
            "            def encode(k, v):",
            "                return \"%s=%s\" % ((quote(k, safe), quote(v, safe)))",
            "",
            "        else:",
            "",
            "            def encode(k, v):",
            "                return urlencode({k: v})",
            "",
            "        for k, list_ in self.lists():",
            "            output.extend(",
            "                encode(k.encode(self.encoding), str(v).encode(self.encoding))",
            "                for v in list_",
            "            )",
            "        return \"&\".join(output)",
            "",
            "",
            "class MediaType:",
            "    def __init__(self, media_type_raw_line):",
            "        full_type, self.params = parse_header(",
            "            media_type_raw_line.encode(\"ascii\") if media_type_raw_line else b\"\"",
            "        )",
            "        self.main_type, _, self.sub_type = full_type.partition(\"/\")",
            "",
            "    def __str__(self):",
            "        params_str = \"\".join(",
            "            \"; %s=%s\" % (k, v.decode(\"ascii\")) for k, v in self.params.items()",
            "        )",
            "        return \"%s%s%s\" % (",
            "            self.main_type,",
            "            (\"/%s\" % self.sub_type) if self.sub_type else \"\",",
            "            params_str,",
            "        )",
            "",
            "    def __repr__(self):",
            "        return \"<%s: %s>\" % (self.__class__.__qualname__, self)",
            "",
            "    @property",
            "    def is_all_types(self):",
            "        return self.main_type == \"*\" and self.sub_type == \"*\"",
            "",
            "    def match(self, other):",
            "        if self.is_all_types:",
            "            return True",
            "        other = MediaType(other)",
            "        if self.main_type == other.main_type and self.sub_type in {\"*\", other.sub_type}:",
            "            return True",
            "        return False",
            "",
            "",
            "# It's neither necessary nor appropriate to use",
            "# django.utils.encoding.force_str() for parsing URLs and form inputs. Thus,",
            "# this slightly more restricted function, used by QueryDict.",
            "def bytes_to_text(s, encoding):",
            "    \"\"\"",
            "    Convert bytes objects to strings, using the given encoding. Illegally",
            "    encoded input characters are replaced with Unicode \"unknown\" codepoint",
            "    (\\ufffd).",
            "",
            "    Return any non-bytes objects without change.",
            "    \"\"\"",
            "    if isinstance(s, bytes):",
            "        return str(s, encoding, \"replace\")",
            "    else:",
            "        return s",
            "",
            "",
            "def split_domain_port(host):",
            "    \"\"\"",
            "    Return a (domain, port) tuple from a given host.",
            "",
            "    Returned domain is lowercased. If the host is invalid, the domain will be",
            "    empty.",
            "    \"\"\"",
            "    host = host.lower()",
            "",
            "    if not host_validation_re.match(host):",
            "        return \"\", \"\"",
            "",
            "    if host[-1] == \"]\":",
            "        # It's an IPv6 address without a port.",
            "        return host, \"\"",
            "    bits = host.rsplit(\":\", 1)",
            "    domain, port = bits if len(bits) == 2 else (bits[0], \"\")",
            "    # Remove a trailing dot (if present) from the domain.",
            "    domain = domain[:-1] if domain.endswith(\".\") else domain",
            "    return domain, port",
            "",
            "",
            "def validate_host(host, allowed_hosts):",
            "    \"\"\"",
            "    Validate the given host for this site.",
            "",
            "    Check that the host looks valid and matches a host or host pattern in the",
            "    given list of ``allowed_hosts``. Any pattern beginning with a period",
            "    matches a domain and all its subdomains (e.g. ``.example.com`` matches",
            "    ``example.com`` and any subdomain), ``*`` matches anything, and anything",
            "    else must match exactly.",
            "",
            "    Note: This function assumes that the given host is lowercased and has",
            "    already had the port, if any, stripped off.",
            "",
            "    Return ``True`` for a valid host, ``False`` otherwise.",
            "    \"\"\"",
            "    return any(",
            "        pattern == \"*\" or is_same_domain(host, pattern) for pattern in allowed_hosts",
            "    )",
            "",
            "",
            "def parse_accept_header(header):",
            "    return [MediaType(token) for token in header.split(\",\") if token.strip()]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "16": [],
            "370": [
                "HttpRequest",
                "_load_post_and_files"
            ]
        },
        "addLocation": []
    }
}