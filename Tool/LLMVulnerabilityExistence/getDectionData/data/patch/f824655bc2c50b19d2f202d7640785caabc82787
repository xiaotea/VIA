{
    "django/utils/http.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from django.utils.functional import keep_lazy_text"
            },
            "1": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from django.utils.six.moves.urllib.parse import ("
            },
            "2": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": "     quote, quote_plus, unquote, unquote_plus, urlencode as original_urlencode,"
            },
            "3": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    urlparse,"
            },
            "4": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " )"
            },
            "5": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+if six.PY2:"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+    from urlparse import ("
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+        ParseResult, SplitResult, _splitnetloc, _splitparams, scheme_chars,"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+        uses_params,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+    )"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+    _coerce_args = None"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+else:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+    from urllib.parse import ("
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+        ParseResult, SplitResult, _coerce_args, _splitnetloc, _splitparams,"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+        scheme_chars, uses_params,"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+    )"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " ETAG_MATCH = re.compile(r'(?:W/)?\"((?:\\\\.|[^\"])*)\"')"
            },
            "19": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " MONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()"
            },
            "21": {
                "beforePatchRowNumber": 298,
                "afterPatchRowNumber": 309,
                "PatchRowcode": "     return _is_safe_url(url, host) and _is_safe_url(url.replace('\\\\', '/'), host)"
            },
            "22": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": 310,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": 311,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 312,
                "PatchRowcode": "+# Copied from urllib.parse.urlparse() but uses fixed urlsplit() function."
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 313,
                "PatchRowcode": "+def _urlparse(url, scheme='', allow_fragments=True):"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 314,
                "PatchRowcode": "+    \"\"\"Parse a URL into 6 components:"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 315,
                "PatchRowcode": "+    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 316,
                "PatchRowcode": "+    Return a 6-tuple: (scheme, netloc, path, params, query, fragment)."
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 317,
                "PatchRowcode": "+    Note that we don't break the components up in smaller bits"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 318,
                "PatchRowcode": "+    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 319,
                "PatchRowcode": "+    if _coerce_args:"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 320,
                "PatchRowcode": "+        url, scheme, _coerce_result = _coerce_args(url, scheme)"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 321,
                "PatchRowcode": "+    splitresult = _urlsplit(url, scheme, allow_fragments)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 322,
                "PatchRowcode": "+    scheme, netloc, url, query, fragment = splitresult"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 323,
                "PatchRowcode": "+    if scheme in uses_params and ';' in url:"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 324,
                "PatchRowcode": "+        url, params = _splitparams(url)"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 325,
                "PatchRowcode": "+    else:"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 326,
                "PatchRowcode": "+        params = ''"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 327,
                "PatchRowcode": "+    result = ParseResult(scheme, netloc, url, params, query, fragment)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 328,
                "PatchRowcode": "+    return _coerce_result(result) if _coerce_args else result"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 329,
                "PatchRowcode": "+"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 330,
                "PatchRowcode": "+"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 331,
                "PatchRowcode": "+# Copied from urllib.parse.urlsplit() with"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+# https://github.com/python/cpython/pull/661 applied."
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 333,
                "PatchRowcode": "+def _urlsplit(url, scheme='', allow_fragments=True):"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 334,
                "PatchRowcode": "+    \"\"\"Parse a URL into 5 components:"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 335,
                "PatchRowcode": "+    <scheme>://<netloc>/<path>?<query>#<fragment>"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 336,
                "PatchRowcode": "+    Return a 5-tuple: (scheme, netloc, path, query, fragment)."
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 337,
                "PatchRowcode": "+    Note that we don't break the components up in smaller bits"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 338,
                "PatchRowcode": "+    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\""
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 339,
                "PatchRowcode": "+    if _coerce_args:"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 340,
                "PatchRowcode": "+        url, scheme, _coerce_result = _coerce_args(url, scheme)"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 341,
                "PatchRowcode": "+    allow_fragments = bool(allow_fragments)"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 342,
                "PatchRowcode": "+    netloc = query = fragment = ''"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+    i = url.find(':')"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 344,
                "PatchRowcode": "+    if i > 0:"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 345,
                "PatchRowcode": "+        for c in url[:i]:"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 346,
                "PatchRowcode": "+            if c not in scheme_chars:"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 347,
                "PatchRowcode": "+                break"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 348,
                "PatchRowcode": "+        else:"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 349,
                "PatchRowcode": "+            scheme, url = url[:i].lower(), url[i + 1:]"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 350,
                "PatchRowcode": "+"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 351,
                "PatchRowcode": "+    if url[:2] == '//':"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 352,
                "PatchRowcode": "+        netloc, url = _splitnetloc(url, 2)"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 353,
                "PatchRowcode": "+        if (('[' in netloc and ']' not in netloc) or"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 354,
                "PatchRowcode": "+                (']' in netloc and '[' not in netloc)):"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 355,
                "PatchRowcode": "+            raise ValueError(\"Invalid IPv6 URL\")"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 356,
                "PatchRowcode": "+    if allow_fragments and '#' in url:"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 357,
                "PatchRowcode": "+        url, fragment = url.split('#', 1)"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 358,
                "PatchRowcode": "+    if '?' in url:"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 359,
                "PatchRowcode": "+        url, query = url.split('?', 1)"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 360,
                "PatchRowcode": "+    v = SplitResult(scheme, netloc, url, query, fragment)"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 361,
                "PatchRowcode": "+    return _coerce_result(v) if _coerce_args else v"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 362,
                "PatchRowcode": "+"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 363,
                "PatchRowcode": "+"
            },
            "76": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": 364,
                "PatchRowcode": " def _is_safe_url(url, host):"
            },
            "77": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": 365,
                "PatchRowcode": "     # Chrome considers any URL with more than two slashes to be absolute, but"
            },
            "78": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": 366,
                "PatchRowcode": "     # urlparse is not so flexible. Treat any url with three slashes as unsafe."
            },
            "79": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 367,
                "PatchRowcode": "     if url.startswith('///'):"
            },
            "80": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": 368,
                "PatchRowcode": "         return False"
            },
            "81": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    url_info = urlparse(url)"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 369,
                "PatchRowcode": "+    url_info = _urlparse(url)"
            },
            "83": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": 370,
                "PatchRowcode": "     # Forbid URLs like http:///example.com - with a scheme, but without a hostname."
            },
            "84": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": 371,
                "PatchRowcode": "     # In that URL, example.com is not the hostname but, a path component. However,"
            },
            "85": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": 372,
                "PatchRowcode": "     # Chrome will still consider example.com to be the hostname, so we must not"
            }
        },
        "frontPatchFile": [
            "from __future__ import unicode_literals",
            "",
            "import base64",
            "import calendar",
            "import datetime",
            "import re",
            "import sys",
            "import unicodedata",
            "from binascii import Error as BinasciiError",
            "from email.utils import formatdate",
            "",
            "from django.core.exceptions import TooManyFieldsSent",
            "from django.utils import six",
            "from django.utils.datastructures import MultiValueDict",
            "from django.utils.encoding import force_bytes, force_str, force_text",
            "from django.utils.functional import keep_lazy_text",
            "from django.utils.six.moves.urllib.parse import (",
            "    quote, quote_plus, unquote, unquote_plus, urlencode as original_urlencode,",
            "    urlparse,",
            ")",
            "",
            "ETAG_MATCH = re.compile(r'(?:W/)?\"((?:\\\\.|[^\"])*)\"')",
            "",
            "MONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()",
            "__D = r'(?P<day>\\d{2})'",
            "__D2 = r'(?P<day>[ \\d]\\d)'",
            "__M = r'(?P<mon>\\w{3})'",
            "__Y = r'(?P<year>\\d{4})'",
            "__Y2 = r'(?P<year>\\d{2})'",
            "__T = r'(?P<hour>\\d{2}):(?P<min>\\d{2}):(?P<sec>\\d{2})'",
            "RFC1123_DATE = re.compile(r'^\\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))",
            "RFC850_DATE = re.compile(r'^\\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))",
            "ASCTIME_DATE = re.compile(r'^\\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))",
            "",
            "RFC3986_GENDELIMS = str(\":/?#[]@\")",
            "RFC3986_SUBDELIMS = str(\"!$&'()*+,;=\")",
            "",
            "FIELDS_MATCH = re.compile('[&;]')",
            "",
            "",
            "@keep_lazy_text",
            "def urlquote(url, safe='/'):",
            "    \"\"\"",
            "    A version of Python's urllib.quote() function that can operate on unicode",
            "    strings. The url is first UTF-8 encoded before quoting. The returned string",
            "    can safely be used as part of an argument to a subsequent iri_to_uri() call",
            "    without double-quoting occurring.",
            "    \"\"\"",
            "    return force_text(quote(force_str(url), force_str(safe)))",
            "",
            "",
            "@keep_lazy_text",
            "def urlquote_plus(url, safe=''):",
            "    \"\"\"",
            "    A version of Python's urllib.quote_plus() function that can operate on",
            "    unicode strings. The url is first UTF-8 encoded before quoting. The",
            "    returned string can safely be used as part of an argument to a subsequent",
            "    iri_to_uri() call without double-quoting occurring.",
            "    \"\"\"",
            "    return force_text(quote_plus(force_str(url), force_str(safe)))",
            "",
            "",
            "@keep_lazy_text",
            "def urlunquote(quoted_url):",
            "    \"\"\"",
            "    A wrapper for Python's urllib.unquote() function that can operate on",
            "    the result of django.utils.http.urlquote().",
            "    \"\"\"",
            "    return force_text(unquote(force_str(quoted_url)))",
            "",
            "",
            "@keep_lazy_text",
            "def urlunquote_plus(quoted_url):",
            "    \"\"\"",
            "    A wrapper for Python's urllib.unquote_plus() function that can operate on",
            "    the result of django.utils.http.urlquote_plus().",
            "    \"\"\"",
            "    return force_text(unquote_plus(force_str(quoted_url)))",
            "",
            "",
            "def urlencode(query, doseq=0):",
            "    \"\"\"",
            "    A version of Python's urllib.urlencode() function that can operate on",
            "    unicode strings. The parameters are first cast to UTF-8 encoded strings and",
            "    then encoded as per normal.",
            "    \"\"\"",
            "    if isinstance(query, MultiValueDict):",
            "        query = query.lists()",
            "    elif hasattr(query, 'items'):",
            "        query = query.items()",
            "    return original_urlencode(",
            "        [(force_str(k),",
            "         [force_str(i) for i in v] if isinstance(v, (list, tuple)) else force_str(v))",
            "            for k, v in query],",
            "        doseq)",
            "",
            "",
            "def cookie_date(epoch_seconds=None):",
            "    \"\"\"",
            "    Formats the time to ensure compatibility with Netscape's cookie standard.",
            "",
            "    Accepts a floating point number expressed in seconds since the epoch, in",
            "    UTC - such as that outputted by time.time(). If set to None, defaults to",
            "    the current time.",
            "",
            "    Outputs a string in the format 'Wdy, DD-Mon-YYYY HH:MM:SS GMT'.",
            "    \"\"\"",
            "    rfcdate = formatdate(epoch_seconds)",
            "    return '%s-%s-%s GMT' % (rfcdate[:7], rfcdate[8:11], rfcdate[12:25])",
            "",
            "",
            "def http_date(epoch_seconds=None):",
            "    \"\"\"",
            "    Formats the time to match the RFC1123 date format as specified by HTTP",
            "    RFC7231 section 7.1.1.1.",
            "",
            "    Accepts a floating point number expressed in seconds since the epoch, in",
            "    UTC - such as that outputted by time.time(). If set to None, defaults to",
            "    the current time.",
            "",
            "    Outputs a string in the format 'Wdy, DD Mon YYYY HH:MM:SS GMT'.",
            "    \"\"\"",
            "    return formatdate(epoch_seconds, usegmt=True)",
            "",
            "",
            "def parse_http_date(date):",
            "    \"\"\"",
            "    Parses a date format as specified by HTTP RFC7231 section 7.1.1.1.",
            "",
            "    The three formats allowed by the RFC are accepted, even if only the first",
            "    one is still in widespread use.",
            "",
            "    Returns an integer expressed in seconds since the epoch, in UTC.",
            "    \"\"\"",
            "    # emails.Util.parsedate does the job for RFC1123 dates; unfortunately",
            "    # RFC7231 makes it mandatory to support RFC850 dates too. So we roll",
            "    # our own RFC-compliant parsing.",
            "    for regex in RFC1123_DATE, RFC850_DATE, ASCTIME_DATE:",
            "        m = regex.match(date)",
            "        if m is not None:",
            "            break",
            "    else:",
            "        raise ValueError(\"%r is not in a valid HTTP date format\" % date)",
            "    try:",
            "        year = int(m.group('year'))",
            "        if year < 100:",
            "            if year < 70:",
            "                year += 2000",
            "            else:",
            "                year += 1900",
            "        month = MONTHS.index(m.group('mon').lower()) + 1",
            "        day = int(m.group('day'))",
            "        hour = int(m.group('hour'))",
            "        min = int(m.group('min'))",
            "        sec = int(m.group('sec'))",
            "        result = datetime.datetime(year, month, day, hour, min, sec)",
            "        return calendar.timegm(result.utctimetuple())",
            "    except Exception:",
            "        six.reraise(ValueError, ValueError(\"%r is not a valid date\" % date), sys.exc_info()[2])",
            "",
            "",
            "def parse_http_date_safe(date):",
            "    \"\"\"",
            "    Same as parse_http_date, but returns None if the input is invalid.",
            "    \"\"\"",
            "    try:",
            "        return parse_http_date(date)",
            "    except Exception:",
            "        pass",
            "",
            "",
            "# Base 36 functions: useful for generating compact URLs",
            "",
            "def base36_to_int(s):",
            "    \"\"\"",
            "    Converts a base 36 string to an ``int``. Raises ``ValueError` if the",
            "    input won't fit into an int.",
            "    \"\"\"",
            "    # To prevent overconsumption of server resources, reject any",
            "    # base36 string that is long than 13 base36 digits (13 digits",
            "    # is sufficient to base36-encode any 64-bit integer)",
            "    if len(s) > 13:",
            "        raise ValueError(\"Base36 input too large\")",
            "    value = int(s, 36)",
            "    # ... then do a final check that the value will fit into an int to avoid",
            "    # returning a long (#15067). The long type was removed in Python 3.",
            "    if six.PY2 and value > sys.maxint:",
            "        raise ValueError(\"Base36 input too large\")",
            "    return value",
            "",
            "",
            "def int_to_base36(i):",
            "    \"\"\"",
            "    Converts an integer to a base36 string",
            "    \"\"\"",
            "    char_set = '0123456789abcdefghijklmnopqrstuvwxyz'",
            "    if i < 0:",
            "        raise ValueError(\"Negative base36 conversion input.\")",
            "    if six.PY2:",
            "        if not isinstance(i, six.integer_types):",
            "            raise TypeError(\"Non-integer base36 conversion input.\")",
            "        if i > sys.maxint:",
            "            raise ValueError(\"Base36 conversion input too large.\")",
            "    if i < 36:",
            "        return char_set[i]",
            "    b36 = ''",
            "    while i != 0:",
            "        i, n = divmod(i, 36)",
            "        b36 = char_set[n] + b36",
            "    return b36",
            "",
            "",
            "def urlsafe_base64_encode(s):",
            "    \"\"\"",
            "    Encodes a bytestring in base64 for use in URLs, stripping any trailing",
            "    equal signs.",
            "    \"\"\"",
            "    return base64.urlsafe_b64encode(s).rstrip(b'\\n=')",
            "",
            "",
            "def urlsafe_base64_decode(s):",
            "    \"\"\"",
            "    Decodes a base64 encoded string, adding back any trailing equal signs that",
            "    might have been stripped.",
            "    \"\"\"",
            "    s = force_bytes(s)",
            "    try:",
            "        return base64.urlsafe_b64decode(s.ljust(len(s) + len(s) % 4, b'='))",
            "    except (LookupError, BinasciiError) as e:",
            "        raise ValueError(e)",
            "",
            "",
            "def parse_etags(etag_str):",
            "    \"\"\"",
            "    Parses a string with one or several etags passed in If-None-Match and",
            "    If-Match headers by the rules in RFC 2616. Returns a list of etags",
            "    without surrounding double quotes (\") and unescaped from \\<CHAR>.",
            "    \"\"\"",
            "    etags = ETAG_MATCH.findall(etag_str)",
            "    if not etags:",
            "        # etag_str has wrong format, treat it as an opaque string then",
            "        return [etag_str]",
            "    etags = [e.encode('ascii').decode('unicode_escape') for e in etags]",
            "    return etags",
            "",
            "",
            "def quote_etag(etag):",
            "    \"\"\"",
            "    Wraps a string in double quotes escaping contents as necessary.",
            "    \"\"\"",
            "    return '\"%s\"' % etag.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')",
            "",
            "",
            "def unquote_etag(etag):",
            "    \"\"\"",
            "    Unquote an ETag string; i.e. revert quote_etag().",
            "    \"\"\"",
            "    return etag.strip('\"').replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\') if etag else etag",
            "",
            "",
            "def is_same_domain(host, pattern):",
            "    \"\"\"",
            "    Return ``True`` if the host is either an exact match or a match",
            "    to the wildcard pattern.",
            "",
            "    Any pattern beginning with a period matches a domain and all of its",
            "    subdomains. (e.g. ``.example.com`` matches ``example.com`` and",
            "    ``foo.example.com``). Anything else is an exact string match.",
            "    \"\"\"",
            "    if not pattern:",
            "        return False",
            "",
            "    pattern = pattern.lower()",
            "    return (",
            "        pattern[0] == '.' and (host.endswith(pattern) or host == pattern[1:]) or",
            "        pattern == host",
            "    )",
            "",
            "",
            "def is_safe_url(url, host=None):",
            "    \"\"\"",
            "    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to",
            "    a different host and uses a safe scheme).",
            "",
            "    Always returns ``False`` on an empty url.",
            "    \"\"\"",
            "    if url is not None:",
            "        url = url.strip()",
            "    if not url:",
            "        return False",
            "    if six.PY2:",
            "        try:",
            "            url = force_text(url)",
            "        except UnicodeDecodeError:",
            "            return False",
            "    # Chrome treats \\ completely as / in paths but it could be part of some",
            "    # basic auth credentials so we need to check both URLs.",
            "    return _is_safe_url(url, host) and _is_safe_url(url.replace('\\\\', '/'), host)",
            "",
            "",
            "def _is_safe_url(url, host):",
            "    # Chrome considers any URL with more than two slashes to be absolute, but",
            "    # urlparse is not so flexible. Treat any url with three slashes as unsafe.",
            "    if url.startswith('///'):",
            "        return False",
            "    url_info = urlparse(url)",
            "    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.",
            "    # In that URL, example.com is not the hostname but, a path component. However,",
            "    # Chrome will still consider example.com to be the hostname, so we must not",
            "    # allow this syntax.",
            "    if not url_info.netloc and url_info.scheme:",
            "        return False",
            "    # Forbid URLs that start with control characters. Some browsers (like",
            "    # Chrome) ignore quite a few control characters at the start of a",
            "    # URL and might consider the URL as scheme relative.",
            "    if unicodedata.category(url[0])[0] == 'C':",
            "        return False",
            "    return ((not url_info.netloc or url_info.netloc == host) and",
            "            (not url_info.scheme or url_info.scheme in ['http', 'https']))",
            "",
            "",
            "def limited_parse_qsl(qs, keep_blank_values=False, encoding='utf-8',",
            "                      errors='replace', fields_limit=None):",
            "    \"\"\"",
            "    Return a list of key/value tuples parsed from query string.",
            "",
            "    Copied from urlparse with an additional \"fields_limit\" argument.",
            "    Copyright (C) 2013 Python Software Foundation (see LICENSE.python).",
            "",
            "    Arguments:",
            "",
            "    qs: percent-encoded query string to be parsed",
            "",
            "    keep_blank_values: flag indicating whether blank values in",
            "        percent-encoded queries should be treated as blank strings. A",
            "        true value indicates that blanks should be retained as blank",
            "        strings. The default false value indicates that blank values",
            "        are to be ignored and treated as if they were  not included.",
            "",
            "    encoding and errors: specify how to decode percent-encoded sequences",
            "        into Unicode characters, as accepted by the bytes.decode() method.",
            "",
            "    fields_limit: maximum number of fields parsed or an exception",
            "        is raised. None means no limit and is the default.",
            "    \"\"\"",
            "    if fields_limit:",
            "        pairs = FIELDS_MATCH.split(qs, fields_limit)",
            "        if len(pairs) > fields_limit:",
            "            raise TooManyFieldsSent(",
            "                'The number of GET/POST parameters exceeded '",
            "                'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'",
            "            )",
            "    else:",
            "        pairs = FIELDS_MATCH.split(qs)",
            "    r = []",
            "    for name_value in pairs:",
            "        if not name_value:",
            "            continue",
            "        nv = name_value.split(str('='), 1)",
            "        if len(nv) != 2:",
            "            # Handle case of a control-name with no equal sign",
            "            if keep_blank_values:",
            "                nv.append('')",
            "            else:",
            "                continue",
            "        if len(nv[1]) or keep_blank_values:",
            "            if six.PY3:",
            "                name = nv[0].replace('+', ' ')",
            "                name = unquote(name, encoding=encoding, errors=errors)",
            "                value = nv[1].replace('+', ' ')",
            "                value = unquote(value, encoding=encoding, errors=errors)",
            "            else:",
            "                name = unquote(nv[0].replace(b'+', b' '))",
            "                value = unquote(nv[1].replace(b'+', b' '))",
            "            r.append((name, value))",
            "    return r"
        ],
        "afterPatchFile": [
            "from __future__ import unicode_literals",
            "",
            "import base64",
            "import calendar",
            "import datetime",
            "import re",
            "import sys",
            "import unicodedata",
            "from binascii import Error as BinasciiError",
            "from email.utils import formatdate",
            "",
            "from django.core.exceptions import TooManyFieldsSent",
            "from django.utils import six",
            "from django.utils.datastructures import MultiValueDict",
            "from django.utils.encoding import force_bytes, force_str, force_text",
            "from django.utils.functional import keep_lazy_text",
            "from django.utils.six.moves.urllib.parse import (",
            "    quote, quote_plus, unquote, unquote_plus, urlencode as original_urlencode,",
            ")",
            "",
            "if six.PY2:",
            "    from urlparse import (",
            "        ParseResult, SplitResult, _splitnetloc, _splitparams, scheme_chars,",
            "        uses_params,",
            "    )",
            "    _coerce_args = None",
            "else:",
            "    from urllib.parse import (",
            "        ParseResult, SplitResult, _coerce_args, _splitnetloc, _splitparams,",
            "        scheme_chars, uses_params,",
            "    )",
            "",
            "ETAG_MATCH = re.compile(r'(?:W/)?\"((?:\\\\.|[^\"])*)\"')",
            "",
            "MONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()",
            "__D = r'(?P<day>\\d{2})'",
            "__D2 = r'(?P<day>[ \\d]\\d)'",
            "__M = r'(?P<mon>\\w{3})'",
            "__Y = r'(?P<year>\\d{4})'",
            "__Y2 = r'(?P<year>\\d{2})'",
            "__T = r'(?P<hour>\\d{2}):(?P<min>\\d{2}):(?P<sec>\\d{2})'",
            "RFC1123_DATE = re.compile(r'^\\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))",
            "RFC850_DATE = re.compile(r'^\\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))",
            "ASCTIME_DATE = re.compile(r'^\\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))",
            "",
            "RFC3986_GENDELIMS = str(\":/?#[]@\")",
            "RFC3986_SUBDELIMS = str(\"!$&'()*+,;=\")",
            "",
            "FIELDS_MATCH = re.compile('[&;]')",
            "",
            "",
            "@keep_lazy_text",
            "def urlquote(url, safe='/'):",
            "    \"\"\"",
            "    A version of Python's urllib.quote() function that can operate on unicode",
            "    strings. The url is first UTF-8 encoded before quoting. The returned string",
            "    can safely be used as part of an argument to a subsequent iri_to_uri() call",
            "    without double-quoting occurring.",
            "    \"\"\"",
            "    return force_text(quote(force_str(url), force_str(safe)))",
            "",
            "",
            "@keep_lazy_text",
            "def urlquote_plus(url, safe=''):",
            "    \"\"\"",
            "    A version of Python's urllib.quote_plus() function that can operate on",
            "    unicode strings. The url is first UTF-8 encoded before quoting. The",
            "    returned string can safely be used as part of an argument to a subsequent",
            "    iri_to_uri() call without double-quoting occurring.",
            "    \"\"\"",
            "    return force_text(quote_plus(force_str(url), force_str(safe)))",
            "",
            "",
            "@keep_lazy_text",
            "def urlunquote(quoted_url):",
            "    \"\"\"",
            "    A wrapper for Python's urllib.unquote() function that can operate on",
            "    the result of django.utils.http.urlquote().",
            "    \"\"\"",
            "    return force_text(unquote(force_str(quoted_url)))",
            "",
            "",
            "@keep_lazy_text",
            "def urlunquote_plus(quoted_url):",
            "    \"\"\"",
            "    A wrapper for Python's urllib.unquote_plus() function that can operate on",
            "    the result of django.utils.http.urlquote_plus().",
            "    \"\"\"",
            "    return force_text(unquote_plus(force_str(quoted_url)))",
            "",
            "",
            "def urlencode(query, doseq=0):",
            "    \"\"\"",
            "    A version of Python's urllib.urlencode() function that can operate on",
            "    unicode strings. The parameters are first cast to UTF-8 encoded strings and",
            "    then encoded as per normal.",
            "    \"\"\"",
            "    if isinstance(query, MultiValueDict):",
            "        query = query.lists()",
            "    elif hasattr(query, 'items'):",
            "        query = query.items()",
            "    return original_urlencode(",
            "        [(force_str(k),",
            "         [force_str(i) for i in v] if isinstance(v, (list, tuple)) else force_str(v))",
            "            for k, v in query],",
            "        doseq)",
            "",
            "",
            "def cookie_date(epoch_seconds=None):",
            "    \"\"\"",
            "    Formats the time to ensure compatibility with Netscape's cookie standard.",
            "",
            "    Accepts a floating point number expressed in seconds since the epoch, in",
            "    UTC - such as that outputted by time.time(). If set to None, defaults to",
            "    the current time.",
            "",
            "    Outputs a string in the format 'Wdy, DD-Mon-YYYY HH:MM:SS GMT'.",
            "    \"\"\"",
            "    rfcdate = formatdate(epoch_seconds)",
            "    return '%s-%s-%s GMT' % (rfcdate[:7], rfcdate[8:11], rfcdate[12:25])",
            "",
            "",
            "def http_date(epoch_seconds=None):",
            "    \"\"\"",
            "    Formats the time to match the RFC1123 date format as specified by HTTP",
            "    RFC7231 section 7.1.1.1.",
            "",
            "    Accepts a floating point number expressed in seconds since the epoch, in",
            "    UTC - such as that outputted by time.time(). If set to None, defaults to",
            "    the current time.",
            "",
            "    Outputs a string in the format 'Wdy, DD Mon YYYY HH:MM:SS GMT'.",
            "    \"\"\"",
            "    return formatdate(epoch_seconds, usegmt=True)",
            "",
            "",
            "def parse_http_date(date):",
            "    \"\"\"",
            "    Parses a date format as specified by HTTP RFC7231 section 7.1.1.1.",
            "",
            "    The three formats allowed by the RFC are accepted, even if only the first",
            "    one is still in widespread use.",
            "",
            "    Returns an integer expressed in seconds since the epoch, in UTC.",
            "    \"\"\"",
            "    # emails.Util.parsedate does the job for RFC1123 dates; unfortunately",
            "    # RFC7231 makes it mandatory to support RFC850 dates too. So we roll",
            "    # our own RFC-compliant parsing.",
            "    for regex in RFC1123_DATE, RFC850_DATE, ASCTIME_DATE:",
            "        m = regex.match(date)",
            "        if m is not None:",
            "            break",
            "    else:",
            "        raise ValueError(\"%r is not in a valid HTTP date format\" % date)",
            "    try:",
            "        year = int(m.group('year'))",
            "        if year < 100:",
            "            if year < 70:",
            "                year += 2000",
            "            else:",
            "                year += 1900",
            "        month = MONTHS.index(m.group('mon').lower()) + 1",
            "        day = int(m.group('day'))",
            "        hour = int(m.group('hour'))",
            "        min = int(m.group('min'))",
            "        sec = int(m.group('sec'))",
            "        result = datetime.datetime(year, month, day, hour, min, sec)",
            "        return calendar.timegm(result.utctimetuple())",
            "    except Exception:",
            "        six.reraise(ValueError, ValueError(\"%r is not a valid date\" % date), sys.exc_info()[2])",
            "",
            "",
            "def parse_http_date_safe(date):",
            "    \"\"\"",
            "    Same as parse_http_date, but returns None if the input is invalid.",
            "    \"\"\"",
            "    try:",
            "        return parse_http_date(date)",
            "    except Exception:",
            "        pass",
            "",
            "",
            "# Base 36 functions: useful for generating compact URLs",
            "",
            "def base36_to_int(s):",
            "    \"\"\"",
            "    Converts a base 36 string to an ``int``. Raises ``ValueError` if the",
            "    input won't fit into an int.",
            "    \"\"\"",
            "    # To prevent overconsumption of server resources, reject any",
            "    # base36 string that is long than 13 base36 digits (13 digits",
            "    # is sufficient to base36-encode any 64-bit integer)",
            "    if len(s) > 13:",
            "        raise ValueError(\"Base36 input too large\")",
            "    value = int(s, 36)",
            "    # ... then do a final check that the value will fit into an int to avoid",
            "    # returning a long (#15067). The long type was removed in Python 3.",
            "    if six.PY2 and value > sys.maxint:",
            "        raise ValueError(\"Base36 input too large\")",
            "    return value",
            "",
            "",
            "def int_to_base36(i):",
            "    \"\"\"",
            "    Converts an integer to a base36 string",
            "    \"\"\"",
            "    char_set = '0123456789abcdefghijklmnopqrstuvwxyz'",
            "    if i < 0:",
            "        raise ValueError(\"Negative base36 conversion input.\")",
            "    if six.PY2:",
            "        if not isinstance(i, six.integer_types):",
            "            raise TypeError(\"Non-integer base36 conversion input.\")",
            "        if i > sys.maxint:",
            "            raise ValueError(\"Base36 conversion input too large.\")",
            "    if i < 36:",
            "        return char_set[i]",
            "    b36 = ''",
            "    while i != 0:",
            "        i, n = divmod(i, 36)",
            "        b36 = char_set[n] + b36",
            "    return b36",
            "",
            "",
            "def urlsafe_base64_encode(s):",
            "    \"\"\"",
            "    Encodes a bytestring in base64 for use in URLs, stripping any trailing",
            "    equal signs.",
            "    \"\"\"",
            "    return base64.urlsafe_b64encode(s).rstrip(b'\\n=')",
            "",
            "",
            "def urlsafe_base64_decode(s):",
            "    \"\"\"",
            "    Decodes a base64 encoded string, adding back any trailing equal signs that",
            "    might have been stripped.",
            "    \"\"\"",
            "    s = force_bytes(s)",
            "    try:",
            "        return base64.urlsafe_b64decode(s.ljust(len(s) + len(s) % 4, b'='))",
            "    except (LookupError, BinasciiError) as e:",
            "        raise ValueError(e)",
            "",
            "",
            "def parse_etags(etag_str):",
            "    \"\"\"",
            "    Parses a string with one or several etags passed in If-None-Match and",
            "    If-Match headers by the rules in RFC 2616. Returns a list of etags",
            "    without surrounding double quotes (\") and unescaped from \\<CHAR>.",
            "    \"\"\"",
            "    etags = ETAG_MATCH.findall(etag_str)",
            "    if not etags:",
            "        # etag_str has wrong format, treat it as an opaque string then",
            "        return [etag_str]",
            "    etags = [e.encode('ascii').decode('unicode_escape') for e in etags]",
            "    return etags",
            "",
            "",
            "def quote_etag(etag):",
            "    \"\"\"",
            "    Wraps a string in double quotes escaping contents as necessary.",
            "    \"\"\"",
            "    return '\"%s\"' % etag.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')",
            "",
            "",
            "def unquote_etag(etag):",
            "    \"\"\"",
            "    Unquote an ETag string; i.e. revert quote_etag().",
            "    \"\"\"",
            "    return etag.strip('\"').replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\') if etag else etag",
            "",
            "",
            "def is_same_domain(host, pattern):",
            "    \"\"\"",
            "    Return ``True`` if the host is either an exact match or a match",
            "    to the wildcard pattern.",
            "",
            "    Any pattern beginning with a period matches a domain and all of its",
            "    subdomains. (e.g. ``.example.com`` matches ``example.com`` and",
            "    ``foo.example.com``). Anything else is an exact string match.",
            "    \"\"\"",
            "    if not pattern:",
            "        return False",
            "",
            "    pattern = pattern.lower()",
            "    return (",
            "        pattern[0] == '.' and (host.endswith(pattern) or host == pattern[1:]) or",
            "        pattern == host",
            "    )",
            "",
            "",
            "def is_safe_url(url, host=None):",
            "    \"\"\"",
            "    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to",
            "    a different host and uses a safe scheme).",
            "",
            "    Always returns ``False`` on an empty url.",
            "    \"\"\"",
            "    if url is not None:",
            "        url = url.strip()",
            "    if not url:",
            "        return False",
            "    if six.PY2:",
            "        try:",
            "            url = force_text(url)",
            "        except UnicodeDecodeError:",
            "            return False",
            "    # Chrome treats \\ completely as / in paths but it could be part of some",
            "    # basic auth credentials so we need to check both URLs.",
            "    return _is_safe_url(url, host) and _is_safe_url(url.replace('\\\\', '/'), host)",
            "",
            "",
            "# Copied from urllib.parse.urlparse() but uses fixed urlsplit() function.",
            "def _urlparse(url, scheme='', allow_fragments=True):",
            "    \"\"\"Parse a URL into 6 components:",
            "    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>",
            "    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).",
            "    Note that we don't break the components up in smaller bits",
            "    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"",
            "    if _coerce_args:",
            "        url, scheme, _coerce_result = _coerce_args(url, scheme)",
            "    splitresult = _urlsplit(url, scheme, allow_fragments)",
            "    scheme, netloc, url, query, fragment = splitresult",
            "    if scheme in uses_params and ';' in url:",
            "        url, params = _splitparams(url)",
            "    else:",
            "        params = ''",
            "    result = ParseResult(scheme, netloc, url, params, query, fragment)",
            "    return _coerce_result(result) if _coerce_args else result",
            "",
            "",
            "# Copied from urllib.parse.urlsplit() with",
            "# https://github.com/python/cpython/pull/661 applied.",
            "def _urlsplit(url, scheme='', allow_fragments=True):",
            "    \"\"\"Parse a URL into 5 components:",
            "    <scheme>://<netloc>/<path>?<query>#<fragment>",
            "    Return a 5-tuple: (scheme, netloc, path, query, fragment).",
            "    Note that we don't break the components up in smaller bits",
            "    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"",
            "    if _coerce_args:",
            "        url, scheme, _coerce_result = _coerce_args(url, scheme)",
            "    allow_fragments = bool(allow_fragments)",
            "    netloc = query = fragment = ''",
            "    i = url.find(':')",
            "    if i > 0:",
            "        for c in url[:i]:",
            "            if c not in scheme_chars:",
            "                break",
            "        else:",
            "            scheme, url = url[:i].lower(), url[i + 1:]",
            "",
            "    if url[:2] == '//':",
            "        netloc, url = _splitnetloc(url, 2)",
            "        if (('[' in netloc and ']' not in netloc) or",
            "                (']' in netloc and '[' not in netloc)):",
            "            raise ValueError(\"Invalid IPv6 URL\")",
            "    if allow_fragments and '#' in url:",
            "        url, fragment = url.split('#', 1)",
            "    if '?' in url:",
            "        url, query = url.split('?', 1)",
            "    v = SplitResult(scheme, netloc, url, query, fragment)",
            "    return _coerce_result(v) if _coerce_args else v",
            "",
            "",
            "def _is_safe_url(url, host):",
            "    # Chrome considers any URL with more than two slashes to be absolute, but",
            "    # urlparse is not so flexible. Treat any url with three slashes as unsafe.",
            "    if url.startswith('///'):",
            "        return False",
            "    url_info = _urlparse(url)",
            "    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.",
            "    # In that URL, example.com is not the hostname but, a path component. However,",
            "    # Chrome will still consider example.com to be the hostname, so we must not",
            "    # allow this syntax.",
            "    if not url_info.netloc and url_info.scheme:",
            "        return False",
            "    # Forbid URLs that start with control characters. Some browsers (like",
            "    # Chrome) ignore quite a few control characters at the start of a",
            "    # URL and might consider the URL as scheme relative.",
            "    if unicodedata.category(url[0])[0] == 'C':",
            "        return False",
            "    return ((not url_info.netloc or url_info.netloc == host) and",
            "            (not url_info.scheme or url_info.scheme in ['http', 'https']))",
            "",
            "",
            "def limited_parse_qsl(qs, keep_blank_values=False, encoding='utf-8',",
            "                      errors='replace', fields_limit=None):",
            "    \"\"\"",
            "    Return a list of key/value tuples parsed from query string.",
            "",
            "    Copied from urlparse with an additional \"fields_limit\" argument.",
            "    Copyright (C) 2013 Python Software Foundation (see LICENSE.python).",
            "",
            "    Arguments:",
            "",
            "    qs: percent-encoded query string to be parsed",
            "",
            "    keep_blank_values: flag indicating whether blank values in",
            "        percent-encoded queries should be treated as blank strings. A",
            "        true value indicates that blanks should be retained as blank",
            "        strings. The default false value indicates that blank values",
            "        are to be ignored and treated as if they were  not included.",
            "",
            "    encoding and errors: specify how to decode percent-encoded sequences",
            "        into Unicode characters, as accepted by the bytes.decode() method.",
            "",
            "    fields_limit: maximum number of fields parsed or an exception",
            "        is raised. None means no limit and is the default.",
            "    \"\"\"",
            "    if fields_limit:",
            "        pairs = FIELDS_MATCH.split(qs, fields_limit)",
            "        if len(pairs) > fields_limit:",
            "            raise TooManyFieldsSent(",
            "                'The number of GET/POST parameters exceeded '",
            "                'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'",
            "            )",
            "    else:",
            "        pairs = FIELDS_MATCH.split(qs)",
            "    r = []",
            "    for name_value in pairs:",
            "        if not name_value:",
            "            continue",
            "        nv = name_value.split(str('='), 1)",
            "        if len(nv) != 2:",
            "            # Handle case of a control-name with no equal sign",
            "            if keep_blank_values:",
            "                nv.append('')",
            "            else:",
            "                continue",
            "        if len(nv[1]) or keep_blank_values:",
            "            if six.PY3:",
            "                name = nv[0].replace('+', ' ')",
            "                name = unquote(name, encoding=encoding, errors=errors)",
            "                value = nv[1].replace('+', ' ')",
            "                value = unquote(value, encoding=encoding, errors=errors)",
            "            else:",
            "                name = unquote(nv[0].replace(b'+', b' '))",
            "                value = unquote(nv[1].replace(b'+', b' '))",
            "            r.append((name, value))",
            "    return r"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "19": [],
            "306": [
                "_is_safe_url"
            ]
        },
        "addLocation": []
    }
}