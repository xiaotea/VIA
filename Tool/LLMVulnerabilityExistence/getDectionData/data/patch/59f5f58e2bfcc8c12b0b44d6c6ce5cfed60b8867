{
    "airflow/providers/apache/sqoop/hooks/sqoop.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "     :param num_mappers: Number of map tasks to import in parallel."
            },
            "1": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "     :param properties: Properties to set via the -D argument"
            },
            "2": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "     :param libjars: Optional Comma separated jar files to include in the classpath."
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+    :param extra_options:  Extra import/export options to pass as dict."
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+        If a key doesn't have a value, just pass an empty string to it."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+        Don't include prefix of -- for sqoop options."
            },
            "6": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "     \"\"\""
            },
            "7": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "     conn_name_attr = \"conn_id\""
            },
            "9": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "         hcatalog_table: str | None = None,"
            },
            "10": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "         properties: dict[str, Any] | None = None,"
            },
            "11": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "         libjars: str | None = None,"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+        extra_options: dict[str, Any] | None = None,"
            },
            "13": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "     ) -> None:"
            },
            "14": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "         # No mutable types in the default parameters"
            },
            "15": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "         super().__init__()"
            },
            "16": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "         self.num_mappers = num_mappers"
            },
            "17": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "         self.properties = properties or {}"
            },
            "18": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "         self.sub_process_pid: int"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+        self._extra_options = extra_options"
            },
            "20": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 87,
                "PatchRowcode": "         self.log.info(\"Using connection to: %s:%s/%s\", self.conn.host, self.conn.port, self.conn.schema)"
            },
            "21": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 88,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "     def get_conn(self) -> Any:"
            },
            "23": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "                 raise AirflowException(f\"Sqoop command failed: {masked_cmd}\")"
            },
            "24": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 119,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "     def _prepare_command(self, export: bool = False) -> list[str]:"
            },
            "26": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if \"?\" in self.conn.host:"
            },
            "27": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise ValueError(\"The sqoop connection host should not contain a '?' character\")"
            },
            "28": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "29": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "         sqoop_cmd_type = \"export\" if export else \"import\""
            },
            "30": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 122,
                "PatchRowcode": "         connection_cmd = [\"sqoop\", sqoop_cmd_type]"
            },
            "31": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 123,
                "PatchRowcode": " "
            },
            "32": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 158,
                "PatchRowcode": "                 connect_str += f\"/{self.conn.schema}\""
            },
            "33": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 159,
                "PatchRowcode": "             else:"
            },
            "34": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 160,
                "PatchRowcode": "                 connect_str += f\";databaseName={self.conn.schema}\""
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 161,
                "PatchRowcode": "+        if \"?\" in connect_str:"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 162,
                "PatchRowcode": "+            raise ValueError(\"The sqoop connection string should not contain a '?' character\")"
            },
            "37": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 163,
                "PatchRowcode": "         connection_cmd += [\"--connect\", connect_str]"
            },
            "38": {
                "beforePatchRowNumber": 160,
                "afterPatchRowNumber": 164,
                "PatchRowcode": " "
            },
            "39": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": 165,
                "PatchRowcode": "         return connection_cmd"
            },
            "40": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "         split_by: str | None,"
            },
            "41": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "         direct: bool | None,"
            },
            "42": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "         driver: Any,"
            },
            "43": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        extra_import_options: Any,"
            },
            "44": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "     ) -> list[str]:"
            },
            "45": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 189,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "         cmd = self._prepare_command(export=False)"
            },
            "47": {
                "beforePatchRowNumber": 203,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "         if driver:"
            },
            "48": {
                "beforePatchRowNumber": 204,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "             cmd += [\"--driver\", driver]"
            },
            "49": {
                "beforePatchRowNumber": 205,
                "afterPatchRowNumber": 208,
                "PatchRowcode": " "
            },
            "50": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if extra_import_options:"
            },
            "51": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            for key, value in extra_import_options.items():"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+        if self._extra_options:"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+            for key, value in self._extra_options.items():"
            },
            "54": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 211,
                "PatchRowcode": "                 cmd += [f\"--{key}\"]"
            },
            "55": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 212,
                "PatchRowcode": "                 if value:"
            },
            "56": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "                     cmd += [str(value)]"
            },
            "57": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 225,
                "PatchRowcode": "         where: str | None = None,"
            },
            "58": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 226,
                "PatchRowcode": "         direct: bool = False,"
            },
            "59": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 227,
                "PatchRowcode": "         driver: Any = None,"
            },
            "60": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        extra_import_options: dict[str, Any] | None = None,"
            },
            "61": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 228,
                "PatchRowcode": "         schema: str | None = None,"
            },
            "62": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "     ) -> Any:"
            },
            "63": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "         \"\"\"Import table from remote location to target dir."
            },
            "64": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "         :param where: WHERE clause to use during import"
            },
            "65": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "         :param direct: Use direct connector if exists for the database"
            },
            "66": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "         :param driver: Manually specify JDBC driver class to use"
            },
            "67": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        :param extra_import_options: Extra import options to pass as dict."
            },
            "68": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            If a key doesn't have a value, just pass an empty string to it."
            },
            "69": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            Don't include prefix of -- for sqoop options."
            },
            "70": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 245,
                "PatchRowcode": "         \"\"\""
            },
            "71": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 246,
                "PatchRowcode": "+        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver)"
            },
            "73": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 247,
                "PatchRowcode": " "
            },
            "74": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 248,
                "PatchRowcode": "         cmd += [\"--table\", table]"
            },
            "75": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": 249,
                "PatchRowcode": " "
            },
            "76": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 265,
                "PatchRowcode": "         split_by: str | None = None,"
            },
            "77": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 266,
                "PatchRowcode": "         direct: bool | None = None,"
            },
            "78": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 267,
                "PatchRowcode": "         driver: Any | None = None,"
            },
            "79": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        extra_import_options: dict[str, Any] | None = None,"
            },
            "80": {
                "beforePatchRowNumber": 270,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "     ) -> Any:"
            },
            "81": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 269,
                "PatchRowcode": "         \"\"\"Import a specific query from the rdbms to hdfs."
            },
            "82": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 270,
                "PatchRowcode": " "
            },
            "83": {
                "beforePatchRowNumber": 278,
                "afterPatchRowNumber": 276,
                "PatchRowcode": "         :param split_by: Column of the table used to split work units"
            },
            "84": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": 277,
                "PatchRowcode": "         :param direct: Use direct import fast path"
            },
            "85": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": 278,
                "PatchRowcode": "         :param driver: Manually specify JDBC driver class to use"
            },
            "86": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        :param extra_import_options: Extra import options to pass as dict."
            },
            "87": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            If a key doesn't have a value, just pass an empty string to it."
            },
            "88": {
                "beforePatchRowNumber": 283,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            Don't include prefix of -- for sqoop options."
            },
            "89": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 279,
                "PatchRowcode": "         \"\"\""
            },
            "90": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 280,
                "PatchRowcode": "+        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver)"
            },
            "92": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 281,
                "PatchRowcode": "         cmd += [\"--query\", query]"
            },
            "93": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": 282,
                "PatchRowcode": " "
            },
            "94": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": 283,
                "PatchRowcode": "         self.popen(cmd)"
            },
            "95": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": 297,
                "PatchRowcode": "         input_optionally_enclosed_by: str | None = None,"
            },
            "96": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": 298,
                "PatchRowcode": "         batch: bool = False,"
            },
            "97": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 299,
                "PatchRowcode": "         relaxed_isolation: bool = False,"
            },
            "98": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        extra_export_options: dict[str, Any] | None = None,"
            },
            "99": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": 300,
                "PatchRowcode": "         schema: str | None = None,"
            },
            "100": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "     ) -> list[str]:"
            },
            "101": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": 302,
                "PatchRowcode": " "
            },
            "102": {
                "beforePatchRowNumber": 344,
                "afterPatchRowNumber": 338,
                "PatchRowcode": "         if export_dir:"
            },
            "103": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "             cmd += [\"--export-dir\", export_dir]"
            },
            "104": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": 340,
                "PatchRowcode": " "
            },
            "105": {
                "beforePatchRowNumber": 347,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if extra_export_options:"
            },
            "106": {
                "beforePatchRowNumber": 348,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            for key, value in extra_export_options.items():"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 341,
                "PatchRowcode": "+        if self._extra_options:"
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 342,
                "PatchRowcode": "+            for key, value in self._extra_options.items():"
            },
            "109": {
                "beforePatchRowNumber": 349,
                "afterPatchRowNumber": 343,
                "PatchRowcode": "                 cmd += [f\"--{key}\"]"
            },
            "110": {
                "beforePatchRowNumber": 350,
                "afterPatchRowNumber": 344,
                "PatchRowcode": "                 if value:"
            },
            "111": {
                "beforePatchRowNumber": 351,
                "afterPatchRowNumber": 345,
                "PatchRowcode": "                     cmd += [str(value)]"
            },
            "112": {
                "beforePatchRowNumber": 373,
                "afterPatchRowNumber": 367,
                "PatchRowcode": "         input_optionally_enclosed_by: str | None = None,"
            },
            "113": {
                "beforePatchRowNumber": 374,
                "afterPatchRowNumber": 368,
                "PatchRowcode": "         batch: bool = False,"
            },
            "114": {
                "beforePatchRowNumber": 375,
                "afterPatchRowNumber": 369,
                "PatchRowcode": "         relaxed_isolation: bool = False,"
            },
            "115": {
                "beforePatchRowNumber": 376,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        extra_export_options: dict[str, Any] | None = None,"
            },
            "116": {
                "beforePatchRowNumber": 377,
                "afterPatchRowNumber": 370,
                "PatchRowcode": "         schema: str | None = None,"
            },
            "117": {
                "beforePatchRowNumber": 378,
                "afterPatchRowNumber": 371,
                "PatchRowcode": "     ) -> None:"
            },
            "118": {
                "beforePatchRowNumber": 379,
                "afterPatchRowNumber": 372,
                "PatchRowcode": "         \"\"\"Export Hive table to remote location."
            },
            "119": {
                "beforePatchRowNumber": 399,
                "afterPatchRowNumber": 392,
                "PatchRowcode": "         :param batch: Use batch mode for underlying statement execution"
            },
            "120": {
                "beforePatchRowNumber": 400,
                "afterPatchRowNumber": 393,
                "PatchRowcode": "         :param relaxed_isolation: Transaction isolation to read uncommitted"
            },
            "121": {
                "beforePatchRowNumber": 401,
                "afterPatchRowNumber": 394,
                "PatchRowcode": "             for the mappers"
            },
            "122": {
                "beforePatchRowNumber": 402,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        :param extra_export_options: Extra export options to pass as dict."
            },
            "123": {
                "beforePatchRowNumber": 403,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            If a key doesn't have a value, just pass an empty string to it."
            },
            "124": {
                "beforePatchRowNumber": 404,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            Don't include prefix of -- for sqoop options."
            },
            "125": {
                "beforePatchRowNumber": 405,
                "afterPatchRowNumber": 395,
                "PatchRowcode": "         \"\"\""
            },
            "126": {
                "beforePatchRowNumber": 406,
                "afterPatchRowNumber": 396,
                "PatchRowcode": "         cmd = self._export_cmd("
            },
            "127": {
                "beforePatchRowNumber": 407,
                "afterPatchRowNumber": 397,
                "PatchRowcode": "             table,"
            },
            "128": {
                "beforePatchRowNumber": 417,
                "afterPatchRowNumber": 407,
                "PatchRowcode": "             input_optionally_enclosed_by,"
            },
            "129": {
                "beforePatchRowNumber": 418,
                "afterPatchRowNumber": 408,
                "PatchRowcode": "             batch,"
            },
            "130": {
                "beforePatchRowNumber": 419,
                "afterPatchRowNumber": 409,
                "PatchRowcode": "             relaxed_isolation,"
            },
            "131": {
                "beforePatchRowNumber": 420,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            extra_export_options,"
            },
            "132": {
                "beforePatchRowNumber": 421,
                "afterPatchRowNumber": 410,
                "PatchRowcode": "             schema,"
            },
            "133": {
                "beforePatchRowNumber": 422,
                "afterPatchRowNumber": 411,
                "PatchRowcode": "         )"
            },
            "134": {
                "beforePatchRowNumber": 423,
                "afterPatchRowNumber": 412,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"This module contains a sqoop 1.x hook.\"\"\"",
            "from __future__ import annotations",
            "",
            "import subprocess",
            "from copy import deepcopy",
            "from typing import Any",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.hooks.base import BaseHook",
            "",
            "",
            "class SqoopHook(BaseHook):",
            "    \"\"\"Wrapper around the sqoop 1 binary.",
            "",
            "    To be able to use the hook, it is required that \"sqoop\" is in the PATH.",
            "",
            "    Additional arguments that can be passed via the 'extra' JSON field of the",
            "    sqoop connection:",
            "",
            "        * ``job_tracker``: Job tracker local|jobtracker:port.",
            "        * ``namenode``: Namenode.",
            "        * ``files``: Comma separated files to be copied to the map reduce cluster.",
            "        * ``archives``: Comma separated archives to be unarchived on the compute",
            "            machines.",
            "        * ``password_file``: Path to file containing the password.",
            "",
            "    :param conn_id: Reference to the sqoop connection.",
            "    :param verbose: Set sqoop to verbose.",
            "    :param num_mappers: Number of map tasks to import in parallel.",
            "    :param properties: Properties to set via the -D argument",
            "    :param libjars: Optional Comma separated jar files to include in the classpath.",
            "    \"\"\"",
            "",
            "    conn_name_attr = \"conn_id\"",
            "    default_conn_name = \"sqoop_default\"",
            "    conn_type = \"sqoop\"",
            "    hook_name = \"Sqoop\"",
            "",
            "    def __init__(",
            "        self,",
            "        conn_id: str = default_conn_name,",
            "        verbose: bool = False,",
            "        num_mappers: int | None = None,",
            "        hcatalog_database: str | None = None,",
            "        hcatalog_table: str | None = None,",
            "        properties: dict[str, Any] | None = None,",
            "        libjars: str | None = None,",
            "    ) -> None:",
            "        # No mutable types in the default parameters",
            "        super().__init__()",
            "        self.conn = self.get_connection(conn_id)",
            "        connection_parameters = self.conn.extra_dejson",
            "        self.job_tracker = connection_parameters.get(\"job_tracker\", None)",
            "        self.namenode = connection_parameters.get(\"namenode\", None)",
            "        self.libjars = libjars",
            "        self.files = connection_parameters.get(\"files\", None)",
            "        self.archives = connection_parameters.get(\"archives\", None)",
            "        self.password_file = connection_parameters.get(\"password_file\", None)",
            "        self.hcatalog_database = hcatalog_database",
            "        self.hcatalog_table = hcatalog_table",
            "        self.verbose = verbose",
            "        self.num_mappers = num_mappers",
            "        self.properties = properties or {}",
            "        self.sub_process_pid: int",
            "        self.log.info(\"Using connection to: %s:%s/%s\", self.conn.host, self.conn.port, self.conn.schema)",
            "",
            "    def get_conn(self) -> Any:",
            "        return self.conn",
            "",
            "    def cmd_mask_password(self, cmd_orig: list[str]) -> list[str]:",
            "        \"\"\"Mask command password for safety.\"\"\"",
            "        cmd = deepcopy(cmd_orig)",
            "        try:",
            "            password_index = cmd.index(\"--password\")",
            "            cmd[password_index + 1] = \"MASKED\"",
            "        except ValueError:",
            "            self.log.debug(\"No password in sqoop cmd\")",
            "        return cmd",
            "",
            "    def popen(self, cmd: list[str], **kwargs: Any) -> None:",
            "        \"\"\"Remote Popen.",
            "",
            "        :param cmd: command to remotely execute",
            "        :param kwargs: extra arguments to Popen (see subprocess.Popen)",
            "        :return: handle to subprocess",
            "        \"\"\"",
            "        masked_cmd = \" \".join(self.cmd_mask_password(cmd))",
            "        self.log.info(\"Executing command: %s\", masked_cmd)",
            "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargs) as sub_process:",
            "            self.sub_process_pid = sub_process.pid",
            "            for line in iter(sub_process.stdout):  # type: ignore",
            "                self.log.info(line.strip())",
            "            sub_process.wait()",
            "            self.log.info(\"Command exited with return code %s\", sub_process.returncode)",
            "            if sub_process.returncode:",
            "                raise AirflowException(f\"Sqoop command failed: {masked_cmd}\")",
            "",
            "    def _prepare_command(self, export: bool = False) -> list[str]:",
            "        if \"?\" in self.conn.host:",
            "            raise ValueError(\"The sqoop connection host should not contain a '?' character\")",
            "",
            "        sqoop_cmd_type = \"export\" if export else \"import\"",
            "        connection_cmd = [\"sqoop\", sqoop_cmd_type]",
            "",
            "        for key, value in self.properties.items():",
            "            connection_cmd += [\"-D\", f\"{key}={value}\"]",
            "",
            "        if self.namenode:",
            "            connection_cmd += [\"-fs\", self.namenode]",
            "        if self.job_tracker:",
            "            connection_cmd += [\"-jt\", self.job_tracker]",
            "        if self.libjars:",
            "            connection_cmd += [\"-libjars\", self.libjars]",
            "        if self.files:",
            "            connection_cmd += [\"-files\", self.files]",
            "        if self.archives:",
            "            connection_cmd += [\"-archives\", self.archives]",
            "        if self.conn.login:",
            "            connection_cmd += [\"--username\", self.conn.login]",
            "        if self.conn.password:",
            "            connection_cmd += [\"--password\", self.conn.password]",
            "        if self.password_file:",
            "            connection_cmd += [\"--password-file\", self.password_file]",
            "        if self.verbose:",
            "            connection_cmd += [\"--verbose\"]",
            "        if self.num_mappers:",
            "            connection_cmd += [\"--num-mappers\", str(self.num_mappers)]",
            "        if self.hcatalog_database:",
            "            connection_cmd += [\"--hcatalog-database\", self.hcatalog_database]",
            "        if self.hcatalog_table:",
            "            connection_cmd += [\"--hcatalog-table\", self.hcatalog_table]",
            "",
            "        connect_str = self.conn.host",
            "        if self.conn.port:",
            "            connect_str += f\":{self.conn.port}\"",
            "        if self.conn.schema:",
            "            self.log.info(\"CONNECTION TYPE %s\", self.conn.conn_type)",
            "            if self.conn.conn_type != \"mssql\":",
            "                connect_str += f\"/{self.conn.schema}\"",
            "            else:",
            "                connect_str += f\";databaseName={self.conn.schema}\"",
            "        connection_cmd += [\"--connect\", connect_str]",
            "",
            "        return connection_cmd",
            "",
            "    @staticmethod",
            "    def _get_export_format_argument(file_type: str = \"text\") -> list[str]:",
            "        if file_type == \"avro\":",
            "            return [\"--as-avrodatafile\"]",
            "        elif file_type == \"sequence\":",
            "            return [\"--as-sequencefile\"]",
            "        elif file_type == \"parquet\":",
            "            return [\"--as-parquetfile\"]",
            "        elif file_type == \"text\":",
            "            return [\"--as-textfile\"]",
            "        else:",
            "            raise AirflowException(\"Argument file_type should be 'avro', 'sequence', 'parquet' or 'text'.\")",
            "",
            "    def _import_cmd(",
            "        self,",
            "        target_dir: str | None,",
            "        append: bool,",
            "        file_type: str,",
            "        split_by: str | None,",
            "        direct: bool | None,",
            "        driver: Any,",
            "        extra_import_options: Any,",
            "    ) -> list[str]:",
            "",
            "        cmd = self._prepare_command(export=False)",
            "",
            "        if target_dir:",
            "            cmd += [\"--target-dir\", target_dir]",
            "",
            "        if append:",
            "            cmd += [\"--append\"]",
            "",
            "        cmd += self._get_export_format_argument(file_type)",
            "",
            "        if split_by:",
            "            cmd += [\"--split-by\", split_by]",
            "",
            "        if direct:",
            "            cmd += [\"--direct\"]",
            "",
            "        if driver:",
            "            cmd += [\"--driver\", driver]",
            "",
            "        if extra_import_options:",
            "            for key, value in extra_import_options.items():",
            "                cmd += [f\"--{key}\"]",
            "                if value:",
            "                    cmd += [str(value)]",
            "",
            "        return cmd",
            "",
            "    def import_table(",
            "        self,",
            "        table: str,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        columns: str | None = None,",
            "        split_by: str | None = None,",
            "        where: str | None = None,",
            "        direct: bool = False,",
            "        driver: Any = None,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> Any:",
            "        \"\"\"Import table from remote location to target dir.",
            "",
            "        Arguments are copies of direct sqoop command line arguments.",
            "",
            "        :param table: Table to read",
            "        :param schema: Schema name",
            "        :param target_dir: HDFS destination dir",
            "        :param append: Append data to an existing dataset in HDFS",
            "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".",
            "            Imports data to into the specified format. Defaults to text.",
            "        :param columns: <col,col,col\u2026> Columns to import from table",
            "        :param split_by: Column of the table used to split work units",
            "        :param where: WHERE clause to use during import",
            "        :param direct: Use direct connector if exists for the database",
            "        :param driver: Manually specify JDBC driver class to use",
            "        :param extra_import_options: Extra import options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)",
            "",
            "        cmd += [\"--table\", table]",
            "",
            "        if columns:",
            "            cmd += [\"--columns\", columns]",
            "        if where:",
            "            cmd += [\"--where\", where]",
            "        if schema:",
            "            cmd += [\"--\", \"--schema\", schema]",
            "",
            "        self.popen(cmd)",
            "",
            "    def import_query(",
            "        self,",
            "        query: str,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        split_by: str | None = None,",
            "        direct: bool | None = None,",
            "        driver: Any | None = None,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "    ) -> Any:",
            "        \"\"\"Import a specific query from the rdbms to hdfs.",
            "",
            "        :param query: Free format query to run",
            "        :param target_dir: HDFS destination dir",
            "        :param append: Append data to an existing dataset in HDFS",
            "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\"",
            "            Imports data to hdfs into the specified format. Defaults to text.",
            "        :param split_by: Column of the table used to split work units",
            "        :param direct: Use direct import fast path",
            "        :param driver: Manually specify JDBC driver class to use",
            "        :param extra_import_options: Extra import options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)",
            "        cmd += [\"--query\", query]",
            "",
            "        self.popen(cmd)",
            "",
            "    def _export_cmd(",
            "        self,",
            "        table: str,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> list[str]:",
            "",
            "        cmd = self._prepare_command(export=True)",
            "",
            "        if input_null_string:",
            "            cmd += [\"--input-null-string\", input_null_string]",
            "",
            "        if input_null_non_string:",
            "            cmd += [\"--input-null-non-string\", input_null_non_string]",
            "",
            "        if staging_table:",
            "            cmd += [\"--staging-table\", staging_table]",
            "",
            "        if clear_staging_table:",
            "            cmd += [\"--clear-staging-table\"]",
            "",
            "        if enclosed_by:",
            "            cmd += [\"--enclosed-by\", enclosed_by]",
            "",
            "        if escaped_by:",
            "            cmd += [\"--escaped-by\", escaped_by]",
            "",
            "        if input_fields_terminated_by:",
            "            cmd += [\"--input-fields-terminated-by\", input_fields_terminated_by]",
            "",
            "        if input_lines_terminated_by:",
            "            cmd += [\"--input-lines-terminated-by\", input_lines_terminated_by]",
            "",
            "        if input_optionally_enclosed_by:",
            "            cmd += [\"--input-optionally-enclosed-by\", input_optionally_enclosed_by]",
            "",
            "        if batch:",
            "            cmd += [\"--batch\"]",
            "",
            "        if relaxed_isolation:",
            "            cmd += [\"--relaxed-isolation\"]",
            "",
            "        if export_dir:",
            "            cmd += [\"--export-dir\", export_dir]",
            "",
            "        if extra_export_options:",
            "            for key, value in extra_export_options.items():",
            "                cmd += [f\"--{key}\"]",
            "                if value:",
            "                    cmd += [str(value)]",
            "",
            "        # The required option",
            "        cmd += [\"--table\", table]",
            "",
            "        if schema:",
            "            cmd += [\"--\", \"--schema\", schema]",
            "",
            "        return cmd",
            "",
            "    def export_table(",
            "        self,",
            "        table: str,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> None:",
            "        \"\"\"Export Hive table to remote location.",
            "",
            "        Arguments are copies of direct Sqoop command line Arguments",
            "",
            "        :param table: Table remote destination",
            "        :param schema: Schema name",
            "        :param export_dir: Hive table to export",
            "        :param input_null_string: The string to be interpreted as null for",
            "            string columns",
            "        :param input_null_non_string: The string to be interpreted as null",
            "            for non-string columns",
            "        :param staging_table: The table in which data will be staged before",
            "            being inserted into the destination table",
            "        :param clear_staging_table: Indicate that any data present in the",
            "            staging table can be deleted",
            "        :param enclosed_by: Sets a required field enclosing character",
            "        :param escaped_by: Sets the escape character",
            "        :param input_fields_terminated_by: Sets the field separator character",
            "        :param input_lines_terminated_by: Sets the end-of-line character",
            "        :param input_optionally_enclosed_by: Sets a field enclosing character",
            "        :param batch: Use batch mode for underlying statement execution",
            "        :param relaxed_isolation: Transaction isolation to read uncommitted",
            "            for the mappers",
            "        :param extra_export_options: Extra export options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._export_cmd(",
            "            table,",
            "            export_dir,",
            "            input_null_string,",
            "            input_null_non_string,",
            "            staging_table,",
            "            clear_staging_table,",
            "            enclosed_by,",
            "            escaped_by,",
            "            input_fields_terminated_by,",
            "            input_lines_terminated_by,",
            "            input_optionally_enclosed_by,",
            "            batch,",
            "            relaxed_isolation,",
            "            extra_export_options,",
            "            schema,",
            "        )",
            "",
            "        self.popen(cmd)"
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"This module contains a sqoop 1.x hook.\"\"\"",
            "from __future__ import annotations",
            "",
            "import subprocess",
            "from copy import deepcopy",
            "from typing import Any",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.hooks.base import BaseHook",
            "",
            "",
            "class SqoopHook(BaseHook):",
            "    \"\"\"Wrapper around the sqoop 1 binary.",
            "",
            "    To be able to use the hook, it is required that \"sqoop\" is in the PATH.",
            "",
            "    Additional arguments that can be passed via the 'extra' JSON field of the",
            "    sqoop connection:",
            "",
            "        * ``job_tracker``: Job tracker local|jobtracker:port.",
            "        * ``namenode``: Namenode.",
            "        * ``files``: Comma separated files to be copied to the map reduce cluster.",
            "        * ``archives``: Comma separated archives to be unarchived on the compute",
            "            machines.",
            "        * ``password_file``: Path to file containing the password.",
            "",
            "    :param conn_id: Reference to the sqoop connection.",
            "    :param verbose: Set sqoop to verbose.",
            "    :param num_mappers: Number of map tasks to import in parallel.",
            "    :param properties: Properties to set via the -D argument",
            "    :param libjars: Optional Comma separated jar files to include in the classpath.",
            "    :param extra_options:  Extra import/export options to pass as dict.",
            "        If a key doesn't have a value, just pass an empty string to it.",
            "        Don't include prefix of -- for sqoop options.",
            "    \"\"\"",
            "",
            "    conn_name_attr = \"conn_id\"",
            "    default_conn_name = \"sqoop_default\"",
            "    conn_type = \"sqoop\"",
            "    hook_name = \"Sqoop\"",
            "",
            "    def __init__(",
            "        self,",
            "        conn_id: str = default_conn_name,",
            "        verbose: bool = False,",
            "        num_mappers: int | None = None,",
            "        hcatalog_database: str | None = None,",
            "        hcatalog_table: str | None = None,",
            "        properties: dict[str, Any] | None = None,",
            "        libjars: str | None = None,",
            "        extra_options: dict[str, Any] | None = None,",
            "    ) -> None:",
            "        # No mutable types in the default parameters",
            "        super().__init__()",
            "        self.conn = self.get_connection(conn_id)",
            "        connection_parameters = self.conn.extra_dejson",
            "        self.job_tracker = connection_parameters.get(\"job_tracker\", None)",
            "        self.namenode = connection_parameters.get(\"namenode\", None)",
            "        self.libjars = libjars",
            "        self.files = connection_parameters.get(\"files\", None)",
            "        self.archives = connection_parameters.get(\"archives\", None)",
            "        self.password_file = connection_parameters.get(\"password_file\", None)",
            "        self.hcatalog_database = hcatalog_database",
            "        self.hcatalog_table = hcatalog_table",
            "        self.verbose = verbose",
            "        self.num_mappers = num_mappers",
            "        self.properties = properties or {}",
            "        self.sub_process_pid: int",
            "        self._extra_options = extra_options",
            "        self.log.info(\"Using connection to: %s:%s/%s\", self.conn.host, self.conn.port, self.conn.schema)",
            "",
            "    def get_conn(self) -> Any:",
            "        return self.conn",
            "",
            "    def cmd_mask_password(self, cmd_orig: list[str]) -> list[str]:",
            "        \"\"\"Mask command password for safety.\"\"\"",
            "        cmd = deepcopy(cmd_orig)",
            "        try:",
            "            password_index = cmd.index(\"--password\")",
            "            cmd[password_index + 1] = \"MASKED\"",
            "        except ValueError:",
            "            self.log.debug(\"No password in sqoop cmd\")",
            "        return cmd",
            "",
            "    def popen(self, cmd: list[str], **kwargs: Any) -> None:",
            "        \"\"\"Remote Popen.",
            "",
            "        :param cmd: command to remotely execute",
            "        :param kwargs: extra arguments to Popen (see subprocess.Popen)",
            "        :return: handle to subprocess",
            "        \"\"\"",
            "        masked_cmd = \" \".join(self.cmd_mask_password(cmd))",
            "        self.log.info(\"Executing command: %s\", masked_cmd)",
            "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargs) as sub_process:",
            "            self.sub_process_pid = sub_process.pid",
            "            for line in iter(sub_process.stdout):  # type: ignore",
            "                self.log.info(line.strip())",
            "            sub_process.wait()",
            "            self.log.info(\"Command exited with return code %s\", sub_process.returncode)",
            "            if sub_process.returncode:",
            "                raise AirflowException(f\"Sqoop command failed: {masked_cmd}\")",
            "",
            "    def _prepare_command(self, export: bool = False) -> list[str]:",
            "        sqoop_cmd_type = \"export\" if export else \"import\"",
            "        connection_cmd = [\"sqoop\", sqoop_cmd_type]",
            "",
            "        for key, value in self.properties.items():",
            "            connection_cmd += [\"-D\", f\"{key}={value}\"]",
            "",
            "        if self.namenode:",
            "            connection_cmd += [\"-fs\", self.namenode]",
            "        if self.job_tracker:",
            "            connection_cmd += [\"-jt\", self.job_tracker]",
            "        if self.libjars:",
            "            connection_cmd += [\"-libjars\", self.libjars]",
            "        if self.files:",
            "            connection_cmd += [\"-files\", self.files]",
            "        if self.archives:",
            "            connection_cmd += [\"-archives\", self.archives]",
            "        if self.conn.login:",
            "            connection_cmd += [\"--username\", self.conn.login]",
            "        if self.conn.password:",
            "            connection_cmd += [\"--password\", self.conn.password]",
            "        if self.password_file:",
            "            connection_cmd += [\"--password-file\", self.password_file]",
            "        if self.verbose:",
            "            connection_cmd += [\"--verbose\"]",
            "        if self.num_mappers:",
            "            connection_cmd += [\"--num-mappers\", str(self.num_mappers)]",
            "        if self.hcatalog_database:",
            "            connection_cmd += [\"--hcatalog-database\", self.hcatalog_database]",
            "        if self.hcatalog_table:",
            "            connection_cmd += [\"--hcatalog-table\", self.hcatalog_table]",
            "",
            "        connect_str = self.conn.host",
            "        if self.conn.port:",
            "            connect_str += f\":{self.conn.port}\"",
            "        if self.conn.schema:",
            "            self.log.info(\"CONNECTION TYPE %s\", self.conn.conn_type)",
            "            if self.conn.conn_type != \"mssql\":",
            "                connect_str += f\"/{self.conn.schema}\"",
            "            else:",
            "                connect_str += f\";databaseName={self.conn.schema}\"",
            "        if \"?\" in connect_str:",
            "            raise ValueError(\"The sqoop connection string should not contain a '?' character\")",
            "        connection_cmd += [\"--connect\", connect_str]",
            "",
            "        return connection_cmd",
            "",
            "    @staticmethod",
            "    def _get_export_format_argument(file_type: str = \"text\") -> list[str]:",
            "        if file_type == \"avro\":",
            "            return [\"--as-avrodatafile\"]",
            "        elif file_type == \"sequence\":",
            "            return [\"--as-sequencefile\"]",
            "        elif file_type == \"parquet\":",
            "            return [\"--as-parquetfile\"]",
            "        elif file_type == \"text\":",
            "            return [\"--as-textfile\"]",
            "        else:",
            "            raise AirflowException(\"Argument file_type should be 'avro', 'sequence', 'parquet' or 'text'.\")",
            "",
            "    def _import_cmd(",
            "        self,",
            "        target_dir: str | None,",
            "        append: bool,",
            "        file_type: str,",
            "        split_by: str | None,",
            "        direct: bool | None,",
            "        driver: Any,",
            "    ) -> list[str]:",
            "",
            "        cmd = self._prepare_command(export=False)",
            "",
            "        if target_dir:",
            "            cmd += [\"--target-dir\", target_dir]",
            "",
            "        if append:",
            "            cmd += [\"--append\"]",
            "",
            "        cmd += self._get_export_format_argument(file_type)",
            "",
            "        if split_by:",
            "            cmd += [\"--split-by\", split_by]",
            "",
            "        if direct:",
            "            cmd += [\"--direct\"]",
            "",
            "        if driver:",
            "            cmd += [\"--driver\", driver]",
            "",
            "        if self._extra_options:",
            "            for key, value in self._extra_options.items():",
            "                cmd += [f\"--{key}\"]",
            "                if value:",
            "                    cmd += [str(value)]",
            "",
            "        return cmd",
            "",
            "    def import_table(",
            "        self,",
            "        table: str,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        columns: str | None = None,",
            "        split_by: str | None = None,",
            "        where: str | None = None,",
            "        direct: bool = False,",
            "        driver: Any = None,",
            "        schema: str | None = None,",
            "    ) -> Any:",
            "        \"\"\"Import table from remote location to target dir.",
            "",
            "        Arguments are copies of direct sqoop command line arguments.",
            "",
            "        :param table: Table to read",
            "        :param schema: Schema name",
            "        :param target_dir: HDFS destination dir",
            "        :param append: Append data to an existing dataset in HDFS",
            "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".",
            "            Imports data to into the specified format. Defaults to text.",
            "        :param columns: <col,col,col\u2026> Columns to import from table",
            "        :param split_by: Column of the table used to split work units",
            "        :param where: WHERE clause to use during import",
            "        :param direct: Use direct connector if exists for the database",
            "        :param driver: Manually specify JDBC driver class to use",
            "        \"\"\"",
            "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver)",
            "",
            "        cmd += [\"--table\", table]",
            "",
            "        if columns:",
            "            cmd += [\"--columns\", columns]",
            "        if where:",
            "            cmd += [\"--where\", where]",
            "        if schema:",
            "            cmd += [\"--\", \"--schema\", schema]",
            "",
            "        self.popen(cmd)",
            "",
            "    def import_query(",
            "        self,",
            "        query: str,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        split_by: str | None = None,",
            "        direct: bool | None = None,",
            "        driver: Any | None = None,",
            "    ) -> Any:",
            "        \"\"\"Import a specific query from the rdbms to hdfs.",
            "",
            "        :param query: Free format query to run",
            "        :param target_dir: HDFS destination dir",
            "        :param append: Append data to an existing dataset in HDFS",
            "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\"",
            "            Imports data to hdfs into the specified format. Defaults to text.",
            "        :param split_by: Column of the table used to split work units",
            "        :param direct: Use direct import fast path",
            "        :param driver: Manually specify JDBC driver class to use",
            "        \"\"\"",
            "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver)",
            "        cmd += [\"--query\", query]",
            "",
            "        self.popen(cmd)",
            "",
            "    def _export_cmd(",
            "        self,",
            "        table: str,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        schema: str | None = None,",
            "    ) -> list[str]:",
            "",
            "        cmd = self._prepare_command(export=True)",
            "",
            "        if input_null_string:",
            "            cmd += [\"--input-null-string\", input_null_string]",
            "",
            "        if input_null_non_string:",
            "            cmd += [\"--input-null-non-string\", input_null_non_string]",
            "",
            "        if staging_table:",
            "            cmd += [\"--staging-table\", staging_table]",
            "",
            "        if clear_staging_table:",
            "            cmd += [\"--clear-staging-table\"]",
            "",
            "        if enclosed_by:",
            "            cmd += [\"--enclosed-by\", enclosed_by]",
            "",
            "        if escaped_by:",
            "            cmd += [\"--escaped-by\", escaped_by]",
            "",
            "        if input_fields_terminated_by:",
            "            cmd += [\"--input-fields-terminated-by\", input_fields_terminated_by]",
            "",
            "        if input_lines_terminated_by:",
            "            cmd += [\"--input-lines-terminated-by\", input_lines_terminated_by]",
            "",
            "        if input_optionally_enclosed_by:",
            "            cmd += [\"--input-optionally-enclosed-by\", input_optionally_enclosed_by]",
            "",
            "        if batch:",
            "            cmd += [\"--batch\"]",
            "",
            "        if relaxed_isolation:",
            "            cmd += [\"--relaxed-isolation\"]",
            "",
            "        if export_dir:",
            "            cmd += [\"--export-dir\", export_dir]",
            "",
            "        if self._extra_options:",
            "            for key, value in self._extra_options.items():",
            "                cmd += [f\"--{key}\"]",
            "                if value:",
            "                    cmd += [str(value)]",
            "",
            "        # The required option",
            "        cmd += [\"--table\", table]",
            "",
            "        if schema:",
            "            cmd += [\"--\", \"--schema\", schema]",
            "",
            "        return cmd",
            "",
            "    def export_table(",
            "        self,",
            "        table: str,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        schema: str | None = None,",
            "    ) -> None:",
            "        \"\"\"Export Hive table to remote location.",
            "",
            "        Arguments are copies of direct Sqoop command line Arguments",
            "",
            "        :param table: Table remote destination",
            "        :param schema: Schema name",
            "        :param export_dir: Hive table to export",
            "        :param input_null_string: The string to be interpreted as null for",
            "            string columns",
            "        :param input_null_non_string: The string to be interpreted as null",
            "            for non-string columns",
            "        :param staging_table: The table in which data will be staged before",
            "            being inserted into the destination table",
            "        :param clear_staging_table: Indicate that any data present in the",
            "            staging table can be deleted",
            "        :param enclosed_by: Sets a required field enclosing character",
            "        :param escaped_by: Sets the escape character",
            "        :param input_fields_terminated_by: Sets the field separator character",
            "        :param input_lines_terminated_by: Sets the end-of-line character",
            "        :param input_optionally_enclosed_by: Sets a field enclosing character",
            "        :param batch: Use batch mode for underlying statement execution",
            "        :param relaxed_isolation: Transaction isolation to read uncommitted",
            "            for the mappers",
            "        \"\"\"",
            "        cmd = self._export_cmd(",
            "            table,",
            "            export_dir,",
            "            input_null_string,",
            "            input_null_non_string,",
            "            staging_table,",
            "            clear_staging_table,",
            "            enclosed_by,",
            "            escaped_by,",
            "            input_fields_terminated_by,",
            "            input_lines_terminated_by,",
            "            input_optionally_enclosed_by,",
            "            batch,",
            "            relaxed_isolation,",
            "            schema,",
            "        )",
            "",
            "        self.popen(cmd)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "116": [
                "SqoopHook",
                "_prepare_command"
            ],
            "117": [
                "SqoopHook",
                "_prepare_command"
            ],
            "118": [
                "SqoopHook",
                "_prepare_command"
            ],
            "184": [
                "SqoopHook",
                "_import_cmd"
            ],
            "206": [
                "SqoopHook",
                "_import_cmd"
            ],
            "207": [
                "SqoopHook",
                "_import_cmd"
            ],
            "225": [
                "SqoopHook",
                "import_table"
            ],
            "243": [
                "SqoopHook",
                "import_table"
            ],
            "244": [
                "SqoopHook",
                "import_table"
            ],
            "245": [
                "SqoopHook",
                "import_table"
            ],
            "247": [
                "SqoopHook",
                "import_table"
            ],
            "269": [
                "SqoopHook",
                "import_query"
            ],
            "281": [
                "SqoopHook",
                "import_query"
            ],
            "282": [
                "SqoopHook",
                "import_query"
            ],
            "283": [
                "SqoopHook",
                "import_query"
            ],
            "285": [
                "SqoopHook",
                "import_query"
            ],
            "305": [
                "SqoopHook",
                "_export_cmd"
            ],
            "347": [
                "SqoopHook",
                "_export_cmd"
            ],
            "348": [
                "SqoopHook",
                "_export_cmd"
            ],
            "376": [
                "SqoopHook",
                "export_table"
            ],
            "402": [
                "SqoopHook",
                "export_table"
            ],
            "403": [
                "SqoopHook",
                "export_table"
            ],
            "404": [
                "SqoopHook",
                "export_table"
            ],
            "420": [
                "SqoopHook",
                "export_table"
            ]
        },
        "addLocation": [
            "airflow.providers.apache.sqoop.hooks.sqoop.SqoopHook.self",
            "src.jinja2.utils",
            "airflow.providers.apache.sqoop.hooks.sqoop.SqoopHook"
        ]
    },
    "airflow/providers/apache/sqoop/operators/sqoop.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "     :param create_hcatalog_table: Have sqoop create the hcatalog table passed"
            },
            "1": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "         in or not"
            },
            "2": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "     :param properties: additional JVM properties passed to sqoop"
            },
            "3": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    :param extra_import_options: Extra import options to pass as dict."
            },
            "4": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        If a key doesn't have a value, just pass an empty string to it."
            },
            "5": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Don't include prefix of -- for sqoop options."
            },
            "6": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    :param extra_export_options: Extra export options to pass as dict."
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+    :param extra_options:  Extra import/export options to pass as dict to the SqoopHook."
            },
            "8": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "         If a key doesn't have a value, just pass an empty string to it."
            },
            "9": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "         Don't include prefix of -- for sqoop options."
            },
            "10": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 82,
                "PatchRowcode": "     :param libjars: Optional Comma separated jar files to include in the classpath."
            },
            "11": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "         \"input_lines_terminated_by\","
            },
            "12": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "         \"input_optionally_enclosed_by\","
            },
            "13": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "         \"properties\","
            },
            "14": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"extra_import_options\","
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+        \"extra_options\","
            },
            "16": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "         \"driver\","
            },
            "17": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"extra_export_options\","
            },
            "18": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "         \"hcatalog_database\","
            },
            "19": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "         \"hcatalog_table\","
            },
            "20": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 109,
                "PatchRowcode": "         \"schema\","
            },
            "21": {
                "beforePatchRowNumber": 148,
                "afterPatchRowNumber": 144,
                "PatchRowcode": "         hcatalog_database: str | None = None,"
            },
            "22": {
                "beforePatchRowNumber": 149,
                "afterPatchRowNumber": 145,
                "PatchRowcode": "         hcatalog_table: str | None = None,"
            },
            "23": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 146,
                "PatchRowcode": "         create_hcatalog_table: bool = False,"
            },
            "24": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        extra_import_options: dict[str, Any] | None = None,"
            },
            "25": {
                "beforePatchRowNumber": 152,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        extra_export_options: dict[str, Any] | None = None,"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 147,
                "PatchRowcode": "+        extra_options: dict[str, Any] | None = None,"
            },
            "27": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": 148,
                "PatchRowcode": "         schema: str | None = None,"
            },
            "28": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 149,
                "PatchRowcode": "         libjars: str | None = None,"
            },
            "29": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 150,
                "PatchRowcode": "         **kwargs: Any,"
            },
            "30": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "         self.hcatalog_table = hcatalog_table"
            },
            "31": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "         self.create_hcatalog_table = create_hcatalog_table"
            },
            "32": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "         self.properties = properties"
            },
            "33": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.extra_import_options = extra_import_options or {}"
            },
            "34": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.extra_export_options = extra_export_options or {}"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+        self.extra_options = extra_options or {}"
            },
            "36": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 184,
                "PatchRowcode": "         self.hook: SqoopHook | None = None"
            },
            "37": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "         self.schema = schema"
            },
            "38": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "         self.libjars = libjars"
            },
            "39": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "                 input_optionally_enclosed_by=self.input_optionally_enclosed_by,"
            },
            "40": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "                 batch=self.batch,"
            },
            "41": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "                 relaxed_isolation=self.relaxed_isolation,"
            },
            "42": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                extra_export_options=self.extra_export_options,"
            },
            "43": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "                 schema=self.schema,"
            },
            "44": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 209,
                "PatchRowcode": "             )"
            },
            "45": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 210,
                "PatchRowcode": "         elif self.cmd_type == \"import\":"
            },
            "46": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # add create hcatalog table to extra import options if option passed"
            },
            "47": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # if new params are added to constructor can pass them in here"
            },
            "48": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # so don't modify sqoop_hook for each param"
            },
            "49": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if self.create_hcatalog_table:"
            },
            "50": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.extra_import_options[\"create-hcatalog-table\"] = \"\""
            },
            "51": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "52": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 211,
                "PatchRowcode": "             if self.table and self.query:"
            },
            "53": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": 212,
                "PatchRowcode": "                 raise AirflowException(\"Cannot specify query and table together. Need to specify either or.\")"
            },
            "54": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 213,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": 222,
                "PatchRowcode": "                     where=self.where,"
            },
            "56": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": 223,
                "PatchRowcode": "                     direct=self.direct,"
            },
            "57": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": 224,
                "PatchRowcode": "                     driver=self.driver,"
            },
            "58": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    extra_import_options=self.extra_import_options,"
            },
            "59": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": 225,
                "PatchRowcode": "                     schema=self.schema,"
            },
            "60": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 226,
                "PatchRowcode": "                 )"
            },
            "61": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 227,
                "PatchRowcode": "             elif self.query:"
            },
            "62": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "                     split_by=self.split_by,"
            },
            "63": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 234,
                "PatchRowcode": "                     direct=self.direct,"
            },
            "64": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 235,
                "PatchRowcode": "                     driver=self.driver,"
            },
            "65": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    extra_import_options=self.extra_import_options,"
            },
            "66": {
                "beforePatchRowNumber": 251,
                "afterPatchRowNumber": 236,
                "PatchRowcode": "                 )"
            },
            "67": {
                "beforePatchRowNumber": 252,
                "afterPatchRowNumber": 237,
                "PatchRowcode": "             else:"
            },
            "68": {
                "beforePatchRowNumber": 253,
                "afterPatchRowNumber": 238,
                "PatchRowcode": "                 raise AirflowException(\"Provide query or table parameter to import using Sqoop\")"
            },
            "69": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "         os.killpg(os.getpgid(self.hook.sub_process_pid), signal.SIGTERM)"
            },
            "70": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 247,
                "PatchRowcode": " "
            },
            "71": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 248,
                "PatchRowcode": "     def _get_hook(self) -> SqoopHook:"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 249,
                "PatchRowcode": "+        \"\"\"Returns a SqoopHook instance.\"\"\""
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 250,
                "PatchRowcode": "+        # Add `create-hcatalog-table` to extra options if option passed to operator in case of `import`"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 251,
                "PatchRowcode": "+        # command. Similarly, if new parameters are added to the operator, you can pass them to"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 252,
                "PatchRowcode": "+        # `extra_options` so that you don't need to modify `SqoopHook` for each new parameter."
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 253,
                "PatchRowcode": "+        if self.cmd_type == \"import\" and self.create_hcatalog_table:"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 254,
                "PatchRowcode": "+            self.extra_options[\"create-hcatalog-table\"] = \"\""
            },
            "78": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "         return SqoopHook("
            },
            "79": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 256,
                "PatchRowcode": "             conn_id=self.conn_id,"
            },
            "80": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 257,
                "PatchRowcode": "             verbose=self.verbose,"
            },
            "81": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": 260,
                "PatchRowcode": "             hcatalog_table=self.hcatalog_table,"
            },
            "82": {
                "beforePatchRowNumber": 270,
                "afterPatchRowNumber": 261,
                "PatchRowcode": "             properties=self.properties,"
            },
            "83": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 262,
                "PatchRowcode": "             libjars=self.libjars,"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 263,
                "PatchRowcode": "+            extra_options=self.extra_options,"
            },
            "85": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 264,
                "PatchRowcode": "         )"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"This module contains a sqoop 1 operator.\"\"\"",
            "from __future__ import annotations",
            "",
            "import os",
            "import signal",
            "from typing import TYPE_CHECKING, Any, Sequence",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.models import BaseOperator",
            "from airflow.providers.apache.sqoop.hooks.sqoop import SqoopHook",
            "",
            "if TYPE_CHECKING:",
            "    from airflow.utils.context import Context",
            "",
            "",
            "class SqoopOperator(BaseOperator):",
            "    \"\"\"",
            "    Execute a Sqoop job.",
            "",
            "    Documentation for Apache Sqoop can be found here: https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html",
            "",
            "    :param conn_id: str",
            "    :param cmd_type: str specify command to execute \"export\" or \"import\"",
            "    :param schema: Schema name",
            "    :param table: Table to read",
            "    :param query: Import result of arbitrary SQL query. Instead of using the table,",
            "        columns and where arguments, you can specify a SQL statement with the query",
            "        argument. Must also specify a destination directory with target_dir.",
            "    :param target_dir: HDFS destination directory where the data",
            "        from the rdbms will be written",
            "    :param append: Append data to an existing dataset in HDFS",
            "    :param file_type: \"avro\", \"sequence\", \"text\" Imports data to",
            "        into the specified format. Defaults to text.",
            "    :param columns: <col,col,col> Columns to import from table",
            "    :param num_mappers: Use n mapper tasks to import/export in parallel",
            "    :param split_by: Column of the table used to split work units",
            "    :param where: WHERE clause to use during import",
            "    :param export_dir: HDFS Hive database directory to export to the rdbms",
            "    :param input_null_string: The string to be interpreted as null",
            "        for string columns",
            "    :param input_null_non_string: The string to be interpreted as null",
            "        for non-string columns",
            "    :param staging_table: The table in which data will be staged before",
            "        being inserted into the destination table",
            "    :param clear_staging_table: Indicate that any data present in the",
            "        staging table can be deleted",
            "    :param enclosed_by: Sets a required field enclosing character",
            "    :param escaped_by: Sets the escape character",
            "    :param input_fields_terminated_by: Sets the input field separator",
            "    :param input_lines_terminated_by: Sets the input end-of-line character",
            "    :param input_optionally_enclosed_by: Sets a field enclosing character",
            "    :param batch: Use batch mode for underlying statement execution",
            "    :param direct: Use direct export fast path",
            "    :param driver: Manually specify JDBC driver class to use",
            "    :param verbose: Switch to more verbose logging for debug purposes",
            "    :param relaxed_isolation: use read uncommitted isolation level",
            "    :param hcatalog_database: Specifies the database name for the HCatalog table",
            "    :param hcatalog_table: The argument value for this option is the HCatalog table",
            "    :param create_hcatalog_table: Have sqoop create the hcatalog table passed",
            "        in or not",
            "    :param properties: additional JVM properties passed to sqoop",
            "    :param extra_import_options: Extra import options to pass as dict.",
            "        If a key doesn't have a value, just pass an empty string to it.",
            "        Don't include prefix of -- for sqoop options.",
            "    :param extra_export_options: Extra export options to pass as dict.",
            "        If a key doesn't have a value, just pass an empty string to it.",
            "        Don't include prefix of -- for sqoop options.",
            "    :param libjars: Optional Comma separated jar files to include in the classpath.",
            "    \"\"\"",
            "",
            "    template_fields: Sequence[str] = (",
            "        \"conn_id\",",
            "        \"cmd_type\",",
            "        \"table\",",
            "        \"query\",",
            "        \"target_dir\",",
            "        \"file_type\",",
            "        \"columns\",",
            "        \"split_by\",",
            "        \"where\",",
            "        \"export_dir\",",
            "        \"input_null_string\",",
            "        \"input_null_non_string\",",
            "        \"staging_table\",",
            "        \"enclosed_by\",",
            "        \"escaped_by\",",
            "        \"input_fields_terminated_by\",",
            "        \"input_lines_terminated_by\",",
            "        \"input_optionally_enclosed_by\",",
            "        \"properties\",",
            "        \"extra_import_options\",",
            "        \"driver\",",
            "        \"extra_export_options\",",
            "        \"hcatalog_database\",",
            "        \"hcatalog_table\",",
            "        \"schema\",",
            "    )",
            "    template_fields_renderers = {\"query\": \"sql\"}",
            "    ui_color = \"#7D8CA4\"",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        conn_id: str = \"sqoop_default\",",
            "        cmd_type: str = \"import\",",
            "        table: str | None = None,",
            "        query: str | None = None,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        columns: str | None = None,",
            "        num_mappers: int | None = None,",
            "        split_by: str | None = None,",
            "        where: str | None = None,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        direct: bool = False,",
            "        driver: Any | None = None,",
            "        verbose: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        properties: dict[str, Any] | None = None,",
            "        hcatalog_database: str | None = None,",
            "        hcatalog_table: str | None = None,",
            "        create_hcatalog_table: bool = False,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "        libjars: str | None = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        super().__init__(**kwargs)",
            "        self.conn_id = conn_id",
            "        self.cmd_type = cmd_type",
            "        self.table = table",
            "        self.query = query",
            "        self.target_dir = target_dir",
            "        self.append = append",
            "        self.file_type = file_type",
            "        self.columns = columns",
            "        self.num_mappers = num_mappers",
            "        self.split_by = split_by",
            "        self.where = where",
            "        self.export_dir = export_dir",
            "        self.input_null_string = input_null_string",
            "        self.input_null_non_string = input_null_non_string",
            "        self.staging_table = staging_table",
            "        self.clear_staging_table = clear_staging_table",
            "        self.enclosed_by = enclosed_by",
            "        self.escaped_by = escaped_by",
            "        self.input_fields_terminated_by = input_fields_terminated_by",
            "        self.input_lines_terminated_by = input_lines_terminated_by",
            "        self.input_optionally_enclosed_by = input_optionally_enclosed_by",
            "        self.batch = batch",
            "        self.direct = direct",
            "        self.driver = driver",
            "        self.verbose = verbose",
            "        self.relaxed_isolation = relaxed_isolation",
            "        self.hcatalog_database = hcatalog_database",
            "        self.hcatalog_table = hcatalog_table",
            "        self.create_hcatalog_table = create_hcatalog_table",
            "        self.properties = properties",
            "        self.extra_import_options = extra_import_options or {}",
            "        self.extra_export_options = extra_export_options or {}",
            "        self.hook: SqoopHook | None = None",
            "        self.schema = schema",
            "        self.libjars = libjars",
            "",
            "    def execute(self, context: Context) -> None:",
            "        \"\"\"Execute sqoop job.\"\"\"",
            "        if self.hook is None:",
            "            self.hook = self._get_hook()",
            "",
            "        if self.cmd_type == \"export\":",
            "            self.hook.export_table(",
            "                table=self.table,  # type: ignore",
            "                export_dir=self.export_dir,",
            "                input_null_string=self.input_null_string,",
            "                input_null_non_string=self.input_null_non_string,",
            "                staging_table=self.staging_table,",
            "                clear_staging_table=self.clear_staging_table,",
            "                enclosed_by=self.enclosed_by,",
            "                escaped_by=self.escaped_by,",
            "                input_fields_terminated_by=self.input_fields_terminated_by,",
            "                input_lines_terminated_by=self.input_lines_terminated_by,",
            "                input_optionally_enclosed_by=self.input_optionally_enclosed_by,",
            "                batch=self.batch,",
            "                relaxed_isolation=self.relaxed_isolation,",
            "                extra_export_options=self.extra_export_options,",
            "                schema=self.schema,",
            "            )",
            "        elif self.cmd_type == \"import\":",
            "            # add create hcatalog table to extra import options if option passed",
            "            # if new params are added to constructor can pass them in here",
            "            # so don't modify sqoop_hook for each param",
            "            if self.create_hcatalog_table:",
            "                self.extra_import_options[\"create-hcatalog-table\"] = \"\"",
            "",
            "            if self.table and self.query:",
            "                raise AirflowException(\"Cannot specify query and table together. Need to specify either or.\")",
            "",
            "            if self.table:",
            "                self.hook.import_table(",
            "                    table=self.table,",
            "                    target_dir=self.target_dir,",
            "                    append=self.append,",
            "                    file_type=self.file_type,",
            "                    columns=self.columns,",
            "                    split_by=self.split_by,",
            "                    where=self.where,",
            "                    direct=self.direct,",
            "                    driver=self.driver,",
            "                    extra_import_options=self.extra_import_options,",
            "                    schema=self.schema,",
            "                )",
            "            elif self.query:",
            "                self.hook.import_query(",
            "                    query=self.query,",
            "                    target_dir=self.target_dir,",
            "                    append=self.append,",
            "                    file_type=self.file_type,",
            "                    split_by=self.split_by,",
            "                    direct=self.direct,",
            "                    driver=self.driver,",
            "                    extra_import_options=self.extra_import_options,",
            "                )",
            "            else:",
            "                raise AirflowException(\"Provide query or table parameter to import using Sqoop\")",
            "        else:",
            "            raise AirflowException(\"cmd_type should be 'import' or 'export'\")",
            "",
            "    def on_kill(self) -> None:",
            "        if self.hook is None:",
            "            self.hook = self._get_hook()",
            "        self.log.info(\"Sending SIGTERM signal to bash process group\")",
            "        os.killpg(os.getpgid(self.hook.sub_process_pid), signal.SIGTERM)",
            "",
            "    def _get_hook(self) -> SqoopHook:",
            "        return SqoopHook(",
            "            conn_id=self.conn_id,",
            "            verbose=self.verbose,",
            "            num_mappers=self.num_mappers,",
            "            hcatalog_database=self.hcatalog_database,",
            "            hcatalog_table=self.hcatalog_table,",
            "            properties=self.properties,",
            "            libjars=self.libjars,",
            "        )"
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"This module contains a sqoop 1 operator.\"\"\"",
            "from __future__ import annotations",
            "",
            "import os",
            "import signal",
            "from typing import TYPE_CHECKING, Any, Sequence",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.models import BaseOperator",
            "from airflow.providers.apache.sqoop.hooks.sqoop import SqoopHook",
            "",
            "if TYPE_CHECKING:",
            "    from airflow.utils.context import Context",
            "",
            "",
            "class SqoopOperator(BaseOperator):",
            "    \"\"\"",
            "    Execute a Sqoop job.",
            "",
            "    Documentation for Apache Sqoop can be found here: https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html",
            "",
            "    :param conn_id: str",
            "    :param cmd_type: str specify command to execute \"export\" or \"import\"",
            "    :param schema: Schema name",
            "    :param table: Table to read",
            "    :param query: Import result of arbitrary SQL query. Instead of using the table,",
            "        columns and where arguments, you can specify a SQL statement with the query",
            "        argument. Must also specify a destination directory with target_dir.",
            "    :param target_dir: HDFS destination directory where the data",
            "        from the rdbms will be written",
            "    :param append: Append data to an existing dataset in HDFS",
            "    :param file_type: \"avro\", \"sequence\", \"text\" Imports data to",
            "        into the specified format. Defaults to text.",
            "    :param columns: <col,col,col> Columns to import from table",
            "    :param num_mappers: Use n mapper tasks to import/export in parallel",
            "    :param split_by: Column of the table used to split work units",
            "    :param where: WHERE clause to use during import",
            "    :param export_dir: HDFS Hive database directory to export to the rdbms",
            "    :param input_null_string: The string to be interpreted as null",
            "        for string columns",
            "    :param input_null_non_string: The string to be interpreted as null",
            "        for non-string columns",
            "    :param staging_table: The table in which data will be staged before",
            "        being inserted into the destination table",
            "    :param clear_staging_table: Indicate that any data present in the",
            "        staging table can be deleted",
            "    :param enclosed_by: Sets a required field enclosing character",
            "    :param escaped_by: Sets the escape character",
            "    :param input_fields_terminated_by: Sets the input field separator",
            "    :param input_lines_terminated_by: Sets the input end-of-line character",
            "    :param input_optionally_enclosed_by: Sets a field enclosing character",
            "    :param batch: Use batch mode for underlying statement execution",
            "    :param direct: Use direct export fast path",
            "    :param driver: Manually specify JDBC driver class to use",
            "    :param verbose: Switch to more verbose logging for debug purposes",
            "    :param relaxed_isolation: use read uncommitted isolation level",
            "    :param hcatalog_database: Specifies the database name for the HCatalog table",
            "    :param hcatalog_table: The argument value for this option is the HCatalog table",
            "    :param create_hcatalog_table: Have sqoop create the hcatalog table passed",
            "        in or not",
            "    :param properties: additional JVM properties passed to sqoop",
            "    :param extra_options:  Extra import/export options to pass as dict to the SqoopHook.",
            "        If a key doesn't have a value, just pass an empty string to it.",
            "        Don't include prefix of -- for sqoop options.",
            "    :param libjars: Optional Comma separated jar files to include in the classpath.",
            "    \"\"\"",
            "",
            "    template_fields: Sequence[str] = (",
            "        \"conn_id\",",
            "        \"cmd_type\",",
            "        \"table\",",
            "        \"query\",",
            "        \"target_dir\",",
            "        \"file_type\",",
            "        \"columns\",",
            "        \"split_by\",",
            "        \"where\",",
            "        \"export_dir\",",
            "        \"input_null_string\",",
            "        \"input_null_non_string\",",
            "        \"staging_table\",",
            "        \"enclosed_by\",",
            "        \"escaped_by\",",
            "        \"input_fields_terminated_by\",",
            "        \"input_lines_terminated_by\",",
            "        \"input_optionally_enclosed_by\",",
            "        \"properties\",",
            "        \"extra_options\",",
            "        \"driver\",",
            "        \"hcatalog_database\",",
            "        \"hcatalog_table\",",
            "        \"schema\",",
            "    )",
            "    template_fields_renderers = {\"query\": \"sql\"}",
            "    ui_color = \"#7D8CA4\"",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        conn_id: str = \"sqoop_default\",",
            "        cmd_type: str = \"import\",",
            "        table: str | None = None,",
            "        query: str | None = None,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        columns: str | None = None,",
            "        num_mappers: int | None = None,",
            "        split_by: str | None = None,",
            "        where: str | None = None,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        direct: bool = False,",
            "        driver: Any | None = None,",
            "        verbose: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        properties: dict[str, Any] | None = None,",
            "        hcatalog_database: str | None = None,",
            "        hcatalog_table: str | None = None,",
            "        create_hcatalog_table: bool = False,",
            "        extra_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "        libjars: str | None = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        super().__init__(**kwargs)",
            "        self.conn_id = conn_id",
            "        self.cmd_type = cmd_type",
            "        self.table = table",
            "        self.query = query",
            "        self.target_dir = target_dir",
            "        self.append = append",
            "        self.file_type = file_type",
            "        self.columns = columns",
            "        self.num_mappers = num_mappers",
            "        self.split_by = split_by",
            "        self.where = where",
            "        self.export_dir = export_dir",
            "        self.input_null_string = input_null_string",
            "        self.input_null_non_string = input_null_non_string",
            "        self.staging_table = staging_table",
            "        self.clear_staging_table = clear_staging_table",
            "        self.enclosed_by = enclosed_by",
            "        self.escaped_by = escaped_by",
            "        self.input_fields_terminated_by = input_fields_terminated_by",
            "        self.input_lines_terminated_by = input_lines_terminated_by",
            "        self.input_optionally_enclosed_by = input_optionally_enclosed_by",
            "        self.batch = batch",
            "        self.direct = direct",
            "        self.driver = driver",
            "        self.verbose = verbose",
            "        self.relaxed_isolation = relaxed_isolation",
            "        self.hcatalog_database = hcatalog_database",
            "        self.hcatalog_table = hcatalog_table",
            "        self.create_hcatalog_table = create_hcatalog_table",
            "        self.properties = properties",
            "        self.extra_options = extra_options or {}",
            "        self.hook: SqoopHook | None = None",
            "        self.schema = schema",
            "        self.libjars = libjars",
            "",
            "    def execute(self, context: Context) -> None:",
            "        \"\"\"Execute sqoop job.\"\"\"",
            "        if self.hook is None:",
            "            self.hook = self._get_hook()",
            "",
            "        if self.cmd_type == \"export\":",
            "            self.hook.export_table(",
            "                table=self.table,  # type: ignore",
            "                export_dir=self.export_dir,",
            "                input_null_string=self.input_null_string,",
            "                input_null_non_string=self.input_null_non_string,",
            "                staging_table=self.staging_table,",
            "                clear_staging_table=self.clear_staging_table,",
            "                enclosed_by=self.enclosed_by,",
            "                escaped_by=self.escaped_by,",
            "                input_fields_terminated_by=self.input_fields_terminated_by,",
            "                input_lines_terminated_by=self.input_lines_terminated_by,",
            "                input_optionally_enclosed_by=self.input_optionally_enclosed_by,",
            "                batch=self.batch,",
            "                relaxed_isolation=self.relaxed_isolation,",
            "                schema=self.schema,",
            "            )",
            "        elif self.cmd_type == \"import\":",
            "            if self.table and self.query:",
            "                raise AirflowException(\"Cannot specify query and table together. Need to specify either or.\")",
            "",
            "            if self.table:",
            "                self.hook.import_table(",
            "                    table=self.table,",
            "                    target_dir=self.target_dir,",
            "                    append=self.append,",
            "                    file_type=self.file_type,",
            "                    columns=self.columns,",
            "                    split_by=self.split_by,",
            "                    where=self.where,",
            "                    direct=self.direct,",
            "                    driver=self.driver,",
            "                    schema=self.schema,",
            "                )",
            "            elif self.query:",
            "                self.hook.import_query(",
            "                    query=self.query,",
            "                    target_dir=self.target_dir,",
            "                    append=self.append,",
            "                    file_type=self.file_type,",
            "                    split_by=self.split_by,",
            "                    direct=self.direct,",
            "                    driver=self.driver,",
            "                )",
            "            else:",
            "                raise AirflowException(\"Provide query or table parameter to import using Sqoop\")",
            "        else:",
            "            raise AirflowException(\"cmd_type should be 'import' or 'export'\")",
            "",
            "    def on_kill(self) -> None:",
            "        if self.hook is None:",
            "            self.hook = self._get_hook()",
            "        self.log.info(\"Sending SIGTERM signal to bash process group\")",
            "        os.killpg(os.getpgid(self.hook.sub_process_pid), signal.SIGTERM)",
            "",
            "    def _get_hook(self) -> SqoopHook:",
            "        \"\"\"Returns a SqoopHook instance.\"\"\"",
            "        # Add `create-hcatalog-table` to extra options if option passed to operator in case of `import`",
            "        # command. Similarly, if new parameters are added to the operator, you can pass them to",
            "        # `extra_options` so that you don't need to modify `SqoopHook` for each new parameter.",
            "        if self.cmd_type == \"import\" and self.create_hcatalog_table:",
            "            self.extra_options[\"create-hcatalog-table\"] = \"\"",
            "        return SqoopHook(",
            "            conn_id=self.conn_id,",
            "            verbose=self.verbose,",
            "            num_mappers=self.num_mappers,",
            "            hcatalog_database=self.hcatalog_database,",
            "            hcatalog_table=self.hcatalog_table,",
            "            properties=self.properties,",
            "            libjars=self.libjars,",
            "            extra_options=self.extra_options,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0"
        ],
        "dele_reviseLocation": {
            "79": [
                "SqoopOperator"
            ],
            "80": [
                "SqoopOperator"
            ],
            "81": [
                "SqoopOperator"
            ],
            "82": [
                "SqoopOperator"
            ],
            "108": [
                "SqoopOperator"
            ],
            "110": [
                "SqoopOperator"
            ],
            "151": [
                "SqoopOperator",
                "__init__"
            ],
            "152": [
                "SqoopOperator",
                "__init__"
            ],
            "188": [
                "SqoopOperator",
                "__init__"
            ],
            "189": [
                "SqoopOperator",
                "__init__"
            ],
            "214": [
                "SqoopOperator",
                "execute"
            ],
            "218": [
                "SqoopOperator",
                "execute"
            ],
            "219": [
                "SqoopOperator",
                "execute"
            ],
            "220": [
                "SqoopOperator",
                "execute"
            ],
            "221": [
                "SqoopOperator",
                "execute"
            ],
            "222": [
                "SqoopOperator",
                "execute"
            ],
            "223": [
                "SqoopOperator",
                "execute"
            ],
            "238": [
                "SqoopOperator",
                "execute"
            ],
            "250": [
                "SqoopOperator",
                "execute"
            ]
        },
        "addLocation": []
    }
}