{
    "superset/connectors/sqla/models.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " from superset.common.db_query_status import QueryStatus"
            },
            "1": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": " from superset.connectors.base.models import BaseColumn, BaseDatasource, BaseMetric"
            },
            "2": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 76,
                "PatchRowcode": " from superset.connectors.sqla.utils import ("
            },
            "3": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    find_cached_objects_in_session,"
            },
            "4": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "     get_columns_description,"
            },
            "5": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "     get_physical_table_metadata,"
            },
            "6": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "     get_virtual_table_metadata,"
            },
            "7": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " )"
            },
            "8": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from superset.datasets.models import Dataset as NewDataset"
            },
            "9": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 81,
                "PatchRowcode": " from superset.db_engine_specs.base import BaseEngineSpec, TimestampExpression"
            },
            "10": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 82,
                "PatchRowcode": " from superset.exceptions import ("
            },
            "11": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "     ColumnNotFoundException,"
            },
            "12": {
                "beforePatchRowNumber": 1430,
                "afterPatchRowNumber": 1428,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 1431,
                "afterPatchRowNumber": 1429,
                "PatchRowcode": "     @staticmethod"
            },
            "14": {
                "beforePatchRowNumber": 1432,
                "afterPatchRowNumber": 1430,
                "PatchRowcode": "     def before_update("
            },
            "15": {
                "beforePatchRowNumber": 1433,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        mapper: Mapper,  # pylint: disable=unused-argument"
            },
            "16": {
                "beforePatchRowNumber": 1434,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        connection: Connection,  # pylint: disable=unused-argument"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1431,
                "PatchRowcode": "+        mapper: Mapper,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1432,
                "PatchRowcode": "+        connection: Connection,"
            },
            "19": {
                "beforePatchRowNumber": 1435,
                "afterPatchRowNumber": 1433,
                "PatchRowcode": "         target: SqlaTable,"
            },
            "20": {
                "beforePatchRowNumber": 1436,
                "afterPatchRowNumber": 1434,
                "PatchRowcode": "     ) -> None:"
            },
            "21": {
                "beforePatchRowNumber": 1437,
                "afterPatchRowNumber": 1435,
                "PatchRowcode": "         \"\"\""
            },
            "22": {
                "beforePatchRowNumber": 1438,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Check before update if the target table already exists."
            },
            "23": {
                "beforePatchRowNumber": 1439,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "24": {
                "beforePatchRowNumber": 1440,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Note this listener is called when any fields are being updated and thus it is"
            },
            "25": {
                "beforePatchRowNumber": 1441,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        necessary to first check whether the reference table is being updated."
            },
            "26": {
                "beforePatchRowNumber": 1442,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "27": {
                "beforePatchRowNumber": 1443,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Note this logic is temporary, given uniqueness is handled via the dataset DAO,"
            },
            "28": {
                "beforePatchRowNumber": 1444,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        but is necessary until both the legacy datasource editor and datasource/save"
            },
            "29": {
                "beforePatchRowNumber": 1445,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        endpoints are deprecated."
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1436,
                "PatchRowcode": "+        Note this listener is called when any fields are being updated"
            },
            "31": {
                "beforePatchRowNumber": 1446,
                "afterPatchRowNumber": 1437,
                "PatchRowcode": " "
            },
            "32": {
                "beforePatchRowNumber": 1447,
                "afterPatchRowNumber": 1438,
                "PatchRowcode": "         :param mapper: The table mapper"
            },
            "33": {
                "beforePatchRowNumber": 1448,
                "afterPatchRowNumber": 1439,
                "PatchRowcode": "         :param connection: The DB-API connection"
            },
            "34": {
                "beforePatchRowNumber": 1449,
                "afterPatchRowNumber": 1440,
                "PatchRowcode": "         :param target: The mapped instance being persisted"
            },
            "35": {
                "beforePatchRowNumber": 1450,
                "afterPatchRowNumber": 1441,
                "PatchRowcode": "         :raises Exception: If the target table is not unique"
            },
            "36": {
                "beforePatchRowNumber": 1451,
                "afterPatchRowNumber": 1442,
                "PatchRowcode": "         \"\"\""
            },
            "37": {
                "beforePatchRowNumber": 1452,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "38": {
                "beforePatchRowNumber": 1453,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # pylint: disable=import-outside-toplevel"
            },
            "39": {
                "beforePatchRowNumber": 1454,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        from superset.daos.dataset import DatasetDAO"
            },
            "40": {
                "beforePatchRowNumber": 1455,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        from superset.datasets.commands.exceptions import get_dataset_exist_error_msg"
            },
            "41": {
                "beforePatchRowNumber": 1456,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "42": {
                "beforePatchRowNumber": 1457,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # Check whether the relevant attributes have changed."
            },
            "43": {
                "beforePatchRowNumber": 1458,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        state = db.inspect(target)  # pylint: disable=no-member"
            },
            "44": {
                "beforePatchRowNumber": 1459,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "45": {
                "beforePatchRowNumber": 1460,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        for attr in [\"database_id\", \"schema\", \"table_name\"]:"
            },
            "46": {
                "beforePatchRowNumber": 1461,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            history = state.get_history(attr, True)"
            },
            "47": {
                "beforePatchRowNumber": 1462,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if history.has_changes():"
            },
            "48": {
                "beforePatchRowNumber": 1463,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                break"
            },
            "49": {
                "beforePatchRowNumber": 1464,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        else:"
            },
            "50": {
                "beforePatchRowNumber": 1465,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return None"
            },
            "51": {
                "beforePatchRowNumber": 1466,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "52": {
                "beforePatchRowNumber": 1467,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not DatasetDAO.validate_uniqueness("
            },
            "53": {
                "beforePatchRowNumber": 1468,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            target.database_id, target.schema, target.table_name, target.id"
            },
            "54": {
                "beforePatchRowNumber": 1469,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ):"
            },
            "55": {
                "beforePatchRowNumber": 1470,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise Exception(get_dataset_exist_error_msg(target.full_name))"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1443,
                "PatchRowcode": "+        target.load_database()"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1444,
                "PatchRowcode": "+        security_manager.dataset_before_update(mapper, connection, target)"
            },
            "58": {
                "beforePatchRowNumber": 1471,
                "afterPatchRowNumber": 1445,
                "PatchRowcode": " "
            },
            "59": {
                "beforePatchRowNumber": 1472,
                "afterPatchRowNumber": 1446,
                "PatchRowcode": "     @staticmethod"
            },
            "60": {
                "beforePatchRowNumber": 1473,
                "afterPatchRowNumber": 1447,
                "PatchRowcode": "     def update_column(  # pylint: disable=unused-argument"
            },
            "61": {
                "beforePatchRowNumber": 1485,
                "afterPatchRowNumber": 1459,
                "PatchRowcode": "         # table is updated. This busts the cache key for all charts that use the table."
            },
            "62": {
                "beforePatchRowNumber": 1486,
                "afterPatchRowNumber": 1460,
                "PatchRowcode": "         session.execute(update(SqlaTable).where(SqlaTable.id == target.table.id))"
            },
            "63": {
                "beforePatchRowNumber": 1487,
                "afterPatchRowNumber": 1461,
                "PatchRowcode": " "
            },
            "64": {
                "beforePatchRowNumber": 1488,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # TODO: This shadow writing is deprecated"
            },
            "65": {
                "beforePatchRowNumber": 1489,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # if table itself has changed, shadow-writing will happen in `after_update` anyway"
            },
            "66": {
                "beforePatchRowNumber": 1490,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if target.table not in session.dirty:"
            },
            "67": {
                "beforePatchRowNumber": 1491,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            dataset: NewDataset = ("
            },
            "68": {
                "beforePatchRowNumber": 1492,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                session.query(NewDataset)"
            },
            "69": {
                "beforePatchRowNumber": 1493,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                .filter_by(uuid=target.table.uuid)"
            },
            "70": {
                "beforePatchRowNumber": 1494,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                .one_or_none()"
            },
            "71": {
                "beforePatchRowNumber": 1495,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "72": {
                "beforePatchRowNumber": 1496,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # Update shadow dataset and columns"
            },
            "73": {
                "beforePatchRowNumber": 1497,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # did we find the dataset?"
            },
            "74": {
                "beforePatchRowNumber": 1498,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if not dataset:"
            },
            "75": {
                "beforePatchRowNumber": 1499,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # if dataset is not found create a new copy"
            },
            "76": {
                "beforePatchRowNumber": 1500,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                target.table.write_shadow_dataset()"
            },
            "77": {
                "beforePatchRowNumber": 1501,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                return"
            },
            "78": {
                "beforePatchRowNumber": 1502,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "79": {
                "beforePatchRowNumber": 1503,
                "afterPatchRowNumber": 1462,
                "PatchRowcode": "     @staticmethod"
            },
            "80": {
                "beforePatchRowNumber": 1504,
                "afterPatchRowNumber": 1463,
                "PatchRowcode": "     def after_insert("
            },
            "81": {
                "beforePatchRowNumber": 1505,
                "afterPatchRowNumber": 1464,
                "PatchRowcode": "         mapper: Mapper,"
            },
            "82": {
                "beforePatchRowNumber": 1506,
                "afterPatchRowNumber": 1465,
                "PatchRowcode": "         connection: Connection,"
            },
            "83": {
                "beforePatchRowNumber": 1507,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sqla_table: SqlaTable,"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1466,
                "PatchRowcode": "+        target: SqlaTable,"
            },
            "85": {
                "beforePatchRowNumber": 1508,
                "afterPatchRowNumber": 1467,
                "PatchRowcode": "     ) -> None:"
            },
            "86": {
                "beforePatchRowNumber": 1509,
                "afterPatchRowNumber": 1468,
                "PatchRowcode": "         \"\"\""
            },
            "87": {
                "beforePatchRowNumber": 1510,
                "afterPatchRowNumber": 1469,
                "PatchRowcode": "         Update dataset permissions after insert"
            },
            "88": {
                "beforePatchRowNumber": 1511,
                "afterPatchRowNumber": 1470,
                "PatchRowcode": "         \"\"\""
            },
            "89": {
                "beforePatchRowNumber": 1512,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        security_manager.dataset_after_insert(mapper, connection, sqla_table)"
            },
            "90": {
                "beforePatchRowNumber": 1513,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "91": {
                "beforePatchRowNumber": 1514,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # TODO: deprecated"
            },
            "92": {
                "beforePatchRowNumber": 1515,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sqla_table.write_shadow_dataset()"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1471,
                "PatchRowcode": "+        target.load_database()"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1472,
                "PatchRowcode": "+        security_manager.dataset_after_insert(mapper, connection, target)"
            },
            "95": {
                "beforePatchRowNumber": 1516,
                "afterPatchRowNumber": 1473,
                "PatchRowcode": " "
            },
            "96": {
                "beforePatchRowNumber": 1517,
                "afterPatchRowNumber": 1474,
                "PatchRowcode": "     @staticmethod"
            },
            "97": {
                "beforePatchRowNumber": 1518,
                "afterPatchRowNumber": 1475,
                "PatchRowcode": "     def after_delete("
            },
            "98": {
                "beforePatchRowNumber": 1525,
                "afterPatchRowNumber": 1482,
                "PatchRowcode": "         \"\"\""
            },
            "99": {
                "beforePatchRowNumber": 1526,
                "afterPatchRowNumber": 1483,
                "PatchRowcode": "         security_manager.dataset_after_delete(mapper, connection, sqla_table)"
            },
            "100": {
                "beforePatchRowNumber": 1527,
                "afterPatchRowNumber": 1484,
                "PatchRowcode": " "
            },
            "101": {
                "beforePatchRowNumber": 1528,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    @staticmethod"
            },
            "102": {
                "beforePatchRowNumber": 1529,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def after_update("
            },
            "103": {
                "beforePatchRowNumber": 1530,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        mapper: Mapper,"
            },
            "104": {
                "beforePatchRowNumber": 1531,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        connection: Connection,"
            },
            "105": {
                "beforePatchRowNumber": 1532,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sqla_table: SqlaTable,"
            },
            "106": {
                "beforePatchRowNumber": 1533,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ) -> None:"
            },
            "107": {
                "beforePatchRowNumber": 1534,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "108": {
                "beforePatchRowNumber": 1535,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Update dataset permissions"
            },
            "109": {
                "beforePatchRowNumber": 1536,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "110": {
                "beforePatchRowNumber": 1537,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # set permissions"
            },
            "111": {
                "beforePatchRowNumber": 1538,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        security_manager.dataset_after_update(mapper, connection, sqla_table)"
            },
            "112": {
                "beforePatchRowNumber": 1539,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "113": {
                "beforePatchRowNumber": 1540,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # TODO: the shadow writing is deprecated"
            },
            "114": {
                "beforePatchRowNumber": 1541,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        inspector = inspect(sqla_table)"
            },
            "115": {
                "beforePatchRowNumber": 1542,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        session = inspector.session"
            },
            "116": {
                "beforePatchRowNumber": 1543,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "117": {
                "beforePatchRowNumber": 1544,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # double-check that ``UPDATE``s are actually pending (this method is called even"
            },
            "118": {
                "beforePatchRowNumber": 1545,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # for instances that have no net changes to their column-based attributes)"
            },
            "119": {
                "beforePatchRowNumber": 1546,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not session.is_modified(sqla_table, include_collections=True):"
            },
            "120": {
                "beforePatchRowNumber": 1547,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return"
            },
            "121": {
                "beforePatchRowNumber": 1548,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "122": {
                "beforePatchRowNumber": 1549,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # find the dataset from the known instance list first"
            },
            "123": {
                "beforePatchRowNumber": 1550,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # (it could be either from a previous query or newly created)"
            },
            "124": {
                "beforePatchRowNumber": 1551,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        dataset = next("
            },
            "125": {
                "beforePatchRowNumber": 1552,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            find_cached_objects_in_session("
            },
            "126": {
                "beforePatchRowNumber": 1553,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                session, NewDataset, uuids=[sqla_table.uuid]"
            },
            "127": {
                "beforePatchRowNumber": 1554,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            ),"
            },
            "128": {
                "beforePatchRowNumber": 1555,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            None,"
            },
            "129": {
                "beforePatchRowNumber": 1556,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "130": {
                "beforePatchRowNumber": 1557,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # if not found, pull from database"
            },
            "131": {
                "beforePatchRowNumber": 1558,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not dataset:"
            },
            "132": {
                "beforePatchRowNumber": 1559,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            dataset = ("
            },
            "133": {
                "beforePatchRowNumber": 1560,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                session.query(NewDataset).filter_by(uuid=sqla_table.uuid).one_or_none()"
            },
            "134": {
                "beforePatchRowNumber": 1561,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "135": {
                "beforePatchRowNumber": 1562,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not dataset:"
            },
            "136": {
                "beforePatchRowNumber": 1563,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            sqla_table.write_shadow_dataset()"
            },
            "137": {
                "beforePatchRowNumber": 1564,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return"
            },
            "138": {
                "beforePatchRowNumber": 1565,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "139": {
                "beforePatchRowNumber": 1566,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def write_shadow_dataset("
            },
            "140": {
                "beforePatchRowNumber": 1567,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self: SqlaTable,"
            },
            "141": {
                "beforePatchRowNumber": 1568,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ) -> None:"
            },
            "142": {
                "beforePatchRowNumber": 1569,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "143": {
                "beforePatchRowNumber": 1570,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        This method is deprecated"
            },
            "144": {
                "beforePatchRowNumber": 1571,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "145": {
                "beforePatchRowNumber": 1572,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        session = inspect(self).session"
            },
            "146": {
                "beforePatchRowNumber": 1573,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # most of the write_shadow_dataset functionality has been removed"
            },
            "147": {
                "beforePatchRowNumber": 1574,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # but leaving this portion in"
            },
            "148": {
                "beforePatchRowNumber": 1575,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # to remove later because it is adding a Database relationship to the session"
            },
            "149": {
                "beforePatchRowNumber": 1576,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # and there is some functionality that depends on this"
            },
            "150": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1485,
                "PatchRowcode": "+    def load_database(self: SqlaTable) -> None:"
            },
            "151": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1486,
                "PatchRowcode": "+        # somehow the database attribute is not loaded on access"
            },
            "152": {
                "beforePatchRowNumber": 1577,
                "afterPatchRowNumber": 1487,
                "PatchRowcode": "         if self.database_id and ("
            },
            "153": {
                "beforePatchRowNumber": 1578,
                "afterPatchRowNumber": 1488,
                "PatchRowcode": "             not self.database or self.database.id != self.database_id"
            },
            "154": {
                "beforePatchRowNumber": 1579,
                "afterPatchRowNumber": 1489,
                "PatchRowcode": "         ):"
            },
            "155": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1490,
                "PatchRowcode": "+            session = inspect(self).session"
            },
            "156": {
                "beforePatchRowNumber": 1580,
                "afterPatchRowNumber": 1491,
                "PatchRowcode": "             self.database = session.query(Database).filter_by(id=self.database_id).one()"
            },
            "157": {
                "beforePatchRowNumber": 1581,
                "afterPatchRowNumber": 1492,
                "PatchRowcode": " "
            },
            "158": {
                "beforePatchRowNumber": 1582,
                "afterPatchRowNumber": 1493,
                "PatchRowcode": " "
            },
            "159": {
                "beforePatchRowNumber": 1583,
                "afterPatchRowNumber": 1494,
                "PatchRowcode": " sa.event.listen(SqlaTable, \"before_update\", SqlaTable.before_update)"
            },
            "160": {
                "beforePatchRowNumber": 1584,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-sa.event.listen(SqlaTable, \"after_update\", SqlaTable.after_update)"
            },
            "161": {
                "beforePatchRowNumber": 1585,
                "afterPatchRowNumber": 1495,
                "PatchRowcode": " sa.event.listen(SqlaTable, \"after_insert\", SqlaTable.after_insert)"
            },
            "162": {
                "beforePatchRowNumber": 1586,
                "afterPatchRowNumber": 1496,
                "PatchRowcode": " sa.event.listen(SqlaTable, \"after_delete\", SqlaTable.after_delete)"
            },
            "163": {
                "beforePatchRowNumber": 1587,
                "afterPatchRowNumber": 1497,
                "PatchRowcode": " sa.event.listen(SqlMetric, \"after_update\", SqlaTable.update_column)"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from dataclasses import dataclass, field",
            "from datetime import datetime, timedelta",
            "from typing import Any, Callable, cast",
            "",
            "import dateutil.parser",
            "import numpy as np",
            "import pandas as pd",
            "import sqlalchemy as sa",
            "import sqlparse",
            "from flask import escape, Markup",
            "from flask_appbuilder import Model",
            "from flask_babel import lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import (",
            "    and_,",
            "    Boolean,",
            "    Column,",
            "    DateTime,",
            "    Enum,",
            "    ForeignKey,",
            "    inspect,",
            "    Integer,",
            "    or_,",
            "    select,",
            "    String,",
            "    Table,",
            "    Text,",
            "    update,",
            ")",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.ext.hybrid import hybrid_property",
            "from sqlalchemy.orm import (",
            "    backref,",
            "    Mapped,",
            "    Query,",
            "    reconstructor,",
            "    relationship,",
            "    RelationshipProperty,",
            "    Session,",
            ")",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.schema import UniqueConstraint",
            "from sqlalchemy.sql import column, ColumnElement, literal_column, table",
            "from sqlalchemy.sql.elements import ColumnClause, TextClause",
            "from sqlalchemy.sql.expression import Label, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "",
            "from superset import app, db, is_feature_enabled, security_manager",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.connectors.base.models import BaseColumn, BaseDatasource, BaseMetric",
            "from superset.connectors.sqla.utils import (",
            "    find_cached_objects_in_session,",
            "    get_columns_description,",
            "    get_physical_table_metadata,",
            "    get_virtual_table_metadata,",
            ")",
            "from superset.datasets.models import Dataset as NewDataset",
            "from superset.db_engine_specs.base import BaseEngineSpec, TimestampExpression",
            "from superset.exceptions import (",
            "    ColumnNotFoundException,",
            "    DatasetInvalidPermissionEvaluationException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetGenericDBErrorException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.jinja_context import (",
            "    BaseTemplateProcessor,",
            "    ExtraCache,",
            "    get_template_processor,",
            ")",
            "from superset.models.annotations import Annotation",
            "from superset.models.core import Database",
            "from superset.models.helpers import (",
            "    AuditMixinNullable,",
            "    CertificationMixin,",
            "    ExploreMixin,",
            "    QueryResult,",
            "    QueryStringExtended,",
            "    validate_adhoc_subquery,",
            ")",
            "from superset.sql_parse import ParsedQuery, sanitize_clause",
            "from superset.superset_typing import (",
            "    AdhocColumn,",
            "    AdhocMetric,",
            "    Metric,",
            "    QueryObjectDict,",
            "    ResultSetColumnType,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.core import GenericDataType, MediumText",
            "",
            "config = app.config",
            "metadata = Model.metadata  # pylint: disable=no-member",
            "logger = logging.getLogger(__name__)",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "",
            "# a non-exhaustive set of additive metrics",
            "ADDITIVE_METRIC_TYPES = {",
            "    \"count\",",
            "    \"sum\",",
            "    \"doubleSum\",",
            "}",
            "ADDITIVE_METRIC_TYPES_LOWER = {op.lower() for op in ADDITIVE_METRIC_TYPES}",
            "",
            "",
            "@dataclass",
            "class MetadataResult:",
            "    added: list[str] = field(default_factory=list)",
            "    removed: list[str] = field(default_factory=list)",
            "    modified: list[str] = field(default_factory=list)",
            "",
            "",
            "class AnnotationDatasource(BaseDatasource):",
            "    \"\"\"Dummy object so we can query annotations using 'Viz' objects just like",
            "    regular datasources.",
            "    \"\"\"",
            "",
            "    cache_timeout = 0",
            "    changed_on = None",
            "    type = \"annotation\"",
            "    column_names = [",
            "        \"created_on\",",
            "        \"changed_on\",",
            "        \"id\",",
            "        \"start_dttm\",",
            "        \"end_dttm\",",
            "        \"layer_id\",",
            "        \"short_descr\",",
            "        \"long_descr\",",
            "        \"json_metadata\",",
            "        \"created_by_fk\",",
            "        \"changed_by_fk\",",
            "    ]",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        error_message = None",
            "        qry = db.session.query(Annotation)",
            "        qry = qry.filter(Annotation.layer_id == query_obj[\"filter\"][0][\"val\"])",
            "        if query_obj[\"from_dttm\"]:",
            "            qry = qry.filter(Annotation.start_dttm >= query_obj[\"from_dttm\"])",
            "        if query_obj[\"to_dttm\"]:",
            "            qry = qry.filter(Annotation.end_dttm <= query_obj[\"to_dttm\"])",
            "        status = QueryStatus.SUCCESS",
            "        try:",
            "            df = pd.read_sql_query(qry.statement, db.engine)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.exception(ex)",
            "            error_message = utils.error_msg_from_exception(ex)",
            "        return QueryResult(",
            "            status=status,",
            "            df=df,",
            "            duration=timedelta(0),",
            "            query=\"\",",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        raise NotImplementedError()",
            "",
            "    def values_for_column(self, column_name: str, limit: int = 10000) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "",
            "class TableColumn(Model, BaseColumn, CertificationMixin):",
            "",
            "    \"\"\"ORM object for table columns, each table can have multiple columns\"\"\"",
            "",
            "    __tablename__ = \"table_columns\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"column_name\"),)",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"columns\",",
            "    )",
            "    is_dttm = Column(Boolean, default=False)",
            "    expression = Column(MediumText())",
            "    python_date_format = Column(String(255))",
            "    extra = Column(Text)",
            "",
            "    export_fields = [",
            "        \"table_id\",",
            "        \"column_name\",",
            "        \"verbose_name\",",
            "        \"is_dttm\",",
            "        \"is_active\",",
            "        \"type\",",
            "        \"advanced_data_type\",",
            "        \"groupby\",",
            "        \"filterable\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"python_date_format\",",
            "        \"extra\",",
            "    ]",
            "",
            "    update_from_object_fields = [s for s in export_fields if s not in (\"table_id\",)]",
            "    export_parent = \"table\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object.",
            "",
            "        Historically a TableColumn object (from an ORM perspective) was tighly bound to",
            "        a SqlaTable object, however with the introduction of the Query datasource this",
            "        is no longer true, i.e., the SqlaTable relationship is optional.",
            "",
            "        Now the TableColumn is either directly associated with the Database object (",
            "        which is unknown to the ORM) or indirectly via the SqlaTable object (courtesy of",
            "        the ORM) depending on the context.",
            "        \"\"\"",
            "",
            "        self._database: Database | None = kwargs.pop(\"database\", None)",
            "        super().__init__(**kwargs)",
            "",
            "    @reconstructor",
            "    def init_on_load(self) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object when invoked via the SQLAlchemy ORM.",
            "        \"\"\"",
            "",
            "        self._database = None",
            "",
            "    @property",
            "    def is_boolean(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a boolean datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.BOOLEAN",
            "",
            "    @property",
            "    def is_numeric(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a numeric datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.NUMERIC",
            "",
            "    @property",
            "    def is_string(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a string datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.STRING",
            "",
            "    @property",
            "    def is_temporal(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a temporal datatype. If column has been set as",
            "        temporal/non-temporal (`is_dttm` is True or False respectively), return that",
            "        value. This usually happens during initial metadata fetching or when a column",
            "        is manually set as temporal (for this `python_date_format` needs to be set).",
            "        \"\"\"",
            "        if self.is_dttm is not None:",
            "            return self.is_dttm",
            "        return self.type_generic == GenericDataType.TEMPORAL",
            "",
            "    @property",
            "    def database(self) -> Database:",
            "        return self.table.database if self.table else self._database",
            "",
            "    @property",
            "    def db_engine_spec(self) -> type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @property",
            "    def type_generic(self) -> utils.GenericDataType | None:",
            "        if self.is_dttm:",
            "            return GenericDataType.TEMPORAL",
            "",
            "        return (",
            "            column_spec.generic_type  # pylint: disable=used-before-assignment",
            "            if (",
            "                column_spec := self.db_engine_spec.get_column_spec(",
            "                    self.type,",
            "                    db_extra=self.db_extra,",
            "                )",
            "            )",
            "            else None",
            "        )",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        col = self.database.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    @property",
            "    def datasource(self) -> RelationshipProperty:",
            "        return self.table",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        time_grain: str | None,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TimestampExpression | Label:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "",
            "        pdf = self.python_date_format",
            "        is_epoch = pdf in (\"epoch_s\", \"epoch_ms\")",
            "        column_spec = self.db_engine_spec.get_column_spec(",
            "            self.type, db_extra=self.db_extra",
            "        )",
            "        type_ = column_spec.sqla_type if column_spec else DateTime",
            "        if not self.expression and not time_grain and not is_epoch:",
            "            sqla_col = column(self.column_name, type_=type_)",
            "            return self.database.make_sqla_column_compatible(sqla_col, label)",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, pdf, time_grain)",
            "        return self.database.make_sqla_column_compatible(time_expr, label)",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"id\",",
            "            \"column_name\",",
            "            \"verbose_name\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"filterable\",",
            "            \"groupby\",",
            "            \"is_dttm\",",
            "            \"type\",",
            "            \"type_generic\",",
            "            \"advanced_data_type\",",
            "            \"python_date_format\",",
            "            \"is_certified\",",
            "            \"certified_by\",",
            "            \"certification_details\",",
            "            \"warning_markdown\",",
            "        )",
            "",
            "        attr_dict = {s: getattr(self, s) for s in attrs if hasattr(self, s)}",
            "",
            "        attr_dict.update(super().data)",
            "",
            "        return attr_dict",
            "",
            "",
            "class SqlMetric(Model, BaseMetric, CertificationMixin):",
            "",
            "    \"\"\"ORM object for metrics, each table can have multiple metrics\"\"\"",
            "",
            "    __tablename__ = \"sql_metrics\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"metric_name\"),)",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"metrics\",",
            "    )",
            "    expression = Column(MediumText(), nullable=False)",
            "    extra = Column(Text)",
            "",
            "    export_fields = [",
            "        \"metric_name\",",
            "        \"verbose_name\",",
            "        \"metric_type\",",
            "        \"table_id\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"d3format\",",
            "        \"currency\",",
            "        \"extra\",",
            "        \"warning_text\",",
            "    ]",
            "    update_from_object_fields = list(s for s in export_fields if s != \"table_id\")",
            "    export_parent = \"table\"",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.metric_name)",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.metric_name",
            "        expression = self.expression",
            "        if template_processor:",
            "            expression = template_processor.process_template(expression)",
            "",
            "        sqla_col: ColumnClause = literal_column(expression)",
            "        return self.table.database.make_sqla_column_compatible(sqla_col, label)",
            "",
            "    @property",
            "    def perm(self) -> str | None:",
            "        return (",
            "            (\"{parent_name}.[{obj.metric_name}](id:{obj.id})\").format(",
            "                obj=self, parent_name=self.table.full_name",
            "            )",
            "            if self.table",
            "            else None",
            "        )",
            "",
            "    def get_perm(self) -> str | None:",
            "        return self.perm",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"is_certified\",",
            "            \"certified_by\",",
            "            \"certification_details\",",
            "            \"warning_markdown\",",
            "        )",
            "        attr_dict = {s: getattr(self, s) for s in attrs}",
            "",
            "        attr_dict.update(super().data)",
            "        return attr_dict",
            "",
            "",
            "sqlatable_user = Table(",
            "    \"sqlatable_user\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"user_id\", Integer, ForeignKey(\"ab_user.id\", ondelete=\"CASCADE\")),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\")),",
            ")",
            "",
            "",
            "def _process_sql_expression(",
            "    expression: str | None,",
            "    database_id: int,",
            "    schema: str,",
            "    template_processor: BaseTemplateProcessor | None = None,",
            ") -> str | None:",
            "    if template_processor and expression:",
            "        expression = template_processor.process_template(expression)",
            "    if expression:",
            "        try:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            expression = sanitize_clause(expression)",
            "        except (QueryClauseValidationException, SupersetSecurityException) as ex:",
            "            raise QueryObjectValidationError(ex.message) from ex",
            "    return expression",
            "",
            "",
            "class SqlaTable(",
            "    Model, BaseDatasource, ExploreMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"An ORM object for SqlAlchemy table references\"\"\"",
            "",
            "    type = \"table\"",
            "    query_language = \"sql\"",
            "    is_rls_supported = True",
            "    columns: Mapped[list[TableColumn]] = relationship(",
            "        TableColumn,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metrics: Mapped[list[SqlMetric]] = relationship(",
            "        SqlMetric,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metric_class = SqlMetric",
            "    column_class = TableColumn",
            "    owner_class = security_manager.user_model",
            "",
            "    __tablename__ = \"tables\"",
            "",
            "    # Note this uniqueness constraint is not part of the physical schema, i.e., it does",
            "    # not exist in the migrations, but is required by `import_from_dict` to ensure the",
            "    # correct filters are applied in order to identify uniqueness.",
            "    #",
            "    # The reason it does not physically exist is MySQL, PostgreSQL, etc. have a",
            "    # different interpretation of uniqueness when it comes to NULL which is problematic",
            "    # given the schema is optional.",
            "    __table_args__ = (UniqueConstraint(\"database_id\", \"schema\", \"table_name\"),)",
            "",
            "    table_name = Column(String(250), nullable=False)",
            "    main_dttm_col = Column(String(250))",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=False)",
            "    fetch_values_predicate = Column(Text)",
            "    owners = relationship(owner_class, secondary=sqlatable_user, backref=\"tables\")",
            "    database: Database = relationship(",
            "        \"Database\",",
            "        backref=backref(\"tables\", cascade=\"all, delete-orphan\"),",
            "        foreign_keys=[database_id],",
            "    )",
            "    schema = Column(String(255))",
            "    sql = Column(MediumText())",
            "    is_sqllab_view = Column(Boolean, default=False)",
            "    template_params = Column(Text)",
            "    extra = Column(Text)",
            "    normalize_columns = Column(Boolean, default=False)",
            "",
            "    baselink = \"tablemodelview\"",
            "",
            "    export_fields = [",
            "        \"table_name\",",
            "        \"main_dttm_col\",",
            "        \"description\",",
            "        \"default_endpoint\",",
            "        \"database_id\",",
            "        \"offset\",",
            "        \"cache_timeout\",",
            "        \"schema\",",
            "        \"sql\",",
            "        \"params\",",
            "        \"template_params\",",
            "        \"filter_select_enabled\",",
            "        \"fetch_values_predicate\",",
            "        \"extra\",",
            "        \"normalize_columns\",",
            "    ]",
            "    update_from_object_fields = [f for f in export_fields if f != \"database_id\"]",
            "    export_parent = \"database\"",
            "    export_children = [\"metrics\", \"columns\"]",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "",
            "    def __repr__(self) -> str:  # pylint: disable=invalid-repr-returned",
            "        return self.name",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: str | None) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    @property",
            "    def db_engine_spec(self) -> __builtins__.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if not self.changed_by:",
            "            return \"\"",
            "        return str(self.changed_by)",
            "",
            "    @property",
            "    def connection(self) -> str:",
            "        return str(self.database)",
            "",
            "    @property",
            "    def description_markeddown(self) -> str:",
            "        return utils.markdown(self.description)",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        return self.table_name",
            "",
            "    @property",
            "    def datasource_type(self) -> str:",
            "        return self.type",
            "",
            "    @property",
            "    def database_name(self) -> str:",
            "        return self.database.name",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls,",
            "        session: Session,",
            "        datasource_name: str,",
            "        schema: str | None,",
            "        database_name: str,",
            "    ) -> SqlaTable | None:",
            "        schema = schema or None",
            "        query = (",
            "            session.query(cls)",
            "            .join(Database)",
            "            .filter(cls.table_name == datasource_name)",
            "            .filter(Database.database_name == database_name)",
            "        )",
            "        # Handling schema being '' or None, which is easier to handle",
            "        # in python than in the SQLA query in a multi-dialect way",
            "        for tbl in query.all():",
            "            if schema == (tbl.schema or None):",
            "                return tbl",
            "        return None",
            "",
            "    @property",
            "    def link(self) -> Markup:",
            "        name = escape(self.name)",
            "        anchor = f'<a target=\"_blank\" href=\"{self.explore_url}\">{name}</a>'",
            "        return Markup(anchor)",
            "",
            "    def get_schema_perm(self) -> str | None:",
            "        \"\"\"Returns schema permission if present, database one otherwise.\"\"\"",
            "        return security_manager.get_schema_perm(self.database, self.schema)",
            "",
            "    def get_perm(self) -> str:",
            "        \"\"\"",
            "        Return this dataset permission name",
            "        :return: dataset permission name",
            "        :raises DatasetInvalidPermissionEvaluationException: When database is missing",
            "        \"\"\"",
            "        if self.database is None:",
            "            raise DatasetInvalidPermissionEvaluationException()",
            "        return f\"[{self.database}].[{self.table_name}](id:{self.id})\"",
            "",
            "    @hybrid_property",
            "    def name(self) -> str:  # pylint: disable=invalid-overridden-method",
            "        return self.schema + \".\" + self.table_name if self.schema else self.table_name",
            "",
            "    @property",
            "    def full_name(self) -> str:",
            "        return utils.get_datasource_full_name(",
            "            self.database, self.table_name, schema=self.schema",
            "        )",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        l = [c.column_name for c in self.columns if c.is_dttm]",
            "        if self.main_dttm_col and self.main_dttm_col not in l:",
            "            l.append(self.main_dttm_col)",
            "        return l",
            "",
            "    @property",
            "    def num_cols(self) -> list[str]:",
            "        return [c.column_name for c in self.columns if c.is_numeric]",
            "",
            "    @property",
            "    def any_dttm_col(self) -> str | None:",
            "        cols = self.dttm_cols",
            "        return cols[0] if cols else None",
            "",
            "    @property",
            "    def html(self) -> str:",
            "        df = pd.DataFrame((c.column_name, c.type) for c in self.columns)",
            "        df.columns = [\"field\", \"type\"]",
            "        return df.to_html(",
            "            index=False,",
            "            classes=(\"dataframe table table-striped table-bordered \" \"table-condensed\"),",
            "        )",
            "",
            "    @property",
            "    def sql_url(self) -> str:",
            "        return self.database.sql_url + \"?table_name=\" + str(self.table_name)",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        # todo(yongjie): create a physical table column type in a separate PR",
            "        if self.sql:",
            "            return get_virtual_table_metadata(dataset=self)",
            "        return get_physical_table_metadata(",
            "            database=self.database,",
            "            table_name=self.table_name,",
            "            schema_name=self.schema,",
            "            normalize_columns=self.normalize_columns,",
            "        )",
            "",
            "    @property",
            "    def time_column_grains(self) -> dict[str, Any]:",
            "        return {",
            "            \"time_columns\": self.dttm_cols,",
            "            \"time_grains\": [grain.name for grain in self.database.grains()],",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        # show_cols and latest_partition set to false to avoid",
            "        # the expensive cost of inspecting the DB",
            "        return self.database.select_star(",
            "            self.table_name, schema=self.schema, show_cols=False, latest_partition=False",
            "        )",
            "",
            "    @property",
            "    def health_check_message(self) -> str | None:",
            "        check = config[\"DATASET_HEALTH_CHECK\"]",
            "        return check(self) if check else None",
            "",
            "    @property",
            "    def granularity_sqla(self) -> list[tuple[Any, Any]]:",
            "        return utils.choicify(self.dttm_cols)",
            "",
            "    @property",
            "    def time_grain_sqla(self) -> list[tuple[Any, Any]]:",
            "        return [(g.duration, g.name) for g in self.database.grains() or []]",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        data_ = super().data",
            "        if self.type == \"table\":",
            "            data_[\"granularity_sqla\"] = self.granularity_sqla",
            "            data_[\"time_grain_sqla\"] = self.time_grain_sqla",
            "            data_[\"main_dttm_col\"] = self.main_dttm_col",
            "            data_[\"fetch_values_predicate\"] = self.fetch_values_predicate",
            "            data_[\"template_params\"] = self.template_params",
            "            data_[\"is_sqllab_view\"] = self.is_sqllab_view",
            "            data_[\"health_check_message\"] = self.health_check_message",
            "            data_[\"extra\"] = self.extra",
            "            data_[\"owners\"] = self.owners_data",
            "        return data_",
            "",
            "    @property",
            "    def extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TextClause:",
            "        fetch_values_predicate = self.fetch_values_predicate",
            "        if template_processor:",
            "            fetch_values_predicate = template_processor.process_template(",
            "                fetch_values_predicate",
            "            )",
            "        try:",
            "            return self.text(fetch_values_predicate)",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in fetch values predicate: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def values_for_column(self, column_name: str, limit: int = 10000) -> list[Any]:",
            "        \"\"\"Runs query against sqla to retrieve some",
            "        sample values for the given column.",
            "        \"\"\"",
            "        cols = {col.column_name: col for col in self.columns}",
            "        target_col = cols[column_name]",
            "        tp = self.get_template_processor()",
            "        tbl, cte = self.get_from_clause(tp)",
            "",
            "        qry = (",
            "            select([target_col.get_sqla_col(template_processor=tp)])",
            "            .select_from(tbl)",
            "            .distinct()",
            "        )",
            "        if limit:",
            "            qry = qry.limit(limit)",
            "",
            "        if self.fetch_values_predicate:",
            "            qry = qry.where(self.get_fetch_values_predicate(template_processor=tp))",
            "",
            "        with self.database.get_sqla_engine_with_context() as engine:",
            "            sql = qry.compile(engine, compile_kwargs={\"literal_binds\": True})",
            "            sql = self._apply_cte(sql, cte)",
            "            sql = self.mutate_query_from_config(sql)",
            "",
            "            df = pd.read_sql_query(sql=sql, con=engine)",
            "            return df[column_name].to_list()",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        sql_query_mutator = config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        return get_template_processor(table=self, database=self.database, **kwargs)",
            "",
            "    def get_query_str_extended(",
            "        self,",
            "        query_obj: QueryObjectDict,",
            "        mutate: bool = True,",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        sql = sqlparse.format(sql, reindent=True)",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def get_sqla_table(self) -> TableClause:",
            "        tbl = table(self.table_name)",
            "        if self.schema:",
            "            tbl.schema = self.schema",
            "        return tbl",
            "",
            "    def get_from_clause(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> tuple[TableClause | Alias, str | None]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "        if not self.is_virtual:",
            "            return self.get_sqla_table(), None",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def get_rendered_sql(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        if len(sqlparse.split(sql)) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "        return sql",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            table_column: TableColumn | None = columns_by_name.get(column_name)",
            "            if table_column:",
            "                sqla_column = table_column.get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "            else:",
            "                sqla_column = column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = _process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    def adhoc_column_to_sqla(  # pylint: disable=too-many-locals",
            "        self,",
            "        col: AdhocColumn,",
            "        force_type_check: bool = False,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc column into a sqlalchemy column.",
            "",
            "        :param col: Adhoc column definition",
            "        :param force_type_check: Should the column type be checked in the db.",
            "               This is needed to validate if a filter with an adhoc column",
            "               is applicable.",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        label = utils.get_column_name(col)",
            "        expression = _process_sql_expression(",
            "            expression=col[\"sqlExpression\"],",
            "            database_id=self.database_id,",
            "            schema=self.schema,",
            "            template_processor=template_processor,",
            "        )",
            "        time_grain = col.get(\"timeGrain\")",
            "        has_timegrain = col.get(\"columnType\") == \"BASE_AXIS\" and time_grain",
            "        is_dttm = False",
            "        if col_in_metadata := self.get_column(expression):",
            "            sqla_column = col_in_metadata.get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "            is_dttm = col_in_metadata.is_temporal",
            "        else:",
            "            sqla_column = literal_column(expression)",
            "            if has_timegrain or force_type_check:",
            "                try:",
            "                    # probe adhoc column type",
            "                    tbl, _ = self.get_from_clause(template_processor)",
            "                    qry = sa.select([sqla_column]).limit(1).select_from(tbl)",
            "                    sql = self.database.compile_sqla_query(qry)",
            "                    col_desc = get_columns_description(self.database, sql)",
            "                    is_dttm = col_desc[0][\"is_dttm\"]  # type: ignore",
            "                except SupersetGenericDBErrorException as ex:",
            "                    raise ColumnNotFoundException(message=str(ex)) from ex",
            "",
            "        if is_dttm and has_timegrain:",
            "            sqla_column = self.db_engine_spec.get_timestamp_expr(",
            "                col=sqla_column,",
            "                pdf=None,",
            "                time_grain=time_grain,",
            "            )",
            "        return self.make_sqla_column_compatible(sqla_column, label)",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[int | str, list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, SqlMetric],",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> str | int | float | bool | Text:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if column_.type and column_.is_temporal and isinstance(value, str):",
            "            sql = self.db_engine_spec.convert_dttm(",
            "                column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "            )",
            "",
            "            if sql:",
            "                value = self.text(sql)",
            "",
            "        return value",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> pd.DataFrame | None:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_sqla_table_object(self) -> Table:",
            "        return self.database.get_table(self.table_name, schema=self.schema)",
            "",
            "    def fetch_metadata(self, commit: bool = True) -> MetadataResult:",
            "        \"\"\"",
            "        Fetches the metadata for the table and merges it in",
            "",
            "        :param commit: should the changes be committed or not.",
            "        :return: Tuple with lists of added, removed and modified column names.",
            "        \"\"\"",
            "        new_columns = self.external_metadata()",
            "        metrics = [",
            "            SqlMetric(**metric)",
            "            for metric in self.database.get_metrics(self.table_name, self.schema)",
            "        ]",
            "        any_date_col = None",
            "        db_engine_spec = self.db_engine_spec",
            "",
            "        # If no `self.id`, then this is a new table, no need to fetch columns",
            "        # from db.  Passing in `self.id` to query will actually automatically",
            "        # generate a new id, which can be tricky during certain transactions.",
            "        old_columns = (",
            "            (",
            "                db.session.query(TableColumn)",
            "                .filter(TableColumn.table_id == self.id)",
            "                .all()",
            "            )",
            "            if self.id",
            "            else self.columns",
            "        )",
            "",
            "        old_columns_by_name: dict[str, TableColumn] = {",
            "            col.column_name: col for col in old_columns",
            "        }",
            "        results = MetadataResult(",
            "            removed=[",
            "                col",
            "                for col in old_columns_by_name",
            "                if col not in {col[\"column_name\"] for col in new_columns}",
            "            ]",
            "        )",
            "",
            "        # clear old columns before adding modified columns back",
            "        columns = []",
            "        for col in new_columns:",
            "            old_column = old_columns_by_name.pop(col[\"column_name\"], None)",
            "            if not old_column:",
            "                results.added.append(col[\"column_name\"])",
            "                new_column = TableColumn(",
            "                    column_name=col[\"column_name\"],",
            "                    type=col[\"type\"],",
            "                    table=self,",
            "                )",
            "                new_column.is_dttm = new_column.is_temporal",
            "                db_engine_spec.alter_new_orm_column(new_column)",
            "            else:",
            "                new_column = old_column",
            "                if new_column.type != col[\"type\"]:",
            "                    results.modified.append(col[\"column_name\"])",
            "                new_column.type = col[\"type\"]",
            "                new_column.expression = \"\"",
            "            new_column.groupby = True",
            "            new_column.filterable = True",
            "            columns.append(new_column)",
            "            if not any_date_col and new_column.is_temporal:",
            "                any_date_col = col[\"column_name\"]",
            "",
            "        # add back calculated (virtual) columns",
            "        columns.extend([col for col in old_columns if col.expression])",
            "        self.columns = columns",
            "",
            "        if not self.main_dttm_col:",
            "            self.main_dttm_col = any_date_col",
            "        self.add_missing_metrics(metrics)",
            "",
            "        # Apply config supplied mutations.",
            "        config[\"SQLA_TABLE_MUTATOR\"](self)",
            "",
            "        db.session.merge(self)",
            "        if commit:",
            "            db.session.commit()",
            "        return results",
            "",
            "    @classmethod",
            "    def query_datasources_by_name(",
            "        cls,",
            "        session: Session,",
            "        database: Database,",
            "        datasource_name: str,",
            "        schema: str | None = None,",
            "    ) -> list[SqlaTable]:",
            "        query = (",
            "            session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter_by(table_name=datasource_name)",
            "        )",
            "        if schema:",
            "            query = query.filter_by(schema=schema)",
            "        return query.all()",
            "",
            "    @classmethod",
            "    def query_datasources_by_permissions(  # pylint: disable=invalid-name",
            "        cls,",
            "        session: Session,",
            "        database: Database,",
            "        permissions: set[str],",
            "        schema_perms: set[str],",
            "    ) -> list[SqlaTable]:",
            "        # TODO(hughhhh): add unit test",
            "        return (",
            "            session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter(",
            "                or_(",
            "                    SqlaTable.perm.in_(permissions),",
            "                    SqlaTable.schema_perm.in_(schema_perms),",
            "                )",
            "            )",
            "            .all()",
            "        )",
            "",
            "    @classmethod",
            "    def get_eager_sqlatable_datasource(",
            "        cls, session: Session, datasource_id: int",
            "    ) -> SqlaTable:",
            "        \"\"\"Returns SqlaTable with columns and metrics.\"\"\"",
            "        return (",
            "            session.query(cls)",
            "            .options(",
            "                sa.orm.subqueryload(cls.columns),",
            "                sa.orm.subqueryload(cls.metrics),",
            "            )",
            "            .filter_by(id=datasource_id)",
            "            .one()",
            "        )",
            "",
            "    @classmethod",
            "    def get_all_datasources(cls, session: Session) -> list[SqlaTable]:",
            "        qry = session.query(cls)",
            "        qry = cls.default_query(qry)",
            "        return qry.all()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry.filter_by(is_sqllab_view=False)",
            "",
            "    def has_extra_cache_key_calls(self, query_obj: QueryObjectDict) -> bool:",
            "        \"\"\"",
            "        Detects the presence of calls to `ExtraCache` methods in items in query_obj that",
            "        can be templated. If any are present, the query must be evaluated to extract",
            "        additional keys for the cache key. This method is needed to avoid executing the",
            "        template code unnecessarily, as it may contain expensive calls, e.g. to extract",
            "        the latest partition of a database.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: True if there are call(s) to an `ExtraCache` method, False otherwise",
            "        \"\"\"",
            "        templatable_statements: list[str] = []",
            "        if self.sql:",
            "            templatable_statements.append(self.sql)",
            "        if self.fetch_values_predicate:",
            "            templatable_statements.append(self.fetch_values_predicate)",
            "        extras = query_obj.get(\"extras\", {})",
            "        if \"where\" in extras:",
            "            templatable_statements.append(extras[\"where\"])",
            "        if \"having\" in extras:",
            "            templatable_statements.append(extras[\"having\"])",
            "        if self.is_rls_supported:",
            "            templatable_statements += [",
            "                f.clause for f in security_manager.get_rls_filters(self)",
            "            ]",
            "        for statement in templatable_statements:",
            "            if ExtraCache.regex.search(statement):",
            "                return True",
            "        return False",
            "",
            "    def get_extra_cache_keys(self, query_obj: QueryObjectDict) -> list[Hashable]:",
            "        \"\"\"",
            "        The cache key of a SqlaTable needs to consider any keys added by the parent",
            "        class and any keys added via `ExtraCache`.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: The extra cache keys",
            "        \"\"\"",
            "        extra_cache_keys = super().get_extra_cache_keys(query_obj)",
            "        if self.has_extra_cache_key_calls(query_obj):",
            "            sqla_query = self.get_sqla_query(**query_obj)",
            "            extra_cache_keys += sqla_query.extra_cache_keys",
            "        return extra_cache_keys",
            "",
            "    @property",
            "    def quote_identifier(self) -> Callable[[str], str]:",
            "        return self.database.quote_identifier",
            "",
            "    @staticmethod",
            "    def before_update(",
            "        mapper: Mapper,  # pylint: disable=unused-argument",
            "        connection: Connection,  # pylint: disable=unused-argument",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Check before update if the target table already exists.",
            "",
            "        Note this listener is called when any fields are being updated and thus it is",
            "        necessary to first check whether the reference table is being updated.",
            "",
            "        Note this logic is temporary, given uniqueness is handled via the dataset DAO,",
            "        but is necessary until both the legacy datasource editor and datasource/save",
            "        endpoints are deprecated.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        :raises Exception: If the target table is not unique",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.daos.dataset import DatasetDAO",
            "        from superset.datasets.commands.exceptions import get_dataset_exist_error_msg",
            "",
            "        # Check whether the relevant attributes have changed.",
            "        state = db.inspect(target)  # pylint: disable=no-member",
            "",
            "        for attr in [\"database_id\", \"schema\", \"table_name\"]:",
            "            history = state.get_history(attr, True)",
            "            if history.has_changes():",
            "                break",
            "        else:",
            "            return None",
            "",
            "        if not DatasetDAO.validate_uniqueness(",
            "            target.database_id, target.schema, target.table_name, target.id",
            "        ):",
            "            raise Exception(get_dataset_exist_error_msg(target.full_name))",
            "",
            "    @staticmethod",
            "    def update_column(  # pylint: disable=unused-argument",
            "        mapper: Mapper, connection: Connection, target: SqlMetric | TableColumn",
            "    ) -> None:",
            "        \"\"\"",
            "        :param mapper: Unused.",
            "        :param connection: Unused.",
            "        :param target: The metric or column that was updated.",
            "        \"\"\"",
            "        inspector = inspect(target)",
            "        session = inspector.session",
            "",
            "        # Forces an update to the table's changed_on value when a metric or column on the",
            "        # table is updated. This busts the cache key for all charts that use the table.",
            "        session.execute(update(SqlaTable).where(SqlaTable.id == target.table.id))",
            "",
            "        # TODO: This shadow writing is deprecated",
            "        # if table itself has changed, shadow-writing will happen in `after_update` anyway",
            "        if target.table not in session.dirty:",
            "            dataset: NewDataset = (",
            "                session.query(NewDataset)",
            "                .filter_by(uuid=target.table.uuid)",
            "                .one_or_none()",
            "            )",
            "            # Update shadow dataset and columns",
            "            # did we find the dataset?",
            "            if not dataset:",
            "                # if dataset is not found create a new copy",
            "                target.table.write_shadow_dataset()",
            "                return",
            "",
            "    @staticmethod",
            "    def after_insert(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        sqla_table: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after insert",
            "        \"\"\"",
            "        security_manager.dataset_after_insert(mapper, connection, sqla_table)",
            "",
            "        # TODO: deprecated",
            "        sqla_table.write_shadow_dataset()",
            "",
            "    @staticmethod",
            "    def after_delete(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        sqla_table: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after delete",
            "        \"\"\"",
            "        security_manager.dataset_after_delete(mapper, connection, sqla_table)",
            "",
            "    @staticmethod",
            "    def after_update(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        sqla_table: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions",
            "        \"\"\"",
            "        # set permissions",
            "        security_manager.dataset_after_update(mapper, connection, sqla_table)",
            "",
            "        # TODO: the shadow writing is deprecated",
            "        inspector = inspect(sqla_table)",
            "        session = inspector.session",
            "",
            "        # double-check that ``UPDATE``s are actually pending (this method is called even",
            "        # for instances that have no net changes to their column-based attributes)",
            "        if not session.is_modified(sqla_table, include_collections=True):",
            "            return",
            "",
            "        # find the dataset from the known instance list first",
            "        # (it could be either from a previous query or newly created)",
            "        dataset = next(",
            "            find_cached_objects_in_session(",
            "                session, NewDataset, uuids=[sqla_table.uuid]",
            "            ),",
            "            None,",
            "        )",
            "        # if not found, pull from database",
            "        if not dataset:",
            "            dataset = (",
            "                session.query(NewDataset).filter_by(uuid=sqla_table.uuid).one_or_none()",
            "            )",
            "        if not dataset:",
            "            sqla_table.write_shadow_dataset()",
            "            return",
            "",
            "    def write_shadow_dataset(",
            "        self: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        This method is deprecated",
            "        \"\"\"",
            "        session = inspect(self).session",
            "        # most of the write_shadow_dataset functionality has been removed",
            "        # but leaving this portion in",
            "        # to remove later because it is adding a Database relationship to the session",
            "        # and there is some functionality that depends on this",
            "        if self.database_id and (",
            "            not self.database or self.database.id != self.database_id",
            "        ):",
            "            self.database = session.query(Database).filter_by(id=self.database_id).one()",
            "",
            "",
            "sa.event.listen(SqlaTable, \"before_update\", SqlaTable.before_update)",
            "sa.event.listen(SqlaTable, \"after_update\", SqlaTable.after_update)",
            "sa.event.listen(SqlaTable, \"after_insert\", SqlaTable.after_insert)",
            "sa.event.listen(SqlaTable, \"after_delete\", SqlaTable.after_delete)",
            "sa.event.listen(SqlMetric, \"after_update\", SqlaTable.update_column)",
            "sa.event.listen(TableColumn, \"after_update\", SqlaTable.update_column)",
            "",
            "RLSFilterRoles = Table(",
            "    \"rls_filter_roles\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"role_id\", Integer, ForeignKey(\"ab_role.id\"), nullable=False),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "RLSFilterTables = Table(",
            "    \"rls_filter_tables\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\")),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "",
            "class RowLevelSecurityFilter(Model, AuditMixinNullable):",
            "    \"\"\"",
            "    Custom where clauses attached to Tables and Roles.",
            "    \"\"\"",
            "",
            "    __tablename__ = \"row_level_security_filters\"",
            "    id = Column(Integer, primary_key=True)",
            "    name = Column(String(255), unique=True, nullable=False)",
            "    description = Column(Text)",
            "    filter_type = Column(",
            "        Enum(*[filter_type.value for filter_type in utils.RowLevelSecurityFilterType])",
            "    )",
            "    group_key = Column(String(255), nullable=True)",
            "    roles = relationship(",
            "        security_manager.role_model,",
            "        secondary=RLSFilterRoles,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    tables = relationship(",
            "        SqlaTable,",
            "        overlaps=\"table\",",
            "        secondary=RLSFilterTables,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    clause = Column(Text, nullable=False)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from dataclasses import dataclass, field",
            "from datetime import datetime, timedelta",
            "from typing import Any, Callable, cast",
            "",
            "import dateutil.parser",
            "import numpy as np",
            "import pandas as pd",
            "import sqlalchemy as sa",
            "import sqlparse",
            "from flask import escape, Markup",
            "from flask_appbuilder import Model",
            "from flask_babel import lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import (",
            "    and_,",
            "    Boolean,",
            "    Column,",
            "    DateTime,",
            "    Enum,",
            "    ForeignKey,",
            "    inspect,",
            "    Integer,",
            "    or_,",
            "    select,",
            "    String,",
            "    Table,",
            "    Text,",
            "    update,",
            ")",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.ext.hybrid import hybrid_property",
            "from sqlalchemy.orm import (",
            "    backref,",
            "    Mapped,",
            "    Query,",
            "    reconstructor,",
            "    relationship,",
            "    RelationshipProperty,",
            "    Session,",
            ")",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.schema import UniqueConstraint",
            "from sqlalchemy.sql import column, ColumnElement, literal_column, table",
            "from sqlalchemy.sql.elements import ColumnClause, TextClause",
            "from sqlalchemy.sql.expression import Label, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "",
            "from superset import app, db, is_feature_enabled, security_manager",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.connectors.base.models import BaseColumn, BaseDatasource, BaseMetric",
            "from superset.connectors.sqla.utils import (",
            "    get_columns_description,",
            "    get_physical_table_metadata,",
            "    get_virtual_table_metadata,",
            ")",
            "from superset.db_engine_specs.base import BaseEngineSpec, TimestampExpression",
            "from superset.exceptions import (",
            "    ColumnNotFoundException,",
            "    DatasetInvalidPermissionEvaluationException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetGenericDBErrorException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.jinja_context import (",
            "    BaseTemplateProcessor,",
            "    ExtraCache,",
            "    get_template_processor,",
            ")",
            "from superset.models.annotations import Annotation",
            "from superset.models.core import Database",
            "from superset.models.helpers import (",
            "    AuditMixinNullable,",
            "    CertificationMixin,",
            "    ExploreMixin,",
            "    QueryResult,",
            "    QueryStringExtended,",
            "    validate_adhoc_subquery,",
            ")",
            "from superset.sql_parse import ParsedQuery, sanitize_clause",
            "from superset.superset_typing import (",
            "    AdhocColumn,",
            "    AdhocMetric,",
            "    Metric,",
            "    QueryObjectDict,",
            "    ResultSetColumnType,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.core import GenericDataType, MediumText",
            "",
            "config = app.config",
            "metadata = Model.metadata  # pylint: disable=no-member",
            "logger = logging.getLogger(__name__)",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "",
            "# a non-exhaustive set of additive metrics",
            "ADDITIVE_METRIC_TYPES = {",
            "    \"count\",",
            "    \"sum\",",
            "    \"doubleSum\",",
            "}",
            "ADDITIVE_METRIC_TYPES_LOWER = {op.lower() for op in ADDITIVE_METRIC_TYPES}",
            "",
            "",
            "@dataclass",
            "class MetadataResult:",
            "    added: list[str] = field(default_factory=list)",
            "    removed: list[str] = field(default_factory=list)",
            "    modified: list[str] = field(default_factory=list)",
            "",
            "",
            "class AnnotationDatasource(BaseDatasource):",
            "    \"\"\"Dummy object so we can query annotations using 'Viz' objects just like",
            "    regular datasources.",
            "    \"\"\"",
            "",
            "    cache_timeout = 0",
            "    changed_on = None",
            "    type = \"annotation\"",
            "    column_names = [",
            "        \"created_on\",",
            "        \"changed_on\",",
            "        \"id\",",
            "        \"start_dttm\",",
            "        \"end_dttm\",",
            "        \"layer_id\",",
            "        \"short_descr\",",
            "        \"long_descr\",",
            "        \"json_metadata\",",
            "        \"created_by_fk\",",
            "        \"changed_by_fk\",",
            "    ]",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        error_message = None",
            "        qry = db.session.query(Annotation)",
            "        qry = qry.filter(Annotation.layer_id == query_obj[\"filter\"][0][\"val\"])",
            "        if query_obj[\"from_dttm\"]:",
            "            qry = qry.filter(Annotation.start_dttm >= query_obj[\"from_dttm\"])",
            "        if query_obj[\"to_dttm\"]:",
            "            qry = qry.filter(Annotation.end_dttm <= query_obj[\"to_dttm\"])",
            "        status = QueryStatus.SUCCESS",
            "        try:",
            "            df = pd.read_sql_query(qry.statement, db.engine)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.exception(ex)",
            "            error_message = utils.error_msg_from_exception(ex)",
            "        return QueryResult(",
            "            status=status,",
            "            df=df,",
            "            duration=timedelta(0),",
            "            query=\"\",",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        raise NotImplementedError()",
            "",
            "    def values_for_column(self, column_name: str, limit: int = 10000) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "",
            "class TableColumn(Model, BaseColumn, CertificationMixin):",
            "",
            "    \"\"\"ORM object for table columns, each table can have multiple columns\"\"\"",
            "",
            "    __tablename__ = \"table_columns\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"column_name\"),)",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"columns\",",
            "    )",
            "    is_dttm = Column(Boolean, default=False)",
            "    expression = Column(MediumText())",
            "    python_date_format = Column(String(255))",
            "    extra = Column(Text)",
            "",
            "    export_fields = [",
            "        \"table_id\",",
            "        \"column_name\",",
            "        \"verbose_name\",",
            "        \"is_dttm\",",
            "        \"is_active\",",
            "        \"type\",",
            "        \"advanced_data_type\",",
            "        \"groupby\",",
            "        \"filterable\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"python_date_format\",",
            "        \"extra\",",
            "    ]",
            "",
            "    update_from_object_fields = [s for s in export_fields if s not in (\"table_id\",)]",
            "    export_parent = \"table\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object.",
            "",
            "        Historically a TableColumn object (from an ORM perspective) was tighly bound to",
            "        a SqlaTable object, however with the introduction of the Query datasource this",
            "        is no longer true, i.e., the SqlaTable relationship is optional.",
            "",
            "        Now the TableColumn is either directly associated with the Database object (",
            "        which is unknown to the ORM) or indirectly via the SqlaTable object (courtesy of",
            "        the ORM) depending on the context.",
            "        \"\"\"",
            "",
            "        self._database: Database | None = kwargs.pop(\"database\", None)",
            "        super().__init__(**kwargs)",
            "",
            "    @reconstructor",
            "    def init_on_load(self) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object when invoked via the SQLAlchemy ORM.",
            "        \"\"\"",
            "",
            "        self._database = None",
            "",
            "    @property",
            "    def is_boolean(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a boolean datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.BOOLEAN",
            "",
            "    @property",
            "    def is_numeric(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a numeric datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.NUMERIC",
            "",
            "    @property",
            "    def is_string(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a string datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.STRING",
            "",
            "    @property",
            "    def is_temporal(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a temporal datatype. If column has been set as",
            "        temporal/non-temporal (`is_dttm` is True or False respectively), return that",
            "        value. This usually happens during initial metadata fetching or when a column",
            "        is manually set as temporal (for this `python_date_format` needs to be set).",
            "        \"\"\"",
            "        if self.is_dttm is not None:",
            "            return self.is_dttm",
            "        return self.type_generic == GenericDataType.TEMPORAL",
            "",
            "    @property",
            "    def database(self) -> Database:",
            "        return self.table.database if self.table else self._database",
            "",
            "    @property",
            "    def db_engine_spec(self) -> type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @property",
            "    def type_generic(self) -> utils.GenericDataType | None:",
            "        if self.is_dttm:",
            "            return GenericDataType.TEMPORAL",
            "",
            "        return (",
            "            column_spec.generic_type  # pylint: disable=used-before-assignment",
            "            if (",
            "                column_spec := self.db_engine_spec.get_column_spec(",
            "                    self.type,",
            "                    db_extra=self.db_extra,",
            "                )",
            "            )",
            "            else None",
            "        )",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        col = self.database.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    @property",
            "    def datasource(self) -> RelationshipProperty:",
            "        return self.table",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        time_grain: str | None,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TimestampExpression | Label:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "",
            "        pdf = self.python_date_format",
            "        is_epoch = pdf in (\"epoch_s\", \"epoch_ms\")",
            "        column_spec = self.db_engine_spec.get_column_spec(",
            "            self.type, db_extra=self.db_extra",
            "        )",
            "        type_ = column_spec.sqla_type if column_spec else DateTime",
            "        if not self.expression and not time_grain and not is_epoch:",
            "            sqla_col = column(self.column_name, type_=type_)",
            "            return self.database.make_sqla_column_compatible(sqla_col, label)",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, pdf, time_grain)",
            "        return self.database.make_sqla_column_compatible(time_expr, label)",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"id\",",
            "            \"column_name\",",
            "            \"verbose_name\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"filterable\",",
            "            \"groupby\",",
            "            \"is_dttm\",",
            "            \"type\",",
            "            \"type_generic\",",
            "            \"advanced_data_type\",",
            "            \"python_date_format\",",
            "            \"is_certified\",",
            "            \"certified_by\",",
            "            \"certification_details\",",
            "            \"warning_markdown\",",
            "        )",
            "",
            "        attr_dict = {s: getattr(self, s) for s in attrs if hasattr(self, s)}",
            "",
            "        attr_dict.update(super().data)",
            "",
            "        return attr_dict",
            "",
            "",
            "class SqlMetric(Model, BaseMetric, CertificationMixin):",
            "",
            "    \"\"\"ORM object for metrics, each table can have multiple metrics\"\"\"",
            "",
            "    __tablename__ = \"sql_metrics\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"metric_name\"),)",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"metrics\",",
            "    )",
            "    expression = Column(MediumText(), nullable=False)",
            "    extra = Column(Text)",
            "",
            "    export_fields = [",
            "        \"metric_name\",",
            "        \"verbose_name\",",
            "        \"metric_type\",",
            "        \"table_id\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"d3format\",",
            "        \"currency\",",
            "        \"extra\",",
            "        \"warning_text\",",
            "    ]",
            "    update_from_object_fields = list(s for s in export_fields if s != \"table_id\")",
            "    export_parent = \"table\"",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.metric_name)",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.metric_name",
            "        expression = self.expression",
            "        if template_processor:",
            "            expression = template_processor.process_template(expression)",
            "",
            "        sqla_col: ColumnClause = literal_column(expression)",
            "        return self.table.database.make_sqla_column_compatible(sqla_col, label)",
            "",
            "    @property",
            "    def perm(self) -> str | None:",
            "        return (",
            "            (\"{parent_name}.[{obj.metric_name}](id:{obj.id})\").format(",
            "                obj=self, parent_name=self.table.full_name",
            "            )",
            "            if self.table",
            "            else None",
            "        )",
            "",
            "    def get_perm(self) -> str | None:",
            "        return self.perm",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"is_certified\",",
            "            \"certified_by\",",
            "            \"certification_details\",",
            "            \"warning_markdown\",",
            "        )",
            "        attr_dict = {s: getattr(self, s) for s in attrs}",
            "",
            "        attr_dict.update(super().data)",
            "        return attr_dict",
            "",
            "",
            "sqlatable_user = Table(",
            "    \"sqlatable_user\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"user_id\", Integer, ForeignKey(\"ab_user.id\", ondelete=\"CASCADE\")),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\")),",
            ")",
            "",
            "",
            "def _process_sql_expression(",
            "    expression: str | None,",
            "    database_id: int,",
            "    schema: str,",
            "    template_processor: BaseTemplateProcessor | None = None,",
            ") -> str | None:",
            "    if template_processor and expression:",
            "        expression = template_processor.process_template(expression)",
            "    if expression:",
            "        try:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            expression = sanitize_clause(expression)",
            "        except (QueryClauseValidationException, SupersetSecurityException) as ex:",
            "            raise QueryObjectValidationError(ex.message) from ex",
            "    return expression",
            "",
            "",
            "class SqlaTable(",
            "    Model, BaseDatasource, ExploreMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"An ORM object for SqlAlchemy table references\"\"\"",
            "",
            "    type = \"table\"",
            "    query_language = \"sql\"",
            "    is_rls_supported = True",
            "    columns: Mapped[list[TableColumn]] = relationship(",
            "        TableColumn,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metrics: Mapped[list[SqlMetric]] = relationship(",
            "        SqlMetric,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metric_class = SqlMetric",
            "    column_class = TableColumn",
            "    owner_class = security_manager.user_model",
            "",
            "    __tablename__ = \"tables\"",
            "",
            "    # Note this uniqueness constraint is not part of the physical schema, i.e., it does",
            "    # not exist in the migrations, but is required by `import_from_dict` to ensure the",
            "    # correct filters are applied in order to identify uniqueness.",
            "    #",
            "    # The reason it does not physically exist is MySQL, PostgreSQL, etc. have a",
            "    # different interpretation of uniqueness when it comes to NULL which is problematic",
            "    # given the schema is optional.",
            "    __table_args__ = (UniqueConstraint(\"database_id\", \"schema\", \"table_name\"),)",
            "",
            "    table_name = Column(String(250), nullable=False)",
            "    main_dttm_col = Column(String(250))",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=False)",
            "    fetch_values_predicate = Column(Text)",
            "    owners = relationship(owner_class, secondary=sqlatable_user, backref=\"tables\")",
            "    database: Database = relationship(",
            "        \"Database\",",
            "        backref=backref(\"tables\", cascade=\"all, delete-orphan\"),",
            "        foreign_keys=[database_id],",
            "    )",
            "    schema = Column(String(255))",
            "    sql = Column(MediumText())",
            "    is_sqllab_view = Column(Boolean, default=False)",
            "    template_params = Column(Text)",
            "    extra = Column(Text)",
            "    normalize_columns = Column(Boolean, default=False)",
            "",
            "    baselink = \"tablemodelview\"",
            "",
            "    export_fields = [",
            "        \"table_name\",",
            "        \"main_dttm_col\",",
            "        \"description\",",
            "        \"default_endpoint\",",
            "        \"database_id\",",
            "        \"offset\",",
            "        \"cache_timeout\",",
            "        \"schema\",",
            "        \"sql\",",
            "        \"params\",",
            "        \"template_params\",",
            "        \"filter_select_enabled\",",
            "        \"fetch_values_predicate\",",
            "        \"extra\",",
            "        \"normalize_columns\",",
            "    ]",
            "    update_from_object_fields = [f for f in export_fields if f != \"database_id\"]",
            "    export_parent = \"database\"",
            "    export_children = [\"metrics\", \"columns\"]",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "",
            "    def __repr__(self) -> str:  # pylint: disable=invalid-repr-returned",
            "        return self.name",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: str | None) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    @property",
            "    def db_engine_spec(self) -> __builtins__.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if not self.changed_by:",
            "            return \"\"",
            "        return str(self.changed_by)",
            "",
            "    @property",
            "    def connection(self) -> str:",
            "        return str(self.database)",
            "",
            "    @property",
            "    def description_markeddown(self) -> str:",
            "        return utils.markdown(self.description)",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        return self.table_name",
            "",
            "    @property",
            "    def datasource_type(self) -> str:",
            "        return self.type",
            "",
            "    @property",
            "    def database_name(self) -> str:",
            "        return self.database.name",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls,",
            "        session: Session,",
            "        datasource_name: str,",
            "        schema: str | None,",
            "        database_name: str,",
            "    ) -> SqlaTable | None:",
            "        schema = schema or None",
            "        query = (",
            "            session.query(cls)",
            "            .join(Database)",
            "            .filter(cls.table_name == datasource_name)",
            "            .filter(Database.database_name == database_name)",
            "        )",
            "        # Handling schema being '' or None, which is easier to handle",
            "        # in python than in the SQLA query in a multi-dialect way",
            "        for tbl in query.all():",
            "            if schema == (tbl.schema or None):",
            "                return tbl",
            "        return None",
            "",
            "    @property",
            "    def link(self) -> Markup:",
            "        name = escape(self.name)",
            "        anchor = f'<a target=\"_blank\" href=\"{self.explore_url}\">{name}</a>'",
            "        return Markup(anchor)",
            "",
            "    def get_schema_perm(self) -> str | None:",
            "        \"\"\"Returns schema permission if present, database one otherwise.\"\"\"",
            "        return security_manager.get_schema_perm(self.database, self.schema)",
            "",
            "    def get_perm(self) -> str:",
            "        \"\"\"",
            "        Return this dataset permission name",
            "        :return: dataset permission name",
            "        :raises DatasetInvalidPermissionEvaluationException: When database is missing",
            "        \"\"\"",
            "        if self.database is None:",
            "            raise DatasetInvalidPermissionEvaluationException()",
            "        return f\"[{self.database}].[{self.table_name}](id:{self.id})\"",
            "",
            "    @hybrid_property",
            "    def name(self) -> str:  # pylint: disable=invalid-overridden-method",
            "        return self.schema + \".\" + self.table_name if self.schema else self.table_name",
            "",
            "    @property",
            "    def full_name(self) -> str:",
            "        return utils.get_datasource_full_name(",
            "            self.database, self.table_name, schema=self.schema",
            "        )",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        l = [c.column_name for c in self.columns if c.is_dttm]",
            "        if self.main_dttm_col and self.main_dttm_col not in l:",
            "            l.append(self.main_dttm_col)",
            "        return l",
            "",
            "    @property",
            "    def num_cols(self) -> list[str]:",
            "        return [c.column_name for c in self.columns if c.is_numeric]",
            "",
            "    @property",
            "    def any_dttm_col(self) -> str | None:",
            "        cols = self.dttm_cols",
            "        return cols[0] if cols else None",
            "",
            "    @property",
            "    def html(self) -> str:",
            "        df = pd.DataFrame((c.column_name, c.type) for c in self.columns)",
            "        df.columns = [\"field\", \"type\"]",
            "        return df.to_html(",
            "            index=False,",
            "            classes=(\"dataframe table table-striped table-bordered \" \"table-condensed\"),",
            "        )",
            "",
            "    @property",
            "    def sql_url(self) -> str:",
            "        return self.database.sql_url + \"?table_name=\" + str(self.table_name)",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        # todo(yongjie): create a physical table column type in a separate PR",
            "        if self.sql:",
            "            return get_virtual_table_metadata(dataset=self)",
            "        return get_physical_table_metadata(",
            "            database=self.database,",
            "            table_name=self.table_name,",
            "            schema_name=self.schema,",
            "            normalize_columns=self.normalize_columns,",
            "        )",
            "",
            "    @property",
            "    def time_column_grains(self) -> dict[str, Any]:",
            "        return {",
            "            \"time_columns\": self.dttm_cols,",
            "            \"time_grains\": [grain.name for grain in self.database.grains()],",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        # show_cols and latest_partition set to false to avoid",
            "        # the expensive cost of inspecting the DB",
            "        return self.database.select_star(",
            "            self.table_name, schema=self.schema, show_cols=False, latest_partition=False",
            "        )",
            "",
            "    @property",
            "    def health_check_message(self) -> str | None:",
            "        check = config[\"DATASET_HEALTH_CHECK\"]",
            "        return check(self) if check else None",
            "",
            "    @property",
            "    def granularity_sqla(self) -> list[tuple[Any, Any]]:",
            "        return utils.choicify(self.dttm_cols)",
            "",
            "    @property",
            "    def time_grain_sqla(self) -> list[tuple[Any, Any]]:",
            "        return [(g.duration, g.name) for g in self.database.grains() or []]",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        data_ = super().data",
            "        if self.type == \"table\":",
            "            data_[\"granularity_sqla\"] = self.granularity_sqla",
            "            data_[\"time_grain_sqla\"] = self.time_grain_sqla",
            "            data_[\"main_dttm_col\"] = self.main_dttm_col",
            "            data_[\"fetch_values_predicate\"] = self.fetch_values_predicate",
            "            data_[\"template_params\"] = self.template_params",
            "            data_[\"is_sqllab_view\"] = self.is_sqllab_view",
            "            data_[\"health_check_message\"] = self.health_check_message",
            "            data_[\"extra\"] = self.extra",
            "            data_[\"owners\"] = self.owners_data",
            "        return data_",
            "",
            "    @property",
            "    def extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TextClause:",
            "        fetch_values_predicate = self.fetch_values_predicate",
            "        if template_processor:",
            "            fetch_values_predicate = template_processor.process_template(",
            "                fetch_values_predicate",
            "            )",
            "        try:",
            "            return self.text(fetch_values_predicate)",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in fetch values predicate: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def values_for_column(self, column_name: str, limit: int = 10000) -> list[Any]:",
            "        \"\"\"Runs query against sqla to retrieve some",
            "        sample values for the given column.",
            "        \"\"\"",
            "        cols = {col.column_name: col for col in self.columns}",
            "        target_col = cols[column_name]",
            "        tp = self.get_template_processor()",
            "        tbl, cte = self.get_from_clause(tp)",
            "",
            "        qry = (",
            "            select([target_col.get_sqla_col(template_processor=tp)])",
            "            .select_from(tbl)",
            "            .distinct()",
            "        )",
            "        if limit:",
            "            qry = qry.limit(limit)",
            "",
            "        if self.fetch_values_predicate:",
            "            qry = qry.where(self.get_fetch_values_predicate(template_processor=tp))",
            "",
            "        with self.database.get_sqla_engine_with_context() as engine:",
            "            sql = qry.compile(engine, compile_kwargs={\"literal_binds\": True})",
            "            sql = self._apply_cte(sql, cte)",
            "            sql = self.mutate_query_from_config(sql)",
            "",
            "            df = pd.read_sql_query(sql=sql, con=engine)",
            "            return df[column_name].to_list()",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        sql_query_mutator = config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        return get_template_processor(table=self, database=self.database, **kwargs)",
            "",
            "    def get_query_str_extended(",
            "        self,",
            "        query_obj: QueryObjectDict,",
            "        mutate: bool = True,",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        sql = sqlparse.format(sql, reindent=True)",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def get_sqla_table(self) -> TableClause:",
            "        tbl = table(self.table_name)",
            "        if self.schema:",
            "            tbl.schema = self.schema",
            "        return tbl",
            "",
            "    def get_from_clause(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> tuple[TableClause | Alias, str | None]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "        if not self.is_virtual:",
            "            return self.get_sqla_table(), None",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def get_rendered_sql(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        if len(sqlparse.split(sql)) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "        return sql",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            table_column: TableColumn | None = columns_by_name.get(column_name)",
            "            if table_column:",
            "                sqla_column = table_column.get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "            else:",
            "                sqla_column = column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = _process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    def adhoc_column_to_sqla(  # pylint: disable=too-many-locals",
            "        self,",
            "        col: AdhocColumn,",
            "        force_type_check: bool = False,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc column into a sqlalchemy column.",
            "",
            "        :param col: Adhoc column definition",
            "        :param force_type_check: Should the column type be checked in the db.",
            "               This is needed to validate if a filter with an adhoc column",
            "               is applicable.",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        label = utils.get_column_name(col)",
            "        expression = _process_sql_expression(",
            "            expression=col[\"sqlExpression\"],",
            "            database_id=self.database_id,",
            "            schema=self.schema,",
            "            template_processor=template_processor,",
            "        )",
            "        time_grain = col.get(\"timeGrain\")",
            "        has_timegrain = col.get(\"columnType\") == \"BASE_AXIS\" and time_grain",
            "        is_dttm = False",
            "        if col_in_metadata := self.get_column(expression):",
            "            sqla_column = col_in_metadata.get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "            is_dttm = col_in_metadata.is_temporal",
            "        else:",
            "            sqla_column = literal_column(expression)",
            "            if has_timegrain or force_type_check:",
            "                try:",
            "                    # probe adhoc column type",
            "                    tbl, _ = self.get_from_clause(template_processor)",
            "                    qry = sa.select([sqla_column]).limit(1).select_from(tbl)",
            "                    sql = self.database.compile_sqla_query(qry)",
            "                    col_desc = get_columns_description(self.database, sql)",
            "                    is_dttm = col_desc[0][\"is_dttm\"]  # type: ignore",
            "                except SupersetGenericDBErrorException as ex:",
            "                    raise ColumnNotFoundException(message=str(ex)) from ex",
            "",
            "        if is_dttm and has_timegrain:",
            "            sqla_column = self.db_engine_spec.get_timestamp_expr(",
            "                col=sqla_column,",
            "                pdf=None,",
            "                time_grain=time_grain,",
            "            )",
            "        return self.make_sqla_column_compatible(sqla_column, label)",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[int | str, list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, SqlMetric],",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> str | int | float | bool | Text:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if column_.type and column_.is_temporal and isinstance(value, str):",
            "            sql = self.db_engine_spec.convert_dttm(",
            "                column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "            )",
            "",
            "            if sql:",
            "                value = self.text(sql)",
            "",
            "        return value",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> pd.DataFrame | None:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_sqla_table_object(self) -> Table:",
            "        return self.database.get_table(self.table_name, schema=self.schema)",
            "",
            "    def fetch_metadata(self, commit: bool = True) -> MetadataResult:",
            "        \"\"\"",
            "        Fetches the metadata for the table and merges it in",
            "",
            "        :param commit: should the changes be committed or not.",
            "        :return: Tuple with lists of added, removed and modified column names.",
            "        \"\"\"",
            "        new_columns = self.external_metadata()",
            "        metrics = [",
            "            SqlMetric(**metric)",
            "            for metric in self.database.get_metrics(self.table_name, self.schema)",
            "        ]",
            "        any_date_col = None",
            "        db_engine_spec = self.db_engine_spec",
            "",
            "        # If no `self.id`, then this is a new table, no need to fetch columns",
            "        # from db.  Passing in `self.id` to query will actually automatically",
            "        # generate a new id, which can be tricky during certain transactions.",
            "        old_columns = (",
            "            (",
            "                db.session.query(TableColumn)",
            "                .filter(TableColumn.table_id == self.id)",
            "                .all()",
            "            )",
            "            if self.id",
            "            else self.columns",
            "        )",
            "",
            "        old_columns_by_name: dict[str, TableColumn] = {",
            "            col.column_name: col for col in old_columns",
            "        }",
            "        results = MetadataResult(",
            "            removed=[",
            "                col",
            "                for col in old_columns_by_name",
            "                if col not in {col[\"column_name\"] for col in new_columns}",
            "            ]",
            "        )",
            "",
            "        # clear old columns before adding modified columns back",
            "        columns = []",
            "        for col in new_columns:",
            "            old_column = old_columns_by_name.pop(col[\"column_name\"], None)",
            "            if not old_column:",
            "                results.added.append(col[\"column_name\"])",
            "                new_column = TableColumn(",
            "                    column_name=col[\"column_name\"],",
            "                    type=col[\"type\"],",
            "                    table=self,",
            "                )",
            "                new_column.is_dttm = new_column.is_temporal",
            "                db_engine_spec.alter_new_orm_column(new_column)",
            "            else:",
            "                new_column = old_column",
            "                if new_column.type != col[\"type\"]:",
            "                    results.modified.append(col[\"column_name\"])",
            "                new_column.type = col[\"type\"]",
            "                new_column.expression = \"\"",
            "            new_column.groupby = True",
            "            new_column.filterable = True",
            "            columns.append(new_column)",
            "            if not any_date_col and new_column.is_temporal:",
            "                any_date_col = col[\"column_name\"]",
            "",
            "        # add back calculated (virtual) columns",
            "        columns.extend([col for col in old_columns if col.expression])",
            "        self.columns = columns",
            "",
            "        if not self.main_dttm_col:",
            "            self.main_dttm_col = any_date_col",
            "        self.add_missing_metrics(metrics)",
            "",
            "        # Apply config supplied mutations.",
            "        config[\"SQLA_TABLE_MUTATOR\"](self)",
            "",
            "        db.session.merge(self)",
            "        if commit:",
            "            db.session.commit()",
            "        return results",
            "",
            "    @classmethod",
            "    def query_datasources_by_name(",
            "        cls,",
            "        session: Session,",
            "        database: Database,",
            "        datasource_name: str,",
            "        schema: str | None = None,",
            "    ) -> list[SqlaTable]:",
            "        query = (",
            "            session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter_by(table_name=datasource_name)",
            "        )",
            "        if schema:",
            "            query = query.filter_by(schema=schema)",
            "        return query.all()",
            "",
            "    @classmethod",
            "    def query_datasources_by_permissions(  # pylint: disable=invalid-name",
            "        cls,",
            "        session: Session,",
            "        database: Database,",
            "        permissions: set[str],",
            "        schema_perms: set[str],",
            "    ) -> list[SqlaTable]:",
            "        # TODO(hughhhh): add unit test",
            "        return (",
            "            session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter(",
            "                or_(",
            "                    SqlaTable.perm.in_(permissions),",
            "                    SqlaTable.schema_perm.in_(schema_perms),",
            "                )",
            "            )",
            "            .all()",
            "        )",
            "",
            "    @classmethod",
            "    def get_eager_sqlatable_datasource(",
            "        cls, session: Session, datasource_id: int",
            "    ) -> SqlaTable:",
            "        \"\"\"Returns SqlaTable with columns and metrics.\"\"\"",
            "        return (",
            "            session.query(cls)",
            "            .options(",
            "                sa.orm.subqueryload(cls.columns),",
            "                sa.orm.subqueryload(cls.metrics),",
            "            )",
            "            .filter_by(id=datasource_id)",
            "            .one()",
            "        )",
            "",
            "    @classmethod",
            "    def get_all_datasources(cls, session: Session) -> list[SqlaTable]:",
            "        qry = session.query(cls)",
            "        qry = cls.default_query(qry)",
            "        return qry.all()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry.filter_by(is_sqllab_view=False)",
            "",
            "    def has_extra_cache_key_calls(self, query_obj: QueryObjectDict) -> bool:",
            "        \"\"\"",
            "        Detects the presence of calls to `ExtraCache` methods in items in query_obj that",
            "        can be templated. If any are present, the query must be evaluated to extract",
            "        additional keys for the cache key. This method is needed to avoid executing the",
            "        template code unnecessarily, as it may contain expensive calls, e.g. to extract",
            "        the latest partition of a database.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: True if there are call(s) to an `ExtraCache` method, False otherwise",
            "        \"\"\"",
            "        templatable_statements: list[str] = []",
            "        if self.sql:",
            "            templatable_statements.append(self.sql)",
            "        if self.fetch_values_predicate:",
            "            templatable_statements.append(self.fetch_values_predicate)",
            "        extras = query_obj.get(\"extras\", {})",
            "        if \"where\" in extras:",
            "            templatable_statements.append(extras[\"where\"])",
            "        if \"having\" in extras:",
            "            templatable_statements.append(extras[\"having\"])",
            "        if self.is_rls_supported:",
            "            templatable_statements += [",
            "                f.clause for f in security_manager.get_rls_filters(self)",
            "            ]",
            "        for statement in templatable_statements:",
            "            if ExtraCache.regex.search(statement):",
            "                return True",
            "        return False",
            "",
            "    def get_extra_cache_keys(self, query_obj: QueryObjectDict) -> list[Hashable]:",
            "        \"\"\"",
            "        The cache key of a SqlaTable needs to consider any keys added by the parent",
            "        class and any keys added via `ExtraCache`.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: The extra cache keys",
            "        \"\"\"",
            "        extra_cache_keys = super().get_extra_cache_keys(query_obj)",
            "        if self.has_extra_cache_key_calls(query_obj):",
            "            sqla_query = self.get_sqla_query(**query_obj)",
            "            extra_cache_keys += sqla_query.extra_cache_keys",
            "        return extra_cache_keys",
            "",
            "    @property",
            "    def quote_identifier(self) -> Callable[[str], str]:",
            "        return self.database.quote_identifier",
            "",
            "    @staticmethod",
            "    def before_update(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Note this listener is called when any fields are being updated",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        :raises Exception: If the target table is not unique",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_before_update(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def update_column(  # pylint: disable=unused-argument",
            "        mapper: Mapper, connection: Connection, target: SqlMetric | TableColumn",
            "    ) -> None:",
            "        \"\"\"",
            "        :param mapper: Unused.",
            "        :param connection: Unused.",
            "        :param target: The metric or column that was updated.",
            "        \"\"\"",
            "        inspector = inspect(target)",
            "        session = inspector.session",
            "",
            "        # Forces an update to the table's changed_on value when a metric or column on the",
            "        # table is updated. This busts the cache key for all charts that use the table.",
            "        session.execute(update(SqlaTable).where(SqlaTable.id == target.table.id))",
            "",
            "    @staticmethod",
            "    def after_insert(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after insert",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_after_insert(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def after_delete(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        sqla_table: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after delete",
            "        \"\"\"",
            "        security_manager.dataset_after_delete(mapper, connection, sqla_table)",
            "",
            "    def load_database(self: SqlaTable) -> None:",
            "        # somehow the database attribute is not loaded on access",
            "        if self.database_id and (",
            "            not self.database or self.database.id != self.database_id",
            "        ):",
            "            session = inspect(self).session",
            "            self.database = session.query(Database).filter_by(id=self.database_id).one()",
            "",
            "",
            "sa.event.listen(SqlaTable, \"before_update\", SqlaTable.before_update)",
            "sa.event.listen(SqlaTable, \"after_insert\", SqlaTable.after_insert)",
            "sa.event.listen(SqlaTable, \"after_delete\", SqlaTable.after_delete)",
            "sa.event.listen(SqlMetric, \"after_update\", SqlaTable.update_column)",
            "sa.event.listen(TableColumn, \"after_update\", SqlaTable.update_column)",
            "",
            "RLSFilterRoles = Table(",
            "    \"rls_filter_roles\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"role_id\", Integer, ForeignKey(\"ab_role.id\"), nullable=False),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "RLSFilterTables = Table(",
            "    \"rls_filter_tables\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\")),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "",
            "class RowLevelSecurityFilter(Model, AuditMixinNullable):",
            "    \"\"\"",
            "    Custom where clauses attached to Tables and Roles.",
            "    \"\"\"",
            "",
            "    __tablename__ = \"row_level_security_filters\"",
            "    id = Column(Integer, primary_key=True)",
            "    name = Column(String(255), unique=True, nullable=False)",
            "    description = Column(Text)",
            "    filter_type = Column(",
            "        Enum(*[filter_type.value for filter_type in utils.RowLevelSecurityFilterType])",
            "    )",
            "    group_key = Column(String(255), nullable=True)",
            "    roles = relationship(",
            "        security_manager.role_model,",
            "        secondary=RLSFilterRoles,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    tables = relationship(",
            "        SqlaTable,",
            "        overlaps=\"table\",",
            "        secondary=RLSFilterTables,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    clause = Column(Text, nullable=False)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "77": [],
            "82": [],
            "1433": [
                "SqlaTable",
                "before_update"
            ],
            "1434": [
                "SqlaTable",
                "before_update"
            ],
            "1438": [
                "SqlaTable",
                "before_update"
            ],
            "1439": [
                "SqlaTable",
                "before_update"
            ],
            "1440": [
                "SqlaTable",
                "before_update"
            ],
            "1441": [
                "SqlaTable",
                "before_update"
            ],
            "1442": [
                "SqlaTable",
                "before_update"
            ],
            "1443": [
                "SqlaTable",
                "before_update"
            ],
            "1444": [
                "SqlaTable",
                "before_update"
            ],
            "1445": [
                "SqlaTable",
                "before_update"
            ],
            "1452": [
                "SqlaTable",
                "before_update"
            ],
            "1453": [
                "SqlaTable",
                "before_update"
            ],
            "1454": [
                "SqlaTable",
                "before_update"
            ],
            "1455": [
                "SqlaTable",
                "before_update"
            ],
            "1456": [
                "SqlaTable",
                "before_update"
            ],
            "1457": [
                "SqlaTable",
                "before_update"
            ],
            "1458": [
                "SqlaTable",
                "before_update"
            ],
            "1459": [
                "SqlaTable",
                "before_update"
            ],
            "1460": [
                "SqlaTable",
                "before_update"
            ],
            "1461": [
                "SqlaTable",
                "before_update"
            ],
            "1462": [
                "SqlaTable",
                "before_update"
            ],
            "1463": [
                "SqlaTable",
                "before_update"
            ],
            "1464": [
                "SqlaTable",
                "before_update"
            ],
            "1465": [
                "SqlaTable",
                "before_update"
            ],
            "1466": [
                "SqlaTable",
                "before_update"
            ],
            "1467": [
                "SqlaTable",
                "before_update"
            ],
            "1468": [
                "SqlaTable",
                "before_update"
            ],
            "1469": [
                "SqlaTable",
                "before_update"
            ],
            "1470": [
                "SqlaTable",
                "before_update"
            ],
            "1488": [
                "SqlaTable",
                "update_column"
            ],
            "1489": [
                "SqlaTable",
                "update_column"
            ],
            "1490": [
                "SqlaTable",
                "update_column"
            ],
            "1491": [
                "SqlaTable",
                "update_column"
            ],
            "1492": [
                "SqlaTable",
                "update_column"
            ],
            "1493": [
                "SqlaTable",
                "update_column"
            ],
            "1494": [
                "SqlaTable",
                "update_column"
            ],
            "1495": [
                "SqlaTable",
                "update_column"
            ],
            "1496": [
                "SqlaTable",
                "update_column"
            ],
            "1497": [
                "SqlaTable",
                "update_column"
            ],
            "1498": [
                "SqlaTable",
                "update_column"
            ],
            "1499": [
                "SqlaTable",
                "update_column"
            ],
            "1500": [
                "SqlaTable",
                "update_column"
            ],
            "1501": [
                "SqlaTable",
                "update_column"
            ],
            "1502": [
                "SqlaTable"
            ],
            "1507": [
                "SqlaTable",
                "after_insert"
            ],
            "1512": [
                "SqlaTable",
                "after_insert"
            ],
            "1513": [
                "SqlaTable",
                "after_insert"
            ],
            "1514": [
                "SqlaTable",
                "after_insert"
            ],
            "1515": [
                "SqlaTable",
                "after_insert"
            ],
            "1528": [
                "SqlaTable"
            ],
            "1529": [
                "SqlaTable",
                "after_update"
            ],
            "1530": [
                "SqlaTable",
                "after_update"
            ],
            "1531": [
                "SqlaTable",
                "after_update"
            ],
            "1532": [
                "SqlaTable",
                "after_update"
            ],
            "1533": [
                "SqlaTable",
                "after_update"
            ],
            "1534": [
                "SqlaTable",
                "after_update"
            ],
            "1535": [
                "SqlaTable",
                "after_update"
            ],
            "1536": [
                "SqlaTable",
                "after_update"
            ],
            "1537": [
                "SqlaTable",
                "after_update"
            ],
            "1538": [
                "SqlaTable",
                "after_update"
            ],
            "1539": [
                "SqlaTable",
                "after_update"
            ],
            "1540": [
                "SqlaTable",
                "after_update"
            ],
            "1541": [
                "SqlaTable",
                "after_update"
            ],
            "1542": [
                "SqlaTable",
                "after_update"
            ],
            "1543": [
                "SqlaTable",
                "after_update"
            ],
            "1544": [
                "SqlaTable",
                "after_update"
            ],
            "1545": [
                "SqlaTable",
                "after_update"
            ],
            "1546": [
                "SqlaTable",
                "after_update"
            ],
            "1547": [
                "SqlaTable",
                "after_update"
            ],
            "1548": [
                "SqlaTable",
                "after_update"
            ],
            "1549": [
                "SqlaTable",
                "after_update"
            ],
            "1550": [
                "SqlaTable",
                "after_update"
            ],
            "1551": [
                "SqlaTable",
                "after_update"
            ],
            "1552": [
                "SqlaTable",
                "after_update"
            ],
            "1553": [
                "SqlaTable",
                "after_update"
            ],
            "1554": [
                "SqlaTable",
                "after_update"
            ],
            "1555": [
                "SqlaTable",
                "after_update"
            ],
            "1556": [
                "SqlaTable",
                "after_update"
            ],
            "1557": [
                "SqlaTable",
                "after_update"
            ],
            "1558": [
                "SqlaTable",
                "after_update"
            ],
            "1559": [
                "SqlaTable",
                "after_update"
            ],
            "1560": [
                "SqlaTable",
                "after_update"
            ],
            "1561": [
                "SqlaTable",
                "after_update"
            ],
            "1562": [
                "SqlaTable",
                "after_update"
            ],
            "1563": [
                "SqlaTable",
                "after_update"
            ],
            "1564": [
                "SqlaTable",
                "after_update"
            ],
            "1565": [
                "SqlaTable"
            ],
            "1566": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1567": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1568": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1569": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1570": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1571": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1572": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1573": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1574": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1575": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1576": [
                "SqlaTable",
                "write_shadow_dataset"
            ],
            "1584": []
        },
        "addLocation": []
    },
    "superset/security/manager.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1323,
                "afterPatchRowNumber": 1323,
                "PatchRowcode": "             mapper, connection, \"datasource_access\", dataset_vm_name"
            },
            "1": {
                "beforePatchRowNumber": 1324,
                "afterPatchRowNumber": 1324,
                "PatchRowcode": "         )"
            },
            "2": {
                "beforePatchRowNumber": 1325,
                "afterPatchRowNumber": 1325,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 1326,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def dataset_after_update("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1326,
                "PatchRowcode": "+    def dataset_before_update("
            },
            "5": {
                "beforePatchRowNumber": 1327,
                "afterPatchRowNumber": 1327,
                "PatchRowcode": "         self,"
            },
            "6": {
                "beforePatchRowNumber": 1328,
                "afterPatchRowNumber": 1328,
                "PatchRowcode": "         mapper: Mapper,"
            },
            "7": {
                "beforePatchRowNumber": 1329,
                "afterPatchRowNumber": 1329,
                "PatchRowcode": "         connection: Connection,"
            },
            "8": {
                "beforePatchRowNumber": 1343,
                "afterPatchRowNumber": 1343,
                "PatchRowcode": "         :param target: The changed dataset object"
            },
            "9": {
                "beforePatchRowNumber": 1344,
                "afterPatchRowNumber": 1344,
                "PatchRowcode": "         :return:"
            },
            "10": {
                "beforePatchRowNumber": 1345,
                "afterPatchRowNumber": 1345,
                "PatchRowcode": "         \"\"\""
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1346,
                "PatchRowcode": "+        # pylint: disable=import-outside-toplevel"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1347,
                "PatchRowcode": "+        from superset.connectors.sqla.models import SqlaTable"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1348,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": 1346,
                "afterPatchRowNumber": 1349,
                "PatchRowcode": "         # Check if watched fields have changed"
            },
            "15": {
                "beforePatchRowNumber": 1347,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        state = inspect(target)"
            },
            "16": {
                "beforePatchRowNumber": 1348,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        history_database = state.get_history(\"database_id\", True)"
            },
            "17": {
                "beforePatchRowNumber": 1349,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        history_table_name = state.get_history(\"table_name\", True)"
            },
            "18": {
                "beforePatchRowNumber": 1350,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        history_schema = state.get_history(\"schema\", True)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1350,
                "PatchRowcode": "+        table = SqlaTable.__table__"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1351,
                "PatchRowcode": "+        current_dataset = connection.execute("
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1352,
                "PatchRowcode": "+            table.select().where(table.c.id == target.id)"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1353,
                "PatchRowcode": "+        ).one()"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1354,
                "PatchRowcode": "+        current_db_id = current_dataset.database_id"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1355,
                "PatchRowcode": "+        current_schema = current_dataset.schema"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1356,
                "PatchRowcode": "+        current_table_name = current_dataset.table_name"
            },
            "26": {
                "beforePatchRowNumber": 1351,
                "afterPatchRowNumber": 1357,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": 1352,
                "afterPatchRowNumber": 1358,
                "PatchRowcode": "         # When database name changes"
            },
            "28": {
                "beforePatchRowNumber": 1353,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if history_database.has_changes() and history_database.deleted:"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1359,
                "PatchRowcode": "+        if current_db_id != target.database_id:"
            },
            "30": {
                "beforePatchRowNumber": 1354,
                "afterPatchRowNumber": 1360,
                "PatchRowcode": "             new_dataset_vm_name = self.get_dataset_perm("
            },
            "31": {
                "beforePatchRowNumber": 1355,
                "afterPatchRowNumber": 1361,
                "PatchRowcode": "                 target.id, target.table_name, target.database.database_name"
            },
            "32": {
                "beforePatchRowNumber": 1356,
                "afterPatchRowNumber": 1362,
                "PatchRowcode": "             )"
            },
            "33": {
                "beforePatchRowNumber": 1370,
                "afterPatchRowNumber": 1376,
                "PatchRowcode": "             )"
            },
            "34": {
                "beforePatchRowNumber": 1371,
                "afterPatchRowNumber": 1377,
                "PatchRowcode": " "
            },
            "35": {
                "beforePatchRowNumber": 1372,
                "afterPatchRowNumber": 1378,
                "PatchRowcode": "         # When table name changes"
            },
            "36": {
                "beforePatchRowNumber": 1373,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if history_table_name.has_changes() and history_table_name.deleted:"
            },
            "37": {
                "beforePatchRowNumber": 1374,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            old_dataset_name = history_table_name.deleted[0]"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1379,
                "PatchRowcode": "+        if current_table_name != target.table_name:"
            },
            "39": {
                "beforePatchRowNumber": 1375,
                "afterPatchRowNumber": 1380,
                "PatchRowcode": "             new_dataset_vm_name = self.get_dataset_perm("
            },
            "40": {
                "beforePatchRowNumber": 1376,
                "afterPatchRowNumber": 1381,
                "PatchRowcode": "                 target.id, target.table_name, target.database.database_name"
            },
            "41": {
                "beforePatchRowNumber": 1377,
                "afterPatchRowNumber": 1382,
                "PatchRowcode": "             )"
            },
            "42": {
                "beforePatchRowNumber": 1378,
                "afterPatchRowNumber": 1383,
                "PatchRowcode": "             old_dataset_vm_name = self.get_dataset_perm("
            },
            "43": {
                "beforePatchRowNumber": 1379,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                target.id, old_dataset_name, target.database.database_name"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1384,
                "PatchRowcode": "+                target.id, current_table_name, target.database.database_name"
            },
            "45": {
                "beforePatchRowNumber": 1380,
                "afterPatchRowNumber": 1385,
                "PatchRowcode": "             )"
            },
            "46": {
                "beforePatchRowNumber": 1381,
                "afterPatchRowNumber": 1386,
                "PatchRowcode": "             self._update_dataset_perm("
            },
            "47": {
                "beforePatchRowNumber": 1382,
                "afterPatchRowNumber": 1387,
                "PatchRowcode": "                 mapper, connection, old_dataset_vm_name, new_dataset_vm_name, target"
            },
            "48": {
                "beforePatchRowNumber": 1383,
                "afterPatchRowNumber": 1388,
                "PatchRowcode": "             )"
            },
            "49": {
                "beforePatchRowNumber": 1384,
                "afterPatchRowNumber": 1389,
                "PatchRowcode": " "
            },
            "50": {
                "beforePatchRowNumber": 1385,
                "afterPatchRowNumber": 1390,
                "PatchRowcode": "         # When schema changes"
            },
            "51": {
                "beforePatchRowNumber": 1386,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if history_schema.has_changes() and history_schema.deleted:"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1391,
                "PatchRowcode": "+        if current_schema != target.schema:"
            },
            "53": {
                "beforePatchRowNumber": 1387,
                "afterPatchRowNumber": 1392,
                "PatchRowcode": "             new_dataset_schema_name = self.get_schema_perm("
            },
            "54": {
                "beforePatchRowNumber": 1388,
                "afterPatchRowNumber": 1393,
                "PatchRowcode": "                 target.database.database_name, target.schema"
            },
            "55": {
                "beforePatchRowNumber": 1389,
                "afterPatchRowNumber": 1394,
                "PatchRowcode": "             )"
            },
            "56": {
                "beforePatchRowNumber": 1414,
                "afterPatchRowNumber": 1419,
                "PatchRowcode": "         :param target: Dataset that was updated"
            },
            "57": {
                "beforePatchRowNumber": 1415,
                "afterPatchRowNumber": 1420,
                "PatchRowcode": "         :return:"
            },
            "58": {
                "beforePatchRowNumber": 1416,
                "afterPatchRowNumber": 1421,
                "PatchRowcode": "         \"\"\""
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1422,
                "PatchRowcode": "+        logger.info(\"Updating schema perm, new: %s\", new_schema_permission_name)"
            },
            "60": {
                "beforePatchRowNumber": 1417,
                "afterPatchRowNumber": 1423,
                "PatchRowcode": "         from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel"
            },
            "61": {
                "beforePatchRowNumber": 1418,
                "afterPatchRowNumber": 1424,
                "PatchRowcode": "             SqlaTable,"
            },
            "62": {
                "beforePatchRowNumber": 1419,
                "afterPatchRowNumber": 1425,
                "PatchRowcode": "         )"
            },
            "63": {
                "beforePatchRowNumber": 1467,
                "afterPatchRowNumber": 1473,
                "PatchRowcode": "         :param target:"
            },
            "64": {
                "beforePatchRowNumber": 1468,
                "afterPatchRowNumber": 1474,
                "PatchRowcode": "         :return:"
            },
            "65": {
                "beforePatchRowNumber": 1469,
                "afterPatchRowNumber": 1475,
                "PatchRowcode": "         \"\"\""
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1476,
                "PatchRowcode": "+        logger.info("
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1477,
                "PatchRowcode": "+            \"Updating dataset perm, old: %s, new: %s\","
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1478,
                "PatchRowcode": "+            old_permission_name,"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1479,
                "PatchRowcode": "+            new_permission_name,"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1480,
                "PatchRowcode": "+        )"
            },
            "71": {
                "beforePatchRowNumber": 1470,
                "afterPatchRowNumber": 1481,
                "PatchRowcode": "         from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel"
            },
            "72": {
                "beforePatchRowNumber": 1471,
                "afterPatchRowNumber": 1482,
                "PatchRowcode": "             SqlaTable,"
            },
            "73": {
                "beforePatchRowNumber": 1472,
                "afterPatchRowNumber": 1483,
                "PatchRowcode": "         )"
            },
            "74": {
                "beforePatchRowNumber": 1481,
                "afterPatchRowNumber": 1492,
                "PatchRowcode": "         new_dataset_view_menu = self.find_view_menu(new_permission_name)"
            },
            "75": {
                "beforePatchRowNumber": 1482,
                "afterPatchRowNumber": 1493,
                "PatchRowcode": "         if new_dataset_view_menu:"
            },
            "76": {
                "beforePatchRowNumber": 1483,
                "afterPatchRowNumber": 1494,
                "PatchRowcode": "             return"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1495,
                "PatchRowcode": "+        old_dataset_view_menu = self.find_view_menu(old_permission_name)"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1496,
                "PatchRowcode": "+        if not old_dataset_view_menu:"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1497,
                "PatchRowcode": "+            logger.warning("
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1498,
                "PatchRowcode": "+                \"Could not find previous dataset permission %s\", old_permission_name"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1499,
                "PatchRowcode": "+            )"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1500,
                "PatchRowcode": "+            self._insert_pvm_on_sqla_event("
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1501,
                "PatchRowcode": "+                mapper, connection, \"datasource_access\", new_permission_name"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1502,
                "PatchRowcode": "+            )"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1503,
                "PatchRowcode": "+            return"
            },
            "86": {
                "beforePatchRowNumber": 1484,
                "afterPatchRowNumber": 1504,
                "PatchRowcode": "         # Update VM"
            },
            "87": {
                "beforePatchRowNumber": 1485,
                "afterPatchRowNumber": 1505,
                "PatchRowcode": "         connection.execute("
            },
            "88": {
                "beforePatchRowNumber": 1486,
                "afterPatchRowNumber": 1506,
                "PatchRowcode": "             view_menu_table.update()"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "\"\"\"A set of constants and methods to manage permissions and security\"\"\"",
            "import json",
            "import logging",
            "import re",
            "import time",
            "from collections import defaultdict",
            "from typing import Any, Callable, cast, NamedTuple, Optional, TYPE_CHECKING, Union",
            "",
            "from flask import current_app, Flask, g, Request",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.security.sqla.manager import SecurityManager",
            "from flask_appbuilder.security.sqla.models import (",
            "    assoc_permissionview_role,",
            "    assoc_user_role,",
            "    Permission,",
            "    PermissionView,",
            "    Role,",
            "    User,",
            "    ViewMenu,",
            ")",
            "from flask_appbuilder.security.views import (",
            "    PermissionModelView,",
            "    PermissionViewModelView,",
            "    RoleModelView,",
            "    UserModelView,",
            "    ViewMenuModelView,",
            ")",
            "from flask_appbuilder.widgets import ListWidget",
            "from flask_babel import lazy_gettext as _",
            "from flask_login import AnonymousUserMixin, LoginManager",
            "from jwt.api_jwt import _jwt_global_obj",
            "from sqlalchemy import and_, inspect, or_",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.orm.query import Query as SqlaQuery",
            "",
            "from superset import sql_parse",
            "from superset.constants import RouteMethod",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    DatasetInvalidPermissionEvaluationException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.security.guest_token import (",
            "    GuestToken,",
            "    GuestTokenResources,",
            "    GuestTokenResourceType,",
            "    GuestTokenRlsRule,",
            "    GuestTokenUser,",
            "    GuestUser,",
            ")",
            "from superset.utils.core import (",
            "    DatasourceName,",
            "    DatasourceType,",
            "    get_user_id,",
            "    RowLevelSecurityFilterType,",
            ")",
            "from superset.utils.filters import get_dataset_access_filters",
            "from superset.utils.urls import get_url_host",
            "",
            "if TYPE_CHECKING:",
            "    from superset.common.query_context import QueryContext",
            "    from superset.connectors.base.models import BaseDatasource",
            "    from superset.connectors.sqla.models import SqlaTable",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.sql_lab import Query",
            "    from superset.sql_parse import Table",
            "    from superset.viz import BaseViz",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "DATABASE_PERM_REGEX = re.compile(r\"^\\[.+\\]\\.\\(id\\:(?P<id>\\d+)\\)$\")",
            "",
            "",
            "class DatabaseAndSchema(NamedTuple):",
            "    database: str",
            "    schema: str",
            "",
            "",
            "class SupersetSecurityListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Redeclaring to avoid circular imports",
            "    \"\"\"",
            "",
            "    template = \"superset/fab_overrides/list.html\"",
            "",
            "",
            "class SupersetRoleListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Role model view from FAB already uses a custom list widget override",
            "    So we override the override",
            "    \"\"\"",
            "",
            "    template = \"superset/fab_overrides/list_role.html\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        kwargs[\"appbuilder\"] = current_app.appbuilder",
            "        super().__init__(**kwargs)",
            "",
            "",
            "UserModelView.list_widget = SupersetSecurityListWidget",
            "RoleModelView.list_widget = SupersetRoleListWidget",
            "PermissionViewModelView.list_widget = SupersetSecurityListWidget",
            "PermissionModelView.list_widget = SupersetSecurityListWidget",
            "",
            "# Limiting routes on FAB model views",
            "UserModelView.include_route_methods = RouteMethod.CRUD_SET | {",
            "    RouteMethod.ACTION,",
            "    RouteMethod.API_READ,",
            "    RouteMethod.ACTION_POST,",
            "    \"userinfo\",",
            "}",
            "RoleModelView.include_route_methods = RouteMethod.CRUD_SET",
            "PermissionViewModelView.include_route_methods = {RouteMethod.LIST}",
            "PermissionModelView.include_route_methods = {RouteMethod.LIST}",
            "ViewMenuModelView.include_route_methods = {RouteMethod.LIST}",
            "",
            "RoleModelView.list_columns = [\"name\"]",
            "RoleModelView.edit_columns = [\"name\", \"permissions\", \"user\"]",
            "RoleModelView.related_views = []",
            "",
            "",
            "class SupersetSecurityManager(  # pylint: disable=too-many-public-methods",
            "    SecurityManager",
            "):",
            "    userstatschartview = None",
            "    READ_ONLY_MODEL_VIEWS = {\"Database\", \"DynamicPlugin\"}",
            "",
            "    USER_MODEL_VIEWS = {",
            "        \"RegisterUserModelView\",",
            "        \"UserDBModelView\",",
            "        \"UserLDAPModelView\",",
            "        \"UserInfoEditView\",",
            "        \"UserOAuthModelView\",",
            "        \"UserOIDModelView\",",
            "        \"UserRemoteUserModelView\",",
            "    }",
            "",
            "    GAMMA_READ_ONLY_MODEL_VIEWS = {",
            "        \"Dataset\",",
            "        \"Datasource\",",
            "    } | READ_ONLY_MODEL_VIEWS",
            "",
            "    ADMIN_ONLY_VIEW_MENUS = {",
            "        \"Access Requests\",",
            "        \"Action Log\",",
            "        \"Log\",",
            "        \"List Users\",",
            "        \"List Roles\",",
            "        \"ResetPasswordView\",",
            "        \"RoleModelView\",",
            "        \"Row Level Security\",",
            "        \"Row Level Security Filters\",",
            "        \"RowLevelSecurityFiltersModelView\",",
            "        \"Security\",",
            "        \"SQL Lab\",",
            "        \"User Registrations\",",
            "        \"User's Statistics\",",
            "    } | USER_MODEL_VIEWS",
            "",
            "    ALPHA_ONLY_VIEW_MENUS = {",
            "        \"Alerts & Report\",",
            "        \"Annotation Layers\",",
            "        \"Annotation\",",
            "        \"CSS Templates\",",
            "        \"ColumnarToDatabaseView\",",
            "        \"CssTemplate\",",
            "        \"CsvToDatabaseView\",",
            "        \"ExcelToDatabaseView\",",
            "        \"Import dashboards\",",
            "        \"ImportExportRestApi\",",
            "        \"Manage\",",
            "        \"Queries\",",
            "        \"ReportSchedule\",",
            "        \"TableSchemaView\",",
            "        \"Upload a CSV\",",
            "    }",
            "",
            "    ADMIN_ONLY_PERMISSIONS = {",
            "        \"can_update_role\",",
            "        \"all_query_access\",",
            "        \"can_grant_guest_token\",",
            "        \"can_set_embedded\",",
            "        \"can_warm_up_cache\",",
            "    }",
            "",
            "    READ_ONLY_PERMISSION = {",
            "        \"can_show\",",
            "        \"can_list\",",
            "        \"can_get\",",
            "        \"can_external_metadata\",",
            "        \"can_external_metadata_by_name\",",
            "        \"can_read\",",
            "    }",
            "",
            "    ALPHA_ONLY_PERMISSIONS = {",
            "        \"muldelete\",",
            "        \"all_database_access\",",
            "        \"all_datasource_access\",",
            "    }",
            "",
            "    OBJECT_SPEC_PERMISSIONS = {",
            "        \"database_access\",",
            "        \"schema_access\",",
            "        \"datasource_access\",",
            "    }",
            "",
            "    ACCESSIBLE_PERMS = {\"can_userinfo\", \"resetmypassword\", \"can_recent_activity\"}",
            "",
            "    SQLLAB_ONLY_PERMISSIONS = {",
            "        (\"can_my_queries\", \"SqlLab\"),",
            "        (\"can_read\", \"SavedQuery\"),",
            "        (\"can_write\", \"SavedQuery\"),",
            "        (\"can_export\", \"SavedQuery\"),",
            "        (\"can_read\", \"Query\"),",
            "        (\"can_export_csv\", \"Query\"),",
            "        (\"can_get_results\", \"SQLLab\"),",
            "        (\"can_execute_sql_query\", \"SQLLab\"),",
            "        (\"can_estimate_query_cost\", \"SQL Lab\"),",
            "        (\"can_export_csv\", \"SQLLab\"),",
            "        (\"can_sqllab_history\", \"Superset\"),",
            "        (\"can_sqllab\", \"Superset\"),",
            "        (\"can_test_conn\", \"Superset\"),  # Deprecated permission remove on 3.0.0",
            "        (\"can_activate\", \"TabStateView\"),",
            "        (\"can_get\", \"TabStateView\"),",
            "        (\"can_delete_query\", \"TabStateView\"),",
            "        (\"can_post\", \"TabStateView\"),",
            "        (\"can_delete\", \"TabStateView\"),",
            "        (\"can_put\", \"TabStateView\"),",
            "        (\"can_migrate_query\", \"TabStateView\"),",
            "        (\"menu_access\", \"SQL Lab\"),",
            "        (\"menu_access\", \"SQL Editor\"),",
            "        (\"menu_access\", \"Saved Queries\"),",
            "        (\"menu_access\", \"Query Search\"),",
            "    }",
            "",
            "    SQLLAB_EXTRA_PERMISSION_VIEWS = {",
            "        (\"can_csv\", \"Superset\"),  # Deprecated permission remove on 3.0.0",
            "        (\"can_read\", \"Superset\"),",
            "        (\"can_read\", \"Database\"),",
            "    }",
            "",
            "    data_access_permissions = (",
            "        \"database_access\",",
            "        \"schema_access\",",
            "        \"datasource_access\",",
            "        \"all_datasource_access\",",
            "        \"all_database_access\",",
            "        \"all_query_access\",",
            "    )",
            "",
            "    guest_user_cls = GuestUser",
            "    pyjwt_for_guest_token = _jwt_global_obj",
            "",
            "    def create_login_manager(self, app: Flask) -> LoginManager:",
            "        lm = super().create_login_manager(app)",
            "        lm.request_loader(self.request_loader)",
            "        return lm",
            "",
            "    def request_loader(self, request: Request) -> Optional[User]:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.extensions import feature_flag_manager",
            "",
            "        if feature_flag_manager.is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "            return self.get_guest_user_from_request(request)",
            "        return None",
            "",
            "    def get_schema_perm(  # pylint: disable=no-self-use",
            "        self, database: Union[\"Database\", str], schema: Optional[str] = None",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the database specific schema permission.",
            "",
            "        :param database: The Superset database or database name",
            "        :param schema: The Superset schema name",
            "        :return: The database specific schema permission",
            "        \"\"\"",
            "",
            "        if schema:",
            "            return f\"[{database}].[{schema}]\"",
            "",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_database_perm(database_id: int, database_name: str) -> str:",
            "        return f\"[{database_name}].(id:{database_id})\"",
            "",
            "    @staticmethod",
            "    def get_dataset_perm(dataset_id: int, dataset_name: str, database_name: str) -> str:",
            "        return f\"[{database_name}].[{dataset_name}](id:{dataset_id})\"",
            "",
            "    def unpack_database_and_schema(  # pylint: disable=no-self-use",
            "        self, schema_permission: str",
            "    ) -> DatabaseAndSchema:",
            "        # [database_name].[schema|table]",
            "",
            "        schema_name = schema_permission.split(\".\")[1][1:-1]",
            "        database_name = schema_permission.split(\".\")[0][1:-1]",
            "        return DatabaseAndSchema(database_name, schema_name)",
            "",
            "    def can_access(self, permission_name: str, view_name: str) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the FAB permission/view, False otherwise.",
            "",
            "        Note this method adds protection from has_access failing from missing",
            "        permission/view entries.",
            "",
            "        :param permission_name: The FAB permission name",
            "        :param view_name: The FAB view-menu name",
            "        :returns: Whether the user can access the FAB permission/view",
            "        \"\"\"",
            "",
            "        user = g.user",
            "        if user.is_anonymous:",
            "            return self.is_item_public(permission_name, view_name)",
            "        return self._has_view_access(user, permission_name, view_name)",
            "",
            "    def can_access_all_queries(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all SQL Lab queries, False otherwise.",
            "",
            "        :returns: Whether the user can access all queries",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_query_access\", \"all_query_access\")",
            "",
            "    def can_access_all_datasources(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all the datasources, False otherwise.",
            "",
            "        :returns: Whether the user can access all the datasources",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_datasource_access\", \"all_datasource_access\")",
            "",
            "    def can_access_all_databases(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all the databases, False otherwise.",
            "",
            "        :returns: Whether the user can access all the databases",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_database_access\", \"all_database_access\")",
            "",
            "    def can_access_database(self, database: \"Database\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified database, False otherwise.",
            "",
            "        :param database: The database",
            "        :returns: Whether the user can access the database",
            "        \"\"\"",
            "",
            "        return (",
            "            self.can_access_all_datasources()",
            "            or self.can_access_all_databases()",
            "            or self.can_access(\"database_access\", database.perm)  # type: ignore",
            "        )",
            "",
            "    def can_access_schema(self, datasource: \"BaseDatasource\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the schema associated with specified",
            "        datasource, False otherwise.",
            "",
            "        :param datasource: The datasource",
            "        :returns: Whether the user can access the datasource's schema",
            "        \"\"\"",
            "",
            "        return (",
            "            self.can_access_all_datasources()",
            "            or self.can_access_database(datasource.database)",
            "            or self.can_access(\"schema_access\", datasource.schema_perm or \"\")",
            "        )",
            "",
            "    def can_access_datasource(self, datasource: \"BaseDatasource\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified datasource, False otherwise.",
            "",
            "        :param datasource: The datasource",
            "        :returns: Whether the user can access the datasource",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(datasource=datasource)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def can_access_dashboard(self, dashboard: \"Dashboard\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified dashboard, False otherwise.",
            "",
            "        :param dashboard: The dashboard",
            "        :returns: Whether the user can access the dashboard",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(dashboard=dashboard)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    # pylint: disable=no-self-use",
            "    def get_dashboard_access_error_object(  # pylint: disable=invalid-name",
            "        self,",
            "        dashboard: \"Dashboard\",  # pylint: disable=unused-argument",
            "    ) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied Superset dashboard.",
            "",
            "        :param dashboard: The denied Superset dashboard",
            "        :returns: The error object",
            "        \"\"\"",
            "",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.DASHBOARD_SECURITY_ACCESS_ERROR,",
            "            message=\"You don't have access to this dashboard.\",",
            "            level=ErrorLevel.ERROR,",
            "        )",
            "",
            "    @staticmethod",
            "    def get_datasource_access_error_msg(datasource: \"BaseDatasource\") -> str:",
            "        \"\"\"",
            "        Return the error message for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The error message",
            "        \"\"\"",
            "",
            "        return (",
            "            f\"This endpoint requires the datasource {datasource.name}, \"",
            "            \"database or `all_datasource_access` permission\"",
            "        )",
            "",
            "    @staticmethod",
            "    def get_datasource_access_link(  # pylint: disable=unused-argument",
            "        datasource: \"BaseDatasource\",",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the link for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The access URL",
            "        \"\"\"",
            "",
            "        return current_app.config.get(\"PERMISSION_INSTRUCTIONS_LINK\")",
            "",
            "    def get_datasource_access_error_object(  # pylint: disable=invalid-name",
            "        self, datasource: \"BaseDatasource\"",
            "    ) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The error object",
            "        \"\"\"",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.DATASOURCE_SECURITY_ACCESS_ERROR,",
            "            message=self.get_datasource_access_error_msg(datasource),",
            "            level=ErrorLevel.ERROR,",
            "            extra={",
            "                \"link\": self.get_datasource_access_link(datasource),",
            "                \"datasource\": datasource.name,",
            "            },",
            "        )",
            "",
            "    def get_table_access_error_msg(  # pylint: disable=no-self-use",
            "        self, tables: set[\"Table\"]",
            "    ) -> str:",
            "        \"\"\"",
            "        Return the error message for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The error message",
            "        \"\"\"",
            "",
            "        quoted_tables = [f\"`{table}`\" for table in tables]",
            "        return f\"\"\"You need access to the following tables: {\", \".join(quoted_tables)},",
            "            `all_database_access` or `all_datasource_access` permission\"\"\"",
            "",
            "    def get_table_access_error_object(self, tables: set[\"Table\"]) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The error object",
            "        \"\"\"",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.TABLE_SECURITY_ACCESS_ERROR,",
            "            message=self.get_table_access_error_msg(tables),",
            "            level=ErrorLevel.ERROR,",
            "            extra={",
            "                \"link\": self.get_table_access_link(tables),",
            "                \"tables\": [str(table) for table in tables],",
            "            },",
            "        )",
            "",
            "    def get_table_access_link(  # pylint: disable=unused-argument,no-self-use",
            "        self, tables: set[\"Table\"]",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the access link for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The access URL",
            "        \"\"\"",
            "",
            "        return current_app.config.get(\"PERMISSION_INSTRUCTIONS_LINK\")",
            "",
            "    def get_user_datasources(self) -> list[\"BaseDatasource\"]:",
            "        \"\"\"",
            "        Collect datasources which the user has explicit permissions to.",
            "",
            "        :returns: The list of datasources",
            "        \"\"\"",
            "",
            "        user_datasources = set()",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        user_datasources.update(",
            "            self.get_session.query(SqlaTable)",
            "            .filter(get_dataset_access_filters(SqlaTable))",
            "            .all()",
            "        )",
            "",
            "        # group all datasources by database",
            "        session = self.get_session",
            "        all_datasources = SqlaTable.get_all_datasources(session)",
            "        datasources_by_database: dict[\"Database\", set[\"SqlaTable\"]] = defaultdict(set)",
            "        for datasource in all_datasources:",
            "            datasources_by_database[datasource.database].add(datasource)",
            "",
            "        # add datasources with implicit permission (eg, database access)",
            "        for database, datasources in datasources_by_database.items():",
            "            if self.can_access_database(database):",
            "                user_datasources.update(datasources)",
            "",
            "        return list(user_datasources)",
            "",
            "    def can_access_table(self, database: \"Database\", table: \"Table\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the SQL table, False otherwise.",
            "",
            "        :param database: The SQL database",
            "        :param table: The SQL table",
            "        :returns: Whether the user can access the SQL table",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(database=database, table=table)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def user_view_menu_names(self, permission_name: str) -> set[str]:",
            "        base_query = (",
            "            self.get_session.query(self.viewmenu_model.name)",
            "            .join(self.permissionview_model)",
            "            .join(self.permission_model)",
            "            .join(assoc_permissionview_role)",
            "            .join(self.role_model)",
            "        )",
            "",
            "        if not g.user.is_anonymous:",
            "            # filter by user id",
            "            view_menu_names = (",
            "                base_query.join(assoc_user_role)",
            "                .join(self.user_model)",
            "                .filter(self.user_model.id == get_user_id())",
            "                .filter(self.permission_model.name == permission_name)",
            "            ).all()",
            "            return {s.name for s in view_menu_names}",
            "",
            "        # Properly treat anonymous user",
            "        if public_role := self.get_public_role():",
            "            # filter by public role",
            "            view_menu_names = (",
            "                base_query.filter(self.role_model.id == public_role.id).filter(",
            "                    self.permission_model.name == permission_name",
            "                )",
            "            ).all()",
            "            return {s.name for s in view_menu_names}",
            "        return set()",
            "",
            "    def get_accessible_databases(self) -> list[int]:",
            "        \"\"\"",
            "        Return the list of databases accessible by the user.",
            "",
            "        :return: The list of accessible Databases",
            "        \"\"\"",
            "        perms = self.user_view_menu_names(\"database_access\")",
            "        return [",
            "            int(match.group(\"id\"))",
            "            for perm in perms",
            "            if (match := DATABASE_PERM_REGEX.match(perm))",
            "        ]",
            "",
            "    def get_schemas_accessible_by_user(",
            "        self, database: \"Database\", schemas: list[str], hierarchical: bool = True",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return the list of SQL schemas accessible by the user.",
            "",
            "        :param database: The SQL database",
            "        :param schemas: The list of eligible SQL schemas",
            "        :param hierarchical: Whether to check using the hierarchical permission logic",
            "        :returns: The list of accessible SQL schemas",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        if hierarchical and self.can_access_database(database):",
            "            return schemas",
            "",
            "        # schema_access",
            "        accessible_schemas = {",
            "            self.unpack_database_and_schema(s).schema",
            "            for s in self.user_view_menu_names(\"schema_access\")",
            "            if s.startswith(f\"[{database}].\")",
            "        }",
            "",
            "        # datasource_access",
            "        if perms := self.user_view_menu_names(\"datasource_access\"):",
            "            tables = (",
            "                self.get_session.query(SqlaTable.schema)",
            "                .filter(SqlaTable.database_id == database.id)",
            "                .filter(SqlaTable.schema.isnot(None))",
            "                .filter(SqlaTable.schema != \"\")",
            "                .filter(or_(SqlaTable.perm.in_(perms)))",
            "                .distinct()",
            "            )",
            "            accessible_schemas.update([table.schema for table in tables])",
            "",
            "        return [s for s in schemas if s in accessible_schemas]",
            "",
            "    def get_datasources_accessible_by_user(  # pylint: disable=invalid-name",
            "        self,",
            "        database: \"Database\",",
            "        datasource_names: list[DatasourceName],",
            "        schema: Optional[str] = None,",
            "    ) -> list[DatasourceName]:",
            "        \"\"\"",
            "        Return the list of SQL tables accessible by the user.",
            "",
            "        :param database: The SQL database",
            "        :param datasource_names: The list of eligible SQL tables w/ schema",
            "        :param schema: The fallback SQL schema if not present in the table name",
            "        :returns: The list of accessible SQL tables w/ schema",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        if self.can_access_database(database):",
            "            return datasource_names",
            "",
            "        if schema:",
            "            schema_perm = self.get_schema_perm(database, schema)",
            "            if schema_perm and self.can_access(\"schema_access\", schema_perm):",
            "                return datasource_names",
            "",
            "        user_perms = self.user_view_menu_names(\"datasource_access\")",
            "        schema_perms = self.user_view_menu_names(\"schema_access\")",
            "        user_datasources = SqlaTable.query_datasources_by_permissions(",
            "            self.get_session, database, user_perms, schema_perms",
            "        )",
            "        if schema:",
            "            names = {d.table_name for d in user_datasources if d.schema == schema}",
            "            return [d for d in datasource_names if d.table in names]",
            "",
            "        full_names = {d.full_name for d in user_datasources}",
            "        return [d for d in datasource_names if f\"[{database}].[{d}]\" in full_names]",
            "",
            "    def merge_perm(self, permission_name: str, view_menu_name: str) -> None:",
            "        \"\"\"",
            "        Add the FAB permission/view-menu.",
            "",
            "        :param permission_name: The FAB permission name",
            "        :param view_menu_names: The FAB view-menu name",
            "        :see: SecurityManager.add_permission_view_menu",
            "        \"\"\"",
            "",
            "        logger.warning(",
            "            \"This method 'merge_perm' is deprecated use add_permission_view_menu\"",
            "        )",
            "        self.add_permission_view_menu(permission_name, view_menu_name)",
            "",
            "    def _is_user_defined_permission(self, perm: Model) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission is user defined, False otherwise.",
            "",
            "        :param perm: The FAB permission",
            "        :returns: Whether the FAB permission is user defined",
            "        \"\"\"",
            "",
            "        return perm.permission.name in self.OBJECT_SPEC_PERMISSIONS",
            "",
            "    def create_custom_permissions(self) -> None:",
            "        \"\"\"",
            "        Create custom FAB permissions.",
            "        \"\"\"",
            "        self.add_permission_view_menu(\"all_datasource_access\", \"all_datasource_access\")",
            "        self.add_permission_view_menu(\"all_database_access\", \"all_database_access\")",
            "        self.add_permission_view_menu(\"all_query_access\", \"all_query_access\")",
            "        self.add_permission_view_menu(\"can_csv\", \"Superset\")",
            "        self.add_permission_view_menu(\"can_share_dashboard\", \"Superset\")",
            "        self.add_permission_view_menu(\"can_share_chart\", \"Superset\")",
            "",
            "    def create_missing_perms(self) -> None:",
            "        \"\"\"",
            "        Creates missing FAB permissions for datasources, schemas and metrics.",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "        from superset.models import core as models",
            "",
            "        logger.info(\"Fetching a set of all perms to lookup which ones are missing\")",
            "        all_pvs = set()",
            "        for pv in self.get_session.query(self.permissionview_model).all():",
            "            if pv.permission and pv.view_menu:",
            "                all_pvs.add((pv.permission.name, pv.view_menu.name))",
            "",
            "        def merge_pv(view_menu: str, perm: Optional[str]) -> None:",
            "            \"\"\"Create permission view menu only if it doesn't exist\"\"\"",
            "            if view_menu and perm and (view_menu, perm) not in all_pvs:",
            "                self.add_permission_view_menu(view_menu, perm)",
            "",
            "        logger.info(\"Creating missing datasource permissions.\")",
            "        datasources = SqlaTable.get_all_datasources(self.get_session)",
            "        for datasource in datasources:",
            "            merge_pv(\"datasource_access\", datasource.get_perm())",
            "            merge_pv(\"schema_access\", datasource.get_schema_perm())",
            "",
            "        logger.info(\"Creating missing database permissions.\")",
            "        databases = self.get_session.query(models.Database).all()",
            "        for database in databases:",
            "            merge_pv(\"database_access\", database.perm)",
            "",
            "    def clean_perms(self) -> None:",
            "        \"\"\"",
            "        Clean up the FAB faulty permissions.",
            "        \"\"\"",
            "",
            "        logger.info(\"Cleaning faulty perms\")",
            "        sesh = self.get_session",
            "        pvms = sesh.query(PermissionView).filter(",
            "            or_(",
            "                PermissionView.permission  # pylint: disable=singleton-comparison",
            "                == None,",
            "                PermissionView.view_menu  # pylint: disable=singleton-comparison",
            "                == None,",
            "            )",
            "        )",
            "        sesh.commit()",
            "        if deleted_count := pvms.delete():",
            "            logger.info(\"Deleted %i faulty permissions\", deleted_count)",
            "",
            "    def sync_role_definitions(self) -> None:",
            "        \"\"\"",
            "        Initialize the Superset application with security roles and such.",
            "        \"\"\"",
            "",
            "        logger.info(\"Syncing role definition\")",
            "",
            "        self.create_custom_permissions()",
            "",
            "        # Creating default roles",
            "        self.set_role(\"Admin\", self._is_admin_pvm)",
            "        self.set_role(\"Alpha\", self._is_alpha_pvm)",
            "        self.set_role(\"Gamma\", self._is_gamma_pvm)",
            "        self.set_role(\"sql_lab\", self._is_sql_lab_pvm)",
            "",
            "        # Configure public role",
            "        if current_app.config[\"PUBLIC_ROLE_LIKE\"]:",
            "            self.copy_role(",
            "                current_app.config[\"PUBLIC_ROLE_LIKE\"],",
            "                self.auth_role_public,",
            "                merge=True,",
            "            )",
            "",
            "        self.create_missing_perms()",
            "",
            "        # commit role and view menu updates",
            "        self.get_session.commit()",
            "        self.clean_perms()",
            "",
            "    def _get_pvms_from_builtin_role(self, role_name: str) -> list[PermissionView]:",
            "        \"\"\"",
            "        Gets a list of model PermissionView permissions inferred from a builtin role",
            "        definition",
            "        \"\"\"",
            "        role_from_permissions_names = self.builtin_roles.get(role_name, [])",
            "        all_pvms = self.get_session.query(PermissionView).all()",
            "        role_from_permissions = []",
            "        for pvm_regex in role_from_permissions_names:",
            "            view_name_regex = pvm_regex[0]",
            "            permission_name_regex = pvm_regex[1]",
            "            for pvm in all_pvms:",
            "                if re.match(view_name_regex, pvm.view_menu.name) and re.match(",
            "                    permission_name_regex, pvm.permission.name",
            "                ):",
            "                    if pvm not in role_from_permissions:",
            "                        role_from_permissions.append(pvm)",
            "        return role_from_permissions",
            "",
            "    def find_roles_by_id(self, role_ids: list[int]) -> list[Role]:",
            "        \"\"\"",
            "        Find a List of models by a list of ids, if defined applies `base_filter`",
            "        \"\"\"",
            "        query = self.get_session.query(Role).filter(Role.id.in_(role_ids))",
            "        return query.all()",
            "",
            "    def copy_role(",
            "        self, role_from_name: str, role_to_name: str, merge: bool = True",
            "    ) -> None:",
            "        \"\"\"",
            "        Copies permissions from a role to another.",
            "",
            "        Note: Supports regex defined builtin roles",
            "",
            "        :param role_from_name: The FAB role name from where the permissions are taken",
            "        :param role_to_name: The FAB role name from where the permissions are copied to",
            "        :param merge: If merge is true, keep data access permissions",
            "            if they already exist on the target role",
            "        \"\"\"",
            "",
            "        logger.info(\"Copy/Merge %s to %s\", role_from_name, role_to_name)",
            "        # If it's a builtin role extract permissions from it",
            "        if role_from_name in self.builtin_roles:",
            "            role_from_permissions = self._get_pvms_from_builtin_role(role_from_name)",
            "        else:",
            "            role_from_permissions = list(self.find_role(role_from_name).permissions)",
            "        role_to = self.add_role(role_to_name)",
            "        # If merge, recover existing data access permissions",
            "        if merge:",
            "            for permission_view in role_to.permissions:",
            "                if (",
            "                    permission_view not in role_from_permissions",
            "                    and permission_view.permission.name in self.data_access_permissions",
            "                ):",
            "                    role_from_permissions.append(permission_view)",
            "        role_to.permissions = role_from_permissions",
            "        self.get_session.merge(role_to)",
            "        self.get_session.commit()",
            "",
            "    def set_role(",
            "        self, role_name: str, pvm_check: Callable[[PermissionView], bool]",
            "    ) -> None:",
            "        \"\"\"",
            "        Set the FAB permission/views for the role.",
            "",
            "        :param role_name: The FAB role name",
            "        :param pvm_check: The FAB permission/view check",
            "        \"\"\"",
            "",
            "        logger.info(\"Syncing %s perms\", role_name)",
            "        pvms = self.get_session.query(PermissionView).all()",
            "        pvms = [p for p in pvms if p.permission and p.view_menu]",
            "        role = self.add_role(role_name)",
            "        role_pvms = [",
            "            permission_view for permission_view in pvms if pvm_check(permission_view)",
            "        ]",
            "        role.permissions = role_pvms",
            "        self.get_session.merge(role)",
            "        self.get_session.commit()",
            "",
            "    def _is_admin_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to only Admin users,",
            "        False otherwise.",
            "",
            "        Note readonly operations on read only model views are allowed only for admins.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to only Admin users",
            "        \"\"\"",
            "",
            "        if (",
            "            pvm.view_menu.name in self.READ_ONLY_MODEL_VIEWS",
            "            and pvm.permission.name not in self.READ_ONLY_PERMISSION",
            "        ):",
            "            return True",
            "        return (",
            "            pvm.view_menu.name in self.ADMIN_ONLY_VIEW_MENUS",
            "            or pvm.permission.name in self.ADMIN_ONLY_PERMISSIONS",
            "        )",
            "",
            "    def _is_alpha_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to only Alpha users,",
            "        False otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to only Alpha users",
            "        \"\"\"",
            "",
            "        if (",
            "            pvm.view_menu.name in self.GAMMA_READ_ONLY_MODEL_VIEWS",
            "            and pvm.permission.name not in self.READ_ONLY_PERMISSION",
            "        ):",
            "            return True",
            "        return (",
            "            pvm.view_menu.name in self.ALPHA_ONLY_VIEW_MENUS",
            "            or pvm.permission.name in self.ALPHA_ONLY_PERMISSIONS",
            "        )",
            "",
            "    def _is_accessible_to_all(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to all, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to all users",
            "        \"\"\"",
            "",
            "        return pvm.permission.name in self.ACCESSIBLE_PERMS",
            "",
            "    def _is_admin_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Admin user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Admin related",
            "        \"\"\"",
            "",
            "        return not self._is_user_defined_permission(pvm)",
            "",
            "    def _is_alpha_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Alpha user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Alpha related",
            "        \"\"\"",
            "",
            "        return not (",
            "            self._is_user_defined_permission(pvm)",
            "            or self._is_admin_only(pvm)",
            "            or self._is_sql_lab_only(pvm)",
            "        ) or self._is_accessible_to_all(pvm)",
            "",
            "    def _is_gamma_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Gamma user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Gamma related",
            "        \"\"\"",
            "",
            "        return not (",
            "            self._is_user_defined_permission(pvm)",
            "            or self._is_admin_only(pvm)",
            "            or self._is_alpha_only(pvm)",
            "            or self._is_sql_lab_only(pvm)",
            "        ) or self._is_accessible_to_all(pvm)",
            "",
            "    def _is_sql_lab_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is only SQL Lab related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is SQL Lab related",
            "        \"\"\"",
            "        return (pvm.permission.name, pvm.view_menu.name) in self.SQLLAB_ONLY_PERMISSIONS",
            "",
            "    def _is_sql_lab_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is SQL Lab related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is SQL Lab related",
            "        \"\"\"",
            "        return (",
            "            self._is_sql_lab_only(pvm)",
            "            or (pvm.permission.name, pvm.view_menu.name)",
            "            in self.SQLLAB_EXTRA_PERMISSION_VIEWS",
            "        )",
            "",
            "    def database_after_insert(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions when a database is created.",
            "        Triggered by a SQLAlchemy after_insert event.",
            "",
            "        We need to create:",
            "         - The database PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"database_access\", target.get_perm()",
            "        )",
            "",
            "    def database_after_delete(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions update when a database is deleted.",
            "        Triggered by a SQLAlchemy after_delete event.",
            "",
            "        We need to delete:",
            "         - The database PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        self._delete_vm_database_access(",
            "            mapper, connection, target.id, target.database_name",
            "        )",
            "",
            "    def database_after_update(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles all permissions update when a database is changed.",
            "        Triggered by a SQLAlchemy after_update event.",
            "",
            "        We need to update:",
            "         - The database PVM",
            "         - All datasets PVMs that reference the db, and it's local perm name",
            "         - All datasets local schema perm that reference the db.",
            "         - All charts local perm related with said datasets",
            "         - All charts local schema perm related with said datasets",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        # Check if database name has changed",
            "        state = inspect(target)",
            "        history = state.get_history(\"database_name\", True)",
            "        if not history.has_changes() or not history.deleted:",
            "            return",
            "",
            "        old_database_name = history.deleted[0]",
            "        # update database access permission",
            "        self._update_vm_database_access(mapper, connection, old_database_name, target)",
            "        # update datasource access",
            "        self._update_vm_datasources_access(",
            "            mapper, connection, old_database_name, target",
            "        )",
            "        # Note schema permissions are updated at the API level",
            "        # (database.commands.update). Since we need to fetch all existing schemas from",
            "        # the db",
            "",
            "    def _delete_vm_database_access(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        database_id: int,",
            "        database_name: str,",
            "    ) -> None:",
            "        view_menu_name = self.get_database_perm(database_id, database_name)",
            "        # Clean database access permission",
            "        self._delete_pvm_on_sqla_event(",
            "            mapper, connection, \"database_access\", view_menu_name",
            "        )",
            "        # Clean database schema permissions",
            "        schema_pvms = (",
            "            self.get_session.query(self.permissionview_model)",
            "            .join(self.permission_model)",
            "            .join(self.viewmenu_model)",
            "            .filter(self.permission_model.name == \"schema_access\")",
            "            .filter(self.viewmenu_model.name.like(f\"[{database_name}].[%]\"))",
            "            .all()",
            "        )",
            "        for schema_pvm in schema_pvms:",
            "            self._delete_pvm_on_sqla_event(mapper, connection, pvm=schema_pvm)",
            "",
            "    def _update_vm_database_access(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_database_name: str,",
            "        target: \"Database\",",
            "    ) -> Optional[ViewMenu]:",
            "        \"\"\"",
            "        Helper method that Updates all database access permission",
            "        when a database name changes.",
            "",
            "        :param connection: Current connection (called on SQLAlchemy event listener scope)",
            "        :param old_database_name: the old database name",
            "        :param target: The database object",
            "        :return: A list of changed view menus (permission resource names)",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        new_database_name = target.database_name",
            "        old_view_menu_name = self.get_database_perm(target.id, old_database_name)",
            "        new_view_menu_name = self.get_database_perm(target.id, new_database_name)",
            "        db_pvm = self.find_permission_view_menu(\"database_access\", old_view_menu_name)",
            "        if not db_pvm:",
            "            logger.warning(",
            "                \"Could not find previous database permission %s\",",
            "                old_view_menu_name,",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"database_access\", new_view_menu_name",
            "            )",
            "            return None",
            "        new_updated_pvm = self.find_permission_view_menu(",
            "            \"database_access\", new_view_menu_name",
            "        )",
            "        if new_updated_pvm:",
            "            logger.info(",
            "                \"New permission [%s] already exists, deleting the previous\",",
            "                new_view_menu_name,",
            "            )",
            "            self._delete_vm_database_access(",
            "                mapper, connection, target.id, old_database_name",
            "            )",
            "            return None",
            "        connection.execute(",
            "            view_menu_table.update()",
            "            .where(view_menu_table.c.id == db_pvm.view_menu_id)",
            "            .values(name=new_view_menu_name)",
            "        )",
            "        new_db_view_menu = self._find_view_menu_on_sqla_event(",
            "            connection, new_view_menu_name",
            "        )",
            "",
            "        self.on_view_menu_after_update(mapper, connection, new_db_view_menu)",
            "        return new_db_view_menu",
            "",
            "    def _update_vm_datasources_access(  # pylint: disable=too-many-locals",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_database_name: str,",
            "        target: \"Database\",",
            "    ) -> list[ViewMenu]:",
            "        \"\"\"",
            "        Helper method that Updates all datasource access permission",
            "        when a database name changes.",
            "",
            "        :param connection: Current connection (called on SQLAlchemy event listener scope)",
            "        :param old_database_name: the old database name",
            "        :param target: The database object",
            "        :return: A list of changed view menus (permission resource names)",
            "        \"\"\"",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "        new_database_name = target.database_name",
            "        datasets = (",
            "            self.get_session.query(SqlaTable)",
            "            .filter(SqlaTable.database_id == target.id)",
            "            .all()",
            "        )",
            "        updated_view_menus: list[ViewMenu] = []",
            "        for dataset in datasets:",
            "            old_dataset_vm_name = self.get_dataset_perm(",
            "                dataset.id, dataset.table_name, old_database_name",
            "            )",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                dataset.id, dataset.table_name, new_database_name",
            "            )",
            "            new_dataset_view_menu = self.find_view_menu(new_dataset_vm_name)",
            "            if new_dataset_view_menu:",
            "                continue",
            "            connection.execute(",
            "                view_menu_table.update()",
            "                .where(view_menu_table.c.name == old_dataset_vm_name)",
            "                .values(name=new_dataset_vm_name)",
            "            )",
            "            # After update refresh",
            "            new_dataset_view_menu = self._find_view_menu_on_sqla_event(",
            "                connection, new_dataset_vm_name",
            "            )",
            "",
            "            # Update dataset (SqlaTable perm field)",
            "            connection.execute(",
            "                sqlatable_table.update()",
            "                .where(",
            "                    sqlatable_table.c.id == dataset.id,",
            "                    sqlatable_table.c.perm == old_dataset_vm_name,",
            "                )",
            "                .values(perm=new_dataset_vm_name)",
            "            )",
            "            # Update charts (Slice perm field)",
            "            connection.execute(",
            "                chart_table.update()",
            "                .where(chart_table.c.perm == old_dataset_vm_name)",
            "                .values(perm=new_dataset_vm_name)",
            "            )",
            "            self.on_view_menu_after_update(mapper, connection, new_dataset_view_menu)",
            "            updated_view_menus.append(new_dataset_view_menu)",
            "        return updated_view_menus",
            "",
            "    def dataset_after_insert(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permission creation when a dataset is inserted.",
            "        Triggered by a SQLAlchemy after_insert event.",
            "",
            "        We need to create:",
            "         - The dataset PVM and set local and schema perm",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        from superset.models.core import (  # pylint: disable=import-outside-toplevel",
            "            Database,",
            "        )",
            "",
            "        try:",
            "            dataset_perm = target.get_perm()",
            "            database = target.database",
            "        except DatasetInvalidPermissionEvaluationException:",
            "            logger.warning(",
            "                \"Dataset has no database will retry with database_id to set permission\"",
            "            )",
            "            database = self.get_session.query(Database).get(target.database_id)",
            "            dataset_perm = self.get_dataset_perm(",
            "                target.id, target.table_name, database.database_name",
            "            )",
            "        dataset_table = target.__table__",
            "",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"datasource_access\", dataset_perm",
            "        )",
            "        if target.perm != dataset_perm:",
            "            target.perm = dataset_perm",
            "            connection.execute(",
            "                dataset_table.update()",
            "                .where(dataset_table.c.id == target.id)",
            "                .values(perm=dataset_perm)",
            "            )",
            "",
            "        if target.schema:",
            "            dataset_schema_perm = self.get_schema_perm(",
            "                database.database_name, target.schema",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"schema_access\", dataset_schema_perm",
            "            )",
            "            target.schema_perm = dataset_schema_perm",
            "            connection.execute(",
            "                dataset_table.update()",
            "                .where(dataset_table.c.id == target.id)",
            "                .values(schema_perm=dataset_schema_perm)",
            "            )",
            "",
            "    def dataset_after_delete(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions update when a dataset is deleted.",
            "        Triggered by a SQLAlchemy after_delete event.",
            "",
            "        We need to delete:",
            "         - The dataset PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        dataset_vm_name = self.get_dataset_perm(",
            "            target.id, target.table_name, target.database.database_name",
            "        )",
            "        self._delete_pvm_on_sqla_event(",
            "            mapper, connection, \"datasource_access\", dataset_vm_name",
            "        )",
            "",
            "    def dataset_after_update(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles all permissions update when a dataset is changed.",
            "        Triggered by a SQLAlchemy after_update event.",
            "",
            "        We need to update:",
            "         - The dataset PVM and local perm",
            "         - All charts local perm related with said datasets",
            "         - All charts local schema perm related with said datasets",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        # Check if watched fields have changed",
            "        state = inspect(target)",
            "        history_database = state.get_history(\"database_id\", True)",
            "        history_table_name = state.get_history(\"table_name\", True)",
            "        history_schema = state.get_history(\"schema\", True)",
            "",
            "        # When database name changes",
            "        if history_database.has_changes() and history_database.deleted:",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, target.table_name, target.database.database_name",
            "            )",
            "            self._update_dataset_perm(",
            "                mapper, connection, target.perm, new_dataset_vm_name, target",
            "            )",
            "",
            "            # Updates schema permissions",
            "            new_dataset_schema_name = self.get_schema_perm(",
            "                target.database.database_name, target.schema",
            "            )",
            "            self._update_dataset_schema_perm(",
            "                mapper,",
            "                connection,",
            "                new_dataset_schema_name,",
            "                target,",
            "            )",
            "",
            "        # When table name changes",
            "        if history_table_name.has_changes() and history_table_name.deleted:",
            "            old_dataset_name = history_table_name.deleted[0]",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, target.table_name, target.database.database_name",
            "            )",
            "            old_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, old_dataset_name, target.database.database_name",
            "            )",
            "            self._update_dataset_perm(",
            "                mapper, connection, old_dataset_vm_name, new_dataset_vm_name, target",
            "            )",
            "",
            "        # When schema changes",
            "        if history_schema.has_changes() and history_schema.deleted:",
            "            new_dataset_schema_name = self.get_schema_perm(",
            "                target.database.database_name, target.schema",
            "            )",
            "            self._update_dataset_schema_perm(",
            "                mapper,",
            "                connection,",
            "                new_dataset_schema_name,",
            "                target,",
            "            )",
            "",
            "    def _update_dataset_schema_perm(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        new_schema_permission_name: Optional[str],",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events on datasets to update",
            "        a new schema permission name, propagates the name change to datasets and charts.",
            "",
            "        If the schema permission name does not exist already has a PVM,",
            "        creates a new one.",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param new_schema_permission_name: The new schema permission name that changed",
            "        :param target: Dataset that was updated",
            "        :return:",
            "        \"\"\"",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "",
            "        # insert new schema PVM if it does not exist",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"schema_access\", new_schema_permission_name",
            "        )",
            "",
            "        # Update dataset (SqlaTable schema_perm field)",
            "        connection.execute(",
            "            sqlatable_table.update()",
            "            .where(",
            "                sqlatable_table.c.id == target.id,",
            "            )",
            "            .values(schema_perm=new_schema_permission_name)",
            "        )",
            "",
            "        # Update charts (Slice schema_perm field)",
            "        connection.execute(",
            "            chart_table.update()",
            "            .where(",
            "                chart_table.c.datasource_id == target.id,",
            "                chart_table.c.datasource_type == DatasourceType.TABLE,",
            "            )",
            "            .values(schema_perm=new_schema_permission_name)",
            "        )",
            "",
            "    def _update_dataset_perm(  # pylint: disable=too-many-arguments",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_permission_name: Optional[str],",
            "        new_permission_name: Optional[str],",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events on datasets to update",
            "        a permission name change, propagates the name change to VM, datasets and charts.",
            "",
            "        :param mapper:",
            "        :param connection:",
            "        :param old_permission_name",
            "        :param new_permission_name:",
            "        :param target:",
            "        :return:",
            "        \"\"\"",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "",
            "        new_dataset_view_menu = self.find_view_menu(new_permission_name)",
            "        if new_dataset_view_menu:",
            "            return",
            "        # Update VM",
            "        connection.execute(",
            "            view_menu_table.update()",
            "            .where(view_menu_table.c.name == old_permission_name)",
            "            .values(name=new_permission_name)",
            "        )",
            "        # VM changed, so call hook",
            "        new_dataset_view_menu = self.find_view_menu(new_permission_name)",
            "        self.on_view_menu_after_update(mapper, connection, new_dataset_view_menu)",
            "        # Update dataset (SqlaTable perm field)",
            "        connection.execute(",
            "            sqlatable_table.update()",
            "            .where(",
            "                sqlatable_table.c.id == target.id,",
            "            )",
            "            .values(perm=new_permission_name)",
            "        )",
            "        # Update charts (Slice perm field)",
            "        connection.execute(",
            "            chart_table.update()",
            "            .where(",
            "                chart_table.c.datasource_type == DatasourceType.TABLE,",
            "                chart_table.c.datasource_id == target.id,",
            "            )",
            "            .values(perm=new_permission_name)",
            "        )",
            "",
            "    def _delete_pvm_on_sqla_event(  # pylint: disable=too-many-arguments",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        permission_name: Optional[str] = None,",
            "        view_menu_name: Optional[str] = None,",
            "        pvm: Optional[PermissionView] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events.",
            "        Deletes a PVM.",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param permission_name: e.g.: datasource_access, schema_access",
            "        :param view_menu_name: e.g. [db1].[public]",
            "        :param pvm: Can be called with the actual PVM already",
            "        :return:",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        permission_view_menu_table = (",
            "            self.permissionview_model.__table__  # pylint: disable=no-member",
            "        )",
            "",
            "        if not pvm:",
            "            pvm = self.find_permission_view_menu(permission_name, view_menu_name)",
            "        if not pvm:",
            "            return",
            "        # Delete Any Role to PVM association",
            "        connection.execute(",
            "            assoc_permissionview_role.delete().where(",
            "                assoc_permissionview_role.c.permission_view_id == pvm.id",
            "            )",
            "        )",
            "        # Delete the database access PVM",
            "        connection.execute(",
            "            permission_view_menu_table.delete().where(",
            "                permission_view_menu_table.c.id == pvm.id",
            "            )",
            "        )",
            "        self.on_permission_view_after_delete(mapper, connection, pvm)",
            "        connection.execute(",
            "            view_menu_table.delete().where(view_menu_table.c.id == pvm.view_menu_id)",
            "        )",
            "",
            "    def _find_permission_on_sqla_event(",
            "        self, connection: Connection, name: str",
            "    ) -> Permission:",
            "        \"\"\"",
            "        Find a FAB Permission using a SQLA connection.",
            "",
            "        A session.query may not return the latest results on newly created/updated",
            "        objects/rows using connection. On this case we should use a connection also",
            "",
            "        :param connection: SQLAlchemy connection",
            "        :param name: The permission name (it's unique)",
            "        :return: Permission",
            "        \"\"\"",
            "        permission_table = self.permission_model.__table__  # pylint: disable=no-member",
            "",
            "        permission_ = connection.execute(",
            "            permission_table.select().where(permission_table.c.name == name)",
            "        ).fetchone()",
            "        permission = Permission()",
            "        # ensures this object is never persisted",
            "        permission.metadata = None",
            "        permission.id = permission_.id",
            "        permission.name = permission_.name",
            "        return permission",
            "",
            "    def _find_view_menu_on_sqla_event(",
            "        self, connection: Connection, name: str",
            "    ) -> ViewMenu:",
            "        \"\"\"",
            "        Find a FAB ViewMenu using a SQLA connection.",
            "",
            "        A session.query may not return the latest results on newly created/updated",
            "        objects/rows using connection. On this case we should use a connection also",
            "",
            "        :param connection: SQLAlchemy connection",
            "        :param name: The ViewMenu name (it's unique)",
            "        :return: ViewMenu",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "",
            "        view_menu_ = connection.execute(",
            "            view_menu_table.select().where(view_menu_table.c.name == name)",
            "        ).fetchone()",
            "        view_menu = ViewMenu()",
            "        # ensures this object is never persisted",
            "        view_menu.metadata = None",
            "        view_menu.id = view_menu_.id",
            "        view_menu.name = view_menu_.name",
            "        return view_menu",
            "",
            "    def _insert_pvm_on_sqla_event(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        permission_name: str,",
            "        view_menu_name: Optional[str],",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events.",
            "        Inserts a new PVM (if it does not exist already)",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param permission_name: e.g.: datasource_access, schema_access",
            "        :param view_menu_name: e.g. [db1].[public]",
            "        :return:",
            "        \"\"\"",
            "        permission_table = self.permission_model.__table__  # pylint: disable=no-member",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        permission_view_table = (",
            "            self.permissionview_model.__table__  # pylint: disable=no-member",
            "        )",
            "        if not view_menu_name:",
            "            return",
            "        pvm = self.find_permission_view_menu(permission_name, view_menu_name)",
            "        if pvm:",
            "            return",
            "        permission = self.find_permission(permission_name)",
            "        view_menu = self.find_view_menu(view_menu_name)",
            "        if not permission:",
            "            _ = connection.execute(",
            "                permission_table.insert().values(name=permission_name)",
            "            )",
            "            permission = self._find_permission_on_sqla_event(",
            "                connection, permission_name",
            "            )",
            "            self.on_permission_after_insert(mapper, connection, permission)",
            "        if not view_menu:",
            "            _ = connection.execute(view_menu_table.insert().values(name=view_menu_name))",
            "            view_menu = self._find_view_menu_on_sqla_event(connection, view_menu_name)",
            "            self.on_view_menu_after_insert(mapper, connection, view_menu)",
            "        connection.execute(",
            "            permission_view_table.insert().values(",
            "                permission_id=permission.id, view_menu_id=view_menu.id",
            "            )",
            "        )",
            "        permission_view = connection.execute(",
            "            permission_view_table.select().where(",
            "                permission_view_table.c.permission_id == permission.id,",
            "                permission_view_table.c.view_menu_id == view_menu.id,",
            "            )",
            "        ).fetchone()",
            "        permission_view_model = PermissionView()",
            "        permission_view_model.metadata = None",
            "        permission_view_model.id = permission_view.id",
            "        permission_view_model.permission_id = permission.id",
            "        permission_view_model.view_menu_id = view_menu.id",
            "        permission_view_model.permission = permission",
            "        permission_view_model.view_menu = view_menu",
            "        self.on_permission_view_after_insert(mapper, connection, permission_view_model)",
            "",
            "    def on_role_after_update(",
            "        self, mapper: Mapper, connection: Connection, target: Role",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a Role update",
            "        is created by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new view_menu's using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being changed",
            "        \"\"\"",
            "",
            "    def on_view_menu_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: ViewMenu",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new ViewMenu",
            "        is created by set_perm.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new view_menu's using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_view_menu_after_update(",
            "        self, mapper: Mapper, connection: Connection, target: ViewMenu",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new ViewMenu",
            "        is updated",
            "",
            "        Since the update may be performed on after_update event. We cannot",
            "        update ViewMenus using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_update.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: Permission",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new permission",
            "        is created by set_perm.",
            "",
            "        Since set_perm is executed by SQLAlchemy after_insert events, we cannot",
            "        create new permissions using a session, so any SQLAlchemy events hooked to",
            "        `Permission` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_view_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: PermissionView",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new PermissionView",
            "        is created by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new pvms using a session, so any SQLAlchemy events hooked to",
            "        `PermissionView` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_view_after_delete(",
            "        self, mapper: Mapper, connection: Connection, target: PermissionView",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new PermissionView",
            "        is delete by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_delete events, we cannot",
            "        delete pvms using a session, so any SQLAlchemy events hooked to",
            "        `PermissionView` will not trigger an after_delete.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    @staticmethod",
            "    def get_exclude_users_from_lists() -> list[str]:",
            "        \"\"\"",
            "        Override to dynamically identify a list of usernames to exclude from",
            "        all UI dropdown lists, owners, created_by filters etc...",
            "",
            "        It will exclude all users from the all endpoints of the form",
            "        ``/api/v1/<modelview>/related/<column>``",
            "",
            "        Optionally you can also exclude them using the `EXCLUDE_USERS_FROM_LISTS`",
            "        config setting.",
            "",
            "        :return: A list of usernames",
            "        \"\"\"",
            "        return []",
            "",
            "    def raise_for_access(",
            "        # pylint: disable=too-many-arguments,too-many-branches,too-many-locals",
            "        self,",
            "        dashboard: Optional[\"Dashboard\"] = None,",
            "        database: Optional[\"Database\"] = None,",
            "        datasource: Optional[\"BaseDatasource\"] = None,",
            "        query: Optional[\"Query\"] = None,",
            "        query_context: Optional[\"QueryContext\"] = None,",
            "        table: Optional[\"Table\"] = None,",
            "        viz: Optional[\"BaseViz\"] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :param database: The Superset database",
            "        :param datasource: The Superset datasource",
            "        :param query: The SQL Lab query",
            "        :param query_context: The query context",
            "        :param table: The Superset table (requires database)",
            "        :param viz: The visualization",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import is_feature_enabled",
            "        from superset.connectors.sqla.models import SqlaTable",
            "        from superset.models.dashboard import Dashboard",
            "        from superset.models.slice import Slice",
            "        from superset.sql_parse import Table",
            "",
            "        if database and table or query:",
            "            if query:",
            "                database = query.database",
            "",
            "            database = cast(\"Database\", database)",
            "",
            "            if self.can_access_database(database):",
            "                return",
            "",
            "            if query:",
            "                default_schema = database.get_default_schema_for_query(query)",
            "                tables = {",
            "                    Table(table_.table, table_.schema or default_schema)",
            "                    for table_ in sql_parse.ParsedQuery(query.sql).tables",
            "                }",
            "            elif table:",
            "                tables = {table}",
            "",
            "            denied = set()",
            "",
            "            for table_ in tables:",
            "                schema_perm = self.get_schema_perm(database, schema=table_.schema)",
            "",
            "                if not (schema_perm and self.can_access(\"schema_access\", schema_perm)):",
            "                    datasources = SqlaTable.query_datasources_by_name(",
            "                        self.get_session, database, table_.table, schema=table_.schema",
            "                    )",
            "",
            "                    # Access to any datasource is suffice.",
            "                    for datasource_ in datasources:",
            "                        if self.can_access(",
            "                            \"datasource_access\", datasource_.perm",
            "                        ) or self.is_owner(datasource_):",
            "                            break",
            "                    else:",
            "                        denied.add(table_)",
            "",
            "            if denied:",
            "                raise SupersetSecurityException(",
            "                    self.get_table_access_error_object(denied)",
            "                )",
            "",
            "        if datasource or query_context or viz:",
            "            form_data = None",
            "",
            "            if query_context:",
            "                datasource = query_context.datasource",
            "                form_data = query_context.form_data",
            "            elif viz:",
            "                datasource = viz.datasource",
            "                form_data = viz.form_data",
            "",
            "            assert datasource",
            "",
            "            if not (",
            "                self.can_access_schema(datasource)",
            "                or self.can_access(\"datasource_access\", datasource.perm or \"\")",
            "                or self.is_owner(datasource)",
            "                or (",
            "                    # Grant access to the datasource only if dashboard RBAC is enabled",
            "                    # and said datasource is associated with the dashboard chart in",
            "                    # question.",
            "                    form_data",
            "                    and is_feature_enabled(\"DASHBOARD_RBAC\")",
            "                    and (dashboard_id := form_data.get(\"dashboardId\"))",
            "                    and (",
            "                        dashboard_ := self.get_session.query(Dashboard)",
            "                        .filter(Dashboard.id == dashboard_id)",
            "                        .one_or_none()",
            "                    )",
            "                    and dashboard_.roles",
            "                    and (",
            "                        (",
            "                            # Native filter.",
            "                            form_data.get(\"type\") == \"NATIVE_FILTER\"",
            "                            and (native_filter_id := form_data.get(\"native_filter_id\"))",
            "                            and dashboard_.json_metadata",
            "                            and (json_metadata := json.loads(dashboard_.json_metadata))",
            "                            and any(",
            "                                target.get(\"datasetId\") == datasource.id",
            "                                for fltr in json_metadata.get(",
            "                                    \"native_filter_configuration\",",
            "                                    [],",
            "                                )",
            "                                for target in fltr.get(\"targets\", [])",
            "                                if native_filter_id == fltr.get(\"id\")",
            "                            )",
            "                        )",
            "                        or (",
            "                            # Chart.",
            "                            form_data.get(\"type\") != \"NATIVE_FILTER\"",
            "                            and (slice_id := form_data.get(\"slice_id\"))",
            "                            and (",
            "                                slc := self.get_session.query(Slice)",
            "                                .filter(Slice.id == slice_id)",
            "                                .one_or_none()",
            "                            )",
            "                            and slc in dashboard_.slices",
            "                            and slc.datasource == datasource",
            "                        )",
            "                    )",
            "                    and self.can_access_dashboard(dashboard_)",
            "                )",
            "            ):",
            "                raise SupersetSecurityException(",
            "                    self.get_datasource_access_error_object(datasource)",
            "                )",
            "",
            "        if dashboard:",
            "            if self.is_guest_user():",
            "                # Guest user is currently used for embedded dashboards only. If the guest",
            "                # user doesn't have access to the dashboard, ignore all other checks.",
            "                if self.has_guest_access(dashboard):",
            "                    return",
            "                raise SupersetSecurityException(",
            "                    self.get_dashboard_access_error_object(dashboard)",
            "                )",
            "",
            "            if self.is_admin() or self.is_owner(dashboard):",
            "                return",
            "",
            "            # RBAC and legacy (datasource inferred) access controls.",
            "            if is_feature_enabled(\"DASHBOARD_RBAC\") and dashboard.roles:",
            "                if dashboard.published and {role.id for role in dashboard.roles} & {",
            "                    role.id for role in self.get_user_roles()",
            "                }:",
            "                    return",
            "            elif (",
            "                # To understand why we rely on status and give access to draft dashboards",
            "                # without roles take a look at:",
            "                #",
            "                #   - https://github.com/apache/superset/pull/24350#discussion_r1225061550",
            "                #   - https://github.com/apache/superset/pull/17511#issuecomment-975870169",
            "                #",
            "                not dashboard.published",
            "                or not dashboard.datasources",
            "                or any(",
            "                    self.can_access_datasource(datasource)",
            "                    for datasource in dashboard.datasources",
            "                )",
            "            ):",
            "                return",
            "",
            "            raise SupersetSecurityException(",
            "                self.get_dashboard_access_error_object(dashboard)",
            "            )",
            "",
            "    def get_user_by_username(",
            "        self, username: str, session: Session = None",
            "    ) -> Optional[User]:",
            "        \"\"\"",
            "        Retrieves a user by it's username case sensitive. Optional session parameter",
            "        utility method normally useful for celery tasks where the session",
            "        need to be scoped",
            "        \"\"\"",
            "        session = session or self.get_session",
            "        return (",
            "            session.query(self.user_model)",
            "            .filter(self.user_model.username == username)",
            "            .one_or_none()",
            "        )",
            "",
            "    def get_anonymous_user(self) -> User:  # pylint: disable=no-self-use",
            "        return AnonymousUserMixin()",
            "",
            "    def get_user_roles(self, user: Optional[User] = None) -> list[Role]:",
            "        if not user:",
            "            user = g.user",
            "        if user.is_anonymous:",
            "            public_role = current_app.config.get(\"AUTH_ROLE_PUBLIC\")",
            "            return [self.get_public_role()] if public_role else []",
            "        return user.roles",
            "",
            "    def get_guest_rls_filters(",
            "        self, dataset: \"BaseDatasource\"",
            "    ) -> list[GuestTokenRlsRule]:",
            "        \"\"\"",
            "        Retrieves the row level security filters for the current user and the dataset,",
            "        if the user is authenticated with a guest token.",
            "        :param dataset: The dataset to check against",
            "        :return: A list of filters",
            "        \"\"\"",
            "        if guest_user := self.get_current_guest_user_if_guest():",
            "            return [",
            "                rule",
            "                for rule in guest_user.rls",
            "                if not rule.get(\"dataset\")",
            "                or str(rule.get(\"dataset\")) == str(dataset.id)",
            "            ]",
            "        return []",
            "",
            "    def get_rls_filters(self, table: \"BaseDatasource\") -> list[SqlaQuery]:",
            "        \"\"\"",
            "        Retrieves the appropriate row level security filters for the current user and",
            "        the passed table.",
            "",
            "        :param table: The table to check against",
            "        :returns: A list of filters",
            "        \"\"\"",
            "",
            "        if not (hasattr(g, \"user\") and g.user is not None):",
            "            return []",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import (",
            "            RLSFilterRoles,",
            "            RLSFilterTables,",
            "            RowLevelSecurityFilter,",
            "        )",
            "",
            "        user_roles = [role.id for role in self.get_user_roles(g.user)]",
            "        regular_filter_roles = (",
            "            self.get_session()",
            "            .query(RLSFilterRoles.c.rls_filter_id)",
            "            .join(RowLevelSecurityFilter)",
            "            .filter(",
            "                RowLevelSecurityFilter.filter_type == RowLevelSecurityFilterType.REGULAR",
            "            )",
            "            .filter(RLSFilterRoles.c.role_id.in_(user_roles))",
            "        )",
            "        base_filter_roles = (",
            "            self.get_session()",
            "            .query(RLSFilterRoles.c.rls_filter_id)",
            "            .join(RowLevelSecurityFilter)",
            "            .filter(",
            "                RowLevelSecurityFilter.filter_type == RowLevelSecurityFilterType.BASE",
            "            )",
            "            .filter(RLSFilterRoles.c.role_id.in_(user_roles))",
            "        )",
            "        filter_tables = (",
            "            self.get_session()",
            "            .query(RLSFilterTables.c.rls_filter_id)",
            "            .filter(RLSFilterTables.c.table_id == table.id)",
            "        )",
            "        query = (",
            "            self.get_session()",
            "            .query(",
            "                RowLevelSecurityFilter.id,",
            "                RowLevelSecurityFilter.group_key,",
            "                RowLevelSecurityFilter.clause,",
            "            )",
            "            .filter(RowLevelSecurityFilter.id.in_(filter_tables))",
            "            .filter(",
            "                or_(",
            "                    and_(",
            "                        RowLevelSecurityFilter.filter_type",
            "                        == RowLevelSecurityFilterType.REGULAR,",
            "                        RowLevelSecurityFilter.id.in_(regular_filter_roles),",
            "                    ),",
            "                    and_(",
            "                        RowLevelSecurityFilter.filter_type",
            "                        == RowLevelSecurityFilterType.BASE,",
            "                        RowLevelSecurityFilter.id.notin_(base_filter_roles),",
            "                    ),",
            "                )",
            "            )",
            "        )",
            "        return query.all()",
            "",
            "    def get_rls_ids(self, table: \"BaseDatasource\") -> list[int]:",
            "        \"\"\"",
            "        Retrieves the appropriate row level security filters IDs for the current user",
            "        and the passed table.",
            "",
            "        :param table: The table to check against",
            "        :returns: A list of IDs",
            "        \"\"\"",
            "        ids = [f.id for f in self.get_rls_filters(table)]",
            "        ids.sort()  # Combinations rather than permutations",
            "        return ids",
            "",
            "    def get_guest_rls_filters_str(self, table: \"BaseDatasource\") -> list[str]:",
            "        return [f.get(\"clause\", \"\") for f in self.get_guest_rls_filters(table)]",
            "",
            "    def get_rls_cache_key(self, datasource: \"BaseDatasource\") -> list[str]:",
            "        rls_ids = []",
            "        if datasource.is_rls_supported:",
            "            rls_ids = self.get_rls_ids(datasource)",
            "        rls_str = [str(rls_id) for rls_id in rls_ids]",
            "        guest_rls = self.get_guest_rls_filters_str(datasource)",
            "        return guest_rls + rls_str",
            "",
            "    @staticmethod",
            "    def _get_current_epoch_time() -> float:",
            "        \"\"\"This is used so the tests can mock time\"\"\"",
            "        return time.time()",
            "",
            "    @staticmethod",
            "    def _get_guest_token_jwt_audience() -> str:",
            "        audience = current_app.config[\"GUEST_TOKEN_JWT_AUDIENCE\"] or get_url_host()",
            "        if callable(audience):",
            "            audience = audience()",
            "        return audience",
            "",
            "    @staticmethod",
            "    def validate_guest_token_resources(resources: GuestTokenResources) -> None:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.daos.dashboard import EmbeddedDashboardDAO",
            "        from superset.embedded_dashboard.commands.exceptions import (",
            "            EmbeddedDashboardNotFoundError,",
            "        )",
            "        from superset.models.dashboard import Dashboard",
            "",
            "        for resource in resources:",
            "            if resource[\"type\"] == GuestTokenResourceType.DASHBOARD.value:",
            "                # TODO (embedded): remove this check once uuids are rolled out",
            "                dashboard = Dashboard.get(str(resource[\"id\"]))",
            "                if not dashboard:",
            "                    embedded = EmbeddedDashboardDAO.find_by_id(str(resource[\"id\"]))",
            "                    if not embedded:",
            "                        raise EmbeddedDashboardNotFoundError()",
            "",
            "    def create_guest_access_token(",
            "        self,",
            "        user: GuestTokenUser,",
            "        resources: GuestTokenResources,",
            "        rls: list[GuestTokenRlsRule],",
            "    ) -> bytes:",
            "        secret = current_app.config[\"GUEST_TOKEN_JWT_SECRET\"]",
            "        algo = current_app.config[\"GUEST_TOKEN_JWT_ALGO\"]",
            "        exp_seconds = current_app.config[\"GUEST_TOKEN_JWT_EXP_SECONDS\"]",
            "        audience = self._get_guest_token_jwt_audience()",
            "        # calculate expiration time",
            "        now = self._get_current_epoch_time()",
            "        exp = now + exp_seconds",
            "        claims = {",
            "            \"user\": user,",
            "            \"resources\": resources,",
            "            \"rls_rules\": rls,",
            "            # standard jwt claims:",
            "            \"iat\": now,  # issued at",
            "            \"exp\": exp,  # expiration time",
            "            \"aud\": audience,",
            "            \"type\": \"guest\",",
            "        }",
            "        token = self.pyjwt_for_guest_token.encode(claims, secret, algorithm=algo)",
            "        return token",
            "",
            "    def get_guest_user_from_request(self, req: Request) -> Optional[GuestUser]:",
            "        \"\"\"",
            "        If there is a guest token in the request (used for embedded),",
            "        parses the token and returns the guest user.",
            "        This is meant to be used as a request loader for the LoginManager.",
            "        The LoginManager will only call this if an active session cannot be found.",
            "",
            "        :return: A guest user object",
            "        \"\"\"",
            "        raw_token = req.headers.get(",
            "            current_app.config[\"GUEST_TOKEN_HEADER_NAME\"]",
            "        ) or req.form.get(\"guest_token\")",
            "        if raw_token is None:",
            "            return None",
            "",
            "        try:",
            "            token = self.parse_jwt_guest_token(raw_token)",
            "            if token.get(\"user\") is None:",
            "                raise ValueError(\"Guest token does not contain a user claim\")",
            "            if token.get(\"resources\") is None:",
            "                raise ValueError(\"Guest token does not contain a resources claim\")",
            "            if token.get(\"rls_rules\") is None:",
            "                raise ValueError(\"Guest token does not contain an rls_rules claim\")",
            "            if token.get(\"type\") != \"guest\":",
            "                raise ValueError(\"This is not a guest token.\")",
            "        except Exception:  # pylint: disable=broad-except",
            "            # The login manager will handle sending 401s.",
            "            # We don't need to send a special error message.",
            "            logger.warning(\"Invalid guest token\", exc_info=True)",
            "            return None",
            "        else:",
            "            return self.get_guest_user_from_token(cast(GuestToken, token))",
            "",
            "    def get_guest_user_from_token(self, token: GuestToken) -> GuestUser:",
            "        return self.guest_user_cls(",
            "            token=token,",
            "            roles=[self.find_role(current_app.config[\"GUEST_ROLE_NAME\"])],",
            "        )",
            "",
            "    def parse_jwt_guest_token(self, raw_token: str) -> dict[str, Any]:",
            "        \"\"\"",
            "        Parses a guest token. Raises an error if the jwt fails standard claims checks.",
            "        :param raw_token: the token gotten from the request",
            "        :return: the same token that was passed in, tested but unchanged",
            "        \"\"\"",
            "        secret = current_app.config[\"GUEST_TOKEN_JWT_SECRET\"]",
            "        algo = current_app.config[\"GUEST_TOKEN_JWT_ALGO\"]",
            "        audience = self._get_guest_token_jwt_audience()",
            "        return self.pyjwt_for_guest_token.decode(",
            "            raw_token, secret, algorithms=[algo], audience=audience",
            "        )",
            "",
            "    @staticmethod",
            "    def is_guest_user(user: Optional[Any] = None) -> bool:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import is_feature_enabled",
            "",
            "        if not is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "            return False",
            "        if not user:",
            "            user = g.user",
            "        return hasattr(user, \"is_guest_user\") and user.is_guest_user",
            "",
            "    def get_current_guest_user_if_guest(self) -> Optional[GuestUser]:",
            "        if self.is_guest_user():",
            "            return g.user",
            "        return None",
            "",
            "    def has_guest_access(self, dashboard: \"Dashboard\") -> bool:",
            "        user = self.get_current_guest_user_if_guest()",
            "        if not user:",
            "            return False",
            "",
            "        dashboards = [",
            "            r",
            "            for r in user.resources",
            "            if r[\"type\"] == GuestTokenResourceType.DASHBOARD.value",
            "        ]",
            "",
            "        # TODO (embedded): remove this check once uuids are rolled out",
            "        for resource in dashboards:",
            "            if str(resource[\"id\"]) == str(dashboard.id):",
            "                return True",
            "",
            "        if not dashboard.embedded:",
            "            return False",
            "",
            "        for resource in dashboards:",
            "            if str(resource[\"id\"]) == str(dashboard.embedded[0].uuid):",
            "                return True",
            "        return False",
            "",
            "    def raise_for_ownership(self, resource: Model) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user does not own the resource.",
            "",
            "        Note admins are deemed owners of all resources.",
            "",
            "        :param resource: The dashboard, dataste, chart, etc. resource",
            "        :raises SupersetSecurityException: If the current user is not an owner",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import db",
            "",
            "        if self.is_admin():",
            "            return",
            "",
            "        orig_resource = db.session.query(resource.__class__).get(resource.id)",
            "        owners = orig_resource.owners if hasattr(orig_resource, \"owners\") else []",
            "",
            "        if g.user.is_anonymous or g.user not in owners:",
            "            raise SupersetSecurityException(",
            "                SupersetError(",
            "                    error_type=SupersetErrorType.MISSING_OWNERSHIP_ERROR,",
            "                    message=_(",
            "                        \"You don't have the rights to alter %(resource)s\",",
            "                        resource=resource,",
            "                    ),",
            "                    level=ErrorLevel.ERROR,",
            "                )",
            "            )",
            "",
            "    def is_owner(self, resource: Model) -> bool:",
            "        \"\"\"",
            "        Returns True if the current user is an owner of the resource, False otherwise.",
            "",
            "        :param resource: The dashboard, dataste, chart, etc. resource",
            "        :returns: Whethe the current user is an owner of the resource",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_ownership(resource)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def is_admin(self) -> bool:",
            "        \"\"\"",
            "        Returns True if the current user is an admin user, False otherwise.",
            "",
            "        :returns: Whehther the current user is an admin user",
            "        \"\"\"",
            "",
            "        return current_app.config[\"AUTH_ROLE_ADMIN\"] in [",
            "            role.name for role in self.get_user_roles()",
            "        ]"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "\"\"\"A set of constants and methods to manage permissions and security\"\"\"",
            "import json",
            "import logging",
            "import re",
            "import time",
            "from collections import defaultdict",
            "from typing import Any, Callable, cast, NamedTuple, Optional, TYPE_CHECKING, Union",
            "",
            "from flask import current_app, Flask, g, Request",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.security.sqla.manager import SecurityManager",
            "from flask_appbuilder.security.sqla.models import (",
            "    assoc_permissionview_role,",
            "    assoc_user_role,",
            "    Permission,",
            "    PermissionView,",
            "    Role,",
            "    User,",
            "    ViewMenu,",
            ")",
            "from flask_appbuilder.security.views import (",
            "    PermissionModelView,",
            "    PermissionViewModelView,",
            "    RoleModelView,",
            "    UserModelView,",
            "    ViewMenuModelView,",
            ")",
            "from flask_appbuilder.widgets import ListWidget",
            "from flask_babel import lazy_gettext as _",
            "from flask_login import AnonymousUserMixin, LoginManager",
            "from jwt.api_jwt import _jwt_global_obj",
            "from sqlalchemy import and_, inspect, or_",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.orm.query import Query as SqlaQuery",
            "",
            "from superset import sql_parse",
            "from superset.constants import RouteMethod",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    DatasetInvalidPermissionEvaluationException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.security.guest_token import (",
            "    GuestToken,",
            "    GuestTokenResources,",
            "    GuestTokenResourceType,",
            "    GuestTokenRlsRule,",
            "    GuestTokenUser,",
            "    GuestUser,",
            ")",
            "from superset.utils.core import (",
            "    DatasourceName,",
            "    DatasourceType,",
            "    get_user_id,",
            "    RowLevelSecurityFilterType,",
            ")",
            "from superset.utils.filters import get_dataset_access_filters",
            "from superset.utils.urls import get_url_host",
            "",
            "if TYPE_CHECKING:",
            "    from superset.common.query_context import QueryContext",
            "    from superset.connectors.base.models import BaseDatasource",
            "    from superset.connectors.sqla.models import SqlaTable",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.sql_lab import Query",
            "    from superset.sql_parse import Table",
            "    from superset.viz import BaseViz",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "DATABASE_PERM_REGEX = re.compile(r\"^\\[.+\\]\\.\\(id\\:(?P<id>\\d+)\\)$\")",
            "",
            "",
            "class DatabaseAndSchema(NamedTuple):",
            "    database: str",
            "    schema: str",
            "",
            "",
            "class SupersetSecurityListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Redeclaring to avoid circular imports",
            "    \"\"\"",
            "",
            "    template = \"superset/fab_overrides/list.html\"",
            "",
            "",
            "class SupersetRoleListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Role model view from FAB already uses a custom list widget override",
            "    So we override the override",
            "    \"\"\"",
            "",
            "    template = \"superset/fab_overrides/list_role.html\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        kwargs[\"appbuilder\"] = current_app.appbuilder",
            "        super().__init__(**kwargs)",
            "",
            "",
            "UserModelView.list_widget = SupersetSecurityListWidget",
            "RoleModelView.list_widget = SupersetRoleListWidget",
            "PermissionViewModelView.list_widget = SupersetSecurityListWidget",
            "PermissionModelView.list_widget = SupersetSecurityListWidget",
            "",
            "# Limiting routes on FAB model views",
            "UserModelView.include_route_methods = RouteMethod.CRUD_SET | {",
            "    RouteMethod.ACTION,",
            "    RouteMethod.API_READ,",
            "    RouteMethod.ACTION_POST,",
            "    \"userinfo\",",
            "}",
            "RoleModelView.include_route_methods = RouteMethod.CRUD_SET",
            "PermissionViewModelView.include_route_methods = {RouteMethod.LIST}",
            "PermissionModelView.include_route_methods = {RouteMethod.LIST}",
            "ViewMenuModelView.include_route_methods = {RouteMethod.LIST}",
            "",
            "RoleModelView.list_columns = [\"name\"]",
            "RoleModelView.edit_columns = [\"name\", \"permissions\", \"user\"]",
            "RoleModelView.related_views = []",
            "",
            "",
            "class SupersetSecurityManager(  # pylint: disable=too-many-public-methods",
            "    SecurityManager",
            "):",
            "    userstatschartview = None",
            "    READ_ONLY_MODEL_VIEWS = {\"Database\", \"DynamicPlugin\"}",
            "",
            "    USER_MODEL_VIEWS = {",
            "        \"RegisterUserModelView\",",
            "        \"UserDBModelView\",",
            "        \"UserLDAPModelView\",",
            "        \"UserInfoEditView\",",
            "        \"UserOAuthModelView\",",
            "        \"UserOIDModelView\",",
            "        \"UserRemoteUserModelView\",",
            "    }",
            "",
            "    GAMMA_READ_ONLY_MODEL_VIEWS = {",
            "        \"Dataset\",",
            "        \"Datasource\",",
            "    } | READ_ONLY_MODEL_VIEWS",
            "",
            "    ADMIN_ONLY_VIEW_MENUS = {",
            "        \"Access Requests\",",
            "        \"Action Log\",",
            "        \"Log\",",
            "        \"List Users\",",
            "        \"List Roles\",",
            "        \"ResetPasswordView\",",
            "        \"RoleModelView\",",
            "        \"Row Level Security\",",
            "        \"Row Level Security Filters\",",
            "        \"RowLevelSecurityFiltersModelView\",",
            "        \"Security\",",
            "        \"SQL Lab\",",
            "        \"User Registrations\",",
            "        \"User's Statistics\",",
            "    } | USER_MODEL_VIEWS",
            "",
            "    ALPHA_ONLY_VIEW_MENUS = {",
            "        \"Alerts & Report\",",
            "        \"Annotation Layers\",",
            "        \"Annotation\",",
            "        \"CSS Templates\",",
            "        \"ColumnarToDatabaseView\",",
            "        \"CssTemplate\",",
            "        \"CsvToDatabaseView\",",
            "        \"ExcelToDatabaseView\",",
            "        \"Import dashboards\",",
            "        \"ImportExportRestApi\",",
            "        \"Manage\",",
            "        \"Queries\",",
            "        \"ReportSchedule\",",
            "        \"TableSchemaView\",",
            "        \"Upload a CSV\",",
            "    }",
            "",
            "    ADMIN_ONLY_PERMISSIONS = {",
            "        \"can_update_role\",",
            "        \"all_query_access\",",
            "        \"can_grant_guest_token\",",
            "        \"can_set_embedded\",",
            "        \"can_warm_up_cache\",",
            "    }",
            "",
            "    READ_ONLY_PERMISSION = {",
            "        \"can_show\",",
            "        \"can_list\",",
            "        \"can_get\",",
            "        \"can_external_metadata\",",
            "        \"can_external_metadata_by_name\",",
            "        \"can_read\",",
            "    }",
            "",
            "    ALPHA_ONLY_PERMISSIONS = {",
            "        \"muldelete\",",
            "        \"all_database_access\",",
            "        \"all_datasource_access\",",
            "    }",
            "",
            "    OBJECT_SPEC_PERMISSIONS = {",
            "        \"database_access\",",
            "        \"schema_access\",",
            "        \"datasource_access\",",
            "    }",
            "",
            "    ACCESSIBLE_PERMS = {\"can_userinfo\", \"resetmypassword\", \"can_recent_activity\"}",
            "",
            "    SQLLAB_ONLY_PERMISSIONS = {",
            "        (\"can_my_queries\", \"SqlLab\"),",
            "        (\"can_read\", \"SavedQuery\"),",
            "        (\"can_write\", \"SavedQuery\"),",
            "        (\"can_export\", \"SavedQuery\"),",
            "        (\"can_read\", \"Query\"),",
            "        (\"can_export_csv\", \"Query\"),",
            "        (\"can_get_results\", \"SQLLab\"),",
            "        (\"can_execute_sql_query\", \"SQLLab\"),",
            "        (\"can_estimate_query_cost\", \"SQL Lab\"),",
            "        (\"can_export_csv\", \"SQLLab\"),",
            "        (\"can_sqllab_history\", \"Superset\"),",
            "        (\"can_sqllab\", \"Superset\"),",
            "        (\"can_test_conn\", \"Superset\"),  # Deprecated permission remove on 3.0.0",
            "        (\"can_activate\", \"TabStateView\"),",
            "        (\"can_get\", \"TabStateView\"),",
            "        (\"can_delete_query\", \"TabStateView\"),",
            "        (\"can_post\", \"TabStateView\"),",
            "        (\"can_delete\", \"TabStateView\"),",
            "        (\"can_put\", \"TabStateView\"),",
            "        (\"can_migrate_query\", \"TabStateView\"),",
            "        (\"menu_access\", \"SQL Lab\"),",
            "        (\"menu_access\", \"SQL Editor\"),",
            "        (\"menu_access\", \"Saved Queries\"),",
            "        (\"menu_access\", \"Query Search\"),",
            "    }",
            "",
            "    SQLLAB_EXTRA_PERMISSION_VIEWS = {",
            "        (\"can_csv\", \"Superset\"),  # Deprecated permission remove on 3.0.0",
            "        (\"can_read\", \"Superset\"),",
            "        (\"can_read\", \"Database\"),",
            "    }",
            "",
            "    data_access_permissions = (",
            "        \"database_access\",",
            "        \"schema_access\",",
            "        \"datasource_access\",",
            "        \"all_datasource_access\",",
            "        \"all_database_access\",",
            "        \"all_query_access\",",
            "    )",
            "",
            "    guest_user_cls = GuestUser",
            "    pyjwt_for_guest_token = _jwt_global_obj",
            "",
            "    def create_login_manager(self, app: Flask) -> LoginManager:",
            "        lm = super().create_login_manager(app)",
            "        lm.request_loader(self.request_loader)",
            "        return lm",
            "",
            "    def request_loader(self, request: Request) -> Optional[User]:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.extensions import feature_flag_manager",
            "",
            "        if feature_flag_manager.is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "            return self.get_guest_user_from_request(request)",
            "        return None",
            "",
            "    def get_schema_perm(  # pylint: disable=no-self-use",
            "        self, database: Union[\"Database\", str], schema: Optional[str] = None",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the database specific schema permission.",
            "",
            "        :param database: The Superset database or database name",
            "        :param schema: The Superset schema name",
            "        :return: The database specific schema permission",
            "        \"\"\"",
            "",
            "        if schema:",
            "            return f\"[{database}].[{schema}]\"",
            "",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_database_perm(database_id: int, database_name: str) -> str:",
            "        return f\"[{database_name}].(id:{database_id})\"",
            "",
            "    @staticmethod",
            "    def get_dataset_perm(dataset_id: int, dataset_name: str, database_name: str) -> str:",
            "        return f\"[{database_name}].[{dataset_name}](id:{dataset_id})\"",
            "",
            "    def unpack_database_and_schema(  # pylint: disable=no-self-use",
            "        self, schema_permission: str",
            "    ) -> DatabaseAndSchema:",
            "        # [database_name].[schema|table]",
            "",
            "        schema_name = schema_permission.split(\".\")[1][1:-1]",
            "        database_name = schema_permission.split(\".\")[0][1:-1]",
            "        return DatabaseAndSchema(database_name, schema_name)",
            "",
            "    def can_access(self, permission_name: str, view_name: str) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the FAB permission/view, False otherwise.",
            "",
            "        Note this method adds protection from has_access failing from missing",
            "        permission/view entries.",
            "",
            "        :param permission_name: The FAB permission name",
            "        :param view_name: The FAB view-menu name",
            "        :returns: Whether the user can access the FAB permission/view",
            "        \"\"\"",
            "",
            "        user = g.user",
            "        if user.is_anonymous:",
            "            return self.is_item_public(permission_name, view_name)",
            "        return self._has_view_access(user, permission_name, view_name)",
            "",
            "    def can_access_all_queries(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all SQL Lab queries, False otherwise.",
            "",
            "        :returns: Whether the user can access all queries",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_query_access\", \"all_query_access\")",
            "",
            "    def can_access_all_datasources(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all the datasources, False otherwise.",
            "",
            "        :returns: Whether the user can access all the datasources",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_datasource_access\", \"all_datasource_access\")",
            "",
            "    def can_access_all_databases(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all the databases, False otherwise.",
            "",
            "        :returns: Whether the user can access all the databases",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_database_access\", \"all_database_access\")",
            "",
            "    def can_access_database(self, database: \"Database\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified database, False otherwise.",
            "",
            "        :param database: The database",
            "        :returns: Whether the user can access the database",
            "        \"\"\"",
            "",
            "        return (",
            "            self.can_access_all_datasources()",
            "            or self.can_access_all_databases()",
            "            or self.can_access(\"database_access\", database.perm)  # type: ignore",
            "        )",
            "",
            "    def can_access_schema(self, datasource: \"BaseDatasource\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the schema associated with specified",
            "        datasource, False otherwise.",
            "",
            "        :param datasource: The datasource",
            "        :returns: Whether the user can access the datasource's schema",
            "        \"\"\"",
            "",
            "        return (",
            "            self.can_access_all_datasources()",
            "            or self.can_access_database(datasource.database)",
            "            or self.can_access(\"schema_access\", datasource.schema_perm or \"\")",
            "        )",
            "",
            "    def can_access_datasource(self, datasource: \"BaseDatasource\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified datasource, False otherwise.",
            "",
            "        :param datasource: The datasource",
            "        :returns: Whether the user can access the datasource",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(datasource=datasource)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def can_access_dashboard(self, dashboard: \"Dashboard\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified dashboard, False otherwise.",
            "",
            "        :param dashboard: The dashboard",
            "        :returns: Whether the user can access the dashboard",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(dashboard=dashboard)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    # pylint: disable=no-self-use",
            "    def get_dashboard_access_error_object(  # pylint: disable=invalid-name",
            "        self,",
            "        dashboard: \"Dashboard\",  # pylint: disable=unused-argument",
            "    ) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied Superset dashboard.",
            "",
            "        :param dashboard: The denied Superset dashboard",
            "        :returns: The error object",
            "        \"\"\"",
            "",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.DASHBOARD_SECURITY_ACCESS_ERROR,",
            "            message=\"You don't have access to this dashboard.\",",
            "            level=ErrorLevel.ERROR,",
            "        )",
            "",
            "    @staticmethod",
            "    def get_datasource_access_error_msg(datasource: \"BaseDatasource\") -> str:",
            "        \"\"\"",
            "        Return the error message for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The error message",
            "        \"\"\"",
            "",
            "        return (",
            "            f\"This endpoint requires the datasource {datasource.name}, \"",
            "            \"database or `all_datasource_access` permission\"",
            "        )",
            "",
            "    @staticmethod",
            "    def get_datasource_access_link(  # pylint: disable=unused-argument",
            "        datasource: \"BaseDatasource\",",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the link for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The access URL",
            "        \"\"\"",
            "",
            "        return current_app.config.get(\"PERMISSION_INSTRUCTIONS_LINK\")",
            "",
            "    def get_datasource_access_error_object(  # pylint: disable=invalid-name",
            "        self, datasource: \"BaseDatasource\"",
            "    ) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The error object",
            "        \"\"\"",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.DATASOURCE_SECURITY_ACCESS_ERROR,",
            "            message=self.get_datasource_access_error_msg(datasource),",
            "            level=ErrorLevel.ERROR,",
            "            extra={",
            "                \"link\": self.get_datasource_access_link(datasource),",
            "                \"datasource\": datasource.name,",
            "            },",
            "        )",
            "",
            "    def get_table_access_error_msg(  # pylint: disable=no-self-use",
            "        self, tables: set[\"Table\"]",
            "    ) -> str:",
            "        \"\"\"",
            "        Return the error message for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The error message",
            "        \"\"\"",
            "",
            "        quoted_tables = [f\"`{table}`\" for table in tables]",
            "        return f\"\"\"You need access to the following tables: {\", \".join(quoted_tables)},",
            "            `all_database_access` or `all_datasource_access` permission\"\"\"",
            "",
            "    def get_table_access_error_object(self, tables: set[\"Table\"]) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The error object",
            "        \"\"\"",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.TABLE_SECURITY_ACCESS_ERROR,",
            "            message=self.get_table_access_error_msg(tables),",
            "            level=ErrorLevel.ERROR,",
            "            extra={",
            "                \"link\": self.get_table_access_link(tables),",
            "                \"tables\": [str(table) for table in tables],",
            "            },",
            "        )",
            "",
            "    def get_table_access_link(  # pylint: disable=unused-argument,no-self-use",
            "        self, tables: set[\"Table\"]",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the access link for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The access URL",
            "        \"\"\"",
            "",
            "        return current_app.config.get(\"PERMISSION_INSTRUCTIONS_LINK\")",
            "",
            "    def get_user_datasources(self) -> list[\"BaseDatasource\"]:",
            "        \"\"\"",
            "        Collect datasources which the user has explicit permissions to.",
            "",
            "        :returns: The list of datasources",
            "        \"\"\"",
            "",
            "        user_datasources = set()",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        user_datasources.update(",
            "            self.get_session.query(SqlaTable)",
            "            .filter(get_dataset_access_filters(SqlaTable))",
            "            .all()",
            "        )",
            "",
            "        # group all datasources by database",
            "        session = self.get_session",
            "        all_datasources = SqlaTable.get_all_datasources(session)",
            "        datasources_by_database: dict[\"Database\", set[\"SqlaTable\"]] = defaultdict(set)",
            "        for datasource in all_datasources:",
            "            datasources_by_database[datasource.database].add(datasource)",
            "",
            "        # add datasources with implicit permission (eg, database access)",
            "        for database, datasources in datasources_by_database.items():",
            "            if self.can_access_database(database):",
            "                user_datasources.update(datasources)",
            "",
            "        return list(user_datasources)",
            "",
            "    def can_access_table(self, database: \"Database\", table: \"Table\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the SQL table, False otherwise.",
            "",
            "        :param database: The SQL database",
            "        :param table: The SQL table",
            "        :returns: Whether the user can access the SQL table",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(database=database, table=table)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def user_view_menu_names(self, permission_name: str) -> set[str]:",
            "        base_query = (",
            "            self.get_session.query(self.viewmenu_model.name)",
            "            .join(self.permissionview_model)",
            "            .join(self.permission_model)",
            "            .join(assoc_permissionview_role)",
            "            .join(self.role_model)",
            "        )",
            "",
            "        if not g.user.is_anonymous:",
            "            # filter by user id",
            "            view_menu_names = (",
            "                base_query.join(assoc_user_role)",
            "                .join(self.user_model)",
            "                .filter(self.user_model.id == get_user_id())",
            "                .filter(self.permission_model.name == permission_name)",
            "            ).all()",
            "            return {s.name for s in view_menu_names}",
            "",
            "        # Properly treat anonymous user",
            "        if public_role := self.get_public_role():",
            "            # filter by public role",
            "            view_menu_names = (",
            "                base_query.filter(self.role_model.id == public_role.id).filter(",
            "                    self.permission_model.name == permission_name",
            "                )",
            "            ).all()",
            "            return {s.name for s in view_menu_names}",
            "        return set()",
            "",
            "    def get_accessible_databases(self) -> list[int]:",
            "        \"\"\"",
            "        Return the list of databases accessible by the user.",
            "",
            "        :return: The list of accessible Databases",
            "        \"\"\"",
            "        perms = self.user_view_menu_names(\"database_access\")",
            "        return [",
            "            int(match.group(\"id\"))",
            "            for perm in perms",
            "            if (match := DATABASE_PERM_REGEX.match(perm))",
            "        ]",
            "",
            "    def get_schemas_accessible_by_user(",
            "        self, database: \"Database\", schemas: list[str], hierarchical: bool = True",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return the list of SQL schemas accessible by the user.",
            "",
            "        :param database: The SQL database",
            "        :param schemas: The list of eligible SQL schemas",
            "        :param hierarchical: Whether to check using the hierarchical permission logic",
            "        :returns: The list of accessible SQL schemas",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        if hierarchical and self.can_access_database(database):",
            "            return schemas",
            "",
            "        # schema_access",
            "        accessible_schemas = {",
            "            self.unpack_database_and_schema(s).schema",
            "            for s in self.user_view_menu_names(\"schema_access\")",
            "            if s.startswith(f\"[{database}].\")",
            "        }",
            "",
            "        # datasource_access",
            "        if perms := self.user_view_menu_names(\"datasource_access\"):",
            "            tables = (",
            "                self.get_session.query(SqlaTable.schema)",
            "                .filter(SqlaTable.database_id == database.id)",
            "                .filter(SqlaTable.schema.isnot(None))",
            "                .filter(SqlaTable.schema != \"\")",
            "                .filter(or_(SqlaTable.perm.in_(perms)))",
            "                .distinct()",
            "            )",
            "            accessible_schemas.update([table.schema for table in tables])",
            "",
            "        return [s for s in schemas if s in accessible_schemas]",
            "",
            "    def get_datasources_accessible_by_user(  # pylint: disable=invalid-name",
            "        self,",
            "        database: \"Database\",",
            "        datasource_names: list[DatasourceName],",
            "        schema: Optional[str] = None,",
            "    ) -> list[DatasourceName]:",
            "        \"\"\"",
            "        Return the list of SQL tables accessible by the user.",
            "",
            "        :param database: The SQL database",
            "        :param datasource_names: The list of eligible SQL tables w/ schema",
            "        :param schema: The fallback SQL schema if not present in the table name",
            "        :returns: The list of accessible SQL tables w/ schema",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        if self.can_access_database(database):",
            "            return datasource_names",
            "",
            "        if schema:",
            "            schema_perm = self.get_schema_perm(database, schema)",
            "            if schema_perm and self.can_access(\"schema_access\", schema_perm):",
            "                return datasource_names",
            "",
            "        user_perms = self.user_view_menu_names(\"datasource_access\")",
            "        schema_perms = self.user_view_menu_names(\"schema_access\")",
            "        user_datasources = SqlaTable.query_datasources_by_permissions(",
            "            self.get_session, database, user_perms, schema_perms",
            "        )",
            "        if schema:",
            "            names = {d.table_name for d in user_datasources if d.schema == schema}",
            "            return [d for d in datasource_names if d.table in names]",
            "",
            "        full_names = {d.full_name for d in user_datasources}",
            "        return [d for d in datasource_names if f\"[{database}].[{d}]\" in full_names]",
            "",
            "    def merge_perm(self, permission_name: str, view_menu_name: str) -> None:",
            "        \"\"\"",
            "        Add the FAB permission/view-menu.",
            "",
            "        :param permission_name: The FAB permission name",
            "        :param view_menu_names: The FAB view-menu name",
            "        :see: SecurityManager.add_permission_view_menu",
            "        \"\"\"",
            "",
            "        logger.warning(",
            "            \"This method 'merge_perm' is deprecated use add_permission_view_menu\"",
            "        )",
            "        self.add_permission_view_menu(permission_name, view_menu_name)",
            "",
            "    def _is_user_defined_permission(self, perm: Model) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission is user defined, False otherwise.",
            "",
            "        :param perm: The FAB permission",
            "        :returns: Whether the FAB permission is user defined",
            "        \"\"\"",
            "",
            "        return perm.permission.name in self.OBJECT_SPEC_PERMISSIONS",
            "",
            "    def create_custom_permissions(self) -> None:",
            "        \"\"\"",
            "        Create custom FAB permissions.",
            "        \"\"\"",
            "        self.add_permission_view_menu(\"all_datasource_access\", \"all_datasource_access\")",
            "        self.add_permission_view_menu(\"all_database_access\", \"all_database_access\")",
            "        self.add_permission_view_menu(\"all_query_access\", \"all_query_access\")",
            "        self.add_permission_view_menu(\"can_csv\", \"Superset\")",
            "        self.add_permission_view_menu(\"can_share_dashboard\", \"Superset\")",
            "        self.add_permission_view_menu(\"can_share_chart\", \"Superset\")",
            "",
            "    def create_missing_perms(self) -> None:",
            "        \"\"\"",
            "        Creates missing FAB permissions for datasources, schemas and metrics.",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "        from superset.models import core as models",
            "",
            "        logger.info(\"Fetching a set of all perms to lookup which ones are missing\")",
            "        all_pvs = set()",
            "        for pv in self.get_session.query(self.permissionview_model).all():",
            "            if pv.permission and pv.view_menu:",
            "                all_pvs.add((pv.permission.name, pv.view_menu.name))",
            "",
            "        def merge_pv(view_menu: str, perm: Optional[str]) -> None:",
            "            \"\"\"Create permission view menu only if it doesn't exist\"\"\"",
            "            if view_menu and perm and (view_menu, perm) not in all_pvs:",
            "                self.add_permission_view_menu(view_menu, perm)",
            "",
            "        logger.info(\"Creating missing datasource permissions.\")",
            "        datasources = SqlaTable.get_all_datasources(self.get_session)",
            "        for datasource in datasources:",
            "            merge_pv(\"datasource_access\", datasource.get_perm())",
            "            merge_pv(\"schema_access\", datasource.get_schema_perm())",
            "",
            "        logger.info(\"Creating missing database permissions.\")",
            "        databases = self.get_session.query(models.Database).all()",
            "        for database in databases:",
            "            merge_pv(\"database_access\", database.perm)",
            "",
            "    def clean_perms(self) -> None:",
            "        \"\"\"",
            "        Clean up the FAB faulty permissions.",
            "        \"\"\"",
            "",
            "        logger.info(\"Cleaning faulty perms\")",
            "        sesh = self.get_session",
            "        pvms = sesh.query(PermissionView).filter(",
            "            or_(",
            "                PermissionView.permission  # pylint: disable=singleton-comparison",
            "                == None,",
            "                PermissionView.view_menu  # pylint: disable=singleton-comparison",
            "                == None,",
            "            )",
            "        )",
            "        sesh.commit()",
            "        if deleted_count := pvms.delete():",
            "            logger.info(\"Deleted %i faulty permissions\", deleted_count)",
            "",
            "    def sync_role_definitions(self) -> None:",
            "        \"\"\"",
            "        Initialize the Superset application with security roles and such.",
            "        \"\"\"",
            "",
            "        logger.info(\"Syncing role definition\")",
            "",
            "        self.create_custom_permissions()",
            "",
            "        # Creating default roles",
            "        self.set_role(\"Admin\", self._is_admin_pvm)",
            "        self.set_role(\"Alpha\", self._is_alpha_pvm)",
            "        self.set_role(\"Gamma\", self._is_gamma_pvm)",
            "        self.set_role(\"sql_lab\", self._is_sql_lab_pvm)",
            "",
            "        # Configure public role",
            "        if current_app.config[\"PUBLIC_ROLE_LIKE\"]:",
            "            self.copy_role(",
            "                current_app.config[\"PUBLIC_ROLE_LIKE\"],",
            "                self.auth_role_public,",
            "                merge=True,",
            "            )",
            "",
            "        self.create_missing_perms()",
            "",
            "        # commit role and view menu updates",
            "        self.get_session.commit()",
            "        self.clean_perms()",
            "",
            "    def _get_pvms_from_builtin_role(self, role_name: str) -> list[PermissionView]:",
            "        \"\"\"",
            "        Gets a list of model PermissionView permissions inferred from a builtin role",
            "        definition",
            "        \"\"\"",
            "        role_from_permissions_names = self.builtin_roles.get(role_name, [])",
            "        all_pvms = self.get_session.query(PermissionView).all()",
            "        role_from_permissions = []",
            "        for pvm_regex in role_from_permissions_names:",
            "            view_name_regex = pvm_regex[0]",
            "            permission_name_regex = pvm_regex[1]",
            "            for pvm in all_pvms:",
            "                if re.match(view_name_regex, pvm.view_menu.name) and re.match(",
            "                    permission_name_regex, pvm.permission.name",
            "                ):",
            "                    if pvm not in role_from_permissions:",
            "                        role_from_permissions.append(pvm)",
            "        return role_from_permissions",
            "",
            "    def find_roles_by_id(self, role_ids: list[int]) -> list[Role]:",
            "        \"\"\"",
            "        Find a List of models by a list of ids, if defined applies `base_filter`",
            "        \"\"\"",
            "        query = self.get_session.query(Role).filter(Role.id.in_(role_ids))",
            "        return query.all()",
            "",
            "    def copy_role(",
            "        self, role_from_name: str, role_to_name: str, merge: bool = True",
            "    ) -> None:",
            "        \"\"\"",
            "        Copies permissions from a role to another.",
            "",
            "        Note: Supports regex defined builtin roles",
            "",
            "        :param role_from_name: The FAB role name from where the permissions are taken",
            "        :param role_to_name: The FAB role name from where the permissions are copied to",
            "        :param merge: If merge is true, keep data access permissions",
            "            if they already exist on the target role",
            "        \"\"\"",
            "",
            "        logger.info(\"Copy/Merge %s to %s\", role_from_name, role_to_name)",
            "        # If it's a builtin role extract permissions from it",
            "        if role_from_name in self.builtin_roles:",
            "            role_from_permissions = self._get_pvms_from_builtin_role(role_from_name)",
            "        else:",
            "            role_from_permissions = list(self.find_role(role_from_name).permissions)",
            "        role_to = self.add_role(role_to_name)",
            "        # If merge, recover existing data access permissions",
            "        if merge:",
            "            for permission_view in role_to.permissions:",
            "                if (",
            "                    permission_view not in role_from_permissions",
            "                    and permission_view.permission.name in self.data_access_permissions",
            "                ):",
            "                    role_from_permissions.append(permission_view)",
            "        role_to.permissions = role_from_permissions",
            "        self.get_session.merge(role_to)",
            "        self.get_session.commit()",
            "",
            "    def set_role(",
            "        self, role_name: str, pvm_check: Callable[[PermissionView], bool]",
            "    ) -> None:",
            "        \"\"\"",
            "        Set the FAB permission/views for the role.",
            "",
            "        :param role_name: The FAB role name",
            "        :param pvm_check: The FAB permission/view check",
            "        \"\"\"",
            "",
            "        logger.info(\"Syncing %s perms\", role_name)",
            "        pvms = self.get_session.query(PermissionView).all()",
            "        pvms = [p for p in pvms if p.permission and p.view_menu]",
            "        role = self.add_role(role_name)",
            "        role_pvms = [",
            "            permission_view for permission_view in pvms if pvm_check(permission_view)",
            "        ]",
            "        role.permissions = role_pvms",
            "        self.get_session.merge(role)",
            "        self.get_session.commit()",
            "",
            "    def _is_admin_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to only Admin users,",
            "        False otherwise.",
            "",
            "        Note readonly operations on read only model views are allowed only for admins.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to only Admin users",
            "        \"\"\"",
            "",
            "        if (",
            "            pvm.view_menu.name in self.READ_ONLY_MODEL_VIEWS",
            "            and pvm.permission.name not in self.READ_ONLY_PERMISSION",
            "        ):",
            "            return True",
            "        return (",
            "            pvm.view_menu.name in self.ADMIN_ONLY_VIEW_MENUS",
            "            or pvm.permission.name in self.ADMIN_ONLY_PERMISSIONS",
            "        )",
            "",
            "    def _is_alpha_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to only Alpha users,",
            "        False otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to only Alpha users",
            "        \"\"\"",
            "",
            "        if (",
            "            pvm.view_menu.name in self.GAMMA_READ_ONLY_MODEL_VIEWS",
            "            and pvm.permission.name not in self.READ_ONLY_PERMISSION",
            "        ):",
            "            return True",
            "        return (",
            "            pvm.view_menu.name in self.ALPHA_ONLY_VIEW_MENUS",
            "            or pvm.permission.name in self.ALPHA_ONLY_PERMISSIONS",
            "        )",
            "",
            "    def _is_accessible_to_all(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to all, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to all users",
            "        \"\"\"",
            "",
            "        return pvm.permission.name in self.ACCESSIBLE_PERMS",
            "",
            "    def _is_admin_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Admin user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Admin related",
            "        \"\"\"",
            "",
            "        return not self._is_user_defined_permission(pvm)",
            "",
            "    def _is_alpha_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Alpha user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Alpha related",
            "        \"\"\"",
            "",
            "        return not (",
            "            self._is_user_defined_permission(pvm)",
            "            or self._is_admin_only(pvm)",
            "            or self._is_sql_lab_only(pvm)",
            "        ) or self._is_accessible_to_all(pvm)",
            "",
            "    def _is_gamma_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Gamma user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Gamma related",
            "        \"\"\"",
            "",
            "        return not (",
            "            self._is_user_defined_permission(pvm)",
            "            or self._is_admin_only(pvm)",
            "            or self._is_alpha_only(pvm)",
            "            or self._is_sql_lab_only(pvm)",
            "        ) or self._is_accessible_to_all(pvm)",
            "",
            "    def _is_sql_lab_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is only SQL Lab related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is SQL Lab related",
            "        \"\"\"",
            "        return (pvm.permission.name, pvm.view_menu.name) in self.SQLLAB_ONLY_PERMISSIONS",
            "",
            "    def _is_sql_lab_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is SQL Lab related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is SQL Lab related",
            "        \"\"\"",
            "        return (",
            "            self._is_sql_lab_only(pvm)",
            "            or (pvm.permission.name, pvm.view_menu.name)",
            "            in self.SQLLAB_EXTRA_PERMISSION_VIEWS",
            "        )",
            "",
            "    def database_after_insert(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions when a database is created.",
            "        Triggered by a SQLAlchemy after_insert event.",
            "",
            "        We need to create:",
            "         - The database PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"database_access\", target.get_perm()",
            "        )",
            "",
            "    def database_after_delete(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions update when a database is deleted.",
            "        Triggered by a SQLAlchemy after_delete event.",
            "",
            "        We need to delete:",
            "         - The database PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        self._delete_vm_database_access(",
            "            mapper, connection, target.id, target.database_name",
            "        )",
            "",
            "    def database_after_update(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles all permissions update when a database is changed.",
            "        Triggered by a SQLAlchemy after_update event.",
            "",
            "        We need to update:",
            "         - The database PVM",
            "         - All datasets PVMs that reference the db, and it's local perm name",
            "         - All datasets local schema perm that reference the db.",
            "         - All charts local perm related with said datasets",
            "         - All charts local schema perm related with said datasets",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        # Check if database name has changed",
            "        state = inspect(target)",
            "        history = state.get_history(\"database_name\", True)",
            "        if not history.has_changes() or not history.deleted:",
            "            return",
            "",
            "        old_database_name = history.deleted[0]",
            "        # update database access permission",
            "        self._update_vm_database_access(mapper, connection, old_database_name, target)",
            "        # update datasource access",
            "        self._update_vm_datasources_access(",
            "            mapper, connection, old_database_name, target",
            "        )",
            "        # Note schema permissions are updated at the API level",
            "        # (database.commands.update). Since we need to fetch all existing schemas from",
            "        # the db",
            "",
            "    def _delete_vm_database_access(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        database_id: int,",
            "        database_name: str,",
            "    ) -> None:",
            "        view_menu_name = self.get_database_perm(database_id, database_name)",
            "        # Clean database access permission",
            "        self._delete_pvm_on_sqla_event(",
            "            mapper, connection, \"database_access\", view_menu_name",
            "        )",
            "        # Clean database schema permissions",
            "        schema_pvms = (",
            "            self.get_session.query(self.permissionview_model)",
            "            .join(self.permission_model)",
            "            .join(self.viewmenu_model)",
            "            .filter(self.permission_model.name == \"schema_access\")",
            "            .filter(self.viewmenu_model.name.like(f\"[{database_name}].[%]\"))",
            "            .all()",
            "        )",
            "        for schema_pvm in schema_pvms:",
            "            self._delete_pvm_on_sqla_event(mapper, connection, pvm=schema_pvm)",
            "",
            "    def _update_vm_database_access(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_database_name: str,",
            "        target: \"Database\",",
            "    ) -> Optional[ViewMenu]:",
            "        \"\"\"",
            "        Helper method that Updates all database access permission",
            "        when a database name changes.",
            "",
            "        :param connection: Current connection (called on SQLAlchemy event listener scope)",
            "        :param old_database_name: the old database name",
            "        :param target: The database object",
            "        :return: A list of changed view menus (permission resource names)",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        new_database_name = target.database_name",
            "        old_view_menu_name = self.get_database_perm(target.id, old_database_name)",
            "        new_view_menu_name = self.get_database_perm(target.id, new_database_name)",
            "        db_pvm = self.find_permission_view_menu(\"database_access\", old_view_menu_name)",
            "        if not db_pvm:",
            "            logger.warning(",
            "                \"Could not find previous database permission %s\",",
            "                old_view_menu_name,",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"database_access\", new_view_menu_name",
            "            )",
            "            return None",
            "        new_updated_pvm = self.find_permission_view_menu(",
            "            \"database_access\", new_view_menu_name",
            "        )",
            "        if new_updated_pvm:",
            "            logger.info(",
            "                \"New permission [%s] already exists, deleting the previous\",",
            "                new_view_menu_name,",
            "            )",
            "            self._delete_vm_database_access(",
            "                mapper, connection, target.id, old_database_name",
            "            )",
            "            return None",
            "        connection.execute(",
            "            view_menu_table.update()",
            "            .where(view_menu_table.c.id == db_pvm.view_menu_id)",
            "            .values(name=new_view_menu_name)",
            "        )",
            "        new_db_view_menu = self._find_view_menu_on_sqla_event(",
            "            connection, new_view_menu_name",
            "        )",
            "",
            "        self.on_view_menu_after_update(mapper, connection, new_db_view_menu)",
            "        return new_db_view_menu",
            "",
            "    def _update_vm_datasources_access(  # pylint: disable=too-many-locals",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_database_name: str,",
            "        target: \"Database\",",
            "    ) -> list[ViewMenu]:",
            "        \"\"\"",
            "        Helper method that Updates all datasource access permission",
            "        when a database name changes.",
            "",
            "        :param connection: Current connection (called on SQLAlchemy event listener scope)",
            "        :param old_database_name: the old database name",
            "        :param target: The database object",
            "        :return: A list of changed view menus (permission resource names)",
            "        \"\"\"",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "        new_database_name = target.database_name",
            "        datasets = (",
            "            self.get_session.query(SqlaTable)",
            "            .filter(SqlaTable.database_id == target.id)",
            "            .all()",
            "        )",
            "        updated_view_menus: list[ViewMenu] = []",
            "        for dataset in datasets:",
            "            old_dataset_vm_name = self.get_dataset_perm(",
            "                dataset.id, dataset.table_name, old_database_name",
            "            )",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                dataset.id, dataset.table_name, new_database_name",
            "            )",
            "            new_dataset_view_menu = self.find_view_menu(new_dataset_vm_name)",
            "            if new_dataset_view_menu:",
            "                continue",
            "            connection.execute(",
            "                view_menu_table.update()",
            "                .where(view_menu_table.c.name == old_dataset_vm_name)",
            "                .values(name=new_dataset_vm_name)",
            "            )",
            "            # After update refresh",
            "            new_dataset_view_menu = self._find_view_menu_on_sqla_event(",
            "                connection, new_dataset_vm_name",
            "            )",
            "",
            "            # Update dataset (SqlaTable perm field)",
            "            connection.execute(",
            "                sqlatable_table.update()",
            "                .where(",
            "                    sqlatable_table.c.id == dataset.id,",
            "                    sqlatable_table.c.perm == old_dataset_vm_name,",
            "                )",
            "                .values(perm=new_dataset_vm_name)",
            "            )",
            "            # Update charts (Slice perm field)",
            "            connection.execute(",
            "                chart_table.update()",
            "                .where(chart_table.c.perm == old_dataset_vm_name)",
            "                .values(perm=new_dataset_vm_name)",
            "            )",
            "            self.on_view_menu_after_update(mapper, connection, new_dataset_view_menu)",
            "            updated_view_menus.append(new_dataset_view_menu)",
            "        return updated_view_menus",
            "",
            "    def dataset_after_insert(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permission creation when a dataset is inserted.",
            "        Triggered by a SQLAlchemy after_insert event.",
            "",
            "        We need to create:",
            "         - The dataset PVM and set local and schema perm",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        from superset.models.core import (  # pylint: disable=import-outside-toplevel",
            "            Database,",
            "        )",
            "",
            "        try:",
            "            dataset_perm = target.get_perm()",
            "            database = target.database",
            "        except DatasetInvalidPermissionEvaluationException:",
            "            logger.warning(",
            "                \"Dataset has no database will retry with database_id to set permission\"",
            "            )",
            "            database = self.get_session.query(Database).get(target.database_id)",
            "            dataset_perm = self.get_dataset_perm(",
            "                target.id, target.table_name, database.database_name",
            "            )",
            "        dataset_table = target.__table__",
            "",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"datasource_access\", dataset_perm",
            "        )",
            "        if target.perm != dataset_perm:",
            "            target.perm = dataset_perm",
            "            connection.execute(",
            "                dataset_table.update()",
            "                .where(dataset_table.c.id == target.id)",
            "                .values(perm=dataset_perm)",
            "            )",
            "",
            "        if target.schema:",
            "            dataset_schema_perm = self.get_schema_perm(",
            "                database.database_name, target.schema",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"schema_access\", dataset_schema_perm",
            "            )",
            "            target.schema_perm = dataset_schema_perm",
            "            connection.execute(",
            "                dataset_table.update()",
            "                .where(dataset_table.c.id == target.id)",
            "                .values(schema_perm=dataset_schema_perm)",
            "            )",
            "",
            "    def dataset_after_delete(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions update when a dataset is deleted.",
            "        Triggered by a SQLAlchemy after_delete event.",
            "",
            "        We need to delete:",
            "         - The dataset PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        dataset_vm_name = self.get_dataset_perm(",
            "            target.id, target.table_name, target.database.database_name",
            "        )",
            "        self._delete_pvm_on_sqla_event(",
            "            mapper, connection, \"datasource_access\", dataset_vm_name",
            "        )",
            "",
            "    def dataset_before_update(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles all permissions update when a dataset is changed.",
            "        Triggered by a SQLAlchemy after_update event.",
            "",
            "        We need to update:",
            "         - The dataset PVM and local perm",
            "         - All charts local perm related with said datasets",
            "         - All charts local schema perm related with said datasets",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        # Check if watched fields have changed",
            "        table = SqlaTable.__table__",
            "        current_dataset = connection.execute(",
            "            table.select().where(table.c.id == target.id)",
            "        ).one()",
            "        current_db_id = current_dataset.database_id",
            "        current_schema = current_dataset.schema",
            "        current_table_name = current_dataset.table_name",
            "",
            "        # When database name changes",
            "        if current_db_id != target.database_id:",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, target.table_name, target.database.database_name",
            "            )",
            "            self._update_dataset_perm(",
            "                mapper, connection, target.perm, new_dataset_vm_name, target",
            "            )",
            "",
            "            # Updates schema permissions",
            "            new_dataset_schema_name = self.get_schema_perm(",
            "                target.database.database_name, target.schema",
            "            )",
            "            self._update_dataset_schema_perm(",
            "                mapper,",
            "                connection,",
            "                new_dataset_schema_name,",
            "                target,",
            "            )",
            "",
            "        # When table name changes",
            "        if current_table_name != target.table_name:",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, target.table_name, target.database.database_name",
            "            )",
            "            old_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, current_table_name, target.database.database_name",
            "            )",
            "            self._update_dataset_perm(",
            "                mapper, connection, old_dataset_vm_name, new_dataset_vm_name, target",
            "            )",
            "",
            "        # When schema changes",
            "        if current_schema != target.schema:",
            "            new_dataset_schema_name = self.get_schema_perm(",
            "                target.database.database_name, target.schema",
            "            )",
            "            self._update_dataset_schema_perm(",
            "                mapper,",
            "                connection,",
            "                new_dataset_schema_name,",
            "                target,",
            "            )",
            "",
            "    def _update_dataset_schema_perm(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        new_schema_permission_name: Optional[str],",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events on datasets to update",
            "        a new schema permission name, propagates the name change to datasets and charts.",
            "",
            "        If the schema permission name does not exist already has a PVM,",
            "        creates a new one.",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param new_schema_permission_name: The new schema permission name that changed",
            "        :param target: Dataset that was updated",
            "        :return:",
            "        \"\"\"",
            "        logger.info(\"Updating schema perm, new: %s\", new_schema_permission_name)",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "",
            "        # insert new schema PVM if it does not exist",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"schema_access\", new_schema_permission_name",
            "        )",
            "",
            "        # Update dataset (SqlaTable schema_perm field)",
            "        connection.execute(",
            "            sqlatable_table.update()",
            "            .where(",
            "                sqlatable_table.c.id == target.id,",
            "            )",
            "            .values(schema_perm=new_schema_permission_name)",
            "        )",
            "",
            "        # Update charts (Slice schema_perm field)",
            "        connection.execute(",
            "            chart_table.update()",
            "            .where(",
            "                chart_table.c.datasource_id == target.id,",
            "                chart_table.c.datasource_type == DatasourceType.TABLE,",
            "            )",
            "            .values(schema_perm=new_schema_permission_name)",
            "        )",
            "",
            "    def _update_dataset_perm(  # pylint: disable=too-many-arguments",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_permission_name: Optional[str],",
            "        new_permission_name: Optional[str],",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events on datasets to update",
            "        a permission name change, propagates the name change to VM, datasets and charts.",
            "",
            "        :param mapper:",
            "        :param connection:",
            "        :param old_permission_name",
            "        :param new_permission_name:",
            "        :param target:",
            "        :return:",
            "        \"\"\"",
            "        logger.info(",
            "            \"Updating dataset perm, old: %s, new: %s\",",
            "            old_permission_name,",
            "            new_permission_name,",
            "        )",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "",
            "        new_dataset_view_menu = self.find_view_menu(new_permission_name)",
            "        if new_dataset_view_menu:",
            "            return",
            "        old_dataset_view_menu = self.find_view_menu(old_permission_name)",
            "        if not old_dataset_view_menu:",
            "            logger.warning(",
            "                \"Could not find previous dataset permission %s\", old_permission_name",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"datasource_access\", new_permission_name",
            "            )",
            "            return",
            "        # Update VM",
            "        connection.execute(",
            "            view_menu_table.update()",
            "            .where(view_menu_table.c.name == old_permission_name)",
            "            .values(name=new_permission_name)",
            "        )",
            "        # VM changed, so call hook",
            "        new_dataset_view_menu = self.find_view_menu(new_permission_name)",
            "        self.on_view_menu_after_update(mapper, connection, new_dataset_view_menu)",
            "        # Update dataset (SqlaTable perm field)",
            "        connection.execute(",
            "            sqlatable_table.update()",
            "            .where(",
            "                sqlatable_table.c.id == target.id,",
            "            )",
            "            .values(perm=new_permission_name)",
            "        )",
            "        # Update charts (Slice perm field)",
            "        connection.execute(",
            "            chart_table.update()",
            "            .where(",
            "                chart_table.c.datasource_type == DatasourceType.TABLE,",
            "                chart_table.c.datasource_id == target.id,",
            "            )",
            "            .values(perm=new_permission_name)",
            "        )",
            "",
            "    def _delete_pvm_on_sqla_event(  # pylint: disable=too-many-arguments",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        permission_name: Optional[str] = None,",
            "        view_menu_name: Optional[str] = None,",
            "        pvm: Optional[PermissionView] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events.",
            "        Deletes a PVM.",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param permission_name: e.g.: datasource_access, schema_access",
            "        :param view_menu_name: e.g. [db1].[public]",
            "        :param pvm: Can be called with the actual PVM already",
            "        :return:",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        permission_view_menu_table = (",
            "            self.permissionview_model.__table__  # pylint: disable=no-member",
            "        )",
            "",
            "        if not pvm:",
            "            pvm = self.find_permission_view_menu(permission_name, view_menu_name)",
            "        if not pvm:",
            "            return",
            "        # Delete Any Role to PVM association",
            "        connection.execute(",
            "            assoc_permissionview_role.delete().where(",
            "                assoc_permissionview_role.c.permission_view_id == pvm.id",
            "            )",
            "        )",
            "        # Delete the database access PVM",
            "        connection.execute(",
            "            permission_view_menu_table.delete().where(",
            "                permission_view_menu_table.c.id == pvm.id",
            "            )",
            "        )",
            "        self.on_permission_view_after_delete(mapper, connection, pvm)",
            "        connection.execute(",
            "            view_menu_table.delete().where(view_menu_table.c.id == pvm.view_menu_id)",
            "        )",
            "",
            "    def _find_permission_on_sqla_event(",
            "        self, connection: Connection, name: str",
            "    ) -> Permission:",
            "        \"\"\"",
            "        Find a FAB Permission using a SQLA connection.",
            "",
            "        A session.query may not return the latest results on newly created/updated",
            "        objects/rows using connection. On this case we should use a connection also",
            "",
            "        :param connection: SQLAlchemy connection",
            "        :param name: The permission name (it's unique)",
            "        :return: Permission",
            "        \"\"\"",
            "        permission_table = self.permission_model.__table__  # pylint: disable=no-member",
            "",
            "        permission_ = connection.execute(",
            "            permission_table.select().where(permission_table.c.name == name)",
            "        ).fetchone()",
            "        permission = Permission()",
            "        # ensures this object is never persisted",
            "        permission.metadata = None",
            "        permission.id = permission_.id",
            "        permission.name = permission_.name",
            "        return permission",
            "",
            "    def _find_view_menu_on_sqla_event(",
            "        self, connection: Connection, name: str",
            "    ) -> ViewMenu:",
            "        \"\"\"",
            "        Find a FAB ViewMenu using a SQLA connection.",
            "",
            "        A session.query may not return the latest results on newly created/updated",
            "        objects/rows using connection. On this case we should use a connection also",
            "",
            "        :param connection: SQLAlchemy connection",
            "        :param name: The ViewMenu name (it's unique)",
            "        :return: ViewMenu",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "",
            "        view_menu_ = connection.execute(",
            "            view_menu_table.select().where(view_menu_table.c.name == name)",
            "        ).fetchone()",
            "        view_menu = ViewMenu()",
            "        # ensures this object is never persisted",
            "        view_menu.metadata = None",
            "        view_menu.id = view_menu_.id",
            "        view_menu.name = view_menu_.name",
            "        return view_menu",
            "",
            "    def _insert_pvm_on_sqla_event(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        permission_name: str,",
            "        view_menu_name: Optional[str],",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events.",
            "        Inserts a new PVM (if it does not exist already)",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param permission_name: e.g.: datasource_access, schema_access",
            "        :param view_menu_name: e.g. [db1].[public]",
            "        :return:",
            "        \"\"\"",
            "        permission_table = self.permission_model.__table__  # pylint: disable=no-member",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        permission_view_table = (",
            "            self.permissionview_model.__table__  # pylint: disable=no-member",
            "        )",
            "        if not view_menu_name:",
            "            return",
            "        pvm = self.find_permission_view_menu(permission_name, view_menu_name)",
            "        if pvm:",
            "            return",
            "        permission = self.find_permission(permission_name)",
            "        view_menu = self.find_view_menu(view_menu_name)",
            "        if not permission:",
            "            _ = connection.execute(",
            "                permission_table.insert().values(name=permission_name)",
            "            )",
            "            permission = self._find_permission_on_sqla_event(",
            "                connection, permission_name",
            "            )",
            "            self.on_permission_after_insert(mapper, connection, permission)",
            "        if not view_menu:",
            "            _ = connection.execute(view_menu_table.insert().values(name=view_menu_name))",
            "            view_menu = self._find_view_menu_on_sqla_event(connection, view_menu_name)",
            "            self.on_view_menu_after_insert(mapper, connection, view_menu)",
            "        connection.execute(",
            "            permission_view_table.insert().values(",
            "                permission_id=permission.id, view_menu_id=view_menu.id",
            "            )",
            "        )",
            "        permission_view = connection.execute(",
            "            permission_view_table.select().where(",
            "                permission_view_table.c.permission_id == permission.id,",
            "                permission_view_table.c.view_menu_id == view_menu.id,",
            "            )",
            "        ).fetchone()",
            "        permission_view_model = PermissionView()",
            "        permission_view_model.metadata = None",
            "        permission_view_model.id = permission_view.id",
            "        permission_view_model.permission_id = permission.id",
            "        permission_view_model.view_menu_id = view_menu.id",
            "        permission_view_model.permission = permission",
            "        permission_view_model.view_menu = view_menu",
            "        self.on_permission_view_after_insert(mapper, connection, permission_view_model)",
            "",
            "    def on_role_after_update(",
            "        self, mapper: Mapper, connection: Connection, target: Role",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a Role update",
            "        is created by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new view_menu's using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being changed",
            "        \"\"\"",
            "",
            "    def on_view_menu_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: ViewMenu",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new ViewMenu",
            "        is created by set_perm.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new view_menu's using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_view_menu_after_update(",
            "        self, mapper: Mapper, connection: Connection, target: ViewMenu",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new ViewMenu",
            "        is updated",
            "",
            "        Since the update may be performed on after_update event. We cannot",
            "        update ViewMenus using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_update.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: Permission",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new permission",
            "        is created by set_perm.",
            "",
            "        Since set_perm is executed by SQLAlchemy after_insert events, we cannot",
            "        create new permissions using a session, so any SQLAlchemy events hooked to",
            "        `Permission` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_view_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: PermissionView",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new PermissionView",
            "        is created by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new pvms using a session, so any SQLAlchemy events hooked to",
            "        `PermissionView` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_view_after_delete(",
            "        self, mapper: Mapper, connection: Connection, target: PermissionView",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new PermissionView",
            "        is delete by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_delete events, we cannot",
            "        delete pvms using a session, so any SQLAlchemy events hooked to",
            "        `PermissionView` will not trigger an after_delete.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    @staticmethod",
            "    def get_exclude_users_from_lists() -> list[str]:",
            "        \"\"\"",
            "        Override to dynamically identify a list of usernames to exclude from",
            "        all UI dropdown lists, owners, created_by filters etc...",
            "",
            "        It will exclude all users from the all endpoints of the form",
            "        ``/api/v1/<modelview>/related/<column>``",
            "",
            "        Optionally you can also exclude them using the `EXCLUDE_USERS_FROM_LISTS`",
            "        config setting.",
            "",
            "        :return: A list of usernames",
            "        \"\"\"",
            "        return []",
            "",
            "    def raise_for_access(",
            "        # pylint: disable=too-many-arguments,too-many-branches,too-many-locals",
            "        self,",
            "        dashboard: Optional[\"Dashboard\"] = None,",
            "        database: Optional[\"Database\"] = None,",
            "        datasource: Optional[\"BaseDatasource\"] = None,",
            "        query: Optional[\"Query\"] = None,",
            "        query_context: Optional[\"QueryContext\"] = None,",
            "        table: Optional[\"Table\"] = None,",
            "        viz: Optional[\"BaseViz\"] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :param database: The Superset database",
            "        :param datasource: The Superset datasource",
            "        :param query: The SQL Lab query",
            "        :param query_context: The query context",
            "        :param table: The Superset table (requires database)",
            "        :param viz: The visualization",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import is_feature_enabled",
            "        from superset.connectors.sqla.models import SqlaTable",
            "        from superset.models.dashboard import Dashboard",
            "        from superset.models.slice import Slice",
            "        from superset.sql_parse import Table",
            "",
            "        if database and table or query:",
            "            if query:",
            "                database = query.database",
            "",
            "            database = cast(\"Database\", database)",
            "",
            "            if self.can_access_database(database):",
            "                return",
            "",
            "            if query:",
            "                default_schema = database.get_default_schema_for_query(query)",
            "                tables = {",
            "                    Table(table_.table, table_.schema or default_schema)",
            "                    for table_ in sql_parse.ParsedQuery(query.sql).tables",
            "                }",
            "            elif table:",
            "                tables = {table}",
            "",
            "            denied = set()",
            "",
            "            for table_ in tables:",
            "                schema_perm = self.get_schema_perm(database, schema=table_.schema)",
            "",
            "                if not (schema_perm and self.can_access(\"schema_access\", schema_perm)):",
            "                    datasources = SqlaTable.query_datasources_by_name(",
            "                        self.get_session, database, table_.table, schema=table_.schema",
            "                    )",
            "",
            "                    # Access to any datasource is suffice.",
            "                    for datasource_ in datasources:",
            "                        if self.can_access(",
            "                            \"datasource_access\", datasource_.perm",
            "                        ) or self.is_owner(datasource_):",
            "                            break",
            "                    else:",
            "                        denied.add(table_)",
            "",
            "            if denied:",
            "                raise SupersetSecurityException(",
            "                    self.get_table_access_error_object(denied)",
            "                )",
            "",
            "        if datasource or query_context or viz:",
            "            form_data = None",
            "",
            "            if query_context:",
            "                datasource = query_context.datasource",
            "                form_data = query_context.form_data",
            "            elif viz:",
            "                datasource = viz.datasource",
            "                form_data = viz.form_data",
            "",
            "            assert datasource",
            "",
            "            if not (",
            "                self.can_access_schema(datasource)",
            "                or self.can_access(\"datasource_access\", datasource.perm or \"\")",
            "                or self.is_owner(datasource)",
            "                or (",
            "                    # Grant access to the datasource only if dashboard RBAC is enabled",
            "                    # and said datasource is associated with the dashboard chart in",
            "                    # question.",
            "                    form_data",
            "                    and is_feature_enabled(\"DASHBOARD_RBAC\")",
            "                    and (dashboard_id := form_data.get(\"dashboardId\"))",
            "                    and (",
            "                        dashboard_ := self.get_session.query(Dashboard)",
            "                        .filter(Dashboard.id == dashboard_id)",
            "                        .one_or_none()",
            "                    )",
            "                    and dashboard_.roles",
            "                    and (",
            "                        (",
            "                            # Native filter.",
            "                            form_data.get(\"type\") == \"NATIVE_FILTER\"",
            "                            and (native_filter_id := form_data.get(\"native_filter_id\"))",
            "                            and dashboard_.json_metadata",
            "                            and (json_metadata := json.loads(dashboard_.json_metadata))",
            "                            and any(",
            "                                target.get(\"datasetId\") == datasource.id",
            "                                for fltr in json_metadata.get(",
            "                                    \"native_filter_configuration\",",
            "                                    [],",
            "                                )",
            "                                for target in fltr.get(\"targets\", [])",
            "                                if native_filter_id == fltr.get(\"id\")",
            "                            )",
            "                        )",
            "                        or (",
            "                            # Chart.",
            "                            form_data.get(\"type\") != \"NATIVE_FILTER\"",
            "                            and (slice_id := form_data.get(\"slice_id\"))",
            "                            and (",
            "                                slc := self.get_session.query(Slice)",
            "                                .filter(Slice.id == slice_id)",
            "                                .one_or_none()",
            "                            )",
            "                            and slc in dashboard_.slices",
            "                            and slc.datasource == datasource",
            "                        )",
            "                    )",
            "                    and self.can_access_dashboard(dashboard_)",
            "                )",
            "            ):",
            "                raise SupersetSecurityException(",
            "                    self.get_datasource_access_error_object(datasource)",
            "                )",
            "",
            "        if dashboard:",
            "            if self.is_guest_user():",
            "                # Guest user is currently used for embedded dashboards only. If the guest",
            "                # user doesn't have access to the dashboard, ignore all other checks.",
            "                if self.has_guest_access(dashboard):",
            "                    return",
            "                raise SupersetSecurityException(",
            "                    self.get_dashboard_access_error_object(dashboard)",
            "                )",
            "",
            "            if self.is_admin() or self.is_owner(dashboard):",
            "                return",
            "",
            "            # RBAC and legacy (datasource inferred) access controls.",
            "            if is_feature_enabled(\"DASHBOARD_RBAC\") and dashboard.roles:",
            "                if dashboard.published and {role.id for role in dashboard.roles} & {",
            "                    role.id for role in self.get_user_roles()",
            "                }:",
            "                    return",
            "            elif (",
            "                # To understand why we rely on status and give access to draft dashboards",
            "                # without roles take a look at:",
            "                #",
            "                #   - https://github.com/apache/superset/pull/24350#discussion_r1225061550",
            "                #   - https://github.com/apache/superset/pull/17511#issuecomment-975870169",
            "                #",
            "                not dashboard.published",
            "                or not dashboard.datasources",
            "                or any(",
            "                    self.can_access_datasource(datasource)",
            "                    for datasource in dashboard.datasources",
            "                )",
            "            ):",
            "                return",
            "",
            "            raise SupersetSecurityException(",
            "                self.get_dashboard_access_error_object(dashboard)",
            "            )",
            "",
            "    def get_user_by_username(",
            "        self, username: str, session: Session = None",
            "    ) -> Optional[User]:",
            "        \"\"\"",
            "        Retrieves a user by it's username case sensitive. Optional session parameter",
            "        utility method normally useful for celery tasks where the session",
            "        need to be scoped",
            "        \"\"\"",
            "        session = session or self.get_session",
            "        return (",
            "            session.query(self.user_model)",
            "            .filter(self.user_model.username == username)",
            "            .one_or_none()",
            "        )",
            "",
            "    def get_anonymous_user(self) -> User:  # pylint: disable=no-self-use",
            "        return AnonymousUserMixin()",
            "",
            "    def get_user_roles(self, user: Optional[User] = None) -> list[Role]:",
            "        if not user:",
            "            user = g.user",
            "        if user.is_anonymous:",
            "            public_role = current_app.config.get(\"AUTH_ROLE_PUBLIC\")",
            "            return [self.get_public_role()] if public_role else []",
            "        return user.roles",
            "",
            "    def get_guest_rls_filters(",
            "        self, dataset: \"BaseDatasource\"",
            "    ) -> list[GuestTokenRlsRule]:",
            "        \"\"\"",
            "        Retrieves the row level security filters for the current user and the dataset,",
            "        if the user is authenticated with a guest token.",
            "        :param dataset: The dataset to check against",
            "        :return: A list of filters",
            "        \"\"\"",
            "        if guest_user := self.get_current_guest_user_if_guest():",
            "            return [",
            "                rule",
            "                for rule in guest_user.rls",
            "                if not rule.get(\"dataset\")",
            "                or str(rule.get(\"dataset\")) == str(dataset.id)",
            "            ]",
            "        return []",
            "",
            "    def get_rls_filters(self, table: \"BaseDatasource\") -> list[SqlaQuery]:",
            "        \"\"\"",
            "        Retrieves the appropriate row level security filters for the current user and",
            "        the passed table.",
            "",
            "        :param table: The table to check against",
            "        :returns: A list of filters",
            "        \"\"\"",
            "",
            "        if not (hasattr(g, \"user\") and g.user is not None):",
            "            return []",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import (",
            "            RLSFilterRoles,",
            "            RLSFilterTables,",
            "            RowLevelSecurityFilter,",
            "        )",
            "",
            "        user_roles = [role.id for role in self.get_user_roles(g.user)]",
            "        regular_filter_roles = (",
            "            self.get_session()",
            "            .query(RLSFilterRoles.c.rls_filter_id)",
            "            .join(RowLevelSecurityFilter)",
            "            .filter(",
            "                RowLevelSecurityFilter.filter_type == RowLevelSecurityFilterType.REGULAR",
            "            )",
            "            .filter(RLSFilterRoles.c.role_id.in_(user_roles))",
            "        )",
            "        base_filter_roles = (",
            "            self.get_session()",
            "            .query(RLSFilterRoles.c.rls_filter_id)",
            "            .join(RowLevelSecurityFilter)",
            "            .filter(",
            "                RowLevelSecurityFilter.filter_type == RowLevelSecurityFilterType.BASE",
            "            )",
            "            .filter(RLSFilterRoles.c.role_id.in_(user_roles))",
            "        )",
            "        filter_tables = (",
            "            self.get_session()",
            "            .query(RLSFilterTables.c.rls_filter_id)",
            "            .filter(RLSFilterTables.c.table_id == table.id)",
            "        )",
            "        query = (",
            "            self.get_session()",
            "            .query(",
            "                RowLevelSecurityFilter.id,",
            "                RowLevelSecurityFilter.group_key,",
            "                RowLevelSecurityFilter.clause,",
            "            )",
            "            .filter(RowLevelSecurityFilter.id.in_(filter_tables))",
            "            .filter(",
            "                or_(",
            "                    and_(",
            "                        RowLevelSecurityFilter.filter_type",
            "                        == RowLevelSecurityFilterType.REGULAR,",
            "                        RowLevelSecurityFilter.id.in_(regular_filter_roles),",
            "                    ),",
            "                    and_(",
            "                        RowLevelSecurityFilter.filter_type",
            "                        == RowLevelSecurityFilterType.BASE,",
            "                        RowLevelSecurityFilter.id.notin_(base_filter_roles),",
            "                    ),",
            "                )",
            "            )",
            "        )",
            "        return query.all()",
            "",
            "    def get_rls_ids(self, table: \"BaseDatasource\") -> list[int]:",
            "        \"\"\"",
            "        Retrieves the appropriate row level security filters IDs for the current user",
            "        and the passed table.",
            "",
            "        :param table: The table to check against",
            "        :returns: A list of IDs",
            "        \"\"\"",
            "        ids = [f.id for f in self.get_rls_filters(table)]",
            "        ids.sort()  # Combinations rather than permutations",
            "        return ids",
            "",
            "    def get_guest_rls_filters_str(self, table: \"BaseDatasource\") -> list[str]:",
            "        return [f.get(\"clause\", \"\") for f in self.get_guest_rls_filters(table)]",
            "",
            "    def get_rls_cache_key(self, datasource: \"BaseDatasource\") -> list[str]:",
            "        rls_ids = []",
            "        if datasource.is_rls_supported:",
            "            rls_ids = self.get_rls_ids(datasource)",
            "        rls_str = [str(rls_id) for rls_id in rls_ids]",
            "        guest_rls = self.get_guest_rls_filters_str(datasource)",
            "        return guest_rls + rls_str",
            "",
            "    @staticmethod",
            "    def _get_current_epoch_time() -> float:",
            "        \"\"\"This is used so the tests can mock time\"\"\"",
            "        return time.time()",
            "",
            "    @staticmethod",
            "    def _get_guest_token_jwt_audience() -> str:",
            "        audience = current_app.config[\"GUEST_TOKEN_JWT_AUDIENCE\"] or get_url_host()",
            "        if callable(audience):",
            "            audience = audience()",
            "        return audience",
            "",
            "    @staticmethod",
            "    def validate_guest_token_resources(resources: GuestTokenResources) -> None:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.daos.dashboard import EmbeddedDashboardDAO",
            "        from superset.embedded_dashboard.commands.exceptions import (",
            "            EmbeddedDashboardNotFoundError,",
            "        )",
            "        from superset.models.dashboard import Dashboard",
            "",
            "        for resource in resources:",
            "            if resource[\"type\"] == GuestTokenResourceType.DASHBOARD.value:",
            "                # TODO (embedded): remove this check once uuids are rolled out",
            "                dashboard = Dashboard.get(str(resource[\"id\"]))",
            "                if not dashboard:",
            "                    embedded = EmbeddedDashboardDAO.find_by_id(str(resource[\"id\"]))",
            "                    if not embedded:",
            "                        raise EmbeddedDashboardNotFoundError()",
            "",
            "    def create_guest_access_token(",
            "        self,",
            "        user: GuestTokenUser,",
            "        resources: GuestTokenResources,",
            "        rls: list[GuestTokenRlsRule],",
            "    ) -> bytes:",
            "        secret = current_app.config[\"GUEST_TOKEN_JWT_SECRET\"]",
            "        algo = current_app.config[\"GUEST_TOKEN_JWT_ALGO\"]",
            "        exp_seconds = current_app.config[\"GUEST_TOKEN_JWT_EXP_SECONDS\"]",
            "        audience = self._get_guest_token_jwt_audience()",
            "        # calculate expiration time",
            "        now = self._get_current_epoch_time()",
            "        exp = now + exp_seconds",
            "        claims = {",
            "            \"user\": user,",
            "            \"resources\": resources,",
            "            \"rls_rules\": rls,",
            "            # standard jwt claims:",
            "            \"iat\": now,  # issued at",
            "            \"exp\": exp,  # expiration time",
            "            \"aud\": audience,",
            "            \"type\": \"guest\",",
            "        }",
            "        token = self.pyjwt_for_guest_token.encode(claims, secret, algorithm=algo)",
            "        return token",
            "",
            "    def get_guest_user_from_request(self, req: Request) -> Optional[GuestUser]:",
            "        \"\"\"",
            "        If there is a guest token in the request (used for embedded),",
            "        parses the token and returns the guest user.",
            "        This is meant to be used as a request loader for the LoginManager.",
            "        The LoginManager will only call this if an active session cannot be found.",
            "",
            "        :return: A guest user object",
            "        \"\"\"",
            "        raw_token = req.headers.get(",
            "            current_app.config[\"GUEST_TOKEN_HEADER_NAME\"]",
            "        ) or req.form.get(\"guest_token\")",
            "        if raw_token is None:",
            "            return None",
            "",
            "        try:",
            "            token = self.parse_jwt_guest_token(raw_token)",
            "            if token.get(\"user\") is None:",
            "                raise ValueError(\"Guest token does not contain a user claim\")",
            "            if token.get(\"resources\") is None:",
            "                raise ValueError(\"Guest token does not contain a resources claim\")",
            "            if token.get(\"rls_rules\") is None:",
            "                raise ValueError(\"Guest token does not contain an rls_rules claim\")",
            "            if token.get(\"type\") != \"guest\":",
            "                raise ValueError(\"This is not a guest token.\")",
            "        except Exception:  # pylint: disable=broad-except",
            "            # The login manager will handle sending 401s.",
            "            # We don't need to send a special error message.",
            "            logger.warning(\"Invalid guest token\", exc_info=True)",
            "            return None",
            "        else:",
            "            return self.get_guest_user_from_token(cast(GuestToken, token))",
            "",
            "    def get_guest_user_from_token(self, token: GuestToken) -> GuestUser:",
            "        return self.guest_user_cls(",
            "            token=token,",
            "            roles=[self.find_role(current_app.config[\"GUEST_ROLE_NAME\"])],",
            "        )",
            "",
            "    def parse_jwt_guest_token(self, raw_token: str) -> dict[str, Any]:",
            "        \"\"\"",
            "        Parses a guest token. Raises an error if the jwt fails standard claims checks.",
            "        :param raw_token: the token gotten from the request",
            "        :return: the same token that was passed in, tested but unchanged",
            "        \"\"\"",
            "        secret = current_app.config[\"GUEST_TOKEN_JWT_SECRET\"]",
            "        algo = current_app.config[\"GUEST_TOKEN_JWT_ALGO\"]",
            "        audience = self._get_guest_token_jwt_audience()",
            "        return self.pyjwt_for_guest_token.decode(",
            "            raw_token, secret, algorithms=[algo], audience=audience",
            "        )",
            "",
            "    @staticmethod",
            "    def is_guest_user(user: Optional[Any] = None) -> bool:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import is_feature_enabled",
            "",
            "        if not is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "            return False",
            "        if not user:",
            "            user = g.user",
            "        return hasattr(user, \"is_guest_user\") and user.is_guest_user",
            "",
            "    def get_current_guest_user_if_guest(self) -> Optional[GuestUser]:",
            "        if self.is_guest_user():",
            "            return g.user",
            "        return None",
            "",
            "    def has_guest_access(self, dashboard: \"Dashboard\") -> bool:",
            "        user = self.get_current_guest_user_if_guest()",
            "        if not user:",
            "            return False",
            "",
            "        dashboards = [",
            "            r",
            "            for r in user.resources",
            "            if r[\"type\"] == GuestTokenResourceType.DASHBOARD.value",
            "        ]",
            "",
            "        # TODO (embedded): remove this check once uuids are rolled out",
            "        for resource in dashboards:",
            "            if str(resource[\"id\"]) == str(dashboard.id):",
            "                return True",
            "",
            "        if not dashboard.embedded:",
            "            return False",
            "",
            "        for resource in dashboards:",
            "            if str(resource[\"id\"]) == str(dashboard.embedded[0].uuid):",
            "                return True",
            "        return False",
            "",
            "    def raise_for_ownership(self, resource: Model) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user does not own the resource.",
            "",
            "        Note admins are deemed owners of all resources.",
            "",
            "        :param resource: The dashboard, dataste, chart, etc. resource",
            "        :raises SupersetSecurityException: If the current user is not an owner",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import db",
            "",
            "        if self.is_admin():",
            "            return",
            "",
            "        orig_resource = db.session.query(resource.__class__).get(resource.id)",
            "        owners = orig_resource.owners if hasattr(orig_resource, \"owners\") else []",
            "",
            "        if g.user.is_anonymous or g.user not in owners:",
            "            raise SupersetSecurityException(",
            "                SupersetError(",
            "                    error_type=SupersetErrorType.MISSING_OWNERSHIP_ERROR,",
            "                    message=_(",
            "                        \"You don't have the rights to alter %(resource)s\",",
            "                        resource=resource,",
            "                    ),",
            "                    level=ErrorLevel.ERROR,",
            "                )",
            "            )",
            "",
            "    def is_owner(self, resource: Model) -> bool:",
            "        \"\"\"",
            "        Returns True if the current user is an owner of the resource, False otherwise.",
            "",
            "        :param resource: The dashboard, dataste, chart, etc. resource",
            "        :returns: Whethe the current user is an owner of the resource",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_ownership(resource)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def is_admin(self) -> bool:",
            "        \"\"\"",
            "        Returns True if the current user is an admin user, False otherwise.",
            "",
            "        :returns: Whehther the current user is an admin user",
            "        \"\"\"",
            "",
            "        return current_app.config[\"AUTH_ROLE_ADMIN\"] in [",
            "            role.name for role in self.get_user_roles()",
            "        ]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1326": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1347": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1348": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1349": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1350": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1353": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1373": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1374": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1379": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ],
            "1386": [
                "SupersetSecurityManager",
                "dataset_after_update"
            ]
        },
        "addLocation": []
    },
    "superset/views/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "     app as superset_app,"
            },
            "1": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "     appbuilder,"
            },
            "2": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "     conf,"
            },
            "3": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    db,"
            },
            "4": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "     get_feature_flags,"
            },
            "5": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "     is_feature_enabled,"
            },
            "6": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "     security_manager,"
            },
            "7": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 62,
                "PatchRowcode": " )"
            },
            "8": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 63,
                "PatchRowcode": " from superset.commands.exceptions import CommandException, CommandInvalidError"
            },
            "9": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " from superset.connectors.sqla import models"
            },
            "10": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from superset.datasets.commands.exceptions import get_dataset_exist_error_msg"
            },
            "11": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 65,
                "PatchRowcode": " from superset.db_engine_specs import get_available_engine_specs"
            },
            "12": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 66,
                "PatchRowcode": " from superset.db_engine_specs.gsheets import GSheetsEngineSpec"
            },
            "13": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 67,
                "PatchRowcode": " from superset.errors import ErrorLevel, SupersetError, SupersetErrorType"
            },
            "14": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": 283,
                "PatchRowcode": "     return functools.update_wrapper(wraps, f)"
            },
            "15": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 284,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": 285,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def validate_sqlatable(table: models.SqlaTable) -> None:"
            },
            "18": {
                "beforePatchRowNumber": 289,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\"Checks the table existence in the database.\"\"\""
            },
            "19": {
                "beforePatchRowNumber": 290,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    with db.session.no_autoflush:"
            },
            "20": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        table_query = db.session.query(models.SqlaTable).filter("
            },
            "21": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            models.SqlaTable.table_name == table.table_name,"
            },
            "22": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            models.SqlaTable.schema == table.schema,"
            },
            "23": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            models.SqlaTable.database_id == table.database.id,"
            },
            "24": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "25": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if db.session.query(table_query.exists()).scalar():"
            },
            "26": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise Exception(get_dataset_exist_error_msg(table.full_name))"
            },
            "27": {
                "beforePatchRowNumber": 298,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "28": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # Fail before adding if the table can't be found"
            },
            "29": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    try:"
            },
            "30": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        table.get_sqla_table_object()"
            },
            "31": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    except Exception as ex:"
            },
            "32": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        logger.exception(\"Got an error in pre_add for %s\", table.name)"
            },
            "33": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        raise Exception("
            },
            "34": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            _("
            },
            "35": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"Table [%{table}s] could not be found, \""
            },
            "36": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"please double check your \""
            },
            "37": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"database connection, schema, and \""
            },
            "38": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"table name, error: {}\""
            },
            "39": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            ).format(table.name, str(ex))"
            },
            "40": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ) from ex"
            },
            "41": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "42": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "43": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": 286,
                "PatchRowcode": " class BaseSupersetView(BaseView):"
            },
            "44": {
                "beforePatchRowNumber": 315,
                "afterPatchRowNumber": 287,
                "PatchRowcode": "     @staticmethod"
            },
            "45": {
                "beforePatchRowNumber": 316,
                "afterPatchRowNumber": 288,
                "PatchRowcode": "     def json_response(obj: Any, status: int = 200) -> FlaskResponse:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import dataclasses",
            "import functools",
            "import logging",
            "import os",
            "import traceback",
            "from datetime import datetime",
            "from typing import Any, Callable, cast, Optional, Union",
            "",
            "import simplejson as json",
            "import yaml",
            "from flask import (",
            "    abort,",
            "    flash,",
            "    g,",
            "    get_flashed_messages,",
            "    redirect,",
            "    request,",
            "    Response,",
            "    send_file,",
            "    session,",
            ")",
            "from flask_appbuilder import BaseView, Model, ModelView",
            "from flask_appbuilder.actions import action",
            "from flask_appbuilder.forms import DynamicForm",
            "from flask_appbuilder.models.sqla.filters import BaseFilter",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_appbuilder.widgets import ListWidget",
            "from flask_babel import get_locale, gettext as __, lazy_gettext as _",
            "from flask_jwt_extended.exceptions import NoAuthorizationError",
            "from flask_wtf.csrf import CSRFError",
            "from flask_wtf.form import FlaskForm",
            "from pkg_resources import resource_filename",
            "from sqlalchemy import exc",
            "from sqlalchemy.orm import Query",
            "from werkzeug.exceptions import HTTPException",
            "from wtforms import Form",
            "from wtforms.fields.core import Field, UnboundField",
            "",
            "from superset import (",
            "    app as superset_app,",
            "    appbuilder,",
            "    conf,",
            "    db,",
            "    get_feature_flags,",
            "    is_feature_enabled,",
            "    security_manager,",
            ")",
            "from superset.commands.exceptions import CommandException, CommandInvalidError",
            "from superset.connectors.sqla import models",
            "from superset.datasets.commands.exceptions import get_dataset_exist_error_msg",
            "from superset.db_engine_specs import get_available_engine_specs",
            "from superset.db_engine_specs.gsheets import GSheetsEngineSpec",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    SupersetErrorException,",
            "    SupersetErrorsException,",
            "    SupersetException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.extensions import cache_manager",
            "from superset.models.helpers import ImportExportMixin",
            "from superset.reports.models import ReportRecipientType",
            "from superset.superset_typing import FlaskResponse",
            "from superset.translations.utils import get_language_pack",
            "from superset.utils import core as utils",
            "from superset.utils.filters import get_dataset_access_filters",
            "",
            "from .utils import bootstrap_user_data",
            "",
            "FRONTEND_CONF_KEYS = (",
            "    \"SUPERSET_WEBSERVER_TIMEOUT\",",
            "    \"SUPERSET_DASHBOARD_POSITION_DATA_LIMIT\",",
            "    \"SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT\",",
            "    \"SUPERSET_DASHBOARD_PERIODICAL_REFRESH_WARNING_MESSAGE\",",
            "    \"DISABLE_DATASET_SOURCE_EDIT\",",
            "    \"ENABLE_JAVASCRIPT_CONTROLS\",",
            "    \"DEFAULT_SQLLAB_LIMIT\",",
            "    \"DEFAULT_VIZ_TYPE\",",
            "    \"SQL_MAX_ROW\",",
            "    \"SUPERSET_WEBSERVER_DOMAINS\",",
            "    \"SQLLAB_SAVE_WARNING_MESSAGE\",",
            "    \"DISPLAY_MAX_ROW\",",
            "    \"GLOBAL_ASYNC_QUERIES_TRANSPORT\",",
            "    \"GLOBAL_ASYNC_QUERIES_POLLING_DELAY\",",
            "    \"SQL_VALIDATORS_BY_ENGINE\",",
            "    \"SQLALCHEMY_DOCS_URL\",",
            "    \"SQLALCHEMY_DISPLAY_TEXT\",",
            "    \"GLOBAL_ASYNC_QUERIES_WEBSOCKET_URL\",",
            "    \"DASHBOARD_AUTO_REFRESH_MODE\",",
            "    \"DASHBOARD_AUTO_REFRESH_INTERVALS\",",
            "    \"DASHBOARD_VIRTUALIZATION\",",
            "    \"SCHEDULED_QUERIES\",",
            "    \"EXCEL_EXTENSIONS\",",
            "    \"CSV_EXTENSIONS\",",
            "    \"COLUMNAR_EXTENSIONS\",",
            "    \"ALLOWED_EXTENSIONS\",",
            "    \"SAMPLES_ROW_LIMIT\",",
            "    \"DEFAULT_TIME_FILTER\",",
            "    \"HTML_SANITIZATION\",",
            "    \"HTML_SANITIZATION_SCHEMA_EXTENSIONS\",",
            "    \"WELCOME_PAGE_LAST_TAB\",",
            "    \"VIZ_TYPE_DENYLIST\",",
            "    \"ALERT_REPORTS_DEFAULT_CRON_VALUE\",",
            "    \"ALERT_REPORTS_DEFAULT_RETENTION\",",
            "    \"ALERT_REPORTS_DEFAULT_WORKING_TIMEOUT\",",
            "    \"NATIVE_FILTER_DEFAULT_ROW_LIMIT\",",
            "    \"PREVENT_UNSAFE_DEFAULT_URLS_ON_DATASET\",",
            ")",
            "",
            "logger = logging.getLogger(__name__)",
            "config = superset_app.config",
            "",
            "",
            "def get_error_msg() -> str:",
            "    if conf.get(\"SHOW_STACKTRACE\"):",
            "        error_msg = traceback.format_exc()",
            "    else:",
            "        error_msg = \"FATAL ERROR \\n\"",
            "        error_msg += (",
            "            \"Stacktrace is hidden. Change the SHOW_STACKTRACE \"",
            "            \"configuration setting to enable it\"",
            "        )",
            "    return error_msg",
            "",
            "",
            "def json_error_response(",
            "    msg: Optional[str] = None,",
            "    status: int = 500,",
            "    payload: Optional[dict[str, Any]] = None,",
            "    link: Optional[str] = None,",
            ") -> FlaskResponse:",
            "    if not payload:",
            "        payload = {\"error\": f\"{msg}\"}",
            "    if link:",
            "        payload[\"link\"] = link",
            "",
            "    return Response(",
            "        json.dumps(payload, default=utils.json_iso_dttm_ser, ignore_nan=True),",
            "        status=status,",
            "        mimetype=\"application/json\",",
            "    )",
            "",
            "",
            "def json_errors_response(",
            "    errors: list[SupersetError],",
            "    status: int = 500,",
            "    payload: Optional[dict[str, Any]] = None,",
            ") -> FlaskResponse:",
            "    if not payload:",
            "        payload = {}",
            "",
            "    payload[\"errors\"] = [dataclasses.asdict(error) for error in errors]",
            "    return Response(",
            "        json.dumps(payload, default=utils.json_iso_dttm_ser, ignore_nan=True),",
            "        status=status,",
            "        mimetype=\"application/json; charset=utf-8\",",
            "    )",
            "",
            "",
            "def json_success(json_msg: str, status: int = 200) -> FlaskResponse:",
            "    return Response(json_msg, status=status, mimetype=\"application/json\")",
            "",
            "",
            "def data_payload_response(payload_json: str, has_error: bool = False) -> FlaskResponse:",
            "    status = 400 if has_error else 200",
            "    return json_success(payload_json, status=status)",
            "",
            "",
            "def generate_download_headers(",
            "    extension: str, filename: Optional[str] = None",
            ") -> dict[str, Any]:",
            "    filename = filename if filename else datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
            "    content_disp = f\"attachment; filename={filename}.{extension}\"",
            "    headers = {\"Content-Disposition\": content_disp}",
            "    return headers",
            "",
            "",
            "def deprecated(",
            "    eol_version: str = \"4.0.0\",",
            "    new_target: Optional[str] = None,",
            ") -> Callable[[Callable[..., FlaskResponse]], Callable[..., FlaskResponse]]:",
            "    \"\"\"",
            "    A decorator to set an API endpoint from SupersetView has deprecated.",
            "    Issues a log warning",
            "    \"\"\"",
            "",
            "    def _deprecated(f: Callable[..., FlaskResponse]) -> Callable[..., FlaskResponse]:",
            "        def wraps(self: \"BaseSupersetView\", *args: Any, **kwargs: Any) -> FlaskResponse:",
            "            messsage = (",
            "                \"%s.%s \"",
            "                \"This API endpoint is deprecated and will be removed in version %s\"",
            "            )",
            "            logger_args = [",
            "                self.__class__.__name__,",
            "                f.__name__,",
            "                eol_version,",
            "            ]",
            "            if new_target:",
            "                messsage += \" . Use the following API endpoint instead: %s\"",
            "                logger_args.append(new_target)",
            "            logger.warning(messsage, *logger_args)",
            "            return f(self, *args, **kwargs)",
            "",
            "        return functools.update_wrapper(wraps, f)",
            "",
            "    return _deprecated",
            "",
            "",
            "def api(f: Callable[..., FlaskResponse]) -> Callable[..., FlaskResponse]:",
            "    \"\"\"",
            "    A decorator to label an endpoint as an API. Catches uncaught exceptions and",
            "    return the response in the JSON format",
            "    \"\"\"",
            "",
            "    def wraps(self: \"BaseSupersetView\", *args: Any, **kwargs: Any) -> FlaskResponse:",
            "        try:",
            "            return f(self, *args, **kwargs)",
            "        except NoAuthorizationError:",
            "            logger.warning(\"Api failed- no authorization\", exc_info=True)",
            "            return json_error_response(get_error_msg(), status=401)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.exception(ex)",
            "            return json_error_response(get_error_msg())",
            "",
            "    return functools.update_wrapper(wraps, f)",
            "",
            "",
            "def handle_api_exception(",
            "    f: Callable[..., FlaskResponse]",
            ") -> Callable[..., FlaskResponse]:",
            "    \"\"\"",
            "    A decorator to catch superset exceptions. Use it after the @api decorator above",
            "    so superset exception handler is triggered before the handler for generic",
            "    exceptions.",
            "    \"\"\"",
            "",
            "    def wraps(self: \"BaseSupersetView\", *args: Any, **kwargs: Any) -> FlaskResponse:",
            "        try:",
            "            return f(self, *args, **kwargs)",
            "        except SupersetSecurityException as ex:",
            "            logger.warning(\"SupersetSecurityException\", exc_info=True)",
            "            return json_errors_response(",
            "                errors=[ex.error], status=ex.status, payload=ex.payload",
            "            )",
            "        except SupersetErrorsException as ex:",
            "            logger.warning(ex, exc_info=True)",
            "            return json_errors_response(errors=ex.errors, status=ex.status)",
            "        except SupersetErrorException as ex:",
            "            logger.warning(\"SupersetErrorException\", exc_info=True)",
            "            return json_errors_response(errors=[ex.error], status=ex.status)",
            "        except SupersetException as ex:",
            "            if ex.status >= 500:",
            "                logger.exception(ex)",
            "            return json_error_response(",
            "                utils.error_msg_from_exception(ex), status=ex.status",
            "            )",
            "        except HTTPException as ex:",
            "            logger.exception(ex)",
            "            return json_error_response(",
            "                utils.error_msg_from_exception(ex), status=cast(int, ex.code)",
            "            )",
            "        except (exc.IntegrityError, exc.DatabaseError, exc.DataError) as ex:",
            "            logger.exception(ex)",
            "            return json_error_response(utils.error_msg_from_exception(ex), status=422)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.exception(ex)",
            "            return json_error_response(utils.error_msg_from_exception(ex))",
            "",
            "    return functools.update_wrapper(wraps, f)",
            "",
            "",
            "def validate_sqlatable(table: models.SqlaTable) -> None:",
            "    \"\"\"Checks the table existence in the database.\"\"\"",
            "    with db.session.no_autoflush:",
            "        table_query = db.session.query(models.SqlaTable).filter(",
            "            models.SqlaTable.table_name == table.table_name,",
            "            models.SqlaTable.schema == table.schema,",
            "            models.SqlaTable.database_id == table.database.id,",
            "        )",
            "        if db.session.query(table_query.exists()).scalar():",
            "            raise Exception(get_dataset_exist_error_msg(table.full_name))",
            "",
            "    # Fail before adding if the table can't be found",
            "    try:",
            "        table.get_sqla_table_object()",
            "    except Exception as ex:",
            "        logger.exception(\"Got an error in pre_add for %s\", table.name)",
            "        raise Exception(",
            "            _(",
            "                \"Table [%{table}s] could not be found, \"",
            "                \"please double check your \"",
            "                \"database connection, schema, and \"",
            "                \"table name, error: {}\"",
            "            ).format(table.name, str(ex))",
            "        ) from ex",
            "",
            "",
            "class BaseSupersetView(BaseView):",
            "    @staticmethod",
            "    def json_response(obj: Any, status: int = 200) -> FlaskResponse:",
            "        return Response(",
            "            json.dumps(obj, default=utils.json_int_dttm_ser, ignore_nan=True),",
            "            status=status,",
            "            mimetype=\"application/json\",",
            "        )",
            "",
            "    def render_app_template(self) -> FlaskResponse:",
            "        payload = {",
            "            \"user\": bootstrap_user_data(g.user, include_perms=True),",
            "            \"common\": common_bootstrap_payload(g.user),",
            "        }",
            "        return self.render_template(",
            "            \"superset/spa.html\",",
            "            entry=\"spa\",",
            "            bootstrap_data=json.dumps(",
            "                payload, default=utils.pessimistic_json_iso_dttm_ser",
            "            ),",
            "        )",
            "",
            "",
            "def get_environment_tag() -> dict[str, Any]:",
            "    # Whether flask is in debug mode (--debug)",
            "    debug = appbuilder.app.config[\"DEBUG\"]",
            "",
            "    # Getting the configuration option for ENVIRONMENT_TAG_CONFIG",
            "    env_tag_config = appbuilder.app.config[\"ENVIRONMENT_TAG_CONFIG\"]",
            "",
            "    # These are the predefined templates define in the config",
            "    env_tag_templates = env_tag_config.get(\"values\")",
            "",
            "    # This is the environment variable name from which to select the template",
            "    # default is SUPERSET_ENV (from FLASK_ENV in previous versions)",
            "    env_envvar = env_tag_config.get(\"variable\")",
            "",
            "    # this is the actual name we want to use",
            "    env_name = os.environ.get(env_envvar)",
            "",
            "    if not env_name or env_name not in env_tag_templates.keys():",
            "        env_name = \"debug\" if debug else None",
            "",
            "    env_tag = env_tag_templates.get(env_name)",
            "    return env_tag or {}",
            "",
            "",
            "def menu_data(user: User) -> dict[str, Any]:",
            "    menu = appbuilder.menu.get_data()",
            "",
            "    languages = {}",
            "    for lang in appbuilder.languages:",
            "        languages[lang] = {",
            "            **appbuilder.languages[lang],",
            "            \"url\": appbuilder.get_url_for_locale(lang),",
            "        }",
            "    brand_text = appbuilder.app.config[\"LOGO_RIGHT_TEXT\"]",
            "    if callable(brand_text):",
            "        brand_text = brand_text()",
            "    build_number = appbuilder.app.config[\"BUILD_NUMBER\"]",
            "",
            "    return {",
            "        \"menu\": menu,",
            "        \"brand\": {",
            "            \"path\": appbuilder.app.config[\"LOGO_TARGET_PATH\"] or \"/superset/welcome/\",",
            "            \"icon\": appbuilder.app_icon,",
            "            \"alt\": appbuilder.app_name,",
            "            \"tooltip\": appbuilder.app.config[\"LOGO_TOOLTIP\"],",
            "            \"text\": brand_text,",
            "        },",
            "        \"environment_tag\": get_environment_tag(),",
            "        \"navbar_right\": {",
            "            # show the watermark if the default app icon has been overridden",
            "            \"show_watermark\": (\"superset-logo-horiz\" not in appbuilder.app_icon),",
            "            \"bug_report_url\": appbuilder.app.config[\"BUG_REPORT_URL\"],",
            "            \"bug_report_icon\": appbuilder.app.config[\"BUG_REPORT_ICON\"],",
            "            \"bug_report_text\": appbuilder.app.config[\"BUG_REPORT_TEXT\"],",
            "            \"documentation_url\": appbuilder.app.config[\"DOCUMENTATION_URL\"],",
            "            \"documentation_icon\": appbuilder.app.config[\"DOCUMENTATION_ICON\"],",
            "            \"documentation_text\": appbuilder.app.config[\"DOCUMENTATION_TEXT\"],",
            "            \"version_string\": appbuilder.app.config[\"VERSION_STRING\"],",
            "            \"version_sha\": appbuilder.app.config[\"VERSION_SHA\"],",
            "            \"build_number\": build_number,",
            "            \"languages\": languages,",
            "            \"show_language_picker\": len(languages.keys()) > 1,",
            "            \"user_is_anonymous\": user.is_anonymous,",
            "            \"user_info_url\": None",
            "            if is_feature_enabled(\"MENU_HIDE_USER_INFO\")",
            "            else appbuilder.get_url_for_userinfo,",
            "            \"user_logout_url\": appbuilder.get_url_for_logout,",
            "            \"user_login_url\": appbuilder.get_url_for_login,",
            "            \"user_profile_url\": None",
            "            if user.is_anonymous or is_feature_enabled(\"MENU_HIDE_USER_INFO\")",
            "            else \"/superset/profile/\",",
            "            \"locale\": session.get(\"locale\", \"en\"),",
            "        },",
            "    }",
            "",
            "",
            "@cache_manager.cache.memoize(timeout=60)",
            "def cached_common_bootstrap_data(user: User, locale: str) -> dict[str, Any]:",
            "    \"\"\"Common data always sent to the client",
            "",
            "    The function is memoized as the return value only changes when user permissions",
            "    or configuration values change.",
            "    \"\"\"",
            "",
            "    # should not expose API TOKEN to frontend",
            "    frontend_config = {",
            "        k: (list(conf.get(k)) if isinstance(conf.get(k), set) else conf.get(k))",
            "        for k in FRONTEND_CONF_KEYS",
            "    }",
            "",
            "    if conf.get(\"SLACK_API_TOKEN\"):",
            "        frontend_config[\"ALERT_REPORTS_NOTIFICATION_METHODS\"] = [",
            "            ReportRecipientType.EMAIL,",
            "            ReportRecipientType.SLACK,",
            "        ]",
            "    else:",
            "        frontend_config[\"ALERT_REPORTS_NOTIFICATION_METHODS\"] = [",
            "            ReportRecipientType.EMAIL,",
            "        ]",
            "",
            "    # verify client has google sheets installed",
            "    available_specs = get_available_engine_specs()",
            "    frontend_config[\"HAS_GSHEETS_INSTALLED\"] = bool(available_specs[GSheetsEngineSpec])",
            "",
            "    bootstrap_data = {",
            "        \"conf\": frontend_config,",
            "        \"locale\": locale,",
            "        \"language_pack\": get_language_pack(locale),",
            "        \"d3_format\": conf.get(\"D3_FORMAT\"),",
            "        \"currencies\": conf.get(\"CURRENCIES\"),",
            "        \"feature_flags\": get_feature_flags(),",
            "        \"extra_sequential_color_schemes\": conf[\"EXTRA_SEQUENTIAL_COLOR_SCHEMES\"],",
            "        \"extra_categorical_color_schemes\": conf[\"EXTRA_CATEGORICAL_COLOR_SCHEMES\"],",
            "        \"theme_overrides\": conf[\"THEME_OVERRIDES\"],",
            "        \"menu_data\": menu_data(user),",
            "    }",
            "    bootstrap_data.update(conf[\"COMMON_BOOTSTRAP_OVERRIDES_FUNC\"](bootstrap_data))",
            "    return bootstrap_data",
            "",
            "",
            "def common_bootstrap_payload(user: User) -> dict[str, Any]:",
            "    return {",
            "        **cached_common_bootstrap_data(user, get_locale()),",
            "        \"flash_messages\": get_flashed_messages(with_categories=True),",
            "    }",
            "",
            "",
            "def get_error_level_from_status_code(  # pylint: disable=invalid-name",
            "    status: int,",
            ") -> ErrorLevel:",
            "    if status < 400:",
            "        return ErrorLevel.INFO",
            "    if status < 500:",
            "        return ErrorLevel.WARNING",
            "    return ErrorLevel.ERROR",
            "",
            "",
            "# SIP-40 compatible error responses; make sure APIs raise",
            "# SupersetErrorException or SupersetErrorsException",
            "@superset_app.errorhandler(SupersetErrorException)",
            "def show_superset_error(ex: SupersetErrorException) -> FlaskResponse:",
            "    logger.warning(\"SupersetErrorException\", exc_info=True)",
            "    return json_errors_response(errors=[ex.error], status=ex.status)",
            "",
            "",
            "@superset_app.errorhandler(SupersetErrorsException)",
            "def show_superset_errors(ex: SupersetErrorsException) -> FlaskResponse:",
            "    logger.warning(\"SupersetErrorsException\", exc_info=True)",
            "    return json_errors_response(errors=ex.errors, status=ex.status)",
            "",
            "",
            "# Redirect to login if the CSRF token is expired",
            "@superset_app.errorhandler(CSRFError)",
            "def refresh_csrf_token(ex: CSRFError) -> FlaskResponse:",
            "    logger.warning(\"Refresh CSRF token error\", exc_info=True)",
            "",
            "    if request.is_json:",
            "        return show_http_exception(ex)",
            "",
            "    return redirect(appbuilder.get_url_for_login)",
            "",
            "",
            "@superset_app.errorhandler(HTTPException)",
            "def show_http_exception(ex: HTTPException) -> FlaskResponse:",
            "    logger.warning(\"HTTPException\", exc_info=True)",
            "    if (",
            "        \"text/html\" in request.accept_mimetypes",
            "        and not config[\"DEBUG\"]",
            "        and ex.code in {404, 500}",
            "    ):",
            "        path = resource_filename(\"superset\", f\"static/assets/{ex.code}.html\")",
            "        return send_file(path, max_age=0), ex.code",
            "",
            "    return json_errors_response(",
            "        errors=[",
            "            SupersetError(",
            "                message=utils.error_msg_from_exception(ex),",
            "                error_type=SupersetErrorType.GENERIC_BACKEND_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            ),",
            "        ],",
            "        status=ex.code or 500,",
            "    )",
            "",
            "",
            "# Temporary handler for CommandException; if an API raises a",
            "# CommandException it should be fixed to map it to SupersetErrorException",
            "# or SupersetErrorsException, with a specific status code and error type",
            "@superset_app.errorhandler(CommandException)",
            "def show_command_errors(ex: CommandException) -> FlaskResponse:",
            "    logger.warning(\"CommandException\", exc_info=True)",
            "    if \"text/html\" in request.accept_mimetypes and not config[\"DEBUG\"]:",
            "        path = resource_filename(\"superset\", \"static/assets/500.html\")",
            "        return send_file(path, max_age=0), 500",
            "",
            "    extra = ex.normalized_messages() if isinstance(ex, CommandInvalidError) else {}",
            "    return json_errors_response(",
            "        errors=[",
            "            SupersetError(",
            "                message=ex.message,",
            "                error_type=SupersetErrorType.GENERIC_COMMAND_ERROR,",
            "                level=get_error_level_from_status_code(ex.status),",
            "                extra=extra,",
            "            ),",
            "        ],",
            "        status=ex.status,",
            "    )",
            "",
            "",
            "# Catch-all, to ensure all errors from the backend conform to SIP-40",
            "@superset_app.errorhandler(Exception)",
            "def show_unexpected_exception(ex: Exception) -> FlaskResponse:",
            "    logger.exception(ex)",
            "    if \"text/html\" in request.accept_mimetypes and not config[\"DEBUG\"]:",
            "        path = resource_filename(\"superset\", \"static/assets/500.html\")",
            "        return send_file(path, max_age=0), 500",
            "",
            "    return json_errors_response(",
            "        errors=[",
            "            SupersetError(",
            "                message=utils.error_msg_from_exception(ex),",
            "                error_type=SupersetErrorType.GENERIC_BACKEND_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            ),",
            "        ],",
            "    )",
            "",
            "",
            "@superset_app.context_processor",
            "def get_common_bootstrap_data() -> dict[str, Any]:",
            "    def serialize_bootstrap_data() -> str:",
            "        return json.dumps(",
            "            {\"common\": common_bootstrap_payload(g.user)},",
            "            default=utils.pessimistic_json_iso_dttm_ser,",
            "        )",
            "",
            "    return {\"bootstrap_data\": serialize_bootstrap_data}",
            "",
            "",
            "class SupersetListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    template = \"superset/fab_overrides/list.html\"",
            "",
            "",
            "class SupersetModelView(ModelView):",
            "    page_size = 100",
            "    list_widget = SupersetListWidget",
            "",
            "    def render_app_template(self) -> FlaskResponse:",
            "        payload = {",
            "            \"user\": bootstrap_user_data(g.user, include_perms=True),",
            "            \"common\": common_bootstrap_payload(g.user),",
            "        }",
            "        return self.render_template(",
            "            \"superset/spa.html\",",
            "            entry=\"spa\",",
            "            bootstrap_data=json.dumps(",
            "                payload, default=utils.pessimistic_json_iso_dttm_ser",
            "            ),",
            "        )",
            "",
            "",
            "class ListWidgetWithCheckboxes(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"An alternative to list view that renders Boolean fields as checkboxes",
            "",
            "    Works in conjunction with the `checkbox` view.\"\"\"",
            "",
            "    template = \"superset/fab_overrides/list_with_checkboxes.html\"",
            "",
            "",
            "def validate_json(form: Form, field: Field) -> None:  # pylint: disable=unused-argument",
            "    try:",
            "        json.loads(field.data)",
            "    except Exception as ex:",
            "        logger.exception(ex)",
            "        raise Exception(_(\"json isn't valid\")) from ex",
            "",
            "",
            "class YamlExportMixin:  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Override this if you want a dict response instead, with a certain key.",
            "    Used on DatabaseView for cli compatibility",
            "    \"\"\"",
            "",
            "    yaml_dict_key: Optional[str] = None",
            "",
            "    @action(\"yaml_export\", __(\"Export to YAML\"), __(\"Export to YAML?\"), \"fa-download\")",
            "    def yaml_export(",
            "        self, items: Union[ImportExportMixin, list[ImportExportMixin]]",
            "    ) -> FlaskResponse:",
            "        if not isinstance(items, list):",
            "            items = [items]",
            "",
            "        data = [t.export_to_dict() for t in items]",
            "",
            "        return Response(",
            "            yaml.safe_dump({self.yaml_dict_key: data} if self.yaml_dict_key else data),",
            "            headers=generate_download_headers(\"yaml\"),",
            "            mimetype=\"application/text\",",
            "        )",
            "",
            "",
            "class DeleteMixin:  # pylint: disable=too-few-public-methods",
            "    def _delete(self: BaseView, primary_key: int) -> None:",
            "        \"\"\"",
            "        Delete function logic, override to implement different logic",
            "        deletes the record with primary_key = primary_key",
            "",
            "        :param primary_key:",
            "            record primary key to delete",
            "        \"\"\"",
            "        item = self.datamodel.get(primary_key, self._base_filters)",
            "        if not item:",
            "            abort(404)",
            "        try:",
            "            self.pre_delete(item)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            flash(str(ex), \"danger\")",
            "        else:",
            "            view_menu = security_manager.find_view_menu(item.get_perm())",
            "            pvs = (",
            "                security_manager.get_session.query(",
            "                    security_manager.permissionview_model",
            "                )",
            "                .filter_by(view_menu=view_menu)",
            "                .all()",
            "            )",
            "",
            "            if self.datamodel.delete(item):",
            "                self.post_delete(item)",
            "",
            "                for pv in pvs:",
            "                    security_manager.get_session.delete(pv)",
            "",
            "                if view_menu:",
            "                    security_manager.get_session.delete(view_menu)",
            "",
            "                security_manager.get_session.commit()",
            "",
            "            flash(*self.datamodel.message)",
            "            self.update_redirect()",
            "",
            "    @action(",
            "        \"muldelete\", __(\"Delete\"), __(\"Delete all Really?\"), \"fa-trash\", single=False",
            "    )",
            "    def muldelete(self: BaseView, items: list[Model]) -> FlaskResponse:",
            "        if not items:",
            "            abort(404)",
            "        for item in items:",
            "            try:",
            "                self.pre_delete(item)",
            "            except Exception as ex:  # pylint: disable=broad-except",
            "                flash(str(ex), \"danger\")",
            "            else:",
            "                self._delete(item.id)",
            "        self.update_redirect()",
            "        return redirect(self.get_redirect())",
            "",
            "",
            "class DatasourceFilter(BaseFilter):  # pylint: disable=too-few-public-methods",
            "    def apply(self, query: Query, value: Any) -> Query:",
            "        if security_manager.can_access_all_datasources():",
            "            return query",
            "        query = query.join(",
            "            models.Database,",
            "            models.Database.id == self.model.database_id,",
            "        )",
            "        return query.filter(get_dataset_access_filters(self.model))",
            "",
            "",
            "class CsvResponse(Response):",
            "    \"\"\"",
            "    Override Response to take into account csv encoding from config.py",
            "    \"\"\"",
            "",
            "    charset = conf[\"CSV_EXPORT\"].get(\"encoding\", \"utf-8\")",
            "    default_mimetype = \"text/csv\"",
            "",
            "",
            "class XlsxResponse(Response):",
            "    \"\"\"",
            "    Override Response to use xlsx mimetype",
            "    \"\"\"",
            "",
            "    charset = \"utf-8\"",
            "    default_mimetype = (",
            "        \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"",
            "    )",
            "",
            "",
            "def bind_field(",
            "    _: Any, form: DynamicForm, unbound_field: UnboundField, options: dict[Any, Any]",
            ") -> Field:",
            "    \"\"\"",
            "    Customize how fields are bound by stripping all whitespace.",
            "",
            "    :param form: The form",
            "    :param unbound_field: The unbound field",
            "    :param options: The field options",
            "    :returns: The bound field",
            "    \"\"\"",
            "",
            "    filters = unbound_field.kwargs.get(\"filters\", [])",
            "    filters.append(lambda x: x.strip() if isinstance(x, str) else x)",
            "    return unbound_field.bind(form=form, filters=filters, **options)",
            "",
            "",
            "FlaskForm.Meta.bind_field = bind_field",
            "",
            "",
            "@superset_app.after_request",
            "def apply_http_headers(response: Response) -> Response:",
            "    \"\"\"Applies the configuration's http headers to all responses\"\"\"",
            "",
            "    # HTTP_HEADERS is deprecated, this provides backwards compatibility",
            "    response.headers.extend(",
            "        {**config[\"OVERRIDE_HTTP_HEADERS\"], **config[\"HTTP_HEADERS\"]}",
            "    )",
            "",
            "    for k, v in config[\"DEFAULT_HTTP_HEADERS\"].items():",
            "        if k not in response.headers:",
            "            response.headers[k] = v",
            "    return response"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import dataclasses",
            "import functools",
            "import logging",
            "import os",
            "import traceback",
            "from datetime import datetime",
            "from typing import Any, Callable, cast, Optional, Union",
            "",
            "import simplejson as json",
            "import yaml",
            "from flask import (",
            "    abort,",
            "    flash,",
            "    g,",
            "    get_flashed_messages,",
            "    redirect,",
            "    request,",
            "    Response,",
            "    send_file,",
            "    session,",
            ")",
            "from flask_appbuilder import BaseView, Model, ModelView",
            "from flask_appbuilder.actions import action",
            "from flask_appbuilder.forms import DynamicForm",
            "from flask_appbuilder.models.sqla.filters import BaseFilter",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_appbuilder.widgets import ListWidget",
            "from flask_babel import get_locale, gettext as __, lazy_gettext as _",
            "from flask_jwt_extended.exceptions import NoAuthorizationError",
            "from flask_wtf.csrf import CSRFError",
            "from flask_wtf.form import FlaskForm",
            "from pkg_resources import resource_filename",
            "from sqlalchemy import exc",
            "from sqlalchemy.orm import Query",
            "from werkzeug.exceptions import HTTPException",
            "from wtforms import Form",
            "from wtforms.fields.core import Field, UnboundField",
            "",
            "from superset import (",
            "    app as superset_app,",
            "    appbuilder,",
            "    conf,",
            "    get_feature_flags,",
            "    is_feature_enabled,",
            "    security_manager,",
            ")",
            "from superset.commands.exceptions import CommandException, CommandInvalidError",
            "from superset.connectors.sqla import models",
            "from superset.db_engine_specs import get_available_engine_specs",
            "from superset.db_engine_specs.gsheets import GSheetsEngineSpec",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    SupersetErrorException,",
            "    SupersetErrorsException,",
            "    SupersetException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.extensions import cache_manager",
            "from superset.models.helpers import ImportExportMixin",
            "from superset.reports.models import ReportRecipientType",
            "from superset.superset_typing import FlaskResponse",
            "from superset.translations.utils import get_language_pack",
            "from superset.utils import core as utils",
            "from superset.utils.filters import get_dataset_access_filters",
            "",
            "from .utils import bootstrap_user_data",
            "",
            "FRONTEND_CONF_KEYS = (",
            "    \"SUPERSET_WEBSERVER_TIMEOUT\",",
            "    \"SUPERSET_DASHBOARD_POSITION_DATA_LIMIT\",",
            "    \"SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT\",",
            "    \"SUPERSET_DASHBOARD_PERIODICAL_REFRESH_WARNING_MESSAGE\",",
            "    \"DISABLE_DATASET_SOURCE_EDIT\",",
            "    \"ENABLE_JAVASCRIPT_CONTROLS\",",
            "    \"DEFAULT_SQLLAB_LIMIT\",",
            "    \"DEFAULT_VIZ_TYPE\",",
            "    \"SQL_MAX_ROW\",",
            "    \"SUPERSET_WEBSERVER_DOMAINS\",",
            "    \"SQLLAB_SAVE_WARNING_MESSAGE\",",
            "    \"DISPLAY_MAX_ROW\",",
            "    \"GLOBAL_ASYNC_QUERIES_TRANSPORT\",",
            "    \"GLOBAL_ASYNC_QUERIES_POLLING_DELAY\",",
            "    \"SQL_VALIDATORS_BY_ENGINE\",",
            "    \"SQLALCHEMY_DOCS_URL\",",
            "    \"SQLALCHEMY_DISPLAY_TEXT\",",
            "    \"GLOBAL_ASYNC_QUERIES_WEBSOCKET_URL\",",
            "    \"DASHBOARD_AUTO_REFRESH_MODE\",",
            "    \"DASHBOARD_AUTO_REFRESH_INTERVALS\",",
            "    \"DASHBOARD_VIRTUALIZATION\",",
            "    \"SCHEDULED_QUERIES\",",
            "    \"EXCEL_EXTENSIONS\",",
            "    \"CSV_EXTENSIONS\",",
            "    \"COLUMNAR_EXTENSIONS\",",
            "    \"ALLOWED_EXTENSIONS\",",
            "    \"SAMPLES_ROW_LIMIT\",",
            "    \"DEFAULT_TIME_FILTER\",",
            "    \"HTML_SANITIZATION\",",
            "    \"HTML_SANITIZATION_SCHEMA_EXTENSIONS\",",
            "    \"WELCOME_PAGE_LAST_TAB\",",
            "    \"VIZ_TYPE_DENYLIST\",",
            "    \"ALERT_REPORTS_DEFAULT_CRON_VALUE\",",
            "    \"ALERT_REPORTS_DEFAULT_RETENTION\",",
            "    \"ALERT_REPORTS_DEFAULT_WORKING_TIMEOUT\",",
            "    \"NATIVE_FILTER_DEFAULT_ROW_LIMIT\",",
            "    \"PREVENT_UNSAFE_DEFAULT_URLS_ON_DATASET\",",
            ")",
            "",
            "logger = logging.getLogger(__name__)",
            "config = superset_app.config",
            "",
            "",
            "def get_error_msg() -> str:",
            "    if conf.get(\"SHOW_STACKTRACE\"):",
            "        error_msg = traceback.format_exc()",
            "    else:",
            "        error_msg = \"FATAL ERROR \\n\"",
            "        error_msg += (",
            "            \"Stacktrace is hidden. Change the SHOW_STACKTRACE \"",
            "            \"configuration setting to enable it\"",
            "        )",
            "    return error_msg",
            "",
            "",
            "def json_error_response(",
            "    msg: Optional[str] = None,",
            "    status: int = 500,",
            "    payload: Optional[dict[str, Any]] = None,",
            "    link: Optional[str] = None,",
            ") -> FlaskResponse:",
            "    if not payload:",
            "        payload = {\"error\": f\"{msg}\"}",
            "    if link:",
            "        payload[\"link\"] = link",
            "",
            "    return Response(",
            "        json.dumps(payload, default=utils.json_iso_dttm_ser, ignore_nan=True),",
            "        status=status,",
            "        mimetype=\"application/json\",",
            "    )",
            "",
            "",
            "def json_errors_response(",
            "    errors: list[SupersetError],",
            "    status: int = 500,",
            "    payload: Optional[dict[str, Any]] = None,",
            ") -> FlaskResponse:",
            "    if not payload:",
            "        payload = {}",
            "",
            "    payload[\"errors\"] = [dataclasses.asdict(error) for error in errors]",
            "    return Response(",
            "        json.dumps(payload, default=utils.json_iso_dttm_ser, ignore_nan=True),",
            "        status=status,",
            "        mimetype=\"application/json; charset=utf-8\",",
            "    )",
            "",
            "",
            "def json_success(json_msg: str, status: int = 200) -> FlaskResponse:",
            "    return Response(json_msg, status=status, mimetype=\"application/json\")",
            "",
            "",
            "def data_payload_response(payload_json: str, has_error: bool = False) -> FlaskResponse:",
            "    status = 400 if has_error else 200",
            "    return json_success(payload_json, status=status)",
            "",
            "",
            "def generate_download_headers(",
            "    extension: str, filename: Optional[str] = None",
            ") -> dict[str, Any]:",
            "    filename = filename if filename else datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
            "    content_disp = f\"attachment; filename={filename}.{extension}\"",
            "    headers = {\"Content-Disposition\": content_disp}",
            "    return headers",
            "",
            "",
            "def deprecated(",
            "    eol_version: str = \"4.0.0\",",
            "    new_target: Optional[str] = None,",
            ") -> Callable[[Callable[..., FlaskResponse]], Callable[..., FlaskResponse]]:",
            "    \"\"\"",
            "    A decorator to set an API endpoint from SupersetView has deprecated.",
            "    Issues a log warning",
            "    \"\"\"",
            "",
            "    def _deprecated(f: Callable[..., FlaskResponse]) -> Callable[..., FlaskResponse]:",
            "        def wraps(self: \"BaseSupersetView\", *args: Any, **kwargs: Any) -> FlaskResponse:",
            "            messsage = (",
            "                \"%s.%s \"",
            "                \"This API endpoint is deprecated and will be removed in version %s\"",
            "            )",
            "            logger_args = [",
            "                self.__class__.__name__,",
            "                f.__name__,",
            "                eol_version,",
            "            ]",
            "            if new_target:",
            "                messsage += \" . Use the following API endpoint instead: %s\"",
            "                logger_args.append(new_target)",
            "            logger.warning(messsage, *logger_args)",
            "            return f(self, *args, **kwargs)",
            "",
            "        return functools.update_wrapper(wraps, f)",
            "",
            "    return _deprecated",
            "",
            "",
            "def api(f: Callable[..., FlaskResponse]) -> Callable[..., FlaskResponse]:",
            "    \"\"\"",
            "    A decorator to label an endpoint as an API. Catches uncaught exceptions and",
            "    return the response in the JSON format",
            "    \"\"\"",
            "",
            "    def wraps(self: \"BaseSupersetView\", *args: Any, **kwargs: Any) -> FlaskResponse:",
            "        try:",
            "            return f(self, *args, **kwargs)",
            "        except NoAuthorizationError:",
            "            logger.warning(\"Api failed- no authorization\", exc_info=True)",
            "            return json_error_response(get_error_msg(), status=401)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.exception(ex)",
            "            return json_error_response(get_error_msg())",
            "",
            "    return functools.update_wrapper(wraps, f)",
            "",
            "",
            "def handle_api_exception(",
            "    f: Callable[..., FlaskResponse]",
            ") -> Callable[..., FlaskResponse]:",
            "    \"\"\"",
            "    A decorator to catch superset exceptions. Use it after the @api decorator above",
            "    so superset exception handler is triggered before the handler for generic",
            "    exceptions.",
            "    \"\"\"",
            "",
            "    def wraps(self: \"BaseSupersetView\", *args: Any, **kwargs: Any) -> FlaskResponse:",
            "        try:",
            "            return f(self, *args, **kwargs)",
            "        except SupersetSecurityException as ex:",
            "            logger.warning(\"SupersetSecurityException\", exc_info=True)",
            "            return json_errors_response(",
            "                errors=[ex.error], status=ex.status, payload=ex.payload",
            "            )",
            "        except SupersetErrorsException as ex:",
            "            logger.warning(ex, exc_info=True)",
            "            return json_errors_response(errors=ex.errors, status=ex.status)",
            "        except SupersetErrorException as ex:",
            "            logger.warning(\"SupersetErrorException\", exc_info=True)",
            "            return json_errors_response(errors=[ex.error], status=ex.status)",
            "        except SupersetException as ex:",
            "            if ex.status >= 500:",
            "                logger.exception(ex)",
            "            return json_error_response(",
            "                utils.error_msg_from_exception(ex), status=ex.status",
            "            )",
            "        except HTTPException as ex:",
            "            logger.exception(ex)",
            "            return json_error_response(",
            "                utils.error_msg_from_exception(ex), status=cast(int, ex.code)",
            "            )",
            "        except (exc.IntegrityError, exc.DatabaseError, exc.DataError) as ex:",
            "            logger.exception(ex)",
            "            return json_error_response(utils.error_msg_from_exception(ex), status=422)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.exception(ex)",
            "            return json_error_response(utils.error_msg_from_exception(ex))",
            "",
            "    return functools.update_wrapper(wraps, f)",
            "",
            "",
            "class BaseSupersetView(BaseView):",
            "    @staticmethod",
            "    def json_response(obj: Any, status: int = 200) -> FlaskResponse:",
            "        return Response(",
            "            json.dumps(obj, default=utils.json_int_dttm_ser, ignore_nan=True),",
            "            status=status,",
            "            mimetype=\"application/json\",",
            "        )",
            "",
            "    def render_app_template(self) -> FlaskResponse:",
            "        payload = {",
            "            \"user\": bootstrap_user_data(g.user, include_perms=True),",
            "            \"common\": common_bootstrap_payload(g.user),",
            "        }",
            "        return self.render_template(",
            "            \"superset/spa.html\",",
            "            entry=\"spa\",",
            "            bootstrap_data=json.dumps(",
            "                payload, default=utils.pessimistic_json_iso_dttm_ser",
            "            ),",
            "        )",
            "",
            "",
            "def get_environment_tag() -> dict[str, Any]:",
            "    # Whether flask is in debug mode (--debug)",
            "    debug = appbuilder.app.config[\"DEBUG\"]",
            "",
            "    # Getting the configuration option for ENVIRONMENT_TAG_CONFIG",
            "    env_tag_config = appbuilder.app.config[\"ENVIRONMENT_TAG_CONFIG\"]",
            "",
            "    # These are the predefined templates define in the config",
            "    env_tag_templates = env_tag_config.get(\"values\")",
            "",
            "    # This is the environment variable name from which to select the template",
            "    # default is SUPERSET_ENV (from FLASK_ENV in previous versions)",
            "    env_envvar = env_tag_config.get(\"variable\")",
            "",
            "    # this is the actual name we want to use",
            "    env_name = os.environ.get(env_envvar)",
            "",
            "    if not env_name or env_name not in env_tag_templates.keys():",
            "        env_name = \"debug\" if debug else None",
            "",
            "    env_tag = env_tag_templates.get(env_name)",
            "    return env_tag or {}",
            "",
            "",
            "def menu_data(user: User) -> dict[str, Any]:",
            "    menu = appbuilder.menu.get_data()",
            "",
            "    languages = {}",
            "    for lang in appbuilder.languages:",
            "        languages[lang] = {",
            "            **appbuilder.languages[lang],",
            "            \"url\": appbuilder.get_url_for_locale(lang),",
            "        }",
            "    brand_text = appbuilder.app.config[\"LOGO_RIGHT_TEXT\"]",
            "    if callable(brand_text):",
            "        brand_text = brand_text()",
            "    build_number = appbuilder.app.config[\"BUILD_NUMBER\"]",
            "",
            "    return {",
            "        \"menu\": menu,",
            "        \"brand\": {",
            "            \"path\": appbuilder.app.config[\"LOGO_TARGET_PATH\"] or \"/superset/welcome/\",",
            "            \"icon\": appbuilder.app_icon,",
            "            \"alt\": appbuilder.app_name,",
            "            \"tooltip\": appbuilder.app.config[\"LOGO_TOOLTIP\"],",
            "            \"text\": brand_text,",
            "        },",
            "        \"environment_tag\": get_environment_tag(),",
            "        \"navbar_right\": {",
            "            # show the watermark if the default app icon has been overridden",
            "            \"show_watermark\": (\"superset-logo-horiz\" not in appbuilder.app_icon),",
            "            \"bug_report_url\": appbuilder.app.config[\"BUG_REPORT_URL\"],",
            "            \"bug_report_icon\": appbuilder.app.config[\"BUG_REPORT_ICON\"],",
            "            \"bug_report_text\": appbuilder.app.config[\"BUG_REPORT_TEXT\"],",
            "            \"documentation_url\": appbuilder.app.config[\"DOCUMENTATION_URL\"],",
            "            \"documentation_icon\": appbuilder.app.config[\"DOCUMENTATION_ICON\"],",
            "            \"documentation_text\": appbuilder.app.config[\"DOCUMENTATION_TEXT\"],",
            "            \"version_string\": appbuilder.app.config[\"VERSION_STRING\"],",
            "            \"version_sha\": appbuilder.app.config[\"VERSION_SHA\"],",
            "            \"build_number\": build_number,",
            "            \"languages\": languages,",
            "            \"show_language_picker\": len(languages.keys()) > 1,",
            "            \"user_is_anonymous\": user.is_anonymous,",
            "            \"user_info_url\": None",
            "            if is_feature_enabled(\"MENU_HIDE_USER_INFO\")",
            "            else appbuilder.get_url_for_userinfo,",
            "            \"user_logout_url\": appbuilder.get_url_for_logout,",
            "            \"user_login_url\": appbuilder.get_url_for_login,",
            "            \"user_profile_url\": None",
            "            if user.is_anonymous or is_feature_enabled(\"MENU_HIDE_USER_INFO\")",
            "            else \"/superset/profile/\",",
            "            \"locale\": session.get(\"locale\", \"en\"),",
            "        },",
            "    }",
            "",
            "",
            "@cache_manager.cache.memoize(timeout=60)",
            "def cached_common_bootstrap_data(user: User, locale: str) -> dict[str, Any]:",
            "    \"\"\"Common data always sent to the client",
            "",
            "    The function is memoized as the return value only changes when user permissions",
            "    or configuration values change.",
            "    \"\"\"",
            "",
            "    # should not expose API TOKEN to frontend",
            "    frontend_config = {",
            "        k: (list(conf.get(k)) if isinstance(conf.get(k), set) else conf.get(k))",
            "        for k in FRONTEND_CONF_KEYS",
            "    }",
            "",
            "    if conf.get(\"SLACK_API_TOKEN\"):",
            "        frontend_config[\"ALERT_REPORTS_NOTIFICATION_METHODS\"] = [",
            "            ReportRecipientType.EMAIL,",
            "            ReportRecipientType.SLACK,",
            "        ]",
            "    else:",
            "        frontend_config[\"ALERT_REPORTS_NOTIFICATION_METHODS\"] = [",
            "            ReportRecipientType.EMAIL,",
            "        ]",
            "",
            "    # verify client has google sheets installed",
            "    available_specs = get_available_engine_specs()",
            "    frontend_config[\"HAS_GSHEETS_INSTALLED\"] = bool(available_specs[GSheetsEngineSpec])",
            "",
            "    bootstrap_data = {",
            "        \"conf\": frontend_config,",
            "        \"locale\": locale,",
            "        \"language_pack\": get_language_pack(locale),",
            "        \"d3_format\": conf.get(\"D3_FORMAT\"),",
            "        \"currencies\": conf.get(\"CURRENCIES\"),",
            "        \"feature_flags\": get_feature_flags(),",
            "        \"extra_sequential_color_schemes\": conf[\"EXTRA_SEQUENTIAL_COLOR_SCHEMES\"],",
            "        \"extra_categorical_color_schemes\": conf[\"EXTRA_CATEGORICAL_COLOR_SCHEMES\"],",
            "        \"theme_overrides\": conf[\"THEME_OVERRIDES\"],",
            "        \"menu_data\": menu_data(user),",
            "    }",
            "    bootstrap_data.update(conf[\"COMMON_BOOTSTRAP_OVERRIDES_FUNC\"](bootstrap_data))",
            "    return bootstrap_data",
            "",
            "",
            "def common_bootstrap_payload(user: User) -> dict[str, Any]:",
            "    return {",
            "        **cached_common_bootstrap_data(user, get_locale()),",
            "        \"flash_messages\": get_flashed_messages(with_categories=True),",
            "    }",
            "",
            "",
            "def get_error_level_from_status_code(  # pylint: disable=invalid-name",
            "    status: int,",
            ") -> ErrorLevel:",
            "    if status < 400:",
            "        return ErrorLevel.INFO",
            "    if status < 500:",
            "        return ErrorLevel.WARNING",
            "    return ErrorLevel.ERROR",
            "",
            "",
            "# SIP-40 compatible error responses; make sure APIs raise",
            "# SupersetErrorException or SupersetErrorsException",
            "@superset_app.errorhandler(SupersetErrorException)",
            "def show_superset_error(ex: SupersetErrorException) -> FlaskResponse:",
            "    logger.warning(\"SupersetErrorException\", exc_info=True)",
            "    return json_errors_response(errors=[ex.error], status=ex.status)",
            "",
            "",
            "@superset_app.errorhandler(SupersetErrorsException)",
            "def show_superset_errors(ex: SupersetErrorsException) -> FlaskResponse:",
            "    logger.warning(\"SupersetErrorsException\", exc_info=True)",
            "    return json_errors_response(errors=ex.errors, status=ex.status)",
            "",
            "",
            "# Redirect to login if the CSRF token is expired",
            "@superset_app.errorhandler(CSRFError)",
            "def refresh_csrf_token(ex: CSRFError) -> FlaskResponse:",
            "    logger.warning(\"Refresh CSRF token error\", exc_info=True)",
            "",
            "    if request.is_json:",
            "        return show_http_exception(ex)",
            "",
            "    return redirect(appbuilder.get_url_for_login)",
            "",
            "",
            "@superset_app.errorhandler(HTTPException)",
            "def show_http_exception(ex: HTTPException) -> FlaskResponse:",
            "    logger.warning(\"HTTPException\", exc_info=True)",
            "    if (",
            "        \"text/html\" in request.accept_mimetypes",
            "        and not config[\"DEBUG\"]",
            "        and ex.code in {404, 500}",
            "    ):",
            "        path = resource_filename(\"superset\", f\"static/assets/{ex.code}.html\")",
            "        return send_file(path, max_age=0), ex.code",
            "",
            "    return json_errors_response(",
            "        errors=[",
            "            SupersetError(",
            "                message=utils.error_msg_from_exception(ex),",
            "                error_type=SupersetErrorType.GENERIC_BACKEND_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            ),",
            "        ],",
            "        status=ex.code or 500,",
            "    )",
            "",
            "",
            "# Temporary handler for CommandException; if an API raises a",
            "# CommandException it should be fixed to map it to SupersetErrorException",
            "# or SupersetErrorsException, with a specific status code and error type",
            "@superset_app.errorhandler(CommandException)",
            "def show_command_errors(ex: CommandException) -> FlaskResponse:",
            "    logger.warning(\"CommandException\", exc_info=True)",
            "    if \"text/html\" in request.accept_mimetypes and not config[\"DEBUG\"]:",
            "        path = resource_filename(\"superset\", \"static/assets/500.html\")",
            "        return send_file(path, max_age=0), 500",
            "",
            "    extra = ex.normalized_messages() if isinstance(ex, CommandInvalidError) else {}",
            "    return json_errors_response(",
            "        errors=[",
            "            SupersetError(",
            "                message=ex.message,",
            "                error_type=SupersetErrorType.GENERIC_COMMAND_ERROR,",
            "                level=get_error_level_from_status_code(ex.status),",
            "                extra=extra,",
            "            ),",
            "        ],",
            "        status=ex.status,",
            "    )",
            "",
            "",
            "# Catch-all, to ensure all errors from the backend conform to SIP-40",
            "@superset_app.errorhandler(Exception)",
            "def show_unexpected_exception(ex: Exception) -> FlaskResponse:",
            "    logger.exception(ex)",
            "    if \"text/html\" in request.accept_mimetypes and not config[\"DEBUG\"]:",
            "        path = resource_filename(\"superset\", \"static/assets/500.html\")",
            "        return send_file(path, max_age=0), 500",
            "",
            "    return json_errors_response(",
            "        errors=[",
            "            SupersetError(",
            "                message=utils.error_msg_from_exception(ex),",
            "                error_type=SupersetErrorType.GENERIC_BACKEND_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            ),",
            "        ],",
            "    )",
            "",
            "",
            "@superset_app.context_processor",
            "def get_common_bootstrap_data() -> dict[str, Any]:",
            "    def serialize_bootstrap_data() -> str:",
            "        return json.dumps(",
            "            {\"common\": common_bootstrap_payload(g.user)},",
            "            default=utils.pessimistic_json_iso_dttm_ser,",
            "        )",
            "",
            "    return {\"bootstrap_data\": serialize_bootstrap_data}",
            "",
            "",
            "class SupersetListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    template = \"superset/fab_overrides/list.html\"",
            "",
            "",
            "class SupersetModelView(ModelView):",
            "    page_size = 100",
            "    list_widget = SupersetListWidget",
            "",
            "    def render_app_template(self) -> FlaskResponse:",
            "        payload = {",
            "            \"user\": bootstrap_user_data(g.user, include_perms=True),",
            "            \"common\": common_bootstrap_payload(g.user),",
            "        }",
            "        return self.render_template(",
            "            \"superset/spa.html\",",
            "            entry=\"spa\",",
            "            bootstrap_data=json.dumps(",
            "                payload, default=utils.pessimistic_json_iso_dttm_ser",
            "            ),",
            "        )",
            "",
            "",
            "class ListWidgetWithCheckboxes(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"An alternative to list view that renders Boolean fields as checkboxes",
            "",
            "    Works in conjunction with the `checkbox` view.\"\"\"",
            "",
            "    template = \"superset/fab_overrides/list_with_checkboxes.html\"",
            "",
            "",
            "def validate_json(form: Form, field: Field) -> None:  # pylint: disable=unused-argument",
            "    try:",
            "        json.loads(field.data)",
            "    except Exception as ex:",
            "        logger.exception(ex)",
            "        raise Exception(_(\"json isn't valid\")) from ex",
            "",
            "",
            "class YamlExportMixin:  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Override this if you want a dict response instead, with a certain key.",
            "    Used on DatabaseView for cli compatibility",
            "    \"\"\"",
            "",
            "    yaml_dict_key: Optional[str] = None",
            "",
            "    @action(\"yaml_export\", __(\"Export to YAML\"), __(\"Export to YAML?\"), \"fa-download\")",
            "    def yaml_export(",
            "        self, items: Union[ImportExportMixin, list[ImportExportMixin]]",
            "    ) -> FlaskResponse:",
            "        if not isinstance(items, list):",
            "            items = [items]",
            "",
            "        data = [t.export_to_dict() for t in items]",
            "",
            "        return Response(",
            "            yaml.safe_dump({self.yaml_dict_key: data} if self.yaml_dict_key else data),",
            "            headers=generate_download_headers(\"yaml\"),",
            "            mimetype=\"application/text\",",
            "        )",
            "",
            "",
            "class DeleteMixin:  # pylint: disable=too-few-public-methods",
            "    def _delete(self: BaseView, primary_key: int) -> None:",
            "        \"\"\"",
            "        Delete function logic, override to implement different logic",
            "        deletes the record with primary_key = primary_key",
            "",
            "        :param primary_key:",
            "            record primary key to delete",
            "        \"\"\"",
            "        item = self.datamodel.get(primary_key, self._base_filters)",
            "        if not item:",
            "            abort(404)",
            "        try:",
            "            self.pre_delete(item)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            flash(str(ex), \"danger\")",
            "        else:",
            "            view_menu = security_manager.find_view_menu(item.get_perm())",
            "            pvs = (",
            "                security_manager.get_session.query(",
            "                    security_manager.permissionview_model",
            "                )",
            "                .filter_by(view_menu=view_menu)",
            "                .all()",
            "            )",
            "",
            "            if self.datamodel.delete(item):",
            "                self.post_delete(item)",
            "",
            "                for pv in pvs:",
            "                    security_manager.get_session.delete(pv)",
            "",
            "                if view_menu:",
            "                    security_manager.get_session.delete(view_menu)",
            "",
            "                security_manager.get_session.commit()",
            "",
            "            flash(*self.datamodel.message)",
            "            self.update_redirect()",
            "",
            "    @action(",
            "        \"muldelete\", __(\"Delete\"), __(\"Delete all Really?\"), \"fa-trash\", single=False",
            "    )",
            "    def muldelete(self: BaseView, items: list[Model]) -> FlaskResponse:",
            "        if not items:",
            "            abort(404)",
            "        for item in items:",
            "            try:",
            "                self.pre_delete(item)",
            "            except Exception as ex:  # pylint: disable=broad-except",
            "                flash(str(ex), \"danger\")",
            "            else:",
            "                self._delete(item.id)",
            "        self.update_redirect()",
            "        return redirect(self.get_redirect())",
            "",
            "",
            "class DatasourceFilter(BaseFilter):  # pylint: disable=too-few-public-methods",
            "    def apply(self, query: Query, value: Any) -> Query:",
            "        if security_manager.can_access_all_datasources():",
            "            return query",
            "        query = query.join(",
            "            models.Database,",
            "            models.Database.id == self.model.database_id,",
            "        )",
            "        return query.filter(get_dataset_access_filters(self.model))",
            "",
            "",
            "class CsvResponse(Response):",
            "    \"\"\"",
            "    Override Response to take into account csv encoding from config.py",
            "    \"\"\"",
            "",
            "    charset = conf[\"CSV_EXPORT\"].get(\"encoding\", \"utf-8\")",
            "    default_mimetype = \"text/csv\"",
            "",
            "",
            "class XlsxResponse(Response):",
            "    \"\"\"",
            "    Override Response to use xlsx mimetype",
            "    \"\"\"",
            "",
            "    charset = \"utf-8\"",
            "    default_mimetype = (",
            "        \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"",
            "    )",
            "",
            "",
            "def bind_field(",
            "    _: Any, form: DynamicForm, unbound_field: UnboundField, options: dict[Any, Any]",
            ") -> Field:",
            "    \"\"\"",
            "    Customize how fields are bound by stripping all whitespace.",
            "",
            "    :param form: The form",
            "    :param unbound_field: The unbound field",
            "    :param options: The field options",
            "    :returns: The bound field",
            "    \"\"\"",
            "",
            "    filters = unbound_field.kwargs.get(\"filters\", [])",
            "    filters.append(lambda x: x.strip() if isinstance(x, str) else x)",
            "    return unbound_field.bind(form=form, filters=filters, **options)",
            "",
            "",
            "FlaskForm.Meta.bind_field = bind_field",
            "",
            "",
            "@superset_app.after_request",
            "def apply_http_headers(response: Response) -> Response:",
            "    \"\"\"Applies the configuration's http headers to all responses\"\"\"",
            "",
            "    # HTTP_HEADERS is deprecated, this provides backwards compatibility",
            "    response.headers.extend(",
            "        {**config[\"OVERRIDE_HTTP_HEADERS\"], **config[\"HTTP_HEADERS\"]}",
            "    )",
            "",
            "    for k, v in config[\"DEFAULT_HTTP_HEADERS\"].items():",
            "        if k not in response.headers:",
            "            response.headers[k] = v",
            "    return response"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "59": [],
            "66": [],
            "288": [
                "validate_sqlatable"
            ],
            "289": [
                "validate_sqlatable"
            ],
            "290": [
                "validate_sqlatable"
            ],
            "291": [
                "validate_sqlatable"
            ],
            "292": [
                "validate_sqlatable"
            ],
            "293": [
                "validate_sqlatable"
            ],
            "294": [
                "validate_sqlatable"
            ],
            "295": [
                "validate_sqlatable"
            ],
            "296": [
                "validate_sqlatable"
            ],
            "297": [
                "validate_sqlatable"
            ],
            "298": [
                "validate_sqlatable"
            ],
            "299": [
                "validate_sqlatable"
            ],
            "300": [
                "validate_sqlatable"
            ],
            "301": [
                "validate_sqlatable"
            ],
            "302": [
                "validate_sqlatable"
            ],
            "303": [
                "validate_sqlatable"
            ],
            "304": [
                "validate_sqlatable"
            ],
            "305": [
                "validate_sqlatable"
            ],
            "306": [
                "validate_sqlatable"
            ],
            "307": [
                "validate_sqlatable"
            ],
            "308": [
                "validate_sqlatable"
            ],
            "309": [
                "validate_sqlatable"
            ],
            "310": [
                "validate_sqlatable"
            ],
            "311": [
                "validate_sqlatable"
            ],
            "312": [],
            "313": []
        },
        "addLocation": []
    }
}