{
    "case_utils/case_file/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " DEFAULT_PREFIX = \"http://example.org/kb/\""
            },
            "2": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 43,
                "PatchRowcode": " # Shortcut syntax for defining an immutable named tuple is noted here:"
            },
            "5": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " # https://docs.python.org/3/library/typing.html#typing.NamedTuple"
            },
            "6": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " # via the \"See also\" box here: https://docs.python.org/3/library/collections.html#collections.namedtuple"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python3",
            "",
            "# This software was developed at the National Institute of Standards",
            "# and Technology by employees of the Federal Government in the course",
            "# of their official duties. Pursuant to title 17 Section 105 of the",
            "# United States Code this software is not subject to copyright",
            "# protection and is in the public domain. NIST assumes no",
            "# responsibility whatsoever for its use by other parties, and makes",
            "# no guarantees, expressed or implied, about its quality,",
            "# reliability, or any other characteristic.",
            "#",
            "# We would appreciate acknowledgement if the software is used.",
            "",
            "\"\"\"",
            "This module creates a graph object that provides a basic UCO characterization of a single file.  The gathered metadata is among the more \"durable\" file characteristics, i.e. characteristics that would remain consistent when transferring a file between locations.",
            "\"\"\"",
            "",
            "__version__ = \"0.4.0\"",
            "",
            "import argparse",
            "import datetime",
            "import hashlib",
            "import logging",
            "import os",
            "import typing",
            "import warnings",
            "",
            "import rdflib",
            "",
            "import case_utils",
            "from case_utils.namespace import (",
            "    NS_RDF,",
            "    NS_UCO_CORE,",
            "    NS_UCO_OBSERVABLE,",
            "    NS_UCO_TYPES,",
            "    NS_UCO_VOCABULARY,",
            "    NS_XSD,",
            ")",
            "",
            "DEFAULT_PREFIX = \"http://example.org/kb/\"",
            "",
            "# Shortcut syntax for defining an immutable named tuple is noted here:",
            "# https://docs.python.org/3/library/typing.html#typing.NamedTuple",
            "# via the \"See also\" box here: https://docs.python.org/3/library/collections.html#collections.namedtuple",
            "class HashDict(typing.NamedTuple):",
            "    filesize: int",
            "    md5: str",
            "    sha1: str",
            "    sha256: str",
            "    sha512: str",
            "",
            "",
            "def create_file_node(",
            "    graph: rdflib.Graph,",
            "    filepath: str,",
            "    node_iri: typing.Optional[str] = None,",
            "    node_prefix: str = DEFAULT_PREFIX,",
            "    disable_hashes: bool = False,",
            "    disable_mtime: bool = False,",
            ") -> rdflib.URIRef:",
            "    r\"\"\"",
            "    This function characterizes the file at filepath.",
            "",
            "    :param graph: The rdflib Graph that will house the new triples characterizing the file.",
            "    :type graph: rdflib.Graph",
            "",
            "    :param filepath: The path to the file to characterize.  Can be relative or absolute.",
            "    :type filepath: str",
            "",
            "    :param node_iri: The desired full IRI for the node.  If absent, will make an IRI of the pattern ``ns_base + 'file-' + uuid4``",
            "    :type node_iri: str",
            "",
            "    :param node_prefix: The base prefix to use if node_iri is not supplied.",
            "    :type node_prefix: str",
            "",
            "    :param disable_hashes: Skip computing hashes.",
            "    :type disable_hashes: bool",
            "",
            "    :param disable_mtime: Skip recording mtime.",
            "    :type disable_mtime: bool",
            "",
            "    :returns: The File Observable Object's node.",
            "    :rtype: rdflib.URIRef",
            "    \"\"\"",
            "    node_namespace = rdflib.Namespace(node_prefix)",
            "",
            "    if node_iri is None:",
            "        node_slug = \"file-\" + case_utils.local_uuid.local_uuid()",
            "        node_iri = node_namespace[node_slug]",
            "    n_file = rdflib.URIRef(node_iri)",
            "    graph.add((n_file, NS_RDF.type, NS_UCO_OBSERVABLE.File))",
            "",
            "    basename = os.path.basename(filepath)",
            "    literal_basename = rdflib.Literal(basename)",
            "",
            "    file_stat = os.stat(filepath)",
            "    n_file_facet = node_namespace[\"file-facet-\" + case_utils.local_uuid.local_uuid()]",
            "    graph.add(",
            "        (",
            "            n_file_facet,",
            "            NS_RDF.type,",
            "            NS_UCO_OBSERVABLE.FileFacet,",
            "        )",
            "    )",
            "    graph.add((n_file_facet, NS_UCO_OBSERVABLE.fileName, literal_basename))",
            "    graph.add(",
            "        (",
            "            n_file_facet,",
            "            NS_UCO_OBSERVABLE.sizeInBytes,",
            "            rdflib.Literal(int(file_stat.st_size)),",
            "        )",
            "    )",
            "    graph.add((n_file, NS_UCO_CORE.hasFacet, n_file_facet))",
            "",
            "    if not disable_mtime:",
            "        mtime_datetime = datetime.datetime.fromtimestamp(",
            "            file_stat.st_mtime, tz=datetime.timezone.utc",
            "        )",
            "        str_mtime = mtime_datetime.isoformat()",
            "        literal_mtime = rdflib.Literal(str_mtime, datatype=NS_XSD.dateTime)",
            "        graph.add((n_file_facet, NS_UCO_OBSERVABLE.modifiedTime, literal_mtime))",
            "",
            "    if not disable_hashes:",
            "        n_contentdata_facet = node_namespace[",
            "            \"content-data-facet-\" + case_utils.local_uuid.local_uuid()",
            "        ]",
            "        graph.add((n_file, NS_UCO_CORE.hasFacet, n_contentdata_facet))",
            "        graph.add(",
            "            (n_contentdata_facet, NS_RDF.type, NS_UCO_OBSERVABLE.ContentDataFacet)",
            "        )",
            "",
            "        # Compute hashes until they are re-computed and match once.  (This is a lesson learned from working with a NAS that had a subtly faulty network cable.)",
            "",
            "        successful_hashdict: typing.Optional[HashDict] = None",
            "        last_hashdict: typing.Optional[HashDict] = None",
            "        for attempt_no in [0, 1, 2, 3]:",
            "            # Hash file's contents.",
            "            # This hashing logic was partially copied from DFXML's walk_to_dfxml.py.",
            "            md5obj = hashlib.md5()",
            "            sha1obj = hashlib.sha1()",
            "            sha256obj = hashlib.sha256()",
            "            sha512obj = hashlib.sha512()",
            "            stashed_error = None",
            "            byte_tally = 0",
            "            with open(filepath, \"rb\") as in_fh:",
            "                chunk_size = 2**22",
            "                while True:",
            "                    buf = b\"\"",
            "                    try:",
            "                        buf = in_fh.read(chunk_size)",
            "                        byte_tally += len(buf)",
            "                    except Exception as e:",
            "                        stashed_error = e",
            "                        buf = b\"\"",
            "                    if buf == b\"\":",
            "                        break",
            "                    md5obj.update(buf)",
            "                    sha1obj.update(buf)",
            "                    sha256obj.update(buf)",
            "                    sha512obj.update(buf)",
            "            if stashed_error is not None:",
            "                raise stashed_error",
            "            current_hashdict = HashDict(",
            "                byte_tally,",
            "                md5obj.hexdigest(),",
            "                sha1obj.hexdigest(),",
            "                sha256obj.hexdigest(),",
            "                sha512obj.hexdigest(),",
            "            )",
            "            if last_hashdict == current_hashdict:",
            "                successful_hashdict = current_hashdict",
            "                break",
            "            else:",
            "                last_hashdict = current_hashdict",
            "        del last_hashdict",
            "        del current_hashdict",
            "        if successful_hashdict is None:",
            "            raise ValueError(\"Failed to confirm hashes of file %r.\" % filepath)",
            "        if successful_hashdict.filesize != file_stat.st_size:",
            "            # TODO - Discuss with AC whether this should be something stronger, like an assertion error.",
            "            warnings.warn(",
            "                \"Inode file size and hashed file sizes disagree: %d vs. %d.\"",
            "                % (file_stat.st_size, successful_hashdict.filesize)",
            "            )",
            "        # TODO - Discuss whether this property should be recorded even if hashes are not attempted.",
            "        graph.add(",
            "            (",
            "                n_contentdata_facet,",
            "                NS_UCO_OBSERVABLE.sizeInBytes,",
            "                rdflib.Literal(successful_hashdict.filesize),",
            "            )",
            "        )",
            "",
            "        # Add confirmed hashes into graph.",
            "        for key in successful_hashdict._fields:",
            "            if key not in (\"md5\", \"sha1\", \"sha256\", \"sha512\"):",
            "                continue",
            "            n_hash = node_namespace[\"hash-\" + case_utils.local_uuid.local_uuid()]",
            "            graph.add((n_contentdata_facet, NS_UCO_OBSERVABLE.hash, n_hash))",
            "            graph.add((n_hash, NS_RDF.type, NS_UCO_TYPES.Hash))",
            "            graph.add(",
            "                (",
            "                    n_hash,",
            "                    NS_UCO_TYPES.hashMethod,",
            "                    rdflib.Literal(",
            "                        key.upper(), datatype=NS_UCO_VOCABULARY.HashNameVocab",
            "                    ),",
            "                )",
            "            )",
            "            hash_value = getattr(successful_hashdict, key)",
            "            graph.add(",
            "                (",
            "                    n_hash,",
            "                    NS_UCO_TYPES.hashValue,",
            "                    rdflib.Literal(hash_value.upper(), datatype=NS_XSD.hexBinary),",
            "                )",
            "            )",
            "",
            "    return n_file",
            "",
            "",
            "def main() -> None:",
            "    parser = argparse.ArgumentParser()",
            "    parser.add_argument(\"--base-prefix\", default=DEFAULT_PREFIX)",
            "    parser.add_argument(\"--debug\", action=\"store_true\")",
            "    parser.add_argument(\"--disable-hashes\", action=\"store_true\")",
            "    parser.add_argument(\"--disable-mtime\", action=\"store_true\")",
            "    parser.add_argument(",
            "        \"--output-format\", help=\"Override extension-based format guesser.\"",
            "    )",
            "    parser.add_argument(\"out_graph\")",
            "    parser.add_argument(\"in_file\")",
            "    args = parser.parse_args()",
            "",
            "    logging.basicConfig(level=logging.DEBUG if args.debug else logging.INFO)",
            "",
            "    case_utils.local_uuid.configure()",
            "",
            "    NS_BASE = rdflib.Namespace(args.base_prefix)",
            "",
            "    graph = rdflib.Graph()",
            "    graph.namespace_manager.bind(\"kb\", NS_BASE)",
            "    graph.namespace_manager.bind(\"uco-core\", NS_UCO_CORE)",
            "    graph.namespace_manager.bind(\"uco-observable\", NS_UCO_OBSERVABLE)",
            "    graph.namespace_manager.bind(\"uco-types\", NS_UCO_TYPES)",
            "    graph.namespace_manager.bind(\"uco-vocabulary\", NS_UCO_VOCABULARY)",
            "    graph.namespace_manager.bind(\"xsd\", NS_XSD)",
            "",
            "    output_format = None",
            "    if args.output_format is None:",
            "        output_format = rdflib.util.guess_format(args.out_graph)",
            "    else:",
            "        output_format = args.output_format",
            "",
            "    serialize_kwargs: typing.Dict[str, typing.Any] = {\"format\": output_format}",
            "    if output_format == \"json-ld\":",
            "        context_dictionary = {k: v for (k, v) in graph.namespace_manager.namespaces()}",
            "        serialize_kwargs[\"context\"] = context_dictionary",
            "",
            "    node_iri = NS_BASE[\"file-\" + case_utils.local_uuid.local_uuid()]",
            "    create_file_node(",
            "        graph,",
            "        args.in_file,",
            "        node_iri=node_iri,",
            "        node_prefix=args.base_prefix,",
            "        disable_hashes=args.disable_hashes,",
            "        disable_mtime=args.disable_mtime,",
            "    )",
            "",
            "    graph.serialize(args.out_graph, **serialize_kwargs)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python3",
            "",
            "# This software was developed at the National Institute of Standards",
            "# and Technology by employees of the Federal Government in the course",
            "# of their official duties. Pursuant to title 17 Section 105 of the",
            "# United States Code this software is not subject to copyright",
            "# protection and is in the public domain. NIST assumes no",
            "# responsibility whatsoever for its use by other parties, and makes",
            "# no guarantees, expressed or implied, about its quality,",
            "# reliability, or any other characteristic.",
            "#",
            "# We would appreciate acknowledgement if the software is used.",
            "",
            "\"\"\"",
            "This module creates a graph object that provides a basic UCO characterization of a single file.  The gathered metadata is among the more \"durable\" file characteristics, i.e. characteristics that would remain consistent when transferring a file between locations.",
            "\"\"\"",
            "",
            "__version__ = \"0.4.0\"",
            "",
            "import argparse",
            "import datetime",
            "import hashlib",
            "import logging",
            "import os",
            "import typing",
            "import warnings",
            "",
            "import rdflib",
            "",
            "import case_utils",
            "from case_utils.namespace import (",
            "    NS_RDF,",
            "    NS_UCO_CORE,",
            "    NS_UCO_OBSERVABLE,",
            "    NS_UCO_TYPES,",
            "    NS_UCO_VOCABULARY,",
            "    NS_XSD,",
            ")",
            "",
            "DEFAULT_PREFIX = \"http://example.org/kb/\"",
            "",
            "",
            "# Shortcut syntax for defining an immutable named tuple is noted here:",
            "# https://docs.python.org/3/library/typing.html#typing.NamedTuple",
            "# via the \"See also\" box here: https://docs.python.org/3/library/collections.html#collections.namedtuple",
            "class HashDict(typing.NamedTuple):",
            "    filesize: int",
            "    md5: str",
            "    sha1: str",
            "    sha256: str",
            "    sha512: str",
            "",
            "",
            "def create_file_node(",
            "    graph: rdflib.Graph,",
            "    filepath: str,",
            "    node_iri: typing.Optional[str] = None,",
            "    node_prefix: str = DEFAULT_PREFIX,",
            "    disable_hashes: bool = False,",
            "    disable_mtime: bool = False,",
            ") -> rdflib.URIRef:",
            "    r\"\"\"",
            "    This function characterizes the file at filepath.",
            "",
            "    :param graph: The rdflib Graph that will house the new triples characterizing the file.",
            "    :type graph: rdflib.Graph",
            "",
            "    :param filepath: The path to the file to characterize.  Can be relative or absolute.",
            "    :type filepath: str",
            "",
            "    :param node_iri: The desired full IRI for the node.  If absent, will make an IRI of the pattern ``ns_base + 'file-' + uuid4``",
            "    :type node_iri: str",
            "",
            "    :param node_prefix: The base prefix to use if node_iri is not supplied.",
            "    :type node_prefix: str",
            "",
            "    :param disable_hashes: Skip computing hashes.",
            "    :type disable_hashes: bool",
            "",
            "    :param disable_mtime: Skip recording mtime.",
            "    :type disable_mtime: bool",
            "",
            "    :returns: The File Observable Object's node.",
            "    :rtype: rdflib.URIRef",
            "    \"\"\"",
            "    node_namespace = rdflib.Namespace(node_prefix)",
            "",
            "    if node_iri is None:",
            "        node_slug = \"file-\" + case_utils.local_uuid.local_uuid()",
            "        node_iri = node_namespace[node_slug]",
            "    n_file = rdflib.URIRef(node_iri)",
            "    graph.add((n_file, NS_RDF.type, NS_UCO_OBSERVABLE.File))",
            "",
            "    basename = os.path.basename(filepath)",
            "    literal_basename = rdflib.Literal(basename)",
            "",
            "    file_stat = os.stat(filepath)",
            "    n_file_facet = node_namespace[\"file-facet-\" + case_utils.local_uuid.local_uuid()]",
            "    graph.add(",
            "        (",
            "            n_file_facet,",
            "            NS_RDF.type,",
            "            NS_UCO_OBSERVABLE.FileFacet,",
            "        )",
            "    )",
            "    graph.add((n_file_facet, NS_UCO_OBSERVABLE.fileName, literal_basename))",
            "    graph.add(",
            "        (",
            "            n_file_facet,",
            "            NS_UCO_OBSERVABLE.sizeInBytes,",
            "            rdflib.Literal(int(file_stat.st_size)),",
            "        )",
            "    )",
            "    graph.add((n_file, NS_UCO_CORE.hasFacet, n_file_facet))",
            "",
            "    if not disable_mtime:",
            "        mtime_datetime = datetime.datetime.fromtimestamp(",
            "            file_stat.st_mtime, tz=datetime.timezone.utc",
            "        )",
            "        str_mtime = mtime_datetime.isoformat()",
            "        literal_mtime = rdflib.Literal(str_mtime, datatype=NS_XSD.dateTime)",
            "        graph.add((n_file_facet, NS_UCO_OBSERVABLE.modifiedTime, literal_mtime))",
            "",
            "    if not disable_hashes:",
            "        n_contentdata_facet = node_namespace[",
            "            \"content-data-facet-\" + case_utils.local_uuid.local_uuid()",
            "        ]",
            "        graph.add((n_file, NS_UCO_CORE.hasFacet, n_contentdata_facet))",
            "        graph.add(",
            "            (n_contentdata_facet, NS_RDF.type, NS_UCO_OBSERVABLE.ContentDataFacet)",
            "        )",
            "",
            "        # Compute hashes until they are re-computed and match once.  (This is a lesson learned from working with a NAS that had a subtly faulty network cable.)",
            "",
            "        successful_hashdict: typing.Optional[HashDict] = None",
            "        last_hashdict: typing.Optional[HashDict] = None",
            "        for attempt_no in [0, 1, 2, 3]:",
            "            # Hash file's contents.",
            "            # This hashing logic was partially copied from DFXML's walk_to_dfxml.py.",
            "            md5obj = hashlib.md5()",
            "            sha1obj = hashlib.sha1()",
            "            sha256obj = hashlib.sha256()",
            "            sha512obj = hashlib.sha512()",
            "            stashed_error = None",
            "            byte_tally = 0",
            "            with open(filepath, \"rb\") as in_fh:",
            "                chunk_size = 2**22",
            "                while True:",
            "                    buf = b\"\"",
            "                    try:",
            "                        buf = in_fh.read(chunk_size)",
            "                        byte_tally += len(buf)",
            "                    except Exception as e:",
            "                        stashed_error = e",
            "                        buf = b\"\"",
            "                    if buf == b\"\":",
            "                        break",
            "                    md5obj.update(buf)",
            "                    sha1obj.update(buf)",
            "                    sha256obj.update(buf)",
            "                    sha512obj.update(buf)",
            "            if stashed_error is not None:",
            "                raise stashed_error",
            "            current_hashdict = HashDict(",
            "                byte_tally,",
            "                md5obj.hexdigest(),",
            "                sha1obj.hexdigest(),",
            "                sha256obj.hexdigest(),",
            "                sha512obj.hexdigest(),",
            "            )",
            "            if last_hashdict == current_hashdict:",
            "                successful_hashdict = current_hashdict",
            "                break",
            "            else:",
            "                last_hashdict = current_hashdict",
            "        del last_hashdict",
            "        del current_hashdict",
            "        if successful_hashdict is None:",
            "            raise ValueError(\"Failed to confirm hashes of file %r.\" % filepath)",
            "        if successful_hashdict.filesize != file_stat.st_size:",
            "            # TODO - Discuss with AC whether this should be something stronger, like an assertion error.",
            "            warnings.warn(",
            "                \"Inode file size and hashed file sizes disagree: %d vs. %d.\"",
            "                % (file_stat.st_size, successful_hashdict.filesize)",
            "            )",
            "        # TODO - Discuss whether this property should be recorded even if hashes are not attempted.",
            "        graph.add(",
            "            (",
            "                n_contentdata_facet,",
            "                NS_UCO_OBSERVABLE.sizeInBytes,",
            "                rdflib.Literal(successful_hashdict.filesize),",
            "            )",
            "        )",
            "",
            "        # Add confirmed hashes into graph.",
            "        for key in successful_hashdict._fields:",
            "            if key not in (\"md5\", \"sha1\", \"sha256\", \"sha512\"):",
            "                continue",
            "            n_hash = node_namespace[\"hash-\" + case_utils.local_uuid.local_uuid()]",
            "            graph.add((n_contentdata_facet, NS_UCO_OBSERVABLE.hash, n_hash))",
            "            graph.add((n_hash, NS_RDF.type, NS_UCO_TYPES.Hash))",
            "            graph.add(",
            "                (",
            "                    n_hash,",
            "                    NS_UCO_TYPES.hashMethod,",
            "                    rdflib.Literal(",
            "                        key.upper(), datatype=NS_UCO_VOCABULARY.HashNameVocab",
            "                    ),",
            "                )",
            "            )",
            "            hash_value = getattr(successful_hashdict, key)",
            "            graph.add(",
            "                (",
            "                    n_hash,",
            "                    NS_UCO_TYPES.hashValue,",
            "                    rdflib.Literal(hash_value.upper(), datatype=NS_XSD.hexBinary),",
            "                )",
            "            )",
            "",
            "    return n_file",
            "",
            "",
            "def main() -> None:",
            "    parser = argparse.ArgumentParser()",
            "    parser.add_argument(\"--base-prefix\", default=DEFAULT_PREFIX)",
            "    parser.add_argument(\"--debug\", action=\"store_true\")",
            "    parser.add_argument(\"--disable-hashes\", action=\"store_true\")",
            "    parser.add_argument(\"--disable-mtime\", action=\"store_true\")",
            "    parser.add_argument(",
            "        \"--output-format\", help=\"Override extension-based format guesser.\"",
            "    )",
            "    parser.add_argument(\"out_graph\")",
            "    parser.add_argument(\"in_file\")",
            "    args = parser.parse_args()",
            "",
            "    logging.basicConfig(level=logging.DEBUG if args.debug else logging.INFO)",
            "",
            "    case_utils.local_uuid.configure()",
            "",
            "    NS_BASE = rdflib.Namespace(args.base_prefix)",
            "",
            "    graph = rdflib.Graph()",
            "    graph.namespace_manager.bind(\"kb\", NS_BASE)",
            "    graph.namespace_manager.bind(\"uco-core\", NS_UCO_CORE)",
            "    graph.namespace_manager.bind(\"uco-observable\", NS_UCO_OBSERVABLE)",
            "    graph.namespace_manager.bind(\"uco-types\", NS_UCO_TYPES)",
            "    graph.namespace_manager.bind(\"uco-vocabulary\", NS_UCO_VOCABULARY)",
            "    graph.namespace_manager.bind(\"xsd\", NS_XSD)",
            "",
            "    output_format = None",
            "    if args.output_format is None:",
            "        output_format = rdflib.util.guess_format(args.out_graph)",
            "    else:",
            "        output_format = args.output_format",
            "",
            "    serialize_kwargs: typing.Dict[str, typing.Any] = {\"format\": output_format}",
            "    if output_format == \"json-ld\":",
            "        context_dictionary = {k: v for (k, v) in graph.namespace_manager.namespaces()}",
            "        serialize_kwargs[\"context\"] = context_dictionary",
            "",
            "    node_iri = NS_BASE[\"file-\" + case_utils.local_uuid.local_uuid()]",
            "    create_file_node(",
            "        graph,",
            "        args.in_file,",
            "        node_iri=node_iri,",
            "        node_prefix=args.base_prefix,",
            "        disable_hashes=args.disable_hashes,",
            "        disable_mtime=args.disable_mtime,",
            "    )",
            "",
            "    graph.serialize(args.out_graph, **serialize_kwargs)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "case_utils/case_sparql_construct/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "     construct_query_result = in_graph.query(construct_query_object)"
            },
            "1": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "     _logger.debug(\"type(construct_query_result) = %r.\" % type(construct_query_result))"
            },
            "2": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "     _logger.debug(\"len(construct_query_result) = %d.\" % len(construct_query_result))"
            },
            "3": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for (row_no, row) in enumerate(construct_query_result):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+    for row_no, row in enumerate(construct_query_result):"
            },
            "5": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "         if row_no == 0:"
            },
            "6": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "             _logger.debug(\"row[0] = %r.\" % (row,))"
            },
            "7": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "         out_graph.add(row)"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python3",
            "",
            "# This software was developed at the National Institute of Standards",
            "# and Technology by employees of the Federal Government in the course",
            "# of their official duties. Pursuant to title 17 Section 105 of the",
            "# United States Code this software is not subject to copyright",
            "# protection and is in the public domain. NIST assumes no",
            "# responsibility whatsoever for its use by other parties, and makes",
            "# no guarantees, expressed or implied, about its quality,",
            "# reliability, or any other characteristic.",
            "#",
            "# We would appreciate acknowledgement if the software is used.",
            "",
            "\"\"\"",
            "This script executes a SPARQL CONSTRUCT query, returning a graph of the generated triples.",
            "\"\"\"",
            "",
            "__version__ = \"0.2.3\"",
            "",
            "import argparse",
            "import logging",
            "import os",
            "import sys",
            "import typing",
            "",
            "import rdflib.plugins.sparql",
            "",
            "import case_utils.ontology",
            "from case_utils.ontology.version_info import (",
            "    CURRENT_CASE_VERSION,",
            "    built_version_choices_list,",
            ")",
            "",
            "_logger = logging.getLogger(os.path.basename(__file__))",
            "",
            "",
            "def main() -> None:",
            "    parser = argparse.ArgumentParser()",
            "",
            "    # Configure debug logging before running parse_args, because there could be an error raised before the construction of the argument parser.",
            "    logging.basicConfig(",
            "        level=logging.DEBUG",
            "        if (\"--debug\" in sys.argv or \"-d\" in sys.argv)",
            "        else logging.INFO",
            "    )",
            "",
            "    parser.add_argument(\"-d\", \"--debug\", action=\"store_true\")",
            "    parser.add_argument(",
            "        \"--built-version\",",
            "        choices=tuple(built_version_choices_list),",
            "        default=\"case-\" + CURRENT_CASE_VERSION,",
            "        help=\"Ontology version to use to supplement query, such as for subclass querying.  Does not require networking to use.  Default is most recent CASE release.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--disallow-empty-results\",",
            "        action=\"store_true\",",
            "        help=\"Raise error if no results are returned for query.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--output-format\", help=\"Override extension-based format guesser.\"",
            "    )",
            "    parser.add_argument(\"out_graph\")",
            "    parser.add_argument(",
            "        \"in_sparql\",",
            "        help=\"File containing a SPARQL CONSTRUCT query.  Note that prefixes not mapped with a PREFIX statement will be mapped according to their first occurrence among input graphs.\",",
            "    )",
            "    parser.add_argument(\"in_graph\", nargs=\"+\")",
            "    args = parser.parse_args()",
            "",
            "    in_graph = rdflib.Graph()",
            "    for in_graph_filename in args.in_graph:",
            "        in_graph.parse(in_graph_filename)",
            "        _logger.debug(\"len(in_graph) = %d.\", len(in_graph))",
            "",
            "    out_graph = rdflib.Graph()",
            "",
            "    # Inherit prefixes defined in input context dictionary.",
            "    nsdict = {k: v for (k, v) in in_graph.namespace_manager.namespaces()}",
            "    for prefix in sorted(nsdict.keys()):",
            "        out_graph.bind(prefix, nsdict[prefix])",
            "",
            "    _logger.debug(\"Running query in %r.\" % args.in_sparql)",
            "    construct_query_text = None",
            "    with open(args.in_sparql, \"r\") as in_fh:",
            "        construct_query_text = in_fh.read().strip()",
            "    assert construct_query_text is not None",
            "",
            "    if \"subClassOf\" in construct_query_text:",
            "        case_utils.ontology.load_subclass_hierarchy(",
            "            in_graph, built_version=args.built_version",
            "        )",
            "",
            "    construct_query_object = rdflib.plugins.sparql.processor.prepareQuery(",
            "        construct_query_text, initNs=nsdict",
            "    )",
            "",
            "    # https://rdfextras.readthedocs.io/en/latest/working_with.html",
            "    construct_query_result = in_graph.query(construct_query_object)",
            "    _logger.debug(\"type(construct_query_result) = %r.\" % type(construct_query_result))",
            "    _logger.debug(\"len(construct_query_result) = %d.\" % len(construct_query_result))",
            "    for (row_no, row) in enumerate(construct_query_result):",
            "        if row_no == 0:",
            "            _logger.debug(\"row[0] = %r.\" % (row,))",
            "        out_graph.add(row)",
            "",
            "    output_format = None",
            "    if args.output_format is None:",
            "        output_format = rdflib.util.guess_format(args.out_graph)",
            "    else:",
            "        output_format = args.output_format",
            "",
            "    serialize_kwargs: typing.Dict[str, typing.Any] = {\"format\": output_format}",
            "    if output_format == \"json-ld\":",
            "        context_dictionary = {",
            "            k: v for (k, v) in out_graph.namespace_manager.namespaces()",
            "        }",
            "        serialize_kwargs[\"context\"] = context_dictionary",
            "",
            "    out_graph.serialize(args.out_graph, **serialize_kwargs)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python3",
            "",
            "# This software was developed at the National Institute of Standards",
            "# and Technology by employees of the Federal Government in the course",
            "# of their official duties. Pursuant to title 17 Section 105 of the",
            "# United States Code this software is not subject to copyright",
            "# protection and is in the public domain. NIST assumes no",
            "# responsibility whatsoever for its use by other parties, and makes",
            "# no guarantees, expressed or implied, about its quality,",
            "# reliability, or any other characteristic.",
            "#",
            "# We would appreciate acknowledgement if the software is used.",
            "",
            "\"\"\"",
            "This script executes a SPARQL CONSTRUCT query, returning a graph of the generated triples.",
            "\"\"\"",
            "",
            "__version__ = \"0.2.3\"",
            "",
            "import argparse",
            "import logging",
            "import os",
            "import sys",
            "import typing",
            "",
            "import rdflib.plugins.sparql",
            "",
            "import case_utils.ontology",
            "from case_utils.ontology.version_info import (",
            "    CURRENT_CASE_VERSION,",
            "    built_version_choices_list,",
            ")",
            "",
            "_logger = logging.getLogger(os.path.basename(__file__))",
            "",
            "",
            "def main() -> None:",
            "    parser = argparse.ArgumentParser()",
            "",
            "    # Configure debug logging before running parse_args, because there could be an error raised before the construction of the argument parser.",
            "    logging.basicConfig(",
            "        level=logging.DEBUG",
            "        if (\"--debug\" in sys.argv or \"-d\" in sys.argv)",
            "        else logging.INFO",
            "    )",
            "",
            "    parser.add_argument(\"-d\", \"--debug\", action=\"store_true\")",
            "    parser.add_argument(",
            "        \"--built-version\",",
            "        choices=tuple(built_version_choices_list),",
            "        default=\"case-\" + CURRENT_CASE_VERSION,",
            "        help=\"Ontology version to use to supplement query, such as for subclass querying.  Does not require networking to use.  Default is most recent CASE release.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--disallow-empty-results\",",
            "        action=\"store_true\",",
            "        help=\"Raise error if no results are returned for query.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--output-format\", help=\"Override extension-based format guesser.\"",
            "    )",
            "    parser.add_argument(\"out_graph\")",
            "    parser.add_argument(",
            "        \"in_sparql\",",
            "        help=\"File containing a SPARQL CONSTRUCT query.  Note that prefixes not mapped with a PREFIX statement will be mapped according to their first occurrence among input graphs.\",",
            "    )",
            "    parser.add_argument(\"in_graph\", nargs=\"+\")",
            "    args = parser.parse_args()",
            "",
            "    in_graph = rdflib.Graph()",
            "    for in_graph_filename in args.in_graph:",
            "        in_graph.parse(in_graph_filename)",
            "        _logger.debug(\"len(in_graph) = %d.\", len(in_graph))",
            "",
            "    out_graph = rdflib.Graph()",
            "",
            "    # Inherit prefixes defined in input context dictionary.",
            "    nsdict = {k: v for (k, v) in in_graph.namespace_manager.namespaces()}",
            "    for prefix in sorted(nsdict.keys()):",
            "        out_graph.bind(prefix, nsdict[prefix])",
            "",
            "    _logger.debug(\"Running query in %r.\" % args.in_sparql)",
            "    construct_query_text = None",
            "    with open(args.in_sparql, \"r\") as in_fh:",
            "        construct_query_text = in_fh.read().strip()",
            "    assert construct_query_text is not None",
            "",
            "    if \"subClassOf\" in construct_query_text:",
            "        case_utils.ontology.load_subclass_hierarchy(",
            "            in_graph, built_version=args.built_version",
            "        )",
            "",
            "    construct_query_object = rdflib.plugins.sparql.processor.prepareQuery(",
            "        construct_query_text, initNs=nsdict",
            "    )",
            "",
            "    # https://rdfextras.readthedocs.io/en/latest/working_with.html",
            "    construct_query_result = in_graph.query(construct_query_object)",
            "    _logger.debug(\"type(construct_query_result) = %r.\" % type(construct_query_result))",
            "    _logger.debug(\"len(construct_query_result) = %d.\" % len(construct_query_result))",
            "    for row_no, row in enumerate(construct_query_result):",
            "        if row_no == 0:",
            "            _logger.debug(\"row[0] = %r.\" % (row,))",
            "        out_graph.add(row)",
            "",
            "    output_format = None",
            "    if args.output_format is None:",
            "        output_format = rdflib.util.guess_format(args.out_graph)",
            "    else:",
            "        output_format = args.output_format",
            "",
            "    serialize_kwargs: typing.Dict[str, typing.Any] = {\"format\": output_format}",
            "    if output_format == \"json-ld\":",
            "        context_dictionary = {",
            "            k: v for (k, v) in out_graph.namespace_manager.namespaces()",
            "        }",
            "        serialize_kwargs[\"context\"] = context_dictionary",
            "",
            "    out_graph.serialize(args.out_graph, **serialize_kwargs)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "101": [
                "main"
            ]
        },
        "addLocation": []
    },
    "case_utils/case_sparql_select/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "     select_query_object = rdflib.plugins.sparql.processor.prepareQuery("
            },
            "1": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "         select_query_text, initNs=nsdict"
            },
            "2": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "     )"
            },
            "3": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for (row_no, row) in enumerate(graph.query(select_query_object)):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+    for row_no, row in enumerate(graph.query(select_query_object)):"
            },
            "5": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "         tally = row_no + 1"
            },
            "6": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "         record = []"
            },
            "7": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        for (column_no, column) in enumerate(row):"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        for column_no, column in enumerate(row):"
            },
            "9": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "             if column is None:"
            },
            "10": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "                 column_value = \"\""
            },
            "11": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "             elif ("
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python3",
            "",
            "# This software was developed at the National Institute of Standards",
            "# and Technology by employees of the Federal Government in the course",
            "# of their official duties. Pursuant to title 17 Section 105 of the",
            "# United States Code this software is not subject to copyright",
            "# protection and is in the public domain. NIST assumes no",
            "# responsibility whatsoever for its use by other parties, and makes",
            "# no guarantees, expressed or implied, about its quality,",
            "# reliability, or any other characteristic.",
            "#",
            "# We would appreciate acknowledgement if the software is used.",
            "",
            "\"\"\"",
            "This script executes a SPARQL SELECT query, returning a table representation.  The design of the workflow is based on this example built on SPARQLWrapper:",
            "https://lawlesst.github.io/notebook/sparql-dataframe.html",
            "",
            "Note that this assumes a limited syntax style in the outer SELECT clause of the query - only named variables, no aggregations, and a single space character separating all variable names.  E.g.:",
            "",
            "SELECT ?x ?y ?z",
            "WHERE",
            "{ ... }",
            "",
            "The word \"DISTINCT\" will also be cut from the query, if present.",
            "",
            "Should a more complex query be necessary, an outer, wrapping SELECT query would let this script continue to function.",
            "\"\"\"",
            "",
            "__version__ = \"0.4.3\"",
            "",
            "import argparse",
            "import binascii",
            "import logging",
            "import os",
            "import sys",
            "",
            "import pandas as pd  # type: ignore",
            "import rdflib.plugins.sparql",
            "",
            "import case_utils.ontology",
            "from case_utils.ontology.version_info import (",
            "    CURRENT_CASE_VERSION,",
            "    built_version_choices_list,",
            ")",
            "",
            "NS_XSD = rdflib.XSD",
            "",
            "_logger = logging.getLogger(os.path.basename(__file__))",
            "",
            "",
            "def main() -> None:",
            "    parser = argparse.ArgumentParser()",
            "",
            "    # Configure debug logging before running parse_args, because there could be an error raised before the construction of the argument parser.",
            "    logging.basicConfig(",
            "        level=logging.DEBUG",
            "        if (\"--debug\" in sys.argv or \"-d\" in sys.argv)",
            "        else logging.INFO",
            "    )",
            "",
            "    parser.add_argument(\"-d\", \"--debug\", action=\"store_true\")",
            "    parser.add_argument(",
            "        \"--built-version\",",
            "        choices=tuple(built_version_choices_list),",
            "        default=\"case-\" + CURRENT_CASE_VERSION,",
            "        help=\"Ontology version to use to supplement query, such as for subclass querying.  Does not require networking to use.  Default is most recent CASE release.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--disallow-empty-results\",",
            "        action=\"store_true\",",
            "        help=\"Raise error if no results are returned for query.\",",
            "    )",
            "    parser.add_argument(",
            "        \"out_table\",",
            "        help=\"Expected extensions are .html for HTML tables or .md for Markdown tables.\",",
            "    )",
            "    parser.add_argument(",
            "        \"in_sparql\",",
            "        help=\"File containing a SPARQL SELECT query.  Note that prefixes not mapped with a PREFIX statement will be mapped according to their first occurrence among input graphs.\",",
            "    )",
            "    parser.add_argument(\"in_graph\", nargs=\"+\")",
            "    args = parser.parse_args()",
            "",
            "    graph = rdflib.Graph()",
            "    for in_graph_filename in args.in_graph:",
            "        graph.parse(in_graph_filename)",
            "",
            "    # Inherit prefixes defined in input context dictionary.",
            "    nsdict = {k: v for (k, v) in graph.namespace_manager.namespaces()}",
            "",
            "    select_query_text = None",
            "    with open(args.in_sparql, \"r\") as in_fh:",
            "        select_query_text = in_fh.read().strip()",
            "    _logger.debug(\"select_query_text = %r.\" % select_query_text)",
            "",
            "    if \"subClassOf\" in select_query_text:",
            "        case_utils.ontology.load_subclass_hierarchy(",
            "            graph, built_version=args.built_version",
            "        )",
            "",
            "    # Build columns list from SELECT line.",
            "    select_query_text_lines = select_query_text.split(\"\\n\")",
            "    select_line = [",
            "        line for line in select_query_text_lines if line.startswith(\"SELECT \")",
            "    ][0]",
            "    variables = select_line.replace(\" DISTINCT\", \"\").replace(\"SELECT \", \"\").split(\" \")",
            "",
            "    tally = 0",
            "    records = []",
            "    select_query_object = rdflib.plugins.sparql.processor.prepareQuery(",
            "        select_query_text, initNs=nsdict",
            "    )",
            "    for (row_no, row) in enumerate(graph.query(select_query_object)):",
            "        tally = row_no + 1",
            "        record = []",
            "        for (column_no, column) in enumerate(row):",
            "            if column is None:",
            "                column_value = \"\"",
            "            elif (",
            "                isinstance(column, rdflib.term.Literal)",
            "                and column.datatype == NS_XSD.hexBinary",
            "            ):",
            "                # Use hexlify to convert xsd:hexBinary to ASCII.",
            "                # The render to ASCII is in support of this script rendering results for website viewing.",
            "                # .decode() is because hexlify returns bytes.",
            "                column_value = binascii.hexlify(column.toPython()).decode()",
            "            else:",
            "                column_value = column.toPython()",
            "            if row_no == 0:",
            "                _logger.debug(\"row[0]column[%d] = %r.\" % (column_no, column_value))",
            "            record.append(column_value)",
            "        records.append(record)",
            "    if tally == 0:",
            "        if args.disallow_empty_results:",
            "            raise ValueError(\"Failed to return any results.\")",
            "",
            "    df = pd.DataFrame(records, columns=variables)",
            "",
            "    table_text = None",
            "    if args.out_table.endswith(\".html\"):",
            "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_html.html",
            "        # Add CSS classes for CASE website Bootstrap support.",
            "        table_text = df.to_html(classes=(\"table\", \"table-bordered\", \"table-condensed\"))",
            "    elif args.out_table.endswith(\".md\"):",
            "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_markdown.html",
            "        # https://pypi.org/project/tabulate/",
            "        # Assume Github-flavored Markdown.",
            "        table_text = df.to_markdown(tablefmt=\"github\")",
            "    if table_text is None:",
            "        raise NotImplementedError(",
            "            \"Unsupported output extension for output filename %r.\", args.out_table",
            "        )",
            "",
            "    with open(args.out_table, \"w\") as out_fh:",
            "        out_fh.write(table_text)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python3",
            "",
            "# This software was developed at the National Institute of Standards",
            "# and Technology by employees of the Federal Government in the course",
            "# of their official duties. Pursuant to title 17 Section 105 of the",
            "# United States Code this software is not subject to copyright",
            "# protection and is in the public domain. NIST assumes no",
            "# responsibility whatsoever for its use by other parties, and makes",
            "# no guarantees, expressed or implied, about its quality,",
            "# reliability, or any other characteristic.",
            "#",
            "# We would appreciate acknowledgement if the software is used.",
            "",
            "\"\"\"",
            "This script executes a SPARQL SELECT query, returning a table representation.  The design of the workflow is based on this example built on SPARQLWrapper:",
            "https://lawlesst.github.io/notebook/sparql-dataframe.html",
            "",
            "Note that this assumes a limited syntax style in the outer SELECT clause of the query - only named variables, no aggregations, and a single space character separating all variable names.  E.g.:",
            "",
            "SELECT ?x ?y ?z",
            "WHERE",
            "{ ... }",
            "",
            "The word \"DISTINCT\" will also be cut from the query, if present.",
            "",
            "Should a more complex query be necessary, an outer, wrapping SELECT query would let this script continue to function.",
            "\"\"\"",
            "",
            "__version__ = \"0.4.3\"",
            "",
            "import argparse",
            "import binascii",
            "import logging",
            "import os",
            "import sys",
            "",
            "import pandas as pd  # type: ignore",
            "import rdflib.plugins.sparql",
            "",
            "import case_utils.ontology",
            "from case_utils.ontology.version_info import (",
            "    CURRENT_CASE_VERSION,",
            "    built_version_choices_list,",
            ")",
            "",
            "NS_XSD = rdflib.XSD",
            "",
            "_logger = logging.getLogger(os.path.basename(__file__))",
            "",
            "",
            "def main() -> None:",
            "    parser = argparse.ArgumentParser()",
            "",
            "    # Configure debug logging before running parse_args, because there could be an error raised before the construction of the argument parser.",
            "    logging.basicConfig(",
            "        level=logging.DEBUG",
            "        if (\"--debug\" in sys.argv or \"-d\" in sys.argv)",
            "        else logging.INFO",
            "    )",
            "",
            "    parser.add_argument(\"-d\", \"--debug\", action=\"store_true\")",
            "    parser.add_argument(",
            "        \"--built-version\",",
            "        choices=tuple(built_version_choices_list),",
            "        default=\"case-\" + CURRENT_CASE_VERSION,",
            "        help=\"Ontology version to use to supplement query, such as for subclass querying.  Does not require networking to use.  Default is most recent CASE release.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--disallow-empty-results\",",
            "        action=\"store_true\",",
            "        help=\"Raise error if no results are returned for query.\",",
            "    )",
            "    parser.add_argument(",
            "        \"out_table\",",
            "        help=\"Expected extensions are .html for HTML tables or .md for Markdown tables.\",",
            "    )",
            "    parser.add_argument(",
            "        \"in_sparql\",",
            "        help=\"File containing a SPARQL SELECT query.  Note that prefixes not mapped with a PREFIX statement will be mapped according to their first occurrence among input graphs.\",",
            "    )",
            "    parser.add_argument(\"in_graph\", nargs=\"+\")",
            "    args = parser.parse_args()",
            "",
            "    graph = rdflib.Graph()",
            "    for in_graph_filename in args.in_graph:",
            "        graph.parse(in_graph_filename)",
            "",
            "    # Inherit prefixes defined in input context dictionary.",
            "    nsdict = {k: v for (k, v) in graph.namespace_manager.namespaces()}",
            "",
            "    select_query_text = None",
            "    with open(args.in_sparql, \"r\") as in_fh:",
            "        select_query_text = in_fh.read().strip()",
            "    _logger.debug(\"select_query_text = %r.\" % select_query_text)",
            "",
            "    if \"subClassOf\" in select_query_text:",
            "        case_utils.ontology.load_subclass_hierarchy(",
            "            graph, built_version=args.built_version",
            "        )",
            "",
            "    # Build columns list from SELECT line.",
            "    select_query_text_lines = select_query_text.split(\"\\n\")",
            "    select_line = [",
            "        line for line in select_query_text_lines if line.startswith(\"SELECT \")",
            "    ][0]",
            "    variables = select_line.replace(\" DISTINCT\", \"\").replace(\"SELECT \", \"\").split(\" \")",
            "",
            "    tally = 0",
            "    records = []",
            "    select_query_object = rdflib.plugins.sparql.processor.prepareQuery(",
            "        select_query_text, initNs=nsdict",
            "    )",
            "    for row_no, row in enumerate(graph.query(select_query_object)):",
            "        tally = row_no + 1",
            "        record = []",
            "        for column_no, column in enumerate(row):",
            "            if column is None:",
            "                column_value = \"\"",
            "            elif (",
            "                isinstance(column, rdflib.term.Literal)",
            "                and column.datatype == NS_XSD.hexBinary",
            "            ):",
            "                # Use hexlify to convert xsd:hexBinary to ASCII.",
            "                # The render to ASCII is in support of this script rendering results for website viewing.",
            "                # .decode() is because hexlify returns bytes.",
            "                column_value = binascii.hexlify(column.toPython()).decode()",
            "            else:",
            "                column_value = column.toPython()",
            "            if row_no == 0:",
            "                _logger.debug(\"row[0]column[%d] = %r.\" % (column_no, column_value))",
            "            record.append(column_value)",
            "        records.append(record)",
            "    if tally == 0:",
            "        if args.disallow_empty_results:",
            "            raise ValueError(\"Failed to return any results.\")",
            "",
            "    df = pd.DataFrame(records, columns=variables)",
            "",
            "    table_text = None",
            "    if args.out_table.endswith(\".html\"):",
            "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_html.html",
            "        # Add CSS classes for CASE website Bootstrap support.",
            "        table_text = df.to_html(classes=(\"table\", \"table-bordered\", \"table-condensed\"))",
            "    elif args.out_table.endswith(\".md\"):",
            "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_markdown.html",
            "        # https://pypi.org/project/tabulate/",
            "        # Assume Github-flavored Markdown.",
            "        table_text = df.to_markdown(tablefmt=\"github\")",
            "    if table_text is None:",
            "        raise NotImplementedError(",
            "            \"Unsupported output extension for output filename %r.\", args.out_table",
            "        )",
            "",
            "    with open(args.out_table, \"w\") as out_fh:",
            "        out_fh.write(table_text)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "113": [
                "main"
            ],
            "116": [
                "main"
            ]
        },
        "addLocation": []
    },
    "case_utils/local_uuid.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " This library is a wrapper for uuid, provided to generate repeatable UUIDs if requested."
            },
            "1": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " \"\"\""
            },
            "2": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-__version__ = \"0.3.0\""
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 18,
                "PatchRowcode": "+__version__ = \"0.3.1\""
            },
            "5": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " import logging"
            },
            "7": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " import os"
            },
            "8": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " _logger = logging.getLogger(pathlib.Path(__file__).name)"
            },
            "9": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+def _is_relative_to(p1: pathlib.Path, p2: pathlib.Path) -> bool:"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+    \"\"\""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+    This function provides pathlib.is_relative_to to Pythons before 3.9.  After the End of Life of Python 3.8, this function can be removed."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+    \"\"\""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+    if sys.version_info < (3, 9):"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+        try:"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+            _ = p1.relative_to(p2)"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+            return True"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+        except ValueError:"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+            return False"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    else:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        return p1.is_relative_to(p2)"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 49,
                "PatchRowcode": " def configure() -> None:"
            },
            "26": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "     global DEMO_UUID_BASE"
            },
            "27": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 51,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+    # _logger.debug(\"sys.argv = %r.\", sys.argv)"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "     if os.getenv(\"DEMO_UUID_REQUESTING_NONRANDOM\") == \"NONRANDOM_REQUESTED\":"
            },
            "31": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "         warnings.warn("
            },
            "32": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "             \"Environment variable DEMO_UUID_REQUESTING_NONRANDOM is deprecated.  See case_utils.local_uuid.demo_uuid for usage notes on its replacement, CASE_DEMO_NONRANDOM_UUID_BASE.  Proceeding with random UUIDs.\","
            },
            "33": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "         demo_uuid_base_parts.append(sys.argv[0])"
            },
            "34": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "     else:"
            },
            "35": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "         command_original_path = pathlib.Path(sys.argv[0])"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+        # _logger.debug(\"command_original_path = %r.\", command_original_path)"
            },
            "37": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "         command_resolved_path = command_original_path.resolve()"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+        # _logger.debug(\"command_resolved_path = %r.\", command_resolved_path)"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+        # The command could be a command embedded in a virtual"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+        # environment, or it could be a script external to any virtual"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+        # environment."
            },
            "43": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "         venv_original_path = pathlib.Path(env_venv_name)"
            },
            "44": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 109,
                "PatchRowcode": "         venv_resolved_path = venv_original_path.resolve()"
            },
            "45": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        try:"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+        if _is_relative_to(command_resolved_path, venv_resolved_path):"
            },
            "47": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "             command_relative_path = command_resolved_path.relative_to("
            },
            "48": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "                 venv_resolved_path"
            },
            "49": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "             )"
            },
            "50": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "             # _logger.debug(\"command_relative_path = %r.\", command_relative_path)"
            },
            "51": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "             demo_uuid_base_parts.append(str(command_relative_path))"
            },
            "52": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        except ValueError:"
            },
            "53": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # _logger.debug(\"Command path is not relative to virtual environment path.\")"
            },
            "54": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            demo_uuid_base_parts.append(str(command_resolved_path))"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        else:"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+            demo_uuid_base_parts.append(str(command_original_path))"
            },
            "57": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 118,
                "PatchRowcode": " "
            },
            "58": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "     if len(sys.argv) > 1:"
            },
            "59": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "         # Component: Arguments of argument vector."
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python3",
            "",
            "# This software was developed at the National Institute of Standards",
            "# and Technology by employees of the Federal Government in the course",
            "# of their official duties. Pursuant to title 17 Section 105 of the",
            "# United States Code this software is not subject to copyright",
            "# protection and is in the public domain. NIST assumes no",
            "# responsibility whatsoever for its use by other parties, and makes",
            "# no guarantees, expressed or implied, about its quality,",
            "# reliability, or any other characteristic.",
            "#",
            "# We would appreciate acknowledgement if the software is used.",
            "",
            "\"\"\"",
            "This library is a wrapper for uuid, provided to generate repeatable UUIDs if requested.",
            "\"\"\"",
            "",
            "__version__ = \"0.3.0\"",
            "",
            "import logging",
            "import os",
            "import pathlib",
            "import sys",
            "import typing",
            "import uuid",
            "import warnings",
            "",
            "DEMO_UUID_BASE: typing.Optional[str] = None",
            "",
            "DEMO_UUID_COUNTER: int = 0",
            "",
            "_logger = logging.getLogger(pathlib.Path(__file__).name)",
            "",
            "",
            "def configure() -> None:",
            "    global DEMO_UUID_BASE",
            "",
            "    if os.getenv(\"DEMO_UUID_REQUESTING_NONRANDOM\") == \"NONRANDOM_REQUESTED\":",
            "        warnings.warn(",
            "            \"Environment variable DEMO_UUID_REQUESTING_NONRANDOM is deprecated.  See case_utils.local_uuid.demo_uuid for usage notes on its replacement, CASE_DEMO_NONRANDOM_UUID_BASE.  Proceeding with random UUIDs.\",",
            "            DeprecationWarning,",
            "        )",
            "        return",
            "",
            "    env_base_dir_name = os.getenv(\"CASE_DEMO_NONRANDOM_UUID_BASE\")",
            "    if env_base_dir_name is None:",
            "        return",
            "",
            "    base_dir_original_path = pathlib.Path(env_base_dir_name)",
            "    if not base_dir_original_path.exists():",
            "        warnings.warn(",
            "            \"Environment variable CASE_DEMO_NONRANDOM_UUID_BASE is expected to refer to an existing directory.  Proceeding with random UUIDs.\"",
            "        )",
            "        return",
            "    if not base_dir_original_path.is_dir():",
            "        warnings.warn(",
            "            \"Environment variable CASE_DEMO_NONRANDOM_UUID_BASE is expected to refer to a directory.  Proceeding with random UUIDs.\"",
            "        )",
            "        return",
            "",
            "    # Component: An emphasis this is an example.",
            "    demo_uuid_base_parts = [\"example.org\"]",
            "",
            "    # Component: Present working directory, relative to CASE_DEMO_NONRANDOM_UUID_BASE if that environment variable is an ancestor of pwd.",
            "    base_dir_resolved_path = base_dir_original_path.resolve()",
            "    srcdir_original_path = pathlib.Path(os.getcwd())",
            "    srcdir_resolved_path = srcdir_original_path.resolve()",
            "    # _logger.debug(\"base_dir_resolved_path = %r.\", base_dir_resolved_path)",
            "    # _logger.debug(\"srcdir_resolved_path = %r.\", srcdir_resolved_path)",
            "    try:",
            "        srcdir_relative_path = srcdir_resolved_path.relative_to(base_dir_resolved_path)",
            "        # _logger.debug(\"srcdir_relative_path = %r.\", srcdir_relative_path)",
            "        demo_uuid_base_parts.append(str(srcdir_relative_path))",
            "    except ValueError:",
            "        # If base_dir is not an ancestor directory of srcdir, default to srcdir.",
            "        # _logger.debug(\"PWD is not relative to base path.\")",
            "        demo_uuid_base_parts.append(str(srcdir_resolved_path))",
            "",
            "    # Component: Command of argument vector.",
            "    env_venv_name = os.getenv(\"VIRTUAL_ENV\")",
            "    if env_venv_name is None:",
            "        demo_uuid_base_parts.append(sys.argv[0])",
            "    else:",
            "        command_original_path = pathlib.Path(sys.argv[0])",
            "        command_resolved_path = command_original_path.resolve()",
            "        venv_original_path = pathlib.Path(env_venv_name)",
            "        venv_resolved_path = venv_original_path.resolve()",
            "        try:",
            "            command_relative_path = command_resolved_path.relative_to(",
            "                venv_resolved_path",
            "            )",
            "            # _logger.debug(\"command_relative_path = %r.\", command_relative_path)",
            "            demo_uuid_base_parts.append(str(command_relative_path))",
            "        except ValueError:",
            "            # _logger.debug(\"Command path is not relative to virtual environment path.\")",
            "            demo_uuid_base_parts.append(str(command_resolved_path))",
            "",
            "    if len(sys.argv) > 1:",
            "        # Component: Arguments of argument vector.",
            "        demo_uuid_base_parts.extend(sys.argv[1:])",
            "",
            "    # _logger.debug(\"demo_uuid_base_parts = %r.\", demo_uuid_base_parts)",
            "",
            "    DEMO_UUID_BASE = \"/\".join(demo_uuid_base_parts)",
            "",
            "",
            "def demo_uuid() -> str:",
            "    \"\"\"",
            "    This function generates a repeatable UUID, drawing on non-varying elements of the environment and process call for entropy.",
            "",
            "    WARNING: This function was developed for use ONLY for reducing (but not eliminating) version-control edits to identifiers in sample data.  It creates UUIDs that are decidedly NOT random, and should remain consistent on repeated calls to the importing script.",
            "",
            "    To prevent accidental non-random UUID usage, an environment variable must be set to a string provided by the caller.  The variable's required value is the path to some directory.  The variable's recommended value is the equivalent of the Make variable \"top_srcdir\" - that is, the root directory of the containing Git repository, some parent of the current process's current working directory.",
            "    \"\"\"",
            "    global DEMO_UUID_BASE",
            "    global DEMO_UUID_COUNTER",
            "",
            "    if os.getenv(\"CASE_DEMO_NONRANDOM_UUID_BASE\") is None:",
            "        raise ValueError(",
            "            \"demo_uuid() called without CASE_DEMO_NONRANDOM_UUID_BASE in environment.\"",
            "        )",
            "",
            "    if DEMO_UUID_BASE is None:",
            "        raise ValueError(\"demo_uuid() called with DEMO_UUID_BASE unset.\")",
            "",
            "    parts = [DEMO_UUID_BASE]",
            "",
            "    # Component: Incrementing counter.",
            "    DEMO_UUID_COUNTER += 1",
            "    parts.append(str(DEMO_UUID_COUNTER))",
            "",
            "    return str(uuid.uuid5(uuid.NAMESPACE_URL, \"/\".join(parts)))",
            "",
            "",
            "def local_uuid() -> str:",
            "    \"\"\"",
            "    Generate either a UUID4, or if requested via environment configuration, a non-random demo UUID.",
            "    \"\"\"",
            "    global DEMO_UUID_BASE",
            "    if DEMO_UUID_BASE is None:",
            "        return str(uuid.uuid4())",
            "    else:",
            "        return demo_uuid()"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python3",
            "",
            "# This software was developed at the National Institute of Standards",
            "# and Technology by employees of the Federal Government in the course",
            "# of their official duties. Pursuant to title 17 Section 105 of the",
            "# United States Code this software is not subject to copyright",
            "# protection and is in the public domain. NIST assumes no",
            "# responsibility whatsoever for its use by other parties, and makes",
            "# no guarantees, expressed or implied, about its quality,",
            "# reliability, or any other characteristic.",
            "#",
            "# We would appreciate acknowledgement if the software is used.",
            "",
            "\"\"\"",
            "This library is a wrapper for uuid, provided to generate repeatable UUIDs if requested.",
            "\"\"\"",
            "",
            "__version__ = \"0.3.1\"",
            "",
            "import logging",
            "import os",
            "import pathlib",
            "import sys",
            "import typing",
            "import uuid",
            "import warnings",
            "",
            "DEMO_UUID_BASE: typing.Optional[str] = None",
            "",
            "DEMO_UUID_COUNTER: int = 0",
            "",
            "_logger = logging.getLogger(pathlib.Path(__file__).name)",
            "",
            "",
            "def _is_relative_to(p1: pathlib.Path, p2: pathlib.Path) -> bool:",
            "    \"\"\"",
            "    This function provides pathlib.is_relative_to to Pythons before 3.9.  After the End of Life of Python 3.8, this function can be removed.",
            "    \"\"\"",
            "    if sys.version_info < (3, 9):",
            "        try:",
            "            _ = p1.relative_to(p2)",
            "            return True",
            "        except ValueError:",
            "            return False",
            "    else:",
            "        return p1.is_relative_to(p2)",
            "",
            "",
            "def configure() -> None:",
            "    global DEMO_UUID_BASE",
            "",
            "    # _logger.debug(\"sys.argv = %r.\", sys.argv)",
            "",
            "    if os.getenv(\"DEMO_UUID_REQUESTING_NONRANDOM\") == \"NONRANDOM_REQUESTED\":",
            "        warnings.warn(",
            "            \"Environment variable DEMO_UUID_REQUESTING_NONRANDOM is deprecated.  See case_utils.local_uuid.demo_uuid for usage notes on its replacement, CASE_DEMO_NONRANDOM_UUID_BASE.  Proceeding with random UUIDs.\",",
            "            DeprecationWarning,",
            "        )",
            "        return",
            "",
            "    env_base_dir_name = os.getenv(\"CASE_DEMO_NONRANDOM_UUID_BASE\")",
            "    if env_base_dir_name is None:",
            "        return",
            "",
            "    base_dir_original_path = pathlib.Path(env_base_dir_name)",
            "    if not base_dir_original_path.exists():",
            "        warnings.warn(",
            "            \"Environment variable CASE_DEMO_NONRANDOM_UUID_BASE is expected to refer to an existing directory.  Proceeding with random UUIDs.\"",
            "        )",
            "        return",
            "    if not base_dir_original_path.is_dir():",
            "        warnings.warn(",
            "            \"Environment variable CASE_DEMO_NONRANDOM_UUID_BASE is expected to refer to a directory.  Proceeding with random UUIDs.\"",
            "        )",
            "        return",
            "",
            "    # Component: An emphasis this is an example.",
            "    demo_uuid_base_parts = [\"example.org\"]",
            "",
            "    # Component: Present working directory, relative to CASE_DEMO_NONRANDOM_UUID_BASE if that environment variable is an ancestor of pwd.",
            "    base_dir_resolved_path = base_dir_original_path.resolve()",
            "    srcdir_original_path = pathlib.Path(os.getcwd())",
            "    srcdir_resolved_path = srcdir_original_path.resolve()",
            "    # _logger.debug(\"base_dir_resolved_path = %r.\", base_dir_resolved_path)",
            "    # _logger.debug(\"srcdir_resolved_path = %r.\", srcdir_resolved_path)",
            "    try:",
            "        srcdir_relative_path = srcdir_resolved_path.relative_to(base_dir_resolved_path)",
            "        # _logger.debug(\"srcdir_relative_path = %r.\", srcdir_relative_path)",
            "        demo_uuid_base_parts.append(str(srcdir_relative_path))",
            "    except ValueError:",
            "        # If base_dir is not an ancestor directory of srcdir, default to srcdir.",
            "        # _logger.debug(\"PWD is not relative to base path.\")",
            "        demo_uuid_base_parts.append(str(srcdir_resolved_path))",
            "",
            "    # Component: Command of argument vector.",
            "    env_venv_name = os.getenv(\"VIRTUAL_ENV\")",
            "    if env_venv_name is None:",
            "        demo_uuid_base_parts.append(sys.argv[0])",
            "    else:",
            "        command_original_path = pathlib.Path(sys.argv[0])",
            "        # _logger.debug(\"command_original_path = %r.\", command_original_path)",
            "        command_resolved_path = command_original_path.resolve()",
            "        # _logger.debug(\"command_resolved_path = %r.\", command_resolved_path)",
            "",
            "        # The command could be a command embedded in a virtual",
            "        # environment, or it could be a script external to any virtual",
            "        # environment.",
            "        venv_original_path = pathlib.Path(env_venv_name)",
            "        venv_resolved_path = venv_original_path.resolve()",
            "        if _is_relative_to(command_resolved_path, venv_resolved_path):",
            "            command_relative_path = command_resolved_path.relative_to(",
            "                venv_resolved_path",
            "            )",
            "            # _logger.debug(\"command_relative_path = %r.\", command_relative_path)",
            "            demo_uuid_base_parts.append(str(command_relative_path))",
            "        else:",
            "            demo_uuid_base_parts.append(str(command_original_path))",
            "",
            "    if len(sys.argv) > 1:",
            "        # Component: Arguments of argument vector.",
            "        demo_uuid_base_parts.extend(sys.argv[1:])",
            "",
            "    # _logger.debug(\"demo_uuid_base_parts = %r.\", demo_uuid_base_parts)",
            "",
            "    DEMO_UUID_BASE = \"/\".join(demo_uuid_base_parts)",
            "",
            "",
            "def demo_uuid() -> str:",
            "    \"\"\"",
            "    This function generates a repeatable UUID, drawing on non-varying elements of the environment and process call for entropy.",
            "",
            "    WARNING: This function was developed for use ONLY for reducing (but not eliminating) version-control edits to identifiers in sample data.  It creates UUIDs that are decidedly NOT random, and should remain consistent on repeated calls to the importing script.",
            "",
            "    To prevent accidental non-random UUID usage, an environment variable must be set to a string provided by the caller.  The variable's required value is the path to some directory.  The variable's recommended value is the equivalent of the Make variable \"top_srcdir\" - that is, the root directory of the containing Git repository, some parent of the current process's current working directory.",
            "    \"\"\"",
            "    global DEMO_UUID_BASE",
            "    global DEMO_UUID_COUNTER",
            "",
            "    if os.getenv(\"CASE_DEMO_NONRANDOM_UUID_BASE\") is None:",
            "        raise ValueError(",
            "            \"demo_uuid() called without CASE_DEMO_NONRANDOM_UUID_BASE in environment.\"",
            "        )",
            "",
            "    if DEMO_UUID_BASE is None:",
            "        raise ValueError(\"demo_uuid() called with DEMO_UUID_BASE unset.\")",
            "",
            "    parts = [DEMO_UUID_BASE]",
            "",
            "    # Component: Incrementing counter.",
            "    DEMO_UUID_COUNTER += 1",
            "    parts.append(str(DEMO_UUID_COUNTER))",
            "",
            "    return str(uuid.uuid5(uuid.NAMESPACE_URL, \"/\".join(parts)))",
            "",
            "",
            "def local_uuid() -> str:",
            "    \"\"\"",
            "    Generate either a UUID4, or if requested via environment configuration, a non-random demo UUID.",
            "    \"\"\"",
            "    global DEMO_UUID_BASE",
            "    if DEMO_UUID_BASE is None:",
            "        return str(uuid.uuid4())",
            "    else:",
            "        return demo_uuid()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "18": [
                "__version__"
            ],
            "88": [
                "configure"
            ],
            "94": [
                "configure"
            ],
            "95": [
                "configure"
            ],
            "96": [
                "configure"
            ]
        },
        "addLocation": []
    }
}