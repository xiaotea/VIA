{
    "src/sentry/api/endpoints/artifact_lookup.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": "     ArtifactBundle,"
            },
            "1": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": "     DebugIdArtifactBundle,"
            },
            "2": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": "     Distribution,"
            },
            "3": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    File,"
            },
            "4": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 23,
                "PatchRowcode": "     Project,"
            },
            "5": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 24,
                "PatchRowcode": "     ProjectArtifactBundle,"
            },
            "6": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 25,
                "PatchRowcode": "     Release,"
            },
            "7": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " class ProjectArtifactLookupEndpoint(ProjectEndpoint):"
            },
            "8": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 45,
                "PatchRowcode": "     permission_classes = (ProjectReleasePermission,)"
            },
            "9": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 46,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def download_file(self, file_id, project: Project):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+    def download_file(self, download_id, project: Project):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+        ty, ty_id = download_id.split(\"/\")"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "         rate_limited = ratelimits.is_limited("
            },
            "15": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "             project=project,"
            },
            "16": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            key=f\"rl:ArtifactLookupEndpoint:download:{file_id}:{project.id}\","
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+            key=f\"rl:ArtifactLookupEndpoint:download:{download_id}:{project.id}\","
            },
            "18": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "             limit=10,"
            },
            "19": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "         )"
            },
            "20": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "         if rate_limited:"
            },
            "21": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "             logger.info("
            },
            "22": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "                 \"notification.rate_limited\","
            },
            "23": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                extra={\"project_id\": project.id, \"file_id\": file_id},"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+                extra={\"project_id\": project.id, \"file_id\": download_id},"
            },
            "25": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "             )"
            },
            "26": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "             return HttpResponse({\"Too many download requests\"}, status=429)"
            },
            "27": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        file = File.objects.filter(id=file_id).first()"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+        file = None"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+        if ty == \"artifact_bundle\":"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+            file = ("
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+                ArtifactBundle.objects.filter("
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+                    id=ty_id,"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+                    projectartifactbundle__project_id=project.id,"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+                )"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+                .select_related(\"file\")"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+                .first()"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+            )"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+        elif ty == \"release_file\":"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+            # NOTE: `ReleaseFile` does have a `project_id`, but that seems to"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+            # be always empty, so using the `organization_id` instead."
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+            file = ("
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+                ReleaseFile.objects.filter(id=ty_id, organization_id=project.organization.id)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+                .select_related(\"file\")"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+                .first()"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+            )"
            },
            "47": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " "
            },
            "48": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "         if file is None:"
            },
            "49": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 82,
                "PatchRowcode": "             raise Http404"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+        file = file.file"
            },
            "51": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " "
            },
            "52": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "         try:"
            },
            "53": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "             fp = file.getfile()"
            },
            "54": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 112,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "         :auth: required"
            },
            "56": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "         \"\"\""
            },
            "57": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if request.GET.get(\"download\") is not None:"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+        if (download_id := request.GET.get(\"download\")) is not None:"
            },
            "59": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "             if has_download_permission(request, project):"
            },
            "60": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                return self.download_file(request.GET.get(\"download\"), project)"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+                return self.download_file(download_id, project)"
            },
            "62": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "             else:"
            },
            "63": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "                 return Response(status=403)"
            },
            "64": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 120,
                "PatchRowcode": " "
            },
            "65": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "         def update_bundles(inner_bundles: Set[Tuple[int, datetime, int]]):"
            },
            "66": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "             for (bundle_id, date_added, file_id) in inner_bundles:"
            },
            "67": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "                 used_artifact_bundles[bundle_id] = date_added"
            },
            "68": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                bundle_file_ids.add(file_id)"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+                bundle_file_ids.add((\"artifact_bundle\", bundle_id, file_id))"
            },
            "70": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 137,
                "PatchRowcode": " "
            },
            "71": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "         if debug_id:"
            },
            "72": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 139,
                "PatchRowcode": "             bundles = get_artifact_bundles_containing_debug_id(debug_id, project)"
            },
            "73": {
                "beforePatchRowNumber": 132,
                "afterPatchRowNumber": 151,
                "PatchRowcode": " "
            },
            "74": {
                "beforePatchRowNumber": 133,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "             release, dist = try_resolve_release_dist(project, release_name, dist_name)"
            },
            "75": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "             if release:"
            },
            "76": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                bundle_file_ids |= get_legacy_release_bundles(release, dist)"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+                for (releasefile_id, file_id) in get_legacy_release_bundles(release, dist):"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 155,
                "PatchRowcode": "+                    bundle_file_ids.add((\"release_file\", releasefile_id, file_id))"
            },
            "79": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "                 individual_files = get_legacy_releasefile_by_file_url(release, dist, url)"
            },
            "80": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": 157,
                "PatchRowcode": " "
            },
            "81": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 158,
                "PatchRowcode": "         if options.get(\"sourcemaps.artifact-bundles.enable-renewal\") == 1.0:"
            },
            "82": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 164,
                "PatchRowcode": "         url_constructor = UrlConstructor(request, project)"
            },
            "83": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": 165,
                "PatchRowcode": " "
            },
            "84": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 166,
                "PatchRowcode": "         found_artifacts = []"
            },
            "85": {
                "beforePatchRowNumber": 147,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        for file_id in bundle_file_ids:"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+        # NOTE: the reason we use the `file_id` as the `id` we return is because"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+        # downstream symbolicator relies on that for its internal caching."
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+        # We do not want to hard-refresh those caches quite yet, and the `id`"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+        # should also be as unique as possible, which the `file_id` is."
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+        for (ty, ty_id, file_id) in bundle_file_ids:"
            },
            "91": {
                "beforePatchRowNumber": 148,
                "afterPatchRowNumber": 172,
                "PatchRowcode": "             found_artifacts.append("
            },
            "92": {
                "beforePatchRowNumber": 149,
                "afterPatchRowNumber": 173,
                "PatchRowcode": "                 {"
            },
            "93": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 174,
                "PatchRowcode": "                     \"id\": str(file_id),"
            },
            "94": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": 175,
                "PatchRowcode": "                     \"type\": \"bundle\","
            },
            "95": {
                "beforePatchRowNumber": 152,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    \"url\": url_constructor.url_for_file_id(file_id),"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 176,
                "PatchRowcode": "+                    \"url\": url_constructor.url_for_file_id(ty, ty_id),"
            },
            "97": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": 177,
                "PatchRowcode": "                 }"
            },
            "98": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 178,
                "PatchRowcode": "             )"
            },
            "99": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 179,
                "PatchRowcode": " "
            },
            "100": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "                 {"
            },
            "101": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 183,
                "PatchRowcode": "                     \"id\": str(release_file.file.id),"
            },
            "102": {
                "beforePatchRowNumber": 160,
                "afterPatchRowNumber": 184,
                "PatchRowcode": "                     \"type\": \"file\","
            },
            "103": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    \"url\": url_constructor.url_for_file_id(release_file.file.id),"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+                    \"url\": url_constructor.url_for_file_id(\"release_file\", release_file.id),"
            },
            "105": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "                     # The `name` is the url/abs_path of the file,"
            },
            "106": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "                     # as in: `\"~/path/to/file.min.js\"`."
            },
            "107": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "                     \"abs_path\": release_file.name,"
            },
            "108": {
                "beforePatchRowNumber": 167,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "                 }"
            },
            "109": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": 192,
                "PatchRowcode": "             )"
            },
            "110": {
                "beforePatchRowNumber": 169,
                "afterPatchRowNumber": 193,
                "PatchRowcode": " "
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+        # make sure we have a stable sort order for tests"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+        found_artifacts.sort(key=lambda x: int(x[\"id\"]))"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+"
            },
            "114": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 197,
                "PatchRowcode": "         # NOTE: We do not paginate this response, as we have very tight limits on all the individual queries."
            },
            "115": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": 198,
                "PatchRowcode": "         return Response(serialize(found_artifacts, request.user))"
            },
            "116": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 199,
                "PatchRowcode": " "
            },
            "117": {
                "beforePatchRowNumber": 259,
                "afterPatchRowNumber": 286,
                "PatchRowcode": "     return release, dist"
            },
            "118": {
                "beforePatchRowNumber": 260,
                "afterPatchRowNumber": 287,
                "PatchRowcode": " "
            },
            "119": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 288,
                "PatchRowcode": " "
            },
            "120": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def get_legacy_release_bundles(release: Release, dist: Optional[Distribution]):"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 289,
                "PatchRowcode": "+def get_legacy_release_bundles("
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 290,
                "PatchRowcode": "+    release: Release, dist: Optional[Distribution]"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 291,
                "PatchRowcode": "+) -> Set[Tuple[int, int]]:"
            },
            "124": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 292,
                "PatchRowcode": "     return set("
            },
            "125": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ReleaseFile.objects.select_related(\"file\")"
            },
            "126": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        .filter("
            },
            "127": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 293,
                "PatchRowcode": "+        ReleaseFile.objects.filter("
            },
            "128": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 294,
                "PatchRowcode": "             release_id=release.id,"
            },
            "129": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 295,
                "PatchRowcode": "             dist_id=dist.id if dist else None,"
            },
            "130": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 296,
                "PatchRowcode": "             # a `ReleaseFile` with `0` artifacts represents a release archive,"
            },
            "131": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 299,
                "PatchRowcode": "             # similarly the special `type` is also used for release archives."
            },
            "132": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 300,
                "PatchRowcode": "             file__type=RELEASE_BUNDLE_TYPE,"
            },
            "133": {
                "beforePatchRowNumber": 273,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "         )"
            },
            "134": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        .values_list(\"file_id\", flat=True)"
            },
            "135": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+        .select_related(\"file\")"
            },
            "136": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+        .values_list(\"id\", \"file_id\")"
            },
            "137": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 304,
                "PatchRowcode": "         # TODO: this `order_by` might be incredibly slow"
            },
            "138": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 305,
                "PatchRowcode": "         # we want to have a hard limit on the returned bundles here. and we would"
            },
            "139": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": 306,
                "PatchRowcode": "         # want to pick the most recently uploaded ones. that should mostly be"
            },
            "140": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 333,
                "PatchRowcode": "         else:"
            },
            "141": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": 334,
                "PatchRowcode": "             self.base_url = request.build_absolute_uri(request.path)"
            },
            "142": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": 335,
                "PatchRowcode": " "
            },
            "143": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def url_for_file_id(self, file_id: int) -> str:"
            },
            "144": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 336,
                "PatchRowcode": "+    def url_for_file_id(self, ty: str, file_id: int) -> str:"
            },
            "145": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": 337,
                "PatchRowcode": "         # NOTE: Returning a self-route that requires authentication (via Bearer token)"
            },
            "146": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": 338,
                "PatchRowcode": "         # is not really forward compatible with a pre-signed URL that does not"
            },
            "147": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "         # require any authentication or headers whatsoever."
            },
            "148": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": 340,
                "PatchRowcode": "         # This also requires a workaround in Symbolicator, as its generic http"
            },
            "149": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 341,
                "PatchRowcode": "         # downloader blocks \"internal\" IPs, whereas the internal Sentry downloader"
            },
            "150": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 342,
                "PatchRowcode": "         # is explicitly exempt."
            },
            "151": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return f\"{self.base_url}?download={file_id}\""
            },
            "152": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+        return f\"{self.base_url}?download={ty}/{file_id}\""
            }
        },
        "frontPatchFile": [
            "import logging",
            "from datetime import datetime, timedelta",
            "from typing import List, Mapping, Optional, Sequence, Set, Tuple",
            "",
            "import pytz",
            "from django.db import transaction",
            "from django.http import Http404, HttpResponse, StreamingHttpResponse",
            "from rest_framework.request import Request",
            "from rest_framework.response import Response",
            "from symbolic import SymbolicError, normalize_debug_id",
            "",
            "from sentry import options, ratelimits",
            "from sentry.api.base import region_silo_endpoint",
            "from sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission",
            "from sentry.api.endpoints.debug_files import has_download_permission",
            "from sentry.api.serializers import serialize",
            "from sentry.auth.system import is_system_auth",
            "from sentry.lang.native.sources import get_internal_artifact_lookup_source_url",
            "from sentry.models import (",
            "    ArtifactBundle,",
            "    DebugIdArtifactBundle,",
            "    Distribution,",
            "    File,",
            "    Project,",
            "    ProjectArtifactBundle,",
            "    Release,",
            "    ReleaseArtifactBundle,",
            "    ReleaseFile,",
            ")",
            "from sentry.utils import metrics",
            "",
            "logger = logging.getLogger(\"sentry.api\")",
            "",
            "# The marker for \"release\" bundles",
            "RELEASE_BUNDLE_TYPE = \"release.bundle\"",
            "# The number of bundles (\"artifact\" or \"release\") that we query",
            "MAX_BUNDLES_QUERY = 5",
            "# The number of files returned by the `get_releasefiles` query",
            "MAX_RELEASEFILES_QUERY = 10",
            "# Number of days that determine whether an artifact bundle is ready for being renewed.",
            "AVAILABLE_FOR_RENEWAL_DAYS = 30",
            "",
            "",
            "@region_silo_endpoint",
            "class ProjectArtifactLookupEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def download_file(self, file_id, project: Project):",
            "        rate_limited = ratelimits.is_limited(",
            "            project=project,",
            "            key=f\"rl:ArtifactLookupEndpoint:download:{file_id}:{project.id}\",",
            "            limit=10,",
            "        )",
            "        if rate_limited:",
            "            logger.info(",
            "                \"notification.rate_limited\",",
            "                extra={\"project_id\": project.id, \"file_id\": file_id},",
            "            )",
            "            return HttpResponse({\"Too many download requests\"}, status=429)",
            "",
            "        file = File.objects.filter(id=file_id).first()",
            "",
            "        if file is None:",
            "            raise Http404",
            "",
            "        try:",
            "            fp = file.getfile()",
            "            response = StreamingHttpResponse(",
            "                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"",
            "            )",
            "            response[\"Content-Length\"] = file.size",
            "            response[\"Content-Disposition\"] = f'attachment; filename=\"{file.name}\"'",
            "            return response",
            "        except OSError:",
            "            raise Http404",
            "",
            "    def get(self, request: Request, project: Project) -> Response:",
            "        \"\"\"",
            "        List a Project's Individual Artifacts or Bundles",
            "        ````````````````````````````````````````",
            "",
            "        Retrieve a list of individual artifacts or artifact bundles for a given project.",
            "",
            "        :pparam string organization_slug: the slug of the organization to query.",
            "        :pparam string project_slug: the slug of the project to query.",
            "        :qparam string debug_id: if set, will query and return the artifact",
            "                                 bundle that matches the given `debug_id`.",
            "        :qparam string url: if set, will query and return all the individual",
            "                            artifacts, or artifact bundles that contain files",
            "                            that match the `url`. This is using a substring-match.",
            "        :qparam string release: used in conjunction with `url`.",
            "        :qparam string dist: used in conjunction with `url`.",
            "",
            "        :auth: required",
            "        \"\"\"",
            "        if request.GET.get(\"download\") is not None:",
            "            if has_download_permission(request, project):",
            "                return self.download_file(request.GET.get(\"download\"), project)",
            "            else:",
            "                return Response(status=403)",
            "",
            "        debug_id = request.GET.get(\"debug_id\")",
            "        try:",
            "            debug_id = normalize_debug_id(debug_id)",
            "        except SymbolicError:",
            "            pass",
            "        url = request.GET.get(\"url\")",
            "        release_name = request.GET.get(\"release\")",
            "        dist_name = request.GET.get(\"dist\")",
            "",
            "        used_artifact_bundles = dict()",
            "        bundle_file_ids = set()",
            "",
            "        def update_bundles(inner_bundles: Set[Tuple[int, datetime, int]]):",
            "            for (bundle_id, date_added, file_id) in inner_bundles:",
            "                used_artifact_bundles[bundle_id] = date_added",
            "                bundle_file_ids.add(file_id)",
            "",
            "        if debug_id:",
            "            bundles = get_artifact_bundles_containing_debug_id(debug_id, project)",
            "            update_bundles(bundles)",
            "",
            "        individual_files = set()",
            "        if url and release_name and not bundle_file_ids:",
            "            # Get both the newest X release artifact bundles,",
            "            # and also query the legacy artifact bundles. One of those should have the",
            "            # file we are looking for. We want to return more here, even bundles that",
            "            # do *not* contain the file, rather than opening up each bundle. We want to",
            "            # avoid opening up bundles at all cost.",
            "            bundles = get_release_artifacts(project, release_name, dist_name)",
            "            update_bundles(bundles)",
            "",
            "            release, dist = try_resolve_release_dist(project, release_name, dist_name)",
            "            if release:",
            "                bundle_file_ids |= get_legacy_release_bundles(release, dist)",
            "                individual_files = get_legacy_releasefile_by_file_url(release, dist, url)",
            "",
            "        if options.get(\"sourcemaps.artifact-bundles.enable-renewal\") == 1.0:",
            "            with metrics.timer(\"artifact_lookup.get.renew_artifact_bundles\"):",
            "                # Before constructing the response, we want to update the artifact bundles renewal date.",
            "                renew_artifact_bundles(used_artifact_bundles)",
            "",
            "        # Then: Construct our response",
            "        url_constructor = UrlConstructor(request, project)",
            "",
            "        found_artifacts = []",
            "        for file_id in bundle_file_ids:",
            "            found_artifacts.append(",
            "                {",
            "                    \"id\": str(file_id),",
            "                    \"type\": \"bundle\",",
            "                    \"url\": url_constructor.url_for_file_id(file_id),",
            "                }",
            "            )",
            "",
            "        for release_file in individual_files:",
            "            found_artifacts.append(",
            "                {",
            "                    \"id\": str(release_file.file.id),",
            "                    \"type\": \"file\",",
            "                    \"url\": url_constructor.url_for_file_id(release_file.file.id),",
            "                    # The `name` is the url/abs_path of the file,",
            "                    # as in: `\"~/path/to/file.min.js\"`.",
            "                    \"abs_path\": release_file.name,",
            "                    # These headers should ideally include the `Sourcemap` reference",
            "                    \"headers\": release_file.file.headers,",
            "                }",
            "            )",
            "",
            "        # NOTE: We do not paginate this response, as we have very tight limits on all the individual queries.",
            "        return Response(serialize(found_artifacts, request.user))",
            "",
            "",
            "def renew_artifact_bundles(used_artifact_bundles: Mapping[int, datetime]):",
            "    # We take a snapshot in time that MUST be consistent across all updates.",
            "    now = datetime.now(tz=pytz.UTC)",
            "    # We compute the threshold used to determine whether we want to renew the specific bundle.",
            "    threshold_date = now - timedelta(days=AVAILABLE_FOR_RENEWAL_DAYS)",
            "",
            "    for (artifact_bundle_id, date_added) in used_artifact_bundles.items():",
            "        metrics.incr(\"artifact_lookup.get.renew_artifact_bundles.should_be_renewed\")",
            "        # We perform the condition check also before running the query, in order to reduce the amount of queries to the",
            "        # database.",
            "        if date_added <= threshold_date:",
            "            metrics.incr(\"artifact_lookup.get.renew_artifact_bundles.renewed\")",
            "            # We want to use a transaction, in order to keep the `date_added` consistent across multiple tables.",
            "            with transaction.atomic():",
            "                # We check again for the date_added condition in order to achieve consistency, this is done because",
            "                # the `can_be_renewed` call is using a time which differs from the one of the actual update in the db.",
            "                updated_rows_count = ArtifactBundle.objects.filter(",
            "                    id=artifact_bundle_id, date_added__lte=threshold_date",
            "                ).update(date_added=now)",
            "                # We want to make cascading queries only if there were actual changes in the db.",
            "                if updated_rows_count > 0:",
            "                    ProjectArtifactBundle.objects.filter(",
            "                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date",
            "                    ).update(date_added=now)",
            "                    ReleaseArtifactBundle.objects.filter(",
            "                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date",
            "                    ).update(date_added=now)",
            "                    DebugIdArtifactBundle.objects.filter(",
            "                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date",
            "                    ).update(date_added=now)",
            "",
            "",
            "def get_artifact_bundles_containing_debug_id(",
            "    debug_id: str, project: Project",
            ") -> Set[Tuple[int, datetime, int]]:",
            "    # We want to have the newest `File` for each `debug_id`.",
            "    return set(",
            "        ArtifactBundle.objects.filter(",
            "            organization_id=project.organization.id,",
            "            debugidartifactbundle__debug_id=debug_id,",
            "        )",
            "        .values_list(\"id\", \"date_added\", \"file_id\")",
            "        .order_by(\"-date_uploaded\")[:1]",
            "    )",
            "",
            "",
            "def get_release_artifacts(",
            "    project: Project,",
            "    release_name: str,",
            "    dist_name: Optional[str],",
            ") -> Set[Tuple[int, datetime, int]]:",
            "    return set(",
            "        ArtifactBundle.objects.filter(",
            "            organization_id=project.organization.id,",
            "            projectartifactbundle__project_id=project.id,",
            "            releaseartifactbundle__release_name=release_name,",
            "            # In case no dist is provided, we will fall back to \"\" which is the NULL equivalent for our tables.",
            "            # See `_create_artifact_bundle` in `src/sentry/tasks/assemble.py` for the reference.",
            "            releaseartifactbundle__dist_name=dist_name or \"\",",
            "        )",
            "        .values_list(\"id\", \"date_added\", \"file_id\")",
            "        .order_by(\"-date_uploaded\")[:MAX_BUNDLES_QUERY]",
            "    )",
            "",
            "",
            "def try_resolve_release_dist(",
            "    project: Project, release_name: str, dist_name: Optional[str]",
            ") -> Tuple[Optional[Release], Optional[Distribution]]:",
            "    release = None",
            "    dist = None",
            "    try:",
            "        release = Release.objects.get(",
            "            organization_id=project.organization_id,",
            "            projects=project,",
            "            version=release_name,",
            "        )",
            "",
            "        # We cannot query for dist without a release anyway",
            "        if dist_name:",
            "            dist = Distribution.objects.get(release=release, name=dist_name)",
            "    except (Release.DoesNotExist, Distribution.DoesNotExist):",
            "        pass",
            "    except Exception as exc:",
            "        logger.error(\"Failed to read\", exc_info=exc)",
            "",
            "    return release, dist",
            "",
            "",
            "def get_legacy_release_bundles(release: Release, dist: Optional[Distribution]):",
            "    return set(",
            "        ReleaseFile.objects.select_related(\"file\")",
            "        .filter(",
            "            release_id=release.id,",
            "            dist_id=dist.id if dist else None,",
            "            # a `ReleaseFile` with `0` artifacts represents a release archive,",
            "            # see the comment above the definition of `artifact_count`.",
            "            artifact_count=0,",
            "            # similarly the special `type` is also used for release archives.",
            "            file__type=RELEASE_BUNDLE_TYPE,",
            "        )",
            "        .values_list(\"file_id\", flat=True)",
            "        # TODO: this `order_by` might be incredibly slow",
            "        # we want to have a hard limit on the returned bundles here. and we would",
            "        # want to pick the most recently uploaded ones. that should mostly be",
            "        # relevant for customers that upload multiple bundles, or are uploading",
            "        # newer files for existing releases. In that case the symbolication is",
            "        # already degraded, so meh...",
            "        # .order_by(\"-file__timestamp\")",
            "        [:MAX_BUNDLES_QUERY]",
            "    )",
            "",
            "",
            "def get_legacy_releasefile_by_file_url(",
            "    release: Release, dist: Optional[Distribution], url: List[str]",
            ") -> Sequence[ReleaseFile]:",
            "    # Exclude files which are also present in archive:",
            "    return (",
            "        ReleaseFile.public_objects.filter(",
            "            release_id=release.id,",
            "            dist_id=dist.id if dist else None,",
            "        )",
            "        .exclude(artifact_count=0)",
            "        .select_related(\"file\")",
            "    ).filter(name__icontains=url)[:MAX_RELEASEFILES_QUERY]",
            "",
            "",
            "class UrlConstructor:",
            "    def __init__(self, request: Request, project: Project):",
            "        if is_system_auth(request.auth):",
            "            self.base_url = get_internal_artifact_lookup_source_url(project)",
            "        else:",
            "            self.base_url = request.build_absolute_uri(request.path)",
            "",
            "    def url_for_file_id(self, file_id: int) -> str:",
            "        # NOTE: Returning a self-route that requires authentication (via Bearer token)",
            "        # is not really forward compatible with a pre-signed URL that does not",
            "        # require any authentication or headers whatsoever.",
            "        # This also requires a workaround in Symbolicator, as its generic http",
            "        # downloader blocks \"internal\" IPs, whereas the internal Sentry downloader",
            "        # is explicitly exempt.",
            "        return f\"{self.base_url}?download={file_id}\""
        ],
        "afterPatchFile": [
            "import logging",
            "from datetime import datetime, timedelta",
            "from typing import List, Mapping, Optional, Sequence, Set, Tuple",
            "",
            "import pytz",
            "from django.db import transaction",
            "from django.http import Http404, HttpResponse, StreamingHttpResponse",
            "from rest_framework.request import Request",
            "from rest_framework.response import Response",
            "from symbolic import SymbolicError, normalize_debug_id",
            "",
            "from sentry import options, ratelimits",
            "from sentry.api.base import region_silo_endpoint",
            "from sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission",
            "from sentry.api.endpoints.debug_files import has_download_permission",
            "from sentry.api.serializers import serialize",
            "from sentry.auth.system import is_system_auth",
            "from sentry.lang.native.sources import get_internal_artifact_lookup_source_url",
            "from sentry.models import (",
            "    ArtifactBundle,",
            "    DebugIdArtifactBundle,",
            "    Distribution,",
            "    Project,",
            "    ProjectArtifactBundle,",
            "    Release,",
            "    ReleaseArtifactBundle,",
            "    ReleaseFile,",
            ")",
            "from sentry.utils import metrics",
            "",
            "logger = logging.getLogger(\"sentry.api\")",
            "",
            "# The marker for \"release\" bundles",
            "RELEASE_BUNDLE_TYPE = \"release.bundle\"",
            "# The number of bundles (\"artifact\" or \"release\") that we query",
            "MAX_BUNDLES_QUERY = 5",
            "# The number of files returned by the `get_releasefiles` query",
            "MAX_RELEASEFILES_QUERY = 10",
            "# Number of days that determine whether an artifact bundle is ready for being renewed.",
            "AVAILABLE_FOR_RENEWAL_DAYS = 30",
            "",
            "",
            "@region_silo_endpoint",
            "class ProjectArtifactLookupEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def download_file(self, download_id, project: Project):",
            "        ty, ty_id = download_id.split(\"/\")",
            "",
            "        rate_limited = ratelimits.is_limited(",
            "            project=project,",
            "            key=f\"rl:ArtifactLookupEndpoint:download:{download_id}:{project.id}\",",
            "            limit=10,",
            "        )",
            "        if rate_limited:",
            "            logger.info(",
            "                \"notification.rate_limited\",",
            "                extra={\"project_id\": project.id, \"file_id\": download_id},",
            "            )",
            "            return HttpResponse({\"Too many download requests\"}, status=429)",
            "",
            "        file = None",
            "        if ty == \"artifact_bundle\":",
            "            file = (",
            "                ArtifactBundle.objects.filter(",
            "                    id=ty_id,",
            "                    projectartifactbundle__project_id=project.id,",
            "                )",
            "                .select_related(\"file\")",
            "                .first()",
            "            )",
            "        elif ty == \"release_file\":",
            "            # NOTE: `ReleaseFile` does have a `project_id`, but that seems to",
            "            # be always empty, so using the `organization_id` instead.",
            "            file = (",
            "                ReleaseFile.objects.filter(id=ty_id, organization_id=project.organization.id)",
            "                .select_related(\"file\")",
            "                .first()",
            "            )",
            "",
            "        if file is None:",
            "            raise Http404",
            "        file = file.file",
            "",
            "        try:",
            "            fp = file.getfile()",
            "            response = StreamingHttpResponse(",
            "                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"",
            "            )",
            "            response[\"Content-Length\"] = file.size",
            "            response[\"Content-Disposition\"] = f'attachment; filename=\"{file.name}\"'",
            "            return response",
            "        except OSError:",
            "            raise Http404",
            "",
            "    def get(self, request: Request, project: Project) -> Response:",
            "        \"\"\"",
            "        List a Project's Individual Artifacts or Bundles",
            "        ````````````````````````````````````````",
            "",
            "        Retrieve a list of individual artifacts or artifact bundles for a given project.",
            "",
            "        :pparam string organization_slug: the slug of the organization to query.",
            "        :pparam string project_slug: the slug of the project to query.",
            "        :qparam string debug_id: if set, will query and return the artifact",
            "                                 bundle that matches the given `debug_id`.",
            "        :qparam string url: if set, will query and return all the individual",
            "                            artifacts, or artifact bundles that contain files",
            "                            that match the `url`. This is using a substring-match.",
            "        :qparam string release: used in conjunction with `url`.",
            "        :qparam string dist: used in conjunction with `url`.",
            "",
            "        :auth: required",
            "        \"\"\"",
            "        if (download_id := request.GET.get(\"download\")) is not None:",
            "            if has_download_permission(request, project):",
            "                return self.download_file(download_id, project)",
            "            else:",
            "                return Response(status=403)",
            "",
            "        debug_id = request.GET.get(\"debug_id\")",
            "        try:",
            "            debug_id = normalize_debug_id(debug_id)",
            "        except SymbolicError:",
            "            pass",
            "        url = request.GET.get(\"url\")",
            "        release_name = request.GET.get(\"release\")",
            "        dist_name = request.GET.get(\"dist\")",
            "",
            "        used_artifact_bundles = dict()",
            "        bundle_file_ids = set()",
            "",
            "        def update_bundles(inner_bundles: Set[Tuple[int, datetime, int]]):",
            "            for (bundle_id, date_added, file_id) in inner_bundles:",
            "                used_artifact_bundles[bundle_id] = date_added",
            "                bundle_file_ids.add((\"artifact_bundle\", bundle_id, file_id))",
            "",
            "        if debug_id:",
            "            bundles = get_artifact_bundles_containing_debug_id(debug_id, project)",
            "            update_bundles(bundles)",
            "",
            "        individual_files = set()",
            "        if url and release_name and not bundle_file_ids:",
            "            # Get both the newest X release artifact bundles,",
            "            # and also query the legacy artifact bundles. One of those should have the",
            "            # file we are looking for. We want to return more here, even bundles that",
            "            # do *not* contain the file, rather than opening up each bundle. We want to",
            "            # avoid opening up bundles at all cost.",
            "            bundles = get_release_artifacts(project, release_name, dist_name)",
            "            update_bundles(bundles)",
            "",
            "            release, dist = try_resolve_release_dist(project, release_name, dist_name)",
            "            if release:",
            "                for (releasefile_id, file_id) in get_legacy_release_bundles(release, dist):",
            "                    bundle_file_ids.add((\"release_file\", releasefile_id, file_id))",
            "                individual_files = get_legacy_releasefile_by_file_url(release, dist, url)",
            "",
            "        if options.get(\"sourcemaps.artifact-bundles.enable-renewal\") == 1.0:",
            "            with metrics.timer(\"artifact_lookup.get.renew_artifact_bundles\"):",
            "                # Before constructing the response, we want to update the artifact bundles renewal date.",
            "                renew_artifact_bundles(used_artifact_bundles)",
            "",
            "        # Then: Construct our response",
            "        url_constructor = UrlConstructor(request, project)",
            "",
            "        found_artifacts = []",
            "        # NOTE: the reason we use the `file_id` as the `id` we return is because",
            "        # downstream symbolicator relies on that for its internal caching.",
            "        # We do not want to hard-refresh those caches quite yet, and the `id`",
            "        # should also be as unique as possible, which the `file_id` is.",
            "        for (ty, ty_id, file_id) in bundle_file_ids:",
            "            found_artifacts.append(",
            "                {",
            "                    \"id\": str(file_id),",
            "                    \"type\": \"bundle\",",
            "                    \"url\": url_constructor.url_for_file_id(ty, ty_id),",
            "                }",
            "            )",
            "",
            "        for release_file in individual_files:",
            "            found_artifacts.append(",
            "                {",
            "                    \"id\": str(release_file.file.id),",
            "                    \"type\": \"file\",",
            "                    \"url\": url_constructor.url_for_file_id(\"release_file\", release_file.id),",
            "                    # The `name` is the url/abs_path of the file,",
            "                    # as in: `\"~/path/to/file.min.js\"`.",
            "                    \"abs_path\": release_file.name,",
            "                    # These headers should ideally include the `Sourcemap` reference",
            "                    \"headers\": release_file.file.headers,",
            "                }",
            "            )",
            "",
            "        # make sure we have a stable sort order for tests",
            "        found_artifacts.sort(key=lambda x: int(x[\"id\"]))",
            "",
            "        # NOTE: We do not paginate this response, as we have very tight limits on all the individual queries.",
            "        return Response(serialize(found_artifacts, request.user))",
            "",
            "",
            "def renew_artifact_bundles(used_artifact_bundles: Mapping[int, datetime]):",
            "    # We take a snapshot in time that MUST be consistent across all updates.",
            "    now = datetime.now(tz=pytz.UTC)",
            "    # We compute the threshold used to determine whether we want to renew the specific bundle.",
            "    threshold_date = now - timedelta(days=AVAILABLE_FOR_RENEWAL_DAYS)",
            "",
            "    for (artifact_bundle_id, date_added) in used_artifact_bundles.items():",
            "        metrics.incr(\"artifact_lookup.get.renew_artifact_bundles.should_be_renewed\")",
            "        # We perform the condition check also before running the query, in order to reduce the amount of queries to the",
            "        # database.",
            "        if date_added <= threshold_date:",
            "            metrics.incr(\"artifact_lookup.get.renew_artifact_bundles.renewed\")",
            "            # We want to use a transaction, in order to keep the `date_added` consistent across multiple tables.",
            "            with transaction.atomic():",
            "                # We check again for the date_added condition in order to achieve consistency, this is done because",
            "                # the `can_be_renewed` call is using a time which differs from the one of the actual update in the db.",
            "                updated_rows_count = ArtifactBundle.objects.filter(",
            "                    id=artifact_bundle_id, date_added__lte=threshold_date",
            "                ).update(date_added=now)",
            "                # We want to make cascading queries only if there were actual changes in the db.",
            "                if updated_rows_count > 0:",
            "                    ProjectArtifactBundle.objects.filter(",
            "                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date",
            "                    ).update(date_added=now)",
            "                    ReleaseArtifactBundle.objects.filter(",
            "                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date",
            "                    ).update(date_added=now)",
            "                    DebugIdArtifactBundle.objects.filter(",
            "                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date",
            "                    ).update(date_added=now)",
            "",
            "",
            "def get_artifact_bundles_containing_debug_id(",
            "    debug_id: str, project: Project",
            ") -> Set[Tuple[int, datetime, int]]:",
            "    # We want to have the newest `File` for each `debug_id`.",
            "    return set(",
            "        ArtifactBundle.objects.filter(",
            "            organization_id=project.organization.id,",
            "            debugidartifactbundle__debug_id=debug_id,",
            "        )",
            "        .values_list(\"id\", \"date_added\", \"file_id\")",
            "        .order_by(\"-date_uploaded\")[:1]",
            "    )",
            "",
            "",
            "def get_release_artifacts(",
            "    project: Project,",
            "    release_name: str,",
            "    dist_name: Optional[str],",
            ") -> Set[Tuple[int, datetime, int]]:",
            "    return set(",
            "        ArtifactBundle.objects.filter(",
            "            organization_id=project.organization.id,",
            "            projectartifactbundle__project_id=project.id,",
            "            releaseartifactbundle__release_name=release_name,",
            "            # In case no dist is provided, we will fall back to \"\" which is the NULL equivalent for our tables.",
            "            # See `_create_artifact_bundle` in `src/sentry/tasks/assemble.py` for the reference.",
            "            releaseartifactbundle__dist_name=dist_name or \"\",",
            "        )",
            "        .values_list(\"id\", \"date_added\", \"file_id\")",
            "        .order_by(\"-date_uploaded\")[:MAX_BUNDLES_QUERY]",
            "    )",
            "",
            "",
            "def try_resolve_release_dist(",
            "    project: Project, release_name: str, dist_name: Optional[str]",
            ") -> Tuple[Optional[Release], Optional[Distribution]]:",
            "    release = None",
            "    dist = None",
            "    try:",
            "        release = Release.objects.get(",
            "            organization_id=project.organization_id,",
            "            projects=project,",
            "            version=release_name,",
            "        )",
            "",
            "        # We cannot query for dist without a release anyway",
            "        if dist_name:",
            "            dist = Distribution.objects.get(release=release, name=dist_name)",
            "    except (Release.DoesNotExist, Distribution.DoesNotExist):",
            "        pass",
            "    except Exception as exc:",
            "        logger.error(\"Failed to read\", exc_info=exc)",
            "",
            "    return release, dist",
            "",
            "",
            "def get_legacy_release_bundles(",
            "    release: Release, dist: Optional[Distribution]",
            ") -> Set[Tuple[int, int]]:",
            "    return set(",
            "        ReleaseFile.objects.filter(",
            "            release_id=release.id,",
            "            dist_id=dist.id if dist else None,",
            "            # a `ReleaseFile` with `0` artifacts represents a release archive,",
            "            # see the comment above the definition of `artifact_count`.",
            "            artifact_count=0,",
            "            # similarly the special `type` is also used for release archives.",
            "            file__type=RELEASE_BUNDLE_TYPE,",
            "        )",
            "        .select_related(\"file\")",
            "        .values_list(\"id\", \"file_id\")",
            "        # TODO: this `order_by` might be incredibly slow",
            "        # we want to have a hard limit on the returned bundles here. and we would",
            "        # want to pick the most recently uploaded ones. that should mostly be",
            "        # relevant for customers that upload multiple bundles, or are uploading",
            "        # newer files for existing releases. In that case the symbolication is",
            "        # already degraded, so meh...",
            "        # .order_by(\"-file__timestamp\")",
            "        [:MAX_BUNDLES_QUERY]",
            "    )",
            "",
            "",
            "def get_legacy_releasefile_by_file_url(",
            "    release: Release, dist: Optional[Distribution], url: List[str]",
            ") -> Sequence[ReleaseFile]:",
            "    # Exclude files which are also present in archive:",
            "    return (",
            "        ReleaseFile.public_objects.filter(",
            "            release_id=release.id,",
            "            dist_id=dist.id if dist else None,",
            "        )",
            "        .exclude(artifact_count=0)",
            "        .select_related(\"file\")",
            "    ).filter(name__icontains=url)[:MAX_RELEASEFILES_QUERY]",
            "",
            "",
            "class UrlConstructor:",
            "    def __init__(self, request: Request, project: Project):",
            "        if is_system_auth(request.auth):",
            "            self.base_url = get_internal_artifact_lookup_source_url(project)",
            "        else:",
            "            self.base_url = request.build_absolute_uri(request.path)",
            "",
            "    def url_for_file_id(self, ty: str, file_id: int) -> str:",
            "        # NOTE: Returning a self-route that requires authentication (via Bearer token)",
            "        # is not really forward compatible with a pre-signed URL that does not",
            "        # require any authentication or headers whatsoever.",
            "        # This also requires a workaround in Symbolicator, as its generic http",
            "        # downloader blocks \"internal\" IPs, whereas the internal Sentry downloader",
            "        # is explicitly exempt.",
            "        return f\"{self.base_url}?download={ty}/{file_id}\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "23": [],
            "48": [
                "ProjectArtifactLookupEndpoint",
                "download_file"
            ],
            "51": [
                "ProjectArtifactLookupEndpoint",
                "download_file"
            ],
            "57": [
                "ProjectArtifactLookupEndpoint",
                "download_file"
            ],
            "61": [
                "ProjectArtifactLookupEndpoint",
                "download_file"
            ],
            "96": [
                "ProjectArtifactLookupEndpoint",
                "get"
            ],
            "98": [
                "ProjectArtifactLookupEndpoint",
                "get"
            ],
            "117": [
                "ProjectArtifactLookupEndpoint",
                "get",
                "update_bundles"
            ],
            "135": [
                "ProjectArtifactLookupEndpoint",
                "get"
            ],
            "147": [
                "ProjectArtifactLookupEndpoint",
                "get"
            ],
            "152": [
                "ProjectArtifactLookupEndpoint",
                "get"
            ],
            "161": [
                "ProjectArtifactLookupEndpoint",
                "get"
            ],
            "262": [
                "get_legacy_release_bundles"
            ],
            "264": [
                "get_legacy_release_bundles"
            ],
            "265": [
                "get_legacy_release_bundles"
            ],
            "274": [
                "get_legacy_release_bundles"
            ],
            "307": [
                "UrlConstructor",
                "url_for_file_id"
            ],
            "314": [
                "UrlConstructor",
                "url_for_file_id"
            ]
        },
        "addLocation": []
    },
    "src/sentry/api/endpoints/debug_files.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "                 \"notification.rate_limited\","
            },
            "1": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "                 extra={\"project_id\": project.id, \"project_debug_file_id\": debug_file_id},"
            },
            "2": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "             )"
            },
            "3": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return HttpResponse({\"Too many download requests\"}, status=403)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+            return HttpResponse({\"Too many download requests\"}, status=429)"
            },
            "5": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 99,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        debug_file = ProjectDebugFile.objects.filter(id=debug_file_id).first()"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+        debug_file = ProjectDebugFile.objects.filter("
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+            id=debug_file_id, project_id=project.id"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+        ).first()"
            },
            "10": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 103,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "         if debug_file is None:"
            },
            "12": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "             raise Http404"
            }
        },
        "frontPatchFile": [
            "import logging",
            "import posixpath",
            "import re",
            "",
            "import jsonschema",
            "from django.db import router",
            "from django.db.models import Q",
            "from django.http import Http404, HttpResponse, StreamingHttpResponse",
            "from rest_framework.request import Request",
            "from rest_framework.response import Response",
            "from symbolic import SymbolicError, normalize_debug_id",
            "",
            "from sentry import ratelimits, roles",
            "from sentry.api.base import region_silo_endpoint",
            "from sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission",
            "from sentry.api.exceptions import ResourceDoesNotExist",
            "from sentry.api.paginator import OffsetPaginator",
            "from sentry.api.serializers import serialize",
            "from sentry.auth.superuser import is_active_superuser",
            "from sentry.auth.system import is_system_auth",
            "from sentry.constants import DEBUG_FILES_ROLE_DEFAULT, KNOWN_DIF_FORMATS",
            "from sentry.models import (",
            "    File,",
            "    FileBlobOwner,",
            "    OrganizationMember,",
            "    ProjectDebugFile,",
            "    Release,",
            "    ReleaseFile,",
            "    create_files_from_dif_zip,",
            ")",
            "from sentry.models.release import get_artifact_counts",
            "from sentry.tasks.assemble import (",
            "    AssembleTask,",
            "    ChunkFileState,",
            "    get_assemble_status,",
            "    set_assemble_status,",
            ")",
            "from sentry.utils import json",
            "from sentry.utils.db import atomic_transaction",
            "",
            "logger = logging.getLogger(\"sentry.api\")",
            "ERR_FILE_EXISTS = \"A file matching this debug identifier already exists\"",
            "DIF_MIMETYPES = {v: k for k, v in KNOWN_DIF_FORMATS.items()}",
            "_release_suffix = re.compile(r\"^(.*)\\s+\\(([^)]+)\\)\\s*$\")",
            "",
            "",
            "def upload_from_request(request, project):",
            "    if \"file\" not in request.data:",
            "        return Response({\"detail\": \"Missing uploaded file\"}, status=400)",
            "    fileobj = request.data[\"file\"]",
            "    files = create_files_from_dif_zip(fileobj, project=project)",
            "    return Response(serialize(files, request.user), status=201)",
            "",
            "",
            "def has_download_permission(request, project):",
            "    if is_system_auth(request.auth) or is_active_superuser(request):",
            "        return True",
            "",
            "    if not request.user.is_authenticated:",
            "        return False",
            "",
            "    organization = project.organization",
            "    required_role = organization.get_option(\"sentry:debug_files_role\") or DEBUG_FILES_ROLE_DEFAULT",
            "",
            "    if request.user.is_sentry_app:",
            "        if roles.get(required_role).priority > roles.get(\"member\").priority:",
            "            return request.access.has_scope(\"project:write\")",
            "        else:",
            "            return request.access.has_scope(\"project:read\")",
            "",
            "    try:",
            "        current_role = (",
            "            OrganizationMember.objects.filter(organization=organization, user_id=request.user.id)",
            "            .values_list(\"role\", flat=True)",
            "            .get()",
            "        )",
            "    except OrganizationMember.DoesNotExist:",
            "        return False",
            "",
            "    return roles.get(current_role).priority >= roles.get(required_role).priority",
            "",
            "",
            "@region_silo_endpoint",
            "class DebugFilesEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def download(self, debug_file_id, project):",
            "        rate_limited = ratelimits.is_limited(",
            "            project=project,",
            "            key=f\"rl:DSymFilesEndpoint:download:{debug_file_id}:{project.id}\",",
            "            limit=10,",
            "        )",
            "        if rate_limited:",
            "            logger.info(",
            "                \"notification.rate_limited\",",
            "                extra={\"project_id\": project.id, \"project_debug_file_id\": debug_file_id},",
            "            )",
            "            return HttpResponse({\"Too many download requests\"}, status=403)",
            "",
            "        debug_file = ProjectDebugFile.objects.filter(id=debug_file_id).first()",
            "",
            "        if debug_file is None:",
            "            raise Http404",
            "",
            "        try:",
            "            fp = debug_file.file.getfile()",
            "            response = StreamingHttpResponse(",
            "                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"",
            "            )",
            "            response[\"Content-Length\"] = debug_file.file.size",
            "            response[\"Content-Disposition\"] = 'attachment; filename=\"{}{}\"'.format(",
            "                posixpath.basename(debug_file.debug_id),",
            "                debug_file.file_extension,",
            "            )",
            "            return response",
            "        except OSError:",
            "            raise Http404",
            "",
            "    def get(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        List a Project's Debug Information Files",
            "        ````````````````````````````````````````",
            "",
            "        Retrieve a list of debug information files for a given project.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                          file belongs to.",
            "        :pparam string project_slug: the slug of the project to list the",
            "                                     DIFs of.",
            "        :qparam string query: If set, this parameter is used to locate DIFs with.",
            "        :qparam string id: If set, the specified DIF will be sent in the response.",
            "        :qparam string file_formats: If set, only DIFs with these formats will be returned.",
            "        :auth: required",
            "        \"\"\"",
            "        download_requested = request.GET.get(\"id\") is not None",
            "        if download_requested and (has_download_permission(request, project)):",
            "            return self.download(request.GET.get(\"id\"), project)",
            "        elif download_requested:",
            "            return Response(status=403)",
            "",
            "        code_id = request.GET.get(\"code_id\")",
            "        debug_id = request.GET.get(\"debug_id\")",
            "        query = request.GET.get(\"query\")",
            "        file_formats = request.GET.getlist(\"file_formats\")",
            "",
            "        # If this query contains a debug identifier, normalize it to allow for",
            "        # more lenient queries (e.g. supporting Breakpad ids). Use the index to",
            "        # speed up such queries.",
            "        if query and len(query) <= 45 and not debug_id:",
            "            try:",
            "                debug_id = normalize_debug_id(query.strip())",
            "            except SymbolicError:",
            "                pass",
            "",
            "        if debug_id:",
            "            # If a debug ID is specified, do not consider the stored code",
            "            # identifier and strictly filter by debug identifier. Often there",
            "            # are mismatches in the code identifier in PEs.",
            "            q = Q(debug_id__exact=debug_id)",
            "        elif code_id:",
            "            q = Q(code_id__exact=code_id)",
            "        elif query:",
            "            q = (",
            "                Q(object_name__icontains=query)",
            "                | Q(debug_id__icontains=query)",
            "                | Q(code_id__icontains=query)",
            "                | Q(cpu_name__icontains=query)",
            "                | Q(file__headers__icontains=query)",
            "            )",
            "",
            "            known_file_format = DIF_MIMETYPES.get(query)",
            "            if known_file_format:",
            "                q |= Q(file__headers__icontains=known_file_format)",
            "        else:",
            "            q = Q()",
            "",
            "        file_format_q = Q()",
            "        for file_format in file_formats:",
            "            known_file_format = DIF_MIMETYPES.get(file_format)",
            "            if known_file_format:",
            "                file_format_q |= Q(file__headers__icontains=known_file_format)",
            "",
            "        q &= file_format_q",
            "",
            "        queryset = ProjectDebugFile.objects.filter(q, project_id=project.id).select_related(\"file\")",
            "",
            "        return self.paginate(",
            "            request=request,",
            "            queryset=queryset,",
            "            order_by=\"-id\",",
            "            paginator_cls=OffsetPaginator,",
            "            default_per_page=20,",
            "            on_results=lambda x: serialize(x, request.user),",
            "        )",
            "",
            "    def delete(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        Delete a specific Project's Debug Information File",
            "        ```````````````````````````````````````````````````",
            "",
            "        Delete a debug information file for a given project.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                          file belongs to.",
            "        :pparam string project_slug: the slug of the project to delete the",
            "                                     DIF.",
            "        :qparam string id: The id of the DIF to delete.",
            "        :auth: required",
            "        \"\"\"",
            "",
            "        if request.GET.get(\"id\") and (request.access.has_scope(\"project:write\")):",
            "            with atomic_transaction(using=router.db_for_write(File)):",
            "                debug_file = (",
            "                    ProjectDebugFile.objects.filter(id=request.GET.get(\"id\"), project_id=project.id)",
            "                    .select_related(\"file\")",
            "                    .first()",
            "                )",
            "                if debug_file is not None:",
            "                    debug_file.delete()",
            "                    return Response(status=204)",
            "",
            "        return Response(status=404)",
            "",
            "    def post(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        Upload a New File",
            "        `````````````````",
            "",
            "        Upload a new debug information file for the given release.",
            "",
            "        Unlike other API requests, files must be uploaded using the",
            "        traditional multipart/form-data content-type.",
            "",
            "        The file uploaded is a zip archive of a Apple .dSYM folder which",
            "        contains the individual debug images.  Uploading through this endpoint",
            "        will create different files for the contained images.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                          release belongs to.",
            "        :pparam string project_slug: the slug of the project to change the",
            "                                     release of.",
            "        :param file file: the multipart encoded file.",
            "        :auth: required",
            "        \"\"\"",
            "        return upload_from_request(request, project=project)",
            "",
            "",
            "@region_silo_endpoint",
            "class UnknownDebugFilesEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def get(self, request: Request, project) -> Response:",
            "        checksums = request.GET.getlist(\"checksums\")",
            "        missing = ProjectDebugFile.objects.find_missing(checksums, project=project)",
            "        return Response({\"missing\": missing})",
            "",
            "",
            "@region_silo_endpoint",
            "class AssociateDSymFilesEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    # Legacy endpoint, kept for backwards compatibility",
            "    def post(self, request: Request, project) -> Response:",
            "        return Response({\"associatedDsymFiles\": []})",
            "",
            "",
            "def find_missing_chunks(organization, chunks):",
            "    \"\"\"Returns a list of chunks which are missing for an org.\"\"\"",
            "    owned = set(",
            "        FileBlobOwner.objects.filter(",
            "            blob__checksum__in=chunks, organization_id=organization.id",
            "        ).values_list(\"blob__checksum\", flat=True)",
            "    )",
            "    return list(set(chunks) - owned)",
            "",
            "",
            "@region_silo_endpoint",
            "class DifAssembleEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def post(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        Assemble one or multiple chunks (FileBlob) into debug files",
            "        ````````````````````````````````````````````````````````````",
            "",
            "        :auth: required",
            "        \"\"\"",
            "        schema = {",
            "            \"type\": \"object\",",
            "            \"patternProperties\": {",
            "                \"^[0-9a-f]{40}$\": {",
            "                    \"type\": \"object\",",
            "                    \"required\": [\"name\", \"chunks\"],",
            "                    \"properties\": {",
            "                        \"name\": {\"type\": \"string\"},",
            "                        \"debug_id\": {\"type\": \"string\"},",
            "                        \"chunks\": {",
            "                            \"type\": \"array\",",
            "                            \"items\": {\"type\": \"string\", \"pattern\": \"^[0-9a-f]{40}$\"},",
            "                        },",
            "                    },",
            "                    \"additionalProperties\": True,",
            "                }",
            "            },",
            "            \"additionalProperties\": False,",
            "        }",
            "",
            "        try:",
            "            files = json.loads(request.body)",
            "            jsonschema.validate(files, schema)",
            "        except jsonschema.ValidationError as e:",
            "            return Response({\"error\": str(e).splitlines()[0]}, status=400)",
            "        except Exception:",
            "            return Response({\"error\": \"Invalid json body\"}, status=400)",
            "",
            "        file_response = {}",
            "",
            "        for checksum, file_to_assemble in files.items():",
            "            name = file_to_assemble.get(\"name\", None)",
            "            debug_id = file_to_assemble.get(\"debug_id\", None)",
            "            chunks = file_to_assemble.get(\"chunks\", [])",
            "",
            "            # First, check the cached assemble status. During assembling, a",
            "            # ProjectDebugFile will be created and we need to prevent a race",
            "            # condition.",
            "            state, detail = get_assemble_status(AssembleTask.DIF, project.id, checksum)",
            "            if state == ChunkFileState.OK:",
            "                file_response[checksum] = {",
            "                    \"state\": state,",
            "                    \"detail\": None,",
            "                    \"missingChunks\": [],",
            "                    \"dif\": detail,",
            "                }",
            "                continue",
            "            elif state is not None:",
            "                file_response[checksum] = {\"state\": state, \"detail\": detail, \"missingChunks\": []}",
            "                continue",
            "",
            "            # Next, check if this project already owns the ProjectDebugFile.",
            "            # This can under rare circumstances yield more than one file",
            "            # which is why we use first() here instead of get().",
            "            dif = (",
            "                ProjectDebugFile.objects.filter(project_id=project.id, checksum=checksum)",
            "                .select_related(\"file\")",
            "                .order_by(\"-id\")",
            "                .first()",
            "            )",
            "",
            "            if dif is not None:",
            "                file_response[checksum] = {",
            "                    \"state\": ChunkFileState.OK,",
            "                    \"detail\": None,",
            "                    \"missingChunks\": [],",
            "                    \"dif\": serialize(dif),",
            "                }",
            "                continue",
            "",
            "            # There is neither a known file nor a cached state, so we will",
            "            # have to create a new file.  Assure that there are checksums.",
            "            # If not, we assume this is a poll and report NOT_FOUND",
            "            if not chunks:",
            "                file_response[checksum] = {\"state\": ChunkFileState.NOT_FOUND, \"missingChunks\": []}",
            "                continue",
            "",
            "            # Check if all requested chunks have been uploaded.",
            "            missing_chunks = find_missing_chunks(project.organization, chunks)",
            "            if missing_chunks:",
            "                file_response[checksum] = {",
            "                    \"state\": ChunkFileState.NOT_FOUND,",
            "                    \"missingChunks\": missing_chunks,",
            "                }",
            "                continue",
            "",
            "            # We don't have a state yet, this means we can now start",
            "            # an assemble job in the background.",
            "            set_assemble_status(AssembleTask.DIF, project.id, checksum, ChunkFileState.CREATED)",
            "",
            "            from sentry.tasks.assemble import assemble_dif",
            "",
            "            assemble_dif.apply_async(",
            "                kwargs={",
            "                    \"project_id\": project.id,",
            "                    \"name\": name,",
            "                    \"debug_id\": debug_id,",
            "                    \"checksum\": checksum,",
            "                    \"chunks\": chunks,",
            "                }",
            "            )",
            "",
            "            file_response[checksum] = {\"state\": ChunkFileState.CREATED, \"missingChunks\": []}",
            "",
            "        return Response(file_response, status=200)",
            "",
            "",
            "@region_silo_endpoint",
            "class SourceMapsEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def get(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        List a Project's Source Map Archives",
            "        ````````````````````````````````````",
            "",
            "        Retrieve a list of source map archives (releases, later bundles) for a given project.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                          source map archive belongs to.",
            "        :pparam string project_slug: the slug of the project to list the",
            "                                     source map archives of.",
            "        :qparam string query: If set, this parameter is used to locate source map archives with.",
            "        :auth: required",
            "        \"\"\"",
            "        query = request.GET.get(\"query\")",
            "",
            "        try:",
            "            queryset = Release.objects.filter(",
            "                projects=project, organization_id=project.organization_id",
            "            ).values(\"id\", \"version\", \"date_added\")",
            "        except Release.DoesNotExist:",
            "            raise ResourceDoesNotExist",
            "",
            "        if query:",
            "            query_q = Q(version__icontains=query)",
            "",
            "            suffix_match = _release_suffix.match(query)",
            "            if suffix_match is not None:",
            "                query_q |= Q(version__icontains=\"%s+%s\" % suffix_match.groups())",
            "",
            "            queryset = queryset.filter(query_q)",
            "",
            "        def expose_release(release, count):",
            "            return {",
            "                \"type\": \"release\",",
            "                \"id\": release[\"id\"],",
            "                \"name\": release[\"version\"],",
            "                \"date\": release[\"date_added\"],",
            "                \"fileCount\": count,",
            "            }",
            "",
            "        def serialize_results(results):",
            "            file_count_map = get_artifact_counts([r[\"id\"] for r in results])",
            "            # In case we didn't find a file count for a specific release, we will return -1, signaling to the",
            "            # frontend that this release doesn't have one or more ReleaseFile.",
            "            return serialize(",
            "                [expose_release(r, file_count_map.get(r[\"id\"], -1)) for r in results], request.user",
            "            )",
            "",
            "        sort_by = request.GET.get(\"sortBy\", \"-date_added\")",
            "        if sort_by not in {\"-date_added\", \"date_added\"}:",
            "            return Response(",
            "                {\"error\": \"You can either sort via 'date_added' or '-date_added'\"}, status=400",
            "            )",
            "",
            "        return self.paginate(",
            "            request=request,",
            "            queryset=queryset,",
            "            order_by=sort_by,",
            "            paginator_cls=OffsetPaginator,",
            "            default_per_page=10,",
            "            on_results=serialize_results,",
            "        )",
            "",
            "    def delete(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        Delete an Archive",
            "        ```````````````````````````````````````````````````",
            "",
            "        Delete all artifacts inside given archive.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                            archive belongs to.",
            "        :pparam string project_slug: the slug of the project to delete the",
            "                                        archive of.",
            "        :qparam string name: The name of the archive to delete.",
            "        :auth: required",
            "        \"\"\"",
            "",
            "        archive_name = request.GET.get(\"name\")",
            "",
            "        if archive_name:",
            "            with atomic_transaction(using=router.db_for_write(ReleaseFile)):",
            "                release = Release.objects.get(",
            "                    organization_id=project.organization_id, projects=project, version=archive_name",
            "                )",
            "                if release is not None:",
            "                    release_files = ReleaseFile.objects.filter(release_id=release.id)",
            "                    release_files.delete()",
            "                    return Response(status=204)",
            "",
            "        return Response(status=404)"
        ],
        "afterPatchFile": [
            "import logging",
            "import posixpath",
            "import re",
            "",
            "import jsonschema",
            "from django.db import router",
            "from django.db.models import Q",
            "from django.http import Http404, HttpResponse, StreamingHttpResponse",
            "from rest_framework.request import Request",
            "from rest_framework.response import Response",
            "from symbolic import SymbolicError, normalize_debug_id",
            "",
            "from sentry import ratelimits, roles",
            "from sentry.api.base import region_silo_endpoint",
            "from sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission",
            "from sentry.api.exceptions import ResourceDoesNotExist",
            "from sentry.api.paginator import OffsetPaginator",
            "from sentry.api.serializers import serialize",
            "from sentry.auth.superuser import is_active_superuser",
            "from sentry.auth.system import is_system_auth",
            "from sentry.constants import DEBUG_FILES_ROLE_DEFAULT, KNOWN_DIF_FORMATS",
            "from sentry.models import (",
            "    File,",
            "    FileBlobOwner,",
            "    OrganizationMember,",
            "    ProjectDebugFile,",
            "    Release,",
            "    ReleaseFile,",
            "    create_files_from_dif_zip,",
            ")",
            "from sentry.models.release import get_artifact_counts",
            "from sentry.tasks.assemble import (",
            "    AssembleTask,",
            "    ChunkFileState,",
            "    get_assemble_status,",
            "    set_assemble_status,",
            ")",
            "from sentry.utils import json",
            "from sentry.utils.db import atomic_transaction",
            "",
            "logger = logging.getLogger(\"sentry.api\")",
            "ERR_FILE_EXISTS = \"A file matching this debug identifier already exists\"",
            "DIF_MIMETYPES = {v: k for k, v in KNOWN_DIF_FORMATS.items()}",
            "_release_suffix = re.compile(r\"^(.*)\\s+\\(([^)]+)\\)\\s*$\")",
            "",
            "",
            "def upload_from_request(request, project):",
            "    if \"file\" not in request.data:",
            "        return Response({\"detail\": \"Missing uploaded file\"}, status=400)",
            "    fileobj = request.data[\"file\"]",
            "    files = create_files_from_dif_zip(fileobj, project=project)",
            "    return Response(serialize(files, request.user), status=201)",
            "",
            "",
            "def has_download_permission(request, project):",
            "    if is_system_auth(request.auth) or is_active_superuser(request):",
            "        return True",
            "",
            "    if not request.user.is_authenticated:",
            "        return False",
            "",
            "    organization = project.organization",
            "    required_role = organization.get_option(\"sentry:debug_files_role\") or DEBUG_FILES_ROLE_DEFAULT",
            "",
            "    if request.user.is_sentry_app:",
            "        if roles.get(required_role).priority > roles.get(\"member\").priority:",
            "            return request.access.has_scope(\"project:write\")",
            "        else:",
            "            return request.access.has_scope(\"project:read\")",
            "",
            "    try:",
            "        current_role = (",
            "            OrganizationMember.objects.filter(organization=organization, user_id=request.user.id)",
            "            .values_list(\"role\", flat=True)",
            "            .get()",
            "        )",
            "    except OrganizationMember.DoesNotExist:",
            "        return False",
            "",
            "    return roles.get(current_role).priority >= roles.get(required_role).priority",
            "",
            "",
            "@region_silo_endpoint",
            "class DebugFilesEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def download(self, debug_file_id, project):",
            "        rate_limited = ratelimits.is_limited(",
            "            project=project,",
            "            key=f\"rl:DSymFilesEndpoint:download:{debug_file_id}:{project.id}\",",
            "            limit=10,",
            "        )",
            "        if rate_limited:",
            "            logger.info(",
            "                \"notification.rate_limited\",",
            "                extra={\"project_id\": project.id, \"project_debug_file_id\": debug_file_id},",
            "            )",
            "            return HttpResponse({\"Too many download requests\"}, status=429)",
            "",
            "        debug_file = ProjectDebugFile.objects.filter(",
            "            id=debug_file_id, project_id=project.id",
            "        ).first()",
            "",
            "        if debug_file is None:",
            "            raise Http404",
            "",
            "        try:",
            "            fp = debug_file.file.getfile()",
            "            response = StreamingHttpResponse(",
            "                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"",
            "            )",
            "            response[\"Content-Length\"] = debug_file.file.size",
            "            response[\"Content-Disposition\"] = 'attachment; filename=\"{}{}\"'.format(",
            "                posixpath.basename(debug_file.debug_id),",
            "                debug_file.file_extension,",
            "            )",
            "            return response",
            "        except OSError:",
            "            raise Http404",
            "",
            "    def get(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        List a Project's Debug Information Files",
            "        ````````````````````````````````````````",
            "",
            "        Retrieve a list of debug information files for a given project.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                          file belongs to.",
            "        :pparam string project_slug: the slug of the project to list the",
            "                                     DIFs of.",
            "        :qparam string query: If set, this parameter is used to locate DIFs with.",
            "        :qparam string id: If set, the specified DIF will be sent in the response.",
            "        :qparam string file_formats: If set, only DIFs with these formats will be returned.",
            "        :auth: required",
            "        \"\"\"",
            "        download_requested = request.GET.get(\"id\") is not None",
            "        if download_requested and (has_download_permission(request, project)):",
            "            return self.download(request.GET.get(\"id\"), project)",
            "        elif download_requested:",
            "            return Response(status=403)",
            "",
            "        code_id = request.GET.get(\"code_id\")",
            "        debug_id = request.GET.get(\"debug_id\")",
            "        query = request.GET.get(\"query\")",
            "        file_formats = request.GET.getlist(\"file_formats\")",
            "",
            "        # If this query contains a debug identifier, normalize it to allow for",
            "        # more lenient queries (e.g. supporting Breakpad ids). Use the index to",
            "        # speed up such queries.",
            "        if query and len(query) <= 45 and not debug_id:",
            "            try:",
            "                debug_id = normalize_debug_id(query.strip())",
            "            except SymbolicError:",
            "                pass",
            "",
            "        if debug_id:",
            "            # If a debug ID is specified, do not consider the stored code",
            "            # identifier and strictly filter by debug identifier. Often there",
            "            # are mismatches in the code identifier in PEs.",
            "            q = Q(debug_id__exact=debug_id)",
            "        elif code_id:",
            "            q = Q(code_id__exact=code_id)",
            "        elif query:",
            "            q = (",
            "                Q(object_name__icontains=query)",
            "                | Q(debug_id__icontains=query)",
            "                | Q(code_id__icontains=query)",
            "                | Q(cpu_name__icontains=query)",
            "                | Q(file__headers__icontains=query)",
            "            )",
            "",
            "            known_file_format = DIF_MIMETYPES.get(query)",
            "            if known_file_format:",
            "                q |= Q(file__headers__icontains=known_file_format)",
            "        else:",
            "            q = Q()",
            "",
            "        file_format_q = Q()",
            "        for file_format in file_formats:",
            "            known_file_format = DIF_MIMETYPES.get(file_format)",
            "            if known_file_format:",
            "                file_format_q |= Q(file__headers__icontains=known_file_format)",
            "",
            "        q &= file_format_q",
            "",
            "        queryset = ProjectDebugFile.objects.filter(q, project_id=project.id).select_related(\"file\")",
            "",
            "        return self.paginate(",
            "            request=request,",
            "            queryset=queryset,",
            "            order_by=\"-id\",",
            "            paginator_cls=OffsetPaginator,",
            "            default_per_page=20,",
            "            on_results=lambda x: serialize(x, request.user),",
            "        )",
            "",
            "    def delete(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        Delete a specific Project's Debug Information File",
            "        ```````````````````````````````````````````````````",
            "",
            "        Delete a debug information file for a given project.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                          file belongs to.",
            "        :pparam string project_slug: the slug of the project to delete the",
            "                                     DIF.",
            "        :qparam string id: The id of the DIF to delete.",
            "        :auth: required",
            "        \"\"\"",
            "",
            "        if request.GET.get(\"id\") and (request.access.has_scope(\"project:write\")):",
            "            with atomic_transaction(using=router.db_for_write(File)):",
            "                debug_file = (",
            "                    ProjectDebugFile.objects.filter(id=request.GET.get(\"id\"), project_id=project.id)",
            "                    .select_related(\"file\")",
            "                    .first()",
            "                )",
            "                if debug_file is not None:",
            "                    debug_file.delete()",
            "                    return Response(status=204)",
            "",
            "        return Response(status=404)",
            "",
            "    def post(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        Upload a New File",
            "        `````````````````",
            "",
            "        Upload a new debug information file for the given release.",
            "",
            "        Unlike other API requests, files must be uploaded using the",
            "        traditional multipart/form-data content-type.",
            "",
            "        The file uploaded is a zip archive of a Apple .dSYM folder which",
            "        contains the individual debug images.  Uploading through this endpoint",
            "        will create different files for the contained images.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                          release belongs to.",
            "        :pparam string project_slug: the slug of the project to change the",
            "                                     release of.",
            "        :param file file: the multipart encoded file.",
            "        :auth: required",
            "        \"\"\"",
            "        return upload_from_request(request, project=project)",
            "",
            "",
            "@region_silo_endpoint",
            "class UnknownDebugFilesEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def get(self, request: Request, project) -> Response:",
            "        checksums = request.GET.getlist(\"checksums\")",
            "        missing = ProjectDebugFile.objects.find_missing(checksums, project=project)",
            "        return Response({\"missing\": missing})",
            "",
            "",
            "@region_silo_endpoint",
            "class AssociateDSymFilesEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    # Legacy endpoint, kept for backwards compatibility",
            "    def post(self, request: Request, project) -> Response:",
            "        return Response({\"associatedDsymFiles\": []})",
            "",
            "",
            "def find_missing_chunks(organization, chunks):",
            "    \"\"\"Returns a list of chunks which are missing for an org.\"\"\"",
            "    owned = set(",
            "        FileBlobOwner.objects.filter(",
            "            blob__checksum__in=chunks, organization_id=organization.id",
            "        ).values_list(\"blob__checksum\", flat=True)",
            "    )",
            "    return list(set(chunks) - owned)",
            "",
            "",
            "@region_silo_endpoint",
            "class DifAssembleEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def post(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        Assemble one or multiple chunks (FileBlob) into debug files",
            "        ````````````````````````````````````````````````````````````",
            "",
            "        :auth: required",
            "        \"\"\"",
            "        schema = {",
            "            \"type\": \"object\",",
            "            \"patternProperties\": {",
            "                \"^[0-9a-f]{40}$\": {",
            "                    \"type\": \"object\",",
            "                    \"required\": [\"name\", \"chunks\"],",
            "                    \"properties\": {",
            "                        \"name\": {\"type\": \"string\"},",
            "                        \"debug_id\": {\"type\": \"string\"},",
            "                        \"chunks\": {",
            "                            \"type\": \"array\",",
            "                            \"items\": {\"type\": \"string\", \"pattern\": \"^[0-9a-f]{40}$\"},",
            "                        },",
            "                    },",
            "                    \"additionalProperties\": True,",
            "                }",
            "            },",
            "            \"additionalProperties\": False,",
            "        }",
            "",
            "        try:",
            "            files = json.loads(request.body)",
            "            jsonschema.validate(files, schema)",
            "        except jsonschema.ValidationError as e:",
            "            return Response({\"error\": str(e).splitlines()[0]}, status=400)",
            "        except Exception:",
            "            return Response({\"error\": \"Invalid json body\"}, status=400)",
            "",
            "        file_response = {}",
            "",
            "        for checksum, file_to_assemble in files.items():",
            "            name = file_to_assemble.get(\"name\", None)",
            "            debug_id = file_to_assemble.get(\"debug_id\", None)",
            "            chunks = file_to_assemble.get(\"chunks\", [])",
            "",
            "            # First, check the cached assemble status. During assembling, a",
            "            # ProjectDebugFile will be created and we need to prevent a race",
            "            # condition.",
            "            state, detail = get_assemble_status(AssembleTask.DIF, project.id, checksum)",
            "            if state == ChunkFileState.OK:",
            "                file_response[checksum] = {",
            "                    \"state\": state,",
            "                    \"detail\": None,",
            "                    \"missingChunks\": [],",
            "                    \"dif\": detail,",
            "                }",
            "                continue",
            "            elif state is not None:",
            "                file_response[checksum] = {\"state\": state, \"detail\": detail, \"missingChunks\": []}",
            "                continue",
            "",
            "            # Next, check if this project already owns the ProjectDebugFile.",
            "            # This can under rare circumstances yield more than one file",
            "            # which is why we use first() here instead of get().",
            "            dif = (",
            "                ProjectDebugFile.objects.filter(project_id=project.id, checksum=checksum)",
            "                .select_related(\"file\")",
            "                .order_by(\"-id\")",
            "                .first()",
            "            )",
            "",
            "            if dif is not None:",
            "                file_response[checksum] = {",
            "                    \"state\": ChunkFileState.OK,",
            "                    \"detail\": None,",
            "                    \"missingChunks\": [],",
            "                    \"dif\": serialize(dif),",
            "                }",
            "                continue",
            "",
            "            # There is neither a known file nor a cached state, so we will",
            "            # have to create a new file.  Assure that there are checksums.",
            "            # If not, we assume this is a poll and report NOT_FOUND",
            "            if not chunks:",
            "                file_response[checksum] = {\"state\": ChunkFileState.NOT_FOUND, \"missingChunks\": []}",
            "                continue",
            "",
            "            # Check if all requested chunks have been uploaded.",
            "            missing_chunks = find_missing_chunks(project.organization, chunks)",
            "            if missing_chunks:",
            "                file_response[checksum] = {",
            "                    \"state\": ChunkFileState.NOT_FOUND,",
            "                    \"missingChunks\": missing_chunks,",
            "                }",
            "                continue",
            "",
            "            # We don't have a state yet, this means we can now start",
            "            # an assemble job in the background.",
            "            set_assemble_status(AssembleTask.DIF, project.id, checksum, ChunkFileState.CREATED)",
            "",
            "            from sentry.tasks.assemble import assemble_dif",
            "",
            "            assemble_dif.apply_async(",
            "                kwargs={",
            "                    \"project_id\": project.id,",
            "                    \"name\": name,",
            "                    \"debug_id\": debug_id,",
            "                    \"checksum\": checksum,",
            "                    \"chunks\": chunks,",
            "                }",
            "            )",
            "",
            "            file_response[checksum] = {\"state\": ChunkFileState.CREATED, \"missingChunks\": []}",
            "",
            "        return Response(file_response, status=200)",
            "",
            "",
            "@region_silo_endpoint",
            "class SourceMapsEndpoint(ProjectEndpoint):",
            "    permission_classes = (ProjectReleasePermission,)",
            "",
            "    def get(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        List a Project's Source Map Archives",
            "        ````````````````````````````````````",
            "",
            "        Retrieve a list of source map archives (releases, later bundles) for a given project.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                          source map archive belongs to.",
            "        :pparam string project_slug: the slug of the project to list the",
            "                                     source map archives of.",
            "        :qparam string query: If set, this parameter is used to locate source map archives with.",
            "        :auth: required",
            "        \"\"\"",
            "        query = request.GET.get(\"query\")",
            "",
            "        try:",
            "            queryset = Release.objects.filter(",
            "                projects=project, organization_id=project.organization_id",
            "            ).values(\"id\", \"version\", \"date_added\")",
            "        except Release.DoesNotExist:",
            "            raise ResourceDoesNotExist",
            "",
            "        if query:",
            "            query_q = Q(version__icontains=query)",
            "",
            "            suffix_match = _release_suffix.match(query)",
            "            if suffix_match is not None:",
            "                query_q |= Q(version__icontains=\"%s+%s\" % suffix_match.groups())",
            "",
            "            queryset = queryset.filter(query_q)",
            "",
            "        def expose_release(release, count):",
            "            return {",
            "                \"type\": \"release\",",
            "                \"id\": release[\"id\"],",
            "                \"name\": release[\"version\"],",
            "                \"date\": release[\"date_added\"],",
            "                \"fileCount\": count,",
            "            }",
            "",
            "        def serialize_results(results):",
            "            file_count_map = get_artifact_counts([r[\"id\"] for r in results])",
            "            # In case we didn't find a file count for a specific release, we will return -1, signaling to the",
            "            # frontend that this release doesn't have one or more ReleaseFile.",
            "            return serialize(",
            "                [expose_release(r, file_count_map.get(r[\"id\"], -1)) for r in results], request.user",
            "            )",
            "",
            "        sort_by = request.GET.get(\"sortBy\", \"-date_added\")",
            "        if sort_by not in {\"-date_added\", \"date_added\"}:",
            "            return Response(",
            "                {\"error\": \"You can either sort via 'date_added' or '-date_added'\"}, status=400",
            "            )",
            "",
            "        return self.paginate(",
            "            request=request,",
            "            queryset=queryset,",
            "            order_by=sort_by,",
            "            paginator_cls=OffsetPaginator,",
            "            default_per_page=10,",
            "            on_results=serialize_results,",
            "        )",
            "",
            "    def delete(self, request: Request, project) -> Response:",
            "        \"\"\"",
            "        Delete an Archive",
            "        ```````````````````````````````````````````````````",
            "",
            "        Delete all artifacts inside given archive.",
            "",
            "        :pparam string organization_slug: the slug of the organization the",
            "                                            archive belongs to.",
            "        :pparam string project_slug: the slug of the project to delete the",
            "                                        archive of.",
            "        :qparam string name: The name of the archive to delete.",
            "        :auth: required",
            "        \"\"\"",
            "",
            "        archive_name = request.GET.get(\"name\")",
            "",
            "        if archive_name:",
            "            with atomic_transaction(using=router.db_for_write(ReleaseFile)):",
            "                release = Release.objects.get(",
            "                    organization_id=project.organization_id, projects=project, version=archive_name",
            "                )",
            "                if release is not None:",
            "                    release_files = ReleaseFile.objects.filter(release_id=release.id)",
            "                    release_files.delete()",
            "                    return Response(status=204)",
            "",
            "        return Response(status=404)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "98": [
                "DebugFilesEndpoint",
                "download"
            ],
            "100": [
                "DebugFilesEndpoint",
                "download"
            ]
        },
        "addLocation": []
    }
}