{
    "changedetectionio/model/Watch.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import re"
            },
            "1": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import time"
            },
            "2": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " import uuid"
            },
            "3": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-SAFE_PROTOCOL_REGEX='^(http|https|ftp):'"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 8,
                "PatchRowcode": "+# Allowable protocols, protects against javascript: etc"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+# file:// is further checked by ALLOW_FILE_URI"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 10,
                "PatchRowcode": "+SAFE_PROTOCOL_REGEX='^(http|https|ftp|file):'"
            },
            "8": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " minimum_seconds_recheck_time = int(os.getenv('MINIMUM_SECONDS_RECHECK_TIME', 60))"
            },
            "10": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " mtable = {'seconds': 1, 'minutes': 60, 'hours': 3600, 'days': 86400, 'weeks': 86400 * 7}"
            },
            "11": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 62,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 63,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " def is_safe_url(test_url):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+    # See https://github.com/dgtlmoon/changedetection.io/issues/1358"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "     # Remove 'source:' prefix so we dont get 'source:javascript:' etc"
            },
            "17": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "     # 'source:' is a valid way to tell us to return the source"
            },
            "18": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 69,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "from distutils.util import strtobool",
            "import logging",
            "import os",
            "import re",
            "import time",
            "import uuid",
            "SAFE_PROTOCOL_REGEX='^(http|https|ftp):'",
            "",
            "minimum_seconds_recheck_time = int(os.getenv('MINIMUM_SECONDS_RECHECK_TIME', 60))",
            "mtable = {'seconds': 1, 'minutes': 60, 'hours': 3600, 'days': 86400, 'weeks': 86400 * 7}",
            "",
            "from changedetectionio.notification import (",
            "    default_notification_format_for_watch",
            ")",
            "",
            "base_config = {",
            "    'body': None,",
            "    'check_unique_lines': False,  # On change-detected, compare against all history if its something new",
            "    'check_count': 0,",
            "    'consecutive_filter_failures': 0,  # Every time the CSS/xPath filter cannot be located, reset when all is fine.",
            "    'extract_text': [],  # Extract text by regex after filters",
            "    'extract_title_as_title': False,",
            "    'fetch_backend': 'system',",
            "    'filter_failure_notification_send': strtobool(os.getenv('FILTER_FAILURE_NOTIFICATION_SEND_DEFAULT', 'True')),",
            "    'has_ldjson_price_data': None,",
            "    'track_ldjson_price_data': None,",
            "    'headers': {},  # Extra headers to send",
            "    'ignore_text': [],  # List of text to ignore when calculating the comparison checksum",
            "    'include_filters': [],",
            "    'last_checked': 0,",
            "    'last_error': False,",
            "    'last_viewed': 0,  # history key value of the last viewed via the [diff] link",
            "    'method': 'GET',",
            "    # Custom notification content",
            "    'notification_body': None,",
            "    'notification_format': default_notification_format_for_watch,",
            "    'notification_muted': False,",
            "    'notification_title': None,",
            "    'notification_screenshot': False,  # Include the latest screenshot if available and supported by the apprise URL",
            "    'notification_urls': [],  # List of URLs to add to the notification Queue (Usually AppRise)",
            "    'paused': False,",
            "    'previous_md5': False,",
            "    'previous_md5_before_filters': False,  # Used for skipping changedetection entirely",
            "    'proxy': None,  # Preferred proxy connection",
            "    'subtractive_selectors': [],",
            "    'tag': None,",
            "    'text_should_not_be_present': [],  # Text that should not present",
            "    # Re #110, so then if this is set to None, we know to use the default value instead",
            "    # Requires setting to None on submit if it's the same as the default",
            "    # Should be all None by default, so we use the system default in this case.",
            "    'time_between_check': {'weeks': None, 'days': None, 'hours': None, 'minutes': None, 'seconds': None},",
            "    'title': None,",
            "    'trigger_text': [],  # List of text or regex to wait for until a change is detected",
            "    'url': '',",
            "    'uuid': str(uuid.uuid4()),",
            "    'webdriver_delay': None,",
            "    'webdriver_js_execute_code': None,  # Run before change-detection",
            "}",
            "",
            "",
            "def is_safe_url(test_url):",
            "    # Remove 'source:' prefix so we dont get 'source:javascript:' etc",
            "    # 'source:' is a valid way to tell us to return the source",
            "",
            "    r = re.compile(re.escape('(source:)'), re.IGNORECASE)",
            "    test_url = r.sub('', test_url)",
            "",
            "    pattern = re.compile(os.getenv('SAFE_PROTOCOL_REGEX', SAFE_PROTOCOL_REGEX), re.IGNORECASE)",
            "    if not pattern.match(test_url.strip()):",
            "        return False",
            "",
            "    return True",
            "",
            "class model(dict):",
            "    __newest_history_key = None",
            "    __history_n = 0",
            "    jitter_seconds = 0",
            "",
            "    def __init__(self, *arg, **kw):",
            "",
            "        self.update(base_config)",
            "        self.__datastore_path = kw['datastore_path']",
            "",
            "        self['uuid'] = str(uuid.uuid4())",
            "",
            "        del kw['datastore_path']",
            "",
            "        if kw.get('default'):",
            "            self.update(kw['default'])",
            "            del kw['default']",
            "",
            "        # Be sure the cached timestamp is ready",
            "        bump = self.history",
            "",
            "        # Goes at the end so we update the default object with the initialiser",
            "        super(model, self).__init__(*arg, **kw)",
            "",
            "    @property",
            "    def viewed(self):",
            "        if int(self['last_viewed']) >= int(self.newest_history_key) :",
            "            return True",
            "",
            "        return False",
            "",
            "    def ensure_data_dir_exists(self):",
            "        if not os.path.isdir(self.watch_data_dir):",
            "            print (\"> Creating data dir {}\".format(self.watch_data_dir))",
            "            os.mkdir(self.watch_data_dir)",
            "",
            "    @property",
            "    def link(self):",
            "",
            "        url = self.get('url', '')",
            "        if not is_safe_url(url):",
            "            return 'DISABLED'",
            "",
            "        ready_url = url",
            "        if '{%' in url or '{{' in url:",
            "            from jinja2 import Environment",
            "            # Jinja2 available in URLs along with https://pypi.org/project/jinja2-time/",
            "            jinja2_env = Environment(extensions=['jinja2_time.TimeExtension'])",
            "            try:",
            "                ready_url = str(jinja2_env.from_string(url).render())",
            "            except Exception as e:",
            "                from flask import (",
            "                    flash, Markup, url_for",
            "                )",
            "                message = Markup('<a href=\"{}#general\">The URL {} is invalid and cannot be used, click to edit</a>'.format(",
            "                    url_for('edit_page', uuid=self.get('uuid')), self.get('url', '')))",
            "                flash(message, 'error')",
            "                return ''",
            "",
            "        return ready_url",
            "",
            "    @property",
            "    def get_fetch_backend(self):",
            "        \"\"\"",
            "        Like just using the `fetch_backend` key but there could be some logic",
            "        :return:",
            "        \"\"\"",
            "        # Maybe also if is_image etc?",
            "        # This is because chrome/playwright wont render the PDF in the browser and we will just fetch it and use pdf2html to see the text.",
            "        if self.is_pdf:",
            "            return 'html_requests'",
            "",
            "        return self.get('fetch_backend')",
            "",
            "    @property",
            "    def is_pdf(self):",
            "        # content_type field is set in the future",
            "        return '.pdf' in self.get('url', '').lower() or 'pdf' in self.get('content_type', '').lower()",
            "",
            "    @property",
            "    def label(self):",
            "        # Used for sorting",
            "        if self['title']:",
            "            return self['title']",
            "        return self['url']",
            "",
            "    @property",
            "    def last_changed(self):",
            "        # last_changed will be the newest snapshot, but when we have just one snapshot, it should be 0",
            "        if self.__history_n <= 1:",
            "            return 0",
            "        if self.__newest_history_key:",
            "            return int(self.__newest_history_key)",
            "        return 0",
            "",
            "    @property",
            "    def history_n(self):",
            "        return self.__history_n",
            "",
            "    @property",
            "    def history(self):",
            "        \"\"\"History index is just a text file as a list",
            "            {watch-uuid}/history.txt",
            "",
            "            contains a list like",
            "",
            "            {epoch-time},{filename}\\n",
            "",
            "            We read in this list as the history information",
            "",
            "        \"\"\"",
            "        tmp_history = {}",
            "",
            "        # Read the history file as a dict",
            "        fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        if os.path.isfile(fname):",
            "            logging.debug(\"Reading history index \" + str(time.time()))",
            "            with open(fname, \"r\") as f:",
            "                for i in f.readlines():",
            "                    if ',' in i:",
            "                        k, v = i.strip().split(',', 2)",
            "",
            "                        # The index history could contain a relative path, so we need to make the fullpath",
            "                        # so that python can read it",
            "                        if not '/' in v and not '\\'' in v:",
            "                            v = os.path.join(self.watch_data_dir, v)",
            "                        else:",
            "                            # It's possible that they moved the datadir on older versions",
            "                            # So the snapshot exists but is in a different path",
            "                            snapshot_fname = v.split('/')[-1]",
            "                            proposed_new_path = os.path.join(self.watch_data_dir, snapshot_fname)",
            "                            if not os.path.exists(v) and os.path.exists(proposed_new_path):",
            "                                v = proposed_new_path",
            "",
            "                        tmp_history[k] = v",
            "",
            "        if len(tmp_history):",
            "            self.__newest_history_key = list(tmp_history.keys())[-1]",
            "",
            "        self.__history_n = len(tmp_history)",
            "",
            "        return tmp_history",
            "",
            "    @property",
            "    def has_history(self):",
            "        fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        return os.path.isfile(fname)",
            "",
            "    # Returns the newest key, but if theres only 1 record, then it's counted as not being new, so return 0.",
            "    @property",
            "    def newest_history_key(self):",
            "        if self.__newest_history_key is not None:",
            "            return self.__newest_history_key",
            "",
            "        if len(self.history) <= 1:",
            "            return 0",
            "",
            "",
            "        bump = self.history",
            "        return self.__newest_history_key",
            "",
            "    # Save some text file to the appropriate path and bump the history",
            "    # result_obj from fetch_site_status.run()",
            "    def save_history_text(self, contents, timestamp):",
            "",
            "        self.ensure_data_dir_exists()",
            "",
            "        # Small hack so that we sleep just enough to allow 1 second  between history snapshots",
            "        # this is because history.txt indexes/keys snapshots by epoch seconds and we dont want dupe keys",
            "        if self.__newest_history_key and int(timestamp) == int(self.__newest_history_key):",
            "            time.sleep(timestamp - self.__newest_history_key)",
            "",
            "        snapshot_fname = \"{}.txt\".format(str(uuid.uuid4()))",
            "",
            "        # in /diff/ and /preview/ we are going to assume for now that it's UTF-8 when reading",
            "        # most sites are utf-8 and some are even broken utf-8",
            "        with open(os.path.join(self.watch_data_dir, snapshot_fname), 'wb') as f:",
            "            f.write(contents)",
            "            f.close()",
            "",
            "        # Append to index",
            "        # @todo check last char was \\n",
            "        index_fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        with open(index_fname, 'a') as f:",
            "            f.write(\"{},{}\\n\".format(timestamp, snapshot_fname))",
            "            f.close()",
            "",
            "        self.__newest_history_key = timestamp",
            "        self.__history_n += 1",
            "",
            "        # @todo bump static cache of the last timestamp so we dont need to examine the file to set a proper ''viewed'' status",
            "        return snapshot_fname",
            "",
            "    @property",
            "    def has_empty_checktime(self):",
            "        # using all() + dictionary comprehension",
            "        # Check if all values are 0 in dictionary",
            "        res = all(x == None or x == False or x==0 for x in self.get('time_between_check', {}).values())",
            "        return res",
            "",
            "    def threshold_seconds(self):",
            "        seconds = 0",
            "        for m, n in mtable.items():",
            "            x = self.get('time_between_check', {}).get(m, None)",
            "            if x:",
            "                seconds += x * n",
            "        return seconds",
            "",
            "    # Iterate over all history texts and see if something new exists",
            "    def lines_contain_something_unique_compared_to_history(self, lines: list):",
            "        local_lines = set([l.decode('utf-8').strip().lower() for l in lines])",
            "",
            "        # Compare each lines (set) against each history text file (set) looking for something new..",
            "        existing_history = set({})",
            "        for k, v in self.history.items():",
            "            alist = set([line.decode('utf-8').strip().lower() for line in open(v, 'rb')])",
            "            existing_history = existing_history.union(alist)",
            "",
            "        # Check that everything in local_lines(new stuff) already exists in existing_history - it should",
            "        # if not, something new happened",
            "        return not local_lines.issubset(existing_history)",
            "",
            "    def get_screenshot(self):",
            "        fname = os.path.join(self.watch_data_dir, \"last-screenshot.png\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "",
            "        # False is not an option for AppRise, must be type None",
            "        return None",
            "",
            "    def get_screenshot_as_jpeg(self):",
            "",
            "        # Created by save_screenshot()",
            "        fname = os.path.join(self.watch_data_dir, \"last-screenshot.jpg\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "",
            "        # False is not an option for AppRise, must be type None",
            "        return None",
            "",
            "",
            "    def __get_file_ctime(self, filename):",
            "        fname = os.path.join(self.watch_data_dir, filename)",
            "        if os.path.isfile(fname):",
            "            return int(os.path.getmtime(fname))",
            "        return False",
            "",
            "    @property",
            "    def error_text_ctime(self):",
            "        return self.__get_file_ctime('last-error.txt')",
            "",
            "    @property",
            "    def snapshot_text_ctime(self):",
            "        if self.history_n==0:",
            "            return False",
            "",
            "        timestamp = list(self.history.keys())[-1]",
            "        return int(timestamp)",
            "",
            "    @property",
            "    def snapshot_screenshot_ctime(self):",
            "        return self.__get_file_ctime('last-screenshot.png')",
            "",
            "    @property",
            "    def snapshot_error_screenshot_ctime(self):",
            "        return self.__get_file_ctime('last-error-screenshot.png')",
            "",
            "    @property",
            "    def watch_data_dir(self):",
            "        # The base dir of the watch data",
            "        return os.path.join(self.__datastore_path, self['uuid'])",
            "    ",
            "    def get_error_text(self):",
            "        \"\"\"Return the text saved from a previous request that resulted in a non-200 error\"\"\"",
            "        fname = os.path.join(self.watch_data_dir, \"last-error.txt\")",
            "        if os.path.isfile(fname):",
            "            with open(fname, 'r') as f:",
            "                return f.read()",
            "        return False",
            "",
            "    def get_error_snapshot(self):",
            "        \"\"\"Return path to the screenshot that resulted in a non-200 error\"\"\"",
            "        fname = os.path.join(self.watch_data_dir, \"last-error-screenshot.png\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "        return False",
            "",
            "    def pause(self):",
            "        self['paused'] = True",
            "",
            "    def unpause(self):",
            "        self['paused'] = False",
            "",
            "    def toggle_pause(self):",
            "        self['paused'] ^= True",
            "",
            "    def mute(self):",
            "        self['notification_muted'] = True",
            "",
            "    def unmute(self):",
            "        self['notification_muted'] = False",
            "",
            "    def toggle_mute(self):",
            "        self['notification_muted'] ^= True",
            "",
            "    def extract_regex_from_all_history(self, regex):",
            "        import csv",
            "        import re",
            "        import datetime",
            "        csv_output_filename = False",
            "        csv_writer = False",
            "        f = None",
            "",
            "        # self.history will be keyed with the full path",
            "        for k, fname in self.history.items():",
            "            if os.path.isfile(fname):",
            "                with open(fname, \"r\") as f:",
            "                    contents = f.read()",
            "                    res = re.findall(regex, contents, re.MULTILINE)",
            "                    if res:",
            "                        if not csv_writer:",
            "                            # A file on the disk can be transferred much faster via flask than a string reply",
            "                            csv_output_filename = 'report.csv'",
            "                            f = open(os.path.join(self.watch_data_dir, csv_output_filename), 'w')",
            "                            # @todo some headers in the future",
            "                            #fieldnames = ['Epoch seconds', 'Date']",
            "                            csv_writer = csv.writer(f,",
            "                                                    delimiter=',',",
            "                                                    quotechar='\"',",
            "                                                    quoting=csv.QUOTE_MINIMAL,",
            "                                                    #fieldnames=fieldnames",
            "                                                    )",
            "                            csv_writer.writerow(['Epoch seconds', 'Date'])",
            "                            # csv_writer.writeheader()",
            "",
            "                        date_str = datetime.datetime.fromtimestamp(int(k)).strftime('%Y-%m-%d %H:%M:%S')",
            "                        for r in res:",
            "                            row = [k, date_str]",
            "                            if isinstance(r, str):",
            "                                row.append(r)",
            "                            else:",
            "                                row+=r",
            "                            csv_writer.writerow(row)",
            "",
            "        if f:",
            "            f.close()",
            "",
            "        return csv_output_filename",
            "",
            "    @property",
            "    # Return list of tags, stripped and lowercase, used for searching",
            "    def all_tags(self):",
            "        return [s.strip().lower() for s in self.get('tag','').split(',')]"
        ],
        "afterPatchFile": [
            "from distutils.util import strtobool",
            "import logging",
            "import os",
            "import re",
            "import time",
            "import uuid",
            "",
            "# Allowable protocols, protects against javascript: etc",
            "# file:// is further checked by ALLOW_FILE_URI",
            "SAFE_PROTOCOL_REGEX='^(http|https|ftp|file):'",
            "",
            "minimum_seconds_recheck_time = int(os.getenv('MINIMUM_SECONDS_RECHECK_TIME', 60))",
            "mtable = {'seconds': 1, 'minutes': 60, 'hours': 3600, 'days': 86400, 'weeks': 86400 * 7}",
            "",
            "from changedetectionio.notification import (",
            "    default_notification_format_for_watch",
            ")",
            "",
            "base_config = {",
            "    'body': None,",
            "    'check_unique_lines': False,  # On change-detected, compare against all history if its something new",
            "    'check_count': 0,",
            "    'consecutive_filter_failures': 0,  # Every time the CSS/xPath filter cannot be located, reset when all is fine.",
            "    'extract_text': [],  # Extract text by regex after filters",
            "    'extract_title_as_title': False,",
            "    'fetch_backend': 'system',",
            "    'filter_failure_notification_send': strtobool(os.getenv('FILTER_FAILURE_NOTIFICATION_SEND_DEFAULT', 'True')),",
            "    'has_ldjson_price_data': None,",
            "    'track_ldjson_price_data': None,",
            "    'headers': {},  # Extra headers to send",
            "    'ignore_text': [],  # List of text to ignore when calculating the comparison checksum",
            "    'include_filters': [],",
            "    'last_checked': 0,",
            "    'last_error': False,",
            "    'last_viewed': 0,  # history key value of the last viewed via the [diff] link",
            "    'method': 'GET',",
            "    # Custom notification content",
            "    'notification_body': None,",
            "    'notification_format': default_notification_format_for_watch,",
            "    'notification_muted': False,",
            "    'notification_title': None,",
            "    'notification_screenshot': False,  # Include the latest screenshot if available and supported by the apprise URL",
            "    'notification_urls': [],  # List of URLs to add to the notification Queue (Usually AppRise)",
            "    'paused': False,",
            "    'previous_md5': False,",
            "    'previous_md5_before_filters': False,  # Used for skipping changedetection entirely",
            "    'proxy': None,  # Preferred proxy connection",
            "    'subtractive_selectors': [],",
            "    'tag': None,",
            "    'text_should_not_be_present': [],  # Text that should not present",
            "    # Re #110, so then if this is set to None, we know to use the default value instead",
            "    # Requires setting to None on submit if it's the same as the default",
            "    # Should be all None by default, so we use the system default in this case.",
            "    'time_between_check': {'weeks': None, 'days': None, 'hours': None, 'minutes': None, 'seconds': None},",
            "    'title': None,",
            "    'trigger_text': [],  # List of text or regex to wait for until a change is detected",
            "    'url': '',",
            "    'uuid': str(uuid.uuid4()),",
            "    'webdriver_delay': None,",
            "    'webdriver_js_execute_code': None,  # Run before change-detection",
            "}",
            "",
            "",
            "def is_safe_url(test_url):",
            "    # See https://github.com/dgtlmoon/changedetection.io/issues/1358",
            "",
            "    # Remove 'source:' prefix so we dont get 'source:javascript:' etc",
            "    # 'source:' is a valid way to tell us to return the source",
            "",
            "    r = re.compile(re.escape('(source:)'), re.IGNORECASE)",
            "    test_url = r.sub('', test_url)",
            "",
            "    pattern = re.compile(os.getenv('SAFE_PROTOCOL_REGEX', SAFE_PROTOCOL_REGEX), re.IGNORECASE)",
            "    if not pattern.match(test_url.strip()):",
            "        return False",
            "",
            "    return True",
            "",
            "class model(dict):",
            "    __newest_history_key = None",
            "    __history_n = 0",
            "    jitter_seconds = 0",
            "",
            "    def __init__(self, *arg, **kw):",
            "",
            "        self.update(base_config)",
            "        self.__datastore_path = kw['datastore_path']",
            "",
            "        self['uuid'] = str(uuid.uuid4())",
            "",
            "        del kw['datastore_path']",
            "",
            "        if kw.get('default'):",
            "            self.update(kw['default'])",
            "            del kw['default']",
            "",
            "        # Be sure the cached timestamp is ready",
            "        bump = self.history",
            "",
            "        # Goes at the end so we update the default object with the initialiser",
            "        super(model, self).__init__(*arg, **kw)",
            "",
            "    @property",
            "    def viewed(self):",
            "        if int(self['last_viewed']) >= int(self.newest_history_key) :",
            "            return True",
            "",
            "        return False",
            "",
            "    def ensure_data_dir_exists(self):",
            "        if not os.path.isdir(self.watch_data_dir):",
            "            print (\"> Creating data dir {}\".format(self.watch_data_dir))",
            "            os.mkdir(self.watch_data_dir)",
            "",
            "    @property",
            "    def link(self):",
            "",
            "        url = self.get('url', '')",
            "        if not is_safe_url(url):",
            "            return 'DISABLED'",
            "",
            "        ready_url = url",
            "        if '{%' in url or '{{' in url:",
            "            from jinja2 import Environment",
            "            # Jinja2 available in URLs along with https://pypi.org/project/jinja2-time/",
            "            jinja2_env = Environment(extensions=['jinja2_time.TimeExtension'])",
            "            try:",
            "                ready_url = str(jinja2_env.from_string(url).render())",
            "            except Exception as e:",
            "                from flask import (",
            "                    flash, Markup, url_for",
            "                )",
            "                message = Markup('<a href=\"{}#general\">The URL {} is invalid and cannot be used, click to edit</a>'.format(",
            "                    url_for('edit_page', uuid=self.get('uuid')), self.get('url', '')))",
            "                flash(message, 'error')",
            "                return ''",
            "",
            "        return ready_url",
            "",
            "    @property",
            "    def get_fetch_backend(self):",
            "        \"\"\"",
            "        Like just using the `fetch_backend` key but there could be some logic",
            "        :return:",
            "        \"\"\"",
            "        # Maybe also if is_image etc?",
            "        # This is because chrome/playwright wont render the PDF in the browser and we will just fetch it and use pdf2html to see the text.",
            "        if self.is_pdf:",
            "            return 'html_requests'",
            "",
            "        return self.get('fetch_backend')",
            "",
            "    @property",
            "    def is_pdf(self):",
            "        # content_type field is set in the future",
            "        return '.pdf' in self.get('url', '').lower() or 'pdf' in self.get('content_type', '').lower()",
            "",
            "    @property",
            "    def label(self):",
            "        # Used for sorting",
            "        if self['title']:",
            "            return self['title']",
            "        return self['url']",
            "",
            "    @property",
            "    def last_changed(self):",
            "        # last_changed will be the newest snapshot, but when we have just one snapshot, it should be 0",
            "        if self.__history_n <= 1:",
            "            return 0",
            "        if self.__newest_history_key:",
            "            return int(self.__newest_history_key)",
            "        return 0",
            "",
            "    @property",
            "    def history_n(self):",
            "        return self.__history_n",
            "",
            "    @property",
            "    def history(self):",
            "        \"\"\"History index is just a text file as a list",
            "            {watch-uuid}/history.txt",
            "",
            "            contains a list like",
            "",
            "            {epoch-time},{filename}\\n",
            "",
            "            We read in this list as the history information",
            "",
            "        \"\"\"",
            "        tmp_history = {}",
            "",
            "        # Read the history file as a dict",
            "        fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        if os.path.isfile(fname):",
            "            logging.debug(\"Reading history index \" + str(time.time()))",
            "            with open(fname, \"r\") as f:",
            "                for i in f.readlines():",
            "                    if ',' in i:",
            "                        k, v = i.strip().split(',', 2)",
            "",
            "                        # The index history could contain a relative path, so we need to make the fullpath",
            "                        # so that python can read it",
            "                        if not '/' in v and not '\\'' in v:",
            "                            v = os.path.join(self.watch_data_dir, v)",
            "                        else:",
            "                            # It's possible that they moved the datadir on older versions",
            "                            # So the snapshot exists but is in a different path",
            "                            snapshot_fname = v.split('/')[-1]",
            "                            proposed_new_path = os.path.join(self.watch_data_dir, snapshot_fname)",
            "                            if not os.path.exists(v) and os.path.exists(proposed_new_path):",
            "                                v = proposed_new_path",
            "",
            "                        tmp_history[k] = v",
            "",
            "        if len(tmp_history):",
            "            self.__newest_history_key = list(tmp_history.keys())[-1]",
            "",
            "        self.__history_n = len(tmp_history)",
            "",
            "        return tmp_history",
            "",
            "    @property",
            "    def has_history(self):",
            "        fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        return os.path.isfile(fname)",
            "",
            "    # Returns the newest key, but if theres only 1 record, then it's counted as not being new, so return 0.",
            "    @property",
            "    def newest_history_key(self):",
            "        if self.__newest_history_key is not None:",
            "            return self.__newest_history_key",
            "",
            "        if len(self.history) <= 1:",
            "            return 0",
            "",
            "",
            "        bump = self.history",
            "        return self.__newest_history_key",
            "",
            "    # Save some text file to the appropriate path and bump the history",
            "    # result_obj from fetch_site_status.run()",
            "    def save_history_text(self, contents, timestamp):",
            "",
            "        self.ensure_data_dir_exists()",
            "",
            "        # Small hack so that we sleep just enough to allow 1 second  between history snapshots",
            "        # this is because history.txt indexes/keys snapshots by epoch seconds and we dont want dupe keys",
            "        if self.__newest_history_key and int(timestamp) == int(self.__newest_history_key):",
            "            time.sleep(timestamp - self.__newest_history_key)",
            "",
            "        snapshot_fname = \"{}.txt\".format(str(uuid.uuid4()))",
            "",
            "        # in /diff/ and /preview/ we are going to assume for now that it's UTF-8 when reading",
            "        # most sites are utf-8 and some are even broken utf-8",
            "        with open(os.path.join(self.watch_data_dir, snapshot_fname), 'wb') as f:",
            "            f.write(contents)",
            "            f.close()",
            "",
            "        # Append to index",
            "        # @todo check last char was \\n",
            "        index_fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        with open(index_fname, 'a') as f:",
            "            f.write(\"{},{}\\n\".format(timestamp, snapshot_fname))",
            "            f.close()",
            "",
            "        self.__newest_history_key = timestamp",
            "        self.__history_n += 1",
            "",
            "        # @todo bump static cache of the last timestamp so we dont need to examine the file to set a proper ''viewed'' status",
            "        return snapshot_fname",
            "",
            "    @property",
            "    def has_empty_checktime(self):",
            "        # using all() + dictionary comprehension",
            "        # Check if all values are 0 in dictionary",
            "        res = all(x == None or x == False or x==0 for x in self.get('time_between_check', {}).values())",
            "        return res",
            "",
            "    def threshold_seconds(self):",
            "        seconds = 0",
            "        for m, n in mtable.items():",
            "            x = self.get('time_between_check', {}).get(m, None)",
            "            if x:",
            "                seconds += x * n",
            "        return seconds",
            "",
            "    # Iterate over all history texts and see if something new exists",
            "    def lines_contain_something_unique_compared_to_history(self, lines: list):",
            "        local_lines = set([l.decode('utf-8').strip().lower() for l in lines])",
            "",
            "        # Compare each lines (set) against each history text file (set) looking for something new..",
            "        existing_history = set({})",
            "        for k, v in self.history.items():",
            "            alist = set([line.decode('utf-8').strip().lower() for line in open(v, 'rb')])",
            "            existing_history = existing_history.union(alist)",
            "",
            "        # Check that everything in local_lines(new stuff) already exists in existing_history - it should",
            "        # if not, something new happened",
            "        return not local_lines.issubset(existing_history)",
            "",
            "    def get_screenshot(self):",
            "        fname = os.path.join(self.watch_data_dir, \"last-screenshot.png\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "",
            "        # False is not an option for AppRise, must be type None",
            "        return None",
            "",
            "    def get_screenshot_as_jpeg(self):",
            "",
            "        # Created by save_screenshot()",
            "        fname = os.path.join(self.watch_data_dir, \"last-screenshot.jpg\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "",
            "        # False is not an option for AppRise, must be type None",
            "        return None",
            "",
            "",
            "    def __get_file_ctime(self, filename):",
            "        fname = os.path.join(self.watch_data_dir, filename)",
            "        if os.path.isfile(fname):",
            "            return int(os.path.getmtime(fname))",
            "        return False",
            "",
            "    @property",
            "    def error_text_ctime(self):",
            "        return self.__get_file_ctime('last-error.txt')",
            "",
            "    @property",
            "    def snapshot_text_ctime(self):",
            "        if self.history_n==0:",
            "            return False",
            "",
            "        timestamp = list(self.history.keys())[-1]",
            "        return int(timestamp)",
            "",
            "    @property",
            "    def snapshot_screenshot_ctime(self):",
            "        return self.__get_file_ctime('last-screenshot.png')",
            "",
            "    @property",
            "    def snapshot_error_screenshot_ctime(self):",
            "        return self.__get_file_ctime('last-error-screenshot.png')",
            "",
            "    @property",
            "    def watch_data_dir(self):",
            "        # The base dir of the watch data",
            "        return os.path.join(self.__datastore_path, self['uuid'])",
            "    ",
            "    def get_error_text(self):",
            "        \"\"\"Return the text saved from a previous request that resulted in a non-200 error\"\"\"",
            "        fname = os.path.join(self.watch_data_dir, \"last-error.txt\")",
            "        if os.path.isfile(fname):",
            "            with open(fname, 'r') as f:",
            "                return f.read()",
            "        return False",
            "",
            "    def get_error_snapshot(self):",
            "        \"\"\"Return path to the screenshot that resulted in a non-200 error\"\"\"",
            "        fname = os.path.join(self.watch_data_dir, \"last-error-screenshot.png\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "        return False",
            "",
            "    def pause(self):",
            "        self['paused'] = True",
            "",
            "    def unpause(self):",
            "        self['paused'] = False",
            "",
            "    def toggle_pause(self):",
            "        self['paused'] ^= True",
            "",
            "    def mute(self):",
            "        self['notification_muted'] = True",
            "",
            "    def unmute(self):",
            "        self['notification_muted'] = False",
            "",
            "    def toggle_mute(self):",
            "        self['notification_muted'] ^= True",
            "",
            "    def extract_regex_from_all_history(self, regex):",
            "        import csv",
            "        import re",
            "        import datetime",
            "        csv_output_filename = False",
            "        csv_writer = False",
            "        f = None",
            "",
            "        # self.history will be keyed with the full path",
            "        for k, fname in self.history.items():",
            "            if os.path.isfile(fname):",
            "                with open(fname, \"r\") as f:",
            "                    contents = f.read()",
            "                    res = re.findall(regex, contents, re.MULTILINE)",
            "                    if res:",
            "                        if not csv_writer:",
            "                            # A file on the disk can be transferred much faster via flask than a string reply",
            "                            csv_output_filename = 'report.csv'",
            "                            f = open(os.path.join(self.watch_data_dir, csv_output_filename), 'w')",
            "                            # @todo some headers in the future",
            "                            #fieldnames = ['Epoch seconds', 'Date']",
            "                            csv_writer = csv.writer(f,",
            "                                                    delimiter=',',",
            "                                                    quotechar='\"',",
            "                                                    quoting=csv.QUOTE_MINIMAL,",
            "                                                    #fieldnames=fieldnames",
            "                                                    )",
            "                            csv_writer.writerow(['Epoch seconds', 'Date'])",
            "                            # csv_writer.writeheader()",
            "",
            "                        date_str = datetime.datetime.fromtimestamp(int(k)).strftime('%Y-%m-%d %H:%M:%S')",
            "                        for r in res:",
            "                            row = [k, date_str]",
            "                            if isinstance(r, str):",
            "                                row.append(r)",
            "                            else:",
            "                                row+=r",
            "                            csv_writer.writerow(row)",
            "",
            "        if f:",
            "            f.close()",
            "",
            "        return csv_output_filename",
            "",
            "    @property",
            "    # Return list of tags, stripped and lowercase, used for searching",
            "    def all_tags(self):",
            "        return [s.strip().lower() for s in self.get('tag','').split(',')]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "7": [
                "SAFE_PROTOCOL_REGEX"
            ]
        },
        "addLocation": []
    },
    "changedetectionio/tests/test_security.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": "     assert b\"1 Imported\" in res.data"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # Attempt to add a body with a GET method"
            },
            "4": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    res = client.post("
            },
            "5": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        url_for(\"edit_page\", uuid=\"first\"),"
            },
            "6": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        data={"
            },
            "7": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-              \"url\": 'file:///etc/passwd',"
            },
            "8": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-              \"tag\": \"\","
            },
            "9": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-              \"method\": \"GET\","
            },
            "10": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-              \"fetch_backend\": \"html_requests\","
            },
            "11": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-              \"body\": \"\"},"
            },
            "12": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        follow_redirects=True"
            },
            "13": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    )"
            },
            "14": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "15": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data"
            },
            "16": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "17": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "18": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 16,
                "PatchRowcode": "     # Attempt to add a body with a GET method"
            },
            "19": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 17,
                "PatchRowcode": "     res = client.post("
            },
            "20": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 18,
                "PatchRowcode": "         url_for(\"edit_page\", uuid=\"first\"),"
            },
            "21": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "     res = client.post("
            },
            "23": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "         url_for(\"form_quick_watch_add\"),"
            },
            "24": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        data={\"url\": 'file:///tasty/disk/drive', \"tag\": ''},"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+        data={\"url\": '%20%20%20javascript:alert(123)%20%20', \"tag\": ''},"
            },
            "26": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "         follow_redirects=True"
            },
            "27": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "     )"
            },
            "28": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 43,
                "PatchRowcode": " "
            },
            "29": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "     assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data"
            },
            "30": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " "
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+"
            },
            "32": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "     res = client.post("
            },
            "33": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "         url_for(\"form_quick_watch_add\"),"
            },
            "34": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        data={\"url\": '%20%20%20javascript:alert(123)%20%20', \"tag\": ''},"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+        data={\"url\": ' source:javascript:alert(document.domain)', \"tag\": ''},"
            },
            "36": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "         follow_redirects=True"
            },
            "37": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "     )"
            },
            "38": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 52,
                "PatchRowcode": " "
            },
            "39": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "     assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data"
            },
            "40": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " "
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+    # file:// is permitted by default, but it will be caught by ALLOW_FILE_URI"
            },
            "42": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 56,
                "PatchRowcode": " "
            },
            "43": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    res = client.post("
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+    client.post("
            },
            "45": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "         url_for(\"form_quick_watch_add\"),"
            },
            "46": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        data={\"url\": ' source:javascript:alert(document.domain)', \"tag\": ''},"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+        data={\"url\": 'file:///tasty/disk/drive', \"tag\": ''},"
            },
            "48": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "         follow_redirects=True"
            },
            "49": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "     )"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+    time.sleep(1)"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+    res = client.get(url_for(\"index\"))"
            },
            "52": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " "
            },
            "53": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+    assert b'file:// type access is denied for security reasons.' in res.data"
            }
        },
        "frontPatchFile": [
            "from flask import url_for",
            "from . util import set_original_response, set_modified_response, live_server_setup",
            "import time",
            "",
            "",
            "def test_bad_access(client, live_server):",
            "    live_server_setup(live_server)",
            "    res = client.post(",
            "        url_for(\"import_page\"),",
            "        data={\"urls\": 'https://localhost'},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b\"1 Imported\" in res.data",
            "",
            "    # Attempt to add a body with a GET method",
            "    res = client.post(",
            "        url_for(\"edit_page\", uuid=\"first\"),",
            "        data={",
            "              \"url\": 'file:///etc/passwd',",
            "              \"tag\": \"\",",
            "              \"method\": \"GET\",",
            "              \"fetch_backend\": \"html_requests\",",
            "              \"body\": \"\"},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "",
            "    # Attempt to add a body with a GET method",
            "    res = client.post(",
            "        url_for(\"edit_page\", uuid=\"first\"),",
            "        data={",
            "              \"url\": 'javascript:alert(document.domain)',",
            "              \"tag\": \"\",",
            "              \"method\": \"GET\",",
            "              \"fetch_backend\": \"html_requests\",",
            "              \"body\": \"\"},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "    res = client.post(",
            "        url_for(\"form_quick_watch_add\"),",
            "        data={\"url\": '            javascript:alert(123)', \"tag\": ''},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "    res = client.post(",
            "        url_for(\"form_quick_watch_add\"),",
            "        data={\"url\": 'file:///tasty/disk/drive', \"tag\": ''},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "    res = client.post(",
            "        url_for(\"form_quick_watch_add\"),",
            "        data={\"url\": '%20%20%20javascript:alert(123)%20%20', \"tag\": ''},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "",
            "    res = client.post(",
            "        url_for(\"form_quick_watch_add\"),",
            "        data={\"url\": ' source:javascript:alert(document.domain)', \"tag\": ''},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data"
        ],
        "afterPatchFile": [
            "from flask import url_for",
            "from . util import set_original_response, set_modified_response, live_server_setup",
            "import time",
            "",
            "",
            "def test_bad_access(client, live_server):",
            "    live_server_setup(live_server)",
            "    res = client.post(",
            "        url_for(\"import_page\"),",
            "        data={\"urls\": 'https://localhost'},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b\"1 Imported\" in res.data",
            "",
            "    # Attempt to add a body with a GET method",
            "    res = client.post(",
            "        url_for(\"edit_page\", uuid=\"first\"),",
            "        data={",
            "              \"url\": 'javascript:alert(document.domain)',",
            "              \"tag\": \"\",",
            "              \"method\": \"GET\",",
            "              \"fetch_backend\": \"html_requests\",",
            "              \"body\": \"\"},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "    res = client.post(",
            "        url_for(\"form_quick_watch_add\"),",
            "        data={\"url\": '            javascript:alert(123)', \"tag\": ''},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "    res = client.post(",
            "        url_for(\"form_quick_watch_add\"),",
            "        data={\"url\": '%20%20%20javascript:alert(123)%20%20', \"tag\": ''},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "",
            "    res = client.post(",
            "        url_for(\"form_quick_watch_add\"),",
            "        data={\"url\": ' source:javascript:alert(document.domain)', \"tag\": ''},",
            "        follow_redirects=True",
            "    )",
            "",
            "    assert b'Watch protocol is not permitted by SAFE_PROTOCOL_REGEX' in res.data",
            "",
            "    # file:// is permitted by default, but it will be caught by ALLOW_FILE_URI",
            "",
            "    client.post(",
            "        url_for(\"form_quick_watch_add\"),",
            "        data={\"url\": 'file:///tasty/disk/drive', \"tag\": ''},",
            "        follow_redirects=True",
            "    )",
            "    time.sleep(1)",
            "    res = client.get(url_for(\"index\"))",
            "",
            "    assert b'file:// type access is denied for security reasons.' in res.data"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "16": [
                "test_bad_access"
            ],
            "17": [
                "test_bad_access"
            ],
            "18": [
                "test_bad_access"
            ],
            "19": [
                "test_bad_access"
            ],
            "20": [
                "test_bad_access"
            ],
            "21": [
                "test_bad_access"
            ],
            "22": [
                "test_bad_access"
            ],
            "23": [
                "test_bad_access"
            ],
            "24": [
                "test_bad_access"
            ],
            "25": [
                "test_bad_access"
            ],
            "26": [
                "test_bad_access"
            ],
            "27": [
                "test_bad_access"
            ],
            "28": [
                "test_bad_access"
            ],
            "29": [
                "test_bad_access"
            ],
            "30": [
                "test_bad_access"
            ],
            "55": [
                "test_bad_access"
            ],
            "63": [
                "test_bad_access"
            ],
            "70": [
                "test_bad_access"
            ],
            "72": [
                "test_bad_access"
            ],
            "76": [
                "test_bad_access"
            ]
        },
        "addLocation": []
    }
}