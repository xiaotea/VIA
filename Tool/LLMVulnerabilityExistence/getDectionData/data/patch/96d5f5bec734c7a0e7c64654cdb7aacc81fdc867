{
    "build_package.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 254,
                "afterPatchRowNumber": 254,
                "PatchRowcode": "         r\".*\\.pyc\", r\".*\\.pyo\", r\".*\\.pyd\", r\"all_tests\\.py\", r\".*~\""
            },
            "1": {
                "beforePatchRowNumber": 255,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "     )"
            },
            "2": {
                "beforePatchRowNumber": 256,
                "afterPatchRowNumber": 256,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 257,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # exclude all the third_party_tls libs under windows"
            },
            "4": {
                "beforePatchRowNumber": 258,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # because windows python has tls built in."
            },
            "5": {
                "beforePatchRowNumber": 259,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    recursively_delete_dirs_by_name(\"third_party_tls\")"
            },
            "6": {
                "beforePatchRowNumber": 260,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "7": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 257,
                "PatchRowcode": "     # Move back up to the root directory and populate the data_files."
            },
            "8": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 258,
                "PatchRowcode": "     os.chdir(\"..\")"
            },
            "9": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 259,
                "PatchRowcode": "     os.chdir(\"data_files\")"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Copyright 2014 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ------------------------------------------------------------------------",
            "#",
            "# Script used to build the RPM, Debian, and tarball packages for releasing Scalyr Agent 2.",
            "#",
            "# To execute this script, you must have installed fpm: https://github.com/jordansissel/fpm",
            "#",
            "# Usage: python build_package.py [options] rpm|tarball|deb",
            "#",
            "# author: Steven Czerwinski <czerwin@scalyr.com>",
            "",
            "from __future__ import absolute_import",
            "from __future__ import print_function",
            "from __future__ import unicode_literals",
            "",
            "__author__ = \"czerwin@scalyr.com\"",
            "",
            "import errno",
            "import glob",
            "import os",
            "import re",
            "import shutil",
            "import stat",
            "import subprocess",
            "import sys",
            "import tarfile",
            "import tempfile",
            "import time",
            "import uuid",
            "",
            "from io import StringIO",
            "from io import BytesIO",
            "from io import open",
            "",
            "from optparse import OptionParser",
            "from time import gmtime, strftime",
            "",
            "from scalyr_agent.__scalyr__ import get_install_root, SCALYR_VERSION, scalyr_init",
            "",
            "scalyr_init()",
            "",
            "import scalyr_agent.util as scalyr_util",
            "",
            "# [start of 2->TODO]",
            "# Check for suitability.",
            "# Important. Import six as any other dependency from \"third_party\" libraries after \"__scalyr__.scalyr_init\"",
            "import six",
            "from six.moves import range",
            "",
            "# [end of 2->TOD0]",
            "",
            "# The root of the Scalyr repository should just be the parent of this file.",
            "__source_root__ = get_install_root()",
            "",
            "# All the different packages that this script can build.",
            "PACKAGE_TYPES = [",
            "    \"rpm\",",
            "    \"tarball\",",
            "    \"deb\",",
            "    \"win32\",",
            "    \"docker_syslog_builder\",",
            "    \"docker_json_builder\",",
            "    \"k8s_builder\",",
            "]",
            "",
            "",
            "def build_package(package_type, variant, no_versioned_file_name, coverage_enabled):",
            "    \"\"\"Builds the scalyr-agent-2 package specified by the arguments.",
            "",
            "    The package is left in the current working directory.  The file name of the",
            "    package is returned by this function.",
            "",
            "    @param package_type: One of `PACKAGE_TYPES`. Determines which package type is built.",
            "    @param variant: Adds the specified string into the package's iteration name. This may be None if no additional",
            "        tweak to the name is required. This is used to produce different packages even for the same package type (such",
            "        as 'rpm').",
            "    @param no_versioned_file_name:  If True, will not embed a version number in the resulting artifact's file name.",
            "        This only has an affect if building one of the tarball formats.",
            "    @param coverage_enabled: If True, enables coverage analysis. Patches Dockerfile to run agent with coverage.",
            "",
            "    @return: The file name of the produced package.",
            "    \"\"\"",
            "    original_cwd = os.getcwd()",
            "",
            "    version = SCALYR_VERSION",
            "",
            "    # Create a temporary directory to build the package in.",
            "    tmp_dir = tempfile.mkdtemp(prefix=\"build-scalyr-agent-packages\")",
            "",
            "    try:",
            "        # Change to that directory and delegate to another method for the specific type.",
            "        os.chdir(tmp_dir)",
            "        if package_type == \"tarball\":",
            "            artifact_file_name = build_tarball_package(",
            "                variant, version, no_versioned_file_name",
            "            )",
            "        elif package_type == \"win32\":",
            "            artifact_file_name = build_win32_installer_package(variant, version)",
            "        elif package_type == \"docker_syslog_builder\":",
            "            # An image for running on Docker configured to receive logs from other containers via syslog.",
            "            # This is the deprecated approach (but is still published under scalyr/scalyr-docker-agent for",
            "            # backward compatibility.)  We also publish this under scalyr/scalyr-docker-agent-syslog to help",
            "            # with the eventual migration.",
            "            artifact_file_name = build_container_builder(",
            "                variant,",
            "                version,",
            "                no_versioned_file_name,",
            "                \"scalyr-docker-agent.tar.gz\",",
            "                \"docker/Dockerfile.syslog\",",
            "                \"docker/docker-syslog-config\",",
            "                \"scalyr-docker-agent-syslog\",",
            "                [\"scalyr/scalyr-agent-docker-syslog\", \"scalyr/scalyr-agent-docker\"],",
            "                coverage_enabled=coverage_enabled,",
            "            )",
            "        elif package_type == \"docker_json_builder\":",
            "            # An image for running on Docker configured to fetch logs via the file system (the container log",
            "            # directory is mounted to the agent container.)  This is the preferred way of running on Docker.",
            "            # This image is published to scalyr/scalyr-agent-docker-json.",
            "            artifact_file_name = build_container_builder(",
            "                variant,",
            "                version,",
            "                no_versioned_file_name,",
            "                \"scalyr-docker-agent.tar.gz\",",
            "                \"docker/Dockerfile\",",
            "                \"docker/docker-json-config\",",
            "                \"scalyr-docker-agent-json\",",
            "                [\"scalyr/scalyr-agent-docker-json\"],",
            "                coverage_enabled=coverage_enabled,",
            "            )",
            "        elif package_type == \"k8s_builder\":",
            "            # An image for running the agent on Kubernetes.",
            "            artifact_file_name = build_container_builder(",
            "                variant,",
            "                version,",
            "                no_versioned_file_name,",
            "                \"scalyr-k8s-agent.tar.gz\",",
            "                \"docker/Dockerfile.k8s\",",
            "                \"docker/k8s-config\",",
            "                \"scalyr-k8s-agent\",",
            "                [\"scalyr/scalyr-k8s-agent\"],",
            "                coverage_enabled=coverage_enabled,",
            "            )",
            "        else:",
            "            assert package_type in (\"deb\", \"rpm\")",
            "            artifact_file_name = build_rpm_or_deb_package(",
            "                package_type == \"rpm\", variant, version",
            "            )",
            "",
            "        os.chdir(original_cwd)",
            "",
            "        # Move the artifact (built package) to the original current working dir.",
            "        shutil.move(os.path.join(tmp_dir, artifact_file_name), artifact_file_name)",
            "        return artifact_file_name",
            "    finally:",
            "        # Be sure to delete the temporary directory.",
            "        os.chdir(original_cwd)",
            "        shutil.rmtree(tmp_dir)",
            "",
            "",
            "# A GUID representing Scalyr products, used to generate a per-version guid for each version of the Windows",
            "# Scalyr Agent.  DO NOT MODIFY THIS VALUE, or already installed software on clients machines will not be able",
            "# to be upgraded.",
            "_scalyr_guid_ = uuid.UUID(\"{0b52b8a0-22c7-4d50-92c1-8ea3b258984e}\")",
            "",
            "",
            "def build_win32_installer_package(variant, version):",
            "    \"\"\"Builds an MSI that will install the agent on a win32 machine in the current working directory.",
            "",
            "    Note, this can only be run on a Windows machine with the proper binaries and packages installed.",
            "",
            "    @param variant: If not None, will add the specified string to the GUID used to identify the installed",
            "        executables.  This can be used to avoid customer builds of the agent from colliding with the Scalyr-built",
            "        ones.",
            "    @param version: The agent version.",
            "",
            "    @return: The file name of the built package.",
            "    \"\"\"",
            "    if os.getenv(\"WIX\") is None:",
            "        print(",
            "            \"Error, the WIX toolset does not appear to be installed.\", file=sys.stderr",
            "        )",
            "        print(",
            "            \"Please install it to build the Windows Scalyr Agent installer.\",",
            "            file=sys.stderr,",
            "        )",
            "        print(\"See http://wixtoolset.org.\", file=sys.stderr)",
            "        sys.exit(1)",
            "",
            "    try:",
            "        import psutil  # NOQA",
            "    except ImportError:",
            "        # noinspection PyUnusedLocal",
            "        print(",
            "            \"Error, the psutil Python module is not installed.  This is required to build the\",",
            "            file=sys.stderr,",
            "        )",
            "        print(",
            "            \"Windows version of the Scalyr Agent.  Please download and install it.\",",
            "            file=sys.stderr,",
            "        )",
            "        print(\"See http://pythonhosted.org/psutil/\", file=sys.stderr)",
            "        print(",
            "            'On many systems, executing \"pip install psutil\" will install the package.',",
            "            file=sys.stderr,",
            "        )",
            "        sys.exit(1)",
            "",
            "    make_directory(\"source_root\")",
            "    make_directory(\"data_files\")",
            "",
            "    agent_source_root = __source_root__",
            "",
            "    # Populate source_root",
            "    os.chdir(\"source_root\")",
            "",
            "    shutil.copytree(make_path(agent_source_root, \"scalyr_agent\"), \"scalyr_agent\")",
            "    # We have to move __scalyr__.py up to the top of the source_root since, when running in the environment",
            "    # generated by PyInstaller, an 'import __scalyr__.py' will not look in the current directory.. it will only look",
            "    # for that module at the top of the sources_root.  Essentially, the PYTHONPATH variable only has a single",
            "    # entry in it, and it does not have '.' in it.  We leave a copy of __scalyr__.py in the original scalyr_agent",
            "    # directory because we need it there when we execute setup.py.  For the same reason, we put a copy of VERSION.",
            "    shutil.copy(convert_path(\"scalyr_agent/__scalyr__.py\"), \"__scalyr__.py\")",
            "    shutil.copy(make_path(agent_source_root, \"VERSION\"), \"VERSION\")",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"VERSION\"), convert_path(\"scalyr_agent/VERSION\"),",
            "    )",
            "",
            "    shutil.copytree(make_path(agent_source_root, \"monitors\"), \"monitors\")",
            "",
            "    os.chdir(\"monitors\")",
            "    recursively_delete_files_by_name(\"README.md\")",
            "    os.chdir(\"..\")",
            "",
            "    # Exclude certain files.",
            "    # TODO:  Should probably use MANIFEST.in to do this, but don't know the Python-fu to do this yet.",
            "    #",
            "    # Don't include the tests directories.  Also, don't include the .idea directory created by IDE.",
            "    recursively_delete_dirs_by_name(r\"\\.idea\", \"tests\")",
            "    recursively_delete_files_by_name(",
            "        r\".*\\.pyc\", r\".*\\.pyo\", r\".*\\.pyd\", r\"all_tests\\.py\", r\".*~\"",
            "    )",
            "",
            "    # exclude all the third_party_tls libs under windows",
            "    # because windows python has tls built in.",
            "    recursively_delete_dirs_by_name(\"third_party_tls\")",
            "",
            "    # Move back up to the root directory and populate the data_files.",
            "    os.chdir(\"..\")",
            "    os.chdir(\"data_files\")",
            "",
            "    # Copy the version file.  We copy it both to the root and the package root.  The package copy is done down below.",
            "",
            "    # make it VERSION.txt because PyInstaller on python 2 expects dll file named VERSION,",
            "    # and then fails with an error because of the invalid DLL loading.",
            "    shutil.copy(make_path(agent_source_root, \"VERSION\"), \"VERSION.txt\")",
            "    shutil.copy(make_path(agent_source_root, \"LICENSE.txt\"), \"LICENSE.txt\")",
            "",
            "    # Also add in build_info file",
            "    try:",
            "        write_to_file(get_build_info(), \"build_info\")",
            "    except Exception as e:",
            "        # NOTE: For now this error is not fatal in case git is not present on the system where",
            "        # we are building a package",
            "        print(\"Failed to retrieve / write build info fail: %s\" % (str(e)))",
            "",
            "    # Copy the third party licenses",
            "    shutil.copytree(",
            "        make_path(agent_source_root, \"scalyr_agent/third_party/licenses\"), \"licenses\"",
            "    )",
            "",
            "    # Copy the config file.",
            "    cat_files(",
            "        [make_path(agent_source_root, \"config/agent.json\")],",
            "        \"agent_config.tmpl\",",
            "        convert_newlines=True,",
            "    )",
            "",
            "    os.chdir(\"..\")",
            "    # We need to place a 'setup.py' here so that when we executed py2exe it finds it.",
            "    shutil.copy(make_path(agent_source_root, \"setup.py\"), \"setup.py\")",
            "",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"DESCRIPTION.rst\"),",
            "        convert_path(\"source_root/DESCRIPTION.rst\"),",
            "    )",
            "    pyinstaller_spec_path = os.path.join(",
            "        agent_source_root, \"win32\", \"scalyr-agent.spec\"",
            "    )",
            "",
            "    shutil.copy(pyinstaller_spec_path, \"scalyr-agent.spec\")",
            "",
            "    shutil.copy(",
            "        os.path.join(agent_source_root, \"win32\", \"dynamic_modules.py\"),",
            "        \"dynamic_modules.py\",",
            "    )",
            "",
            "    shutil.copy(",
            "        os.path.join(agent_source_root, \"win32\", \"wix-heat-bin-transform.xsl\"),",
            "        \"wix-heat-bin-transform.xsl\",",
            "    )",
            "",
            "    shutil.copy(",
            "        os.path.join(agent_source_root, \"win32\", \"scalyr_agent.wxs\"), \"scalyr_agent.wxs\"",
            "    )",
            "",
            "    run_command(",
            "        \"{0} -m PyInstaller scalyr-agent.spec\".format(sys.executable),",
            "        exit_on_fail=True,",
            "        command_name=\"pyinstaller\",",
            "    )",
            "",
            "    make_directory(\"Scalyr/certs\")",
            "    make_directory(\"Scalyr/logs\")",
            "    make_directory(\"Scalyr/data\")",
            "    make_directory(\"Scalyr/config/agent.d\")",
            "    os.rename(os.path.join(\"dist\", \"scalyr-agent-2\"), convert_path(\"Scalyr/bin\"))",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"win32/ScalyrShell.cmd\"),",
            "        \"Scalyr/bin/ScalyrShell.cmd\",",
            "    )",
            "",
            "    # Copy the cert files.",
            "    # AGENT-283: Certificate validation on windows seems to fail when the intermediate certs are present, skipping them",
            "    cat_files(",
            "        glob_files(make_path(agent_source_root, \"certs/*_root.pem\")),",
            "        \"Scalyr/certs/ca_certs.crt\",",
            "        convert_newlines=True,",
            "    )",
            "",
            "    # TODO: Check certificate expiration same as we do as part of tox lint target",
            "    # NOTE: This requires us to update Jenkins pipeline and other places where this script is called",
            "    # to install cryptography library",
            "",
            "    # Get ready to run wix.  Add in WIX to the PATH variable.",
            "    os.environ[\"PATH\"] = \"%s;%s\\\\bin\" % (os.getenv(\"PATH\"), os.getenv(\"WIX\"))",
            "",
            "    if variant is None:",
            "        variant = \"main\"",
            "",
            "    # Generate a unique identifier used to identify this version of the Scalyr Agent to windows.",
            "    product_code = create_scalyr_uuid3(\"ProductID:%s:%s\" % (variant, version))",
            "    # The upgrade code identifies all families of versions that can be upgraded from one to the other.  So, this",
            "    # should be a single number for all Scalyr produced ones.",
            "    upgrade_code = create_scalyr_uuid3(\"UpgradeCode:%s\" % variant)",
            "",
            "    # For prereleases, we use weird version numbers like 4.0.4.pre5.1 .  That does not work for Windows which",
            "    # requires X.X.X.X.  So, we convert if necessary.",
            "    if len(version.split(\".\")) == 5:",
            "        parts = version.split(\".\")",
            "        del parts[3]",
            "        version = \".\".join(parts)",
            "",
            "    # Gather files by 'heat' tool from WIX and generate .wxs file for 'bin' folder.",
            "    run_command(",
            "        \"heat dir Scalyr/bin -sreg -ag -cg BIN -dr APPLICATIONROOTDIRECTORY -var var.BinFolderSource -t wix-heat-bin-transform.xsl -o bin.wxs\",",
            "        exit_on_fail=True,",
            "        command_name=\"heat\",",
            "    )",
            "",
            "    run_command(",
            "        'candle -nologo -out bin.wixobj bin.wxs -dBinFolderSource=\"Scalyr/bin\"',",
            "        exit_on_fail=True,",
            "        command_name=\"candle\",",
            "    )",
            "",
            "    run_command(",
            "        'candle -nologo -out ScalyrAgent.wixobj -dVERSION=\"%s\" -dUPGRADECODE=\"%s\" '",
            "        '-dPRODUCTCODE=\"%s\" scalyr_agent.wxs' % (version, upgrade_code, product_code),",
            "        exit_on_fail=True,",
            "        command_name=\"candle\",",
            "    )",
            "",
            "    installer_name = \"ScalyrAgentInstaller-%s.msi\" % version",
            "",
            "    run_command(",
            "        \"light -nologo -ext WixUtilExtension.dll -ext WixUIExtension -out %s ScalyrAgent.wixobj bin.wixobj -v\"",
            "        % installer_name,",
            "        exit_on_fail=True,",
            "        command_name=\"light\",",
            "    )",
            "    return installer_name",
            "",
            "",
            "def create_wxs_file(template_path, dist_path, destination_path):",
            "    \"\"\"Performs a rewrite of the Wix file to replace template-like poritions with information about the",
            "    binaries/files in `dist_path`.",
            "",
            "    This is required so that our Windows installer includes all of the DLLs, Python compiled files, etc that PyInstaller",
            "    produced.  This list can change over time and is dependent on the build machine, so we cannot hard code this",
            "    list.  It must be determined dynamically.",
            "",
            "    The file is rewrite by expanding the 'templates' found between the '<!-- EXPAND_FROM_BIN' markers.  This will",
            "    make a copy of the included template, once for each file in the `dist_path`, replacing such variables as",
            "    $COMPONENT_ID, $COMPONENT_GUID, $FILE_ID, and $FILE_SOURCE with values calculated on the file's information.",
            "",
            "    You may also specify a list of files to exclude in `dist_path` from the template expansion.  This is used for",
            "    well-known files that are already in the Wix file.",
            "",
            "    Here is an example:",
            "      <!-- EXPAND_FROM_BIN EXCLUDE:scalyr-agent-2.exe,scalyr-agent-2-config.exe,ScalyrAgentService.exe -->",
            "        <Component Id='$COMPONENT_ID' Guid='$COMPONENT_GUID' >",
            "          <File Id='$FILE_ID' DiskId='1' KeyPath='yes' Checksum='yes'  Source='$FILE_SOURCE' />",
            "         </Component>",
            "      <!-- EXPAND_FROM_BIN -->",
            "",
            "    @param template_path: The file path storing the Wix file to copy/rewrite.",
            "    @param dist_path: The path to the directory containing the files that should be included in the template",
            "        expansion.",
            "    @param destination_path: The file path to write the result",
            "",
            "    @type template_path: str",
            "    @type dist_path: str",
            "    @type destination_path: str",
            "    \"\"\"",
            "    # First, calculate all of the per-file information for each file in the distribution directory.",
            "    dist_files = []",
            "    for dist_file_path in glob.glob(\"%s/*\" % dist_path):",
            "        base_file = os.path.basename(dist_file_path)",
            "        file_id = base_file.replace(\".\", \"_\").replace(\"-\", \"_\")",
            "        entry = {",
            "            \"BASE\": base_file,",
            "            \"FILE_ID\": file_id,",
            "            \"COMPONENT_GUID\": str(create_scalyr_uuid3(\"DistComp%s\" % base_file)),",
            "            \"COMPONENT_ID\": \"%s_comp\" % file_id,",
            "            \"FILE_SOURCE\": dist_file_path,",
            "        }",
            "",
            "        dist_files.append(entry)",
            "",
            "    # For the sake of easier coding, we read all of the lines of the input file into an array.",
            "    f = open(template_path)",
            "    try:",
            "        template_lines = f.readlines()",
            "    finally:",
            "        f.close()",
            "",
            "    # Now go through, looking for the markers, and when we find them, do the replacement.",
            "    result = []",
            "    while len(template_lines) > 0:",
            "        if \"<!-- EXPAND_FROM_BIN\" in template_lines[0]:",
            "            result.extend(expand_template(template_lines, dist_files))",
            "        else:",
            "            line = template_lines[0]",
            "            del template_lines[0]",
            "            result.append(line)",
            "",
            "    # Write the resulting lines out.",
            "    f = open(destination_path, \"w\")",
            "    try:",
            "        for line in result:",
            "            f.write(line)",
            "    finally:",
            "        f.close()",
            "",
            "",
            "def create_scalyr_uuid3(name):",
            "    \"\"\"",
            "    Create a UUID based on the Scalyr UUID namespace and a hash of `name`.",
            "",
            "    :param name: The name",
            "    :type name: six.text",
            "    :return: The UUID",
            "    :rtype: uuid.UUID",
            "    \"\"\"",
            "    return scalyr_util.create_uuid3(_scalyr_guid_, name)",
            "",
            "",
            "def expand_template(input_lines, dist_files):",
            "    \"\"\"Reads the template starting at the first entry in `input_lines` and generates a copy of it for each",
            "    item in `dist_files` that is not excluded.",
            "",
            "    Used by `create_wxs_file`.",
            "",
            "    This consumes the lines from the `input_lines` list.",
            "",
            "    @param input_lines: The list of input lines from the file, with the first beginning a template expansion",
            "        (should have the <!-- EXPAND_FROM_BIN pragma in it).",
            "    @param dist_files: The list of file entries from the distribution directory.  The template should be expanded",
            "        once for each entry (unless it was specifically excluded).",
            "",
            "    @type input_lines: [str]",
            "    @type dist_files:  [{}]",
            "",
            "    @return: The list of lines produced by the expansion.",
            "    @rtype: [str]",
            "    \"\"\"",
            "    # First, see if there were any files that should be excluded.  This will be in the first line, prefaced by",
            "    # EXCLUDED and a comma separated list.",
            "    match = re.search(r\"EXCLUDE:(\\S*)\", input_lines[0])",
            "    del input_lines[0]",
            "",
            "    if match is not None:",
            "        excluded_files = match.group(1).split(\",\")",
            "    else:",
            "        excluded_files = []",
            "",
            "    # Create a list of just the template.  We need to find where it ends in the input lines.",
            "    template_lines = []",
            "    found_end = False",
            "    while len(input_lines) > 0:",
            "        line = input_lines[0]",
            "        del input_lines[0]",
            "        if \"<!-- EXPAND_FROM_BIN\" in line:",
            "            found_end = True",
            "            break",
            "        else:",
            "            template_lines.append(line)",
            "",
            "    if not found_end:",
            "        raise Exception(\"Did not find termination for EXPAND_FROM_BIN\")",
            "",
            "    result = []",
            "    # Do the expansion.",
            "    for dist_entry in dist_files:",
            "        if dist_entry[\"BASE\"] in excluded_files:",
            "            continue",
            "",
            "        for template_line in template_lines:",
            "            line = template_line.replace(\"$FILE_ID\", dist_entry[\"FILE_ID\"])",
            "            line = line.replace(\"$COMPONENT_GUID\", dist_entry[\"COMPONENT_GUID\"])",
            "            line = line.replace(\"$COMPONENT_ID\", dist_entry[\"COMPONENT_ID\"])",
            "            line = line.replace(\"$FILE_SOURCE\", dist_entry[\"FILE_SOURCE\"])",
            "",
            "            result.append(line)",
            "",
            "    return result",
            "",
            "",
            "def build_common_docker_and_package_files(create_initd_link, base_configs=None):",
            "    \"\"\"Builds the common `root` system used by Debian, RPM, and container source tarballs in the current working",
            "    directory.",
            "",
            "    @param create_initd_link: Whether or not to create the link from initd to the scalyr agent binary.",
            "    @param base_configs:  The directory (relative to the top of the source tree) that contains the configuration",
            "        files to copy (such as the agent.json and agent.d directory).  If None, then will use `config`.",
            "    @type create_initd_link: bool",
            "    @type base_configs: str",
            "    \"\"\"",
            "    original_dir = os.getcwd()",
            "",
            "    # Create the directory structure for where the RPM/Debian package will place files on the system.",
            "    make_directory(\"root/etc/init.d\")",
            "    make_directory(\"root/var/log/scalyr-agent-2\")",
            "    make_directory(\"root/var/lib/scalyr-agent-2\")",
            "    make_directory(\"root/usr/share\")",
            "    make_directory(\"root/usr/sbin\")",
            "",
            "    # Place all of the import source in /usr/share/scalyr-agent-2.",
            "    os.chdir(\"root/usr/share\")",
            "",
            "    build_base_files(base_configs=base_configs)",
            "",
            "    os.chdir(\"scalyr-agent-2\")",
            "    # The build_base_files leaves the config directory in config, but we have to move it to its etc",
            "    # location.  We just rename it to the right directory.",
            "    shutil.move(",
            "        convert_path(\"config\"), make_path(original_dir, \"root/etc/scalyr-agent-2\")",
            "    )",
            "    os.chdir(original_dir)",
            "",
            "    # Make sure there is an agent.d directory regardless of the config directory we used.",
            "    make_directory(\"root/etc/scalyr-agent-2/agent.d\")",
            "",
            "    # Create the links to the appropriate commands in /usr/sbin and /etc/init.d/",
            "    if create_initd_link:",
            "        make_soft_link(",
            "            \"/usr/share/scalyr-agent-2/bin/scalyr-agent-2\",",
            "            \"root/etc/init.d/scalyr-agent-2\",",
            "        )",
            "    make_soft_link(",
            "        \"/usr/share/scalyr-agent-2/bin/scalyr-agent-2\", \"root/usr/sbin/scalyr-agent-2\"",
            "    )",
            "    make_soft_link(",
            "        \"/usr/share/scalyr-agent-2/bin/scalyr-agent-2-config\",",
            "        \"root/usr/sbin/scalyr-agent-2-config\",",
            "    )",
            "    make_soft_link(",
            "        \"/usr/share/scalyr-agent-2/bin/scalyr-switch-python\",",
            "        \"root/usr/sbin/scalyr-switch-python\",",
            "    )",
            "",
            "",
            "def build_container_builder(",
            "    variant,",
            "    version,",
            "    no_versioned_file_name,",
            "    source_tarball,",
            "    dockerfile,",
            "    base_configs,",
            "    image_name,",
            "    image_repos,",
            "    coverage_enabled=False,",
            "):",
            "    \"\"\"Builds an executable script in the current working directory that will build the container image for the various",
            "    Docker and Kubernetes targets.  This script embeds all assets it needs in it so it can be a standalone artifact.",
            "    The script is based on `docker/scripts/container_builder_base.sh`.  See that script for information on it can",
            "    be used.",
            "",
            "    @param variant: If not None, will add the specified string into the final script name. This allows for different",
            "        scripts to be built for the same type and same version.",
            "    @param version: The agent version.",
            "    @param no_versioned_file_name:  True if the version number should not be embedded in the script's file name.",
            "    @param source_tarball:  The filename for the source tarball (including the `.tar.gz` extension) that will",
            "        be built and then embedded in the artifact.  The contents of the Dockerfile will determine what this",
            "        name should be.",
            "    @param dockerfile:  The file path for the Dockerfile to embed in the script, relative to the top of the",
            "        agent source directory.",
            "    @param base_configs:  The file path for the configuration to use when building the container image, relative",
            "        to the top of the agent source directory.  This allows for different `agent.json` and `agent.d` directories",
            "        to be used for Kubernetes, docker, etc.",
            "    @param image_name:  The name for the image that is being built.  Will be used for the artifact's name.",
            "    @param image_repos:  A list of repositories that should be added as tags to the image once it is built.",
            "        Each repository will have two tags added -- one for the specific agent version and one for `latest`.",
            "    @param coverage_enabled: Path Dockerfile to run agent with enabled coverage.",
            "",
            "    @return: The file name of the built artifact.",
            "    \"\"\"",
            "    build_container_tarball(source_tarball, base_configs=base_configs)",
            "",
            "    agent_source_root = __source_root__",
            "    # Make a copy of the right Dockerfile to embed in the script.",
            "    shutil.copy(make_path(agent_source_root, dockerfile), \"Dockerfile\")",
            "    # copy requirements file with dependencies for docker builds.",
            "    shutil.copy(",
            "        make_path(agent_source_root, os.path.join(\"docker\", \"requirements.txt\")),",
            "        \"requirements.txt\",",
            "    )",
            "",
            "    if variant is None:",
            "        version_string = version",
            "    else:",
            "        version_string = \"%s.%s\" % (version, variant)",
            "",
            "    # Read the base builder script into memory",
            "    base_fp = open(",
            "        make_path(agent_source_root, \"docker/scripts/container_builder_base.sh\"), \"r\"",
            "    )",
            "    base_script = base_fp.read()",
            "    base_fp.close()",
            "",
            "    # The script has two lines defining environment variables (REPOSITORIES and TAGS) that we need to overwrite to",
            "    # set them to what we want.  We'll just do some regex replace to do that.",
            "    base_script = re.sub(",
            "        r\"\\n.*OVERRIDE_REPOSITORIES.*\\n\",",
            "        '\\nREPOSITORIES=\"%s\"\\n' % \",\".join(image_repos),",
            "        base_script,",
            "    )",
            "    base_script = re.sub(",
            "        r\"\\n.*OVERRIDE_TAGS.*\\n\",",
            "        '\\nTAGS=\"%s\"\\n' % \"%s,latest\" % version_string,",
            "        base_script,",
            "    )",
            "",
            "    if no_versioned_file_name:",
            "        output_name = image_name",
            "    else:",
            "        output_name = \"%s-%s\" % (image_name, version_string)",
            "",
            "    # Tar it up but hold the tarfile in memory.  Note, if the source tarball really becomes massive, might have to",
            "    # rethink this.",
            "    tar_out = BytesIO()",
            "    tar = tarfile.open(\"assets.tar.gz\", \"w|gz\", tar_out)",
            "",
            "    # if coverage enabled patch Dockerfile to install coverage package with pip.",
            "    if coverage_enabled:",
            "        with open(\"Dockerfile\", \"r\") as file:",
            "            data = file.read()",
            "        new_dockerfile_source = re.sub(r\"(RUN\\spip\\s.*)\", r\"\\1 coverage==4.5.4\", data)",
            "        new_dockerfile_source = re.sub(",
            "            r\"CMD .*\\n\",",
            "            'CMD [\"coverage\", \"run\", \"--branch\", \"/usr/share/scalyr-agent-2/py/scalyr_agent/agent_main.py\", '",
            "            '\"--no-fork\", \"--no-change-user\", \"start\"]',",
            "            new_dockerfile_source,",
            "        )",
            "",
            "        with open(\"Dockerfile\", \"w\") as file:",
            "            file.write(new_dockerfile_source)",
            "",
            "    tar.add(\"Dockerfile\")",
            "    tar.add(\"requirements.txt\")",
            "    tar.add(source_tarball)",
            "    tar.close()",
            "",
            "    # Write one file that has the contents of the script followed by the contents of the tarfile.",
            "    builder_fp = open(output_name, \"wb\")",
            "    builder_fp.write(base_script.encode(\"utf-8\"))",
            "    builder_fp.write(tar_out.getvalue())",
            "    builder_fp.close()",
            "",
            "    # Make the script executable.",
            "    st = os.stat(output_name)",
            "    os.chmod(output_name, st.st_mode | stat.S_IEXEC | stat.S_IXGRP)",
            "",
            "    return output_name",
            "",
            "",
            "def build_container_tarball(tarball_name, base_configs=None):",
            "    \"\"\"Builds the scalyr-agent-2 tarball for either Docker or Kubernetes in the current working directory.",
            "",
            "    @param tarball_name:  The name for the output tarball (including the `.tar.gz` extension)",
            "    @param base_configs: The directory (relative to the top of the source tree) that contains the configuration",
            "        files to copy (such as the agent.json and agent.d directory).  If None, then will use `config`.",
            "    @type tarball_name: str",
            "    @type base_configs: str",
            "",
            "    @return: The file name of the built tarball.",
            "    \"\"\"",
            "    build_common_docker_and_package_files(False, base_configs=base_configs)",
            "",
            "    # Need to create some docker specific files",
            "    make_directory(\"root/var/log/scalyr-agent-2/containers\")",
            "",
            "    # Tar it up.",
            "    tar = tarfile.open(tarball_name, \"w:gz\")",
            "    original_dir = os.getcwd()",
            "",
            "    os.chdir(\"root\")",
            "",
            "    # Do a manual walk over the contents of root so that we can use `addfile` to add the tarfile... which allows",
            "    # us to reset the owner/group to root.  This might not be that portable to Windows, but for now, Docker is mainly",
            "    # Posix.",
            "    for root, dirs, files in os.walk(\".\"):",
            "        to_copy = []",
            "        for name in dirs:",
            "            to_copy.append(os.path.join(root, name))",
            "        for name in files:",
            "            to_copy.append(os.path.join(root, name))",
            "",
            "        for x in to_copy:",
            "            file_entry = tar.gettarinfo(x)",
            "            file_entry.uname = \"root\"",
            "            file_entry.gname = \"root\"",
            "            file_entry.uid = 0",
            "            file_entry.gid = 0",
            "",
            "            if file_entry.isreg():",
            "                fp = open(file_entry.name, \"rb\")",
            "                tar.addfile(file_entry, fp)",
            "                fp.close()",
            "            else:",
            "                tar.addfile(file_entry)",
            "",
            "    os.chdir(original_dir)",
            "",
            "    tar.close()",
            "",
            "    return tarball_name",
            "",
            "",
            "def build_rpm_or_deb_package(is_rpm, variant, version):",
            "    \"\"\"Builds either an RPM or Debian package in the current working directory.",
            "",
            "    @param is_rpm: True if an RPM should be built. Otherwise a Debian package will be built.",
            "    @param variant: If not None, will add the specified string into the iteration identifier for the package. This",
            "        allows for different packages to be built for the same type and same version.",
            "    @param version: The agent version.",
            "",
            "    @return: The file name of the built package.",
            "    \"\"\"",
            "    build_common_docker_and_package_files(True)",
            "",
            "    # Create the scriplets the RPM/Debian package invokes when uninstalling or upgrading.",
            "    create_scriptlets()",
            "    # Produce the change logs that we will embed in the package, based on the CHANGELOG.md in this directory.",
            "    create_change_logs()",
            "",
            "    if is_rpm:",
            "        package_type = \"rpm\"",
            "    else:",
            "        package_type = \"deb\"",
            "",
            "    # Only change the iteration label if we need to embed a variant.",
            "    if variant is not None:",
            "        iteration_arg = \"--iteration 1.%s\" % variant",
            "    else:",
            "        iteration_arg = \"\"",
            "",
            "    description = (",
            "        \"Scalyr Agent 2 is the daemon process Scalyr customers run on their servers to collect metrics and \"",
            "        \"log files and transmit them to Scalyr.\"",
            "    )",
            "",
            "    run_command(",
            "        'fpm -s dir -a all -t %s -n \"scalyr-agent-2\" -v %s '",
            "        '  --license \"Apache 2.0\" '",
            "        \"  --vendor Scalyr %s \"",
            "        \"  --maintainer czerwin@scalyr.com \"",
            "        \"  --provides scalyr-agent-2 \"",
            "        '  --description \"%s\" '",
            "        '  --depends \"bash >= 3.2\" '",
            "        \"  --url https://www.scalyr.com \"",
            "        \"  --deb-user root \"",
            "        \"  --deb-group root \"",
            "        \"  --deb-changelog changelog-deb \"",
            "        \"  --rpm-user root \"",
            "        \"  --rpm-group root \"",
            "        \"  --rpm-changelog changelog-rpm\"",
            "        \"  --before-install preinstall.sh \"",
            "        \"  --after-install postinstall.sh \"",
            "        \"  --before-remove preuninstall.sh \"",
            "        \"  --deb-no-default-config-files \"",
            "        \"  --no-deb-auto-config-files \"",
            "        \"  --config-files /etc/scalyr-agent-2/agent.json \"",
            "        # NOTE: We leave those two files in place since they are symlinks which might have been",
            "        # updated by scalyr-switch-python and we want to leave this in place - aka make sure",
            "        # selected Python version is preserved on upgrade",
            "        \"  --config-files /usr/share/scalyr-agent-2/bin/scalyr-agent-2 \"",
            "        \"  --config-files /usr/share/scalyr-agent-2/bin/scalyr-agent-2-config \"",
            "        \"  --directories /usr/share/scalyr-agent-2 \"",
            "        \"  --directories /var/lib/scalyr-agent-2 \"",
            "        \"  --directories /var/log/scalyr-agent-2 \"",
            "        \"  -C root usr etc var\" % (package_type, version, iteration_arg, description),",
            "        exit_on_fail=True,",
            "        command_name=\"fpm\",",
            "    )",
            "",
            "    # We determine the artifact name in a little bit of loose fashion.. we just glob over the current",
            "    # directory looking for something either ending in .rpm or .deb.  There should only be one package,",
            "    # so that is fine.",
            "    if is_rpm:",
            "        files = glob.glob(\"*.rpm\")",
            "    else:",
            "        files = glob.glob(\"*.deb\")",
            "",
            "    if len(files) != 1:",
            "        raise Exception(",
            "            \"Could not find resulting rpm or debian package in the build directory.\"",
            "        )",
            "",
            "    return files[0]",
            "",
            "",
            "def build_tarball_package(variant, version, no_versioned_file_name):",
            "    \"\"\"Builds the scalyr-agent-2 tarball in the current working directory.",
            "",
            "    @param variant: If not None, will add the specified string into the final tarball name. This allows for different",
            "        tarballs to be built for the same type and same version.",
            "    @param version: The agent version.",
            "    @param no_versioned_file_name:  True if the version number should not be embedded in the artifact's file name.",
            "",
            "    @return: The file name of the built tarball.",
            "    \"\"\"",
            "    # Use build_base_files to build all of the important stuff in ./scalyr-agent-2",
            "    build_base_files()",
            "",
            "    # Build the rest of the directories required for the tarball install.  Mainly, the log and data directories",
            "    # in the tarball itself where the running process will store its state.",
            "    make_directory(\"scalyr-agent-2/data\")",
            "    make_directory(\"scalyr-agent-2/log\")",
            "    make_directory(\"scalyr-agent-2/config/agent.d\")",
            "",
            "    # Create a file named packageless.  This signals to the agent that",
            "    # this a tarball install instead of an RPM/Debian install, which changes",
            "    # the default paths for th econfig, logs, data, etc directories.  See",
            "    # configuration.py.",
            "    write_to_file(\"1\", \"scalyr-agent-2/packageless\")",
            "",
            "    if variant is None:",
            "        base_archive_name = \"scalyr-agent-%s\" % version",
            "    else:",
            "        base_archive_name = \"scalyr-agent-%s.%s\" % (version, variant)",
            "",
            "    shutil.move(\"scalyr-agent-2\", base_archive_name)",
            "",
            "    output_name = (",
            "        \"%s.tar.gz\" % base_archive_name",
            "        if not no_versioned_file_name",
            "        else \"scalyr-agent.tar.gz\"",
            "    )",
            "    # Tar it up.",
            "    tar = tarfile.open(output_name, \"w:gz\")",
            "    tar.add(base_archive_name)",
            "    tar.close()",
            "",
            "    return output_name",
            "",
            "",
            "def replace_shebang(path, new_path, new_shebang):",
            "    # type: (six.text_type, six.text_type, six.text_type) ->None",
            "    with open(path, \"r\") as f:",
            "        with open(new_path, \"w\") as newf:",
            "            # skip shebang",
            "            f.readline()",
            "            newf.write(new_shebang)",
            "            newf.write(\"\\n\")",
            "            newf.write(f.read())",
            "",
            "",
            "def build_base_files(base_configs=\"config\"):",
            "    \"\"\"Build the basic structure for a package in a new directory scalyr-agent-2 in the current working directory.",
            "",
            "    This creates scalyr-agent-2 in the current working directory and then populates it with the basic structure",
            "    required by most of the packages.",
            "",
            "    It copies the source files, the certs, the configuration directories, etc.  This will make sure to exclude",
            "    files like .pyc, .pyo, etc.",
            "",
            "    In the end, the structure will look like:",
            "      scalyr-agent-2:",
            "        py/scalyr_agent/           -- All the scalyr_agent source files",
            "        certs/ca_certs.pem         -- The trusted SSL CA root list.",
            "        config/agent.json          -- The configuration file.",
            "        bin/scalyr-agent-2         -- Symlink to the agent_main.py file to run the agent.",
            "        bin/scalyr-agent-2-config  -- Symlink to config_main.py to run the configuration tool",
            "        build_info                 -- A file containing the commit id of the latest commit included in this package,",
            "                                      the time it was built, and other information.",
            "",
            "    @param base_configs:  The directory (relative to the top of the source tree) that contains the configuration",
            "        files to copy (such as the agent.json and agent.d directory).  If None, then will use `config`.",
            "    \"\"\"",
            "    original_dir = os.getcwd()",
            "    # This will return the parent directory of this file.  We will use that to determine the path",
            "    # to files like scalyr_agent/ to copy the source files",
            "    agent_source_root = __source_root__",
            "",
            "    make_directory(\"scalyr-agent-2/py\")",
            "    os.chdir(\"scalyr-agent-2\")",
            "",
            "    make_directory(\"certs\")",
            "    make_directory(\"bin\")",
            "    make_directory(\"misc\")",
            "",
            "    # Copy the version file.  We copy it both to the root and the package root.  The package copy is done down below.",
            "    shutil.copy(make_path(agent_source_root, \"VERSION\"), \"VERSION\")",
            "",
            "    # Copy the source files.",
            "    os.chdir(\"py\")",
            "",
            "    shutil.copytree(make_path(agent_source_root, \"scalyr_agent\"), \"scalyr_agent\")",
            "    shutil.copytree(make_path(agent_source_root, \"monitors\"), \"monitors\")",
            "    os.chdir(\"monitors\")",
            "    recursively_delete_files_by_name(\"README.md\")",
            "    os.chdir(\"..\")",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"VERSION\"),",
            "        os.path.join(\"scalyr_agent\", \"VERSION\"),",
            "    )",
            "",
            "    # create copies of the agent_main.py with python2 and python3 shebang.",
            "    agent_main_path = os.path.join(agent_source_root, \"scalyr_agent\", \"agent_main.py\")",
            "    agent_main_py2_path = os.path.join(\"scalyr_agent\", \"agent_main_py2.py\")",
            "    agent_main_py3_path = os.path.join(\"scalyr_agent\", \"agent_main_py3.py\")",
            "    replace_shebang(agent_main_path, agent_main_py2_path, \"#!/usr/bin/env python2\")",
            "    replace_shebang(agent_main_path, agent_main_py3_path, \"#!/usr/bin/env python3\")",
            "    main_permissions = os.stat(agent_main_path).st_mode",
            "    os.chmod(agent_main_py2_path, main_permissions)",
            "    os.chmod(agent_main_py3_path, main_permissions)",
            "",
            "    # create copies of the config_main.py with python2 and python3 shebang.",
            "    config_main_path = os.path.join(agent_source_root, \"scalyr_agent\", \"config_main.py\")",
            "    config_main_py2_path = os.path.join(\"scalyr_agent\", \"config_main_py2.py\")",
            "    config_main_py3_path = os.path.join(\"scalyr_agent\", \"config_main_py3.py\")",
            "    replace_shebang(config_main_path, config_main_py2_path, \"#!/usr/bin/env python2\")",
            "    replace_shebang(config_main_path, config_main_py3_path, \"#!/usr/bin/env python3\")",
            "    config_permissions = os.stat(config_main_path).st_mode",
            "    os.chmod(config_main_py2_path, config_permissions)",
            "    os.chmod(config_main_py3_path, config_permissions)",
            "",
            "    # Exclude certain files.",
            "    # TODO:  Should probably use MANIFEST.in to do this, but don't know the Python-fu to do this yet.",
            "    #",
            "    # Don't include the tests directories.  Also, don't include the .idea directory created by IDE.",
            "    recursively_delete_dirs_by_name(r\"\\.idea\", \"tests\")",
            "    recursively_delete_files_by_name(",
            "        r\".*\\.pyc\", r\".*\\.pyo\", r\".*\\.pyd\", r\"all_tests\\.py\", r\".*~\"",
            "    )",
            "",
            "    os.chdir(\"..\")",
            "",
            "    # Copy the config",
            "    if base_configs is not None:",
            "        config_path = base_configs",
            "    else:",
            "        config_path = \"config\"",
            "    shutil.copytree(make_path(agent_source_root, config_path), \"config\")",
            "",
            "    # Create the trusted CA root list.",
            "    os.chdir(\"certs\")",
            "    cat_files(",
            "        glob_files(make_path(agent_source_root, \"certs/*_root.pem\")), \"ca_certs.crt\"",
            "    )",
            "    cat_files(",
            "        glob_files(make_path(agent_source_root, \"certs/*_intermediate.pem\")),",
            "        \"intermediate_certs.pem\",",
            "    )",
            "    for cert_file in glob_files(make_path(agent_source_root, \"certs/*.pem\")):",
            "        shutil.copy(cert_file, cert_file.split(\"/\")[-1])",
            "",
            "    # TODO: Check certificate expiration same as we do as part of tox lint target",
            "    # NOTE: This requires us to update Jenkins pipeline and other places where this script is called",
            "    # to install cryptography library",
            "    os.chdir(\"..\")",
            "",
            "    # Misc extra files needed for some features.",
            "    os.chdir(\"misc\")",
            "    # This docker file is needed by the `scalyr-agent-2-config --docker-create-custom-dockerfile` command.  We",
            "    # put it in all distributions (not just the docker_tarball) in case a customer creates an imagine using a package.",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"docker/Dockerfile.custom_agent_config\"),",
            "        \"Dockerfile.custom_agent_config\",",
            "    )",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"docker/Dockerfile.custom_k8s_config\"),",
            "        \"Dockerfile.custom_k8s_config\",",
            "    )",
            "    os.chdir(\"..\")",
            "",
            "    # Create symlinks for the two commands",
            "    os.chdir(\"bin\")",
            "",
            "    make_soft_link(\"../py/scalyr_agent/agent_main.py\", \"scalyr-agent-2\")",
            "    make_soft_link(\"../py/scalyr_agent/config_main.py\", \"scalyr-agent-2-config\")",
            "",
            "    # add switch python version script.",
            "    shutil.copy(",
            "        os.path.join(",
            "            agent_source_root, \"installer\", \"scripts\", \"scalyr-switch-python.sh\"",
            "        ),",
            "        \"scalyr-switch-python\",",
            "    )",
            "",
            "    os.chdir(\"..\")",
            "",
            "    write_to_file(get_build_info(), \"build_info\")",
            "",
            "    os.chdir(original_dir)",
            "",
            "",
            "def make_directory(path):",
            "    \"\"\"Creates the specified directory including any parents that do not yet exist.",
            "",
            "    @param path: The path of the directory to create. This string can use a forward slash to separate path",
            "           components regardless of the separator character for this platform.  This method will perform the necessary",
            "           conversion.",
            "    \"\"\"",
            "    converted_path = convert_path(path)",
            "    try:",
            "        os.makedirs(converted_path)",
            "    except OSError as error:",
            "        if error.errno == errno.EEXIST and os.path.isdir(converted_path):",
            "            pass",
            "        else:",
            "            raise",
            "",
            "",
            "def make_path(parent_directory, path):",
            "    \"\"\"Returns the full path created by joining path to parent_directory.",
            "",
            "    This method is a convenience function because it allows path to use forward slashes",
            "    to separate path components rather than the platform's separator character.",
            "",
            "    @param parent_directory: The parent directory. This argument must use the system's separator character. This may be",
            "        None if path is relative to the current working directory.",
            "    @param path: The path to add to parent_directory. This should use forward slashes as the separator character,",
            "        regardless of the platform's character.",
            "",
            "    @return:  The path created by joining the two with using the system's separator character.",
            "    \"\"\"",
            "    if parent_directory is None and os.path.sep == \"/\":",
            "        return path",
            "",
            "    if parent_directory is None:",
            "        result = \"\"",
            "    elif path.startswith(\"/\"):",
            "        result = \"\"",
            "    else:",
            "        result = parent_directory",
            "",
            "    for path_part in path.split(\"/\"):",
            "        if len(path_part) > 0:",
            "            result = os.path.join(result, path_part)",
            "",
            "    return result",
            "",
            "",
            "def convert_path(path):",
            "    \"\"\"Converts the forward slashes in path to the platform's separator and returns the value.",
            "",
            "    @param path: The path to convert. This should use forward slashes as the separator character, regardless of the",
            "        platform's character.",
            "",
            "    @return: The path created by converting the forward slashes to the platform's separator.",
            "    \"\"\"",
            "    return make_path(None, path)",
            "",
            "",
            "def make_soft_link(source, link_path):",
            "    \"\"\"Creates a soft link at link_path to source.",
            "",
            "    @param source: The path that the link will point to. This should use a forward slash as the separator, regardless",
            "        of the platform's separator.",
            "    @param link_path: The path where the link will be created. This should use a forward slash as the separator,",
            "        regardless of the platform's separator.",
            "    \"\"\"",
            "    os.symlink(convert_path(source), convert_path(link_path))",
            "",
            "",
            "def glob_files(path):",
            "    \"\"\"Returns the paths that match the specified path glob (based on current working directory).",
            "",
            "    @param path: The path with glob wildcard characters to match. This should use a forward slash as the separator,",
            "        regardless of the platform's separator.",
            "",
            "    @return: The list of matched paths.",
            "    \"\"\"",
            "    return glob.glob(convert_path(path))",
            "",
            "",
            "def recursively_delete_dirs_by_name(*dir_names):",
            "    \"\"\"Deletes any directories that are in the current working directory or any of its children whose file names",
            "    match the specified regular expressions.",
            "",
            "    This will recursively examine all children of the current working directory.",
            "",
            "    If a directory is found that needs to be deleted, all of it and its children are deleted.",
            "",
            "    @param dir_names: A variable number of strings containing regular expressions that should match the file names of",
            "        the directories that should be deleted.",
            "    \"\"\"",
            "    # Compile the strings into actual regular expression match objects.",
            "    matchers = []",
            "    for dir_name in dir_names:",
            "        matchers.append(re.compile(dir_name))",
            "",
            "    # Walk down the file tree, top down, allowing us to prune directories as we go.",
            "    for root, dirs, files in os.walk(\".\"):",
            "        # The list of directories at the current level to delete.",
            "        to_remove = []",
            "",
            "        # Examine all directories at this level, see if any get a match",
            "        for dir_path in dirs:",
            "            remove_it = False",
            "            for matcher in matchers:",
            "                if matcher.match(dir_path):",
            "                    remove_it = True",
            "            if remove_it:",
            "                to_remove.append(dir_path)",
            "",
            "        # Go back and delete it.  Also, remove it from dirs so that we don't try to walk down it.",
            "        for remove_dir_path in to_remove:",
            "            shutil.rmtree(os.path.join(root, remove_dir_path))",
            "            dirs.remove(remove_dir_path)",
            "",
            "",
            "def recursively_delete_files_by_name(*file_names):",
            "    \"\"\"Deletes any files that are in the current working directory or any of its children whose file names",
            "    match the specified regular expressions.",
            "",
            "    This will recursively examine all children of the current working directory.",
            "",
            "    @param file_names: A variable number of strings containing regular expressions that should match the file names of",
            "        the files that should be deleted.",
            "    \"\"\"",
            "    # Compile the strings into actual regular expression match objects.",
            "    matchers = []",
            "    for file_name in file_names:",
            "        matchers.append(re.compile(file_name))",
            "",
            "    # Walk down the current directory.",
            "    for root, dirs, files in os.walk(\".\"):",
            "        # See if any of the files at this level match any of the matchers.",
            "        for file_path in files:",
            "            remove_it = False",
            "            for matcher in matchers:",
            "                if matcher.match(file_path):",
            "                    remove_it = True",
            "            # Delete it if it did match.",
            "            if remove_it:",
            "                os.unlink(os.path.join(root, file_path))",
            "",
            "",
            "def cat_files(file_paths, destination, convert_newlines=False):",
            "    \"\"\"Concatenates the contents of the specified files and writes it to a new file at destination.",
            "",
            "    @param file_paths: A list of paths for the files that should be read. The concatenating will be done in the same",
            "        order as the list.",
            "    @param destination: The path of the file to write the contents to.",
            "    @param convert_newlines: If True, the final file will use Windows newlines (i.e., CR LF).",
            "    \"\"\"",
            "    dest_fp = open(destination, \"w\")",
            "    for file_path in file_paths:",
            "        in_fp = open(file_path, \"r\")",
            "        for line in in_fp:",
            "            if convert_newlines:",
            "                line.replace(\"\\n\", \"\\r\\n\")",
            "            dest_fp.write(line)",
            "        in_fp.close()",
            "    dest_fp.close()",
            "",
            "",
            "def write_to_file(string_value, file_path):",
            "    \"\"\"Writes the specified string to a new file.",
            "",
            "    This removes trailing newlines, etc, to avoid adding an extra blank line.",
            "",
            "    @param string_value: The value to write to the file.",
            "    @param file_path: The path of the file to write to.",
            "    \"\"\"",
            "    dest_fp = open(file_path, \"w\")",
            "    dest_fp.write(string_value.rstrip())",
            "    dest_fp.write(six.ensure_text(os.linesep))",
            "    dest_fp.close()",
            "",
            "",
            "def parse_date(date_str):",
            "    \"\"\"Parses a date time string of the format MMM DD, YYYY HH:MM +ZZZZ and returns seconds past epoch.",
            "",
            "    Example of the format is: Oct 10, 2014 17:00 -0700",
            "",
            "    @param date_str: A string containing the date and time in the format described above.",
            "",
            "    @return: The number of seconds past epoch.",
            "",
            "    @raise ValueError: if there is a parsing problem.",
            "    \"\"\"",
            "    # For some reason, it was hard to parse this particular format with the existing Python libraries,",
            "    # especially when the timezone was not the same as the local time zone.  So, we have to do this the",
            "    # sort of hard way.",
            "    #",
            "    # It is a little annoying that strptime only takes Sep for September and not Sep which is more common",
            "    # in US-eng, so we cheat here and just swap it out.",
            "    adjusted = date_str.replace(\"Sept\", \"Sep\")",
            "",
            "    # Find the timezone string at the end of the string.",
            "    if re.search(r\"[\\-+]\\d\\d\\d\\d$\", adjusted) is None:",
            "        raise ValueError(",
            "            \"Value '%s' does not meet required time format of 'MMM DD, YYYY HH:MM +ZZZZ' (or \"",
            "            \"as an example, ' 'Oct 10, 2014 17:00 -0700'\" % date_str",
            "        )",
            "",
            "    # Use the existing Python string parsing calls to just parse the time and date.  We will handle the timezone",
            "    # separately.",
            "    try:",
            "        base_time = time.mktime(time.strptime(adjusted[0:-6], \"%b %d, %Y %H:%M\"))",
            "    except ValueError:",
            "        raise ValueError(",
            "            \"Value '%s' does not meet required time format of 'MMM DD, YYYY HH:MM +ZZZZ' (or \"",
            "            \"as an example, ' 'Oct 10, 2014 17:00 -0700'\" % date_str",
            "        )",
            "",
            "    # Since mktime assumes the time is in localtime, we might have a different time zone",
            "    # in tz_str, we must manually added in the difference.",
            "    # First, convert -0700 to seconds.. the second two digits are the number of hours",
            "    # and the last two are the minute of minutes.",
            "    tz_str = adjusted[-5:]",
            "    tz_offset_secs = int(tz_str[1:3]) * 3600 + int(tz_str[3:5]) * 60",
            "",
            "    if tz_str.startswith(\"-\"):",
            "        tz_offset_secs *= -1",
            "",
            "    # Determine the offset for the local timezone.",
            "    if time.daylight:",
            "        local_offset_secs = -1 * time.altzone",
            "    else:",
            "        local_offset_secs = -1 * time.timezone",
            "",
            "    base_time += local_offset_secs - tz_offset_secs",
            "    return base_time",
            "",
            "",
            "# TODO:  This code is shared with config_main.py.  We should move this into a common",
            "# utility location both commands can import it from.",
            "def run_command(command_str, exit_on_fail=True, fail_quietly=False, command_name=None):",
            "    \"\"\"Executes the specified command string returning the exit status.",
            "",
            "    @param command_str: The command to execute.",
            "    @param exit_on_fail: If True, will exit this process with a non-zero status if the command fails.",
            "    @param fail_quietly:  If True, nothing will be emitted to stderr/stdout on failure.  If this is true,",
            "        exit_on_fail will be ignored.",
            "    @param command_name: The name to use to identify the command in error output.",
            "",
            "    @return: The exist status and output string of the command.",
            "    \"\"\"",
            "    # We have to use a temporary file to hold the output to stdout and stderr.",
            "    output_file_fd, output_file = tempfile.mkstemp()",
            "",
            "    output_fp = os.fdopen(output_file_fd, \"w\")",
            "",
            "    try:",
            "        return_code = subprocess.call(",
            "            command_str, stdin=None, stderr=output_fp, stdout=output_fp, shell=True",
            "        )",
            "        output_fp.flush()",
            "",
            "        # Read the output back into a string.  We cannot use a cStringIO.StringIO buffer directly above with",
            "        # subprocess.call because that method expects fileno support which StringIO doesn't support.",
            "        output_buffer = StringIO()",
            "        input_fp = open(output_file, \"rb\")",
            "        for line in input_fp:",
            "            output_buffer.write(line.decode(\"utf-8\"))",
            "        input_fp.close()",
            "",
            "        output_str = output_buffer.getvalue()",
            "        output_buffer.close()",
            "",
            "        if return_code != 0 and not fail_quietly:",
            "            if command_name is not None:",
            "                print(",
            "                    \"Executing %s failed and returned a non-zero result of %d\"",
            "                    % (command_name, return_code,),",
            "                    file=sys.stderr,",
            "                )",
            "            else:",
            "                print(",
            "                    \"Executing the following command failed and returned a non-zero result of %d\"",
            "                    % return_code,",
            "                    file=sys.stderr,",
            "                )",
            "                print('  Command: \"%s\"' % command_str, file=sys.stderr)",
            "",
            "            print(\"The output was:\", file=sys.stderr)",
            "            print(output_str, file=sys.stderr)",
            "",
            "            if exit_on_fail:",
            "                print(\"Exiting due to failure.\", file=sys.stderr)",
            "                sys.exit(1)",
            "",
            "        if isinstance(output_str, six.binary_type):",
            "            # Ensure we return unicode type",
            "            output_str = output_str.decode(\"utf-8\")",
            "",
            "        return return_code, output_str",
            "",
            "    finally:",
            "        # Be sure to close the temporary file and delete it.",
            "        output_fp.close()",
            "        os.unlink(output_file)",
            "",
            "",
            "def create_scriptlets():",
            "    \"\"\"Copy three scriptlets required by the RPM and Debian package to the current working directory.",
            "",
            "    These are the preinstall.sh, preuninstall.sh, and postuninstall.sh scripts.",
            "    \"\"\"",
            "",
            "    scripts_path = os.path.join(__source_root__, \"installer\", \"scripts\")",
            "",
            "    shutil.copy(os.path.join(scripts_path, \"preinstall.sh\"), \"preinstall.sh\")",
            "    shutil.copy(os.path.join(scripts_path, \"preuninstall.sh\"), \"preuninstall.sh\")",
            "    shutil.copy(os.path.join(scripts_path, \"postinstall.sh\"), \"postinstall.sh\")",
            "",
            "",
            "def create_change_logs():",
            "    \"\"\"Creates the necessary change logs for both RPM and Debian based on CHANGELOG.md.",
            "",
            "    Creates two files in the current working directory named 'changelog-rpm' and 'changelog-deb'.  They",
            "    will have the same content as CHANGELOG.md but formatted by the respective standards for the different",
            "    packaging systems.",
            "    \"\"\"",
            "    # We define a helper function named print_release_notes that is used down below.",
            "    def print_release_notes(output_fp, notes, level_prefixes, level=0):",
            "        \"\"\"Emits the notes for a single release to output_fp.",
            "",
            "        @param output_fp: The file to write the notes to",
            "        @param notes: An array of strings containing the notes for the release. Some elements may be lists of strings",
            "            themselves to represent sublists. Only three levels of nested lists are allowed. This is the same format",
            "            returned by parse_change_log() method.",
            "        @param level_prefixes: The prefix to use for each of the three levels of notes.",
            "        @param level: The current level in the notes.",
            "        \"\"\"",
            "        prefix = level_prefixes[level]",
            "        for note in notes:",
            "            if isinstance(note, list):",
            "                # If a sublist, then recursively call this function, increasing the level.",
            "                print_release_notes(output_fp, note, level_prefixes, level + 1)",
            "                if level == 0:",
            "                    print(\"\", file=output_fp)",
            "            else:",
            "                # Otherwise emit the note with the prefix for this level.",
            "                print(\"%s%s\" % (prefix, note), file=output_fp)",
            "",
            "    # Handle the RPM log first.  We parse CHANGELOG.md and then emit the notes in the expected format.",
            "    fp = open(\"changelog-rpm\", \"w\")",
            "    try:",
            "        for release in parse_change_log():",
            "            date_str = time.strftime(\"%a %b %d %Y\", time.localtime(release[\"time\"]))",
            "",
            "            # RPM expects the leading line for a relesae to start with an asterisk, then have",
            "            # the name of the person doing the release, their e-mail and then the version.",
            "            print(",
            "                \"* %s %s <%s> %s\"",
            "                % (",
            "                    date_str,",
            "                    release[\"packager\"],",
            "                    release[\"packager_email\"],",
            "                    release[\"version\"],",
            "                ),",
            "                file=fp,",
            "            )",
            "            print(\"\", file=fp)",
            "            print(\"Release: %s (%s)\" % (release[\"version\"], release[\"name\"]), file=fp)",
            "            print(\"\", file=fp)",
            "            # Include the release notes, with the first level with no indent, an asterisk for the second level",
            "            # and a dash for the third.",
            "            print_release_notes(fp, release[\"notes\"], [\"\", \" * \", \"   - \"])",
            "            print(\"\", file=fp)",
            "    finally:",
            "        fp.close()",
            "",
            "    # Next, create the Debian change log.",
            "    fp = open(\"changelog-deb\", \"w\")",
            "    try:",
            "        for release in parse_change_log():",
            "            # Debian expects a leading line that starts with the package, including the version, the distribution",
            "            # urgency.  Then, anything goes until the last line for the release, which begins with two dashes.",
            "            date_str = time.strftime(",
            "                \"%a, %d %b %Y %H:%M:%S %z\", time.localtime(release[\"time\"])",
            "            )",
            "            print(",
            "                \"scalyr-agent-2 (%s) stable; urgency=low\" % release[\"version\"], file=fp",
            "            )",
            "            # Include release notes with an indented first level (using asterisk, then a dash for the next level,",
            "            # finally a plus sign.",
            "            print_release_notes(fp, release[\"notes\"], [\" * \", \"   - \", \"     + \"])",
            "            print(",
            "                \"-- %s <%s>  %s\"",
            "                % (release[\"packager\"], release[\"packager_email\"], date_str,),",
            "                file=fp,",
            "            )",
            "    finally:",
            "        fp.close()",
            "",
            "",
            "def parse_change_log():",
            "    \"\"\"Parses the contents of CHANGELOG.md and returns the content in a structured way.",
            "",
            "    @return: A list of dicts, one for each release in CHANGELOG.md.  Each release dict will have with several fields:",
            "            name:  The name of the release",
            "            version:  The version of the release",
            "            packager:  The name of the packager, such as 'Steven Czerwinski'",
            "            packager_email:  The email for the packager",
            "            time:  The seconds past epoch when the package was created",
            "            notes:  A list of strings or lists representing the notes for the release.  The list may",
            "                have elements that are strings (for a single line of notes) or lists (for a nested list under",
            "                the last string element).  Only three levels of nesting are allowed.",
            "    \"\"\"",
            "    # Some regular expressions matching what we expect to see in CHANGELOG.md.",
            "    # Each release section should start with a '##' line for major header.",
            "    release_matcher = re.compile(r'## ([\\d\\._]+) \"(.*)\"')",
            "    # The expected pattern we will include in a HTML comment to give information on the packager.",
            "    packaged_matcher = re.compile(",
            "        r\"Packaged by (.*) <(.*)> on (\\w+ \\d+, \\d+ \\d+:\\d\\d [+-]\\d\\d\\d\\d)\"",
            "    )",
            "",
            "    # Listed below are the deliminators we use to extract the structure from the changelog release",
            "    # sections.  We fix our markdown syntax to make it easier for us.",
            "    #",
            "    # Our change log will look something like this:",
            "    #",
            "    # ## 2.0.1 \"Aggravated Aardvark\"",
            "    #",
            "    # New core features:",
            "    # * Blah blah",
            "    # * Blah Blah",
            "    #   - sub point",
            "    #",
            "    # Bug fixes:",
            "    # * Blah Blah",
            "",
            "    # The deliminators, each level is marked by what pattern we should see in the next line to either",
            "    # go up a level, go down a level, or confirm it is at the same level.",
            "    section_delims = [",
            "        # First level does not have any prefix.. just plain text.",
            "        # So, the level up is the release header, which begins with '##'",
            "        # The level down is ' *'.",
            "        {",
            "            \"up\": re.compile(\"## \"),",
            "            \"down\": re.compile(r\"\\* \"),",
            "            \"same\": re.compile(r\"[^\\s\\*\\-#]\"),",
            "            \"prefix\": \"\",",
            "        },",
            "        # Second level always begins with an asterisk.",
            "        {",
            "            \"up\": re.compile(r\"[^\\s\\*\\-#]\"),",
            "            \"down\": re.compile(\"    - \"),",
            "            \"same\": re.compile(r\"\\* \"),",
            "            \"prefix\": \"* \",",
            "        },",
            "        # Third level always begins with '  -'",
            "        {",
            "            \"up\": re.compile(r\"\\* \"),",
            "            \"down\": None,",
            "            \"same\": re.compile(\"    - \"),",
            "            \"prefix\": \"    - \",",
            "        },",
            "    ]",
            "",
            "    # Helper function.",
            "    def read_section(lines, level=0):",
            "        \"\"\"Transforms the lines representing the notes for a single release into the desired nested representation.",
            "",
            "        @param lines: The lines for the notes for a release including markup. NOTE, this list must be in reverse order,",
            "            where the next line to be scanned is the last line in the list.",
            "        @param level: The nesting level that these lines are at.",
            "",
            "        @return: A list containing the notes, with nested lists as appropriate.",
            "        \"\"\"",
            "        result = []",
            "",
            "        if len(lines) == 0:",
            "            return result",
            "",
            "        while len(lines) > 0:",
            "            # Go over each line, seeing based on its content, if we should go up a nesting level, down a level,",
            "            # or just stay at the same level.",
            "            my_line = lines.pop()",
            "",
            "            # If the next line is at our same level, then just add it to our current list and continue.",
            "            if section_delims[level][\"same\"].match(my_line) is not None:",
            "                result.append(my_line[len(section_delims[level][\"prefix\"]) :])",
            "                continue",
            "",
            "            # For all other cases, someone else is going to have to look at this line, so add it back to the list.",
            "            lines.append(my_line)",
            "",
            "            # If the next line looks like it belongs any previous nesting levels, then we must have exited out of",
            "            # our current nesting level, so just return what we have gathered for this sublist.",
            "            for i in range(level + 1):",
            "                if section_delims[i][\"up\"].match(my_line) is not None:",
            "                    return result",
            "            if (",
            "                section_delims[level][\"down\"] is not None",
            "                and section_delims[level][\"down\"].match(my_line) is not None",
            "            ):",
            "                # Otherwise, it looks like the next line belongs to a sublist.  Recursively call ourselves, going",
            "                # down a level in nesting.",
            "                result.append(read_section(lines, level + 1))",
            "            else:",
            "                raise BadChangeLogFormat(",
            "                    \"Release not line did not match expect format at level %d: %s\"",
            "                    % (level, my_line)",
            "                )",
            "        return result",
            "",
            "    # Begin the real work here.  Read the change log.",
            "    change_log_fp = open(os.path.join(__source_root__, \"CHANGELOG.md\"), \"r\")",
            "",
            "    try:",
            "        # Skip over the first two lines since it should be header.",
            "        change_log_fp.readline()",
            "        change_log_fp.readline()",
            "",
            "        # Read over all the lines, eliminating the comment lines and other useless things.  Also strip out all newlines.",
            "        content = []",
            "        in_comment = False",
            "        for line in change_log_fp:",
            "            line = line.rstrip()",
            "            if len(line) == 0:",
            "                continue",
            "",
            "            # Check for a comment.. either beginning or closing.",
            "            if line == \"<!---\":",
            "                in_comment = True",
            "            elif line == \"--->\":",
            "                in_comment = False",
            "            elif packaged_matcher.match(line) is not None:",
            "                # The only thing we will pay attention to while in a comment is our packaged line.  If we see it,",
            "                # grab it.",
            "                content.append(line)",
            "            elif not in_comment:",
            "                # Keep any non-comments.",
            "                content.append(line)",
            "",
            "        change_log_fp.close()",
            "        change_log_fp = None",
            "    finally:",
            "        if change_log_fp is not None:",
            "            change_log_fp.close()",
            "",
            "    # We reverse the content list so the first lines to be read are at the end.  This way we can use pop down below.",
            "    content.reverse()",
            "",
            "    # The list of release objects",
            "    releases = []",
            "",
            "    # The rest of the log should just contain release notes for each release.  Iterate over the content,",
            "    # reading out the release notes for each release.",
            "    while len(content) > 0:",
            "        # Each release must begin with at least two lines -- one for the release name and then one for the",
            "        # 'Packaged by Steven Czerwinski on... ' line that we pulled out of the HTML comment.",
            "        if len(content) < 2:",
            "            raise BadChangeLogFormat(",
            "                \"New release section does not contain at least two lines.\"",
            "            )",
            "",
            "        # Extract the information from each of those two lines.",
            "        current_line = content.pop()",
            "        release_version_name = release_matcher.match(current_line)",
            "        if release_version_name is None:",
            "            raise BadChangeLogFormat(",
            "                \"Header line for release did not match expected format: %s\"",
            "                % current_line",
            "            )",
            "",
            "        current_line = content.pop()",
            "        packager_info = packaged_matcher.match(current_line)",
            "        if packager_info is None:",
            "            raise BadChangeLogFormat(",
            "                \"Packager line for release did not match expected format: %s\"",
            "                % current_line",
            "            )",
            "",
            "        # Read the section notes until we hit a '##' line.",
            "        release_notes = read_section(content)",
            "",
            "        try:",
            "            time_value = parse_date(packager_info.group(3))",
            "        except ValueError as err:",
            "            message = getattr(err, \"message\", str(err))",
            "            raise BadChangeLogFormat(message)",
            "",
            "        releases.append(",
            "            {",
            "                \"name\": release_version_name.group(2),",
            "                \"version\": release_version_name.group(1),",
            "                \"packager\": packager_info.group(1),",
            "                \"packager_email\": packager_info.group(2),",
            "                \"time\": time_value,",
            "                \"notes\": release_notes,",
            "            }",
            "        )",
            "",
            "    return releases",
            "",
            "",
            "# A string containing the build info for this build, to be placed in the 'build_info' file.",
            "__build_info__ = None",
            "",
            "",
            "def get_build_info():",
            "    \"\"\"Returns a string containing the build info.\"\"\"",
            "    global __build_info__",
            "    if __build_info__ is not None:",
            "        return __build_info__",
            "",
            "    build_info_buffer = StringIO()",
            "    original_dir = os.getcwd()",
            "",
            "    try:",
            "        # We need to execute the git command in the source root.",
            "        os.chdir(__source_root__)",
            "        # Add in the e-mail address of the user building it.",
            "        (rc, packager_email) = run_command(",
            "            \"git config user.email\", fail_quietly=True, command_name=\"git\"",
            "        )",
            "        if rc != 0:",
            "            packager_email = \"unknown\"",
            "",
            "        print(\"Packaged by: %s\" % packager_email.strip(), file=build_info_buffer)",
            "",
            "        # Determine the last commit from the log.",
            "        (_, commit_id) = run_command(",
            "            \"git log --summary -1 | head -n 1 | cut -d ' ' -f 2\",",
            "            exit_on_fail=True,",
            "            command_name=\"git\",",
            "        )",
            "        print(\"Latest commit: %s\" % commit_id.strip(), file=build_info_buffer)",
            "",
            "        # Include the branch just for safety sake.",
            "        (_, branch) = run_command(",
            "            \"git branch | cut -d ' ' -f 2\", exit_on_fail=True, command_name=\"git\"",
            "        )",
            "        print(\"From branch: %s\" % branch.strip(), file=build_info_buffer)",
            "",
            "        # Add a timestamp.",
            "        print(",
            "            \"Build time: %s\"",
            "            % six.text_type(strftime(\"%Y-%m-%d %H:%M:%S UTC\", gmtime())),",
            "            file=build_info_buffer,",
            "        )",
            "",
            "        __build_info__ = build_info_buffer.getvalue()",
            "        return __build_info__",
            "    finally:",
            "        os.chdir(original_dir)",
            "",
            "        if build_info_buffer is not None:",
            "            build_info_buffer.close()",
            "",
            "",
            "def set_build_info(build_info_file_path):",
            "    \"\"\"Sets the file to use as the build_info file to include in the package.",
            "",
            "    If this is called, then future calls to get_build_info will return the contents of this file",
            "    and will not use other commands such as 'git' to try to create it on its own.",
            "",
            "    This is useful when you are running trying create a package on a system that does not have full access",
            "    to git.",
            "",
            "    @param build_info_file_path: The path to the build_info file to use.",
            "    \"\"\"",
            "    global __build_info__",
            "    fp = open(build_info_file_path, \"r\")",
            "    __build_info__ = fp.read()",
            "    fp.close()",
            "",
            "    return __build_info__",
            "",
            "",
            "class BadChangeLogFormat(Exception):",
            "    pass",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    parser = OptionParser(",
            "        usage=\"Usage: python build_package.py [options] %s\" % \"|\".join(PACKAGE_TYPES)",
            "    )",
            "    parser.add_option(",
            "        \"-v\",",
            "        \"--variant\",",
            "        dest=\"variant\",",
            "        default=None,",
            "        help=\"An optional string that is included in the package name to identify a variant \"",
            "        \"of the main release created by a different packager.  \"",
            "        \"Most users do not need to use this option.\",",
            "    )",
            "    parser.add_option(",
            "        \"\",",
            "        \"--only-create-build-info\",",
            "        action=\"store_true\",",
            "        dest=\"build_info_only\",",
            "        default=False,",
            "        help=\"If true, will only create the build_info file and exit.  This can be used in conjunction \"",
            "        \"with the --set-build-info option to create the build_info file on one host and then build the \"",
            "        \"rest of the package on another.  This is useful when the final host does not have full access \"",
            "        \"to git\",",
            "    )",
            "",
            "    parser.add_option(",
            "        \"\",",
            "        \"--no-versioned-file-name\",",
            "        action=\"store_true\",",
            "        dest=\"no_versioned_file_name\",",
            "        default=False,",
            "        help=\"If true, will not embed the version number in the artifact's file name.  This only \"",
            "        \"applies to the `tarball` and container builders artifacts.\",",
            "    )",
            "",
            "    parser.add_option(",
            "        \"\",",
            "        \"--set-build-info\",",
            "        dest=\"build_info\",",
            "        default=None,",
            "        help=\"The path to the build_info file to include in the final package.  If this is used, \"",
            "        \"this process will not invoke commands such as git in order to compute the build information \"",
            "        \"itself.  The file should be one built by a previous run of this script.\",",
            "    )",
            "",
            "    parser.add_option(",
            "        \"\",",
            "        \"--coverage\",",
            "        dest=\"coverage\",",
            "        action=\"store_true\",",
            "        default=False,",
            "        help=\"Enable coverage analysis. Can be used in smoketests. Only works with docker/k8s.\",",
            "    )",
            "",
            "    (options, args) = parser.parse_args()",
            "    # If we are just suppose to create the build_info, then do it and exit.  We do not bother to check to see",
            "    # if they specified a package.",
            "    if options.build_info_only:",
            "        write_to_file(get_build_info(), \"build_info\")",
            "        print(\"Built build_info\")",
            "        sys.exit(0)",
            "",
            "    if len(args) < 1:",
            "        print(",
            "            \"You must specify the package you wish to build, one of the following: %s.\"",
            "            % \", \".join(PACKAGE_TYPES),",
            "            file=sys.stderr,",
            "        )",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "    elif len(args) > 1:",
            "        print(\"You may only specify one package to build.\", file=sys.stderr)",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "    elif args[0] not in PACKAGE_TYPES:",
            "        print('Unknown package type given: \"%s\"' % args[0], file=sys.stderr)",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "",
            "    if options.build_info is not None:",
            "        set_build_info(options.build_info)",
            "",
            "    artifact = build_package(",
            "        args[0], options.variant, options.no_versioned_file_name, options.coverage,",
            "    )",
            "    print(\"Built %s\" % artifact)",
            "    sys.exit(0)"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Copyright 2014 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ------------------------------------------------------------------------",
            "#",
            "# Script used to build the RPM, Debian, and tarball packages for releasing Scalyr Agent 2.",
            "#",
            "# To execute this script, you must have installed fpm: https://github.com/jordansissel/fpm",
            "#",
            "# Usage: python build_package.py [options] rpm|tarball|deb",
            "#",
            "# author: Steven Czerwinski <czerwin@scalyr.com>",
            "",
            "from __future__ import absolute_import",
            "from __future__ import print_function",
            "from __future__ import unicode_literals",
            "",
            "__author__ = \"czerwin@scalyr.com\"",
            "",
            "import errno",
            "import glob",
            "import os",
            "import re",
            "import shutil",
            "import stat",
            "import subprocess",
            "import sys",
            "import tarfile",
            "import tempfile",
            "import time",
            "import uuid",
            "",
            "from io import StringIO",
            "from io import BytesIO",
            "from io import open",
            "",
            "from optparse import OptionParser",
            "from time import gmtime, strftime",
            "",
            "from scalyr_agent.__scalyr__ import get_install_root, SCALYR_VERSION, scalyr_init",
            "",
            "scalyr_init()",
            "",
            "import scalyr_agent.util as scalyr_util",
            "",
            "# [start of 2->TODO]",
            "# Check for suitability.",
            "# Important. Import six as any other dependency from \"third_party\" libraries after \"__scalyr__.scalyr_init\"",
            "import six",
            "from six.moves import range",
            "",
            "# [end of 2->TOD0]",
            "",
            "# The root of the Scalyr repository should just be the parent of this file.",
            "__source_root__ = get_install_root()",
            "",
            "# All the different packages that this script can build.",
            "PACKAGE_TYPES = [",
            "    \"rpm\",",
            "    \"tarball\",",
            "    \"deb\",",
            "    \"win32\",",
            "    \"docker_syslog_builder\",",
            "    \"docker_json_builder\",",
            "    \"k8s_builder\",",
            "]",
            "",
            "",
            "def build_package(package_type, variant, no_versioned_file_name, coverage_enabled):",
            "    \"\"\"Builds the scalyr-agent-2 package specified by the arguments.",
            "",
            "    The package is left in the current working directory.  The file name of the",
            "    package is returned by this function.",
            "",
            "    @param package_type: One of `PACKAGE_TYPES`. Determines which package type is built.",
            "    @param variant: Adds the specified string into the package's iteration name. This may be None if no additional",
            "        tweak to the name is required. This is used to produce different packages even for the same package type (such",
            "        as 'rpm').",
            "    @param no_versioned_file_name:  If True, will not embed a version number in the resulting artifact's file name.",
            "        This only has an affect if building one of the tarball formats.",
            "    @param coverage_enabled: If True, enables coverage analysis. Patches Dockerfile to run agent with coverage.",
            "",
            "    @return: The file name of the produced package.",
            "    \"\"\"",
            "    original_cwd = os.getcwd()",
            "",
            "    version = SCALYR_VERSION",
            "",
            "    # Create a temporary directory to build the package in.",
            "    tmp_dir = tempfile.mkdtemp(prefix=\"build-scalyr-agent-packages\")",
            "",
            "    try:",
            "        # Change to that directory and delegate to another method for the specific type.",
            "        os.chdir(tmp_dir)",
            "        if package_type == \"tarball\":",
            "            artifact_file_name = build_tarball_package(",
            "                variant, version, no_versioned_file_name",
            "            )",
            "        elif package_type == \"win32\":",
            "            artifact_file_name = build_win32_installer_package(variant, version)",
            "        elif package_type == \"docker_syslog_builder\":",
            "            # An image for running on Docker configured to receive logs from other containers via syslog.",
            "            # This is the deprecated approach (but is still published under scalyr/scalyr-docker-agent for",
            "            # backward compatibility.)  We also publish this under scalyr/scalyr-docker-agent-syslog to help",
            "            # with the eventual migration.",
            "            artifact_file_name = build_container_builder(",
            "                variant,",
            "                version,",
            "                no_versioned_file_name,",
            "                \"scalyr-docker-agent.tar.gz\",",
            "                \"docker/Dockerfile.syslog\",",
            "                \"docker/docker-syslog-config\",",
            "                \"scalyr-docker-agent-syslog\",",
            "                [\"scalyr/scalyr-agent-docker-syslog\", \"scalyr/scalyr-agent-docker\"],",
            "                coverage_enabled=coverage_enabled,",
            "            )",
            "        elif package_type == \"docker_json_builder\":",
            "            # An image for running on Docker configured to fetch logs via the file system (the container log",
            "            # directory is mounted to the agent container.)  This is the preferred way of running on Docker.",
            "            # This image is published to scalyr/scalyr-agent-docker-json.",
            "            artifact_file_name = build_container_builder(",
            "                variant,",
            "                version,",
            "                no_versioned_file_name,",
            "                \"scalyr-docker-agent.tar.gz\",",
            "                \"docker/Dockerfile\",",
            "                \"docker/docker-json-config\",",
            "                \"scalyr-docker-agent-json\",",
            "                [\"scalyr/scalyr-agent-docker-json\"],",
            "                coverage_enabled=coverage_enabled,",
            "            )",
            "        elif package_type == \"k8s_builder\":",
            "            # An image for running the agent on Kubernetes.",
            "            artifact_file_name = build_container_builder(",
            "                variant,",
            "                version,",
            "                no_versioned_file_name,",
            "                \"scalyr-k8s-agent.tar.gz\",",
            "                \"docker/Dockerfile.k8s\",",
            "                \"docker/k8s-config\",",
            "                \"scalyr-k8s-agent\",",
            "                [\"scalyr/scalyr-k8s-agent\"],",
            "                coverage_enabled=coverage_enabled,",
            "            )",
            "        else:",
            "            assert package_type in (\"deb\", \"rpm\")",
            "            artifact_file_name = build_rpm_or_deb_package(",
            "                package_type == \"rpm\", variant, version",
            "            )",
            "",
            "        os.chdir(original_cwd)",
            "",
            "        # Move the artifact (built package) to the original current working dir.",
            "        shutil.move(os.path.join(tmp_dir, artifact_file_name), artifact_file_name)",
            "        return artifact_file_name",
            "    finally:",
            "        # Be sure to delete the temporary directory.",
            "        os.chdir(original_cwd)",
            "        shutil.rmtree(tmp_dir)",
            "",
            "",
            "# A GUID representing Scalyr products, used to generate a per-version guid for each version of the Windows",
            "# Scalyr Agent.  DO NOT MODIFY THIS VALUE, or already installed software on clients machines will not be able",
            "# to be upgraded.",
            "_scalyr_guid_ = uuid.UUID(\"{0b52b8a0-22c7-4d50-92c1-8ea3b258984e}\")",
            "",
            "",
            "def build_win32_installer_package(variant, version):",
            "    \"\"\"Builds an MSI that will install the agent on a win32 machine in the current working directory.",
            "",
            "    Note, this can only be run on a Windows machine with the proper binaries and packages installed.",
            "",
            "    @param variant: If not None, will add the specified string to the GUID used to identify the installed",
            "        executables.  This can be used to avoid customer builds of the agent from colliding with the Scalyr-built",
            "        ones.",
            "    @param version: The agent version.",
            "",
            "    @return: The file name of the built package.",
            "    \"\"\"",
            "    if os.getenv(\"WIX\") is None:",
            "        print(",
            "            \"Error, the WIX toolset does not appear to be installed.\", file=sys.stderr",
            "        )",
            "        print(",
            "            \"Please install it to build the Windows Scalyr Agent installer.\",",
            "            file=sys.stderr,",
            "        )",
            "        print(\"See http://wixtoolset.org.\", file=sys.stderr)",
            "        sys.exit(1)",
            "",
            "    try:",
            "        import psutil  # NOQA",
            "    except ImportError:",
            "        # noinspection PyUnusedLocal",
            "        print(",
            "            \"Error, the psutil Python module is not installed.  This is required to build the\",",
            "            file=sys.stderr,",
            "        )",
            "        print(",
            "            \"Windows version of the Scalyr Agent.  Please download and install it.\",",
            "            file=sys.stderr,",
            "        )",
            "        print(\"See http://pythonhosted.org/psutil/\", file=sys.stderr)",
            "        print(",
            "            'On many systems, executing \"pip install psutil\" will install the package.',",
            "            file=sys.stderr,",
            "        )",
            "        sys.exit(1)",
            "",
            "    make_directory(\"source_root\")",
            "    make_directory(\"data_files\")",
            "",
            "    agent_source_root = __source_root__",
            "",
            "    # Populate source_root",
            "    os.chdir(\"source_root\")",
            "",
            "    shutil.copytree(make_path(agent_source_root, \"scalyr_agent\"), \"scalyr_agent\")",
            "    # We have to move __scalyr__.py up to the top of the source_root since, when running in the environment",
            "    # generated by PyInstaller, an 'import __scalyr__.py' will not look in the current directory.. it will only look",
            "    # for that module at the top of the sources_root.  Essentially, the PYTHONPATH variable only has a single",
            "    # entry in it, and it does not have '.' in it.  We leave a copy of __scalyr__.py in the original scalyr_agent",
            "    # directory because we need it there when we execute setup.py.  For the same reason, we put a copy of VERSION.",
            "    shutil.copy(convert_path(\"scalyr_agent/__scalyr__.py\"), \"__scalyr__.py\")",
            "    shutil.copy(make_path(agent_source_root, \"VERSION\"), \"VERSION\")",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"VERSION\"), convert_path(\"scalyr_agent/VERSION\"),",
            "    )",
            "",
            "    shutil.copytree(make_path(agent_source_root, \"monitors\"), \"monitors\")",
            "",
            "    os.chdir(\"monitors\")",
            "    recursively_delete_files_by_name(\"README.md\")",
            "    os.chdir(\"..\")",
            "",
            "    # Exclude certain files.",
            "    # TODO:  Should probably use MANIFEST.in to do this, but don't know the Python-fu to do this yet.",
            "    #",
            "    # Don't include the tests directories.  Also, don't include the .idea directory created by IDE.",
            "    recursively_delete_dirs_by_name(r\"\\.idea\", \"tests\")",
            "    recursively_delete_files_by_name(",
            "        r\".*\\.pyc\", r\".*\\.pyo\", r\".*\\.pyd\", r\"all_tests\\.py\", r\".*~\"",
            "    )",
            "",
            "    # Move back up to the root directory and populate the data_files.",
            "    os.chdir(\"..\")",
            "    os.chdir(\"data_files\")",
            "",
            "    # Copy the version file.  We copy it both to the root and the package root.  The package copy is done down below.",
            "",
            "    # make it VERSION.txt because PyInstaller on python 2 expects dll file named VERSION,",
            "    # and then fails with an error because of the invalid DLL loading.",
            "    shutil.copy(make_path(agent_source_root, \"VERSION\"), \"VERSION.txt\")",
            "    shutil.copy(make_path(agent_source_root, \"LICENSE.txt\"), \"LICENSE.txt\")",
            "",
            "    # Also add in build_info file",
            "    try:",
            "        write_to_file(get_build_info(), \"build_info\")",
            "    except Exception as e:",
            "        # NOTE: For now this error is not fatal in case git is not present on the system where",
            "        # we are building a package",
            "        print(\"Failed to retrieve / write build info fail: %s\" % (str(e)))",
            "",
            "    # Copy the third party licenses",
            "    shutil.copytree(",
            "        make_path(agent_source_root, \"scalyr_agent/third_party/licenses\"), \"licenses\"",
            "    )",
            "",
            "    # Copy the config file.",
            "    cat_files(",
            "        [make_path(agent_source_root, \"config/agent.json\")],",
            "        \"agent_config.tmpl\",",
            "        convert_newlines=True,",
            "    )",
            "",
            "    os.chdir(\"..\")",
            "    # We need to place a 'setup.py' here so that when we executed py2exe it finds it.",
            "    shutil.copy(make_path(agent_source_root, \"setup.py\"), \"setup.py\")",
            "",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"DESCRIPTION.rst\"),",
            "        convert_path(\"source_root/DESCRIPTION.rst\"),",
            "    )",
            "    pyinstaller_spec_path = os.path.join(",
            "        agent_source_root, \"win32\", \"scalyr-agent.spec\"",
            "    )",
            "",
            "    shutil.copy(pyinstaller_spec_path, \"scalyr-agent.spec\")",
            "",
            "    shutil.copy(",
            "        os.path.join(agent_source_root, \"win32\", \"dynamic_modules.py\"),",
            "        \"dynamic_modules.py\",",
            "    )",
            "",
            "    shutil.copy(",
            "        os.path.join(agent_source_root, \"win32\", \"wix-heat-bin-transform.xsl\"),",
            "        \"wix-heat-bin-transform.xsl\",",
            "    )",
            "",
            "    shutil.copy(",
            "        os.path.join(agent_source_root, \"win32\", \"scalyr_agent.wxs\"), \"scalyr_agent.wxs\"",
            "    )",
            "",
            "    run_command(",
            "        \"{0} -m PyInstaller scalyr-agent.spec\".format(sys.executable),",
            "        exit_on_fail=True,",
            "        command_name=\"pyinstaller\",",
            "    )",
            "",
            "    make_directory(\"Scalyr/certs\")",
            "    make_directory(\"Scalyr/logs\")",
            "    make_directory(\"Scalyr/data\")",
            "    make_directory(\"Scalyr/config/agent.d\")",
            "    os.rename(os.path.join(\"dist\", \"scalyr-agent-2\"), convert_path(\"Scalyr/bin\"))",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"win32/ScalyrShell.cmd\"),",
            "        \"Scalyr/bin/ScalyrShell.cmd\",",
            "    )",
            "",
            "    # Copy the cert files.",
            "    # AGENT-283: Certificate validation on windows seems to fail when the intermediate certs are present, skipping them",
            "    cat_files(",
            "        glob_files(make_path(agent_source_root, \"certs/*_root.pem\")),",
            "        \"Scalyr/certs/ca_certs.crt\",",
            "        convert_newlines=True,",
            "    )",
            "",
            "    # TODO: Check certificate expiration same as we do as part of tox lint target",
            "    # NOTE: This requires us to update Jenkins pipeline and other places where this script is called",
            "    # to install cryptography library",
            "",
            "    # Get ready to run wix.  Add in WIX to the PATH variable.",
            "    os.environ[\"PATH\"] = \"%s;%s\\\\bin\" % (os.getenv(\"PATH\"), os.getenv(\"WIX\"))",
            "",
            "    if variant is None:",
            "        variant = \"main\"",
            "",
            "    # Generate a unique identifier used to identify this version of the Scalyr Agent to windows.",
            "    product_code = create_scalyr_uuid3(\"ProductID:%s:%s\" % (variant, version))",
            "    # The upgrade code identifies all families of versions that can be upgraded from one to the other.  So, this",
            "    # should be a single number for all Scalyr produced ones.",
            "    upgrade_code = create_scalyr_uuid3(\"UpgradeCode:%s\" % variant)",
            "",
            "    # For prereleases, we use weird version numbers like 4.0.4.pre5.1 .  That does not work for Windows which",
            "    # requires X.X.X.X.  So, we convert if necessary.",
            "    if len(version.split(\".\")) == 5:",
            "        parts = version.split(\".\")",
            "        del parts[3]",
            "        version = \".\".join(parts)",
            "",
            "    # Gather files by 'heat' tool from WIX and generate .wxs file for 'bin' folder.",
            "    run_command(",
            "        \"heat dir Scalyr/bin -sreg -ag -cg BIN -dr APPLICATIONROOTDIRECTORY -var var.BinFolderSource -t wix-heat-bin-transform.xsl -o bin.wxs\",",
            "        exit_on_fail=True,",
            "        command_name=\"heat\",",
            "    )",
            "",
            "    run_command(",
            "        'candle -nologo -out bin.wixobj bin.wxs -dBinFolderSource=\"Scalyr/bin\"',",
            "        exit_on_fail=True,",
            "        command_name=\"candle\",",
            "    )",
            "",
            "    run_command(",
            "        'candle -nologo -out ScalyrAgent.wixobj -dVERSION=\"%s\" -dUPGRADECODE=\"%s\" '",
            "        '-dPRODUCTCODE=\"%s\" scalyr_agent.wxs' % (version, upgrade_code, product_code),",
            "        exit_on_fail=True,",
            "        command_name=\"candle\",",
            "    )",
            "",
            "    installer_name = \"ScalyrAgentInstaller-%s.msi\" % version",
            "",
            "    run_command(",
            "        \"light -nologo -ext WixUtilExtension.dll -ext WixUIExtension -out %s ScalyrAgent.wixobj bin.wixobj -v\"",
            "        % installer_name,",
            "        exit_on_fail=True,",
            "        command_name=\"light\",",
            "    )",
            "    return installer_name",
            "",
            "",
            "def create_wxs_file(template_path, dist_path, destination_path):",
            "    \"\"\"Performs a rewrite of the Wix file to replace template-like poritions with information about the",
            "    binaries/files in `dist_path`.",
            "",
            "    This is required so that our Windows installer includes all of the DLLs, Python compiled files, etc that PyInstaller",
            "    produced.  This list can change over time and is dependent on the build machine, so we cannot hard code this",
            "    list.  It must be determined dynamically.",
            "",
            "    The file is rewrite by expanding the 'templates' found between the '<!-- EXPAND_FROM_BIN' markers.  This will",
            "    make a copy of the included template, once for each file in the `dist_path`, replacing such variables as",
            "    $COMPONENT_ID, $COMPONENT_GUID, $FILE_ID, and $FILE_SOURCE with values calculated on the file's information.",
            "",
            "    You may also specify a list of files to exclude in `dist_path` from the template expansion.  This is used for",
            "    well-known files that are already in the Wix file.",
            "",
            "    Here is an example:",
            "      <!-- EXPAND_FROM_BIN EXCLUDE:scalyr-agent-2.exe,scalyr-agent-2-config.exe,ScalyrAgentService.exe -->",
            "        <Component Id='$COMPONENT_ID' Guid='$COMPONENT_GUID' >",
            "          <File Id='$FILE_ID' DiskId='1' KeyPath='yes' Checksum='yes'  Source='$FILE_SOURCE' />",
            "         </Component>",
            "      <!-- EXPAND_FROM_BIN -->",
            "",
            "    @param template_path: The file path storing the Wix file to copy/rewrite.",
            "    @param dist_path: The path to the directory containing the files that should be included in the template",
            "        expansion.",
            "    @param destination_path: The file path to write the result",
            "",
            "    @type template_path: str",
            "    @type dist_path: str",
            "    @type destination_path: str",
            "    \"\"\"",
            "    # First, calculate all of the per-file information for each file in the distribution directory.",
            "    dist_files = []",
            "    for dist_file_path in glob.glob(\"%s/*\" % dist_path):",
            "        base_file = os.path.basename(dist_file_path)",
            "        file_id = base_file.replace(\".\", \"_\").replace(\"-\", \"_\")",
            "        entry = {",
            "            \"BASE\": base_file,",
            "            \"FILE_ID\": file_id,",
            "            \"COMPONENT_GUID\": str(create_scalyr_uuid3(\"DistComp%s\" % base_file)),",
            "            \"COMPONENT_ID\": \"%s_comp\" % file_id,",
            "            \"FILE_SOURCE\": dist_file_path,",
            "        }",
            "",
            "        dist_files.append(entry)",
            "",
            "    # For the sake of easier coding, we read all of the lines of the input file into an array.",
            "    f = open(template_path)",
            "    try:",
            "        template_lines = f.readlines()",
            "    finally:",
            "        f.close()",
            "",
            "    # Now go through, looking for the markers, and when we find them, do the replacement.",
            "    result = []",
            "    while len(template_lines) > 0:",
            "        if \"<!-- EXPAND_FROM_BIN\" in template_lines[0]:",
            "            result.extend(expand_template(template_lines, dist_files))",
            "        else:",
            "            line = template_lines[0]",
            "            del template_lines[0]",
            "            result.append(line)",
            "",
            "    # Write the resulting lines out.",
            "    f = open(destination_path, \"w\")",
            "    try:",
            "        for line in result:",
            "            f.write(line)",
            "    finally:",
            "        f.close()",
            "",
            "",
            "def create_scalyr_uuid3(name):",
            "    \"\"\"",
            "    Create a UUID based on the Scalyr UUID namespace and a hash of `name`.",
            "",
            "    :param name: The name",
            "    :type name: six.text",
            "    :return: The UUID",
            "    :rtype: uuid.UUID",
            "    \"\"\"",
            "    return scalyr_util.create_uuid3(_scalyr_guid_, name)",
            "",
            "",
            "def expand_template(input_lines, dist_files):",
            "    \"\"\"Reads the template starting at the first entry in `input_lines` and generates a copy of it for each",
            "    item in `dist_files` that is not excluded.",
            "",
            "    Used by `create_wxs_file`.",
            "",
            "    This consumes the lines from the `input_lines` list.",
            "",
            "    @param input_lines: The list of input lines from the file, with the first beginning a template expansion",
            "        (should have the <!-- EXPAND_FROM_BIN pragma in it).",
            "    @param dist_files: The list of file entries from the distribution directory.  The template should be expanded",
            "        once for each entry (unless it was specifically excluded).",
            "",
            "    @type input_lines: [str]",
            "    @type dist_files:  [{}]",
            "",
            "    @return: The list of lines produced by the expansion.",
            "    @rtype: [str]",
            "    \"\"\"",
            "    # First, see if there were any files that should be excluded.  This will be in the first line, prefaced by",
            "    # EXCLUDED and a comma separated list.",
            "    match = re.search(r\"EXCLUDE:(\\S*)\", input_lines[0])",
            "    del input_lines[0]",
            "",
            "    if match is not None:",
            "        excluded_files = match.group(1).split(\",\")",
            "    else:",
            "        excluded_files = []",
            "",
            "    # Create a list of just the template.  We need to find where it ends in the input lines.",
            "    template_lines = []",
            "    found_end = False",
            "    while len(input_lines) > 0:",
            "        line = input_lines[0]",
            "        del input_lines[0]",
            "        if \"<!-- EXPAND_FROM_BIN\" in line:",
            "            found_end = True",
            "            break",
            "        else:",
            "            template_lines.append(line)",
            "",
            "    if not found_end:",
            "        raise Exception(\"Did not find termination for EXPAND_FROM_BIN\")",
            "",
            "    result = []",
            "    # Do the expansion.",
            "    for dist_entry in dist_files:",
            "        if dist_entry[\"BASE\"] in excluded_files:",
            "            continue",
            "",
            "        for template_line in template_lines:",
            "            line = template_line.replace(\"$FILE_ID\", dist_entry[\"FILE_ID\"])",
            "            line = line.replace(\"$COMPONENT_GUID\", dist_entry[\"COMPONENT_GUID\"])",
            "            line = line.replace(\"$COMPONENT_ID\", dist_entry[\"COMPONENT_ID\"])",
            "            line = line.replace(\"$FILE_SOURCE\", dist_entry[\"FILE_SOURCE\"])",
            "",
            "            result.append(line)",
            "",
            "    return result",
            "",
            "",
            "def build_common_docker_and_package_files(create_initd_link, base_configs=None):",
            "    \"\"\"Builds the common `root` system used by Debian, RPM, and container source tarballs in the current working",
            "    directory.",
            "",
            "    @param create_initd_link: Whether or not to create the link from initd to the scalyr agent binary.",
            "    @param base_configs:  The directory (relative to the top of the source tree) that contains the configuration",
            "        files to copy (such as the agent.json and agent.d directory).  If None, then will use `config`.",
            "    @type create_initd_link: bool",
            "    @type base_configs: str",
            "    \"\"\"",
            "    original_dir = os.getcwd()",
            "",
            "    # Create the directory structure for where the RPM/Debian package will place files on the system.",
            "    make_directory(\"root/etc/init.d\")",
            "    make_directory(\"root/var/log/scalyr-agent-2\")",
            "    make_directory(\"root/var/lib/scalyr-agent-2\")",
            "    make_directory(\"root/usr/share\")",
            "    make_directory(\"root/usr/sbin\")",
            "",
            "    # Place all of the import source in /usr/share/scalyr-agent-2.",
            "    os.chdir(\"root/usr/share\")",
            "",
            "    build_base_files(base_configs=base_configs)",
            "",
            "    os.chdir(\"scalyr-agent-2\")",
            "    # The build_base_files leaves the config directory in config, but we have to move it to its etc",
            "    # location.  We just rename it to the right directory.",
            "    shutil.move(",
            "        convert_path(\"config\"), make_path(original_dir, \"root/etc/scalyr-agent-2\")",
            "    )",
            "    os.chdir(original_dir)",
            "",
            "    # Make sure there is an agent.d directory regardless of the config directory we used.",
            "    make_directory(\"root/etc/scalyr-agent-2/agent.d\")",
            "",
            "    # Create the links to the appropriate commands in /usr/sbin and /etc/init.d/",
            "    if create_initd_link:",
            "        make_soft_link(",
            "            \"/usr/share/scalyr-agent-2/bin/scalyr-agent-2\",",
            "            \"root/etc/init.d/scalyr-agent-2\",",
            "        )",
            "    make_soft_link(",
            "        \"/usr/share/scalyr-agent-2/bin/scalyr-agent-2\", \"root/usr/sbin/scalyr-agent-2\"",
            "    )",
            "    make_soft_link(",
            "        \"/usr/share/scalyr-agent-2/bin/scalyr-agent-2-config\",",
            "        \"root/usr/sbin/scalyr-agent-2-config\",",
            "    )",
            "    make_soft_link(",
            "        \"/usr/share/scalyr-agent-2/bin/scalyr-switch-python\",",
            "        \"root/usr/sbin/scalyr-switch-python\",",
            "    )",
            "",
            "",
            "def build_container_builder(",
            "    variant,",
            "    version,",
            "    no_versioned_file_name,",
            "    source_tarball,",
            "    dockerfile,",
            "    base_configs,",
            "    image_name,",
            "    image_repos,",
            "    coverage_enabled=False,",
            "):",
            "    \"\"\"Builds an executable script in the current working directory that will build the container image for the various",
            "    Docker and Kubernetes targets.  This script embeds all assets it needs in it so it can be a standalone artifact.",
            "    The script is based on `docker/scripts/container_builder_base.sh`.  See that script for information on it can",
            "    be used.",
            "",
            "    @param variant: If not None, will add the specified string into the final script name. This allows for different",
            "        scripts to be built for the same type and same version.",
            "    @param version: The agent version.",
            "    @param no_versioned_file_name:  True if the version number should not be embedded in the script's file name.",
            "    @param source_tarball:  The filename for the source tarball (including the `.tar.gz` extension) that will",
            "        be built and then embedded in the artifact.  The contents of the Dockerfile will determine what this",
            "        name should be.",
            "    @param dockerfile:  The file path for the Dockerfile to embed in the script, relative to the top of the",
            "        agent source directory.",
            "    @param base_configs:  The file path for the configuration to use when building the container image, relative",
            "        to the top of the agent source directory.  This allows for different `agent.json` and `agent.d` directories",
            "        to be used for Kubernetes, docker, etc.",
            "    @param image_name:  The name for the image that is being built.  Will be used for the artifact's name.",
            "    @param image_repos:  A list of repositories that should be added as tags to the image once it is built.",
            "        Each repository will have two tags added -- one for the specific agent version and one for `latest`.",
            "    @param coverage_enabled: Path Dockerfile to run agent with enabled coverage.",
            "",
            "    @return: The file name of the built artifact.",
            "    \"\"\"",
            "    build_container_tarball(source_tarball, base_configs=base_configs)",
            "",
            "    agent_source_root = __source_root__",
            "    # Make a copy of the right Dockerfile to embed in the script.",
            "    shutil.copy(make_path(agent_source_root, dockerfile), \"Dockerfile\")",
            "    # copy requirements file with dependencies for docker builds.",
            "    shutil.copy(",
            "        make_path(agent_source_root, os.path.join(\"docker\", \"requirements.txt\")),",
            "        \"requirements.txt\",",
            "    )",
            "",
            "    if variant is None:",
            "        version_string = version",
            "    else:",
            "        version_string = \"%s.%s\" % (version, variant)",
            "",
            "    # Read the base builder script into memory",
            "    base_fp = open(",
            "        make_path(agent_source_root, \"docker/scripts/container_builder_base.sh\"), \"r\"",
            "    )",
            "    base_script = base_fp.read()",
            "    base_fp.close()",
            "",
            "    # The script has two lines defining environment variables (REPOSITORIES and TAGS) that we need to overwrite to",
            "    # set them to what we want.  We'll just do some regex replace to do that.",
            "    base_script = re.sub(",
            "        r\"\\n.*OVERRIDE_REPOSITORIES.*\\n\",",
            "        '\\nREPOSITORIES=\"%s\"\\n' % \",\".join(image_repos),",
            "        base_script,",
            "    )",
            "    base_script = re.sub(",
            "        r\"\\n.*OVERRIDE_TAGS.*\\n\",",
            "        '\\nTAGS=\"%s\"\\n' % \"%s,latest\" % version_string,",
            "        base_script,",
            "    )",
            "",
            "    if no_versioned_file_name:",
            "        output_name = image_name",
            "    else:",
            "        output_name = \"%s-%s\" % (image_name, version_string)",
            "",
            "    # Tar it up but hold the tarfile in memory.  Note, if the source tarball really becomes massive, might have to",
            "    # rethink this.",
            "    tar_out = BytesIO()",
            "    tar = tarfile.open(\"assets.tar.gz\", \"w|gz\", tar_out)",
            "",
            "    # if coverage enabled patch Dockerfile to install coverage package with pip.",
            "    if coverage_enabled:",
            "        with open(\"Dockerfile\", \"r\") as file:",
            "            data = file.read()",
            "        new_dockerfile_source = re.sub(r\"(RUN\\spip\\s.*)\", r\"\\1 coverage==4.5.4\", data)",
            "        new_dockerfile_source = re.sub(",
            "            r\"CMD .*\\n\",",
            "            'CMD [\"coverage\", \"run\", \"--branch\", \"/usr/share/scalyr-agent-2/py/scalyr_agent/agent_main.py\", '",
            "            '\"--no-fork\", \"--no-change-user\", \"start\"]',",
            "            new_dockerfile_source,",
            "        )",
            "",
            "        with open(\"Dockerfile\", \"w\") as file:",
            "            file.write(new_dockerfile_source)",
            "",
            "    tar.add(\"Dockerfile\")",
            "    tar.add(\"requirements.txt\")",
            "    tar.add(source_tarball)",
            "    tar.close()",
            "",
            "    # Write one file that has the contents of the script followed by the contents of the tarfile.",
            "    builder_fp = open(output_name, \"wb\")",
            "    builder_fp.write(base_script.encode(\"utf-8\"))",
            "    builder_fp.write(tar_out.getvalue())",
            "    builder_fp.close()",
            "",
            "    # Make the script executable.",
            "    st = os.stat(output_name)",
            "    os.chmod(output_name, st.st_mode | stat.S_IEXEC | stat.S_IXGRP)",
            "",
            "    return output_name",
            "",
            "",
            "def build_container_tarball(tarball_name, base_configs=None):",
            "    \"\"\"Builds the scalyr-agent-2 tarball for either Docker or Kubernetes in the current working directory.",
            "",
            "    @param tarball_name:  The name for the output tarball (including the `.tar.gz` extension)",
            "    @param base_configs: The directory (relative to the top of the source tree) that contains the configuration",
            "        files to copy (such as the agent.json and agent.d directory).  If None, then will use `config`.",
            "    @type tarball_name: str",
            "    @type base_configs: str",
            "",
            "    @return: The file name of the built tarball.",
            "    \"\"\"",
            "    build_common_docker_and_package_files(False, base_configs=base_configs)",
            "",
            "    # Need to create some docker specific files",
            "    make_directory(\"root/var/log/scalyr-agent-2/containers\")",
            "",
            "    # Tar it up.",
            "    tar = tarfile.open(tarball_name, \"w:gz\")",
            "    original_dir = os.getcwd()",
            "",
            "    os.chdir(\"root\")",
            "",
            "    # Do a manual walk over the contents of root so that we can use `addfile` to add the tarfile... which allows",
            "    # us to reset the owner/group to root.  This might not be that portable to Windows, but for now, Docker is mainly",
            "    # Posix.",
            "    for root, dirs, files in os.walk(\".\"):",
            "        to_copy = []",
            "        for name in dirs:",
            "            to_copy.append(os.path.join(root, name))",
            "        for name in files:",
            "            to_copy.append(os.path.join(root, name))",
            "",
            "        for x in to_copy:",
            "            file_entry = tar.gettarinfo(x)",
            "            file_entry.uname = \"root\"",
            "            file_entry.gname = \"root\"",
            "            file_entry.uid = 0",
            "            file_entry.gid = 0",
            "",
            "            if file_entry.isreg():",
            "                fp = open(file_entry.name, \"rb\")",
            "                tar.addfile(file_entry, fp)",
            "                fp.close()",
            "            else:",
            "                tar.addfile(file_entry)",
            "",
            "    os.chdir(original_dir)",
            "",
            "    tar.close()",
            "",
            "    return tarball_name",
            "",
            "",
            "def build_rpm_or_deb_package(is_rpm, variant, version):",
            "    \"\"\"Builds either an RPM or Debian package in the current working directory.",
            "",
            "    @param is_rpm: True if an RPM should be built. Otherwise a Debian package will be built.",
            "    @param variant: If not None, will add the specified string into the iteration identifier for the package. This",
            "        allows for different packages to be built for the same type and same version.",
            "    @param version: The agent version.",
            "",
            "    @return: The file name of the built package.",
            "    \"\"\"",
            "    build_common_docker_and_package_files(True)",
            "",
            "    # Create the scriplets the RPM/Debian package invokes when uninstalling or upgrading.",
            "    create_scriptlets()",
            "    # Produce the change logs that we will embed in the package, based on the CHANGELOG.md in this directory.",
            "    create_change_logs()",
            "",
            "    if is_rpm:",
            "        package_type = \"rpm\"",
            "    else:",
            "        package_type = \"deb\"",
            "",
            "    # Only change the iteration label if we need to embed a variant.",
            "    if variant is not None:",
            "        iteration_arg = \"--iteration 1.%s\" % variant",
            "    else:",
            "        iteration_arg = \"\"",
            "",
            "    description = (",
            "        \"Scalyr Agent 2 is the daemon process Scalyr customers run on their servers to collect metrics and \"",
            "        \"log files and transmit them to Scalyr.\"",
            "    )",
            "",
            "    run_command(",
            "        'fpm -s dir -a all -t %s -n \"scalyr-agent-2\" -v %s '",
            "        '  --license \"Apache 2.0\" '",
            "        \"  --vendor Scalyr %s \"",
            "        \"  --maintainer czerwin@scalyr.com \"",
            "        \"  --provides scalyr-agent-2 \"",
            "        '  --description \"%s\" '",
            "        '  --depends \"bash >= 3.2\" '",
            "        \"  --url https://www.scalyr.com \"",
            "        \"  --deb-user root \"",
            "        \"  --deb-group root \"",
            "        \"  --deb-changelog changelog-deb \"",
            "        \"  --rpm-user root \"",
            "        \"  --rpm-group root \"",
            "        \"  --rpm-changelog changelog-rpm\"",
            "        \"  --before-install preinstall.sh \"",
            "        \"  --after-install postinstall.sh \"",
            "        \"  --before-remove preuninstall.sh \"",
            "        \"  --deb-no-default-config-files \"",
            "        \"  --no-deb-auto-config-files \"",
            "        \"  --config-files /etc/scalyr-agent-2/agent.json \"",
            "        # NOTE: We leave those two files in place since they are symlinks which might have been",
            "        # updated by scalyr-switch-python and we want to leave this in place - aka make sure",
            "        # selected Python version is preserved on upgrade",
            "        \"  --config-files /usr/share/scalyr-agent-2/bin/scalyr-agent-2 \"",
            "        \"  --config-files /usr/share/scalyr-agent-2/bin/scalyr-agent-2-config \"",
            "        \"  --directories /usr/share/scalyr-agent-2 \"",
            "        \"  --directories /var/lib/scalyr-agent-2 \"",
            "        \"  --directories /var/log/scalyr-agent-2 \"",
            "        \"  -C root usr etc var\" % (package_type, version, iteration_arg, description),",
            "        exit_on_fail=True,",
            "        command_name=\"fpm\",",
            "    )",
            "",
            "    # We determine the artifact name in a little bit of loose fashion.. we just glob over the current",
            "    # directory looking for something either ending in .rpm or .deb.  There should only be one package,",
            "    # so that is fine.",
            "    if is_rpm:",
            "        files = glob.glob(\"*.rpm\")",
            "    else:",
            "        files = glob.glob(\"*.deb\")",
            "",
            "    if len(files) != 1:",
            "        raise Exception(",
            "            \"Could not find resulting rpm or debian package in the build directory.\"",
            "        )",
            "",
            "    return files[0]",
            "",
            "",
            "def build_tarball_package(variant, version, no_versioned_file_name):",
            "    \"\"\"Builds the scalyr-agent-2 tarball in the current working directory.",
            "",
            "    @param variant: If not None, will add the specified string into the final tarball name. This allows for different",
            "        tarballs to be built for the same type and same version.",
            "    @param version: The agent version.",
            "    @param no_versioned_file_name:  True if the version number should not be embedded in the artifact's file name.",
            "",
            "    @return: The file name of the built tarball.",
            "    \"\"\"",
            "    # Use build_base_files to build all of the important stuff in ./scalyr-agent-2",
            "    build_base_files()",
            "",
            "    # Build the rest of the directories required for the tarball install.  Mainly, the log and data directories",
            "    # in the tarball itself where the running process will store its state.",
            "    make_directory(\"scalyr-agent-2/data\")",
            "    make_directory(\"scalyr-agent-2/log\")",
            "    make_directory(\"scalyr-agent-2/config/agent.d\")",
            "",
            "    # Create a file named packageless.  This signals to the agent that",
            "    # this a tarball install instead of an RPM/Debian install, which changes",
            "    # the default paths for th econfig, logs, data, etc directories.  See",
            "    # configuration.py.",
            "    write_to_file(\"1\", \"scalyr-agent-2/packageless\")",
            "",
            "    if variant is None:",
            "        base_archive_name = \"scalyr-agent-%s\" % version",
            "    else:",
            "        base_archive_name = \"scalyr-agent-%s.%s\" % (version, variant)",
            "",
            "    shutil.move(\"scalyr-agent-2\", base_archive_name)",
            "",
            "    output_name = (",
            "        \"%s.tar.gz\" % base_archive_name",
            "        if not no_versioned_file_name",
            "        else \"scalyr-agent.tar.gz\"",
            "    )",
            "    # Tar it up.",
            "    tar = tarfile.open(output_name, \"w:gz\")",
            "    tar.add(base_archive_name)",
            "    tar.close()",
            "",
            "    return output_name",
            "",
            "",
            "def replace_shebang(path, new_path, new_shebang):",
            "    # type: (six.text_type, six.text_type, six.text_type) ->None",
            "    with open(path, \"r\") as f:",
            "        with open(new_path, \"w\") as newf:",
            "            # skip shebang",
            "            f.readline()",
            "            newf.write(new_shebang)",
            "            newf.write(\"\\n\")",
            "            newf.write(f.read())",
            "",
            "",
            "def build_base_files(base_configs=\"config\"):",
            "    \"\"\"Build the basic structure for a package in a new directory scalyr-agent-2 in the current working directory.",
            "",
            "    This creates scalyr-agent-2 in the current working directory and then populates it with the basic structure",
            "    required by most of the packages.",
            "",
            "    It copies the source files, the certs, the configuration directories, etc.  This will make sure to exclude",
            "    files like .pyc, .pyo, etc.",
            "",
            "    In the end, the structure will look like:",
            "      scalyr-agent-2:",
            "        py/scalyr_agent/           -- All the scalyr_agent source files",
            "        certs/ca_certs.pem         -- The trusted SSL CA root list.",
            "        config/agent.json          -- The configuration file.",
            "        bin/scalyr-agent-2         -- Symlink to the agent_main.py file to run the agent.",
            "        bin/scalyr-agent-2-config  -- Symlink to config_main.py to run the configuration tool",
            "        build_info                 -- A file containing the commit id of the latest commit included in this package,",
            "                                      the time it was built, and other information.",
            "",
            "    @param base_configs:  The directory (relative to the top of the source tree) that contains the configuration",
            "        files to copy (such as the agent.json and agent.d directory).  If None, then will use `config`.",
            "    \"\"\"",
            "    original_dir = os.getcwd()",
            "    # This will return the parent directory of this file.  We will use that to determine the path",
            "    # to files like scalyr_agent/ to copy the source files",
            "    agent_source_root = __source_root__",
            "",
            "    make_directory(\"scalyr-agent-2/py\")",
            "    os.chdir(\"scalyr-agent-2\")",
            "",
            "    make_directory(\"certs\")",
            "    make_directory(\"bin\")",
            "    make_directory(\"misc\")",
            "",
            "    # Copy the version file.  We copy it both to the root and the package root.  The package copy is done down below.",
            "    shutil.copy(make_path(agent_source_root, \"VERSION\"), \"VERSION\")",
            "",
            "    # Copy the source files.",
            "    os.chdir(\"py\")",
            "",
            "    shutil.copytree(make_path(agent_source_root, \"scalyr_agent\"), \"scalyr_agent\")",
            "    shutil.copytree(make_path(agent_source_root, \"monitors\"), \"monitors\")",
            "    os.chdir(\"monitors\")",
            "    recursively_delete_files_by_name(\"README.md\")",
            "    os.chdir(\"..\")",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"VERSION\"),",
            "        os.path.join(\"scalyr_agent\", \"VERSION\"),",
            "    )",
            "",
            "    # create copies of the agent_main.py with python2 and python3 shebang.",
            "    agent_main_path = os.path.join(agent_source_root, \"scalyr_agent\", \"agent_main.py\")",
            "    agent_main_py2_path = os.path.join(\"scalyr_agent\", \"agent_main_py2.py\")",
            "    agent_main_py3_path = os.path.join(\"scalyr_agent\", \"agent_main_py3.py\")",
            "    replace_shebang(agent_main_path, agent_main_py2_path, \"#!/usr/bin/env python2\")",
            "    replace_shebang(agent_main_path, agent_main_py3_path, \"#!/usr/bin/env python3\")",
            "    main_permissions = os.stat(agent_main_path).st_mode",
            "    os.chmod(agent_main_py2_path, main_permissions)",
            "    os.chmod(agent_main_py3_path, main_permissions)",
            "",
            "    # create copies of the config_main.py with python2 and python3 shebang.",
            "    config_main_path = os.path.join(agent_source_root, \"scalyr_agent\", \"config_main.py\")",
            "    config_main_py2_path = os.path.join(\"scalyr_agent\", \"config_main_py2.py\")",
            "    config_main_py3_path = os.path.join(\"scalyr_agent\", \"config_main_py3.py\")",
            "    replace_shebang(config_main_path, config_main_py2_path, \"#!/usr/bin/env python2\")",
            "    replace_shebang(config_main_path, config_main_py3_path, \"#!/usr/bin/env python3\")",
            "    config_permissions = os.stat(config_main_path).st_mode",
            "    os.chmod(config_main_py2_path, config_permissions)",
            "    os.chmod(config_main_py3_path, config_permissions)",
            "",
            "    # Exclude certain files.",
            "    # TODO:  Should probably use MANIFEST.in to do this, but don't know the Python-fu to do this yet.",
            "    #",
            "    # Don't include the tests directories.  Also, don't include the .idea directory created by IDE.",
            "    recursively_delete_dirs_by_name(r\"\\.idea\", \"tests\")",
            "    recursively_delete_files_by_name(",
            "        r\".*\\.pyc\", r\".*\\.pyo\", r\".*\\.pyd\", r\"all_tests\\.py\", r\".*~\"",
            "    )",
            "",
            "    os.chdir(\"..\")",
            "",
            "    # Copy the config",
            "    if base_configs is not None:",
            "        config_path = base_configs",
            "    else:",
            "        config_path = \"config\"",
            "    shutil.copytree(make_path(agent_source_root, config_path), \"config\")",
            "",
            "    # Create the trusted CA root list.",
            "    os.chdir(\"certs\")",
            "    cat_files(",
            "        glob_files(make_path(agent_source_root, \"certs/*_root.pem\")), \"ca_certs.crt\"",
            "    )",
            "    cat_files(",
            "        glob_files(make_path(agent_source_root, \"certs/*_intermediate.pem\")),",
            "        \"intermediate_certs.pem\",",
            "    )",
            "    for cert_file in glob_files(make_path(agent_source_root, \"certs/*.pem\")):",
            "        shutil.copy(cert_file, cert_file.split(\"/\")[-1])",
            "",
            "    # TODO: Check certificate expiration same as we do as part of tox lint target",
            "    # NOTE: This requires us to update Jenkins pipeline and other places where this script is called",
            "    # to install cryptography library",
            "    os.chdir(\"..\")",
            "",
            "    # Misc extra files needed for some features.",
            "    os.chdir(\"misc\")",
            "    # This docker file is needed by the `scalyr-agent-2-config --docker-create-custom-dockerfile` command.  We",
            "    # put it in all distributions (not just the docker_tarball) in case a customer creates an imagine using a package.",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"docker/Dockerfile.custom_agent_config\"),",
            "        \"Dockerfile.custom_agent_config\",",
            "    )",
            "    shutil.copy(",
            "        make_path(agent_source_root, \"docker/Dockerfile.custom_k8s_config\"),",
            "        \"Dockerfile.custom_k8s_config\",",
            "    )",
            "    os.chdir(\"..\")",
            "",
            "    # Create symlinks for the two commands",
            "    os.chdir(\"bin\")",
            "",
            "    make_soft_link(\"../py/scalyr_agent/agent_main.py\", \"scalyr-agent-2\")",
            "    make_soft_link(\"../py/scalyr_agent/config_main.py\", \"scalyr-agent-2-config\")",
            "",
            "    # add switch python version script.",
            "    shutil.copy(",
            "        os.path.join(",
            "            agent_source_root, \"installer\", \"scripts\", \"scalyr-switch-python.sh\"",
            "        ),",
            "        \"scalyr-switch-python\",",
            "    )",
            "",
            "    os.chdir(\"..\")",
            "",
            "    write_to_file(get_build_info(), \"build_info\")",
            "",
            "    os.chdir(original_dir)",
            "",
            "",
            "def make_directory(path):",
            "    \"\"\"Creates the specified directory including any parents that do not yet exist.",
            "",
            "    @param path: The path of the directory to create. This string can use a forward slash to separate path",
            "           components regardless of the separator character for this platform.  This method will perform the necessary",
            "           conversion.",
            "    \"\"\"",
            "    converted_path = convert_path(path)",
            "    try:",
            "        os.makedirs(converted_path)",
            "    except OSError as error:",
            "        if error.errno == errno.EEXIST and os.path.isdir(converted_path):",
            "            pass",
            "        else:",
            "            raise",
            "",
            "",
            "def make_path(parent_directory, path):",
            "    \"\"\"Returns the full path created by joining path to parent_directory.",
            "",
            "    This method is a convenience function because it allows path to use forward slashes",
            "    to separate path components rather than the platform's separator character.",
            "",
            "    @param parent_directory: The parent directory. This argument must use the system's separator character. This may be",
            "        None if path is relative to the current working directory.",
            "    @param path: The path to add to parent_directory. This should use forward slashes as the separator character,",
            "        regardless of the platform's character.",
            "",
            "    @return:  The path created by joining the two with using the system's separator character.",
            "    \"\"\"",
            "    if parent_directory is None and os.path.sep == \"/\":",
            "        return path",
            "",
            "    if parent_directory is None:",
            "        result = \"\"",
            "    elif path.startswith(\"/\"):",
            "        result = \"\"",
            "    else:",
            "        result = parent_directory",
            "",
            "    for path_part in path.split(\"/\"):",
            "        if len(path_part) > 0:",
            "            result = os.path.join(result, path_part)",
            "",
            "    return result",
            "",
            "",
            "def convert_path(path):",
            "    \"\"\"Converts the forward slashes in path to the platform's separator and returns the value.",
            "",
            "    @param path: The path to convert. This should use forward slashes as the separator character, regardless of the",
            "        platform's character.",
            "",
            "    @return: The path created by converting the forward slashes to the platform's separator.",
            "    \"\"\"",
            "    return make_path(None, path)",
            "",
            "",
            "def make_soft_link(source, link_path):",
            "    \"\"\"Creates a soft link at link_path to source.",
            "",
            "    @param source: The path that the link will point to. This should use a forward slash as the separator, regardless",
            "        of the platform's separator.",
            "    @param link_path: The path where the link will be created. This should use a forward slash as the separator,",
            "        regardless of the platform's separator.",
            "    \"\"\"",
            "    os.symlink(convert_path(source), convert_path(link_path))",
            "",
            "",
            "def glob_files(path):",
            "    \"\"\"Returns the paths that match the specified path glob (based on current working directory).",
            "",
            "    @param path: The path with glob wildcard characters to match. This should use a forward slash as the separator,",
            "        regardless of the platform's separator.",
            "",
            "    @return: The list of matched paths.",
            "    \"\"\"",
            "    return glob.glob(convert_path(path))",
            "",
            "",
            "def recursively_delete_dirs_by_name(*dir_names):",
            "    \"\"\"Deletes any directories that are in the current working directory or any of its children whose file names",
            "    match the specified regular expressions.",
            "",
            "    This will recursively examine all children of the current working directory.",
            "",
            "    If a directory is found that needs to be deleted, all of it and its children are deleted.",
            "",
            "    @param dir_names: A variable number of strings containing regular expressions that should match the file names of",
            "        the directories that should be deleted.",
            "    \"\"\"",
            "    # Compile the strings into actual regular expression match objects.",
            "    matchers = []",
            "    for dir_name in dir_names:",
            "        matchers.append(re.compile(dir_name))",
            "",
            "    # Walk down the file tree, top down, allowing us to prune directories as we go.",
            "    for root, dirs, files in os.walk(\".\"):",
            "        # The list of directories at the current level to delete.",
            "        to_remove = []",
            "",
            "        # Examine all directories at this level, see if any get a match",
            "        for dir_path in dirs:",
            "            remove_it = False",
            "            for matcher in matchers:",
            "                if matcher.match(dir_path):",
            "                    remove_it = True",
            "            if remove_it:",
            "                to_remove.append(dir_path)",
            "",
            "        # Go back and delete it.  Also, remove it from dirs so that we don't try to walk down it.",
            "        for remove_dir_path in to_remove:",
            "            shutil.rmtree(os.path.join(root, remove_dir_path))",
            "            dirs.remove(remove_dir_path)",
            "",
            "",
            "def recursively_delete_files_by_name(*file_names):",
            "    \"\"\"Deletes any files that are in the current working directory or any of its children whose file names",
            "    match the specified regular expressions.",
            "",
            "    This will recursively examine all children of the current working directory.",
            "",
            "    @param file_names: A variable number of strings containing regular expressions that should match the file names of",
            "        the files that should be deleted.",
            "    \"\"\"",
            "    # Compile the strings into actual regular expression match objects.",
            "    matchers = []",
            "    for file_name in file_names:",
            "        matchers.append(re.compile(file_name))",
            "",
            "    # Walk down the current directory.",
            "    for root, dirs, files in os.walk(\".\"):",
            "        # See if any of the files at this level match any of the matchers.",
            "        for file_path in files:",
            "            remove_it = False",
            "            for matcher in matchers:",
            "                if matcher.match(file_path):",
            "                    remove_it = True",
            "            # Delete it if it did match.",
            "            if remove_it:",
            "                os.unlink(os.path.join(root, file_path))",
            "",
            "",
            "def cat_files(file_paths, destination, convert_newlines=False):",
            "    \"\"\"Concatenates the contents of the specified files and writes it to a new file at destination.",
            "",
            "    @param file_paths: A list of paths for the files that should be read. The concatenating will be done in the same",
            "        order as the list.",
            "    @param destination: The path of the file to write the contents to.",
            "    @param convert_newlines: If True, the final file will use Windows newlines (i.e., CR LF).",
            "    \"\"\"",
            "    dest_fp = open(destination, \"w\")",
            "    for file_path in file_paths:",
            "        in_fp = open(file_path, \"r\")",
            "        for line in in_fp:",
            "            if convert_newlines:",
            "                line.replace(\"\\n\", \"\\r\\n\")",
            "            dest_fp.write(line)",
            "        in_fp.close()",
            "    dest_fp.close()",
            "",
            "",
            "def write_to_file(string_value, file_path):",
            "    \"\"\"Writes the specified string to a new file.",
            "",
            "    This removes trailing newlines, etc, to avoid adding an extra blank line.",
            "",
            "    @param string_value: The value to write to the file.",
            "    @param file_path: The path of the file to write to.",
            "    \"\"\"",
            "    dest_fp = open(file_path, \"w\")",
            "    dest_fp.write(string_value.rstrip())",
            "    dest_fp.write(six.ensure_text(os.linesep))",
            "    dest_fp.close()",
            "",
            "",
            "def parse_date(date_str):",
            "    \"\"\"Parses a date time string of the format MMM DD, YYYY HH:MM +ZZZZ and returns seconds past epoch.",
            "",
            "    Example of the format is: Oct 10, 2014 17:00 -0700",
            "",
            "    @param date_str: A string containing the date and time in the format described above.",
            "",
            "    @return: The number of seconds past epoch.",
            "",
            "    @raise ValueError: if there is a parsing problem.",
            "    \"\"\"",
            "    # For some reason, it was hard to parse this particular format with the existing Python libraries,",
            "    # especially when the timezone was not the same as the local time zone.  So, we have to do this the",
            "    # sort of hard way.",
            "    #",
            "    # It is a little annoying that strptime only takes Sep for September and not Sep which is more common",
            "    # in US-eng, so we cheat here and just swap it out.",
            "    adjusted = date_str.replace(\"Sept\", \"Sep\")",
            "",
            "    # Find the timezone string at the end of the string.",
            "    if re.search(r\"[\\-+]\\d\\d\\d\\d$\", adjusted) is None:",
            "        raise ValueError(",
            "            \"Value '%s' does not meet required time format of 'MMM DD, YYYY HH:MM +ZZZZ' (or \"",
            "            \"as an example, ' 'Oct 10, 2014 17:00 -0700'\" % date_str",
            "        )",
            "",
            "    # Use the existing Python string parsing calls to just parse the time and date.  We will handle the timezone",
            "    # separately.",
            "    try:",
            "        base_time = time.mktime(time.strptime(adjusted[0:-6], \"%b %d, %Y %H:%M\"))",
            "    except ValueError:",
            "        raise ValueError(",
            "            \"Value '%s' does not meet required time format of 'MMM DD, YYYY HH:MM +ZZZZ' (or \"",
            "            \"as an example, ' 'Oct 10, 2014 17:00 -0700'\" % date_str",
            "        )",
            "",
            "    # Since mktime assumes the time is in localtime, we might have a different time zone",
            "    # in tz_str, we must manually added in the difference.",
            "    # First, convert -0700 to seconds.. the second two digits are the number of hours",
            "    # and the last two are the minute of minutes.",
            "    tz_str = adjusted[-5:]",
            "    tz_offset_secs = int(tz_str[1:3]) * 3600 + int(tz_str[3:5]) * 60",
            "",
            "    if tz_str.startswith(\"-\"):",
            "        tz_offset_secs *= -1",
            "",
            "    # Determine the offset for the local timezone.",
            "    if time.daylight:",
            "        local_offset_secs = -1 * time.altzone",
            "    else:",
            "        local_offset_secs = -1 * time.timezone",
            "",
            "    base_time += local_offset_secs - tz_offset_secs",
            "    return base_time",
            "",
            "",
            "# TODO:  This code is shared with config_main.py.  We should move this into a common",
            "# utility location both commands can import it from.",
            "def run_command(command_str, exit_on_fail=True, fail_quietly=False, command_name=None):",
            "    \"\"\"Executes the specified command string returning the exit status.",
            "",
            "    @param command_str: The command to execute.",
            "    @param exit_on_fail: If True, will exit this process with a non-zero status if the command fails.",
            "    @param fail_quietly:  If True, nothing will be emitted to stderr/stdout on failure.  If this is true,",
            "        exit_on_fail will be ignored.",
            "    @param command_name: The name to use to identify the command in error output.",
            "",
            "    @return: The exist status and output string of the command.",
            "    \"\"\"",
            "    # We have to use a temporary file to hold the output to stdout and stderr.",
            "    output_file_fd, output_file = tempfile.mkstemp()",
            "",
            "    output_fp = os.fdopen(output_file_fd, \"w\")",
            "",
            "    try:",
            "        return_code = subprocess.call(",
            "            command_str, stdin=None, stderr=output_fp, stdout=output_fp, shell=True",
            "        )",
            "        output_fp.flush()",
            "",
            "        # Read the output back into a string.  We cannot use a cStringIO.StringIO buffer directly above with",
            "        # subprocess.call because that method expects fileno support which StringIO doesn't support.",
            "        output_buffer = StringIO()",
            "        input_fp = open(output_file, \"rb\")",
            "        for line in input_fp:",
            "            output_buffer.write(line.decode(\"utf-8\"))",
            "        input_fp.close()",
            "",
            "        output_str = output_buffer.getvalue()",
            "        output_buffer.close()",
            "",
            "        if return_code != 0 and not fail_quietly:",
            "            if command_name is not None:",
            "                print(",
            "                    \"Executing %s failed and returned a non-zero result of %d\"",
            "                    % (command_name, return_code,),",
            "                    file=sys.stderr,",
            "                )",
            "            else:",
            "                print(",
            "                    \"Executing the following command failed and returned a non-zero result of %d\"",
            "                    % return_code,",
            "                    file=sys.stderr,",
            "                )",
            "                print('  Command: \"%s\"' % command_str, file=sys.stderr)",
            "",
            "            print(\"The output was:\", file=sys.stderr)",
            "            print(output_str, file=sys.stderr)",
            "",
            "            if exit_on_fail:",
            "                print(\"Exiting due to failure.\", file=sys.stderr)",
            "                sys.exit(1)",
            "",
            "        if isinstance(output_str, six.binary_type):",
            "            # Ensure we return unicode type",
            "            output_str = output_str.decode(\"utf-8\")",
            "",
            "        return return_code, output_str",
            "",
            "    finally:",
            "        # Be sure to close the temporary file and delete it.",
            "        output_fp.close()",
            "        os.unlink(output_file)",
            "",
            "",
            "def create_scriptlets():",
            "    \"\"\"Copy three scriptlets required by the RPM and Debian package to the current working directory.",
            "",
            "    These are the preinstall.sh, preuninstall.sh, and postuninstall.sh scripts.",
            "    \"\"\"",
            "",
            "    scripts_path = os.path.join(__source_root__, \"installer\", \"scripts\")",
            "",
            "    shutil.copy(os.path.join(scripts_path, \"preinstall.sh\"), \"preinstall.sh\")",
            "    shutil.copy(os.path.join(scripts_path, \"preuninstall.sh\"), \"preuninstall.sh\")",
            "    shutil.copy(os.path.join(scripts_path, \"postinstall.sh\"), \"postinstall.sh\")",
            "",
            "",
            "def create_change_logs():",
            "    \"\"\"Creates the necessary change logs for both RPM and Debian based on CHANGELOG.md.",
            "",
            "    Creates two files in the current working directory named 'changelog-rpm' and 'changelog-deb'.  They",
            "    will have the same content as CHANGELOG.md but formatted by the respective standards for the different",
            "    packaging systems.",
            "    \"\"\"",
            "    # We define a helper function named print_release_notes that is used down below.",
            "    def print_release_notes(output_fp, notes, level_prefixes, level=0):",
            "        \"\"\"Emits the notes for a single release to output_fp.",
            "",
            "        @param output_fp: The file to write the notes to",
            "        @param notes: An array of strings containing the notes for the release. Some elements may be lists of strings",
            "            themselves to represent sublists. Only three levels of nested lists are allowed. This is the same format",
            "            returned by parse_change_log() method.",
            "        @param level_prefixes: The prefix to use for each of the three levels of notes.",
            "        @param level: The current level in the notes.",
            "        \"\"\"",
            "        prefix = level_prefixes[level]",
            "        for note in notes:",
            "            if isinstance(note, list):",
            "                # If a sublist, then recursively call this function, increasing the level.",
            "                print_release_notes(output_fp, note, level_prefixes, level + 1)",
            "                if level == 0:",
            "                    print(\"\", file=output_fp)",
            "            else:",
            "                # Otherwise emit the note with the prefix for this level.",
            "                print(\"%s%s\" % (prefix, note), file=output_fp)",
            "",
            "    # Handle the RPM log first.  We parse CHANGELOG.md and then emit the notes in the expected format.",
            "    fp = open(\"changelog-rpm\", \"w\")",
            "    try:",
            "        for release in parse_change_log():",
            "            date_str = time.strftime(\"%a %b %d %Y\", time.localtime(release[\"time\"]))",
            "",
            "            # RPM expects the leading line for a relesae to start with an asterisk, then have",
            "            # the name of the person doing the release, their e-mail and then the version.",
            "            print(",
            "                \"* %s %s <%s> %s\"",
            "                % (",
            "                    date_str,",
            "                    release[\"packager\"],",
            "                    release[\"packager_email\"],",
            "                    release[\"version\"],",
            "                ),",
            "                file=fp,",
            "            )",
            "            print(\"\", file=fp)",
            "            print(\"Release: %s (%s)\" % (release[\"version\"], release[\"name\"]), file=fp)",
            "            print(\"\", file=fp)",
            "            # Include the release notes, with the first level with no indent, an asterisk for the second level",
            "            # and a dash for the third.",
            "            print_release_notes(fp, release[\"notes\"], [\"\", \" * \", \"   - \"])",
            "            print(\"\", file=fp)",
            "    finally:",
            "        fp.close()",
            "",
            "    # Next, create the Debian change log.",
            "    fp = open(\"changelog-deb\", \"w\")",
            "    try:",
            "        for release in parse_change_log():",
            "            # Debian expects a leading line that starts with the package, including the version, the distribution",
            "            # urgency.  Then, anything goes until the last line for the release, which begins with two dashes.",
            "            date_str = time.strftime(",
            "                \"%a, %d %b %Y %H:%M:%S %z\", time.localtime(release[\"time\"])",
            "            )",
            "            print(",
            "                \"scalyr-agent-2 (%s) stable; urgency=low\" % release[\"version\"], file=fp",
            "            )",
            "            # Include release notes with an indented first level (using asterisk, then a dash for the next level,",
            "            # finally a plus sign.",
            "            print_release_notes(fp, release[\"notes\"], [\" * \", \"   - \", \"     + \"])",
            "            print(",
            "                \"-- %s <%s>  %s\"",
            "                % (release[\"packager\"], release[\"packager_email\"], date_str,),",
            "                file=fp,",
            "            )",
            "    finally:",
            "        fp.close()",
            "",
            "",
            "def parse_change_log():",
            "    \"\"\"Parses the contents of CHANGELOG.md and returns the content in a structured way.",
            "",
            "    @return: A list of dicts, one for each release in CHANGELOG.md.  Each release dict will have with several fields:",
            "            name:  The name of the release",
            "            version:  The version of the release",
            "            packager:  The name of the packager, such as 'Steven Czerwinski'",
            "            packager_email:  The email for the packager",
            "            time:  The seconds past epoch when the package was created",
            "            notes:  A list of strings or lists representing the notes for the release.  The list may",
            "                have elements that are strings (for a single line of notes) or lists (for a nested list under",
            "                the last string element).  Only three levels of nesting are allowed.",
            "    \"\"\"",
            "    # Some regular expressions matching what we expect to see in CHANGELOG.md.",
            "    # Each release section should start with a '##' line for major header.",
            "    release_matcher = re.compile(r'## ([\\d\\._]+) \"(.*)\"')",
            "    # The expected pattern we will include in a HTML comment to give information on the packager.",
            "    packaged_matcher = re.compile(",
            "        r\"Packaged by (.*) <(.*)> on (\\w+ \\d+, \\d+ \\d+:\\d\\d [+-]\\d\\d\\d\\d)\"",
            "    )",
            "",
            "    # Listed below are the deliminators we use to extract the structure from the changelog release",
            "    # sections.  We fix our markdown syntax to make it easier for us.",
            "    #",
            "    # Our change log will look something like this:",
            "    #",
            "    # ## 2.0.1 \"Aggravated Aardvark\"",
            "    #",
            "    # New core features:",
            "    # * Blah blah",
            "    # * Blah Blah",
            "    #   - sub point",
            "    #",
            "    # Bug fixes:",
            "    # * Blah Blah",
            "",
            "    # The deliminators, each level is marked by what pattern we should see in the next line to either",
            "    # go up a level, go down a level, or confirm it is at the same level.",
            "    section_delims = [",
            "        # First level does not have any prefix.. just plain text.",
            "        # So, the level up is the release header, which begins with '##'",
            "        # The level down is ' *'.",
            "        {",
            "            \"up\": re.compile(\"## \"),",
            "            \"down\": re.compile(r\"\\* \"),",
            "            \"same\": re.compile(r\"[^\\s\\*\\-#]\"),",
            "            \"prefix\": \"\",",
            "        },",
            "        # Second level always begins with an asterisk.",
            "        {",
            "            \"up\": re.compile(r\"[^\\s\\*\\-#]\"),",
            "            \"down\": re.compile(\"    - \"),",
            "            \"same\": re.compile(r\"\\* \"),",
            "            \"prefix\": \"* \",",
            "        },",
            "        # Third level always begins with '  -'",
            "        {",
            "            \"up\": re.compile(r\"\\* \"),",
            "            \"down\": None,",
            "            \"same\": re.compile(\"    - \"),",
            "            \"prefix\": \"    - \",",
            "        },",
            "    ]",
            "",
            "    # Helper function.",
            "    def read_section(lines, level=0):",
            "        \"\"\"Transforms the lines representing the notes for a single release into the desired nested representation.",
            "",
            "        @param lines: The lines for the notes for a release including markup. NOTE, this list must be in reverse order,",
            "            where the next line to be scanned is the last line in the list.",
            "        @param level: The nesting level that these lines are at.",
            "",
            "        @return: A list containing the notes, with nested lists as appropriate.",
            "        \"\"\"",
            "        result = []",
            "",
            "        if len(lines) == 0:",
            "            return result",
            "",
            "        while len(lines) > 0:",
            "            # Go over each line, seeing based on its content, if we should go up a nesting level, down a level,",
            "            # or just stay at the same level.",
            "            my_line = lines.pop()",
            "",
            "            # If the next line is at our same level, then just add it to our current list and continue.",
            "            if section_delims[level][\"same\"].match(my_line) is not None:",
            "                result.append(my_line[len(section_delims[level][\"prefix\"]) :])",
            "                continue",
            "",
            "            # For all other cases, someone else is going to have to look at this line, so add it back to the list.",
            "            lines.append(my_line)",
            "",
            "            # If the next line looks like it belongs any previous nesting levels, then we must have exited out of",
            "            # our current nesting level, so just return what we have gathered for this sublist.",
            "            for i in range(level + 1):",
            "                if section_delims[i][\"up\"].match(my_line) is not None:",
            "                    return result",
            "            if (",
            "                section_delims[level][\"down\"] is not None",
            "                and section_delims[level][\"down\"].match(my_line) is not None",
            "            ):",
            "                # Otherwise, it looks like the next line belongs to a sublist.  Recursively call ourselves, going",
            "                # down a level in nesting.",
            "                result.append(read_section(lines, level + 1))",
            "            else:",
            "                raise BadChangeLogFormat(",
            "                    \"Release not line did not match expect format at level %d: %s\"",
            "                    % (level, my_line)",
            "                )",
            "        return result",
            "",
            "    # Begin the real work here.  Read the change log.",
            "    change_log_fp = open(os.path.join(__source_root__, \"CHANGELOG.md\"), \"r\")",
            "",
            "    try:",
            "        # Skip over the first two lines since it should be header.",
            "        change_log_fp.readline()",
            "        change_log_fp.readline()",
            "",
            "        # Read over all the lines, eliminating the comment lines and other useless things.  Also strip out all newlines.",
            "        content = []",
            "        in_comment = False",
            "        for line in change_log_fp:",
            "            line = line.rstrip()",
            "            if len(line) == 0:",
            "                continue",
            "",
            "            # Check for a comment.. either beginning or closing.",
            "            if line == \"<!---\":",
            "                in_comment = True",
            "            elif line == \"--->\":",
            "                in_comment = False",
            "            elif packaged_matcher.match(line) is not None:",
            "                # The only thing we will pay attention to while in a comment is our packaged line.  If we see it,",
            "                # grab it.",
            "                content.append(line)",
            "            elif not in_comment:",
            "                # Keep any non-comments.",
            "                content.append(line)",
            "",
            "        change_log_fp.close()",
            "        change_log_fp = None",
            "    finally:",
            "        if change_log_fp is not None:",
            "            change_log_fp.close()",
            "",
            "    # We reverse the content list so the first lines to be read are at the end.  This way we can use pop down below.",
            "    content.reverse()",
            "",
            "    # The list of release objects",
            "    releases = []",
            "",
            "    # The rest of the log should just contain release notes for each release.  Iterate over the content,",
            "    # reading out the release notes for each release.",
            "    while len(content) > 0:",
            "        # Each release must begin with at least two lines -- one for the release name and then one for the",
            "        # 'Packaged by Steven Czerwinski on... ' line that we pulled out of the HTML comment.",
            "        if len(content) < 2:",
            "            raise BadChangeLogFormat(",
            "                \"New release section does not contain at least two lines.\"",
            "            )",
            "",
            "        # Extract the information from each of those two lines.",
            "        current_line = content.pop()",
            "        release_version_name = release_matcher.match(current_line)",
            "        if release_version_name is None:",
            "            raise BadChangeLogFormat(",
            "                \"Header line for release did not match expected format: %s\"",
            "                % current_line",
            "            )",
            "",
            "        current_line = content.pop()",
            "        packager_info = packaged_matcher.match(current_line)",
            "        if packager_info is None:",
            "            raise BadChangeLogFormat(",
            "                \"Packager line for release did not match expected format: %s\"",
            "                % current_line",
            "            )",
            "",
            "        # Read the section notes until we hit a '##' line.",
            "        release_notes = read_section(content)",
            "",
            "        try:",
            "            time_value = parse_date(packager_info.group(3))",
            "        except ValueError as err:",
            "            message = getattr(err, \"message\", str(err))",
            "            raise BadChangeLogFormat(message)",
            "",
            "        releases.append(",
            "            {",
            "                \"name\": release_version_name.group(2),",
            "                \"version\": release_version_name.group(1),",
            "                \"packager\": packager_info.group(1),",
            "                \"packager_email\": packager_info.group(2),",
            "                \"time\": time_value,",
            "                \"notes\": release_notes,",
            "            }",
            "        )",
            "",
            "    return releases",
            "",
            "",
            "# A string containing the build info for this build, to be placed in the 'build_info' file.",
            "__build_info__ = None",
            "",
            "",
            "def get_build_info():",
            "    \"\"\"Returns a string containing the build info.\"\"\"",
            "    global __build_info__",
            "    if __build_info__ is not None:",
            "        return __build_info__",
            "",
            "    build_info_buffer = StringIO()",
            "    original_dir = os.getcwd()",
            "",
            "    try:",
            "        # We need to execute the git command in the source root.",
            "        os.chdir(__source_root__)",
            "        # Add in the e-mail address of the user building it.",
            "        (rc, packager_email) = run_command(",
            "            \"git config user.email\", fail_quietly=True, command_name=\"git\"",
            "        )",
            "        if rc != 0:",
            "            packager_email = \"unknown\"",
            "",
            "        print(\"Packaged by: %s\" % packager_email.strip(), file=build_info_buffer)",
            "",
            "        # Determine the last commit from the log.",
            "        (_, commit_id) = run_command(",
            "            \"git log --summary -1 | head -n 1 | cut -d ' ' -f 2\",",
            "            exit_on_fail=True,",
            "            command_name=\"git\",",
            "        )",
            "        print(\"Latest commit: %s\" % commit_id.strip(), file=build_info_buffer)",
            "",
            "        # Include the branch just for safety sake.",
            "        (_, branch) = run_command(",
            "            \"git branch | cut -d ' ' -f 2\", exit_on_fail=True, command_name=\"git\"",
            "        )",
            "        print(\"From branch: %s\" % branch.strip(), file=build_info_buffer)",
            "",
            "        # Add a timestamp.",
            "        print(",
            "            \"Build time: %s\"",
            "            % six.text_type(strftime(\"%Y-%m-%d %H:%M:%S UTC\", gmtime())),",
            "            file=build_info_buffer,",
            "        )",
            "",
            "        __build_info__ = build_info_buffer.getvalue()",
            "        return __build_info__",
            "    finally:",
            "        os.chdir(original_dir)",
            "",
            "        if build_info_buffer is not None:",
            "            build_info_buffer.close()",
            "",
            "",
            "def set_build_info(build_info_file_path):",
            "    \"\"\"Sets the file to use as the build_info file to include in the package.",
            "",
            "    If this is called, then future calls to get_build_info will return the contents of this file",
            "    and will not use other commands such as 'git' to try to create it on its own.",
            "",
            "    This is useful when you are running trying create a package on a system that does not have full access",
            "    to git.",
            "",
            "    @param build_info_file_path: The path to the build_info file to use.",
            "    \"\"\"",
            "    global __build_info__",
            "    fp = open(build_info_file_path, \"r\")",
            "    __build_info__ = fp.read()",
            "    fp.close()",
            "",
            "    return __build_info__",
            "",
            "",
            "class BadChangeLogFormat(Exception):",
            "    pass",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    parser = OptionParser(",
            "        usage=\"Usage: python build_package.py [options] %s\" % \"|\".join(PACKAGE_TYPES)",
            "    )",
            "    parser.add_option(",
            "        \"-v\",",
            "        \"--variant\",",
            "        dest=\"variant\",",
            "        default=None,",
            "        help=\"An optional string that is included in the package name to identify a variant \"",
            "        \"of the main release created by a different packager.  \"",
            "        \"Most users do not need to use this option.\",",
            "    )",
            "    parser.add_option(",
            "        \"\",",
            "        \"--only-create-build-info\",",
            "        action=\"store_true\",",
            "        dest=\"build_info_only\",",
            "        default=False,",
            "        help=\"If true, will only create the build_info file and exit.  This can be used in conjunction \"",
            "        \"with the --set-build-info option to create the build_info file on one host and then build the \"",
            "        \"rest of the package on another.  This is useful when the final host does not have full access \"",
            "        \"to git\",",
            "    )",
            "",
            "    parser.add_option(",
            "        \"\",",
            "        \"--no-versioned-file-name\",",
            "        action=\"store_true\",",
            "        dest=\"no_versioned_file_name\",",
            "        default=False,",
            "        help=\"If true, will not embed the version number in the artifact's file name.  This only \"",
            "        \"applies to the `tarball` and container builders artifacts.\",",
            "    )",
            "",
            "    parser.add_option(",
            "        \"\",",
            "        \"--set-build-info\",",
            "        dest=\"build_info\",",
            "        default=None,",
            "        help=\"The path to the build_info file to include in the final package.  If this is used, \"",
            "        \"this process will not invoke commands such as git in order to compute the build information \"",
            "        \"itself.  The file should be one built by a previous run of this script.\",",
            "    )",
            "",
            "    parser.add_option(",
            "        \"\",",
            "        \"--coverage\",",
            "        dest=\"coverage\",",
            "        action=\"store_true\",",
            "        default=False,",
            "        help=\"Enable coverage analysis. Can be used in smoketests. Only works with docker/k8s.\",",
            "    )",
            "",
            "    (options, args) = parser.parse_args()",
            "    # If we are just suppose to create the build_info, then do it and exit.  We do not bother to check to see",
            "    # if they specified a package.",
            "    if options.build_info_only:",
            "        write_to_file(get_build_info(), \"build_info\")",
            "        print(\"Built build_info\")",
            "        sys.exit(0)",
            "",
            "    if len(args) < 1:",
            "        print(",
            "            \"You must specify the package you wish to build, one of the following: %s.\"",
            "            % \", \".join(PACKAGE_TYPES),",
            "            file=sys.stderr,",
            "        )",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "    elif len(args) > 1:",
            "        print(\"You may only specify one package to build.\", file=sys.stderr)",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "    elif args[0] not in PACKAGE_TYPES:",
            "        print('Unknown package type given: \"%s\"' % args[0], file=sys.stderr)",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "",
            "    if options.build_info is not None:",
            "        set_build_info(options.build_info)",
            "",
            "    artifact = build_package(",
            "        args[0], options.variant, options.no_versioned_file_name, options.coverage,",
            "    )",
            "    print(\"Built %s\" % artifact)",
            "    sys.exit(0)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "257": [
                "build_win32_installer_package"
            ],
            "258": [
                "build_win32_installer_package"
            ],
            "259": [
                "build_win32_installer_package"
            ],
            "260": [
                "build_win32_installer_package"
            ]
        },
        "addLocation": []
    },
    "scalyr_agent/__scalyr__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " import sys"
            },
            "1": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " from io import open"
            },
            "2": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+PY2 = sys.version_info[0] == 2"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+PY3 = sys.version_info[0] == 3"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+PY2_pre_279 = PY2 and sys.version_info < (2, 7, 9)"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+PY3_pre_32 = PY3 and sys.version_info < (3, 2)"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " # One of the main things this file does is correctly give the full path to two key directories regardless of install"
            },
            "9": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 49,
                "PatchRowcode": " # type :"
            },
            "10": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " #   package_root:  The directory containing the Scalyr source (contains files like 'agent_main.py', etc)"
            },
            "11": {
                "beforePatchRowNumber": 175,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "         sys.path.insert(0, os.path.join(get_package_root(), \"third_party_python3\"))"
            },
            "12": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 181,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "     # if we are not on windows, prepend the third party tls directory first so it appears after the package root"
            },
            "14": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not __is_frozen__:"
            },
            "15": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sys.path.insert(0, os.path.join(get_package_root(), \"third_party_tls\"))"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+    if not __is_frozen__ and (PY2_pre_279 or PY3_pre_32):"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+        # Special case for backports module which is a multiple package module so we also need to"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+        # add sub directory to pack when bundling a package and not installing it using setup.py"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+        sys.path.insert("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+            0,"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+            os.path.join("
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+                get_package_root(), \"third_party_tls/backports_ssl_match_hostname\""
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+            ),"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+        )"
            },
            "25": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 192,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 193,
                "PatchRowcode": "     sys.path.insert(0, os.path.dirname(get_package_root()))"
            },
            "27": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 194,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright 2014 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ------------------------------------------------------------------------",
            "#",
            "# author: Steven Czerwinski <czerwin@scalyr.com>",
            "",
            "from __future__ import absolute_import",
            "",
            "__author__ = \"czerwin@scalyr.com\"",
            "",
            "# [start of 2->TODO]",
            "# \"Modernize\" tool added \"six\" library as a dependency in this file.",
            "# But in case of absence of six in site-packages we can not import \"six\" before scalyr_init.",
            "# The first option is to provide 2->3 compatibility without \"six\". This is easy for now,",
            "# because there is only one incompatible piece of code here.",
            "# and it can be fixed in code below...",
            "try:",
            "    # Python2",
            "    text_type = unicode  # type: ignore",
            "except NameError:",
            "    # Python3",
            "    text_type = str",
            "# The second option is to assure that \"six\" library installed in current python environment.",
            "# [end of 2->TOD0]",
            "",
            "",
            "import inspect",
            "import os",
            "import sys",
            "from io import open",
            "",
            "# One of the main things this file does is correctly give the full path to two key directories regardless of install",
            "# type :",
            "#   package_root:  The directory containing the Scalyr source (contains files like 'agent_main.py', etc)",
            "#   install_root:  The top-level directory where Scalyr is currently installed (contains files like 'CHANGELOG.md')",
            "#",
            "# There are several different ways we can be running, and this files has to give the correct values for each of them.",
            "# For example, we could just be running out of the source tree as checked into github.  Or we can be running",
            "# out of an RPM.  Or as part of a single Windows executable created by PyInstaller.  We handle of these cases.",
            "#",
            "# Example layouts for different install types:",
            "#",
            "# Running from source:",
            "#     ~/scalyr-agent-2/VERSION",
            "#     ~/scalyr-agent-2/scalyr-agent/__scalyr__.py",
            "#     ~/scalyr-agent-2/scalyr-agent/third_party",
            "#",
            "#   Here the install root is ~/scalyr-agent-2 and the package root is ~/scalyr-agent-2/scalyr-agent",
            "#",
            "# Install using tarball:",
            "#     ~/scalyr-agent-2/py/scalyr_agent/VERSION",
            "#     ~/scalyr-agent-2/py/scalyr_agent/__scalyr__py",
            "#     ~/scalyr-agent-2/py/scalyr_agent/third_party",
            "#",
            "#   Here the install root is ~/scalyr-agent-2 and the package root is ~/scalyr-agent-2/py/scalyr-agent",
            "#",
            "# Install using rpm/deb package:",
            "#     /usr/share/scalyr-agent-2/py/scalyr_agent/VERSION",
            "#     /usr/share/scalyr-agent-2/py/scalyr_agent/__scalyr__py",
            "#     /usr/share/scalyr-agent-2/py/scalyr_agent/third_party",
            "#",
            "#   Here the install root is /usr/share/scalyr-agent-2 and the package root is /usr/share/scalyr-agent-2/py/scalyr-agent",
            "#",
            "# Install using win32 exe:",
            "#     C:\\Program Files (x86)\\Scalyr\\program_files\\VERSION",
            "#     C:\\Program Files (x86)\\Scalyr\\program_files\\__scalyr__.py",
            "#     (There is no third party directory... its contents gets added directly to program_files",
            "#",
            "#   Here the install root is C:\\Program Files (x86)\\Scalyr\\ and the package root is",
            "#   C:\\Program Files (x86)\\Scalyr\\program_files\\",
            "",
            "# Indicates if this code was compiled into a single Windows executable via PyInstaller.  If that's the case,",
            "# then we cannot rely on __file__ and the source is kind of through into the same directory.",
            "__is_frozen__ = hasattr(sys, \"frozen\")",
            "",
            "",
            "def scalyr_init():",
            "    \"\"\"Initializes the environment to execute a Scalyr script.",
            "",
            "    This should be invoked by any Scalyr module that has a main (i.e., can invoked by the commandline to",
            "    perform some action).",
            "",
            "    It performs such tasks as ensures PYTHONPATH include the Scalyr package and fixing some third-party import issues.",
            "    \"\"\"",
            "    # If this is a win32 executable, then all the packages have already been bundled in the exec and there is no",
            "    # need to change the PYTHONPATH",
            "    if not __is_frozen__:",
            "        __add_scalyr_package_to_path()",
            "",
            "",
            "def __determine_package_root():",
            "    \"\"\"Returns the absolute file path to the package root.",
            "",
            "    This must be invoked before the current working directory is changed by the code, so therefore should be",
            "    invoked during the module load.",
            "",
            "    @return: The absolute file path for the package root.",
            "    \"\"\"",
            "    # We rely on the fact this file (__scalyr__.py) should be in the directory that is the package root.",
            "    # We could just return the parent of __file__, however, this apparently is not portable on all version of",
            "    # Windows.  Moreover, when running as a win32 exe, __file__ is not set.",
            "    if not __is_frozen__:",
            "        base = os.getcwd()",
            "        file_path = inspect.stack()[1][1]",
            "        if not os.path.isabs(file_path):",
            "            file_path = os.path.join(base, file_path)",
            "        file_path = os.path.dirname(os.path.realpath(file_path))",
            "    else:",
            "        # encode python executable path for python 2.",
            "        executable_path = sys.executable",
            "        if type(executable_path) != text_type:",
            "            executable_path = text_type(executable_path, sys.getfilesystemencoding())",
            "        return os.path.dirname(executable_path)",
            "",
            "    return file_path",
            "",
            "",
            "__package_root__ = __determine_package_root()",
            "",
            "",
            "def get_package_root():",
            "    \"\"\"Returns the absolute path to the scalyr_agent Python package, including the scalyr_agent directory name.",
            "",
            "    @return: The path to the scalyr_agent directory (which contains the Python package).",
            "    @rtype: six.text_type",
            "    \"\"\"",
            "    return __package_root__",
            "",
            "",
            "def get_install_root():",
            "    \"\"\"Returns the absolute path to the root of the install location to scalyr-agent-2.  This",
            "    works for the different types of installation such as RPM and Debian, as well as when this",
            "    is running from the source tree.",
            "",
            "    For example, it will return '/usr/share/scalyr-agent-2', for Linux installs and",
            "    the top of the repository when running from the source tree.",
            "",
            "    @return:  The path to the scalyr-agent-2 directory.",
            "    @rtype: six.text_type",
            "    \"\"\"",
            "    # See the listed cases above.  From that, it should be clear that these rules work for the different cases.",
            "    parent_of_package_install = os.path.dirname(get_package_root())",
            "    if __is_frozen__:  # win32 install",
            "        return parent_of_package_install",
            "    elif os.path.basename(parent_of_package_install) != \"py\":  # Running from Source",
            "        return parent_of_package_install",
            "    else:  # Installed using tarball or rpm/debian package",
            "        return os.path.dirname(parent_of_package_install)",
            "",
            "",
            "def __add_scalyr_package_to_path():",
            "    \"\"\"Adds the path for the scalyr package and embedded third party packages to the PYTHONPATH.",
            "",
            "    If you add any new paths in this method, be sure to add them near the top of `setup.py` as well so as not",
            "    to break the Windows builds.",
            "    \"\"\"",
            "    # prepend the third party directory first so it appears after the package root, third_party_pythonX",
            "    # and third_party_tls directories",
            "    sys.path.insert(0, os.path.join(get_package_root(), \"third_party\"))",
            "",
            "    if sys.version_info[0] == 2:",
            "        sys.path.insert(0, os.path.join(get_package_root(), \"third_party_python2\"))",
            "    else:",
            "        sys.path.insert(0, os.path.join(get_package_root(), \"third_party_python3\"))",
            "",
            "    # if we are not on windows, prepend the third party tls directory first so it appears after the package root",
            "    if not __is_frozen__:",
            "        sys.path.insert(0, os.path.join(get_package_root(), \"third_party_tls\"))",
            "",
            "    sys.path.insert(0, os.path.dirname(get_package_root()))",
            "",
            "",
            "def __determine_version():",
            "    \"\"\"Returns the agent version number, read from the VERSION file.",
            "    \"\"\"",
            "",
            "    file_names = [\"VERSION\"]",
            "",
            "    if __is_frozen__:",
            "        # also check for VERSION.txt file because there is a reserved filename - \"VERSION\" in Pyinstaller,",
            "        # and it expects that this file is a DLL.",
            "        file_names.append(\"VERSION.txt\")",
            "",
            "    def find_path():",
            "        # This file can be either in the package root or the install root (if you examine the cases",
            "        # from above).  So, just check both locations.",
            "        for root in [get_install_root(), get_package_root()]:",
            "            for file_name in file_names:",
            "                path = os.path.join(root, file_name)",
            "                if os.path.isfile(path):",
            "                    return path",
            "",
            "        raise Exception(\"Could not locate VERSION file!\")",
            "",
            "    version_path = find_path()",
            "",
            "    version_fp = open(version_path, \"r\")",
            "    try:",
            "        return version_fp.readline().strip()",
            "    finally:",
            "        version_fp.close()",
            "",
            "",
            "SCALYR_VERSION = __determine_version()",
            "",
            "",
            "# The constants for INSTALL_TYPE, a variable declared down below.",
            "PACKAGE_INSTALL = 1  # Indicates source code was installed via a package manager such as RPM or Windows executable.",
            "TARBALL_INSTALL = 2  # Indicates source code was installed via a tarball created by the build_package.py script.",
            "DEV_INSTALL = 3  # Indicates source code is running out of the original source tree, usually during dev testing.",
            "MSI_INSTALL = 4  # Indicates source code was installed via a Windows MSI package",
            "",
            "",
            "def __determine_install_type():",
            "    \"\"\"Returns the type of install that was used for the source currently running.",
            "",
            "    @return: The install type, drawn from the constants above.",
            "    @rtype: int",
            "    \"\"\"",
            "    # Determine which type of install this is.  We do this based on",
            "    # whether or not certain files exist in the root of the source tree.",
            "    install_root = get_install_root()",
            "    if os.path.exists(os.path.join(install_root, \"packageless\")):",
            "        install_type = TARBALL_INSTALL",
            "    elif os.path.exists(os.path.join(install_root, \"run_tests.py\")):",
            "        install_type = DEV_INSTALL",
            "    elif hasattr(sys, \"frozen\"):",
            "        install_type = MSI_INSTALL",
            "    else:",
            "        install_type = PACKAGE_INSTALL",
            "    return install_type",
            "",
            "",
            "# Holds which type of installation we are currently running from.",
            "INSTALL_TYPE = __determine_install_type()"
        ],
        "afterPatchFile": [
            "# Copyright 2014 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ------------------------------------------------------------------------",
            "#",
            "# author: Steven Czerwinski <czerwin@scalyr.com>",
            "",
            "from __future__ import absolute_import",
            "",
            "__author__ = \"czerwin@scalyr.com\"",
            "",
            "# [start of 2->TODO]",
            "# \"Modernize\" tool added \"six\" library as a dependency in this file.",
            "# But in case of absence of six in site-packages we can not import \"six\" before scalyr_init.",
            "# The first option is to provide 2->3 compatibility without \"six\". This is easy for now,",
            "# because there is only one incompatible piece of code here.",
            "# and it can be fixed in code below...",
            "try:",
            "    # Python2",
            "    text_type = unicode  # type: ignore",
            "except NameError:",
            "    # Python3",
            "    text_type = str",
            "# The second option is to assure that \"six\" library installed in current python environment.",
            "# [end of 2->TOD0]",
            "",
            "",
            "import inspect",
            "import os",
            "import sys",
            "from io import open",
            "",
            "PY2 = sys.version_info[0] == 2",
            "PY3 = sys.version_info[0] == 3",
            "PY2_pre_279 = PY2 and sys.version_info < (2, 7, 9)",
            "PY3_pre_32 = PY3 and sys.version_info < (3, 2)",
            "",
            "# One of the main things this file does is correctly give the full path to two key directories regardless of install",
            "# type :",
            "#   package_root:  The directory containing the Scalyr source (contains files like 'agent_main.py', etc)",
            "#   install_root:  The top-level directory where Scalyr is currently installed (contains files like 'CHANGELOG.md')",
            "#",
            "# There are several different ways we can be running, and this files has to give the correct values for each of them.",
            "# For example, we could just be running out of the source tree as checked into github.  Or we can be running",
            "# out of an RPM.  Or as part of a single Windows executable created by PyInstaller.  We handle of these cases.",
            "#",
            "# Example layouts for different install types:",
            "#",
            "# Running from source:",
            "#     ~/scalyr-agent-2/VERSION",
            "#     ~/scalyr-agent-2/scalyr-agent/__scalyr__.py",
            "#     ~/scalyr-agent-2/scalyr-agent/third_party",
            "#",
            "#   Here the install root is ~/scalyr-agent-2 and the package root is ~/scalyr-agent-2/scalyr-agent",
            "#",
            "# Install using tarball:",
            "#     ~/scalyr-agent-2/py/scalyr_agent/VERSION",
            "#     ~/scalyr-agent-2/py/scalyr_agent/__scalyr__py",
            "#     ~/scalyr-agent-2/py/scalyr_agent/third_party",
            "#",
            "#   Here the install root is ~/scalyr-agent-2 and the package root is ~/scalyr-agent-2/py/scalyr-agent",
            "#",
            "# Install using rpm/deb package:",
            "#     /usr/share/scalyr-agent-2/py/scalyr_agent/VERSION",
            "#     /usr/share/scalyr-agent-2/py/scalyr_agent/__scalyr__py",
            "#     /usr/share/scalyr-agent-2/py/scalyr_agent/third_party",
            "#",
            "#   Here the install root is /usr/share/scalyr-agent-2 and the package root is /usr/share/scalyr-agent-2/py/scalyr-agent",
            "#",
            "# Install using win32 exe:",
            "#     C:\\Program Files (x86)\\Scalyr\\program_files\\VERSION",
            "#     C:\\Program Files (x86)\\Scalyr\\program_files\\__scalyr__.py",
            "#     (There is no third party directory... its contents gets added directly to program_files",
            "#",
            "#   Here the install root is C:\\Program Files (x86)\\Scalyr\\ and the package root is",
            "#   C:\\Program Files (x86)\\Scalyr\\program_files\\",
            "",
            "# Indicates if this code was compiled into a single Windows executable via PyInstaller.  If that's the case,",
            "# then we cannot rely on __file__ and the source is kind of through into the same directory.",
            "__is_frozen__ = hasattr(sys, \"frozen\")",
            "",
            "",
            "def scalyr_init():",
            "    \"\"\"Initializes the environment to execute a Scalyr script.",
            "",
            "    This should be invoked by any Scalyr module that has a main (i.e., can invoked by the commandline to",
            "    perform some action).",
            "",
            "    It performs such tasks as ensures PYTHONPATH include the Scalyr package and fixing some third-party import issues.",
            "    \"\"\"",
            "    # If this is a win32 executable, then all the packages have already been bundled in the exec and there is no",
            "    # need to change the PYTHONPATH",
            "    if not __is_frozen__:",
            "        __add_scalyr_package_to_path()",
            "",
            "",
            "def __determine_package_root():",
            "    \"\"\"Returns the absolute file path to the package root.",
            "",
            "    This must be invoked before the current working directory is changed by the code, so therefore should be",
            "    invoked during the module load.",
            "",
            "    @return: The absolute file path for the package root.",
            "    \"\"\"",
            "    # We rely on the fact this file (__scalyr__.py) should be in the directory that is the package root.",
            "    # We could just return the parent of __file__, however, this apparently is not portable on all version of",
            "    # Windows.  Moreover, when running as a win32 exe, __file__ is not set.",
            "    if not __is_frozen__:",
            "        base = os.getcwd()",
            "        file_path = inspect.stack()[1][1]",
            "        if not os.path.isabs(file_path):",
            "            file_path = os.path.join(base, file_path)",
            "        file_path = os.path.dirname(os.path.realpath(file_path))",
            "    else:",
            "        # encode python executable path for python 2.",
            "        executable_path = sys.executable",
            "        if type(executable_path) != text_type:",
            "            executable_path = text_type(executable_path, sys.getfilesystemencoding())",
            "        return os.path.dirname(executable_path)",
            "",
            "    return file_path",
            "",
            "",
            "__package_root__ = __determine_package_root()",
            "",
            "",
            "def get_package_root():",
            "    \"\"\"Returns the absolute path to the scalyr_agent Python package, including the scalyr_agent directory name.",
            "",
            "    @return: The path to the scalyr_agent directory (which contains the Python package).",
            "    @rtype: six.text_type",
            "    \"\"\"",
            "    return __package_root__",
            "",
            "",
            "def get_install_root():",
            "    \"\"\"Returns the absolute path to the root of the install location to scalyr-agent-2.  This",
            "    works for the different types of installation such as RPM and Debian, as well as when this",
            "    is running from the source tree.",
            "",
            "    For example, it will return '/usr/share/scalyr-agent-2', for Linux installs and",
            "    the top of the repository when running from the source tree.",
            "",
            "    @return:  The path to the scalyr-agent-2 directory.",
            "    @rtype: six.text_type",
            "    \"\"\"",
            "    # See the listed cases above.  From that, it should be clear that these rules work for the different cases.",
            "    parent_of_package_install = os.path.dirname(get_package_root())",
            "    if __is_frozen__:  # win32 install",
            "        return parent_of_package_install",
            "    elif os.path.basename(parent_of_package_install) != \"py\":  # Running from Source",
            "        return parent_of_package_install",
            "    else:  # Installed using tarball or rpm/debian package",
            "        return os.path.dirname(parent_of_package_install)",
            "",
            "",
            "def __add_scalyr_package_to_path():",
            "    \"\"\"Adds the path for the scalyr package and embedded third party packages to the PYTHONPATH.",
            "",
            "    If you add any new paths in this method, be sure to add them near the top of `setup.py` as well so as not",
            "    to break the Windows builds.",
            "    \"\"\"",
            "    # prepend the third party directory first so it appears after the package root, third_party_pythonX",
            "    # and third_party_tls directories",
            "    sys.path.insert(0, os.path.join(get_package_root(), \"third_party\"))",
            "",
            "    if sys.version_info[0] == 2:",
            "        sys.path.insert(0, os.path.join(get_package_root(), \"third_party_python2\"))",
            "    else:",
            "        sys.path.insert(0, os.path.join(get_package_root(), \"third_party_python3\"))",
            "",
            "    # if we are not on windows, prepend the third party tls directory first so it appears after the package root",
            "    if not __is_frozen__ and (PY2_pre_279 or PY3_pre_32):",
            "        # Special case for backports module which is a multiple package module so we also need to",
            "        # add sub directory to pack when bundling a package and not installing it using setup.py",
            "        sys.path.insert(",
            "            0,",
            "            os.path.join(",
            "                get_package_root(), \"third_party_tls/backports_ssl_match_hostname\"",
            "            ),",
            "        )",
            "",
            "    sys.path.insert(0, os.path.dirname(get_package_root()))",
            "",
            "",
            "def __determine_version():",
            "    \"\"\"Returns the agent version number, read from the VERSION file.",
            "    \"\"\"",
            "",
            "    file_names = [\"VERSION\"]",
            "",
            "    if __is_frozen__:",
            "        # also check for VERSION.txt file because there is a reserved filename - \"VERSION\" in Pyinstaller,",
            "        # and it expects that this file is a DLL.",
            "        file_names.append(\"VERSION.txt\")",
            "",
            "    def find_path():",
            "        # This file can be either in the package root or the install root (if you examine the cases",
            "        # from above).  So, just check both locations.",
            "        for root in [get_install_root(), get_package_root()]:",
            "            for file_name in file_names:",
            "                path = os.path.join(root, file_name)",
            "                if os.path.isfile(path):",
            "                    return path",
            "",
            "        raise Exception(\"Could not locate VERSION file!\")",
            "",
            "    version_path = find_path()",
            "",
            "    version_fp = open(version_path, \"r\")",
            "    try:",
            "        return version_fp.readline().strip()",
            "    finally:",
            "        version_fp.close()",
            "",
            "",
            "SCALYR_VERSION = __determine_version()",
            "",
            "",
            "# The constants for INSTALL_TYPE, a variable declared down below.",
            "PACKAGE_INSTALL = 1  # Indicates source code was installed via a package manager such as RPM or Windows executable.",
            "TARBALL_INSTALL = 2  # Indicates source code was installed via a tarball created by the build_package.py script.",
            "DEV_INSTALL = 3  # Indicates source code is running out of the original source tree, usually during dev testing.",
            "MSI_INSTALL = 4  # Indicates source code was installed via a Windows MSI package",
            "",
            "",
            "def __determine_install_type():",
            "    \"\"\"Returns the type of install that was used for the source currently running.",
            "",
            "    @return: The install type, drawn from the constants above.",
            "    @rtype: int",
            "    \"\"\"",
            "    # Determine which type of install this is.  We do this based on",
            "    # whether or not certain files exist in the root of the source tree.",
            "    install_root = get_install_root()",
            "    if os.path.exists(os.path.join(install_root, \"packageless\")):",
            "        install_type = TARBALL_INSTALL",
            "    elif os.path.exists(os.path.join(install_root, \"run_tests.py\")):",
            "        install_type = DEV_INSTALL",
            "    elif hasattr(sys, \"frozen\"):",
            "        install_type = MSI_INSTALL",
            "    else:",
            "        install_type = PACKAGE_INSTALL",
            "    return install_type",
            "",
            "",
            "# Holds which type of installation we are currently running from.",
            "INSTALL_TYPE = __determine_install_type()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "178": [
                "__add_scalyr_package_to_path"
            ],
            "179": [
                "__add_scalyr_package_to_path"
            ]
        },
        "addLocation": []
    },
    "scalyr_agent/agent_main.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " import sys"
            },
            "1": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " import time"
            },
            "2": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 46,
                "PatchRowcode": " import re"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+import ssl"
            },
            "4": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " from io import open"
            },
            "5": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 49,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " try:"
            },
            "7": {
                "beforePatchRowNumber": 974,
                "afterPatchRowNumber": 975,
                "PatchRowcode": "                 # 2->TODO it was very helpful to see what python version does agent run on. Maybe we can keep it?"
            },
            "8": {
                "beforePatchRowNumber": 975,
                "afterPatchRowNumber": 976,
                "PatchRowcode": "                 python_version_str = sys.version.replace(\"\\n\", \"\")"
            },
            "9": {
                "beforePatchRowNumber": 976,
                "afterPatchRowNumber": 977,
                "PatchRowcode": "                 build_revision = get_build_revision()"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 978,
                "PatchRowcode": "+                openssl_version = getattr(ssl, \"OPENSSL_VERSION\", \"unknown\")"
            },
            "11": {
                "beforePatchRowNumber": 977,
                "afterPatchRowNumber": 979,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 978,
                "afterPatchRowNumber": 980,
                "PatchRowcode": "                 # TODO: Why do we log the same line under info and debug? Intentional?"
            },
            "13": {
                "beforePatchRowNumber": 979,
                "afterPatchRowNumber": 981,
                "PatchRowcode": "                 msg = ("
            },
            "14": {
                "beforePatchRowNumber": 980,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    \"Starting scalyr agent... (version=%s) (revision=%s) %s (Python version: %s)\""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 982,
                "PatchRowcode": "+                    \"Starting scalyr agent... (version=%s) (revision=%s) %s (Python version: %s) \""
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 983,
                "PatchRowcode": "+                    \"(OpenSSL version: %s)\""
            },
            "17": {
                "beforePatchRowNumber": 981,
                "afterPatchRowNumber": 984,
                "PatchRowcode": "                     % ("
            },
            "18": {
                "beforePatchRowNumber": 982,
                "afterPatchRowNumber": 985,
                "PatchRowcode": "                         SCALYR_VERSION,"
            },
            "19": {
                "beforePatchRowNumber": 983,
                "afterPatchRowNumber": 986,
                "PatchRowcode": "                         build_revision,"
            },
            "20": {
                "beforePatchRowNumber": 984,
                "afterPatchRowNumber": 987,
                "PatchRowcode": "                         scalyr_util.get_pid_tid(),"
            },
            "21": {
                "beforePatchRowNumber": 985,
                "afterPatchRowNumber": 988,
                "PatchRowcode": "                         python_version_str,"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 989,
                "PatchRowcode": "+                        openssl_version,"
            },
            "23": {
                "beforePatchRowNumber": 986,
                "afterPatchRowNumber": 990,
                "PatchRowcode": "                     )"
            },
            "24": {
                "beforePatchRowNumber": 987,
                "afterPatchRowNumber": 991,
                "PatchRowcode": "                 )"
            },
            "25": {
                "beforePatchRowNumber": 988,
                "afterPatchRowNumber": 992,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 1414,
                "afterPatchRowNumber": 1418,
                "PatchRowcode": "             ca_file = None"
            },
            "27": {
                "beforePatchRowNumber": 1415,
                "afterPatchRowNumber": 1419,
                "PatchRowcode": "             intermediate_certs_file = None"
            },
            "28": {
                "beforePatchRowNumber": 1416,
                "afterPatchRowNumber": 1420,
                "PatchRowcode": "         use_requests_lib = self.__config.use_requests_lib"
            },
            "29": {
                "beforePatchRowNumber": 1417,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        use_tlslite = self.__config.use_tlslite"
            },
            "30": {
                "beforePatchRowNumber": 1418,
                "afterPatchRowNumber": 1421,
                "PatchRowcode": "         return ScalyrClientSession("
            },
            "31": {
                "beforePatchRowNumber": 1419,
                "afterPatchRowNumber": 1422,
                "PatchRowcode": "             self.__config.scalyr_server,"
            },
            "32": {
                "beforePatchRowNumber": 1420,
                "afterPatchRowNumber": 1423,
                "PatchRowcode": "             self.__config.api_key,"
            },
            "33": {
                "beforePatchRowNumber": 1424,
                "afterPatchRowNumber": 1427,
                "PatchRowcode": "             ca_file=ca_file,"
            },
            "34": {
                "beforePatchRowNumber": 1425,
                "afterPatchRowNumber": 1428,
                "PatchRowcode": "             intermediate_certs_file=intermediate_certs_file,"
            },
            "35": {
                "beforePatchRowNumber": 1426,
                "afterPatchRowNumber": 1429,
                "PatchRowcode": "             use_requests_lib=use_requests_lib,"
            },
            "36": {
                "beforePatchRowNumber": 1427,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            use_tlslite=use_tlslite,"
            },
            "37": {
                "beforePatchRowNumber": 1428,
                "afterPatchRowNumber": 1430,
                "PatchRowcode": "             compression_type=self.__config.compression_type,"
            },
            "38": {
                "beforePatchRowNumber": 1429,
                "afterPatchRowNumber": 1431,
                "PatchRowcode": "             compression_level=self.__config.compression_level,"
            },
            "39": {
                "beforePatchRowNumber": 1430,
                "afterPatchRowNumber": 1432,
                "PatchRowcode": "             proxies=self.__config.network_proxies,"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python",
            "# Copyright 2014 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ------------------------------------------------------------------------",
            "#",
            "# flake8: noqa: E266",
            "###",
            "# chkconfig: 2345 98 02",
            "# description: Manages the Scalyr Agent 2, which provides log copying",
            "#     and basic system metric collection.",
            "###",
            "### BEGIN INIT INFO",
            "# Provides: scalyr-agent-2",
            "# Required-Start: $network",
            "# Required-Stop: $network",
            "# Default-Start: 2 3 4 5",
            "# Default-Stop: 0 1 6",
            "# Description: Manages the Scalyr Agent 2, which provides log copying",
            "#     and back system metric collection.",
            "### END INIT INFO",
            "#",
            "# author: Steven Czerwinski <czerwin@scalyr.com>",
            "from __future__ import unicode_literals",
            "from __future__ import print_function",
            "from __future__ import absolute_import",
            "",
            "__author__ = \"czerwin@scalyr.com\"",
            "",
            "import traceback",
            "import errno",
            "import gc",
            "import os",
            "import sys",
            "import time",
            "import re",
            "from io import open",
            "",
            "try:",
            "    from __scalyr__ import SCALYR_VERSION",
            "    from __scalyr__ import scalyr_init",
            "    from __scalyr__ import INSTALL_TYPE",
            "    from __scalyr__ import DEV_INSTALL",
            "    from __scalyr__ import MSI_INSTALL",
            "except ImportError:",
            "    from scalyr_agent.__scalyr__ import SCALYR_VERSION",
            "    from scalyr_agent.__scalyr__ import scalyr_init",
            "    from scalyr_agent.__scalyr__ import INSTALL_TYPE",
            "    from scalyr_agent.__scalyr__ import DEV_INSTALL",
            "    from scalyr_agent.__scalyr__ import MSI_INSTALL",
            "",
            "# We must invoke this since we are an executable script.",
            "scalyr_init()",
            "",
            "import six",
            "",
            "import scalyr_agent.scalyr_logging as scalyr_logging",
            "import scalyr_agent.util as scalyr_util",
            "import scalyr_agent.remote_shell as remote_shell",
            "",
            "# We have to be careful to set this logger class very early in processing, even before other",
            "# imports to ensure that any loggers created are AgentLoggers.",
            "from scalyr_agent.monitors_manager import MonitorsManager",
            "from scalyr_agent.scalyr_monitor import UnsupportedSystem",
            "",
            "# Set up the main logger.  We set it up initially to log to standard out,",
            "# but once we run fork off the daemon, we will use a rotating log file.",
            "log = scalyr_logging.getLogger(\"scalyr_agent\")",
            "",
            "scalyr_logging.set_log_destination(use_stdout=True)",
            "",
            "",
            "from optparse import OptionParser",
            "",
            "from scalyr_agent.profiler import ScalyrProfiler",
            "from scalyr_agent.scalyr_client import ScalyrClientSession",
            "from scalyr_agent.copying_manager import CopyingManager",
            "from scalyr_agent.configuration import Configuration",
            "from scalyr_agent.util import RunState, ScriptEscalator",
            "from scalyr_agent.agent_status import AgentStatus",
            "from scalyr_agent.agent_status import ConfigStatus",
            "from scalyr_agent.agent_status import OverallStats",
            "from scalyr_agent.agent_status import GCStatus",
            "from scalyr_agent.agent_status import report_status",
            "from scalyr_agent.platform_controller import (",
            "    PlatformController,",
            "    AgentAlreadyRunning,",
            "    CannotExecuteAsUser,",
            ")",
            "from scalyr_agent.platform_controller import AgentNotRunning",
            "from scalyr_agent.build_info import get_build_revision",
            "",
            "",
            "STATUS_FILE = \"last_status\"",
            "STATUS_FORMAT_FILE = \"status_format\"",
            "",
            "VALID_STATUS_FORMATS = [\"text\", \"json\"]",
            "",
            "AGENT_LOG_FILENAME = \"agent.log\"",
            "",
            "AGENT_NOT_RUNNING_MESSAGE = \"The agent does not appear to be running.\"",
            "",
            "",
            "def _update_disabled_until(config_value, current_time):",
            "    if config_value is not None:",
            "        return config_value + current_time",
            "    else:",
            "        return current_time",
            "",
            "",
            "def _check_disabled(current_time, other_time, message):",
            "    result = current_time < other_time",
            "    if result:",
            "        log.log(",
            "            scalyr_logging.DEBUG_LEVEL_0,",
            "            \"%s disabled for %d more seconds\" % (message, other_time - current_time),",
            "        )",
            "    return result",
            "",
            "",
            "class ScalyrAgent(object):",
            "    \"\"\"Encapsulates the entire Scalyr Agent 2 application.",
            "    \"\"\"",
            "",
            "    def __init__(self, platform_controller):",
            "        \"\"\"Initialize the object.",
            "",
            "        @param platform_controller:  The controller for this platform.",
            "        @type platform_controller: PlatformController",
            "        \"\"\"",
            "        # NOTE:  This abstraction is not thread safe, but it does not need to be.  Even the calls to",
            "        # create the status file are always issued on the main thread since that's how signals are handled.",
            "",
            "        # The current config being used to run the agent.  This may not be the latest",
            "        # version of the config, if that latest version had parsing errors.",
            "        self.__config = None",
            "        # The platform-specific controller that does things like fork daemon processes, sleeps, etc.",
            "        self.__controller = platform_controller",
            "        # A helper for running this script as another user if need be.",
            "        self.__escalator = None",
            "",
            "        # The DefaultPaths object for defining the default paths for various things like the log directory based on",
            "        # the platform.",
            "        self.__default_paths = platform_controller.default_paths",
            "",
            "        self.__config_file_path = None",
            "",
            "        # An extra directory for config snippets",
            "        self.__extra_config_dir = None",
            "",
            "        # If the current contents of the configuration file has errors in it, then this will be set to the config",
            "        # object produced by reading it.",
            "        self.__current_bad_config = None",
            "        # The last time the configuration file was checked to see if it had changed.",
            "        self.__last_config_check_time = None",
            "        self.__start_time = None",
            "        # The path where the agent log file is being written.",
            "        self.__log_file_path = None",
            "        # The current copying manager.",
            "        self.__copying_manager = None",
            "        # The current monitors manager.",
            "        self.__monitors_manager = None",
            "        # The current ScalyrClientSession to use for sending requests.",
            "        self.__scalyr_client = None",
            "",
            "        # Tracks whether or not the agent should still be running.  When a terminate signal is received,",
            "        # the run state is set to false.  Threads are expected to notice this and finish as quickly as",
            "        # possible.",
            "        self.__run_state = None",
            "",
            "        # Store references to the last value of the OverallStats instance",
            "        self.__overall_stats = OverallStats()",
            "",
            "        # Whether or not the unsafe debugging mode is running (meaning the RemoteShell is accepting connections",
            "        # on the local host port and the memory profiler is turned on).  Note, this mode is very unsafe since",
            "        # arbitrary python commands can be executed by any user on the system as the user running the agent.",
            "        self.__unsafe_debugging_running = False",
            "        # A reference to the remote shell debug server.",
            "        self.__debug_server = None",
            "        # Used below for a small cache for a slight optimization.",
            "        self.__last_verify_config = None",
            "",
            "        self.__no_fork = False",
            "        self.__last_total_bytes_skipped = 0",
            "        self.__last_total_bytes_copied = 0",
            "        self.__last_total_bytes_pending = 0",
            "        self.__last_total_rate_limited_time = 0",
            "",
            "    @staticmethod",
            "    def agent_run_method(controller, config_file_path, perform_config_check=False):",
            "        \"\"\"Begins executing the agent service on the current thread.",
            "",
            "        This will not return until the service is requested to terminate.",
            "",
            "        This method can be used as an entry point by PlatformControllers that cannot invoke the ``agent_run_method``",
            "        argument passed in the ``start_agent_service`` method.  It immediately because execution of the service.",
            "",
            "        @param controller: The controller to use to run the service.",
            "        @param config_file_path: The path to the configuration file to use.",
            "        @param perform_config_check:  If true, will check common configuration errors such as forgetting to",
            "            provide an api token and raise an exception if they fail.",
            "",
            "        @type controller: PlatformController",
            "        @type config_file_path: six.text_type",
            "        @type perform_config_check: bool",
            "",
            "        @return: The return code when the agent exits.",
            "        @rtype: int",
            "        \"\"\"",
            "",
            "        class Options(object):",
            "            pass",
            "",
            "        my_options = Options()",
            "        my_options.quiet = True",
            "        my_options.verbose = False",
            "        my_options.health_check = False",
            "        my_options.status_format = \"text\"",
            "        my_options.no_fork = True",
            "        my_options.no_change_user = True",
            "        my_options.no_check_remote = False",
            "        my_options.extra_config_dir = None",
            "",
            "        if perform_config_check:",
            "            command = \"inner_run_with_checks\"",
            "        else:",
            "            command = \"inner_run\"",
            "        try:",
            "            return ScalyrAgent(controller).main(config_file_path, command, my_options)",
            "        except Exception:",
            "            log.exception(\"Agent failed while running.  Will be shutting down.\")",
            "            raise",
            "",
            "    def main(self, config_file_path, command, command_options):",
            "        \"\"\"Run the Scalyr Agent.",
            "",
            "        @param config_file_path: The path to the configuration file.",
            "        @param command: The command passed in at the commandline for the agent to execute, such as 'start', 'stop', etc.",
            "        @param command_options: The options from the commandline.  These will include 'quiet', 'verbose', etc.",
            "",
            "        @type config_file_path: six.text_type",
            "        @type command: six.text_type",
            "",
            "        @return:  The exit status code to exit with, such as 0 for success.",
            "        @rtype: int",
            "        \"\"\"",
            "        quiet = command_options.quiet",
            "        verbose = command_options.verbose",
            "        health_check = command_options.health_check",
            "        status_format = command_options.status_format",
            "        extra_config_dir = command_options.extra_config_dir",
            "        self.__no_fork = command_options.no_fork",
            "        no_check_remote = False",
            "",
            "        self.__extra_config_dir = Configuration.get_extra_config_dir(extra_config_dir)",
            "",
            "        # We process for the 'version' command early since we do not need the configuration file for it.",
            "        if command == \"version\":",
            "            print(\"The Scalyr Agent 2 version is %s\" % SCALYR_VERSION)",
            "            return 0",
            "",
            "        # Read the configuration file.  Fail if we can't read it, unless the command is stop or status.",
            "        if config_file_path is None:",
            "            config_file_path = self.__default_paths.config_file_path",
            "",
            "        self.__config_file_path = config_file_path",
            "",
            "        try:",
            "            self.__config = self.__read_and_verify_config(config_file_path)",
            "",
            "            # check if not a tty and override the no check remote variable",
            "            if not sys.stdout.isatty():",
            "                no_check_remote = not self.__config.check_remote_if_no_tty",
            "        except Exception as e:",
            "            # We ignore a bad configuration file for 'stop' and 'status' because sometimes you do just accidentally",
            "            # screw up the config and you want to let the rest of the system work enough to do the stop or get the",
            "            # status.",
            "            if command != \"stop\" and command != \"status\":",
            "                import traceback",
            "",
            "                raise Exception(",
            "                    \"Error reading configuration file: %s\\n\"",
            "                    \"Terminating agent, please fix the configuration file and restart agent.\\n%s\"",
            "                    % (six.text_type(e), traceback.format_exc())",
            "                )",
            "            else:",
            "                self.__config = None",
            "                print(",
            "                    \"Could not parse configuration file at '%s'\" % config_file_path,",
            "                    file=sys.stderr,",
            "                )",
            "",
            "        self.__controller.consume_config(self.__config, config_file_path)",
            "",
            "        self.__escalator = ScriptEscalator(",
            "            self.__controller,",
            "            config_file_path,",
            "            os.getcwd(),",
            "            command_options.no_change_user,",
            "        )",
            "",
            "        if command_options.no_check_remote is not None:",
            "            no_check_remote = True",
            "",
            "        # noinspection PyBroadException",
            "        try:",
            "            # Execute the command.",
            "            if command == \"start\":",
            "                return self.__start(quiet, no_check_remote)",
            "            elif command == \"inner_run_with_checks\":",
            "                self.__perform_config_checks(no_check_remote)",
            "                return self.__run(self.__controller)",
            "            elif command == \"inner_run\":",
            "                return self.__run(self.__controller)",
            "            elif command == \"stop\":",
            "                return self.__stop(quiet)",
            "            elif command == \"status\" and not (verbose or health_check):",
            "                return self.__status()",
            "            elif command == \"status\" and (verbose or health_check):",
            "                if self.__config is not None:",
            "                    agent_data_path = self.__config.agent_data_path",
            "                else:",
            "                    agent_data_path = self.__default_paths.agent_data_path",
            "                    print(",
            "                        \"Assuming agent data path is '%s'\" % agent_data_path,",
            "                        file=sys.stderr,",
            "                    )",
            "                return self.__detailed_status(",
            "                    agent_data_path,",
            "                    status_format=status_format,",
            "                    health_check=health_check,",
            "                )",
            "            elif command == \"restart\":",
            "                return self.__restart(quiet, no_check_remote)",
            "            elif command == \"condrestart\":",
            "                return self.__condrestart(quiet, no_check_remote)",
            "            else:",
            "                print('Unknown command given: \"%s\".' % command, file=sys.stderr)",
            "                return 1",
            "        except SystemExit:",
            "            return 0",
            "        except Exception as e:",
            "            # We special case the inner_run_with checks since we know that exception is human-readable.",
            "            if command == \"inner_run_with_checks\":",
            "                raise e",
            "            else:",
            "                raise Exception(",
            "                    \"Caught exception when attempt to execute command %s.  Exception was %s\"",
            "                    % (command, six.text_type(e))",
            "                )",
            "",
            "    def __read_and_verify_config(self, config_file_path):",
            "        \"\"\"Reads the configuration and verifies it can be successfully parsed including the monitors existing and",
            "        having valid configurations.",
            "",
            "        @param config_file_path: The path to read the configuration from.",
            "        @type config_file_path: six.text_type",
            "",
            "        @return: The configuration object.",
            "        @rtype: scalyr_agent.Configuration",
            "        \"\"\"",
            "        config = self.__make_config(config_file_path)",
            "        self.__verify_config(config)",
            "        return config",
            "",
            "    def __make_config(self, config_file_path):",
            "        \"\"\"Make Configuration object. Does not read nor verify.",
            "",
            "        You must call ``__verify_config`` to read and fully verify the configuration.",
            "",
            "        @param config_file_path: The path to read the configuration from.",
            "        @type config_file_path: six.text_type",
            "",
            "        @return: The configuration object.",
            "        @rtype: scalyr_agent.Configuration",
            "        \"\"\"",
            "        return Configuration(",
            "            config_file_path,",
            "            self.__default_paths,",
            "            log,",
            "            extra_config_dir=self.__extra_config_dir,",
            "        )",
            "",
            "    def __verify_config(",
            "        self,",
            "        config,",
            "        disable_create_monitors_manager=False,",
            "        disable_create_copying_manager=False,",
            "        disable_cache_config=False,",
            "    ):",
            "        \"\"\"Verifies the passed-in configuration object is valid, and that the referenced monitors exist and have",
            "        valid configuration.",
            "",
            "        @param config: The configuration object.",
            "        @type config: scalyr_agent.Configuration",
            "        @return: A boolean value indicating whether or not the configuration was fully verified",
            "        \"\"\"",
            "        try:",
            "            config.parse()",
            "",
            "            if disable_create_monitors_manager:",
            "                log.info(\"verify_config - creation of monitors manager disabled\")",
            "                return False",
            "",
            "            monitors_manager = MonitorsManager(config, self.__controller)",
            "",
            "            if disable_create_copying_manager:",
            "                log.info(\"verify_config - creation of copying manager disabled\")",
            "                return False",
            "",
            "            copying_manager = CopyingManager(config, monitors_manager.monitors)",
            "            # To do the full verification, we have to create the managers.  However, this call does not need them,",
            "            # but it is very likely the caller of this method will invoke ``__create_worker_thread`` next, so let's",
            "            # save them for that call.  This helps us avoid having to read and instantiate the monitors multiple times.",
            "            if disable_cache_config:",
            "                log.info(\"verify_config - not caching verify_config results\")",
            "                self.__last_verify_config = None",
            "                # return true here because config is verified, just not cached",
            "                # this means the rest of the loop will continue but the config",
            "                # will be verified again when the worker thread is created",
            "                return True",
            "",
            "            self.__last_verify_config = {",
            "                \"config\": config,",
            "                \"monitors_manager\": monitors_manager,",
            "                \"copying_manager\": copying_manager,",
            "            }",
            "        except UnsupportedSystem as e:",
            "            # We want to emit a better error message for this exception, so capture it here.",
            "            raise Exception(",
            "                \"Configuration file uses a monitor that is not supported on this system Monitor '%s' \"",
            "                \"cannot be used due to: %s.  If you require support for this monitor for your system, \"",
            "                \"please e-mail contact@scalyr.com\" % (e.monitor_name, six.text_type(e))",
            "            )",
            "        return True",
            "",
            "    def __create_worker_thread(self, config):",
            "        \"\"\"Creates the worker thread that will run the copying and monitor managers for the specified configuration.",
            "",
            "        @param config: The configuration object.",
            "        @type config: scalyr_agent.Configuration",
            "",
            "        @return: The worker thread object to use.  You must start it.",
            "        @rtype: WorkerThread",
            "        \"\"\"",
            "        # Use the cached results from __last_verify_config if available.  If not, force it to create them.",
            "        if (",
            "            self.__last_verify_config is None",
            "            or self.__last_verify_config[\"config\"] is not config",
            "        ):",
            "            self.__verify_config(config)",
            "",
            "        # Apply any global config options",
            "        if self.__last_verify_config and self.__last_verify_config.get(\"config\", None):",
            "            self.__last_verify_config[\"config\"].apply_config()",
            "",
            "        return WorkerThread(",
            "            self.__last_verify_config[\"config\"],",
            "            self.__last_verify_config[\"copying_manager\"],",
            "            self.__last_verify_config[\"monitors_manager\"],",
            "        )",
            "",
            "    def __perform_config_checks(self, no_check_remote):",
            "        \"\"\"Perform checks for common configuration errors.  Raises an exception with a human-readable message",
            "        if any of the checks fail.",
            "",
            "        In particular, this checks if (1) the user has actually entered an api_key, (2) the agent process can",
            "        write to the logs directory, (3) we can send a request to the the configured scalyr server",
            "        and (4) the api key is correct.",
            "        \"\"\"",
            "        # Make sure the user has set an API key... a common step that can be forgotten.",
            "        # If they haven't set it, it will have REPLACE_THIS as the value since that's what is in the template.",
            "        if self.__config.api_key == \"REPLACE_THIS\" or self.__config.api_key == \"\":",
            "            raise Exception(",
            "                \"Error, you have not set a valid api key in the configuration file.\\n\"",
            "                'Edit the file %s and replace the value for \"api_key\" with a valid logs '",
            "                \"write key for your account.\\n\"",
            "                \"You can see your write logs keys at https://www.scalyr.com/keys\"",
            "                % self.__config.file_path",
            "            )",
            "",
            "        self.__verify_can_write_to_logs_and_data(self.__config)",
            "",
            "        # Begin writing to the log once we confirm we are able to, so we can log any connection errors",
            "        scalyr_logging.set_log_destination(",
            "            use_disk=True,",
            "            no_fork=self.__no_fork,",
            "            stdout_severity=self.__config.stdout_severity,",
            "            max_bytes=self.__config.log_rotation_max_bytes,",
            "            backup_count=self.__config.log_rotation_backup_count,",
            "            logs_directory=self.__config.agent_log_path,",
            "            agent_log_file_path=AGENT_LOG_FILENAME,",
            "        )",
            "",
            "        # Send a test message to the server to make sure everything works.  If not, print a decent error message.",
            "        if not no_check_remote:",
            "            client = self.__create_client(quiet=True)",
            "            try:",
            "                ping_result = client.ping()",
            "                if ping_result != \"success\":",
            "                    if \"badClientClockSkew\" in ping_result:",
            "                        # TODO:  The server does not yet send this error message, but it will in the future.",
            "                        log.error(",
            "                            \"Sending request to the server failed due to bad clock skew.  The system \"",
            "                            \"clock on this host is too far off from actual time. The agent will keep \"",
            "                            \"trying to connect in the background.\"",
            "                        )",
            "                        print(",
            "                            \"Sending request to the server failed due to bad clock skew.  The system \"",
            "                            \"clock on this host is too far off from actual time. The agent will keep \"",
            "                            \"trying to connect in the background.\",",
            "                            file=sys.stderr,",
            "                        )",
            "                    elif \"invalidApiKey\" in ping_result:",
            "                        # TODO:  The server does not yet send this error message, but it will in the future.",
            "                        raise Exception(",
            "                            \"Sending request to the server failed due to an invalid API key.  This probably \"",
            "                            \"means the 'api_key' field in configuration file  '%s' is not correct.  \"",
            "                            \"Please visit https://www.scalyr.com/keys and copy a Write Logs key into the \"",
            "                            \"'api_key' field in the configuration file\"",
            "                            % self.__config.file_path",
            "                        )",
            "                    else:",
            "                        log.error(",
            "                            \"Failed to send request to the server.  The server address could be \"",
            "                            \"wrong, there could be a network connectivity issue, or the provided \"",
            "                            \"token could be incorrect. The agent will keep trying to connect in the \"",
            "                            \"background. You can disable this check with --no-check-remote-server.\"",
            "                        )",
            "                        print(",
            "                            \"Failed to send request to the server.  The server address could be \"",
            "                            \"wrong, there could be a network connectivity issue, or the provided \"",
            "                            \"token could be incorrect. The agent will keep trying to connect in the \"",
            "                            \"background. You can disable this check with --no-check-remote-server.\",",
            "                            file=sys.stderr,",
            "                        )",
            "            finally:",
            "                client.close()",
            "",
            "    def __start(self, quiet, no_check_remote):",
            "        \"\"\"Executes the start command.",
            "",
            "        This will perform some initial checks to see if the agent can be started, such as making sure it can",
            "        read and write to the logs and data directory, and that it can send a successful message to the",
            "        Scalyr servers (therefore verifying the authentication token is correct.)",
            "",
            "        After it determines that the agent is likely to be able to run, it will start the real agent.  If self.__no_fork",
            "        is False, then a new process will be started in the background and this method will return.  Otherwise,",
            "        this method will not return.",
            "",
            "        @param quiet: True if output should be kept to a minimal and only record errors that occur.",
            "        @param no_check_remote:  True if this method should not try to contact the remote Scalyr servers to",
            "            verify connectivity.  If it does try to contact the remote servers and it cannot connect, then",
            "            the script exits with a failure.",
            "        @type quiet: bool",
            "        @type no_check_remote: bool",
            "",
            "        @return:  The exit status code for the process.",
            "        @rtype: int",
            "        \"\"\"",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            return self.__escalator.change_user_and_rerun_script(",
            "                \"start the scalyr agent\"",
            "            )",
            "",
            "        # Make sure we do not try to start it up again.",
            "        self.__fail_if_already_running()",
            "",
            "        # noinspection PyBroadException",
            "        try:",
            "            self.__perform_config_checks(no_check_remote)",
            "        except Exception as e:",
            "            print(file=sys.stderr)",
            "            traceback.print_exc(file=sys.stderr)",
            "            print(",
            "                \"Terminating agent, please fix the error and restart the agent.\",",
            "                file=sys.stderr,",
            "            )",
            "            log.error(\"%s\" % six.text_type(e))",
            "            log.error(\"Terminating agent, please fix the error and restart the agent.\")",
            "            return 1",
            "",
            "        if sys.version_info[:2] < (2, 6):",
            "            print(",
            "                \"Warning, the Scalyr Agent will not support running on Python 2.4, 2.5 after Oct 2019\",",
            "                file=sys.stderr,",
            "            )",
            "            log.error(",
            "                \"Warning, the Scalyr Agent will not support running on Python 2.4, 2.5 after Oct 2019\"",
            "            )",
            "",
            "        if not self.__no_fork:",
            "            # Do one last check to just cut down on the window of race conditions.",
            "            self.__fail_if_already_running()",
            "",
            "            if not quiet:",
            "                if no_check_remote:",
            "                    print(\"Configuration verified, starting agent in background.\")",
            "                else:",
            "                    print(",
            "                        \"Configuration and server connection verified, starting agent in background.\"",
            "                    )",
            "            self.__controller.start_agent_service(self.__run, quiet, fork=True)",
            "        else:",
            "            self.__controller.start_agent_service(self.__run, quiet, fork=False)",
            "",
            "        return 0",
            "",
            "    def __handle_terminate(self):",
            "        \"\"\"Invoked when the process is requested to shutdown, such as by a signal\"\"\"",
            "        if self.__run_state.is_running():",
            "            log.info(\"Received signal to shutdown, attempt to shutdown cleanly.\")",
            "            self.__run_state.stop()",
            "",
            "    def __detailed_status(",
            "        self, data_directory, status_format=\"text\", health_check=False",
            "    ):",
            "        \"\"\"Execute the status -v or -H command.",
            "",
            "        This will request the current agent to dump its detailed status to a file in the data directory, which",
            "        this process will then read.",
            "",
            "        @param data_directory: The path to the data directory.",
            "        @type data_directory: str",
            "",
            "        @return:  An exit status code for the status command indicating success or failure.",
            "        @rtype: int",
            "        \"\"\"",
            "        # Health check ignores format but uses `json` under the hood",
            "        if health_check:",
            "            status_format = \"json\"",
            "",
            "        if status_format not in VALID_STATUS_FORMATS:",
            "            print(",
            "                \"Invalid status format: %s. Valid formats are: %s\"",
            "                % (status_format, \", \".join(VALID_STATUS_FORMATS))",
            "            )",
            "            return 1",
            "",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            try:",
            "                return self.__escalator.change_user_and_rerun_script(",
            "                    \"retrieved detailed status\", handle_error=False",
            "                )",
            "            except CannotExecuteAsUser:",
            "                # For now, we just ignore the error and try to get the status anyway.  This might work on Linux",
            "                # platforms depending on permissions.  This is legacy behavior.",
            "                pass",
            "",
            "        try:",
            "            self.__controller.is_agent_running(fail_if_not_running=True)",
            "        except AgentNotRunning as e:",
            "            print(AGENT_NOT_RUNNING_MESSAGE)",
            "            print(\"%s\" % six.text_type(e))",
            "            return 1",
            "",
            "        # The status works by sending telling the running agent to dump the status into a well known file and",
            "        # then we read it from there, echoing it to stdout.",
            "        if not os.path.isdir(data_directory):",
            "            print(",
            "                'Cannot get status due to bad config.  The data path \"%s\" is not a directory'",
            "                % data_directory,",
            "                file=sys.stderr,",
            "            )",
            "            return 1",
            "",
            "        status_file = os.path.join(data_directory, STATUS_FILE)",
            "        status_format_file = os.path.join(data_directory, STATUS_FORMAT_FILE)",
            "",
            "        # This users needs to zero out the current status file (if it exists), so they need write access to it.",
            "        # When we do create the status file, we give everyone read/write access, so it should not be an issue.",
            "        if os.path.isfile(status_file) and not os.access(status_file, os.W_OK):",
            "            print(",
            "                \"Cannot get status due to insufficient permissions.  The current user does not \"",
            "                'have write access to \"%s\" as required.' % status_file,",
            "                file=sys.stderr,",
            "            )",
            "            return 1",
            "",
            "        # Zero out the current file so that we can detect once the agent process has updated it.",
            "        if os.path.isfile(status_file):",
            "            f = open(status_file, \"w\")",
            "            f.truncate(0)",
            "            f.close()",
            "",
            "        # Write the file with the format we need to use",
            "        with open(status_format_file, \"w\") as fp:",
            "            status_format = six.text_type(status_format)",
            "            fp.write(status_format)",
            "",
            "        # Signal to the running process.  This should cause that process to write to the status file",
            "        result = self.__controller.request_agent_status()",
            "        if result is not None:",
            "            if result == errno.ESRCH:",
            "                print(AGENT_NOT_RUNNING_MESSAGE, file=sys.stderr)",
            "                return 1",
            "            elif result == errno.EPERM:",
            "                # TODO:  We probably should just get the name of the user running the agent and output it",
            "                # here, instead of hard coding it to root.",
            "                print(",
            "                    \"To view agent status, you must be running as the same user as the agent. \"",
            "                    \"Try running this command as root or Administrator.\",",
            "                    file=sys.stderr,",
            "                )",
            "                return 2",
            "",
            "        # We wait for five seconds at most to get the status.",
            "        deadline = time.time() + 5",
            "",
            "        # Now loop until we see it show up.",
            "        while True:",
            "            if os.path.isfile(status_file) and os.path.getsize(status_file) > 0:",
            "                break",
            "",
            "            if time.time() > deadline:",
            "                if self.__config is not None:",
            "                    agent_log = os.path.join(",
            "                        self.__config.agent_log_path, AGENT_LOG_FILENAME",
            "                    )",
            "                else:",
            "                    agent_log = os.path.join(",
            "                        self.__default_paths.agent_log_path, AGENT_LOG_FILENAME",
            "                    )",
            "                print(",
            "                    \"Failed to get status within 5 seconds.  Giving up.  The agent process is \"",
            "                    \"possibly stuck.  See %s for more details.\" % agent_log,",
            "                    file=sys.stderr,",
            "                )",
            "                return 1",
            "",
            "            time.sleep(0.03)",
            "",
            "        if not os.access(status_file, os.R_OK):",
            "            print(",
            "                \"Cannot get status due to insufficient permissions.  The current user does not \"",
            "                'have read access to \"%s\" as required.' % status_file,",
            "                file=sys.stderr,",
            "            )",
            "            return 1",
            "",
            "        return_code = 0",
            "        fp = open(status_file)",
            "        for line in fp:",
            "            if not health_check:",
            "                print(line.rstrip())",
            "",
            "            if status_format == \"json\" or health_check:",
            "                health_result = self.__find_health_result_in_status_json(line)",
            "                if health_result:",
            "                    if health_check:",
            "                        print(\"Health check: %s\" % health_result)",
            "                    if health_result != \"Good\":",
            "                        return_code = 2",
            "                elif health_check:",
            "                    print(\"Cannot get health check result.\")",
            "            elif (",
            "                status_format == \"text\"",
            "                and \"Health check:\" in line",
            "                and not re.match(r\"^Health check\\:\\s+Good$\", line.strip())",
            "            ):",
            "                return_code = 2",
            "        fp.close()",
            "        return return_code",
            "",
            "    @staticmethod",
            "    def __find_health_result_in_status_json(line):",
            "        try:",
            "            status = scalyr_util.json_decode(line)",
            "            if (",
            "                \"copying_manager_status\" in status",
            "                and \"health_check_result\" in status[\"copying_manager_status\"]",
            "            ):",
            "                return status[\"copying_manager_status\"][\"health_check_result\"]",
            "        except ValueError:",
            "            pass",
            "        return None",
            "",
            "    def __stop(self, quiet):",
            "        \"\"\"Stop the current agent.",
            "",
            "        @param quiet: Whether or not only errors should be written to stdout.",
            "        @type quiet: bool",
            "",
            "        @return: the exit status code",
            "        @rtype: int",
            "        \"\"\"",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            return self.__escalator.change_user_and_rerun_script(",
            "                \"stop the scalyr agent\"",
            "            )",
            "",
            "        try:",
            "            self.__controller.is_agent_running(fail_if_not_running=True)",
            "            status = self.__controller.stop_agent_service(quiet)",
            "            return status",
            "        except AgentNotRunning as e:",
            "            print(",
            "                \"Failed to stop the agent because it does not appear to be running.\",",
            "                file=sys.stderr,",
            "            )",
            "            print(\"%s\" % six.text_type(e), file=sys.stderr)",
            "            return 0  # For the sake of restart, we need to return non-error code here.",
            "",
            "    def __status(self):",
            "        \"\"\"Execute the 'status' command to indicate if the agent is running or not.",
            "",
            "        @return: The exit status code.  It will return zero only if it is running.",
            "        @rtype: int",
            "        \"\"\"",
            "        if self.__controller.is_agent_running():",
            "            print('The agent is running. For details, use \"scalyr-agent-2 status -v\".')",
            "            return 0",
            "        else:",
            "            print(AGENT_NOT_RUNNING_MESSAGE)",
            "            return 4",
            "",
            "    def __condrestart(self, quiet, no_check_remote):",
            "        \"\"\"Execute the 'condrestart' command which will only restart the agent if it is already running.",
            "        self.__no_form determines if this method should not fork a separate process to run the agent, but run it",
            "        directly instead.  If it is False, then a daemon process will be forked and will run the agent.",
            "",
            "        @param quiet: True if output should be kept to a minimal and only record errors that occur.",
            "        @param no_check_remote:  True if this method should not try to contact the remote Scalyr servers to",
            "            verify connectivity.  If it does try to contact the remote servers and it cannot connect, then",
            "            the script exits with a failure.",
            "",
            "        @type quiet: bool",
            "        @type no_check_remote: bool",
            "",
            "        @return: the exit status code",
            "        @rtype: int",
            "        \"\"\"",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            return self.__escalator.change_user_and_rerun_script(",
            "                \"restart the scalyr agent\"",
            "            )",
            "",
            "        if self.__controller.is_agent_running():",
            "            if not quiet:",
            "                print(\"Agent is running, restarting now.\")",
            "            if self.__stop(quiet) != 0:",
            "                print(",
            "                    \"Failed to stop the running agent.  Cannot restart until it is killed.\",",
            "                    file=sys.stderr,",
            "                )",
            "                return 1",
            "",
            "            return self.__start(quiet, no_check_remote)",
            "        elif not quiet:",
            "            print(\"Agent is not running, not restarting.\")",
            "            return 0",
            "        else:",
            "            return 0",
            "",
            "    def __restart(self, quiet, no_check_remote):",
            "        \"\"\"Execute the 'restart' which will start the agent, stopping the existing agent if it is running.",
            "        self.__no_fork determines if this method should not fork a separate process to run the agent, but run it",
            "        directly instead.  If it is False, then a daemon process will be forked and will run the agent.",
            "",
            "        @param quiet: True if output should be kept to a minimal and only record errors that occur.",
            "        @param no_check_remote:  True if this method should not try to contact the remote Scalyr servers to",
            "            verify connectivity.  If it does try to contact the remote servers and it cannot connect, then",
            "            the script exits with a failure.",
            "",
            "        @type quiet: bool",
            "        @type no_check_remote: bool",
            "",
            "        @return: the exit status code, zero if it was successfully restarted, non-zero if it was not running or",
            "            could not be started.",
            "        @rtype: int",
            "        \"\"\"",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            return self.__escalator.change_user_and_rerun_script(",
            "                \"restart the scalyr agent\"",
            "            )",
            "",
            "        if self.__controller.is_agent_running():",
            "            if not quiet:",
            "                print(\"Agent is running, stopping it now.\")",
            "            if self.__stop(quiet) != 0:",
            "                print(",
            "                    \"Failed to stop the running agent.  Cannot restart until it is killed\",",
            "                    file=sys.stderr,",
            "                )",
            "                return 1",
            "",
            "        return self.__start(quiet, no_check_remote)",
            "",
            "    def __print_force_https_message(self, scalyr_server, raw_scalyr_server):",
            "        \"\"\"Convenience function for printing a message stating whether the scalyr_server was forced to use https\"\"\"",
            "        if scalyr_server != raw_scalyr_server:",
            "            log.info(",
            "                \"Forcing https protocol for server url: %s -> %s.  You can prevent this by setting the `allow_http` global config option, but be mindful that there are security implications with doing this, including tramsitting your Scalyr api key over an insecure connection.\"",
            "                % (raw_scalyr_server, scalyr_server)",
            "            )",
            "",
            "    def __run(self, controller):",
            "        \"\"\"Runs the Scalyr Agent 2.",
            "",
            "        This method will not return until a TERM signal is received or a fatal error occurs.",
            "",
            "        @param controller The controller that started this agent service.",
            "        @type controller: PlatformController",
            "",
            "        @return: the exit status code",
            "        @rtype: int",
            "        \"\"\"",
            "",
            "        self.__start_time = time.time()",
            "        controller.register_for_termination(self.__handle_terminate)",
            "",
            "        # Register handler for when we get an interrupt signal.  That indicates we should dump the status to",
            "        # a file because a user has run the 'detailed_status' command.",
            "        self.__controller.register_for_status_requests(self.__report_status_to_file)",
            "",
            "        # The stats we track for the lifetime of the agent.  This variable tracks the accumulated stats since the",
            "        # last stat reset (the stats get reset every time we read a new configuration).",
            "        base_overall_stats = OverallStats()",
            "",
            "        # We only emit the overall stats once ever ten minutes.  Track when we last reported it.",
            "        last_overall_stats_report_time = self.__start_time",
            "        # We only emit the bandwidth stats once every minute.  Track when we last reported it.",
            "        last_bw_stats_report_time = self.__start_time",
            "        # We only emit the copying_manager stats once every 5 minutes.  Track when we last reported it.",
            "        last_copy_manager_stats_report_time = self.__start_time",
            "",
            "        # The thread that runs the monitors and and the log copier.",
            "        worker_thread = None",
            "",
            "        try:",
            "            # noinspection PyBroadException",
            "            try:",
            "                self.__run_state = RunState()",
            "                self.__log_file_path = os.path.join(",
            "                    self.__config.agent_log_path, AGENT_LOG_FILENAME",
            "                )",
            "                scalyr_logging.set_log_destination(",
            "                    use_disk=True,",
            "                    no_fork=self.__no_fork,",
            "                    stdout_severity=self.__config.stdout_severity,",
            "                    max_bytes=self.__config.log_rotation_max_bytes,",
            "                    backup_count=self.__config.log_rotation_backup_count,",
            "                    logs_directory=self.__config.agent_log_path,",
            "                    agent_log_file_path=AGENT_LOG_FILENAME,",
            "                )",
            "",
            "                self.__update_debug_log_level(self.__config.debug_level)",
            "",
            "                # We record where the log file currently is so that we can (in the worse case) start copying it",
            "                # from this position.  That way we capture the first 'Starting scalyr agent' call.",
            "                agent_log_position = self.__get_file_initial_position(",
            "                    self.__log_file_path",
            "                )",
            "                if agent_log_position is not None:",
            "                    logs_initial_positions = {self.__log_file_path: agent_log_position}",
            "                else:",
            "                    logs_initial_positions = None",
            "",
            "                # 2->TODO it was very helpful to see what python version does agent run on. Maybe we can keep it?",
            "                python_version_str = sys.version.replace(\"\\n\", \"\")",
            "                build_revision = get_build_revision()",
            "",
            "                # TODO: Why do we log the same line under info and debug? Intentional?",
            "                msg = (",
            "                    \"Starting scalyr agent... (version=%s) (revision=%s) %s (Python version: %s)\"",
            "                    % (",
            "                        SCALYR_VERSION,",
            "                        build_revision,",
            "                        scalyr_util.get_pid_tid(),",
            "                        python_version_str,",
            "                    )",
            "                )",
            "",
            "                log.info(msg)",
            "                log.log(scalyr_logging.DEBUG_LEVEL_1, msg)",
            "",
            "                self.__controller.emit_init_log(log, self.__config.debug_init)",
            "",
            "                self.__start_or_stop_unsafe_debugging()",
            "",
            "                scalyr_server = self.__config.scalyr_server",
            "                raw_scalyr_server = self.__config.raw_scalyr_server",
            "                self.__print_force_https_message(scalyr_server, raw_scalyr_server)",
            "",
            "                self.__config.print_useful_settings()",
            "",
            "                self.__scalyr_client = self.__create_client()",
            "",
            "                def start_worker_thread(config, logs_initial_positions=None):",
            "                    wt = self.__create_worker_thread(config)",
            "                    # attach callbacks before starting monitors",
            "                    wt.monitors_manager.set_user_agent_augment_callback(",
            "                        self.__scalyr_client.augment_user_agent",
            "                    )",
            "                    wt.start(self.__scalyr_client, logs_initial_positions)",
            "                    return wt, wt.copying_manager, wt.monitors_manager",
            "",
            "                (",
            "                    worker_thread,",
            "                    self.__copying_manager,",
            "                    self.__monitors_manager,",
            "                ) = start_worker_thread(self.__config, logs_initial_positions)",
            "",
            "                # JSON library setting is applied as part of __create_worker_thread method",
            "                log.log(",
            "                    scalyr_logging.DEBUG_LEVEL_0,",
            "                    'Using JSON library \"%s\"' % (scalyr_util.get_json_lib()),",
            "                )",
            "",
            "                log.log(",
            "                    scalyr_logging.DEBUG_LEVEL_0,",
            "                    'Using \"%s\" compression algorithm with level \"%s\"'",
            "                    % (self.__config.compression_type, self.__config.compression_level),",
            "                )",
            "",
            "                current_time = time.time()",
            "",
            "                disable_all_config_updates_until = _update_disabled_until(",
            "                    self.__config.disable_all_config_updates, current_time",
            "                )",
            "                disable_verify_config_until = _update_disabled_until(",
            "                    self.__config.disable_verify_config, current_time",
            "                )",
            "                disable_config_equivalence_check_until = _update_disabled_until(",
            "                    self.__config.disable_config_equivalence_check, current_time",
            "                )",
            "                disable_verify_can_write_to_logs_until = _update_disabled_until(",
            "                    self.__config.disable_verify_can_write_to_logs, current_time",
            "                )",
            "                disable_config_reload_until = _update_disabled_until(",
            "                    self.__config.disable_config_reload, current_time",
            "                )",
            "                disable_verify_config_create_monitors_manager_until = _update_disabled_until(",
            "                    self.__config.disable_verify_config_create_monitors_manager,",
            "                    current_time,",
            "                )",
            "                disable_verify_config_create_copying_manager_until = _update_disabled_until(",
            "                    self.__config.disable_verify_config_create_copying_manager,",
            "                    current_time,",
            "                )",
            "",
            "                config_change_check_interval = (",
            "                    self.__config.config_change_check_interval",
            "                )",
            "",
            "                gc_interval = self.__config.garbage_collect_interval",
            "                last_gc_time = current_time",
            "",
            "                prev_server = scalyr_server",
            "",
            "                profiler = ScalyrProfiler(self.__config)",
            "",
            "                while not self.__run_state.sleep_but_awaken_if_stopped(",
            "                    config_change_check_interval",
            "                ):",
            "                    current_time = time.time()",
            "                    self.__last_config_check_time = current_time",
            "",
            "                    profiler.update(self.__config, current_time)",
            "",
            "                    if self.__config.disable_overall_stats:",
            "                        log.log(scalyr_logging.DEBUG_LEVEL_0, \"overall stats disabled\")",
            "                    else:",
            "                        # Log the overall stats once every 10 mins (by default)",
            "                        log_stats_delta = self.__config.overall_stats_log_interval",
            "                        if (",
            "                            current_time",
            "                            > last_overall_stats_report_time + log_stats_delta",
            "                        ):",
            "                            self.__overall_stats = self.__calculate_overall_stats(",
            "                                base_overall_stats,",
            "                            )",
            "                            self.__log_overall_stats(self.__overall_stats)",
            "                            last_overall_stats_report_time = current_time",
            "",
            "                    if self.__config.disable_bandwidth_stats:",
            "                        log.log(",
            "                            scalyr_logging.DEBUG_LEVEL_0, \"bandwidth stats disabled\"",
            "                        )",
            "                    else:",
            "                        # Log the bandwidth-related stats once every minute:",
            "                        log_stats_delta = self.__config.bandwidth_stats_log_interval",
            "                        if current_time > last_bw_stats_report_time + log_stats_delta:",
            "                            self.__overall_stats = self.__calculate_overall_stats(",
            "                                base_overall_stats",
            "                            )",
            "",
            "                            self.__log_bandwidth_stats(",
            "                                self.__calculate_overall_stats(self.__overall_stats)",
            "                            )",
            "                            last_bw_stats_report_time = current_time",
            "",
            "                    if self.__config.disable_copy_manager_stats:",
            "                        log.log(",
            "                            scalyr_logging.DEBUG_LEVEL_0, \"copy manager stats disabled\"",
            "                        )",
            "                    else:",
            "                        # Log the copy manager stats once every 5 mins (by default)",
            "                        log_stats_delta = (",
            "                            self.__config.copying_manager_stats_log_interval",
            "                        )",
            "                        if (",
            "                            current_time",
            "                            > last_copy_manager_stats_report_time + log_stats_delta",
            "                        ):",
            "                            self.__overall_stats = self.__calculate_overall_stats(",
            "                                base_overall_stats, copy_manager_warnings=True,",
            "                            )",
            "                            self.__log_copy_manager_stats(self.__overall_stats)",
            "                            last_copy_manager_stats_report_time = current_time",
            "",
            "                    log.log(",
            "                        scalyr_logging.DEBUG_LEVEL_1,",
            "                        \"Checking for any changes to config file\",",
            "                    )",
            "                    new_config = None",
            "                    try:",
            "                        if _check_disabled(",
            "                            current_time,",
            "                            disable_all_config_updates_until,",
            "                            \"all config updates\",",
            "                        ):",
            "                            continue",
            "",
            "                        new_config = self.__make_config(self.__config_file_path)",
            "                        # TODO:  By parsing the configuration file, we are doing a lot of work just to have it thrown",
            "                        # out in a few seconds when we discover it is equivalent to the previous one.  Maybe we should",
            "                        # rework the equivalence so that it can work on the raw files, but this is difficult since",
            "                        # we need to parse the main configuration file to at least get the fragment directory.  For",
            "                        # now, we will just wait this work.  We only do it once every 30 secs anyway.",
            "",
            "                        if _check_disabled(",
            "                            current_time, disable_verify_config_until, \"verify config\"",
            "                        ):",
            "                            continue",
            "",
            "                        disable_create_monitors_manager = _check_disabled(",
            "                            current_time,",
            "                            disable_verify_config_create_monitors_manager_until,",
            "                            \"verify config create monitors manager\",",
            "                        )",
            "                        disable_create_copying_manager = _check_disabled(",
            "                            current_time,",
            "                            disable_verify_config_create_copying_manager_until,",
            "                            \"verify config create copying manager\",",
            "                        )",
            "                        disable_cache_config = (",
            "                            self.__config.disable_verify_config_cache_config",
            "                        )",
            "                        verified = self.__verify_config(",
            "                            new_config,",
            "                            disable_create_monitors_manager=disable_create_monitors_manager,",
            "                            disable_create_copying_manager=disable_create_copying_manager,",
            "                            disable_cache_config=disable_cache_config,",
            "                        )",
            "",
            "                        # Skip the rest of the loop if the config wasn't fully verified because",
            "                        # later code relies on the config being fully validated",
            "                        if not verified:",
            "                            continue",
            "",
            "                        if self.__config.disable_update_debug_log_level:",
            "                            log.log(",
            "                                scalyr_logging.DEBUG_LEVEL_0,",
            "                                \"update debug_log_level disabled\",",
            "                            )",
            "                        else:",
            "                            # Update the debug_level based on the new config.. we always update it.",
            "                            self.__update_debug_log_level(new_config.debug_level)",
            "",
            "                        # see if we need to perform a garbage collection",
            "                        if gc_interval > 0 and current_time > (",
            "                            last_gc_time + gc_interval",
            "                        ):",
            "                            gc.collect()",
            "                            last_gc_time = current_time",
            "",
            "                        if self.__config.enable_gc_stats:",
            "                            # If GC stats are enabled, enable tracking uncollectable objects",
            "                            if gc.get_debug() == 0:",
            "                                log.log(",
            "                                    scalyr_logging.DEBUG_LEVEL_5,",
            "                                    \"Enabling GC debug mode\",",
            "                                )",
            "                                gc.set_debug(gc.DEBUG_UNCOLLECTABLE)",
            "                        else:",
            "                            if gc.get_debug() != 0:",
            "                                log.log(",
            "                                    scalyr_logging.DEBUG_LEVEL_5,",
            "                                    \"Disabling GC debug mode\",",
            "                                )",
            "                                gc.set_debug(0)",
            "",
            "                        if _check_disabled(",
            "                            current_time,",
            "                            disable_config_equivalence_check_until,",
            "                            \"config equivalence check\",",
            "                        ):",
            "                            continue",
            "",
            "                        if self.__current_bad_config is None and new_config.equivalent(",
            "                            self.__config, exclude_debug_level=True",
            "                        ):",
            "                            log.log(",
            "                                scalyr_logging.DEBUG_LEVEL_1,",
            "                                \"Config was not different than previous\",",
            "                            )",
            "                            continue",
            "",
            "                        if _check_disabled(",
            "                            current_time,",
            "                            disable_verify_can_write_to_logs_until,",
            "                            \"verify check for writing to logs and data\",",
            "                        ):",
            "                            continue",
            "",
            "                        self.__verify_can_write_to_logs_and_data(new_config)",
            "",
            "                    except Exception as e:",
            "                        if self.__current_bad_config is None:",
            "                            log.error(",
            "                                \"Bad configuration file seen.  Ignoring, using last known good configuration file.  \"",
            "                                'Exception was \"%s\"',",
            "                                six.text_type(e),",
            "                                error_code=\"badConfigFile\",",
            "                            )",
            "                        self.__current_bad_config = new_config",
            "                        log.log(",
            "                            scalyr_logging.DEBUG_LEVEL_1,",
            "                            \"Config could not be read or parsed\",",
            "                        )",
            "                        continue",
            "",
            "                    if _check_disabled(",
            "                        current_time, disable_config_reload_until, \"config reload\"",
            "                    ):",
            "                        continue",
            "",
            "                    log.log(",
            "                        scalyr_logging.DEBUG_LEVEL_1,",
            "                        \"Config was different than previous.  Reloading.\",",
            "                    )",
            "                    # We are about to reset the current workers and ScalyrClientSession, so we will lose their",
            "                    # contribution to the stats, so recalculate the base.",
            "                    base_overall_stats = self.__calculate_overall_stats(",
            "                        base_overall_stats",
            "                    )",
            "                    log.info(\"New configuration file seen.\")",
            "                    log.info(\"Stopping copying and metrics threads.\")",
            "                    worker_thread.stop()",
            "",
            "                    worker_thread = None",
            "",
            "                    new_config.print_useful_settings(self.__config)",
            "",
            "                    self.__config = new_config",
            "                    self.__controller.consume_config(new_config, new_config.file_path)",
            "",
            "                    self.__start_or_stop_unsafe_debugging()",
            "",
            "                    # get the server and the raw server to see if we forced https",
            "                    scalyr_server = self.__config.scalyr_server",
            "                    raw_scalyr_server = self.__config.raw_scalyr_server",
            "",
            "                    # only print a message if this is the first time we have seen this scalyr_server",
            "                    # and the server field is different from the raw server field",
            "                    if scalyr_server != prev_server:",
            "                        self.__print_force_https_message(",
            "                            scalyr_server, raw_scalyr_server",
            "                        )",
            "",
            "                    prev_server = scalyr_server",
            "",
            "                    self.__scalyr_client = self.__create_client()",
            "",
            "                    log.info(\"Starting new copying and metrics threads\")",
            "                    (",
            "                        worker_thread,",
            "                        self.__copying_manager,",
            "                        self.__monitors_manager,",
            "                    ) = start_worker_thread(new_config)",
            "",
            "                    self.__current_bad_config = None",
            "                    config_change_check_interval = (",
            "                        self.__config.config_change_check_interval",
            "                    )",
            "                    gc_interval = self.__config.garbage_collect_interval",
            "",
            "                    disable_all_config_updates_until = _update_disabled_until(",
            "                        self.__config.disable_all_config_updates, current_time",
            "                    )",
            "                    disable_verify_config_until = _update_disabled_until(",
            "                        self.__config.disable_verify_config, current_time",
            "                    )",
            "                    disable_config_equivalence_check_until = _update_disabled_until(",
            "                        self.__config.disable_config_equivalence_check, current_time",
            "                    )",
            "                    disable_verify_can_write_to_logs_until = _update_disabled_until(",
            "                        self.__config.disable_verify_can_write_to_logs, current_time",
            "                    )",
            "                    disable_config_reload_until = _update_disabled_until(",
            "                        self.__config.disable_config_reload, current_time",
            "                    )",
            "                    disable_verify_config_create_monitors_manager_until = _update_disabled_until(",
            "                        self.__config.disable_verify_config_create_monitors_manager,",
            "                        current_time,",
            "                    )",
            "                    disable_verify_config_create_copying_manager_until = _update_disabled_until(",
            "                        self.__config.disable_verify_config_create_copying_manager,",
            "                        current_time,",
            "                    )",
            "",
            "                # Log the stats one more time before we terminate.",
            "                self.__log_overall_stats(",
            "                    self.__calculate_overall_stats(base_overall_stats)",
            "                )",
            "",
            "            except Exception:",
            "                log.exception(",
            "                    \"Main run method for agent failed due to exception\",",
            "                    error_code=\"failedAgentMain\",",
            "                )",
            "        finally:",
            "            if worker_thread is not None:",
            "                worker_thread.stop()",
            "",
            "            # NOTE: We manually call close_handlers() here instead of registering it to call it as",
            "            # part of run state stop routine.",
            "            # The reason for that is that run state stop callbacks are called before we get here",
            "            # which means that some messages which are produced after that and before fully shutting",
            "            # down are lost and not logged.",
            "            scalyr_logging.close_handlers()",
            "",
            "    def __fail_if_already_running(self):",
            "        \"\"\"If the agent is already running, prints an appropriate error message and exits the process.",
            "        \"\"\"",
            "        try:",
            "            self.__controller.is_agent_running(fail_if_running=True)",
            "        except AgentAlreadyRunning as e:",
            "            print(",
            "                \"Failed to start agent because it is already running.\", file=sys.stderr",
            "            )",
            "            print(\"%s\" % six.text_type(e), file=sys.stderr)",
            "            sys.exit(4)",
            "",
            "    def __update_debug_log_level(self, debug_level):",
            "        \"\"\"Updates the debug log level of the agent.",
            "        @param debug_level: The debug level, ranging from 0 (no debug) to 5.",
            "",
            "        @type debug_level: int",
            "        \"\"\"",
            "        levels = [",
            "            scalyr_logging.DEBUG_LEVEL_0,",
            "            scalyr_logging.DEBUG_LEVEL_1,",
            "            scalyr_logging.DEBUG_LEVEL_2,",
            "            scalyr_logging.DEBUG_LEVEL_3,",
            "            scalyr_logging.DEBUG_LEVEL_4,",
            "            scalyr_logging.DEBUG_LEVEL_5,",
            "        ]",
            "",
            "        scalyr_logging.set_log_level(levels[debug_level])",
            "",
            "    def __create_client(self, quiet=False):",
            "        \"\"\"Creates and returns a new client to the Scalyr servers.",
            "",
            "        @param quiet: If true, only errors should be written to stdout.",
            "        @type quiet: bool",
            "",
            "        @return: The client to use for sending requests to Scalyr, using the server address and API write logs",
            "            key in the configuration file.",
            "        @rtype: ScalyrClientSession",
            "        \"\"\"",
            "        if self.__config.verify_server_certificate:",
            "            is_dev_install = INSTALL_TYPE == DEV_INSTALL",
            "            is_dev_or_msi_install = INSTALL_TYPE in [DEV_INSTALL, MSI_INSTALL]",
            "",
            "            ca_file = self.__config.ca_cert_path",
            "            intermediate_certs_file = self.__config.intermediate_certs_path",
            "",
            "            # Validate provided CA cert file and intermediate cert file exists. If they don't",
            "            # exist, throw and fail early and loudly",
            "            if not is_dev_install and not os.path.isfile(ca_file):",
            "                raise ValueError(",
            "                    'Invalid path \"%s\" specified for the \"ca_cert_path\" config '",
            "                    \"option: file does not exist\" % (ca_file)",
            "                )",
            "",
            "            # NOTE: We don't include intermediate certs in the Windows binary so we skip that check",
            "            # under the MSI / Windows install",
            "            if not is_dev_or_msi_install and not os.path.isfile(",
            "                intermediate_certs_file",
            "            ):",
            "                raise ValueError(",
            "                    'Invalid path \"%s\" specified for the '",
            "                    '\"intermediate_certs_path\" config '",
            "                    \"option: file does not exist\" % (intermediate_certs_file)",
            "                )",
            "        else:",
            "            ca_file = None",
            "            intermediate_certs_file = None",
            "        use_requests_lib = self.__config.use_requests_lib",
            "        use_tlslite = self.__config.use_tlslite",
            "        return ScalyrClientSession(",
            "            self.__config.scalyr_server,",
            "            self.__config.api_key,",
            "            SCALYR_VERSION,",
            "            quiet=quiet,",
            "            request_deadline=self.__config.request_deadline,",
            "            ca_file=ca_file,",
            "            intermediate_certs_file=intermediate_certs_file,",
            "            use_requests_lib=use_requests_lib,",
            "            use_tlslite=use_tlslite,",
            "            compression_type=self.__config.compression_type,",
            "            compression_level=self.__config.compression_level,",
            "            proxies=self.__config.network_proxies,",
            "            disable_send_requests=self.__config.disable_send_requests,",
            "            disable_logfile_addevents_format=self.__config.disable_logfile_addevents_format,",
            "            enforce_monotonic_timestamps=self.__config.enforce_monotonic_timestamps,",
            "        )",
            "",
            "    def __get_file_initial_position(self, path):",
            "        \"\"\"Returns the file size for the specified file.",
            "",
            "        @param path: The path of the file",
            "        @type path: str",
            "",
            "        @return: The file size",
            "        @rtype: int",
            "        \"\"\"",
            "        try:",
            "            return os.path.getsize(path)",
            "        except OSError as e:",
            "            if e.errno == errno.EPERM:",
            "                log.warn(\"Insufficient permissions to read agent logs initial position\")",
            "                return None",
            "            elif e.errno == errno.ENOENT:",
            "                # If file doesn't exist, just return 0 as its initial position",
            "                return 0",
            "            else:",
            "                log.exception(\"Error trying to read agent logs initial position\")",
            "                return None",
            "",
            "    def __verify_can_write_to_logs_and_data(self, config):",
            "        \"\"\"Checks to make sure the user account running the agent has permission to read and write files",
            "        to the log and data directories as specified in the config.",
            "",
            "        If any verification fails, an exception is raised.",
            "",
            "        @param config: The configuration",
            "        @type config: Configuration",
            "        \"\"\"",
            "",
            "        if self.__controller.install_type == DEV_INSTALL:",
            "            # The agent is running from source, make sure that its directories exist.",
            "            if not os.path.exists(config.agent_log_path):",
            "                os.makedirs(config.agent_log_path)",
            "            if not os.path.exists(config.agent_data_path):",
            "                os.makedirs(config.agent_data_path)",
            "",
            "        if not os.path.isdir(config.agent_log_path):",
            "            raise Exception(",
            "                \"The agent log directory '%s' does not exist.\" % config.agent_log_path",
            "            )",
            "",
            "        if not os.access(config.agent_log_path, os.W_OK):",
            "            raise Exception(",
            "                \"User cannot write to agent log directory '%s'.\" % config.agent_log_path",
            "            )",
            "",
            "        if not os.path.isdir(config.agent_data_path):",
            "            raise Exception(",
            "                \"The agent data directory '%s' does not exist.\" % config.agent_data_path",
            "            )",
            "",
            "        if not os.access(config.agent_data_path, os.W_OK):",
            "            raise Exception(",
            "                \"User cannot write to agent data directory '%s'.\"",
            "                % config.agent_data_path",
            "            )",
            "",
            "    def __start_or_stop_unsafe_debugging(self):",
            "        \"\"\"Starts or stops the debugging tool.",
            "",
            "        This runs a thread that listens to a server port and connects any incoming connection to a shell",
            "        that can be used to execute Python commands to investigate issues on the live server.",
            "",
            "        Since opening up a socket is dangerous, we control this with a configuration option and only use it",
            "        when really needed.",
            "        \"\"\"",
            "        should_be_running = self.__config.use_unsafe_debugging",
            "",
            "        if should_be_running and not self.__unsafe_debugging_running:",
            "            self.__unsafe_debugging_running = True",
            "            self.__debug_server = remote_shell.DebugServer()",
            "            self.__debug_server.start()",
            "        elif not should_be_running and self.__unsafe_debugging_running:",
            "            if self.__debug_server is not None:",
            "                self.__debug_server.stop()",
            "                self.__debug_server = None",
            "            self.__unsafe_debugging_running = False",
            "",
            "    def __generate_status(self, warn_on_rate_limit=False):",
            "        \"\"\"Generates the server status object and returns it.",
            "",
            "        The returned status object is used to create the detailed status page.",
            "",
            "        @return: The status object filled in with the values from the current agent.",
            "        @rtype: AgentStatus",
            "        \"\"\"",
            "        # Basic agent stats first.",
            "        result = AgentStatus()",
            "        result.launch_time = self.__start_time",
            "        result.user = self.__controller.get_current_user()",
            "        result.revision = get_build_revision()",
            "        result.version = SCALYR_VERSION",
            "        result.server_host = self.__config.server_attributes[\"serverHost\"]",
            "        result.compression_type = self.__config.compression_type",
            "        result.compression_level = self.__config.compression_level",
            "        result.scalyr_server = self.__config.scalyr_server",
            "        result.log_path = self.__log_file_path",
            "        result.python_version = sys.version.replace(\"\\n\", \"\")",
            "",
            "        # Describe the status of the configuration file.",
            "        config_result = ConfigStatus()",
            "        result.config_status = config_result",
            "",
            "        config_result.last_check_time = self.__last_config_check_time",
            "        if self.__current_bad_config is not None:",
            "            config_result.path = self.__current_bad_config.file_path",
            "            config_result.additional_paths = list(",
            "                self.__current_bad_config.additional_file_paths",
            "            )",
            "            config_result.last_read_time = self.__current_bad_config.read_time",
            "            config_result.status = \"Error, using last good configuration\"",
            "            config_result.last_error = self.__current_bad_config.last_error",
            "            config_result.last_good_read = self.__config.read_time",
            "            config_result.last_check_time = self.__last_config_check_time",
            "        else:",
            "            config_result.path = self.__config.file_path",
            "            config_result.additional_paths = list(self.__config.additional_file_paths)",
            "            config_result.last_read_time = self.__config.read_time",
            "            config_result.status = \"Good\"",
            "            config_result.last_error = None",
            "            config_result.last_good_read = self.__config.read_time",
            "",
            "        # Include the copying and monitors status.",
            "        if self.__copying_manager is not None:",
            "            result.copying_manager_status = self.__copying_manager.generate_status(",
            "                warn_on_rate_limit=warn_on_rate_limit",
            "            )",
            "        if self.__monitors_manager is not None:",
            "            result.monitor_manager_status = self.__monitors_manager.generate_status()",
            "",
            "        # Include GC stats (if enabled)",
            "        if self.__config.enable_gc_stats:",
            "            gc_stats = GCStatus()",
            "            gc_stats.garbage = len(gc.garbage)",
            "            result.gc_stats = gc_stats",
            "",
            "        return result",
            "",
            "    def __log_overall_stats(self, overall_stats):",
            "        \"\"\"Logs the agent_status message that we periodically write to the agent log to give overall stats.",
            "",
            "        This includes such metrics as the number of logs being copied, the total bytes copied, the number of",
            "        running monitors, etc.",
            "",
            "        @param overall_stats: The overall stats for the agent.",
            "        @type overall_stats: OverallStats",
            "        \"\"\"",
            "        stats = overall_stats",
            "        log.info(",
            "            'agent_status launch_time=\"%s\" version=\"%s\" watched_paths=%ld copying_paths=%ld total_bytes_copied=%ld '",
            "            \"total_bytes_skipped=%ld total_bytes_subsampled=%ld total_redactions=%ld total_bytes_failed=%ld \"",
            "            \"total_copy_request_errors=%ld total_monitor_reported_lines=%ld running_monitors=%ld dead_monitors=%ld \"",
            "            \"user_cpu_=%f system_cpu=%f ram_usage=%ld skipped_new_bytes=%ld skipped_preexisting_bytes=%ld \"",
            "            \"total_bytes_pending=%ld\"",
            "            % (",
            "                scalyr_util.format_time(stats.launch_time),",
            "                stats.version,",
            "                stats.num_watched_paths,",
            "                stats.num_copying_paths,",
            "                stats.total_bytes_copied,",
            "                stats.total_bytes_skipped,",
            "                stats.total_bytes_subsampled,",
            "                stats.total_redactions,",
            "                stats.total_bytes_failed,",
            "                stats.total_copy_requests_errors,",
            "                stats.total_monitor_reported_lines,",
            "                stats.num_running_monitors,",
            "                stats.num_dead_monitor,",
            "                stats.user_cpu,",
            "                stats.system_cpu,",
            "                stats.rss_size,",
            "                stats.skipped_new_bytes,",
            "                stats.skipped_preexisting_bytes,",
            "                stats.total_bytes_pending,",
            "            )",
            "        )",
            "",
            "    def __log_bandwidth_stats(self, overall_stats):",
            "        \"\"\"Logs the agent_requests message that we periodically write to the agent log to give overall request",
            "        stats.",
            "",
            "        This includes such metrics the total bytes sent and received, failed requests, etc.",
            "",
            "        @param overall_stats: The overall stats for the agent.",
            "        @type overall_stats: OverallStats",
            "        \"\"\"",
            "        stats = overall_stats",
            "        # Ok, this is cheating, but we are going to hide some debug information in this line when it is turned on.",
            "        if self.__config.debug_init:",
            "            extra = \" is_agent=%d\" % self.__controller.is_agent()",
            "        else:",
            "            extra = \"\"",
            "",
            "        log.info(",
            "            \"agent_requests requests_sent=%ld requests_failed=%ld bytes_sent=%ld compressed_bytes_sent=%ld bytes_received=%ld \"",
            "            \"request_latency_secs=%lf connections_created=%ld%s\"",
            "            % (",
            "                stats.total_requests_sent,",
            "                stats.total_requests_failed,",
            "                stats.total_request_bytes_sent,",
            "                stats.total_compressed_request_bytes_sent,",
            "                stats.total_response_bytes_received,",
            "                stats.total_request_latency_secs,",
            "                stats.total_connections_created,",
            "                extra,",
            "            )",
            "        )",
            "",
            "    def __log_copy_manager_stats(self, overall_stats):",
            "        \"\"\"Logs the copy_manager_status message that we periodically write to the agent log to give copying manager",
            "        stats.",
            "",
            "        This includes such metrics as the amount of times through the main loop and time spent in various sections.",
            "",
            "        @param overall_stats: The overall stats for the agent.",
            "        @type overall_stats: OverallStats",
            "        \"\"\"",
            "        stats = overall_stats",
            "",
            "        log.info(",
            "            \"copy_manager_status total_copy_iterations=%ld total_read_time=%lf total_compression_time=%lf total_waiting_time=%lf total_blocking_response_time=%lf \"",
            "            \"total_request_time=%lf total_pipelined_requests=%ld avg_bytes_produced_rate=%lf avg_bytes_copied_rate=%lf\"",
            "            % (",
            "                stats.total_copy_iterations,",
            "                stats.total_read_time,",
            "                stats.total_compression_time,",
            "                stats.total_waiting_time,",
            "                stats.total_blocking_response_time,",
            "                stats.total_request_time,",
            "                stats.total_pipelined_requests,",
            "                stats.avg_bytes_produced_rate,",
            "                stats.avg_bytes_copied_rate,",
            "            )",
            "        )",
            "",
            "    def __calculate_overall_stats(",
            "        self, base_overall_stats, copy_manager_warnings=False",
            "    ):",
            "        \"\"\"Return a newly calculated overall stats for the agent.",
            "",
            "        This will calculate the latest stats based on the running agent.  Since most stats only can be",
            "        calculated since the last time the configuration file changed and was read, we need to seperately",
            "        track the accumulated stats that occurred before the last config change.",
            "",
            "        @param base_overall_stats: The accummulated stats from before the last config change.",
            "        @type base_overall_stats: OverallStats",
            "",
            "        @return:  The combined stats",
            "        @rtype: OverallStats",
            "        \"\"\"",
            "        current_status = self.__generate_status(",
            "            warn_on_rate_limit=copy_manager_warnings",
            "        )",
            "",
            "        delta_stats = OverallStats()",
            "",
            "        watched_paths = 0",
            "        copying_paths = 0",
            "",
            "        # Accumulate all the stats from the running processors that are copying log files.",
            "        if current_status.copying_manager_status is not None:",
            "            delta_stats.total_copy_requests_errors = (",
            "                current_status.copying_manager_status.total_errors",
            "            )",
            "            delta_stats.total_rate_limited_time = (",
            "                current_status.copying_manager_status.total_rate_limited_time",
            "            )",
            "            delta_stats.total_copy_iterations = (",
            "                current_status.copying_manager_status.total_copy_iterations",
            "            )",
            "            delta_stats.total_read_time = (",
            "                current_status.copying_manager_status.total_read_time",
            "            )",
            "            delta_stats.total_waiting_time = (",
            "                current_status.copying_manager_status.total_waiting_time",
            "            )",
            "            delta_stats.total_blocking_response_time = (",
            "                current_status.copying_manager_status.total_blocking_response_time",
            "            )",
            "            delta_stats.total_request_time = (",
            "                current_status.copying_manager_status.total_request_time",
            "            )",
            "            delta_stats.total_pipelined_requests = (",
            "                current_status.copying_manager_status.total_pipelined_requests",
            "            )",
            "            delta_stats.rate_limited_time_since_last_status = (",
            "                current_status.copying_manager_status.rate_limited_time_since_last_status",
            "            )",
            "            watched_paths = len(current_status.copying_manager_status.log_matchers)",
            "            for matcher in current_status.copying_manager_status.log_matchers:",
            "                copying_paths += len(matcher.log_processors_status)",
            "                for processor_status in matcher.log_processors_status:",
            "                    delta_stats.total_bytes_copied += (",
            "                        processor_status.total_bytes_copied",
            "                    )",
            "                    delta_stats.total_bytes_pending += (",
            "                        processor_status.total_bytes_pending",
            "                    )",
            "                    delta_stats.total_bytes_skipped += (",
            "                        processor_status.total_bytes_skipped",
            "                    )",
            "                    delta_stats.skipped_new_bytes += processor_status.skipped_new_bytes",
            "                    delta_stats.skipped_preexisting_bytes += (",
            "                        processor_status.skipped_preexisting_bytes",
            "                    )",
            "                    delta_stats.total_bytes_subsampled += (",
            "                        processor_status.total_bytes_dropped_by_sampling",
            "                    )",
            "                    delta_stats.total_bytes_failed += (",
            "                        processor_status.total_bytes_failed",
            "                    )",
            "                    delta_stats.total_redactions += processor_status.total_redactions",
            "",
            "        running_monitors = 0",
            "        dead_monitors = 0",
            "",
            "        if current_status.monitor_manager_status is not None:",
            "            running_monitors = (",
            "                current_status.monitor_manager_status.total_alive_monitors",
            "            )",
            "            dead_monitors = (",
            "                len(current_status.monitor_manager_status.monitors_status)",
            "                - running_monitors",
            "            )",
            "            for monitor_status in current_status.monitor_manager_status.monitors_status:",
            "                delta_stats.total_monitor_reported_lines += (",
            "                    monitor_status.reported_lines",
            "                )",
            "                delta_stats.total_monitor_errors += monitor_status.errors",
            "",
            "        delta_stats.total_requests_sent = self.__scalyr_client.total_requests_sent",
            "        delta_stats.total_requests_failed = self.__scalyr_client.total_requests_failed",
            "        delta_stats.total_request_bytes_sent = (",
            "            self.__scalyr_client.total_request_bytes_sent",
            "        )",
            "        delta_stats.total_compressed_request_bytes_sent = (",
            "            self.__scalyr_client.total_compressed_request_bytes_sent",
            "        )",
            "        delta_stats.total_response_bytes_received = (",
            "            self.__scalyr_client.total_response_bytes_received",
            "        )",
            "        delta_stats.total_request_latency_secs = (",
            "            self.__scalyr_client.total_request_latency_secs",
            "        )",
            "        delta_stats.total_connections_created = (",
            "            self.__scalyr_client.total_connections_created",
            "        )",
            "        delta_stats.total_compression_time = self.__scalyr_client.total_compression_time",
            "",
            "        # Add in the latest stats to the stats before the last restart.",
            "        result = delta_stats + base_overall_stats",
            "",
            "        # Overwrite some of the stats that are not affected by the add operation.",
            "        result.launch_time = current_status.launch_time",
            "        result.version = current_status.version",
            "        result.num_watched_paths = watched_paths",
            "        result.num_copying_paths = copying_paths",
            "        result.num_running_monitors = running_monitors",
            "        result.num_dead_monitors = dead_monitors",
            "",
            "        (",
            "            result.user_cpu,",
            "            result.system_cpu,",
            "            result.rss_size,",
            "        ) = self.__controller.get_usage_info()",
            "",
            "        if copy_manager_warnings:",
            "            result.avg_bytes_copied_rate = (",
            "                result.total_bytes_copied - self.__last_total_bytes_copied",
            "            ) / self.__config.copying_manager_stats_log_interval",
            "",
            "            total_bytes_produced = (",
            "                result.total_bytes_skipped",
            "                + result.total_bytes_copied",
            "                + result.total_bytes_pending",
            "            )",
            "            last_total_bytes_produced = (",
            "                self.__last_total_bytes_skipped",
            "                + self.__last_total_bytes_copied",
            "                + self.__last_total_bytes_pending",
            "            )",
            "            result.avg_bytes_produced_rate = (",
            "                total_bytes_produced - last_total_bytes_produced",
            "            ) / self.__config.copying_manager_stats_log_interval",
            "            if result.total_bytes_skipped > self.__last_total_bytes_skipped:",
            "                if self.__config.parsed_max_send_rate_enforcement:",
            "                    log.warning(",
            "                        \"Warning, skipping copying log lines.  Only copied %.1f MB/s log bytes when %.1f MB/s \"",
            "                        \"were generated over the last %.1f minutes. This may be due to \"",
            "                        \"max_send_rate_enforcement. Log upload has been delayed %.1f seconds in the last \"",
            "                        \"%.1f minutes  This may be desired (due to excessive \"",
            "                        \"bytes from a problematic log file).  Please contact support@scalyr.com for additional \"",
            "                        \"help.\"",
            "                        % (",
            "                            result.avg_bytes_copied_rate / 1000000,",
            "                            result.avg_bytes_produced_rate / 1000000,",
            "                            self.__config.copying_manager_stats_log_interval / 60.0,",
            "                            result.rate_limited_time_since_last_status,",
            "                            self.__config.copying_manager_stats_log_interval / 60.0,",
            "                        )",
            "                    )",
            "                else:",
            "                    log.warning(",
            "                        \"Warning, skipping copying log lines.  Only copied %.1f MB/s log bytes when %.1f MB/s \"",
            "                        \"were generated over the last %.1f minutes.  This may be desired (due to excessive \"",
            "                        \"bytes from a problematic log file).  Please contact support@scalyr.com for additional \"",
            "                        \"help.\"",
            "                        % (",
            "                            result.avg_bytes_copied_rate / 1000000,",
            "                            result.avg_bytes_produced_rate / 1000000,",
            "                            self.__config.copying_manager_stats_log_interval / 60.0,",
            "                        )",
            "                    )",
            "            self.__last_total_bytes_skipped = result.total_bytes_skipped",
            "            self.__last_total_bytes_copied = result.total_bytes_copied",
            "            self.__last_total_bytes_pending = result.total_bytes_pending",
            "",
            "        return result",
            "",
            "    def __report_status_to_file(self):",
            "        # type: () -> str",
            "        \"\"\"",
            "        Handles the signal sent to request this process write its current detailed status out.",
            "",
            "        :return: File path status data has been written to.",
            "        :rtype: ``str``",
            "        \"\"\"",
            "        # First determine the format user request. If no file with the requested format, we assume",
            "        # text format is used (this way it's backward compatible and works correctly on upgraded)",
            "        status_format = \"text\"",
            "",
            "        status_format_file = os.path.join(",
            "            self.__config.agent_data_path, STATUS_FORMAT_FILE",
            "        )",
            "        if os.path.isfile(status_format_file):",
            "            with open(status_format_file, \"r\") as fp:",
            "                status_format = fp.read().strip()",
            "",
            "        if not status_format or status_format not in VALID_STATUS_FORMATS:",
            "            status_format = \"text\"",
            "",
            "        tmp_file = None",
            "        try:",
            "            # We do a little dance to write the status.  We write it to a temporary file first, and then",
            "            # move it into the real location after the write has finished.  This way, the process watching",
            "            # the file we are writing does not accidentally read it when it is only partially written.",
            "            tmp_file_path = os.path.join(",
            "                self.__config.agent_data_path, \"last_status.tmp\"",
            "            )",
            "            final_file_path = os.path.join(self.__config.agent_data_path, \"last_status\")",
            "",
            "            if os.path.isfile(final_file_path):",
            "                os.remove(final_file_path)",
            "            tmp_file = open(tmp_file_path, \"w\")",
            "",
            "            agent_status = self.__generate_status()",
            "",
            "            if not status_format or status_format == \"text\":",
            "                report_status(tmp_file, agent_status, time.time())",
            "            elif status_format == \"json\":",
            "                status_data = agent_status.to_dict()",
            "                status_data[\"overall_stats\"] = self.__overall_stats.to_dict()",
            "                tmp_file.write(scalyr_util.json_encode(status_data))",
            "",
            "            tmp_file.close()",
            "            tmp_file = None",
            "",
            "            os.rename(tmp_file_path, final_file_path)",
            "        except (OSError, IOError):",
            "            log.exception(",
            "                \"Exception caught will try to report status\", error_code=\"failedStatus\"",
            "            )",
            "            if tmp_file is not None:",
            "                tmp_file.close()",
            "",
            "        log.log(",
            "            scalyr_logging.DEBUG_LEVEL_4,",
            "            'Wrote agent status data in \"%s\" format to %s'",
            "            % (status_format, final_file_path),",
            "        )",
            "",
            "        return final_file_path",
            "",
            "",
            "class WorkerThread(object):",
            "    \"\"\"A thread used to run the log copier and the monitor manager.",
            "    \"\"\"",
            "",
            "    def __init__(self, configuration, copying_manager, monitors):",
            "        self.__scalyr_client = None",
            "        self.config = configuration",
            "        self.copying_manager = copying_manager",
            "        self.monitors_manager = monitors",
            "",
            "    def start(self, scalyr_client, log_initial_positions=None):",
            "        if self.__scalyr_client is not None:",
            "            self.__scalyr_client.close()",
            "        self.__scalyr_client = scalyr_client",
            "",
            "        self.copying_manager.start_manager(scalyr_client, log_initial_positions)",
            "        # We purposely wait for the copying manager to begin copying so that if the monitors create any new",
            "        # files, they will be guaranteed to be copying up to the server starting at byte index zero.",
            "        # Note, if copying never begins then the copying manager will sys exit, so this next call will never just",
            "        # block indefinitely will the process hangs around.",
            "        self.copying_manager.wait_for_copying_to_begin()",
            "        self.monitors_manager.start_manager()",
            "",
            "    def stop(self):",
            "        log.debug(\"Shutting down monitors\")",
            "        self.monitors_manager.stop_manager()",
            "",
            "        log.debug(\"Shutting copy monitors\")",
            "        self.copying_manager.stop_manager()",
            "",
            "        log.debug(\"Shutting client\")",
            "        if self.__scalyr_client is not None:",
            "            self.__scalyr_client.close()",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    my_controller = PlatformController.new_platform()",
            "    parser = OptionParser(",
            "        usage=\"Usage: scalyr-agent-2 [options] (start|stop|status|restart|condrestart|version)\",",
            "        version=\"scalyr-agent v\" + SCALYR_VERSION,",
            "    )",
            "    parser.add_option(",
            "        \"-c\",",
            "        \"--config-file\",",
            "        dest=\"config_filename\",",
            "        help=\"Read configuration from FILE\",",
            "        metavar=\"FILE\",",
            "    )",
            "    parser.add_option(",
            "        \"--extra-config-dir\",",
            "        default=None,",
            "        help=\"An extra directory to check for configuration files\",",
            "        metavar=\"PATH\",",
            "    )",
            "    parser.add_option(",
            "        \"-q\",",
            "        \"--quiet\",",
            "        action=\"store_true\",",
            "        dest=\"quiet\",",
            "        default=False,",
            "        help=\"Only print error messages when running the start, stop, and condrestart commands\",",
            "    )",
            "    parser.add_option(",
            "        \"-v\",",
            "        \"--verbose\",",
            "        action=\"store_true\",",
            "        dest=\"verbose\",",
            "        default=False,",
            "        help=\"For status command, prints detailed information about running agent.\",",
            "    )",
            "    parser.add_option(",
            "        \"-H\",",
            "        \"--health_check\",",
            "        action=\"store_true\",",
            "        dest=\"health_check\",",
            "        default=False,",
            "        help=\"For status command, prints health check status. Return code will be 0 for a passing check, and 2 for failing\",",
            "    )",
            "    parser.add_option(",
            "        \"--format\",",
            "        dest=\"status_format\",",
            "        default=\"text\",",
            "        help=\"Format to use (text / json) for the agent status command.\",",
            "    )",
            "",
            "    parser.add_option(",
            "        \"\",",
            "        \"--no-fork\",",
            "        action=\"store_true\",",
            "        dest=\"no_fork\",",
            "        default=False,",
            "        help=\"For the run command, does not fork the program to the background.\",",
            "    )",
            "    parser.add_option(",
            "        \"\",",
            "        \"--no-check-remote-server\",",
            "        action=\"store_true\",",
            "        dest=\"no_check_remote\",",
            "        help=\"For the start command, does not perform the first check to see if the agent can \"",
            "        \"communicate with the Scalyr servers.  The agent will just keep trying to contact it in \"",
            "        \"the backgroudn until it is successful.  This is useful if the network is not immediately \"",
            "        \"available when the agent starts.\",",
            "    )",
            "    my_controller.add_options(parser)",
            "",
            "    (options, args) = parser.parse_args()",
            "    my_controller.consume_options(options)",
            "",
            "    if len(args) < 1:",
            "        print(",
            "            'You must specify a command, such as \"start\", \"stop\", or \"status\".',",
            "            file=sys.stderr,",
            "        )",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "    elif len(args) > 1:",
            "        print(",
            "            'Too many commands specified.  Only specify one of \"start\", \"stop\", \"status\".',",
            "            file=sys.stderr,",
            "        )",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "    elif args[0] not in (",
            "        \"start\",",
            "        \"stop\",",
            "        \"status\",",
            "        \"restart\",",
            "        \"condrestart\",",
            "        \"version\",",
            "    ):",
            "        print('Unknown command given: \"%s\"' % args[0], file=sys.stderr)",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "",
            "    if options.config_filename is not None and not os.path.isabs(",
            "        options.config_filename",
            "    ):",
            "        options.config_filename = os.path.abspath(options.config_filename)",
            "",
            "    main_rc = 1",
            "    try:",
            "        main_rc = ScalyrAgent(my_controller).main(",
            "            options.config_filename, args[0], options",
            "        )",
            "    except Exception as mainExcept:",
            "        print(six.text_type(mainExcept), file=sys.stderr)",
            "        sys.exit(1)",
            "",
            "    # We do this outside of the try block above because sys.exit raises an exception itself.",
            "    sys.exit(main_rc)"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python",
            "# Copyright 2014 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ------------------------------------------------------------------------",
            "#",
            "# flake8: noqa: E266",
            "###",
            "# chkconfig: 2345 98 02",
            "# description: Manages the Scalyr Agent 2, which provides log copying",
            "#     and basic system metric collection.",
            "###",
            "### BEGIN INIT INFO",
            "# Provides: scalyr-agent-2",
            "# Required-Start: $network",
            "# Required-Stop: $network",
            "# Default-Start: 2 3 4 5",
            "# Default-Stop: 0 1 6",
            "# Description: Manages the Scalyr Agent 2, which provides log copying",
            "#     and back system metric collection.",
            "### END INIT INFO",
            "#",
            "# author: Steven Czerwinski <czerwin@scalyr.com>",
            "from __future__ import unicode_literals",
            "from __future__ import print_function",
            "from __future__ import absolute_import",
            "",
            "__author__ = \"czerwin@scalyr.com\"",
            "",
            "import traceback",
            "import errno",
            "import gc",
            "import os",
            "import sys",
            "import time",
            "import re",
            "import ssl",
            "from io import open",
            "",
            "try:",
            "    from __scalyr__ import SCALYR_VERSION",
            "    from __scalyr__ import scalyr_init",
            "    from __scalyr__ import INSTALL_TYPE",
            "    from __scalyr__ import DEV_INSTALL",
            "    from __scalyr__ import MSI_INSTALL",
            "except ImportError:",
            "    from scalyr_agent.__scalyr__ import SCALYR_VERSION",
            "    from scalyr_agent.__scalyr__ import scalyr_init",
            "    from scalyr_agent.__scalyr__ import INSTALL_TYPE",
            "    from scalyr_agent.__scalyr__ import DEV_INSTALL",
            "    from scalyr_agent.__scalyr__ import MSI_INSTALL",
            "",
            "# We must invoke this since we are an executable script.",
            "scalyr_init()",
            "",
            "import six",
            "",
            "import scalyr_agent.scalyr_logging as scalyr_logging",
            "import scalyr_agent.util as scalyr_util",
            "import scalyr_agent.remote_shell as remote_shell",
            "",
            "# We have to be careful to set this logger class very early in processing, even before other",
            "# imports to ensure that any loggers created are AgentLoggers.",
            "from scalyr_agent.monitors_manager import MonitorsManager",
            "from scalyr_agent.scalyr_monitor import UnsupportedSystem",
            "",
            "# Set up the main logger.  We set it up initially to log to standard out,",
            "# but once we run fork off the daemon, we will use a rotating log file.",
            "log = scalyr_logging.getLogger(\"scalyr_agent\")",
            "",
            "scalyr_logging.set_log_destination(use_stdout=True)",
            "",
            "",
            "from optparse import OptionParser",
            "",
            "from scalyr_agent.profiler import ScalyrProfiler",
            "from scalyr_agent.scalyr_client import ScalyrClientSession",
            "from scalyr_agent.copying_manager import CopyingManager",
            "from scalyr_agent.configuration import Configuration",
            "from scalyr_agent.util import RunState, ScriptEscalator",
            "from scalyr_agent.agent_status import AgentStatus",
            "from scalyr_agent.agent_status import ConfigStatus",
            "from scalyr_agent.agent_status import OverallStats",
            "from scalyr_agent.agent_status import GCStatus",
            "from scalyr_agent.agent_status import report_status",
            "from scalyr_agent.platform_controller import (",
            "    PlatformController,",
            "    AgentAlreadyRunning,",
            "    CannotExecuteAsUser,",
            ")",
            "from scalyr_agent.platform_controller import AgentNotRunning",
            "from scalyr_agent.build_info import get_build_revision",
            "",
            "",
            "STATUS_FILE = \"last_status\"",
            "STATUS_FORMAT_FILE = \"status_format\"",
            "",
            "VALID_STATUS_FORMATS = [\"text\", \"json\"]",
            "",
            "AGENT_LOG_FILENAME = \"agent.log\"",
            "",
            "AGENT_NOT_RUNNING_MESSAGE = \"The agent does not appear to be running.\"",
            "",
            "",
            "def _update_disabled_until(config_value, current_time):",
            "    if config_value is not None:",
            "        return config_value + current_time",
            "    else:",
            "        return current_time",
            "",
            "",
            "def _check_disabled(current_time, other_time, message):",
            "    result = current_time < other_time",
            "    if result:",
            "        log.log(",
            "            scalyr_logging.DEBUG_LEVEL_0,",
            "            \"%s disabled for %d more seconds\" % (message, other_time - current_time),",
            "        )",
            "    return result",
            "",
            "",
            "class ScalyrAgent(object):",
            "    \"\"\"Encapsulates the entire Scalyr Agent 2 application.",
            "    \"\"\"",
            "",
            "    def __init__(self, platform_controller):",
            "        \"\"\"Initialize the object.",
            "",
            "        @param platform_controller:  The controller for this platform.",
            "        @type platform_controller: PlatformController",
            "        \"\"\"",
            "        # NOTE:  This abstraction is not thread safe, but it does not need to be.  Even the calls to",
            "        # create the status file are always issued on the main thread since that's how signals are handled.",
            "",
            "        # The current config being used to run the agent.  This may not be the latest",
            "        # version of the config, if that latest version had parsing errors.",
            "        self.__config = None",
            "        # The platform-specific controller that does things like fork daemon processes, sleeps, etc.",
            "        self.__controller = platform_controller",
            "        # A helper for running this script as another user if need be.",
            "        self.__escalator = None",
            "",
            "        # The DefaultPaths object for defining the default paths for various things like the log directory based on",
            "        # the platform.",
            "        self.__default_paths = platform_controller.default_paths",
            "",
            "        self.__config_file_path = None",
            "",
            "        # An extra directory for config snippets",
            "        self.__extra_config_dir = None",
            "",
            "        # If the current contents of the configuration file has errors in it, then this will be set to the config",
            "        # object produced by reading it.",
            "        self.__current_bad_config = None",
            "        # The last time the configuration file was checked to see if it had changed.",
            "        self.__last_config_check_time = None",
            "        self.__start_time = None",
            "        # The path where the agent log file is being written.",
            "        self.__log_file_path = None",
            "        # The current copying manager.",
            "        self.__copying_manager = None",
            "        # The current monitors manager.",
            "        self.__monitors_manager = None",
            "        # The current ScalyrClientSession to use for sending requests.",
            "        self.__scalyr_client = None",
            "",
            "        # Tracks whether or not the agent should still be running.  When a terminate signal is received,",
            "        # the run state is set to false.  Threads are expected to notice this and finish as quickly as",
            "        # possible.",
            "        self.__run_state = None",
            "",
            "        # Store references to the last value of the OverallStats instance",
            "        self.__overall_stats = OverallStats()",
            "",
            "        # Whether or not the unsafe debugging mode is running (meaning the RemoteShell is accepting connections",
            "        # on the local host port and the memory profiler is turned on).  Note, this mode is very unsafe since",
            "        # arbitrary python commands can be executed by any user on the system as the user running the agent.",
            "        self.__unsafe_debugging_running = False",
            "        # A reference to the remote shell debug server.",
            "        self.__debug_server = None",
            "        # Used below for a small cache for a slight optimization.",
            "        self.__last_verify_config = None",
            "",
            "        self.__no_fork = False",
            "        self.__last_total_bytes_skipped = 0",
            "        self.__last_total_bytes_copied = 0",
            "        self.__last_total_bytes_pending = 0",
            "        self.__last_total_rate_limited_time = 0",
            "",
            "    @staticmethod",
            "    def agent_run_method(controller, config_file_path, perform_config_check=False):",
            "        \"\"\"Begins executing the agent service on the current thread.",
            "",
            "        This will not return until the service is requested to terminate.",
            "",
            "        This method can be used as an entry point by PlatformControllers that cannot invoke the ``agent_run_method``",
            "        argument passed in the ``start_agent_service`` method.  It immediately because execution of the service.",
            "",
            "        @param controller: The controller to use to run the service.",
            "        @param config_file_path: The path to the configuration file to use.",
            "        @param perform_config_check:  If true, will check common configuration errors such as forgetting to",
            "            provide an api token and raise an exception if they fail.",
            "",
            "        @type controller: PlatformController",
            "        @type config_file_path: six.text_type",
            "        @type perform_config_check: bool",
            "",
            "        @return: The return code when the agent exits.",
            "        @rtype: int",
            "        \"\"\"",
            "",
            "        class Options(object):",
            "            pass",
            "",
            "        my_options = Options()",
            "        my_options.quiet = True",
            "        my_options.verbose = False",
            "        my_options.health_check = False",
            "        my_options.status_format = \"text\"",
            "        my_options.no_fork = True",
            "        my_options.no_change_user = True",
            "        my_options.no_check_remote = False",
            "        my_options.extra_config_dir = None",
            "",
            "        if perform_config_check:",
            "            command = \"inner_run_with_checks\"",
            "        else:",
            "            command = \"inner_run\"",
            "        try:",
            "            return ScalyrAgent(controller).main(config_file_path, command, my_options)",
            "        except Exception:",
            "            log.exception(\"Agent failed while running.  Will be shutting down.\")",
            "            raise",
            "",
            "    def main(self, config_file_path, command, command_options):",
            "        \"\"\"Run the Scalyr Agent.",
            "",
            "        @param config_file_path: The path to the configuration file.",
            "        @param command: The command passed in at the commandline for the agent to execute, such as 'start', 'stop', etc.",
            "        @param command_options: The options from the commandline.  These will include 'quiet', 'verbose', etc.",
            "",
            "        @type config_file_path: six.text_type",
            "        @type command: six.text_type",
            "",
            "        @return:  The exit status code to exit with, such as 0 for success.",
            "        @rtype: int",
            "        \"\"\"",
            "        quiet = command_options.quiet",
            "        verbose = command_options.verbose",
            "        health_check = command_options.health_check",
            "        status_format = command_options.status_format",
            "        extra_config_dir = command_options.extra_config_dir",
            "        self.__no_fork = command_options.no_fork",
            "        no_check_remote = False",
            "",
            "        self.__extra_config_dir = Configuration.get_extra_config_dir(extra_config_dir)",
            "",
            "        # We process for the 'version' command early since we do not need the configuration file for it.",
            "        if command == \"version\":",
            "            print(\"The Scalyr Agent 2 version is %s\" % SCALYR_VERSION)",
            "            return 0",
            "",
            "        # Read the configuration file.  Fail if we can't read it, unless the command is stop or status.",
            "        if config_file_path is None:",
            "            config_file_path = self.__default_paths.config_file_path",
            "",
            "        self.__config_file_path = config_file_path",
            "",
            "        try:",
            "            self.__config = self.__read_and_verify_config(config_file_path)",
            "",
            "            # check if not a tty and override the no check remote variable",
            "            if not sys.stdout.isatty():",
            "                no_check_remote = not self.__config.check_remote_if_no_tty",
            "        except Exception as e:",
            "            # We ignore a bad configuration file for 'stop' and 'status' because sometimes you do just accidentally",
            "            # screw up the config and you want to let the rest of the system work enough to do the stop or get the",
            "            # status.",
            "            if command != \"stop\" and command != \"status\":",
            "                import traceback",
            "",
            "                raise Exception(",
            "                    \"Error reading configuration file: %s\\n\"",
            "                    \"Terminating agent, please fix the configuration file and restart agent.\\n%s\"",
            "                    % (six.text_type(e), traceback.format_exc())",
            "                )",
            "            else:",
            "                self.__config = None",
            "                print(",
            "                    \"Could not parse configuration file at '%s'\" % config_file_path,",
            "                    file=sys.stderr,",
            "                )",
            "",
            "        self.__controller.consume_config(self.__config, config_file_path)",
            "",
            "        self.__escalator = ScriptEscalator(",
            "            self.__controller,",
            "            config_file_path,",
            "            os.getcwd(),",
            "            command_options.no_change_user,",
            "        )",
            "",
            "        if command_options.no_check_remote is not None:",
            "            no_check_remote = True",
            "",
            "        # noinspection PyBroadException",
            "        try:",
            "            # Execute the command.",
            "            if command == \"start\":",
            "                return self.__start(quiet, no_check_remote)",
            "            elif command == \"inner_run_with_checks\":",
            "                self.__perform_config_checks(no_check_remote)",
            "                return self.__run(self.__controller)",
            "            elif command == \"inner_run\":",
            "                return self.__run(self.__controller)",
            "            elif command == \"stop\":",
            "                return self.__stop(quiet)",
            "            elif command == \"status\" and not (verbose or health_check):",
            "                return self.__status()",
            "            elif command == \"status\" and (verbose or health_check):",
            "                if self.__config is not None:",
            "                    agent_data_path = self.__config.agent_data_path",
            "                else:",
            "                    agent_data_path = self.__default_paths.agent_data_path",
            "                    print(",
            "                        \"Assuming agent data path is '%s'\" % agent_data_path,",
            "                        file=sys.stderr,",
            "                    )",
            "                return self.__detailed_status(",
            "                    agent_data_path,",
            "                    status_format=status_format,",
            "                    health_check=health_check,",
            "                )",
            "            elif command == \"restart\":",
            "                return self.__restart(quiet, no_check_remote)",
            "            elif command == \"condrestart\":",
            "                return self.__condrestart(quiet, no_check_remote)",
            "            else:",
            "                print('Unknown command given: \"%s\".' % command, file=sys.stderr)",
            "                return 1",
            "        except SystemExit:",
            "            return 0",
            "        except Exception as e:",
            "            # We special case the inner_run_with checks since we know that exception is human-readable.",
            "            if command == \"inner_run_with_checks\":",
            "                raise e",
            "            else:",
            "                raise Exception(",
            "                    \"Caught exception when attempt to execute command %s.  Exception was %s\"",
            "                    % (command, six.text_type(e))",
            "                )",
            "",
            "    def __read_and_verify_config(self, config_file_path):",
            "        \"\"\"Reads the configuration and verifies it can be successfully parsed including the monitors existing and",
            "        having valid configurations.",
            "",
            "        @param config_file_path: The path to read the configuration from.",
            "        @type config_file_path: six.text_type",
            "",
            "        @return: The configuration object.",
            "        @rtype: scalyr_agent.Configuration",
            "        \"\"\"",
            "        config = self.__make_config(config_file_path)",
            "        self.__verify_config(config)",
            "        return config",
            "",
            "    def __make_config(self, config_file_path):",
            "        \"\"\"Make Configuration object. Does not read nor verify.",
            "",
            "        You must call ``__verify_config`` to read and fully verify the configuration.",
            "",
            "        @param config_file_path: The path to read the configuration from.",
            "        @type config_file_path: six.text_type",
            "",
            "        @return: The configuration object.",
            "        @rtype: scalyr_agent.Configuration",
            "        \"\"\"",
            "        return Configuration(",
            "            config_file_path,",
            "            self.__default_paths,",
            "            log,",
            "            extra_config_dir=self.__extra_config_dir,",
            "        )",
            "",
            "    def __verify_config(",
            "        self,",
            "        config,",
            "        disable_create_monitors_manager=False,",
            "        disable_create_copying_manager=False,",
            "        disable_cache_config=False,",
            "    ):",
            "        \"\"\"Verifies the passed-in configuration object is valid, and that the referenced monitors exist and have",
            "        valid configuration.",
            "",
            "        @param config: The configuration object.",
            "        @type config: scalyr_agent.Configuration",
            "        @return: A boolean value indicating whether or not the configuration was fully verified",
            "        \"\"\"",
            "        try:",
            "            config.parse()",
            "",
            "            if disable_create_monitors_manager:",
            "                log.info(\"verify_config - creation of monitors manager disabled\")",
            "                return False",
            "",
            "            monitors_manager = MonitorsManager(config, self.__controller)",
            "",
            "            if disable_create_copying_manager:",
            "                log.info(\"verify_config - creation of copying manager disabled\")",
            "                return False",
            "",
            "            copying_manager = CopyingManager(config, monitors_manager.monitors)",
            "            # To do the full verification, we have to create the managers.  However, this call does not need them,",
            "            # but it is very likely the caller of this method will invoke ``__create_worker_thread`` next, so let's",
            "            # save them for that call.  This helps us avoid having to read and instantiate the monitors multiple times.",
            "            if disable_cache_config:",
            "                log.info(\"verify_config - not caching verify_config results\")",
            "                self.__last_verify_config = None",
            "                # return true here because config is verified, just not cached",
            "                # this means the rest of the loop will continue but the config",
            "                # will be verified again when the worker thread is created",
            "                return True",
            "",
            "            self.__last_verify_config = {",
            "                \"config\": config,",
            "                \"monitors_manager\": monitors_manager,",
            "                \"copying_manager\": copying_manager,",
            "            }",
            "        except UnsupportedSystem as e:",
            "            # We want to emit a better error message for this exception, so capture it here.",
            "            raise Exception(",
            "                \"Configuration file uses a monitor that is not supported on this system Monitor '%s' \"",
            "                \"cannot be used due to: %s.  If you require support for this monitor for your system, \"",
            "                \"please e-mail contact@scalyr.com\" % (e.monitor_name, six.text_type(e))",
            "            )",
            "        return True",
            "",
            "    def __create_worker_thread(self, config):",
            "        \"\"\"Creates the worker thread that will run the copying and monitor managers for the specified configuration.",
            "",
            "        @param config: The configuration object.",
            "        @type config: scalyr_agent.Configuration",
            "",
            "        @return: The worker thread object to use.  You must start it.",
            "        @rtype: WorkerThread",
            "        \"\"\"",
            "        # Use the cached results from __last_verify_config if available.  If not, force it to create them.",
            "        if (",
            "            self.__last_verify_config is None",
            "            or self.__last_verify_config[\"config\"] is not config",
            "        ):",
            "            self.__verify_config(config)",
            "",
            "        # Apply any global config options",
            "        if self.__last_verify_config and self.__last_verify_config.get(\"config\", None):",
            "            self.__last_verify_config[\"config\"].apply_config()",
            "",
            "        return WorkerThread(",
            "            self.__last_verify_config[\"config\"],",
            "            self.__last_verify_config[\"copying_manager\"],",
            "            self.__last_verify_config[\"monitors_manager\"],",
            "        )",
            "",
            "    def __perform_config_checks(self, no_check_remote):",
            "        \"\"\"Perform checks for common configuration errors.  Raises an exception with a human-readable message",
            "        if any of the checks fail.",
            "",
            "        In particular, this checks if (1) the user has actually entered an api_key, (2) the agent process can",
            "        write to the logs directory, (3) we can send a request to the the configured scalyr server",
            "        and (4) the api key is correct.",
            "        \"\"\"",
            "        # Make sure the user has set an API key... a common step that can be forgotten.",
            "        # If they haven't set it, it will have REPLACE_THIS as the value since that's what is in the template.",
            "        if self.__config.api_key == \"REPLACE_THIS\" or self.__config.api_key == \"\":",
            "            raise Exception(",
            "                \"Error, you have not set a valid api key in the configuration file.\\n\"",
            "                'Edit the file %s and replace the value for \"api_key\" with a valid logs '",
            "                \"write key for your account.\\n\"",
            "                \"You can see your write logs keys at https://www.scalyr.com/keys\"",
            "                % self.__config.file_path",
            "            )",
            "",
            "        self.__verify_can_write_to_logs_and_data(self.__config)",
            "",
            "        # Begin writing to the log once we confirm we are able to, so we can log any connection errors",
            "        scalyr_logging.set_log_destination(",
            "            use_disk=True,",
            "            no_fork=self.__no_fork,",
            "            stdout_severity=self.__config.stdout_severity,",
            "            max_bytes=self.__config.log_rotation_max_bytes,",
            "            backup_count=self.__config.log_rotation_backup_count,",
            "            logs_directory=self.__config.agent_log_path,",
            "            agent_log_file_path=AGENT_LOG_FILENAME,",
            "        )",
            "",
            "        # Send a test message to the server to make sure everything works.  If not, print a decent error message.",
            "        if not no_check_remote:",
            "            client = self.__create_client(quiet=True)",
            "            try:",
            "                ping_result = client.ping()",
            "                if ping_result != \"success\":",
            "                    if \"badClientClockSkew\" in ping_result:",
            "                        # TODO:  The server does not yet send this error message, but it will in the future.",
            "                        log.error(",
            "                            \"Sending request to the server failed due to bad clock skew.  The system \"",
            "                            \"clock on this host is too far off from actual time. The agent will keep \"",
            "                            \"trying to connect in the background.\"",
            "                        )",
            "                        print(",
            "                            \"Sending request to the server failed due to bad clock skew.  The system \"",
            "                            \"clock on this host is too far off from actual time. The agent will keep \"",
            "                            \"trying to connect in the background.\",",
            "                            file=sys.stderr,",
            "                        )",
            "                    elif \"invalidApiKey\" in ping_result:",
            "                        # TODO:  The server does not yet send this error message, but it will in the future.",
            "                        raise Exception(",
            "                            \"Sending request to the server failed due to an invalid API key.  This probably \"",
            "                            \"means the 'api_key' field in configuration file  '%s' is not correct.  \"",
            "                            \"Please visit https://www.scalyr.com/keys and copy a Write Logs key into the \"",
            "                            \"'api_key' field in the configuration file\"",
            "                            % self.__config.file_path",
            "                        )",
            "                    else:",
            "                        log.error(",
            "                            \"Failed to send request to the server.  The server address could be \"",
            "                            \"wrong, there could be a network connectivity issue, or the provided \"",
            "                            \"token could be incorrect. The agent will keep trying to connect in the \"",
            "                            \"background. You can disable this check with --no-check-remote-server.\"",
            "                        )",
            "                        print(",
            "                            \"Failed to send request to the server.  The server address could be \"",
            "                            \"wrong, there could be a network connectivity issue, or the provided \"",
            "                            \"token could be incorrect. The agent will keep trying to connect in the \"",
            "                            \"background. You can disable this check with --no-check-remote-server.\",",
            "                            file=sys.stderr,",
            "                        )",
            "            finally:",
            "                client.close()",
            "",
            "    def __start(self, quiet, no_check_remote):",
            "        \"\"\"Executes the start command.",
            "",
            "        This will perform some initial checks to see if the agent can be started, such as making sure it can",
            "        read and write to the logs and data directory, and that it can send a successful message to the",
            "        Scalyr servers (therefore verifying the authentication token is correct.)",
            "",
            "        After it determines that the agent is likely to be able to run, it will start the real agent.  If self.__no_fork",
            "        is False, then a new process will be started in the background and this method will return.  Otherwise,",
            "        this method will not return.",
            "",
            "        @param quiet: True if output should be kept to a minimal and only record errors that occur.",
            "        @param no_check_remote:  True if this method should not try to contact the remote Scalyr servers to",
            "            verify connectivity.  If it does try to contact the remote servers and it cannot connect, then",
            "            the script exits with a failure.",
            "        @type quiet: bool",
            "        @type no_check_remote: bool",
            "",
            "        @return:  The exit status code for the process.",
            "        @rtype: int",
            "        \"\"\"",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            return self.__escalator.change_user_and_rerun_script(",
            "                \"start the scalyr agent\"",
            "            )",
            "",
            "        # Make sure we do not try to start it up again.",
            "        self.__fail_if_already_running()",
            "",
            "        # noinspection PyBroadException",
            "        try:",
            "            self.__perform_config_checks(no_check_remote)",
            "        except Exception as e:",
            "            print(file=sys.stderr)",
            "            traceback.print_exc(file=sys.stderr)",
            "            print(",
            "                \"Terminating agent, please fix the error and restart the agent.\",",
            "                file=sys.stderr,",
            "            )",
            "            log.error(\"%s\" % six.text_type(e))",
            "            log.error(\"Terminating agent, please fix the error and restart the agent.\")",
            "            return 1",
            "",
            "        if sys.version_info[:2] < (2, 6):",
            "            print(",
            "                \"Warning, the Scalyr Agent will not support running on Python 2.4, 2.5 after Oct 2019\",",
            "                file=sys.stderr,",
            "            )",
            "            log.error(",
            "                \"Warning, the Scalyr Agent will not support running on Python 2.4, 2.5 after Oct 2019\"",
            "            )",
            "",
            "        if not self.__no_fork:",
            "            # Do one last check to just cut down on the window of race conditions.",
            "            self.__fail_if_already_running()",
            "",
            "            if not quiet:",
            "                if no_check_remote:",
            "                    print(\"Configuration verified, starting agent in background.\")",
            "                else:",
            "                    print(",
            "                        \"Configuration and server connection verified, starting agent in background.\"",
            "                    )",
            "            self.__controller.start_agent_service(self.__run, quiet, fork=True)",
            "        else:",
            "            self.__controller.start_agent_service(self.__run, quiet, fork=False)",
            "",
            "        return 0",
            "",
            "    def __handle_terminate(self):",
            "        \"\"\"Invoked when the process is requested to shutdown, such as by a signal\"\"\"",
            "        if self.__run_state.is_running():",
            "            log.info(\"Received signal to shutdown, attempt to shutdown cleanly.\")",
            "            self.__run_state.stop()",
            "",
            "    def __detailed_status(",
            "        self, data_directory, status_format=\"text\", health_check=False",
            "    ):",
            "        \"\"\"Execute the status -v or -H command.",
            "",
            "        This will request the current agent to dump its detailed status to a file in the data directory, which",
            "        this process will then read.",
            "",
            "        @param data_directory: The path to the data directory.",
            "        @type data_directory: str",
            "",
            "        @return:  An exit status code for the status command indicating success or failure.",
            "        @rtype: int",
            "        \"\"\"",
            "        # Health check ignores format but uses `json` under the hood",
            "        if health_check:",
            "            status_format = \"json\"",
            "",
            "        if status_format not in VALID_STATUS_FORMATS:",
            "            print(",
            "                \"Invalid status format: %s. Valid formats are: %s\"",
            "                % (status_format, \", \".join(VALID_STATUS_FORMATS))",
            "            )",
            "            return 1",
            "",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            try:",
            "                return self.__escalator.change_user_and_rerun_script(",
            "                    \"retrieved detailed status\", handle_error=False",
            "                )",
            "            except CannotExecuteAsUser:",
            "                # For now, we just ignore the error and try to get the status anyway.  This might work on Linux",
            "                # platforms depending on permissions.  This is legacy behavior.",
            "                pass",
            "",
            "        try:",
            "            self.__controller.is_agent_running(fail_if_not_running=True)",
            "        except AgentNotRunning as e:",
            "            print(AGENT_NOT_RUNNING_MESSAGE)",
            "            print(\"%s\" % six.text_type(e))",
            "            return 1",
            "",
            "        # The status works by sending telling the running agent to dump the status into a well known file and",
            "        # then we read it from there, echoing it to stdout.",
            "        if not os.path.isdir(data_directory):",
            "            print(",
            "                'Cannot get status due to bad config.  The data path \"%s\" is not a directory'",
            "                % data_directory,",
            "                file=sys.stderr,",
            "            )",
            "            return 1",
            "",
            "        status_file = os.path.join(data_directory, STATUS_FILE)",
            "        status_format_file = os.path.join(data_directory, STATUS_FORMAT_FILE)",
            "",
            "        # This users needs to zero out the current status file (if it exists), so they need write access to it.",
            "        # When we do create the status file, we give everyone read/write access, so it should not be an issue.",
            "        if os.path.isfile(status_file) and not os.access(status_file, os.W_OK):",
            "            print(",
            "                \"Cannot get status due to insufficient permissions.  The current user does not \"",
            "                'have write access to \"%s\" as required.' % status_file,",
            "                file=sys.stderr,",
            "            )",
            "            return 1",
            "",
            "        # Zero out the current file so that we can detect once the agent process has updated it.",
            "        if os.path.isfile(status_file):",
            "            f = open(status_file, \"w\")",
            "            f.truncate(0)",
            "            f.close()",
            "",
            "        # Write the file with the format we need to use",
            "        with open(status_format_file, \"w\") as fp:",
            "            status_format = six.text_type(status_format)",
            "            fp.write(status_format)",
            "",
            "        # Signal to the running process.  This should cause that process to write to the status file",
            "        result = self.__controller.request_agent_status()",
            "        if result is not None:",
            "            if result == errno.ESRCH:",
            "                print(AGENT_NOT_RUNNING_MESSAGE, file=sys.stderr)",
            "                return 1",
            "            elif result == errno.EPERM:",
            "                # TODO:  We probably should just get the name of the user running the agent and output it",
            "                # here, instead of hard coding it to root.",
            "                print(",
            "                    \"To view agent status, you must be running as the same user as the agent. \"",
            "                    \"Try running this command as root or Administrator.\",",
            "                    file=sys.stderr,",
            "                )",
            "                return 2",
            "",
            "        # We wait for five seconds at most to get the status.",
            "        deadline = time.time() + 5",
            "",
            "        # Now loop until we see it show up.",
            "        while True:",
            "            if os.path.isfile(status_file) and os.path.getsize(status_file) > 0:",
            "                break",
            "",
            "            if time.time() > deadline:",
            "                if self.__config is not None:",
            "                    agent_log = os.path.join(",
            "                        self.__config.agent_log_path, AGENT_LOG_FILENAME",
            "                    )",
            "                else:",
            "                    agent_log = os.path.join(",
            "                        self.__default_paths.agent_log_path, AGENT_LOG_FILENAME",
            "                    )",
            "                print(",
            "                    \"Failed to get status within 5 seconds.  Giving up.  The agent process is \"",
            "                    \"possibly stuck.  See %s for more details.\" % agent_log,",
            "                    file=sys.stderr,",
            "                )",
            "                return 1",
            "",
            "            time.sleep(0.03)",
            "",
            "        if not os.access(status_file, os.R_OK):",
            "            print(",
            "                \"Cannot get status due to insufficient permissions.  The current user does not \"",
            "                'have read access to \"%s\" as required.' % status_file,",
            "                file=sys.stderr,",
            "            )",
            "            return 1",
            "",
            "        return_code = 0",
            "        fp = open(status_file)",
            "        for line in fp:",
            "            if not health_check:",
            "                print(line.rstrip())",
            "",
            "            if status_format == \"json\" or health_check:",
            "                health_result = self.__find_health_result_in_status_json(line)",
            "                if health_result:",
            "                    if health_check:",
            "                        print(\"Health check: %s\" % health_result)",
            "                    if health_result != \"Good\":",
            "                        return_code = 2",
            "                elif health_check:",
            "                    print(\"Cannot get health check result.\")",
            "            elif (",
            "                status_format == \"text\"",
            "                and \"Health check:\" in line",
            "                and not re.match(r\"^Health check\\:\\s+Good$\", line.strip())",
            "            ):",
            "                return_code = 2",
            "        fp.close()",
            "        return return_code",
            "",
            "    @staticmethod",
            "    def __find_health_result_in_status_json(line):",
            "        try:",
            "            status = scalyr_util.json_decode(line)",
            "            if (",
            "                \"copying_manager_status\" in status",
            "                and \"health_check_result\" in status[\"copying_manager_status\"]",
            "            ):",
            "                return status[\"copying_manager_status\"][\"health_check_result\"]",
            "        except ValueError:",
            "            pass",
            "        return None",
            "",
            "    def __stop(self, quiet):",
            "        \"\"\"Stop the current agent.",
            "",
            "        @param quiet: Whether or not only errors should be written to stdout.",
            "        @type quiet: bool",
            "",
            "        @return: the exit status code",
            "        @rtype: int",
            "        \"\"\"",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            return self.__escalator.change_user_and_rerun_script(",
            "                \"stop the scalyr agent\"",
            "            )",
            "",
            "        try:",
            "            self.__controller.is_agent_running(fail_if_not_running=True)",
            "            status = self.__controller.stop_agent_service(quiet)",
            "            return status",
            "        except AgentNotRunning as e:",
            "            print(",
            "                \"Failed to stop the agent because it does not appear to be running.\",",
            "                file=sys.stderr,",
            "            )",
            "            print(\"%s\" % six.text_type(e), file=sys.stderr)",
            "            return 0  # For the sake of restart, we need to return non-error code here.",
            "",
            "    def __status(self):",
            "        \"\"\"Execute the 'status' command to indicate if the agent is running or not.",
            "",
            "        @return: The exit status code.  It will return zero only if it is running.",
            "        @rtype: int",
            "        \"\"\"",
            "        if self.__controller.is_agent_running():",
            "            print('The agent is running. For details, use \"scalyr-agent-2 status -v\".')",
            "            return 0",
            "        else:",
            "            print(AGENT_NOT_RUNNING_MESSAGE)",
            "            return 4",
            "",
            "    def __condrestart(self, quiet, no_check_remote):",
            "        \"\"\"Execute the 'condrestart' command which will only restart the agent if it is already running.",
            "        self.__no_form determines if this method should not fork a separate process to run the agent, but run it",
            "        directly instead.  If it is False, then a daemon process will be forked and will run the agent.",
            "",
            "        @param quiet: True if output should be kept to a minimal and only record errors that occur.",
            "        @param no_check_remote:  True if this method should not try to contact the remote Scalyr servers to",
            "            verify connectivity.  If it does try to contact the remote servers and it cannot connect, then",
            "            the script exits with a failure.",
            "",
            "        @type quiet: bool",
            "        @type no_check_remote: bool",
            "",
            "        @return: the exit status code",
            "        @rtype: int",
            "        \"\"\"",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            return self.__escalator.change_user_and_rerun_script(",
            "                \"restart the scalyr agent\"",
            "            )",
            "",
            "        if self.__controller.is_agent_running():",
            "            if not quiet:",
            "                print(\"Agent is running, restarting now.\")",
            "            if self.__stop(quiet) != 0:",
            "                print(",
            "                    \"Failed to stop the running agent.  Cannot restart until it is killed.\",",
            "                    file=sys.stderr,",
            "                )",
            "                return 1",
            "",
            "            return self.__start(quiet, no_check_remote)",
            "        elif not quiet:",
            "            print(\"Agent is not running, not restarting.\")",
            "            return 0",
            "        else:",
            "            return 0",
            "",
            "    def __restart(self, quiet, no_check_remote):",
            "        \"\"\"Execute the 'restart' which will start the agent, stopping the existing agent if it is running.",
            "        self.__no_fork determines if this method should not fork a separate process to run the agent, but run it",
            "        directly instead.  If it is False, then a daemon process will be forked and will run the agent.",
            "",
            "        @param quiet: True if output should be kept to a minimal and only record errors that occur.",
            "        @param no_check_remote:  True if this method should not try to contact the remote Scalyr servers to",
            "            verify connectivity.  If it does try to contact the remote servers and it cannot connect, then",
            "            the script exits with a failure.",
            "",
            "        @type quiet: bool",
            "        @type no_check_remote: bool",
            "",
            "        @return: the exit status code, zero if it was successfully restarted, non-zero if it was not running or",
            "            could not be started.",
            "        @rtype: int",
            "        \"\"\"",
            "        # First, see if we have to change the user that is executing this script to match the owner of the config.",
            "        if self.__escalator.is_user_change_required():",
            "            return self.__escalator.change_user_and_rerun_script(",
            "                \"restart the scalyr agent\"",
            "            )",
            "",
            "        if self.__controller.is_agent_running():",
            "            if not quiet:",
            "                print(\"Agent is running, stopping it now.\")",
            "            if self.__stop(quiet) != 0:",
            "                print(",
            "                    \"Failed to stop the running agent.  Cannot restart until it is killed\",",
            "                    file=sys.stderr,",
            "                )",
            "                return 1",
            "",
            "        return self.__start(quiet, no_check_remote)",
            "",
            "    def __print_force_https_message(self, scalyr_server, raw_scalyr_server):",
            "        \"\"\"Convenience function for printing a message stating whether the scalyr_server was forced to use https\"\"\"",
            "        if scalyr_server != raw_scalyr_server:",
            "            log.info(",
            "                \"Forcing https protocol for server url: %s -> %s.  You can prevent this by setting the `allow_http` global config option, but be mindful that there are security implications with doing this, including tramsitting your Scalyr api key over an insecure connection.\"",
            "                % (raw_scalyr_server, scalyr_server)",
            "            )",
            "",
            "    def __run(self, controller):",
            "        \"\"\"Runs the Scalyr Agent 2.",
            "",
            "        This method will not return until a TERM signal is received or a fatal error occurs.",
            "",
            "        @param controller The controller that started this agent service.",
            "        @type controller: PlatformController",
            "",
            "        @return: the exit status code",
            "        @rtype: int",
            "        \"\"\"",
            "",
            "        self.__start_time = time.time()",
            "        controller.register_for_termination(self.__handle_terminate)",
            "",
            "        # Register handler for when we get an interrupt signal.  That indicates we should dump the status to",
            "        # a file because a user has run the 'detailed_status' command.",
            "        self.__controller.register_for_status_requests(self.__report_status_to_file)",
            "",
            "        # The stats we track for the lifetime of the agent.  This variable tracks the accumulated stats since the",
            "        # last stat reset (the stats get reset every time we read a new configuration).",
            "        base_overall_stats = OverallStats()",
            "",
            "        # We only emit the overall stats once ever ten minutes.  Track when we last reported it.",
            "        last_overall_stats_report_time = self.__start_time",
            "        # We only emit the bandwidth stats once every minute.  Track when we last reported it.",
            "        last_bw_stats_report_time = self.__start_time",
            "        # We only emit the copying_manager stats once every 5 minutes.  Track when we last reported it.",
            "        last_copy_manager_stats_report_time = self.__start_time",
            "",
            "        # The thread that runs the monitors and and the log copier.",
            "        worker_thread = None",
            "",
            "        try:",
            "            # noinspection PyBroadException",
            "            try:",
            "                self.__run_state = RunState()",
            "                self.__log_file_path = os.path.join(",
            "                    self.__config.agent_log_path, AGENT_LOG_FILENAME",
            "                )",
            "                scalyr_logging.set_log_destination(",
            "                    use_disk=True,",
            "                    no_fork=self.__no_fork,",
            "                    stdout_severity=self.__config.stdout_severity,",
            "                    max_bytes=self.__config.log_rotation_max_bytes,",
            "                    backup_count=self.__config.log_rotation_backup_count,",
            "                    logs_directory=self.__config.agent_log_path,",
            "                    agent_log_file_path=AGENT_LOG_FILENAME,",
            "                )",
            "",
            "                self.__update_debug_log_level(self.__config.debug_level)",
            "",
            "                # We record where the log file currently is so that we can (in the worse case) start copying it",
            "                # from this position.  That way we capture the first 'Starting scalyr agent' call.",
            "                agent_log_position = self.__get_file_initial_position(",
            "                    self.__log_file_path",
            "                )",
            "                if agent_log_position is not None:",
            "                    logs_initial_positions = {self.__log_file_path: agent_log_position}",
            "                else:",
            "                    logs_initial_positions = None",
            "",
            "                # 2->TODO it was very helpful to see what python version does agent run on. Maybe we can keep it?",
            "                python_version_str = sys.version.replace(\"\\n\", \"\")",
            "                build_revision = get_build_revision()",
            "                openssl_version = getattr(ssl, \"OPENSSL_VERSION\", \"unknown\")",
            "",
            "                # TODO: Why do we log the same line under info and debug? Intentional?",
            "                msg = (",
            "                    \"Starting scalyr agent... (version=%s) (revision=%s) %s (Python version: %s) \"",
            "                    \"(OpenSSL version: %s)\"",
            "                    % (",
            "                        SCALYR_VERSION,",
            "                        build_revision,",
            "                        scalyr_util.get_pid_tid(),",
            "                        python_version_str,",
            "                        openssl_version,",
            "                    )",
            "                )",
            "",
            "                log.info(msg)",
            "                log.log(scalyr_logging.DEBUG_LEVEL_1, msg)",
            "",
            "                self.__controller.emit_init_log(log, self.__config.debug_init)",
            "",
            "                self.__start_or_stop_unsafe_debugging()",
            "",
            "                scalyr_server = self.__config.scalyr_server",
            "                raw_scalyr_server = self.__config.raw_scalyr_server",
            "                self.__print_force_https_message(scalyr_server, raw_scalyr_server)",
            "",
            "                self.__config.print_useful_settings()",
            "",
            "                self.__scalyr_client = self.__create_client()",
            "",
            "                def start_worker_thread(config, logs_initial_positions=None):",
            "                    wt = self.__create_worker_thread(config)",
            "                    # attach callbacks before starting monitors",
            "                    wt.monitors_manager.set_user_agent_augment_callback(",
            "                        self.__scalyr_client.augment_user_agent",
            "                    )",
            "                    wt.start(self.__scalyr_client, logs_initial_positions)",
            "                    return wt, wt.copying_manager, wt.monitors_manager",
            "",
            "                (",
            "                    worker_thread,",
            "                    self.__copying_manager,",
            "                    self.__monitors_manager,",
            "                ) = start_worker_thread(self.__config, logs_initial_positions)",
            "",
            "                # JSON library setting is applied as part of __create_worker_thread method",
            "                log.log(",
            "                    scalyr_logging.DEBUG_LEVEL_0,",
            "                    'Using JSON library \"%s\"' % (scalyr_util.get_json_lib()),",
            "                )",
            "",
            "                log.log(",
            "                    scalyr_logging.DEBUG_LEVEL_0,",
            "                    'Using \"%s\" compression algorithm with level \"%s\"'",
            "                    % (self.__config.compression_type, self.__config.compression_level),",
            "                )",
            "",
            "                current_time = time.time()",
            "",
            "                disable_all_config_updates_until = _update_disabled_until(",
            "                    self.__config.disable_all_config_updates, current_time",
            "                )",
            "                disable_verify_config_until = _update_disabled_until(",
            "                    self.__config.disable_verify_config, current_time",
            "                )",
            "                disable_config_equivalence_check_until = _update_disabled_until(",
            "                    self.__config.disable_config_equivalence_check, current_time",
            "                )",
            "                disable_verify_can_write_to_logs_until = _update_disabled_until(",
            "                    self.__config.disable_verify_can_write_to_logs, current_time",
            "                )",
            "                disable_config_reload_until = _update_disabled_until(",
            "                    self.__config.disable_config_reload, current_time",
            "                )",
            "                disable_verify_config_create_monitors_manager_until = _update_disabled_until(",
            "                    self.__config.disable_verify_config_create_monitors_manager,",
            "                    current_time,",
            "                )",
            "                disable_verify_config_create_copying_manager_until = _update_disabled_until(",
            "                    self.__config.disable_verify_config_create_copying_manager,",
            "                    current_time,",
            "                )",
            "",
            "                config_change_check_interval = (",
            "                    self.__config.config_change_check_interval",
            "                )",
            "",
            "                gc_interval = self.__config.garbage_collect_interval",
            "                last_gc_time = current_time",
            "",
            "                prev_server = scalyr_server",
            "",
            "                profiler = ScalyrProfiler(self.__config)",
            "",
            "                while not self.__run_state.sleep_but_awaken_if_stopped(",
            "                    config_change_check_interval",
            "                ):",
            "                    current_time = time.time()",
            "                    self.__last_config_check_time = current_time",
            "",
            "                    profiler.update(self.__config, current_time)",
            "",
            "                    if self.__config.disable_overall_stats:",
            "                        log.log(scalyr_logging.DEBUG_LEVEL_0, \"overall stats disabled\")",
            "                    else:",
            "                        # Log the overall stats once every 10 mins (by default)",
            "                        log_stats_delta = self.__config.overall_stats_log_interval",
            "                        if (",
            "                            current_time",
            "                            > last_overall_stats_report_time + log_stats_delta",
            "                        ):",
            "                            self.__overall_stats = self.__calculate_overall_stats(",
            "                                base_overall_stats,",
            "                            )",
            "                            self.__log_overall_stats(self.__overall_stats)",
            "                            last_overall_stats_report_time = current_time",
            "",
            "                    if self.__config.disable_bandwidth_stats:",
            "                        log.log(",
            "                            scalyr_logging.DEBUG_LEVEL_0, \"bandwidth stats disabled\"",
            "                        )",
            "                    else:",
            "                        # Log the bandwidth-related stats once every minute:",
            "                        log_stats_delta = self.__config.bandwidth_stats_log_interval",
            "                        if current_time > last_bw_stats_report_time + log_stats_delta:",
            "                            self.__overall_stats = self.__calculate_overall_stats(",
            "                                base_overall_stats",
            "                            )",
            "",
            "                            self.__log_bandwidth_stats(",
            "                                self.__calculate_overall_stats(self.__overall_stats)",
            "                            )",
            "                            last_bw_stats_report_time = current_time",
            "",
            "                    if self.__config.disable_copy_manager_stats:",
            "                        log.log(",
            "                            scalyr_logging.DEBUG_LEVEL_0, \"copy manager stats disabled\"",
            "                        )",
            "                    else:",
            "                        # Log the copy manager stats once every 5 mins (by default)",
            "                        log_stats_delta = (",
            "                            self.__config.copying_manager_stats_log_interval",
            "                        )",
            "                        if (",
            "                            current_time",
            "                            > last_copy_manager_stats_report_time + log_stats_delta",
            "                        ):",
            "                            self.__overall_stats = self.__calculate_overall_stats(",
            "                                base_overall_stats, copy_manager_warnings=True,",
            "                            )",
            "                            self.__log_copy_manager_stats(self.__overall_stats)",
            "                            last_copy_manager_stats_report_time = current_time",
            "",
            "                    log.log(",
            "                        scalyr_logging.DEBUG_LEVEL_1,",
            "                        \"Checking for any changes to config file\",",
            "                    )",
            "                    new_config = None",
            "                    try:",
            "                        if _check_disabled(",
            "                            current_time,",
            "                            disable_all_config_updates_until,",
            "                            \"all config updates\",",
            "                        ):",
            "                            continue",
            "",
            "                        new_config = self.__make_config(self.__config_file_path)",
            "                        # TODO:  By parsing the configuration file, we are doing a lot of work just to have it thrown",
            "                        # out in a few seconds when we discover it is equivalent to the previous one.  Maybe we should",
            "                        # rework the equivalence so that it can work on the raw files, but this is difficult since",
            "                        # we need to parse the main configuration file to at least get the fragment directory.  For",
            "                        # now, we will just wait this work.  We only do it once every 30 secs anyway.",
            "",
            "                        if _check_disabled(",
            "                            current_time, disable_verify_config_until, \"verify config\"",
            "                        ):",
            "                            continue",
            "",
            "                        disable_create_monitors_manager = _check_disabled(",
            "                            current_time,",
            "                            disable_verify_config_create_monitors_manager_until,",
            "                            \"verify config create monitors manager\",",
            "                        )",
            "                        disable_create_copying_manager = _check_disabled(",
            "                            current_time,",
            "                            disable_verify_config_create_copying_manager_until,",
            "                            \"verify config create copying manager\",",
            "                        )",
            "                        disable_cache_config = (",
            "                            self.__config.disable_verify_config_cache_config",
            "                        )",
            "                        verified = self.__verify_config(",
            "                            new_config,",
            "                            disable_create_monitors_manager=disable_create_monitors_manager,",
            "                            disable_create_copying_manager=disable_create_copying_manager,",
            "                            disable_cache_config=disable_cache_config,",
            "                        )",
            "",
            "                        # Skip the rest of the loop if the config wasn't fully verified because",
            "                        # later code relies on the config being fully validated",
            "                        if not verified:",
            "                            continue",
            "",
            "                        if self.__config.disable_update_debug_log_level:",
            "                            log.log(",
            "                                scalyr_logging.DEBUG_LEVEL_0,",
            "                                \"update debug_log_level disabled\",",
            "                            )",
            "                        else:",
            "                            # Update the debug_level based on the new config.. we always update it.",
            "                            self.__update_debug_log_level(new_config.debug_level)",
            "",
            "                        # see if we need to perform a garbage collection",
            "                        if gc_interval > 0 and current_time > (",
            "                            last_gc_time + gc_interval",
            "                        ):",
            "                            gc.collect()",
            "                            last_gc_time = current_time",
            "",
            "                        if self.__config.enable_gc_stats:",
            "                            # If GC stats are enabled, enable tracking uncollectable objects",
            "                            if gc.get_debug() == 0:",
            "                                log.log(",
            "                                    scalyr_logging.DEBUG_LEVEL_5,",
            "                                    \"Enabling GC debug mode\",",
            "                                )",
            "                                gc.set_debug(gc.DEBUG_UNCOLLECTABLE)",
            "                        else:",
            "                            if gc.get_debug() != 0:",
            "                                log.log(",
            "                                    scalyr_logging.DEBUG_LEVEL_5,",
            "                                    \"Disabling GC debug mode\",",
            "                                )",
            "                                gc.set_debug(0)",
            "",
            "                        if _check_disabled(",
            "                            current_time,",
            "                            disable_config_equivalence_check_until,",
            "                            \"config equivalence check\",",
            "                        ):",
            "                            continue",
            "",
            "                        if self.__current_bad_config is None and new_config.equivalent(",
            "                            self.__config, exclude_debug_level=True",
            "                        ):",
            "                            log.log(",
            "                                scalyr_logging.DEBUG_LEVEL_1,",
            "                                \"Config was not different than previous\",",
            "                            )",
            "                            continue",
            "",
            "                        if _check_disabled(",
            "                            current_time,",
            "                            disable_verify_can_write_to_logs_until,",
            "                            \"verify check for writing to logs and data\",",
            "                        ):",
            "                            continue",
            "",
            "                        self.__verify_can_write_to_logs_and_data(new_config)",
            "",
            "                    except Exception as e:",
            "                        if self.__current_bad_config is None:",
            "                            log.error(",
            "                                \"Bad configuration file seen.  Ignoring, using last known good configuration file.  \"",
            "                                'Exception was \"%s\"',",
            "                                six.text_type(e),",
            "                                error_code=\"badConfigFile\",",
            "                            )",
            "                        self.__current_bad_config = new_config",
            "                        log.log(",
            "                            scalyr_logging.DEBUG_LEVEL_1,",
            "                            \"Config could not be read or parsed\",",
            "                        )",
            "                        continue",
            "",
            "                    if _check_disabled(",
            "                        current_time, disable_config_reload_until, \"config reload\"",
            "                    ):",
            "                        continue",
            "",
            "                    log.log(",
            "                        scalyr_logging.DEBUG_LEVEL_1,",
            "                        \"Config was different than previous.  Reloading.\",",
            "                    )",
            "                    # We are about to reset the current workers and ScalyrClientSession, so we will lose their",
            "                    # contribution to the stats, so recalculate the base.",
            "                    base_overall_stats = self.__calculate_overall_stats(",
            "                        base_overall_stats",
            "                    )",
            "                    log.info(\"New configuration file seen.\")",
            "                    log.info(\"Stopping copying and metrics threads.\")",
            "                    worker_thread.stop()",
            "",
            "                    worker_thread = None",
            "",
            "                    new_config.print_useful_settings(self.__config)",
            "",
            "                    self.__config = new_config",
            "                    self.__controller.consume_config(new_config, new_config.file_path)",
            "",
            "                    self.__start_or_stop_unsafe_debugging()",
            "",
            "                    # get the server and the raw server to see if we forced https",
            "                    scalyr_server = self.__config.scalyr_server",
            "                    raw_scalyr_server = self.__config.raw_scalyr_server",
            "",
            "                    # only print a message if this is the first time we have seen this scalyr_server",
            "                    # and the server field is different from the raw server field",
            "                    if scalyr_server != prev_server:",
            "                        self.__print_force_https_message(",
            "                            scalyr_server, raw_scalyr_server",
            "                        )",
            "",
            "                    prev_server = scalyr_server",
            "",
            "                    self.__scalyr_client = self.__create_client()",
            "",
            "                    log.info(\"Starting new copying and metrics threads\")",
            "                    (",
            "                        worker_thread,",
            "                        self.__copying_manager,",
            "                        self.__monitors_manager,",
            "                    ) = start_worker_thread(new_config)",
            "",
            "                    self.__current_bad_config = None",
            "                    config_change_check_interval = (",
            "                        self.__config.config_change_check_interval",
            "                    )",
            "                    gc_interval = self.__config.garbage_collect_interval",
            "",
            "                    disable_all_config_updates_until = _update_disabled_until(",
            "                        self.__config.disable_all_config_updates, current_time",
            "                    )",
            "                    disable_verify_config_until = _update_disabled_until(",
            "                        self.__config.disable_verify_config, current_time",
            "                    )",
            "                    disable_config_equivalence_check_until = _update_disabled_until(",
            "                        self.__config.disable_config_equivalence_check, current_time",
            "                    )",
            "                    disable_verify_can_write_to_logs_until = _update_disabled_until(",
            "                        self.__config.disable_verify_can_write_to_logs, current_time",
            "                    )",
            "                    disable_config_reload_until = _update_disabled_until(",
            "                        self.__config.disable_config_reload, current_time",
            "                    )",
            "                    disable_verify_config_create_monitors_manager_until = _update_disabled_until(",
            "                        self.__config.disable_verify_config_create_monitors_manager,",
            "                        current_time,",
            "                    )",
            "                    disable_verify_config_create_copying_manager_until = _update_disabled_until(",
            "                        self.__config.disable_verify_config_create_copying_manager,",
            "                        current_time,",
            "                    )",
            "",
            "                # Log the stats one more time before we terminate.",
            "                self.__log_overall_stats(",
            "                    self.__calculate_overall_stats(base_overall_stats)",
            "                )",
            "",
            "            except Exception:",
            "                log.exception(",
            "                    \"Main run method for agent failed due to exception\",",
            "                    error_code=\"failedAgentMain\",",
            "                )",
            "        finally:",
            "            if worker_thread is not None:",
            "                worker_thread.stop()",
            "",
            "            # NOTE: We manually call close_handlers() here instead of registering it to call it as",
            "            # part of run state stop routine.",
            "            # The reason for that is that run state stop callbacks are called before we get here",
            "            # which means that some messages which are produced after that and before fully shutting",
            "            # down are lost and not logged.",
            "            scalyr_logging.close_handlers()",
            "",
            "    def __fail_if_already_running(self):",
            "        \"\"\"If the agent is already running, prints an appropriate error message and exits the process.",
            "        \"\"\"",
            "        try:",
            "            self.__controller.is_agent_running(fail_if_running=True)",
            "        except AgentAlreadyRunning as e:",
            "            print(",
            "                \"Failed to start agent because it is already running.\", file=sys.stderr",
            "            )",
            "            print(\"%s\" % six.text_type(e), file=sys.stderr)",
            "            sys.exit(4)",
            "",
            "    def __update_debug_log_level(self, debug_level):",
            "        \"\"\"Updates the debug log level of the agent.",
            "        @param debug_level: The debug level, ranging from 0 (no debug) to 5.",
            "",
            "        @type debug_level: int",
            "        \"\"\"",
            "        levels = [",
            "            scalyr_logging.DEBUG_LEVEL_0,",
            "            scalyr_logging.DEBUG_LEVEL_1,",
            "            scalyr_logging.DEBUG_LEVEL_2,",
            "            scalyr_logging.DEBUG_LEVEL_3,",
            "            scalyr_logging.DEBUG_LEVEL_4,",
            "            scalyr_logging.DEBUG_LEVEL_5,",
            "        ]",
            "",
            "        scalyr_logging.set_log_level(levels[debug_level])",
            "",
            "    def __create_client(self, quiet=False):",
            "        \"\"\"Creates and returns a new client to the Scalyr servers.",
            "",
            "        @param quiet: If true, only errors should be written to stdout.",
            "        @type quiet: bool",
            "",
            "        @return: The client to use for sending requests to Scalyr, using the server address and API write logs",
            "            key in the configuration file.",
            "        @rtype: ScalyrClientSession",
            "        \"\"\"",
            "        if self.__config.verify_server_certificate:",
            "            is_dev_install = INSTALL_TYPE == DEV_INSTALL",
            "            is_dev_or_msi_install = INSTALL_TYPE in [DEV_INSTALL, MSI_INSTALL]",
            "",
            "            ca_file = self.__config.ca_cert_path",
            "            intermediate_certs_file = self.__config.intermediate_certs_path",
            "",
            "            # Validate provided CA cert file and intermediate cert file exists. If they don't",
            "            # exist, throw and fail early and loudly",
            "            if not is_dev_install and not os.path.isfile(ca_file):",
            "                raise ValueError(",
            "                    'Invalid path \"%s\" specified for the \"ca_cert_path\" config '",
            "                    \"option: file does not exist\" % (ca_file)",
            "                )",
            "",
            "            # NOTE: We don't include intermediate certs in the Windows binary so we skip that check",
            "            # under the MSI / Windows install",
            "            if not is_dev_or_msi_install and not os.path.isfile(",
            "                intermediate_certs_file",
            "            ):",
            "                raise ValueError(",
            "                    'Invalid path \"%s\" specified for the '",
            "                    '\"intermediate_certs_path\" config '",
            "                    \"option: file does not exist\" % (intermediate_certs_file)",
            "                )",
            "        else:",
            "            ca_file = None",
            "            intermediate_certs_file = None",
            "        use_requests_lib = self.__config.use_requests_lib",
            "        return ScalyrClientSession(",
            "            self.__config.scalyr_server,",
            "            self.__config.api_key,",
            "            SCALYR_VERSION,",
            "            quiet=quiet,",
            "            request_deadline=self.__config.request_deadline,",
            "            ca_file=ca_file,",
            "            intermediate_certs_file=intermediate_certs_file,",
            "            use_requests_lib=use_requests_lib,",
            "            compression_type=self.__config.compression_type,",
            "            compression_level=self.__config.compression_level,",
            "            proxies=self.__config.network_proxies,",
            "            disable_send_requests=self.__config.disable_send_requests,",
            "            disable_logfile_addevents_format=self.__config.disable_logfile_addevents_format,",
            "            enforce_monotonic_timestamps=self.__config.enforce_monotonic_timestamps,",
            "        )",
            "",
            "    def __get_file_initial_position(self, path):",
            "        \"\"\"Returns the file size for the specified file.",
            "",
            "        @param path: The path of the file",
            "        @type path: str",
            "",
            "        @return: The file size",
            "        @rtype: int",
            "        \"\"\"",
            "        try:",
            "            return os.path.getsize(path)",
            "        except OSError as e:",
            "            if e.errno == errno.EPERM:",
            "                log.warn(\"Insufficient permissions to read agent logs initial position\")",
            "                return None",
            "            elif e.errno == errno.ENOENT:",
            "                # If file doesn't exist, just return 0 as its initial position",
            "                return 0",
            "            else:",
            "                log.exception(\"Error trying to read agent logs initial position\")",
            "                return None",
            "",
            "    def __verify_can_write_to_logs_and_data(self, config):",
            "        \"\"\"Checks to make sure the user account running the agent has permission to read and write files",
            "        to the log and data directories as specified in the config.",
            "",
            "        If any verification fails, an exception is raised.",
            "",
            "        @param config: The configuration",
            "        @type config: Configuration",
            "        \"\"\"",
            "",
            "        if self.__controller.install_type == DEV_INSTALL:",
            "            # The agent is running from source, make sure that its directories exist.",
            "            if not os.path.exists(config.agent_log_path):",
            "                os.makedirs(config.agent_log_path)",
            "            if not os.path.exists(config.agent_data_path):",
            "                os.makedirs(config.agent_data_path)",
            "",
            "        if not os.path.isdir(config.agent_log_path):",
            "            raise Exception(",
            "                \"The agent log directory '%s' does not exist.\" % config.agent_log_path",
            "            )",
            "",
            "        if not os.access(config.agent_log_path, os.W_OK):",
            "            raise Exception(",
            "                \"User cannot write to agent log directory '%s'.\" % config.agent_log_path",
            "            )",
            "",
            "        if not os.path.isdir(config.agent_data_path):",
            "            raise Exception(",
            "                \"The agent data directory '%s' does not exist.\" % config.agent_data_path",
            "            )",
            "",
            "        if not os.access(config.agent_data_path, os.W_OK):",
            "            raise Exception(",
            "                \"User cannot write to agent data directory '%s'.\"",
            "                % config.agent_data_path",
            "            )",
            "",
            "    def __start_or_stop_unsafe_debugging(self):",
            "        \"\"\"Starts or stops the debugging tool.",
            "",
            "        This runs a thread that listens to a server port and connects any incoming connection to a shell",
            "        that can be used to execute Python commands to investigate issues on the live server.",
            "",
            "        Since opening up a socket is dangerous, we control this with a configuration option and only use it",
            "        when really needed.",
            "        \"\"\"",
            "        should_be_running = self.__config.use_unsafe_debugging",
            "",
            "        if should_be_running and not self.__unsafe_debugging_running:",
            "            self.__unsafe_debugging_running = True",
            "            self.__debug_server = remote_shell.DebugServer()",
            "            self.__debug_server.start()",
            "        elif not should_be_running and self.__unsafe_debugging_running:",
            "            if self.__debug_server is not None:",
            "                self.__debug_server.stop()",
            "                self.__debug_server = None",
            "            self.__unsafe_debugging_running = False",
            "",
            "    def __generate_status(self, warn_on_rate_limit=False):",
            "        \"\"\"Generates the server status object and returns it.",
            "",
            "        The returned status object is used to create the detailed status page.",
            "",
            "        @return: The status object filled in with the values from the current agent.",
            "        @rtype: AgentStatus",
            "        \"\"\"",
            "        # Basic agent stats first.",
            "        result = AgentStatus()",
            "        result.launch_time = self.__start_time",
            "        result.user = self.__controller.get_current_user()",
            "        result.revision = get_build_revision()",
            "        result.version = SCALYR_VERSION",
            "        result.server_host = self.__config.server_attributes[\"serverHost\"]",
            "        result.compression_type = self.__config.compression_type",
            "        result.compression_level = self.__config.compression_level",
            "        result.scalyr_server = self.__config.scalyr_server",
            "        result.log_path = self.__log_file_path",
            "        result.python_version = sys.version.replace(\"\\n\", \"\")",
            "",
            "        # Describe the status of the configuration file.",
            "        config_result = ConfigStatus()",
            "        result.config_status = config_result",
            "",
            "        config_result.last_check_time = self.__last_config_check_time",
            "        if self.__current_bad_config is not None:",
            "            config_result.path = self.__current_bad_config.file_path",
            "            config_result.additional_paths = list(",
            "                self.__current_bad_config.additional_file_paths",
            "            )",
            "            config_result.last_read_time = self.__current_bad_config.read_time",
            "            config_result.status = \"Error, using last good configuration\"",
            "            config_result.last_error = self.__current_bad_config.last_error",
            "            config_result.last_good_read = self.__config.read_time",
            "            config_result.last_check_time = self.__last_config_check_time",
            "        else:",
            "            config_result.path = self.__config.file_path",
            "            config_result.additional_paths = list(self.__config.additional_file_paths)",
            "            config_result.last_read_time = self.__config.read_time",
            "            config_result.status = \"Good\"",
            "            config_result.last_error = None",
            "            config_result.last_good_read = self.__config.read_time",
            "",
            "        # Include the copying and monitors status.",
            "        if self.__copying_manager is not None:",
            "            result.copying_manager_status = self.__copying_manager.generate_status(",
            "                warn_on_rate_limit=warn_on_rate_limit",
            "            )",
            "        if self.__monitors_manager is not None:",
            "            result.monitor_manager_status = self.__monitors_manager.generate_status()",
            "",
            "        # Include GC stats (if enabled)",
            "        if self.__config.enable_gc_stats:",
            "            gc_stats = GCStatus()",
            "            gc_stats.garbage = len(gc.garbage)",
            "            result.gc_stats = gc_stats",
            "",
            "        return result",
            "",
            "    def __log_overall_stats(self, overall_stats):",
            "        \"\"\"Logs the agent_status message that we periodically write to the agent log to give overall stats.",
            "",
            "        This includes such metrics as the number of logs being copied, the total bytes copied, the number of",
            "        running monitors, etc.",
            "",
            "        @param overall_stats: The overall stats for the agent.",
            "        @type overall_stats: OverallStats",
            "        \"\"\"",
            "        stats = overall_stats",
            "        log.info(",
            "            'agent_status launch_time=\"%s\" version=\"%s\" watched_paths=%ld copying_paths=%ld total_bytes_copied=%ld '",
            "            \"total_bytes_skipped=%ld total_bytes_subsampled=%ld total_redactions=%ld total_bytes_failed=%ld \"",
            "            \"total_copy_request_errors=%ld total_monitor_reported_lines=%ld running_monitors=%ld dead_monitors=%ld \"",
            "            \"user_cpu_=%f system_cpu=%f ram_usage=%ld skipped_new_bytes=%ld skipped_preexisting_bytes=%ld \"",
            "            \"total_bytes_pending=%ld\"",
            "            % (",
            "                scalyr_util.format_time(stats.launch_time),",
            "                stats.version,",
            "                stats.num_watched_paths,",
            "                stats.num_copying_paths,",
            "                stats.total_bytes_copied,",
            "                stats.total_bytes_skipped,",
            "                stats.total_bytes_subsampled,",
            "                stats.total_redactions,",
            "                stats.total_bytes_failed,",
            "                stats.total_copy_requests_errors,",
            "                stats.total_monitor_reported_lines,",
            "                stats.num_running_monitors,",
            "                stats.num_dead_monitor,",
            "                stats.user_cpu,",
            "                stats.system_cpu,",
            "                stats.rss_size,",
            "                stats.skipped_new_bytes,",
            "                stats.skipped_preexisting_bytes,",
            "                stats.total_bytes_pending,",
            "            )",
            "        )",
            "",
            "    def __log_bandwidth_stats(self, overall_stats):",
            "        \"\"\"Logs the agent_requests message that we periodically write to the agent log to give overall request",
            "        stats.",
            "",
            "        This includes such metrics the total bytes sent and received, failed requests, etc.",
            "",
            "        @param overall_stats: The overall stats for the agent.",
            "        @type overall_stats: OverallStats",
            "        \"\"\"",
            "        stats = overall_stats",
            "        # Ok, this is cheating, but we are going to hide some debug information in this line when it is turned on.",
            "        if self.__config.debug_init:",
            "            extra = \" is_agent=%d\" % self.__controller.is_agent()",
            "        else:",
            "            extra = \"\"",
            "",
            "        log.info(",
            "            \"agent_requests requests_sent=%ld requests_failed=%ld bytes_sent=%ld compressed_bytes_sent=%ld bytes_received=%ld \"",
            "            \"request_latency_secs=%lf connections_created=%ld%s\"",
            "            % (",
            "                stats.total_requests_sent,",
            "                stats.total_requests_failed,",
            "                stats.total_request_bytes_sent,",
            "                stats.total_compressed_request_bytes_sent,",
            "                stats.total_response_bytes_received,",
            "                stats.total_request_latency_secs,",
            "                stats.total_connections_created,",
            "                extra,",
            "            )",
            "        )",
            "",
            "    def __log_copy_manager_stats(self, overall_stats):",
            "        \"\"\"Logs the copy_manager_status message that we periodically write to the agent log to give copying manager",
            "        stats.",
            "",
            "        This includes such metrics as the amount of times through the main loop and time spent in various sections.",
            "",
            "        @param overall_stats: The overall stats for the agent.",
            "        @type overall_stats: OverallStats",
            "        \"\"\"",
            "        stats = overall_stats",
            "",
            "        log.info(",
            "            \"copy_manager_status total_copy_iterations=%ld total_read_time=%lf total_compression_time=%lf total_waiting_time=%lf total_blocking_response_time=%lf \"",
            "            \"total_request_time=%lf total_pipelined_requests=%ld avg_bytes_produced_rate=%lf avg_bytes_copied_rate=%lf\"",
            "            % (",
            "                stats.total_copy_iterations,",
            "                stats.total_read_time,",
            "                stats.total_compression_time,",
            "                stats.total_waiting_time,",
            "                stats.total_blocking_response_time,",
            "                stats.total_request_time,",
            "                stats.total_pipelined_requests,",
            "                stats.avg_bytes_produced_rate,",
            "                stats.avg_bytes_copied_rate,",
            "            )",
            "        )",
            "",
            "    def __calculate_overall_stats(",
            "        self, base_overall_stats, copy_manager_warnings=False",
            "    ):",
            "        \"\"\"Return a newly calculated overall stats for the agent.",
            "",
            "        This will calculate the latest stats based on the running agent.  Since most stats only can be",
            "        calculated since the last time the configuration file changed and was read, we need to seperately",
            "        track the accumulated stats that occurred before the last config change.",
            "",
            "        @param base_overall_stats: The accummulated stats from before the last config change.",
            "        @type base_overall_stats: OverallStats",
            "",
            "        @return:  The combined stats",
            "        @rtype: OverallStats",
            "        \"\"\"",
            "        current_status = self.__generate_status(",
            "            warn_on_rate_limit=copy_manager_warnings",
            "        )",
            "",
            "        delta_stats = OverallStats()",
            "",
            "        watched_paths = 0",
            "        copying_paths = 0",
            "",
            "        # Accumulate all the stats from the running processors that are copying log files.",
            "        if current_status.copying_manager_status is not None:",
            "            delta_stats.total_copy_requests_errors = (",
            "                current_status.copying_manager_status.total_errors",
            "            )",
            "            delta_stats.total_rate_limited_time = (",
            "                current_status.copying_manager_status.total_rate_limited_time",
            "            )",
            "            delta_stats.total_copy_iterations = (",
            "                current_status.copying_manager_status.total_copy_iterations",
            "            )",
            "            delta_stats.total_read_time = (",
            "                current_status.copying_manager_status.total_read_time",
            "            )",
            "            delta_stats.total_waiting_time = (",
            "                current_status.copying_manager_status.total_waiting_time",
            "            )",
            "            delta_stats.total_blocking_response_time = (",
            "                current_status.copying_manager_status.total_blocking_response_time",
            "            )",
            "            delta_stats.total_request_time = (",
            "                current_status.copying_manager_status.total_request_time",
            "            )",
            "            delta_stats.total_pipelined_requests = (",
            "                current_status.copying_manager_status.total_pipelined_requests",
            "            )",
            "            delta_stats.rate_limited_time_since_last_status = (",
            "                current_status.copying_manager_status.rate_limited_time_since_last_status",
            "            )",
            "            watched_paths = len(current_status.copying_manager_status.log_matchers)",
            "            for matcher in current_status.copying_manager_status.log_matchers:",
            "                copying_paths += len(matcher.log_processors_status)",
            "                for processor_status in matcher.log_processors_status:",
            "                    delta_stats.total_bytes_copied += (",
            "                        processor_status.total_bytes_copied",
            "                    )",
            "                    delta_stats.total_bytes_pending += (",
            "                        processor_status.total_bytes_pending",
            "                    )",
            "                    delta_stats.total_bytes_skipped += (",
            "                        processor_status.total_bytes_skipped",
            "                    )",
            "                    delta_stats.skipped_new_bytes += processor_status.skipped_new_bytes",
            "                    delta_stats.skipped_preexisting_bytes += (",
            "                        processor_status.skipped_preexisting_bytes",
            "                    )",
            "                    delta_stats.total_bytes_subsampled += (",
            "                        processor_status.total_bytes_dropped_by_sampling",
            "                    )",
            "                    delta_stats.total_bytes_failed += (",
            "                        processor_status.total_bytes_failed",
            "                    )",
            "                    delta_stats.total_redactions += processor_status.total_redactions",
            "",
            "        running_monitors = 0",
            "        dead_monitors = 0",
            "",
            "        if current_status.monitor_manager_status is not None:",
            "            running_monitors = (",
            "                current_status.monitor_manager_status.total_alive_monitors",
            "            )",
            "            dead_monitors = (",
            "                len(current_status.monitor_manager_status.monitors_status)",
            "                - running_monitors",
            "            )",
            "            for monitor_status in current_status.monitor_manager_status.monitors_status:",
            "                delta_stats.total_monitor_reported_lines += (",
            "                    monitor_status.reported_lines",
            "                )",
            "                delta_stats.total_monitor_errors += monitor_status.errors",
            "",
            "        delta_stats.total_requests_sent = self.__scalyr_client.total_requests_sent",
            "        delta_stats.total_requests_failed = self.__scalyr_client.total_requests_failed",
            "        delta_stats.total_request_bytes_sent = (",
            "            self.__scalyr_client.total_request_bytes_sent",
            "        )",
            "        delta_stats.total_compressed_request_bytes_sent = (",
            "            self.__scalyr_client.total_compressed_request_bytes_sent",
            "        )",
            "        delta_stats.total_response_bytes_received = (",
            "            self.__scalyr_client.total_response_bytes_received",
            "        )",
            "        delta_stats.total_request_latency_secs = (",
            "            self.__scalyr_client.total_request_latency_secs",
            "        )",
            "        delta_stats.total_connections_created = (",
            "            self.__scalyr_client.total_connections_created",
            "        )",
            "        delta_stats.total_compression_time = self.__scalyr_client.total_compression_time",
            "",
            "        # Add in the latest stats to the stats before the last restart.",
            "        result = delta_stats + base_overall_stats",
            "",
            "        # Overwrite some of the stats that are not affected by the add operation.",
            "        result.launch_time = current_status.launch_time",
            "        result.version = current_status.version",
            "        result.num_watched_paths = watched_paths",
            "        result.num_copying_paths = copying_paths",
            "        result.num_running_monitors = running_monitors",
            "        result.num_dead_monitors = dead_monitors",
            "",
            "        (",
            "            result.user_cpu,",
            "            result.system_cpu,",
            "            result.rss_size,",
            "        ) = self.__controller.get_usage_info()",
            "",
            "        if copy_manager_warnings:",
            "            result.avg_bytes_copied_rate = (",
            "                result.total_bytes_copied - self.__last_total_bytes_copied",
            "            ) / self.__config.copying_manager_stats_log_interval",
            "",
            "            total_bytes_produced = (",
            "                result.total_bytes_skipped",
            "                + result.total_bytes_copied",
            "                + result.total_bytes_pending",
            "            )",
            "            last_total_bytes_produced = (",
            "                self.__last_total_bytes_skipped",
            "                + self.__last_total_bytes_copied",
            "                + self.__last_total_bytes_pending",
            "            )",
            "            result.avg_bytes_produced_rate = (",
            "                total_bytes_produced - last_total_bytes_produced",
            "            ) / self.__config.copying_manager_stats_log_interval",
            "            if result.total_bytes_skipped > self.__last_total_bytes_skipped:",
            "                if self.__config.parsed_max_send_rate_enforcement:",
            "                    log.warning(",
            "                        \"Warning, skipping copying log lines.  Only copied %.1f MB/s log bytes when %.1f MB/s \"",
            "                        \"were generated over the last %.1f minutes. This may be due to \"",
            "                        \"max_send_rate_enforcement. Log upload has been delayed %.1f seconds in the last \"",
            "                        \"%.1f minutes  This may be desired (due to excessive \"",
            "                        \"bytes from a problematic log file).  Please contact support@scalyr.com for additional \"",
            "                        \"help.\"",
            "                        % (",
            "                            result.avg_bytes_copied_rate / 1000000,",
            "                            result.avg_bytes_produced_rate / 1000000,",
            "                            self.__config.copying_manager_stats_log_interval / 60.0,",
            "                            result.rate_limited_time_since_last_status,",
            "                            self.__config.copying_manager_stats_log_interval / 60.0,",
            "                        )",
            "                    )",
            "                else:",
            "                    log.warning(",
            "                        \"Warning, skipping copying log lines.  Only copied %.1f MB/s log bytes when %.1f MB/s \"",
            "                        \"were generated over the last %.1f minutes.  This may be desired (due to excessive \"",
            "                        \"bytes from a problematic log file).  Please contact support@scalyr.com for additional \"",
            "                        \"help.\"",
            "                        % (",
            "                            result.avg_bytes_copied_rate / 1000000,",
            "                            result.avg_bytes_produced_rate / 1000000,",
            "                            self.__config.copying_manager_stats_log_interval / 60.0,",
            "                        )",
            "                    )",
            "            self.__last_total_bytes_skipped = result.total_bytes_skipped",
            "            self.__last_total_bytes_copied = result.total_bytes_copied",
            "            self.__last_total_bytes_pending = result.total_bytes_pending",
            "",
            "        return result",
            "",
            "    def __report_status_to_file(self):",
            "        # type: () -> str",
            "        \"\"\"",
            "        Handles the signal sent to request this process write its current detailed status out.",
            "",
            "        :return: File path status data has been written to.",
            "        :rtype: ``str``",
            "        \"\"\"",
            "        # First determine the format user request. If no file with the requested format, we assume",
            "        # text format is used (this way it's backward compatible and works correctly on upgraded)",
            "        status_format = \"text\"",
            "",
            "        status_format_file = os.path.join(",
            "            self.__config.agent_data_path, STATUS_FORMAT_FILE",
            "        )",
            "        if os.path.isfile(status_format_file):",
            "            with open(status_format_file, \"r\") as fp:",
            "                status_format = fp.read().strip()",
            "",
            "        if not status_format or status_format not in VALID_STATUS_FORMATS:",
            "            status_format = \"text\"",
            "",
            "        tmp_file = None",
            "        try:",
            "            # We do a little dance to write the status.  We write it to a temporary file first, and then",
            "            # move it into the real location after the write has finished.  This way, the process watching",
            "            # the file we are writing does not accidentally read it when it is only partially written.",
            "            tmp_file_path = os.path.join(",
            "                self.__config.agent_data_path, \"last_status.tmp\"",
            "            )",
            "            final_file_path = os.path.join(self.__config.agent_data_path, \"last_status\")",
            "",
            "            if os.path.isfile(final_file_path):",
            "                os.remove(final_file_path)",
            "            tmp_file = open(tmp_file_path, \"w\")",
            "",
            "            agent_status = self.__generate_status()",
            "",
            "            if not status_format or status_format == \"text\":",
            "                report_status(tmp_file, agent_status, time.time())",
            "            elif status_format == \"json\":",
            "                status_data = agent_status.to_dict()",
            "                status_data[\"overall_stats\"] = self.__overall_stats.to_dict()",
            "                tmp_file.write(scalyr_util.json_encode(status_data))",
            "",
            "            tmp_file.close()",
            "            tmp_file = None",
            "",
            "            os.rename(tmp_file_path, final_file_path)",
            "        except (OSError, IOError):",
            "            log.exception(",
            "                \"Exception caught will try to report status\", error_code=\"failedStatus\"",
            "            )",
            "            if tmp_file is not None:",
            "                tmp_file.close()",
            "",
            "        log.log(",
            "            scalyr_logging.DEBUG_LEVEL_4,",
            "            'Wrote agent status data in \"%s\" format to %s'",
            "            % (status_format, final_file_path),",
            "        )",
            "",
            "        return final_file_path",
            "",
            "",
            "class WorkerThread(object):",
            "    \"\"\"A thread used to run the log copier and the monitor manager.",
            "    \"\"\"",
            "",
            "    def __init__(self, configuration, copying_manager, monitors):",
            "        self.__scalyr_client = None",
            "        self.config = configuration",
            "        self.copying_manager = copying_manager",
            "        self.monitors_manager = monitors",
            "",
            "    def start(self, scalyr_client, log_initial_positions=None):",
            "        if self.__scalyr_client is not None:",
            "            self.__scalyr_client.close()",
            "        self.__scalyr_client = scalyr_client",
            "",
            "        self.copying_manager.start_manager(scalyr_client, log_initial_positions)",
            "        # We purposely wait for the copying manager to begin copying so that if the monitors create any new",
            "        # files, they will be guaranteed to be copying up to the server starting at byte index zero.",
            "        # Note, if copying never begins then the copying manager will sys exit, so this next call will never just",
            "        # block indefinitely will the process hangs around.",
            "        self.copying_manager.wait_for_copying_to_begin()",
            "        self.monitors_manager.start_manager()",
            "",
            "    def stop(self):",
            "        log.debug(\"Shutting down monitors\")",
            "        self.monitors_manager.stop_manager()",
            "",
            "        log.debug(\"Shutting copy monitors\")",
            "        self.copying_manager.stop_manager()",
            "",
            "        log.debug(\"Shutting client\")",
            "        if self.__scalyr_client is not None:",
            "            self.__scalyr_client.close()",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    my_controller = PlatformController.new_platform()",
            "    parser = OptionParser(",
            "        usage=\"Usage: scalyr-agent-2 [options] (start|stop|status|restart|condrestart|version)\",",
            "        version=\"scalyr-agent v\" + SCALYR_VERSION,",
            "    )",
            "    parser.add_option(",
            "        \"-c\",",
            "        \"--config-file\",",
            "        dest=\"config_filename\",",
            "        help=\"Read configuration from FILE\",",
            "        metavar=\"FILE\",",
            "    )",
            "    parser.add_option(",
            "        \"--extra-config-dir\",",
            "        default=None,",
            "        help=\"An extra directory to check for configuration files\",",
            "        metavar=\"PATH\",",
            "    )",
            "    parser.add_option(",
            "        \"-q\",",
            "        \"--quiet\",",
            "        action=\"store_true\",",
            "        dest=\"quiet\",",
            "        default=False,",
            "        help=\"Only print error messages when running the start, stop, and condrestart commands\",",
            "    )",
            "    parser.add_option(",
            "        \"-v\",",
            "        \"--verbose\",",
            "        action=\"store_true\",",
            "        dest=\"verbose\",",
            "        default=False,",
            "        help=\"For status command, prints detailed information about running agent.\",",
            "    )",
            "    parser.add_option(",
            "        \"-H\",",
            "        \"--health_check\",",
            "        action=\"store_true\",",
            "        dest=\"health_check\",",
            "        default=False,",
            "        help=\"For status command, prints health check status. Return code will be 0 for a passing check, and 2 for failing\",",
            "    )",
            "    parser.add_option(",
            "        \"--format\",",
            "        dest=\"status_format\",",
            "        default=\"text\",",
            "        help=\"Format to use (text / json) for the agent status command.\",",
            "    )",
            "",
            "    parser.add_option(",
            "        \"\",",
            "        \"--no-fork\",",
            "        action=\"store_true\",",
            "        dest=\"no_fork\",",
            "        default=False,",
            "        help=\"For the run command, does not fork the program to the background.\",",
            "    )",
            "    parser.add_option(",
            "        \"\",",
            "        \"--no-check-remote-server\",",
            "        action=\"store_true\",",
            "        dest=\"no_check_remote\",",
            "        help=\"For the start command, does not perform the first check to see if the agent can \"",
            "        \"communicate with the Scalyr servers.  The agent will just keep trying to contact it in \"",
            "        \"the backgroudn until it is successful.  This is useful if the network is not immediately \"",
            "        \"available when the agent starts.\",",
            "    )",
            "    my_controller.add_options(parser)",
            "",
            "    (options, args) = parser.parse_args()",
            "    my_controller.consume_options(options)",
            "",
            "    if len(args) < 1:",
            "        print(",
            "            'You must specify a command, such as \"start\", \"stop\", or \"status\".',",
            "            file=sys.stderr,",
            "        )",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "    elif len(args) > 1:",
            "        print(",
            "            'Too many commands specified.  Only specify one of \"start\", \"stop\", \"status\".',",
            "            file=sys.stderr,",
            "        )",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "    elif args[0] not in (",
            "        \"start\",",
            "        \"stop\",",
            "        \"status\",",
            "        \"restart\",",
            "        \"condrestart\",",
            "        \"version\",",
            "    ):",
            "        print('Unknown command given: \"%s\"' % args[0], file=sys.stderr)",
            "        parser.print_help(sys.stderr)",
            "        sys.exit(1)",
            "",
            "    if options.config_filename is not None and not os.path.isabs(",
            "        options.config_filename",
            "    ):",
            "        options.config_filename = os.path.abspath(options.config_filename)",
            "",
            "    main_rc = 1",
            "    try:",
            "        main_rc = ScalyrAgent(my_controller).main(",
            "            options.config_filename, args[0], options",
            "        )",
            "    except Exception as mainExcept:",
            "        print(six.text_type(mainExcept), file=sys.stderr)",
            "        sys.exit(1)",
            "",
            "    # We do this outside of the try block above because sys.exit raises an exception itself.",
            "    sys.exit(main_rc)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "980": [
                "ScalyrAgent",
                "__run"
            ],
            "1417": [
                "ScalyrAgent",
                "__create_client"
            ],
            "1427": [
                "ScalyrAgent",
                "__create_client"
            ]
        },
        "addLocation": [
            "scalyr_agent.agent_main.ScalyrAgent.__run.logs_initial_positions",
            "src.pyload.core",
            "scalyr_agent.agent_main.ScalyrAgent.main",
            "scalyr_agent.agent_main.ScalyrAgent.__run.start_worker_thread.config"
        ]
    },
    "scalyr_agent/compat.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " import six"
            },
            "2": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+PY2 = sys.version_info[0] == 2"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+PY3 = sys.version_info[0] == 3"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+PY26 = sys.version_info[0] == 2 and sys.version_info[1] == 6"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+PY2_pre_279 = PY2 and sys.version_info < (2, 7, 9)"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+PY_post_equal_279 = sys.version_info >= (2, 7, 9)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+PY3_pre_32 = PY3 and sys.version_info < (3, 2)"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+# NOTE: ssl.match_hostname was added in Python 2.7.9 so for earlier versions, we need to use"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+# version from backports package"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+if PY2_pre_279 or PY3_pre_32:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+    try:"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+        from backports.ssl_match_hostname import ("
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+            match_hostname as ssl_match_hostname,"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+        )  # NOQA"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+        from backports.ssl_match_hostname import CertificateError  # NOQA"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+    except ImportError:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+        # NOTE: We should never come here in real life. If we do, it indicates we messed up package"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+        # creation and / or path mangling in scalyr_init()."
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+        raise Exception("
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+            \"Missing backports.ssl_match_hostname module, hostname verification can't \""
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+            \"be performed\""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+        )"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+else:"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+    # ssl module in Python 2 >= 2.7.9 and Python 3 >= 3.2 includes match hostname function"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+    from ssl import match_hostname as ssl_match_hostname  # NOQA"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+    from ssl import CertificateError  # type: ignore # NOQA"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " "
            },
            "31": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 55,
                "PatchRowcode": " def custom_any(iterable):"
            },
            "32": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "     if sys.version_info[:2] > (2, 4):"
            }
        },
        "frontPatchFile": [
            "# Copyright 2014-2020 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "from __future__ import absolute_import",
            "",
            "if False:  # NOSONAR",
            "    from typing import Union, Tuple, Any, Generator, Iterable, Optional",
            "",
            "import sys",
            "import struct",
            "import os",
            "import subprocess",
            "",
            "import six",
            "",
            "",
            "def custom_any(iterable):",
            "    if sys.version_info[:2] > (2, 4):",
            "        return any(iterable)",
            "    else:",
            "        for element in iterable:",
            "            if element:",
            "                return True",
            "        return False",
            "",
            "",
            "def custom_all(iterable):",
            "    if sys.version_info[:2] > (2, 4):",
            "        return all(iterable)",
            "    else:",
            "        for element in iterable:",
            "            if not element:",
            "                return False",
            "        return True",
            "",
            "",
            "def custom_defaultdict(default_type):",
            "    if sys.version_info[:2] > (2, 4):",
            "        from collections import defaultdict",
            "",
            "        return defaultdict(default_type)",
            "    else:",
            "",
            "        class DefaultDict(dict):",
            "            def __getitem__(self, key):",
            "                if key not in self:",
            "                    dict.__setitem__(self, key, default_type())",
            "                return dict.__getitem__(self, key)",
            "",
            "        return DefaultDict()",
            "",
            "",
            "if six.PY2:",
            "",
            "    class EnvironUnicode(object):",
            "        \"\"\"Just a wrapper for os.environ, to convert its items to unicode in python2.\"\"\"",
            "",
            "        def __getitem__(self, item):",
            "            value = os.environ[item]",
            "            return six.ensure_text(value)",
            "",
            "        def get(self, item, default=None):",
            "            value = os.environ.get(item, default)",
            "            if value is not None:",
            "                value = six.ensure_text(value)",
            "            return value",
            "",
            "        def pop(self, item, default=None):",
            "            value = os.environ.pop(item, default)",
            "            if value is not None:",
            "                value = six.ensure_text(value)",
            "            return value",
            "",
            "        def __setitem__(self, key, value):",
            "            key = six.ensure_text(key)",
            "            value = six.ensure_text(value)",
            "            os.environ[key] = value",
            "",
            "        @staticmethod",
            "        def _iterable_elements_to_unicode_generator(iterable):",
            "            # type: (Iterable) -> Generator[Union[Tuple, Any]]",
            "            \"\"\"Generator that gets values from original iterable and converts its 'str' values to 'unicode'\"\"\"",
            "            for element in iterable:",
            "                if type(element) is tuple:",
            "                    yield tuple(",
            "                        v.decode(\"utf-8\") if type(v) is six.binary_type else v",
            "                        for v in element",
            "                    )",
            "                else:",
            "                    yield six.ensure_text(element)",
            "",
            "        def iteritems(self):",
            "            return self._iterable_elements_to_unicode_generator(",
            "                six.iteritems(os.environ)",
            "            )",
            "",
            "        def items(self):",
            "            return list(",
            "                self._iterable_elements_to_unicode_generator(os.environ.items())",
            "            )",
            "",
            "        def iterkeys(self):",
            "            return self._iterable_elements_to_unicode_generator(",
            "                six.iterkeys(os.environ)",
            "            )",
            "",
            "        def keys(self):",
            "            return list(self._iterable_elements_to_unicode_generator(os.environ.keys()))",
            "",
            "        def itervalues(self):",
            "            return self._iterable_elements_to_unicode_generator(",
            "                six.itervalues(os.environ)",
            "            )",
            "",
            "        def values(self):",
            "            return list(",
            "                self._iterable_elements_to_unicode_generator(os.environ.values())",
            "            )",
            "",
            "        def copy(self):",
            "            return dict(self.items())",
            "",
            "        def __iter__(self):",
            "            return self.iterkeys()",
            "",
            "    def os_getenv_unicode(name, default=None):",
            "        \"\"\"The same logic as in os.environ, but with None check.\"\"\"",
            "        result = os.getenv(name, default)",
            "        if result is not None:",
            "            result = six.ensure_text(result)",
            "        return result",
            "",
            "    os_environ_unicode = EnvironUnicode()",
            "",
            "",
            "else:",
            "    os_environ_unicode = os.environ",
            "    os_getenv_unicode = os.getenv",
            "",
            "# 2->TODO struct.pack|unpack, does not accept unicode as format string.",
            "# see more: https://python-future.org/stdlib_incompatibilities.html#struct-pack",
            "# to avoid conversion of format string on every struct.pack call, we can monkey patch it here.",
            "if sys.version_info[:3] < (2, 7, 7):",
            "",
            "    def python_unicode_pack_unpack_wrapper(f):",
            "        def _pack_unpack(format_str, *args):",
            "            \"\"\"wrapper for struct.pack function that converts unicode format string to 'str'\"\"\"",
            "            binary_format_str = six.ensure_binary(format_str)",
            "            return f(binary_format_str, *args)",
            "",
            "        return _pack_unpack",
            "",
            "    struct_pack_unicode = python_unicode_pack_unpack_wrapper(struct.pack)",
            "    struct_unpack_unicode = python_unicode_pack_unpack_wrapper(struct.unpack)",
            "else:",
            "    struct_pack_unicode = struct.pack",
            "    struct_unpack_unicode = struct.unpack",
            "",
            "",
            "def which(executable):",
            "    # type: (str) -> Optional[str]",
            "    \"\"\"",
            "    Search for the provided executable in PATH and return path to it if found.",
            "    \"\"\"",
            "    paths = os.environ[\"PATH\"].split(os.pathsep)",
            "    for path in paths:",
            "        full_path = os.path.join(path, executable)",
            "",
            "        if os.path.exists(full_path) and os.access(full_path, os.X_OK):",
            "            return full_path",
            "",
            "    return None",
            "",
            "",
            "def find_executable(executable):",
            "    # type: (str) -> Optional[str]",
            "    \"\"\"",
            "    Wrapper around distutils.spawn.find_executable which is not available in some default Python 3",
            "    installations where full blown python3-distutils package is not installed.",
            "    \"\"\"",
            "    try:",
            "        from distutils.spawn import find_executable as distutils_find_executable",
            "    except ImportError:",
            "        # Likely Ubuntu 18.04 where python3-distutils package is not present (default behavior)",
            "        return which(executable)",
            "",
            "    return distutils_find_executable(executable)",
            "",
            "",
            "def subprocess_check_output(cmd, *args, **kwargs):",
            "    \"\"\"",
            "    Wrapper around subprocess.check_output which is not available under Python 2.6.",
            "    \"\"\"",
            "    if sys.version_info < (2, 7, 0):",
            "        output = subprocess.Popen(",
            "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, *args, **kwargs",
            "        ).communicate()[0]",
            "    else:",
            "        output = subprocess.check_output(cmd, *args, **kwargs)",
            "",
            "    return output"
        ],
        "afterPatchFile": [
            "# Copyright 2014-2020 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "from __future__ import absolute_import",
            "",
            "if False:  # NOSONAR",
            "    from typing import Union, Tuple, Any, Generator, Iterable, Optional",
            "",
            "import sys",
            "import struct",
            "import os",
            "import subprocess",
            "",
            "import six",
            "",
            "PY2 = sys.version_info[0] == 2",
            "PY3 = sys.version_info[0] == 3",
            "PY26 = sys.version_info[0] == 2 and sys.version_info[1] == 6",
            "PY2_pre_279 = PY2 and sys.version_info < (2, 7, 9)",
            "PY_post_equal_279 = sys.version_info >= (2, 7, 9)",
            "PY3_pre_32 = PY3 and sys.version_info < (3, 2)",
            "",
            "# NOTE: ssl.match_hostname was added in Python 2.7.9 so for earlier versions, we need to use",
            "# version from backports package",
            "if PY2_pre_279 or PY3_pre_32:",
            "    try:",
            "        from backports.ssl_match_hostname import (",
            "            match_hostname as ssl_match_hostname,",
            "        )  # NOQA",
            "        from backports.ssl_match_hostname import CertificateError  # NOQA",
            "    except ImportError:",
            "        # NOTE: We should never come here in real life. If we do, it indicates we messed up package",
            "        # creation and / or path mangling in scalyr_init().",
            "        raise Exception(",
            "            \"Missing backports.ssl_match_hostname module, hostname verification can't \"",
            "            \"be performed\"",
            "        )",
            "else:",
            "    # ssl module in Python 2 >= 2.7.9 and Python 3 >= 3.2 includes match hostname function",
            "    from ssl import match_hostname as ssl_match_hostname  # NOQA",
            "    from ssl import CertificateError  # type: ignore # NOQA",
            "",
            "",
            "def custom_any(iterable):",
            "    if sys.version_info[:2] > (2, 4):",
            "        return any(iterable)",
            "    else:",
            "        for element in iterable:",
            "            if element:",
            "                return True",
            "        return False",
            "",
            "",
            "def custom_all(iterable):",
            "    if sys.version_info[:2] > (2, 4):",
            "        return all(iterable)",
            "    else:",
            "        for element in iterable:",
            "            if not element:",
            "                return False",
            "        return True",
            "",
            "",
            "def custom_defaultdict(default_type):",
            "    if sys.version_info[:2] > (2, 4):",
            "        from collections import defaultdict",
            "",
            "        return defaultdict(default_type)",
            "    else:",
            "",
            "        class DefaultDict(dict):",
            "            def __getitem__(self, key):",
            "                if key not in self:",
            "                    dict.__setitem__(self, key, default_type())",
            "                return dict.__getitem__(self, key)",
            "",
            "        return DefaultDict()",
            "",
            "",
            "if six.PY2:",
            "",
            "    class EnvironUnicode(object):",
            "        \"\"\"Just a wrapper for os.environ, to convert its items to unicode in python2.\"\"\"",
            "",
            "        def __getitem__(self, item):",
            "            value = os.environ[item]",
            "            return six.ensure_text(value)",
            "",
            "        def get(self, item, default=None):",
            "            value = os.environ.get(item, default)",
            "            if value is not None:",
            "                value = six.ensure_text(value)",
            "            return value",
            "",
            "        def pop(self, item, default=None):",
            "            value = os.environ.pop(item, default)",
            "            if value is not None:",
            "                value = six.ensure_text(value)",
            "            return value",
            "",
            "        def __setitem__(self, key, value):",
            "            key = six.ensure_text(key)",
            "            value = six.ensure_text(value)",
            "            os.environ[key] = value",
            "",
            "        @staticmethod",
            "        def _iterable_elements_to_unicode_generator(iterable):",
            "            # type: (Iterable) -> Generator[Union[Tuple, Any]]",
            "            \"\"\"Generator that gets values from original iterable and converts its 'str' values to 'unicode'\"\"\"",
            "            for element in iterable:",
            "                if type(element) is tuple:",
            "                    yield tuple(",
            "                        v.decode(\"utf-8\") if type(v) is six.binary_type else v",
            "                        for v in element",
            "                    )",
            "                else:",
            "                    yield six.ensure_text(element)",
            "",
            "        def iteritems(self):",
            "            return self._iterable_elements_to_unicode_generator(",
            "                six.iteritems(os.environ)",
            "            )",
            "",
            "        def items(self):",
            "            return list(",
            "                self._iterable_elements_to_unicode_generator(os.environ.items())",
            "            )",
            "",
            "        def iterkeys(self):",
            "            return self._iterable_elements_to_unicode_generator(",
            "                six.iterkeys(os.environ)",
            "            )",
            "",
            "        def keys(self):",
            "            return list(self._iterable_elements_to_unicode_generator(os.environ.keys()))",
            "",
            "        def itervalues(self):",
            "            return self._iterable_elements_to_unicode_generator(",
            "                six.itervalues(os.environ)",
            "            )",
            "",
            "        def values(self):",
            "            return list(",
            "                self._iterable_elements_to_unicode_generator(os.environ.values())",
            "            )",
            "",
            "        def copy(self):",
            "            return dict(self.items())",
            "",
            "        def __iter__(self):",
            "            return self.iterkeys()",
            "",
            "    def os_getenv_unicode(name, default=None):",
            "        \"\"\"The same logic as in os.environ, but with None check.\"\"\"",
            "        result = os.getenv(name, default)",
            "        if result is not None:",
            "            result = six.ensure_text(result)",
            "        return result",
            "",
            "    os_environ_unicode = EnvironUnicode()",
            "",
            "",
            "else:",
            "    os_environ_unicode = os.environ",
            "    os_getenv_unicode = os.getenv",
            "",
            "# 2->TODO struct.pack|unpack, does not accept unicode as format string.",
            "# see more: https://python-future.org/stdlib_incompatibilities.html#struct-pack",
            "# to avoid conversion of format string on every struct.pack call, we can monkey patch it here.",
            "if sys.version_info[:3] < (2, 7, 7):",
            "",
            "    def python_unicode_pack_unpack_wrapper(f):",
            "        def _pack_unpack(format_str, *args):",
            "            \"\"\"wrapper for struct.pack function that converts unicode format string to 'str'\"\"\"",
            "            binary_format_str = six.ensure_binary(format_str)",
            "            return f(binary_format_str, *args)",
            "",
            "        return _pack_unpack",
            "",
            "    struct_pack_unicode = python_unicode_pack_unpack_wrapper(struct.pack)",
            "    struct_unpack_unicode = python_unicode_pack_unpack_wrapper(struct.unpack)",
            "else:",
            "    struct_pack_unicode = struct.pack",
            "    struct_unpack_unicode = struct.unpack",
            "",
            "",
            "def which(executable):",
            "    # type: (str) -> Optional[str]",
            "    \"\"\"",
            "    Search for the provided executable in PATH and return path to it if found.",
            "    \"\"\"",
            "    paths = os.environ[\"PATH\"].split(os.pathsep)",
            "    for path in paths:",
            "        full_path = os.path.join(path, executable)",
            "",
            "        if os.path.exists(full_path) and os.access(full_path, os.X_OK):",
            "            return full_path",
            "",
            "    return None",
            "",
            "",
            "def find_executable(executable):",
            "    # type: (str) -> Optional[str]",
            "    \"\"\"",
            "    Wrapper around distutils.spawn.find_executable which is not available in some default Python 3",
            "    installations where full blown python3-distutils package is not installed.",
            "    \"\"\"",
            "    try:",
            "        from distutils.spawn import find_executable as distutils_find_executable",
            "    except ImportError:",
            "        # Likely Ubuntu 18.04 where python3-distutils package is not present (default behavior)",
            "        return which(executable)",
            "",
            "    return distutils_find_executable(executable)",
            "",
            "",
            "def subprocess_check_output(cmd, *args, **kwargs):",
            "    \"\"\"",
            "    Wrapper around subprocess.check_output which is not available under Python 2.6.",
            "    \"\"\"",
            "    if sys.version_info < (2, 7, 0):",
            "        output = subprocess.Popen(",
            "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, *args, **kwargs",
            "        ).communicate()[0]",
            "    else:",
            "        output = subprocess.check_output(cmd, *args, **kwargs)",
            "",
            "    return output"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "scalyr_agent/configuration.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 360,
                "afterPatchRowNumber": 360,
                "PatchRowcode": "         \"\"\""
            },
            "1": {
                "beforePatchRowNumber": 361,
                "afterPatchRowNumber": 361,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 362,
                "afterPatchRowNumber": 362,
                "PatchRowcode": "         options = ["
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 363,
                "PatchRowcode": "+            \"verify_server_certificate\","
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 364,
                "PatchRowcode": "+            \"ca_cert_path\","
            },
            "5": {
                "beforePatchRowNumber": 363,
                "afterPatchRowNumber": 365,
                "PatchRowcode": "             \"compression_type\","
            },
            "6": {
                "beforePatchRowNumber": 364,
                "afterPatchRowNumber": 366,
                "PatchRowcode": "             \"compression_level\","
            },
            "7": {
                "beforePatchRowNumber": 365,
                "afterPatchRowNumber": 367,
                "PatchRowcode": "             \"pipeline_threshold\","
            }
        },
        "frontPatchFile": [
            "# Copyright 2014 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ------------------------------------------------------------------------",
            "#",
            "#",
            "# author: Steven Czerwinski <czerwin@scalyr.com>",
            "",
            "from __future__ import unicode_literals",
            "from __future__ import absolute_import",
            "",
            "__author__ = \"czerwin@scalyr.com\"",
            "",
            "import os",
            "import re",
            "import socket",
            "import time",
            "import logging",
            "",
            "import six",
            "import six.moves.urllib.parse",
            "from six.moves import range",
            "",
            "import scalyr_agent.util as scalyr_util",
            "",
            "from scalyr_agent.json_lib import JsonConversionException, JsonMissingFieldException",
            "from scalyr_agent.json_lib.objects import (",
            "    JsonObject,",
            "    JsonArray,",
            "    ArrayOfStrings,",
            "    SpaceAndCommaSeparatedArrayOfStrings,",
            ")",
            "from scalyr_agent.monitor_utils.blocking_rate_limiter import BlockingRateLimiter",
            "from scalyr_agent.util import JsonReadFileException",
            "from scalyr_agent.config_util import BadConfiguration, get_config_from_env",
            "",
            "from scalyr_agent.__scalyr__ import get_install_root",
            "from scalyr_agent.compat import os_environ_unicode",
            "from scalyr_agent import compat",
            "",
            "",
            "class Configuration(object):",
            "    \"\"\"Encapsulates the results of a single read of the configuration file.",
            "",
            "    An instance of this object can be used to read and validate the configuration file.  It supports",
            "    reading the main contents of the configuration, as well as configuration fragments in the 'configs.d'",
            "    directory.  It handles merging the contents of the configuration files together and filling in any default",
            "    values for fields not set.  You can also use the equivalent method to determine if two instances of this",
            "    object have the same configuration content.",
            "",
            "    You may also use environment variable substitution in any string value in the configuration file.  You just",
            "    need to define the ``import_vars`` configuration field to be a list of variable names to import from the",
            "    shell and then use the $VAR_NAME in any string field.  Each entry in ``import_vars`` may also be a dict with",
            "    two entries: ``var`` for the name of the environment variable and ``default`` for the value to use if the",
            "    the environment variable is not set or is empty.",
            "",
            "    This also handles reporting status information about the configuration state, including what time it was",
            "    read and what error (if any) was raised.",
            "",
            "    Note:",
            "    UNDOCUMENTED_CONFIG: These are undocumented config params, meaning they are not described in the public online docs",
            "        in order to simplify the mental model.  In rare cases, a customer may need to tune these params under direct",
            "        guidance from support.",
            "    \"\"\"",
            "",
            "    DEFAULT_K8S_IGNORE_NAMESPACES = [\"kube-system\"]",
            "    DEFAULT_K8S_INCLUDE_NAMESPACES = [\"*\"]",
            "",
            "    def __init__(self, file_path, default_paths, logger, extra_config_dir=None):",
            "        # Captures all environment aware variables for testing purposes",
            "        self._environment_aware_map = {}",
            "        self.__file_path = os.path.abspath(file_path)",
            "        # Paths for additional configuration files that were read (from the config directory).",
            "        self.__additional_paths = []",
            "        # The JsonObject holding the contents of the configuration file, along with any default",
            "        # values filled in.",
            "        self.__config = None",
            "        # The number of seconds past epoch when the file was read.",
            "        self.__read_time = None",
            "        # The exception, if any, that was raised when the state was read.  This will be a BadConfiguration exception.",
            "        self.__last_error = None",
            "        # The log configuration objects from the configuration file.  This does not include logs required by",
            "        # the monitors.",
            "        self.__log_configs = []",
            "        # Optional configuration for journald monitor logging",
            "        self.__journald_log_configs = []",
            "        # Optional configuration for k8s monitor logging",
            "        self.__k8s_log_configs = []",
            "        # The monitor configuration objects from the configuration file.  This does not include monitors that",
            "        # are created by default by the platform.",
            "        self.__monitor_configs = []",
            "",
            "        # The DefaultPaths object that specifies the default paths for things like the data and log directory",
            "        # based on platform.",
            "        self.__default_paths = default_paths",
            "",
            "        # FIX THESE:",
            "        # Add documentation, verify, etc.",
            "        self.max_retry_time = 15 * 60",
            "        self.max_allowed_checkpoint_age = 15 * 60",
            "",
            "        # An additional directory to look for config snippets",
            "        self.__extra_config_directory = extra_config_dir",
            "",
            "        self.__logger = logger",
            "",
            "    def parse(self):",
            "        self.__read_time = time.time()",
            "",
            "        try:",
            "            try:",
            "                # First read the file.  This makes sure it exists and can be parsed.",
            "                self.__config = scalyr_util.read_config_file_as_json(self.__file_path)",
            "",
            "                # What implicit entries do we need to add?  metric monitor, agent.log, and then logs from all monitors.",
            "            except JsonReadFileException as e:",
            "                raise BadConfiguration(six.text_type(e), None, \"fileParseError\")",
            "",
            "            # Import any requested variables from the shell and use them for substitutions.",
            "            self.__perform_substitutions(self.__config)",
            "",
            "            # get initial list of already seen config keys (we need to do this before",
            "            # defaults have been applied)",
            "            already_seen = {}",
            "            for k in self.__config.keys():",
            "                already_seen[k] = self.__file_path",
            "",
            "            self.__verify_main_config_and_apply_defaults(",
            "                self.__config, self.__file_path",
            "            )",
            "            api_key, api_config_file = self.__check_field(",
            "                \"api_key\", self.__config, self.__file_path",
            "            )",
            "            scalyr_server, scalyr_server_config_file = self.__check_field(",
            "                \"scalyr_server\", self.__config, self.__file_path",
            "            )",
            "            self.__verify_logs_and_monitors_configs_and_apply_defaults(",
            "                self.__config, self.__file_path",
            "            )",
            "",
            "            # these variables are allowed to appear in multiple files",
            "            allowed_multiple_keys = (",
            "                \"import_vars\",",
            "                \"logs\",",
            "                \"journald_logs\",",
            "                \"k8s_logs\",",
            "                \"monitors\",",
            "                \"server_attributes\",",
            "            )",
            "",
            "            # Get any configuration snippets in the config directory",
            "            extra_config = self.__list_files(self.config_directory)",
            "",
            "            # Plus any configuration snippets in the additional config directory",
            "            extra_config.extend(self.__list_files(self.extra_config_directory))",
            "",
            "            # Now, look for any additional configuration in the config fragment directory.",
            "            for fp in extra_config:",
            "                self.__additional_paths.append(fp)",
            "                content = scalyr_util.read_config_file_as_json(fp)",
            "                for k in content.keys():",
            "                    if k not in allowed_multiple_keys:",
            "                        if k in already_seen:",
            "                            self.__last_error = BadConfiguration(",
            "                                'Configuration fragment file \"%s\" redefines the config key \"%s\", first seen in \"%s\". '",
            "                                \"The only config items that can be defined in multiple config files are: %s.\"",
            "                                % (fp, k, already_seen[k], allowed_multiple_keys),",
            "                                k,",
            "                                \"multipleKeys\",",
            "                            )",
            "                            raise self.__last_error",
            "                        else:",
            "                            already_seen[k] = fp",
            "",
            "                self.__perform_substitutions(content)",
            "                self.__verify_main_config(content, self.__file_path)",
            "                self.__verify_logs_and_monitors_configs_and_apply_defaults(content, fp)",
            "",
            "                for (key, value) in six.iteritems(content):",
            "                    if key not in allowed_multiple_keys:",
            "                        self.__config.put(key, value)",
            "",
            "                self.__add_elements_from_array(\"logs\", content, self.__config)",
            "                self.__add_elements_from_array(\"journald_logs\", content, self.__config)",
            "                self.__add_elements_from_array(\"k8s_logs\", content, self.__config)",
            "                self.__add_elements_from_array(\"monitors\", content, self.__config)",
            "                self.__merge_server_attributes(fp, content, self.__config)",
            "",
            "            self.__set_api_key(self.__config, api_key)",
            "            if scalyr_server is not None:",
            "                self.__config.put(\"scalyr_server\", scalyr_server)",
            "            self.__verify_or_set_optional_string(",
            "                self.__config,",
            "                \"scalyr_server\",",
            "                \"https://agent.scalyr.com\",",
            "                \"configuration file %s\" % self.__file_path,",
            "                env_name=\"SCALYR_SERVER\",",
            "            )",
            "",
            "            self.__config[\"raw_scalyr_server\"] = self.__config[\"scalyr_server\"]",
            "",
            "            # force https unless otherwise instructed not to",
            "            if not self.__config[\"allow_http\"]:",
            "                server = self.__config[\"scalyr_server\"].strip()",
            "                https_server = server",
            "",
            "                parts = six.moves.urllib.parse.urlparse(server)",
            "",
            "                # use index-based addressing for 2.4 compatibility",
            "                scheme = parts[0]",
            "",
            "                if not scheme:",
            "                    https_server = \"https://\" + server",
            "                elif scheme == \"http\":",
            "                    https_server = re.sub(\"^http://\", \"https://\", server)",
            "",
            "                if https_server != server:",
            "                    self.__config[\"scalyr_server\"] = https_server",
            "",
            "            # Set defaults based on `max_send_rate_enforcement` value",
            "            if (",
            "                not self.__config[\"disable_max_send_rate_enforcement_overrides\"]",
            "                and not self.__config[\"max_send_rate_enforcement\"] == \"legacy\"",
            "            ):",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"max_allowed_request_size\", 1024 * 1024",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"pipeline_threshold\", 1.1",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"min_request_spacing_interval\", 1.0",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"max_request_spacing_interval\", 5.0",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"max_log_offset_size\", 5 * 1024 * 1024",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"max_existing_log_offset_size\", 100 * 1024 * 1024",
            "                )",
            "",
            "                self.__config[\"max_allowed_request_size\"] = 5900000",
            "                self.__config[\"pipeline_threshold\"] = 0",
            "                self.__config[\"min_request_spacing_interval\"] = 0.0",
            "                self.__config[\"max_request_spacing_interval\"] = 5.0",
            "                self.__config[\"max_log_offset_size\"] = 200000000",
            "                self.__config[\"max_existing_log_offset_size\"] = 200000000",
            "",
            "            # Parse `max_send_rate_enforcement`",
            "            if (",
            "                self.__config[\"max_send_rate_enforcement\"] != \"unlimited\"",
            "                and self.__config[\"max_send_rate_enforcement\"] != \"legacy\"",
            "            ):",
            "                try:",
            "                    self.__config[",
            "                        \"parsed_max_send_rate_enforcement\"",
            "                    ] = scalyr_util.parse_data_rate_string(",
            "                        self.__config[\"max_send_rate_enforcement\"]",
            "                    )",
            "                except ValueError as e:",
            "                    raise BadConfiguration(",
            "                        six.text_type(e), \"max_send_rate_enforcement\", \"notDataRate\"",
            "                    )",
            "",
            "            # Add in 'serverHost' to server_attributes if it is not set.  We must do this after merging any",
            "            # server attributes from the config fragments.",
            "            if \"serverHost\" not in self.server_attributes:",
            "                self.__config[\"server_attributes\"][",
            "                    \"serverHost\"",
            "                ] = self.__get_default_hostname()",
            "",
            "            # Add in implicit entry to collect the log generated by this agent.",
            "            agent_log = None",
            "            if self.implicit_agent_log_collection:",
            "                config = JsonObject(path=\"agent.log\", parser=\"scalyrAgentLog\")",
            "                self.__verify_log_entry_and_set_defaults(",
            "                    config, description=\"implicit rule\"",
            "                )",
            "                agent_log = config",
            "",
            "            self.__log_configs = list(self.__config.get_json_array(\"logs\"))",
            "            if agent_log is not None:",
            "                self.__log_configs.append(agent_log)",
            "",
            "            self.__journald_log_configs = list(",
            "                self.__config.get_json_array(\"journald_logs\")",
            "            )",
            "",
            "            self.__k8s_log_configs = list(self.__config.get_json_array(\"k8s_logs\"))",
            "",
            "            # add in the profile log if we have enabled profiling",
            "            if self.enable_profiling:",
            "                profile_config = JsonObject(",
            "                    path=self.profile_log_name,",
            "                    copy_from_start=True,",
            "                    staleness_threshold_secs=20 * 60,",
            "                    parser=\"scalyrAgentProfiling\",",
            "                )",
            "                self.__verify_log_entry_and_set_defaults(",
            "                    profile_config, description=\"CPU profile log config\"",
            "                )",
            "                self.__log_configs.append(profile_config)",
            "",
            "            self.__monitor_configs = list(self.__config.get_json_array(\"monitors\"))",
            "        except BadConfiguration as e:",
            "            self.__last_error = e",
            "            raise e",
            "",
            "    def _warn_of_override_due_to_rate_enforcement(self, config_option, default):",
            "        if self.__config[config_option] != default:",
            "            self.__logger.warn(",
            "                \"Configured option %s is being overridden due to max_send_rate_enforcement setting.\"",
            "                % config_option,",
            "                limit_once_per_x_secs=86400,",
            "                limit_key=\"max_send_rate_enforcement_override\",",
            "            )",
            "",
            "    def apply_config(self):",
            "        \"\"\"",
            "        Apply global configuration object based on the configuration values.",
            "",
            "        At this point this only applies to the JSON library which is used.",
            "        \"\"\"",
            "        if not self.__config:",
            "            # parse() hasn't been called yet. We should probably throw here",
            "            return",
            "",
            "        # Set json library based on the config value. If \"auto\" is provided this means we use",
            "        # default behavior which is try to use ujson and if that's not available fall back to",
            "        # stdlib json",
            "        json_library = self.json_library",
            "        current_json_library = scalyr_util.get_json_lib()",
            "",
            "        if json_library != \"auto\" and json_library != current_json_library:",
            "            self.__logger.debug(",
            "                'Changing JSON library from \"%s\" to \"%s\"'",
            "                % (current_json_library, json_library)",
            "            )",
            "            scalyr_util.set_json_lib(json_library)",
            "",
            "    def print_useful_settings(self, other_config=None):",
            "        \"\"\"",
            "        Prints various useful configuration settings to the agent log, so we have a record",
            "        in the log of the settings that are currently in use.",
            "",
            "        @param other_config: Another configuration option.  If not None, this function will",
            "        only print configuration options that are different between the two objects.",
            "        \"\"\"",
            "",
            "        options = [",
            "            \"compression_type\",",
            "            \"compression_level\",",
            "            \"pipeline_threshold\",",
            "            \"max_send_rate_enforcement\",",
            "            \"disable_max_send_rate_enforcement_overrides\",",
            "            \"min_allowed_request_size\",",
            "            \"max_allowed_request_size\",",
            "            \"min_request_spacing_interval\",",
            "            \"max_request_spacing_interval\",",
            "            \"read_page_size\",",
            "            \"max_line_size\",",
            "            \"internal_parse_max_line_size\",",
            "            \"line_completion_wait_time\",",
            "            \"max_log_offset_size\",",
            "            \"max_existing_log_offset_size\",",
            "        ]",
            "",
            "        # get options (if any) from the other configuration object",
            "        other_options = None",
            "        if other_config is not None:",
            "            other_options = {}",
            "            for option in options:",
            "                other_options[option] = getattr(other_config, option, None)",
            "",
            "        first = True",
            "        for option in options:",
            "            value = getattr(self, option, None)",
            "            print_value = False",
            "",
            "            # check to see if we should be printing this option which will will",
            "            # be True if other_config is None or if the other_config had a setting",
            "            # that was different from our current setting",
            "            if other_config is None:",
            "                print_value = True",
            "            elif (",
            "                other_options is not None",
            "                and option in other_options",
            "                and other_options[option] != value",
            "            ):",
            "                print_value = True",
            "",
            "            if print_value:",
            "                # if this is the first option we are printing, output a header",
            "                if first:",
            "                    self.__logger.info(\"Configuration settings\")",
            "                    first = False",
            "",
            "                self.__logger.info(\"\\t%s: %s\" % (option, value))",
            "",
            "    def __get_default_hostname(self):",
            "        \"\"\"Returns the default hostname for this host.",
            "        @return: The default hostname for this host.",
            "        @rtype: str",
            "        \"\"\"",
            "        result = six.ensure_text(socket.gethostname())",
            "        if result is not None and self.strip_domain_from_default_server_host:",
            "            result = result.split(\".\")[0]",
            "        return result",
            "",
            "    def parse_log_config(",
            "        self,",
            "        log_config,",
            "        default_parser=None,",
            "        context_description=\"uncategorized log entry\",",
            "    ):",
            "        \"\"\"Parses a given configuration stanza for a log file and returns the complete config with all the",
            "        default values filled in.",
            "",
            "        This is useful for parsing a log configuration entry that was not originally included in the configuration",
            "        but whose contents still must be verified.  For example, a log configuration entry from a monitor.",
            "",
            "        @param log_config: The configuration entry for the log.",
            "        @param default_parser: If the configuration entry does not have a ``parser`` entry, set this as the default.",
            "        @param context_description: The context of where this entry came from, used when creating an error message",
            "            for the user.",
            "",
            "        @type log_config: dict|JsonObject",
            "        @type default_parser: str",
            "        @type context_description: str",
            "",
            "        @return: The full configuration for the log.",
            "        @rtype: JsonObject",
            "        \"\"\"",
            "        if type(log_config) is dict:",
            "            log_config = JsonObject(content=log_config)",
            "",
            "        log_config = log_config.copy()",
            "",
            "        if default_parser is not None:",
            "            self.__verify_or_set_optional_string(",
            "                log_config, \"parser\", default_parser, context_description",
            "            )",
            "        self.__verify_log_entry_and_set_defaults(",
            "            log_config, description=context_description",
            "        )",
            "",
            "        return log_config",
            "",
            "    def parse_monitor_config(",
            "        self, monitor_config, context_description=\"uncategorized monitor entry\"",
            "    ):",
            "        \"\"\"Parses a given monitor configuration entry and returns the complete config with all default values",
            "        filled in.",
            "",
            "        This is useful for parsing a monitor configuration entry that was not originally included in the",
            "        configuration but whose contents still must be verified.  For example, a default monitor supplied by the",
            "        platform.",
            "",
            "        @param monitor_config: The configuration entry for the monitor.",
            "        @param context_description: The context of where this entry came from, used when creating an error message",
            "            for the user.",
            "",
            "        @type monitor_config: dict|JsonObject",
            "        @type context_description: str",
            "",
            "        @return: The full configuration for the monitor.",
            "        @rtype: JsonObject",
            "        \"\"\"",
            "        if type(monitor_config) is dict:",
            "            monitor_config = JsonObject(content=monitor_config)",
            "",
            "        monitor_config = monitor_config.copy()",
            "",
            "        self.__verify_monitor_entry_and_set_defaults(",
            "            monitor_config, context_description=context_description",
            "        )",
            "",
            "        return monitor_config",
            "",
            "    # k8s cache options",
            "    @property",
            "    def k8s_ignore_namespaces(self):",
            "        return self.__get_config().get_json_array(\"k8s_ignore_namespaces\")",
            "",
            "    @property",
            "    def k8s_include_namespaces(self):",
            "        return self.__get_config().get_json_array(\"k8s_include_namespaces\")",
            "",
            "    @property",
            "    def k8s_api_url(self):",
            "        return self.__get_config().get_string(\"k8s_api_url\")",
            "",
            "    @property",
            "    def k8s_verify_api_queries(self):",
            "        return self.__get_config().get_bool(\"k8s_verify_api_queries\")",
            "",
            "    @property",
            "    def k8s_verify_kubelet_queries(self):",
            "        return self.__get_config().get_bool(\"k8s_verify_kubelet_queries\")",
            "",
            "    @property",
            "    def k8s_kubelet_ca_cert(self):",
            "        return self.__get_config().get_string(\"k8s_kubelet_ca_cert\")",
            "",
            "    @property",
            "    def k8s_cache_query_timeout_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_query_timeout_secs\")",
            "",
            "    @property",
            "    def k8s_cache_expiry_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_expiry_secs\")",
            "",
            "    @property",
            "    def k8s_cache_expiry_fuzz_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_expiry_fuzz_secs\")",
            "",
            "    @property",
            "    def k8s_cache_start_fuzz_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_start_fuzz_secs\")",
            "",
            "    @property",
            "    def k8s_cache_purge_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_purge_secs\")",
            "",
            "    @property",
            "    def k8s_service_account_cert(self):",
            "        return self.__get_config().get_string(\"k8s_service_account_cert\")",
            "",
            "    @property",
            "    def k8s_service_account_token(self):",
            "        return self.__get_config().get_string(\"k8s_service_account_token\")",
            "",
            "    @property",
            "    def k8s_service_account_namespace(self):",
            "        return self.__get_config().get_string(\"k8s_service_account_namespace\")",
            "",
            "    @property",
            "    def k8s_log_api_responses(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_bool(\"k8s_log_api_responses\")",
            "",
            "    @property",
            "    def k8s_log_api_exclude_200s(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_bool(\"k8s_log_api_exclude_200s\")",
            "",
            "    @property",
            "    def k8s_log_api_min_response_len(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_log_api_min_response_len\")",
            "",
            "    @property",
            "    def k8s_log_api_min_latency(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_log_api_min_latency\")",
            "",
            "    @property",
            "    def k8s_log_api_ratelimit_interval(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_log_api_ratelimit_interval\")",
            "",
            "    @property",
            "    def k8s_controlled_warmer_max_attempts(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_controlled_warmer_max_attempts\")",
            "",
            "    @property",
            "    def k8s_controlled_warmer_max_query_retries(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_controlled_warmer_max_query_retries\")",
            "",
            "    @property",
            "    def k8s_controlled_warmer_blacklist_time(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_controlled_warmer_blacklist_time\")",
            "",
            "    @property",
            "    def k8s_events_disable(self):",
            "        return self.__get_config().get_bool(\"k8s_events_disable\")",
            "",
            "    @property",
            "    def k8s_ratelimit_cluster_num_agents(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_ratelimit_cluster_num_agents\")",
            "",
            "    @property",
            "    def k8s_ratelimit_cluster_rps_init(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_cluster_rps_init\")",
            "",
            "    @property",
            "    def k8s_ratelimit_cluster_rps_min(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_cluster_rps_min\")",
            "",
            "    @property",
            "    def k8s_ratelimit_cluster_rps_max(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_cluster_rps_max\")",
            "",
            "    @property",
            "    def k8s_ratelimit_consecutive_increase_threshold(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(",
            "            \"k8s_ratelimit_consecutive_increase_threshold\"",
            "        )",
            "",
            "    @property",
            "    def k8s_ratelimit_strategy(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_string(\"k8s_ratelimit_strategy\")",
            "",
            "    @property",
            "    def k8s_ratelimit_increase_factor(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_increase_factor\")",
            "",
            "    @property",
            "    def k8s_ratelimit_backoff_factor(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_backoff_factor\")",
            "",
            "    @property",
            "    def k8s_ratelimit_max_concurrency(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_ratelimit_max_concurrency\")",
            "",
            "    @property",
            "    def enforce_monotonic_timestamps(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_bool(\"enforce_monotonic_timestamps\")",
            "",
            "    @property",
            "    def include_raw_timestamp_field(self):",
            "        \"\"\"If True, adds an attribute called `raw_timestamp` to all events parsed using the",
            "        parse_as_json or parse_as_cri features.  This field will be set to the timestamp included in the JSON or",
            "        CRI line.  When parsing Docker or K8s logs, this represents the timestamp of the log",
            "        message as recorded by those systems.\"\"\"",
            "        return self.__get_config().get_bool(\"include_raw_timestamp_field\")",
            "",
            "    @property",
            "    def enable_profiling(self):",
            "        return self.__get_config().get_bool(\"enable_profiling\")",
            "",
            "    @property",
            "    def max_profile_interval_minutes(self):",
            "        return self.__get_config().get_int(\"max_profile_interval_minutes\")",
            "",
            "    @property",
            "    def profile_duration_minutes(self):",
            "        return self.__get_config().get_int(\"profile_duration_minutes\")",
            "",
            "    @property",
            "    def profile_clock(self):",
            "        return self.__get_config().get_string(\"profile_clock\")",
            "",
            "    @property",
            "    def profile_log_name(self):",
            "        return self.__get_config().get_string(\"profile_log_name\")",
            "",
            "    @property",
            "    def memory_profile_log_name(self):",
            "        return self.__get_config().get_string(\"memory_profile_log_name\")",
            "",
            "    @property",
            "    def disable_logfile_addevents_format(self):",
            "        return self.__get_config().get_bool(\"disable_logfile_addevents_format\")",
            "",
            "    @property",
            "    def json_library(self):",
            "        return self.__get_config().get_string(\"json_library\")",
            "",
            "    # Debug leak flags",
            "    @property",
            "    def disable_send_requests(self):",
            "        return self.__get_config().get_bool(\"disable_send_requests\")",
            "",
            "    # Debug leak flags",
            "    @property",
            "    def disable_monitor_threads(self):",
            "        return self.__get_config().get_bool(\"disable_leak_monitor_threads\")",
            "",
            "    @property",
            "    def disable_monitors_creation(self):",
            "        return self.__get_config().get_bool(\"disable_leak_monitors_creation\")",
            "",
            "    @property",
            "    def disable_new_file_matches(self):",
            "        return self.__get_config().get_bool(\"disable_leak_new_file_matches\")",
            "",
            "    @property",
            "    def disable_scan_for_new_bytes(self):",
            "        return self.__get_config().get_bool(\"disable_leak_scan_for_new_bytes\")",
            "",
            "    @property",
            "    def disable_processing_new_bytes(self):",
            "        return self.__get_config().get_bool(\"disable_leak_processing_new_bytes\")",
            "",
            "    @property",
            "    def disable_copying_thread(self):",
            "        return self.__get_config().get_bool(\"disable_leak_copying_thread\")",
            "",
            "    @property",
            "    def disable_overall_stats(self):",
            "        return self.__get_config().get_bool(\"disable_leak_overall_stats\")",
            "",
            "    @property",
            "    def disable_bandwidth_stats(self):",
            "        return self.__get_config().get_bool(\"disable_leak_bandwidth_stats\")",
            "",
            "    @property",
            "    def disable_copy_manager_stats(self):",
            "        return self.__get_config().get_bool(\"disable_copy_manager_stats\")",
            "",
            "    @property",
            "    def disable_update_debug_log_level(self):",
            "        return self.__get_config().get_bool(\"disable_leak_update_debug_log_level\")",
            "",
            "    @property",
            "    def enable_gc_stats(self):",
            "        return self.__get_config().get_bool(\"enable_gc_stats\")",
            "",
            "    @property",
            "    def disable_all_config_updates(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_all_config_updates\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_verify_config(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_verify_config\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_config_equivalence_check(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_config_equivalence_check\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_verify_can_write_to_logs(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_verify_can_write_to_logs\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_config_reload(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_config_reload\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def config_change_check_interval(self):",
            "        return self.__get_config().get_float(\"config_change_check_interval\")",
            "",
            "    @property",
            "    def overall_stats_log_interval(self):",
            "        return self.__get_config().get_float(\"overall_stats_log_interval\")",
            "",
            "    @property",
            "    def copying_manager_stats_log_interval(self):",
            "        return self.__get_config().get_float(\"copying_manager_stats_log_interval\")",
            "",
            "    @property",
            "    def bandwidth_stats_log_interval(self):",
            "        return self.__get_config().get_float(\"bandwidth_stats_log_interval\")",
            "",
            "    @property",
            "    def user_agent_refresh_interval(self):",
            "        return self.__get_config().get_float(\"user_agent_refresh_interval\")",
            "",
            "    @property",
            "    def garbage_collect_interval(self):",
            "        return self.__get_config().get_int(\"garbage_collect_interval\")",
            "",
            "    @property",
            "    def disable_verify_config_create_monitors_manager(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_verify_config_create_monitors_manager\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_verify_config_create_copying_manager(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_verify_config_create_copying_manager\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_verify_config_cache_config(self):",
            "        return self.__get_config().get_bool(\"disable_leak_verify_config_cache_config\")",
            "",
            "    # end Debug leak flags",
            "",
            "    @property",
            "    def read_time(self):",
            "        \"\"\"Returns the time this configuration file was read.\"\"\"",
            "        return self.__read_time",
            "",
            "    @property",
            "    def file_path(self):",
            "        \"\"\"Returns the time this path of the file that was read for the configuration.\"\"\"",
            "        return self.__file_path",
            "",
            "    @property",
            "    def additional_file_paths(self):",
            "        \"\"\"Returns a list of the paths for the additional files from the configuration directory that were read.\"\"\"",
            "        return self.__additional_paths",
            "",
            "    @property",
            "    def last_error(self):",
            "        \"\"\"Returns the error seen (if any) while processing the configuration.\"\"\"",
            "        return self.__last_error",
            "",
            "    @property",
            "    def log_configs(self):",
            "        \"\"\"Returns the list of configuration entries for all the logs specified in the configuration file.",
            "",
            "        Note, this does not include logs required by monitors.  It is only logs explicitly listed in the configuration",
            "        file and possible the agent log as well.",
            "",
            "        @rtype list<JsonObject>\"\"\"",
            "        return self.__log_configs",
            "",
            "    @property",
            "    def journald_log_configs(self):",
            "        \"\"\"Returns the list of configuration entries for all the journald loggers specified in the configuration file.",
            "",
            "        @rtype list<JsonObject>\"\"\"",
            "        return self.__journald_log_configs",
            "",
            "    @property",
            "    def k8s_log_configs(self):",
            "        \"\"\"Returns the list of configuration entries for all the k8s loggers specified in the configuration file.",
            "",
            "        @rtype list<JsonObject>\"\"\"",
            "        return self.__k8s_log_configs",
            "",
            "    @property",
            "    def monitor_configs(self):",
            "        \"\"\"Returns the list of configuration entries for all monitores specified in the configuration file.",
            "",
            "        Note, this does not include default monitors for the platform.  It is only monitors explicitly listed in the",
            "        configuration file.",
            "",
            "        @rtype list<JsonObject>\"\"\"",
            "        return self.__monitor_configs",
            "",
            "    @property",
            "    def agent_data_path(self):",
            "        \"\"\"Returns the configuration value for 'agent_data_path'.\"\"\"",
            "        return self.__get_config().get_string(\"agent_data_path\")",
            "",
            "    @property",
            "    def agent_log_path(self):",
            "        \"\"\"Returns the configuration value for 'agent_log_path'.\"\"\"",
            "        return self.__get_config().get_string(\"agent_log_path\")",
            "",
            "    @property",
            "    def additional_monitor_module_paths(self):",
            "        \"\"\"Returns the configuration value for 'additional_monitor_module_paths'.\"\"\"",
            "        return self.__get_config().get_string(\"additional_monitor_module_paths\")",
            "",
            "    @property",
            "    def api_key(self):",
            "        \"\"\"Returns the configuration value for 'api_key'.\"\"\"",
            "        return self.__get_config().get_string(\"api_key\")",
            "",
            "    @property",
            "    def scalyr_server(self):",
            "        \"\"\"Returns the configuration value for 'scalyr_server'.\"\"\"",
            "        return self.__get_config().get_string(\"scalyr_server\")",
            "",
            "    @property",
            "    def raw_scalyr_server(self):",
            "        \"\"\"Returns the configuration value for 'raw_scalyr_server'.\"\"\"",
            "        return self.__get_config().get_string(\"raw_scalyr_server\")",
            "",
            "    @property",
            "    def check_remote_if_no_tty(self):",
            "        \"\"\"Returns the configuration value for `check_remote_if_no_tty`\"\"\"",
            "        return self.__get_config().get_bool(\"check_remote_if_no_tty\")",
            "",
            "    @property",
            "    def server_attributes(self):",
            "        \"\"\"Returns the configuration value for 'server_attributes'.\"\"\"",
            "        return self.__get_config().get_json_object(\"server_attributes\")",
            "",
            "    @property",
            "    def implicit_agent_log_collection(self):",
            "        \"\"\"Returns the configuration value for 'implicit_agent_log_collection'.\"\"\"",
            "        return self.__get_config().get_bool(\"implicit_agent_log_collection\")",
            "",
            "    @property",
            "    def implicit_metric_monitor(self):",
            "        \"\"\"Returns the configuration value for 'implicit_metric_monitor'.\"\"\"",
            "        return self.__get_config().get_bool(\"implicit_metric_monitor\")",
            "",
            "    @property",
            "    def implicit_agent_process_metrics_monitor(self):",
            "        \"\"\"Returns the configuration value for 'implicit_agent_process_metrics_monitor'.\"\"\"",
            "        return self.__get_config().get_bool(\"implicit_agent_process_metrics_monitor\")",
            "",
            "    @property",
            "    def use_unsafe_debugging(self):",
            "        \"\"\"Returns the configuration value for 'unsafe_debugging'.",
            "",
            "        Note, this should be used with extreme care.  It allows arbitrary commands to be executed by any local",
            "        user on the system as the user running the agent.\"\"\"",
            "        return self.__get_config().get_bool(\"use_unsafe_debugging\")",
            "",
            "    @property",
            "    def copying_thread_profile_interval(self):",
            "        \"\"\"Returns the interval (in seconds) between outputs of the profiling for the copying thread.",
            "        This should be zero unless you are profiling the copying thread.",
            "        \"\"\"",
            "        return self.__get_config().get_int(\"copying_thread_profile_interval\")",
            "",
            "    @property",
            "    def copying_thread_profile_output_path(self):",
            "        \"\"\"Returns the path prefix for writing all profiling dumps for the copying thread, when",
            "        ``copying_thread_profile_interval`` is greater than zero.",
            "        @return:",
            "        @rtype:",
            "        \"\"\"",
            "        return self.__get_config().get_string(\"copying_thread_profile_output_path\")",
            "",
            "    @property",
            "    def config_directory(self):",
            "        \"\"\"Returns the configuration value for 'config_directory', resolved to full path if necessary.\"\"\"",
            "        config_directory = self.__get_config().get_string(\"config_directory\")",
            "",
            "        # The configuration directory's path is relative to the the directory this configuration",
            "        # file is stored in.",
            "        return self.__resolve_absolute_path(",
            "            config_directory, self.__get_parent_directory(self.__file_path)",
            "        )",
            "",
            "    @property",
            "    def config_directory_raw(self):",
            "        \"\"\"Returns the configuration value for 'config_directory', as recorded in the configuration file.\"\"\"",
            "        return self.__get_config().get_string(\"config_directory\")",
            "",
            "    @property",
            "    def parsed_max_send_rate_enforcement(self):",
            "        \"\"\"Returns the configuration value for 'max_send_rate_enforcement' in bytes per second if not `unlimited` or `legacy`.\"\"\"",
            "        return self.__get_config().get_float(",
            "            \"parsed_max_send_rate_enforcement\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def max_send_rate_enforcement(self):",
            "        \"\"\"Returns the raw value for 'max_send_rate_enforcement'.\"\"\"",
            "        return self.__get_config().get_string(\"max_send_rate_enforcement\")",
            "",
            "    @property",
            "    def disable_max_send_rate_enforcement_overrides(self):",
            "        \"\"\"Returns the configuration value for 'disable_max_send_rate_enforcement_overrides'.\"\"\"",
            "        return self.__get_config().get_bool(",
            "            \"disable_max_send_rate_enforcement_overrides\"",
            "        )",
            "",
            "    @property",
            "    def extra_config_directory(self):",
            "        \"\"\"Returns the configuration value for `extra_config_directory`, resolved to full path if",
            "        necessary.  \"\"\"",
            "",
            "        # If `extra_config_directory` is a relative path, then it will be relative",
            "        # to the directory containing the main config file",
            "        if self.__extra_config_directory is None:",
            "            return None",
            "",
            "        return self.__resolve_absolute_path(",
            "            self.__extra_config_directory,",
            "            self.__get_parent_directory(self.__file_path),",
            "        )",
            "",
            "    @property",
            "    def extra_config_directory_raw(self):",
            "        \"\"\"Returns the configuration value for 'extra_config_directory'.\"\"\"",
            "        return self.__extra_config_directory",
            "",
            "    @property",
            "    def max_allowed_request_size(self):",
            "        \"\"\"Returns the configuration value for 'max_allowed_request_size'.\"\"\"",
            "        return self.__get_config().get_int(\"max_allowed_request_size\")",
            "",
            "    @property",
            "    def min_allowed_request_size(self):",
            "        \"\"\"Returns the configuration value for 'min_allowed_request_size'.\"\"\"",
            "        return self.__get_config().get_int(\"min_allowed_request_size\")",
            "",
            "    @property",
            "    def min_request_spacing_interval(self):",
            "        \"\"\"Returns the configuration value for 'min_request_spacing_interval'.\"\"\"",
            "        return self.__get_config().get_float(\"min_request_spacing_interval\")",
            "",
            "    @property",
            "    def max_request_spacing_interval(self):",
            "        \"\"\"Returns the configuration value for 'max_request_spacing_interval'.\"\"\"",
            "        return self.__get_config().get_float(\"max_request_spacing_interval\")",
            "",
            "    @property",
            "    def max_error_request_spacing_interval(self):",
            "        \"\"\"Returns the configuration value for 'max_error_request_spacing_interval'.\"\"\"",
            "        return self.__get_config().get_float(\"max_error_request_spacing_interval\")",
            "",
            "    @property",
            "    def low_water_bytes_sent(self):",
            "        \"\"\"Returns the configuration value for 'low_water_bytes_sent'.\"\"\"",
            "        return self.__get_config().get_int(\"low_water_bytes_sent\")",
            "",
            "    @property",
            "    def low_water_request_spacing_adjustment(self):",
            "        \"\"\"Returns the configuration value for 'low_water_request_spacing_adjustment'.\"\"\"",
            "        return self.__get_config().get_float(\"low_water_request_spacing_adjustment\")",
            "",
            "    @property",
            "    def high_water_bytes_sent(self):",
            "        \"\"\"Returns the configuration value for 'high_water_bytes_sent'.\"\"\"",
            "        return self.__get_config().get_int(\"high_water_bytes_sent\")",
            "",
            "    @property",
            "    def high_water_request_spacing_adjustment(self):",
            "        \"\"\"Returns the configuration value for 'high_water_request_spacing_adjustment'.\"\"\"",
            "        return self.__get_config().get_float(\"high_water_request_spacing_adjustment\")",
            "",
            "    @property",
            "    def max_new_log_detection_time(self):",
            "        \"\"\"Returns the configuration value for 'max_new_log_detection_time'.\"\"\"",
            "        return self.__get_config().get_float(\"max_new_log_detection_time\")",
            "",
            "    @property",
            "    def failure_request_spacing_adjustment(self):",
            "        \"\"\"Returns the configuration value for 'failure_request_spacing_adjustment'.\"\"\"",
            "        return self.__get_config().get_float(\"failure_request_spacing_adjustment\")",
            "",
            "    @property",
            "    def request_too_large_adjustment(self):",
            "        \"\"\"Returns the configuration value for 'request_too_large_adjustment'.\"\"\"",
            "        return self.__get_config().get_float(\"request_too_large_adjustment\")",
            "",
            "    @property",
            "    def request_deadline(self):",
            "        \"\"\"Returns the configuration value for 'request_deadline'.\"\"\"",
            "        return self.__get_config().get_float(\"request_deadline\")",
            "",
            "    @property",
            "    def debug_level(self):",
            "        \"\"\"Returns the configuration value for 'debug_level'.\"\"\"",
            "        return self.__get_config().get_int(\"debug_level\")",
            "",
            "    @property",
            "    def stdout_severity(self):",
            "        \"\"\"Returns the configuration value for 'stdout_severity'.",
            "        Only used when running in no-fork mode.",
            "        \"\"\"",
            "        return self.__get_config().get_string(\"stdout_severity\").upper()",
            "",
            "    @property",
            "    def ca_cert_path(self):",
            "        \"\"\"Returns the configuration value for 'ca_cert_path'.\"\"\"",
            "        return self.__get_config().get_string(\"ca_cert_path\")",
            "",
            "    @property",
            "    def compression_type(self):",
            "        \"\"\"Returns the configuration value for 'compression_type'.\"\"\"",
            "        return self.__get_config().get_string(\"compression_type\", none_if_missing=True)",
            "",
            "    @property",
            "    def compression_level(self):",
            "        \"\"\"Returns the configuration value for 'compression_level'.\"\"\"",
            "        return self.__get_config().get_int(\"compression_level\", default_value=9)",
            "",
            "    @property",
            "    def use_requests_lib(self):",
            "        \"\"\"Returns the configuration value for 'use_requests_lib'.\"\"\"",
            "        return self.__get_config().get_bool(\"use_requests_lib\")",
            "",
            "    @property",
            "    def use_tlslite(self):",
            "        \"\"\"Returns the configuration value for 'use_tlslite'.\"\"\"",
            "        return self.__get_config().get_bool(\"use_tlslite\")",
            "",
            "    @property",
            "    def network_proxies(self):",
            "        \"\"\"Returns the proxy map created by the 'https_proxy' and 'http_proxy' configuration variables, or",
            "        None if neither of those is set.",
            "        \"\"\"",
            "        https_proxy = self.__get_config().get_string(",
            "            \"https_proxy\", none_if_missing=True",
            "        )",
            "        http_proxy = self.__get_config().get_string(\"http_proxy\", none_if_missing=True)",
            "        if https_proxy is None and http_proxy is None:",
            "            return None",
            "        result = {}",
            "        if https_proxy is not None:",
            "            result[\"https\"] = https_proxy",
            "        if http_proxy is not None:",
            "            result[\"http\"] = http_proxy",
            "        return result",
            "",
            "    @property",
            "    def global_monitor_sample_interval(self):",
            "        \"\"\"Returns the configuration value for 'global_monitor_sample_interval'.\"\"\"",
            "        return self.__get_config().get_float(\"global_monitor_sample_interval\")",
            "",
            "    @property",
            "    def full_checkpoint_interval(self):",
            "        \"\"\"Returns the configuration value for 'full_checkpoint_interval_in_seconds'.\"\"\"",
            "        return self.__get_config().get_int(\"full_checkpoint_interval_in_seconds\")",
            "",
            "    @property",
            "    def minimum_scan_interval(self):",
            "        \"\"\"Returns the configuration value for 'minimum_scan_interval'.\"\"\"",
            "        return self.__get_config().get_int(",
            "            \"minimum_scan_interval\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def close_old_files_duration_in_seconds(self):",
            "        \"\"\"Returns the configuration value for 'close_old_files_duration_in_seconds'.\"\"\"",
            "        return self.__get_config().get_int(\"close_old_files_duration_in_seconds\")",
            "",
            "    @property",
            "    def max_line_size(self):",
            "        \"\"\"Returns the configuration value for 'max_line_size'.\"\"\"",
            "        return self.__get_config().get_int(\"max_line_size\")",
            "",
            "    @property",
            "    def line_completion_wait_time(self):",
            "        \"\"\"Returns the configuration value for 'line_completion_wait_time'.\"\"\"",
            "        return self.__get_config().get_float(\"line_completion_wait_time\")",
            "",
            "    @property",
            "    def internal_parse_max_line_size(self):",
            "        \"\"\"Returns the configuration value for 'internal_parse_max_line_size'.\"\"\"",
            "        return self.__get_config().get_int(\"internal_parse_max_line_size\")",
            "",
            "    @property",
            "    def max_log_offset_size(self):",
            "        \"\"\"Returns the configuration value for 'max_log_offset_size'.\"\"\"",
            "        return self.__get_config().get_int(\"max_log_offset_size\")",
            "",
            "    @property",
            "    def max_existing_log_offset_size(self):",
            "        \"\"\"Returns the configuration value for 'max_existing_log_offset_size'.\"\"\"",
            "        return self.__get_config().get_int(\"max_existing_log_offset_size\")",
            "",
            "    @property",
            "    def max_sequence_number(self):",
            "        \"\"\"Returns the maximum sequence number\"\"\"",
            "        return self.__get_config().get_int(\"max_sequence_number\")",
            "",
            "    @property",
            "    def read_page_size(self):",
            "        \"\"\"Returns the configuration value for 'read_page_size'.\"\"\"",
            "        return self.__get_config().get_int(\"read_page_size\")",
            "",
            "    @property",
            "    def log_deletion_delay(self):",
            "        \"\"\"Returns the configuration value for 'log_deletion_delay'.\"\"\"",
            "        return self.__get_config().get_float(\"log_deletion_delay\")",
            "",
            "    @property",
            "    def log_rotation_max_bytes(self):",
            "        \"\"\"Returns the configuration value for 'log_rotation_max_bytes'.\"\"\"",
            "        return self.__get_config().get_int(\"log_rotation_max_bytes\")",
            "",
            "    @property",
            "    def log_rotation_backup_count(self):",
            "        \"\"\"Returns the configuration value for 'log_rotation_backup_count'.\"\"\"",
            "        return self.__get_config().get_int(\"log_rotation_backup_count\")",
            "",
            "    @property",
            "    def copy_staleness_threshold(self):",
            "        \"\"\"Returns the configuration value for 'copy_staleness_threshold'.\"\"\"",
            "        return self.__get_config().get_float(\"copy_staleness_threshold\")",
            "",
            "    @property",
            "    def debug_init(self):",
            "        \"\"\"Returns the configuration value for 'debug_init'.\"\"\"",
            "        return self.__get_config().get_bool(\"debug_init\")",
            "",
            "    @property",
            "    def pidfile_advanced_reuse_guard(self):",
            "        \"\"\"Returns the configuration value for 'pidfile_advanced_reuse_guard'.\"\"\"",
            "        return self.__get_config().get_bool(\"pidfile_advanced_reuse_guard\")",
            "",
            "    @property",
            "    def verify_server_certificate(self):",
            "        \"\"\"Returns the configuration value for 'verify_server_certificate'.\"\"\"",
            "        return self.__get_config().get_bool(\"verify_server_certificate\")",
            "",
            "    @property",
            "    def pipeline_threshold(self):",
            "        \"\"\"Returns the percentage an add events request must be of the maximum allowed request size to",
            "        trigger pipelining the next add events request.",
            "        \"\"\"",
            "        return self.__get_config().get_float(\"pipeline_threshold\")",
            "",
            "    @property",
            "    def strip_domain_from_default_server_host(self):",
            "        \"\"\"Returns whether or not we should remove the domain name from the default server host that is used to",
            "        identify the source of the logs from this agent.  For example, if the hostname is `foo.scalyr.com`, setting",
            "        this field to `true` will result in `foo` being used as the reported `serverHost` name.",
            "",
            "        This only applies if you do not set an explicit `serverHost` attribute in the server attributes.",
            "        \"\"\"",
            "        return self.__get_config().get_bool(\"strip_domain_from_default_server_host\")",
            "",
            "    @property",
            "    def healthy_max_time_since_last_copy_attempt(self):",
            "        \"\"\"Returns the max amount of time since the last copy attempt to consider the agent 'healthy' for",
            "        the purpose of a health check using `status -v` or `-H`. This copy attempt need not be successful, since this is",
            "        just to check that the agent is healthy and should not reflect server side errors.",
            "        \"\"\"",
            "        return self.__get_config().get_float(\"healthy_max_time_since_last_copy_attempt\")",
            "",
            "    def equivalent(self, other, exclude_debug_level=False):",
            "        \"\"\"Returns true if other contains the same configuration information as this object.",
            "",
            "        This is different than an '_eq_' method because this comparison ignores some of the fields",
            "        such as what times the files were read at.  Also, it compares the final results of the configuration,",
            "        after defaults have been applied.",
            "",
            "        @param exclude_debug_level: If True, will also ignore the values for 'debug_level' when doing comparison.",
            "        \"\"\"",
            "        if self.__last_error != other.__last_error:",
            "            return False",
            "",
            "        original_debug_level = None",
            "        try:",
            "            # If we are ignoring debug level, then we do a little hack here where we just put the value for",
            "            # this config into other's config.. and then just put the original value back after we've done the",
            "            # comparison.",
            "            if exclude_debug_level:",
            "                original_debug_level = other.__config.get(\"debug_level\")",
            "                other.__config.put(\"debug_level\", self.__config.get(\"debug_level\"))",
            "",
            "            if self.__config != other.__config:",
            "                return False",
            "            return True",
            "        finally:",
            "            if original_debug_level is not None:",
            "                other.__config.put(\"debug_level\", original_debug_level)",
            "",
            "    @staticmethod",
            "    def get_extra_config_dir(extra_config_dir):",
            "        \"\"\"",
            "        Returns the value for the additional config directory - either from the value passed",
            "        in, or from the environment variable `SCALYR_EXTRA_CONFIG_DIR`.",
            "",
            "        @param extra_config_dir: the additinal configuration directory.  If this value is",
            "            None, then the environment variable `SCALYR_EXTRA_CONFIG_DIR` is read for the result",
            "        \"\"\"",
            "        result = extra_config_dir",
            "        if extra_config_dir is None:",
            "            result = compat.os_getenv_unicode(\"SCALYR_EXTRA_CONFIG_DIR\")",
            "        return result",
            "",
            "    @staticmethod",
            "    def default_ca_cert_path():",
            "        \"\"\"Returns the default configuration file path for the agent.\"\"\"",
            "        # TODO:  Support more platforms.",
            "        return Configuration.__resolve_to_install_location(\"certs\", \"ca_certs.crt\")",
            "",
            "    @property",
            "    def intermediate_certs_path(self):",
            "        \"\"\"Returns the intermediate certs path.\"\"\"",
            "        return Configuration.__resolve_to_install_location(",
            "            \"certs\", \"intermediate_certs.pem\"",
            "        )",
            "",
            "    @staticmethod",
            "    def __resolve_to_install_location(*paths):",
            "        \"\"\"Returns the absolute path created by joining the specified intermediate paths to",
            "        the install location for this package.",
            "",
            "        @param paths: The file components of the desired path. There can be multiple, starting with the outer directory",
            "            first and finishing with the last file.",
            "        \"\"\"",
            "        result = get_install_root()",
            "        for path in paths:",
            "            result = os.path.join(result, path)",
            "        return result",
            "",
            "    def __get_parent_directory(self, file_path):",
            "        \"\"\"Returns the directory containing the specified file.",
            "",
            "        @param file_path: The absolute file path.",
            "",
            "        @return: The absolute path for the parent directory.\"\"\"",
            "        return os.path.dirname(file_path)",
            "",
            "    def __resolve_absolute_path(self, file_path, working_directory):",
            "        \"\"\"Returns the full path for the specified file.",
            "",
            "        If the specified file path is relative, then working_directory is used to resolve the relative path.",
            "        This function does not do any existence checks on either file_path or working_directory.",
            "",
            "        @param file_path: The path of the file.",
            "        @param working_directory: The directory to use to resolve relative file paths.",
            "",
            "        @return: The absolute path for the specified file.",
            "        \"\"\"",
            "        if os.path.isabs(file_path):",
            "            return file_path",
            "",
            "        return os.path.join(working_directory, file_path)",
            "",
            "    def __list_files(self, directory_path):",
            "        \"\"\"Returns a list of the files ending in .json for the specified directory.",
            "",
            "        This only returns files in the directory.  Also, if the directory does not exist",
            "        or cannot be read, an empty list is returned.",
            "",
            "        @param directory_path: The path of the directory.",
            "",
            "        @return: If the directory exists and can be read, the list of files ending in .json (not directories).",
            "        \"\"\"",
            "        result = []",
            "        if directory_path is None:",
            "            return result",
            "        if not os.path.isdir(directory_path):",
            "            return result",
            "        if not os.access(directory_path, os.R_OK):",
            "            return result",
            "",
            "        for f in sorted(os.listdir(directory_path)):",
            "            if f.endswith(\".json\"):",
            "                full_path = os.path.join(directory_path, f)",
            "                if os.path.isfile(full_path):",
            "                    result.append(full_path)",
            "        return result",
            "",
            "    def __add_elements_from_array(self, field, source_json, destination_json):",
            "        \"\"\"Appends any elements in the JsonArray in source_json to destination_json.",
            "",
            "        @param field: The name of the field containing the JsonArray.",
            "        @param source_json: The JsonObject containing the JsonArray from which to retrieve elements.",
            "        @param destination_json: The JsonObject to which the elements should be added (in the JsonArray named field.",
            "        \"\"\"",
            "        destination_array = destination_json.get_json_array(field)",
            "        for element in source_json.get_json_array(field):",
            "            destination_array.add(element)",
            "",
            "    def __set_api_key(self, config, api_key):",
            "        \"\"\"",
            "        Sets the api_key of the config, and throws errors if there are any problems",
            "        \"\"\"",
            "        if api_key:",
            "            config.put(\"api_key\", api_key)",
            "",
            "        if \"api_key\" not in config:",
            "            raise BadConfiguration(",
            "                'The configuration file is missing the required field \"api_key\" that '",
            "                \"sets the authentication key to use when writing logs to Scalyr.  Please update \"",
            "                \"the config file with a Write Logs key from https://www.scalyr.com/keys\",",
            "                \"api_key\",",
            "                \"missingApiKey\",",
            "            )",
            "",
            "        if config.get_string(\"api_key\") == \"\":",
            "            raise BadConfiguration(",
            "                \"The configuration file contains an empty string for the required field \"",
            "                '\"api_key\" that sets the authentication key to use when writing logs to Scalyr. '",
            "                \"Please update the config file with a Write Logs key from https://www.scalyr.com/keys\",",
            "                \"api_key\",",
            "                \"emptyApiKey\",",
            "            )",
            "",
            "    def __check_field(",
            "        self,",
            "        field,",
            "        config,",
            "        file_path,",
            "        previous_value=None,",
            "        previous_config_file=None,",
            "        error_code=None,",
            "    ):",
            "        \"\"\"",
            "        Checks to see if the config contains a value for `field` , and if so verifies that it is a valid string.",
            "",
            "        @param field:  The name of the field to check.",
            "        @param config:  The contents of the config.",
            "        @param file_path:  The file path to the config.",
            "        @param previous_value:  If this field has been already defined in another config, the value it was given.",
            "        @param previous_config_file:  If not None, the path to a config file that has already set this field.  If this",
            "            is not None and this config does define the field, a `BadConfiguration` exception is raised.",
            "        @param error_code:  The error code to return if it is detected the field has been set in multiple config files.",
            "        @return the field's value and file_path if the field is found, else return None and None",
            "",
            "        \"\"\"",
            "",
            "        description = 'configuration file \"%s\"' % file_path",
            "",
            "        if field in config:",
            "            self.__verify_required_string(config, field, description)",
            "            result_key = config.get_string(field)",
            "            result_file = file_path",
            "",
            "            if previous_config_file is not None:",
            "                raise BadConfiguration(",
            "                    'The configuration file \"%s\" contains an \"%s\" value, but that field has already been set in '",
            "                    '\"%s\".  Please ensure that the \"%s\" value is set only once'",
            "                    % (file_path, field, previous_config_file, field),",
            "                    field,",
            "                    error_code,",
            "                )",
            "",
            "            return result_key, result_file",
            "        return previous_value, previous_config_file",
            "",
            "    def __verify_main_config(self, config, file_path):",
            "        self.__verify_main_config_and_apply_defaults(",
            "            config, file_path, apply_defaults=False",
            "        )",
            "",
            "    def __verify_main_config_and_apply_defaults(",
            "        self, config, file_path, apply_defaults=True",
            "    ):",
            "        \"\"\"Verifies the contents of the configuration object and updates missing fields with defaults.",
            "",
            "        This will verify and possibly update all the fields in the configuration file except for",
            "        the 'logs' and 'monitors' json arrays.  If any of the fields do not meet their type requirement,",
            "        an exception will be raised.  If any of the fields are not present, then config will be updated with",
            "        the appropriate default value.",
            "",
            "        @param config: The main JsonObject configuration object.",
            "        @param file_path: The file that was read to retrieve the config object. This is used in error reporting.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "    \"\"\"",
            "        description = 'configuration file \"%s\"' % file_path",
            "",
            "        self.__verify_or_set_optional_string(",
            "            config, \"api_key\", \"\", description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"allow_http\", False, description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"check_remote_if_no_tty\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"compression_type\",",
            "            \"deflate\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "            valid_values=scalyr_util.SUPPORTED_COMPRESSION_ALGORITHMS,",
            "        )",
            "        self.__verify_compression_type(self.compression_type)",
            "",
            "        # NOTE: If not explicitly specified by the user, we use compression algorithm specific",
            "        # default value",
            "        default_compression_level = scalyr_util.COMPRESSION_TYPE_TO_DEFAULT_LEVEL[",
            "            self.compression_type",
            "        ]",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"compression_level\",",
            "            default_compression_level,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_compression_level(self.compression_level)",
            "",
            "        self.__verify_or_set_optional_attributes(",
            "            config, \"server_attributes\", description, apply_defaults, env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"agent_log_path\",",
            "            self.__default_paths.agent_log_path,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"agent_data_path\",",
            "            self.__default_paths.agent_data_path,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"additional_monitor_module_paths\",",
            "            \"\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"config_directory\",",
            "            \"agent.d\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"json_library\",",
            "            \"auto\",",
            "            \"JSON serialization and deserializarion library to use. Valid options are auto, json, ujson and orjson\",",
            "            apply_defaults,",
            "            env_aware=True,",
            "            valid_values=[\"auto\", \"json\", \"ujson\", \"orjson\"],",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"implicit_agent_log_collection\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"implicit_metric_monitor\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"implicit_agent_process_metrics_monitor\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"use_unsafe_debugging\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"copying_thread_profile_interval\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"copying_thread_profile_output_path\",",
            "            \"/tmp/copying_thread_profiles_\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"global_monitor_sample_interval\",",
            "            30.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"close_old_files_duration_in_seconds\",",
            "            60 * 60 * 1,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"full_checkpoint_interval_in_seconds\",",
            "            60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"max_send_rate_enforcement\",",
            "            \"unlimited\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_max_send_rate_enforcement_overrides\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_allowed_request_size\",",
            "            1 * 1024 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"min_allowed_request_size\",",
            "            100 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"min_request_spacing_interval\",",
            "            1.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"max_request_spacing_interval\",",
            "            5.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"max_error_request_spacing_interval\",",
            "            30.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"minimum_scan_interval\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"low_water_bytes_sent\",",
            "            20 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"low_water_request_spacing_adjustment\",",
            "            1.5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"high_water_bytes_sent\",",
            "            100 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"high_water_request_spacing_adjustment\",",
            "            0.6,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"failure_request_spacing_adjustment\",",
            "            1.5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"request_too_large_adjustment\",",
            "            0.5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"max_new_log_detection_time\",",
            "            60.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # These parameters are used in log_processing.py to govern how logs are copied.",
            "",
            "        # The maximum allowed size for a line when reading from a log file.",
            "        # We do not strictly enforce this -- some lines returned by LogFileIterator may be",
            "        # longer than this due to some edge cases.",
            "        self.__verify_or_set_optional_int(",
            "            config, \"max_line_size\", 49900, description, apply_defaults, env_aware=True",
            "        )",
            "",
            "        # The number of seconds we are willing to wait when encountering a log line at the end of a log file that does",
            "        # not currently end in a new line (referred to as a partial line).  It could be that the full line just hasn't",
            "        # made it all the way to disk yet.  After this time though, we will just return the bytes as a line",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"line_completion_wait_time\",",
            "            5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The maximum negative offset relative to the end of a previously unseen log the log file",
            "        # iterator is allowed to become.  If bytes are not being read quickly enough, then",
            "        # the iterator will automatically advance so that it is no more than this length",
            "        # to the end of the file.  This is essentially the maximum bytes a new log file",
            "        # is allowed to be caught up when used in copying logs to Scalyr.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_log_offset_size\",",
            "            5 * 1024 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The maximum negative offset relative to the end of an existing log the log file",
            "        # iterator is allowed to become.  If bytes are not being read quickly enough, then",
            "        # the iterator will automatically advance so that it is no more than this length",
            "        # to the end of the file.  This is essentially the maximum bytes an existing log file",
            "        # is allowed to be caught up when used in copying logs to Scalyr.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_existing_log_offset_size\",",
            "            100 * 1024 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The maximum sequence number for a given sequence",
            "        # The sequence number is typically the total number of bytes read from a given file",
            "        # (across restarts and log rotations), and it resets to zero (and begins a new sequence)",
            "        # for each file once the current sequence_number exceeds this value",
            "        # defaults to 1 TB",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_sequence_number\",",
            "            1024 ** 4,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The number of bytes to read from a file at a time into the buffer.  This must",
            "        # always be greater than the MAX_LINE_SIZE",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"read_page_size\",",
            "            64 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"internal_parse_max_line_size\",",
            "            config.get_int(\"read_page_size\", 64 * 1024),",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The minimum time we wait for a log file to reappear on a file system after it has been removed before",
            "        # we consider it deleted.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"log_deletion_delay\",",
            "            10 * 60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # How many log rotations to do",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"log_rotation_backup_count\",",
            "            2,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The size of each log rotation file",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"log_rotation_max_bytes\",",
            "            20 * 1024 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The percentage of the maximum message size a message (max_allowed_request_size) has to be to trigger",
            "        # pipelining the next add events request.  This intentionally set to 110% to prevent it from being used unless",
            "        # explicitly requested.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"pipeline_threshold\",",
            "            1.1,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # If we have noticed that new bytes have appeared in a file but we do not read them before this threshold",
            "        # is exceeded, then we consider those bytes to be stale and just skip to reading from the end to get the",
            "        # freshest bytes.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"copy_staleness_threshold\",",
            "            15 * 60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"debug_init\", False, description, apply_defaults, env_aware=True",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"pidfile_advanced_reuse_guard\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"strip_domain_from_default_server_host\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"healthy_max_time_since_last_copy_attempt\",",
            "            60.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config, \"debug_level\", 0, description, apply_defaults, env_aware=True",
            "        )",
            "        debug_level = config.get_int(\"debug_level\", apply_defaults)",
            "        if debug_level < 0 or debug_level > 5:",
            "            raise BadConfiguration(",
            "                \"The debug level must be between 0 and 5 inclusive\",",
            "                \"debug_level\",",
            "                \"badDebugLevel\",",
            "            )",
            "",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"stdout_severity\",",
            "            \"NOTSET\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        stdout_severity = config.get_string(\"stdout_severity\", default_value=\"NOTSET\")",
            "        if not hasattr(logging, stdout_severity.upper()):",
            "            raise BadConfiguration(",
            "                \"The stdout severity must be a valid logging level name\",",
            "                \"stdout_severity\",",
            "                \"badStdoutSeverity\",",
            "            )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"request_deadline\",",
            "            60.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"ca_cert_path\",",
            "            Configuration.default_ca_cert_path(),",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"use_requests_lib\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"use_tlslite\", False, description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"verify_server_certificate\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config, \"http_proxy\", None, description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config, \"https_proxy\", None, description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_array_of_strings(",
            "            config,",
            "            \"k8s_ignore_namespaces\",",
            "            Configuration.DEFAULT_K8S_IGNORE_NAMESPACES,",
            "            description,",
            "            apply_defaults,",
            "            separators=[None, \",\"],",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_array_of_strings(",
            "            config,",
            "            \"k8s_include_namespaces\",",
            "            Configuration.DEFAULT_K8S_INCLUDE_NAMESPACES,",
            "            description,",
            "            apply_defaults,",
            "            separators=[None, \",\"],",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_api_url\",",
            "            \"https://kubernetes.default\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_verify_api_queries\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_verify_kubelet_queries\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_kubelet_ca_cert\",",
            "            \"/run/secrets/kubernetes.io/serviceaccount/ca.crt\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_query_timeout_secs\",",
            "            20,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_expiry_secs\",",
            "            30,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_expiry_fuzz_secs\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_start_fuzz_secs\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_purge_secs\",",
            "            300,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_service_account_cert\",",
            "            \"/run/secrets/kubernetes.io/serviceaccount/ca.crt\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_service_account_token\",",
            "            \"/var/run/secrets/kubernetes.io/serviceaccount/token\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_service_account_namespace\",",
            "            \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # Whether to log api responses to agent_debug.log",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_log_api_responses\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # If set to True, do not log successes (response code 2xx)",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_log_api_exclude_200s\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # Minimum response length of api responses to be logged.  Responses smaller than this limit are not logged.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_log_api_min_response_len\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # Minimum latency of responses to be logged.  Responses faster than this limit are not logged.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_log_api_min_latency\",",
            "            0.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # If positive, api calls with the same path will be rate-limited to a message every interval seconds",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_log_api_ratelimit_interval\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # TODO-163 : make other settings more aggressive",
            "",
            "        # Optional (defaults to 3). The number of times the warmer will retry a query to warm a pod before giving up and",
            "        # classifying it as a Temporary Error",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_controlled_warmer_max_query_retries\",",
            "            3,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # Optional (defaults to 5). The maximum number of Temporary Errors that may occur when warming a pod's entry,",
            "        # before the warmer blacklists it.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_controlled_warmer_max_attempts\",",
            "            5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # Optional (defaults to 300). When a pod is blacklisted, how many secs it must wait until it is",
            "        # tried again for warming.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_controlled_warmer_blacklist_time\",",
            "            300,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_events_disable\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # Agent-wide k8s rate limiter settings",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_ratelimit_cluster_num_agents\",",
            "            1,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_cluster_rps_init\",",
            "            1000.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_cluster_rps_min\",",
            "            1.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_cluster_rps_max\",",
            "            1e9,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_ratelimit_consecutive_increase_threshold\",",
            "            5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_ratelimit_strategy\",",
            "            BlockingRateLimiter.STRATEGY_MULTIPLY,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_increase_factor\",",
            "            2.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_backoff_factor\",",
            "            0.5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_ratelimit_max_concurrency\",",
            "            1,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_send_requests\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"enforce_monotonic_timestamps\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"include_raw_timestamp_field\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"enable_profiling\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_profile_interval_minutes\",",
            "            60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"profile_duration_minutes\",",
            "            2,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"profile_clock\",",
            "            \"random\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"profile_log_name\",",
            "            \"agent.callgrind\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"memory_profile_log_name\",",
            "            \"agent.meminfo\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # AGENT-263: controls sending in the new format or not as a safety in case it is broken somewhere in the chain",
            "        # TODO: Remove this in a future release once we are more certain that it works",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_logfile_addevents_format\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # Debug leak flags",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_monitor_threads\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_monitors_creation\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_new_file_matches\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_leak_scan_for_new_bytes\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_leak_processing_new_bytes\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_copying_thread\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_overall_stats\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_bandwidth_stats\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_copy_manager_stats\", False, description, apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_leak_update_debug_log_level\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"enable_gc_stats\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config, \"disable_leak_all_config_updates\", None, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config, \"disable_leak_verify_config\", None, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"disable_leak_config_equivalence_check\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"disable_leak_verify_can_write_to_logs\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config, \"disable_leak_config_reload\", None, description, apply_defaults",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"config_change_check_interval\",",
            "            30,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # How often to capture and log overall agent stats (in seconds).",
            "        # NOTE: This values must be >= config_change_check_interval.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"overall_stats_log_interval\",",
            "            600,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # How often to capture and log copying manager agent stats (in seconds).",
            "        # NOTE: This values must be >= config_change_check_interval.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"copying_manager_stats_log_interval\",",
            "            300,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # How often to capture and log bandwidth related stats (in seconds).",
            "        # NOTE: This values must be >= config_change_check_interval.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"bandwidth_stats_log_interval\",",
            "            60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"user_agent_refresh_interval\",",
            "            60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"garbage_collect_interval\",",
            "            300,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"disable_leak_verify_config_create_monitors_manager\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"disable_leak_verify_config_create_copying_manager\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_leak_verify_config_cache_config\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "",
            "    def __verify_compression_type(self, compression_type):",
            "        \"\"\"",
            "        Verify that the library for the specified compression type (algorithm) is available.",
            "        \"\"\"",
            "        library_name = scalyr_util.COMPRESSION_TYPE_TO_PYTHON_LIBRARY.get(",
            "            compression_type, \"unknown\"",
            "        )",
            "",
            "        try:",
            "            _, _ = scalyr_util.get_compress_and_decompress_func(compression_type)",
            "        except (ImportError, ValueError) as e:",
            "            msg = (",
            "                'Failed to set compression type to \"%s\". Make sure that the corresponding Python '",
            "                \"library is available. You can install it using this command:\\n\\npip install %s\\n\\n \"",
            "                \"Original error: %s\" % (compression_type, library_name, str(e))",
            "            )",
            "            raise BadConfiguration(msg, \"compression_type\", \"invalidCompressionType\")",
            "",
            "    def __verify_compression_level(self, compression_level):",
            "        \"\"\"",
            "        Verify that the provided compression level is valid for the configured compression type.",
            "",
            "        If it's not, we use a default value for that compression algorithm. Keep in mind that this",
            "        behavior is there for backward compatibility reasons, otherwise it would be better to just",
            "        throw in such scenario",
            "        \"\"\"",
            "        compression_type = self.compression_type",
            "",
            "        valid_level_min, valid_level_max = scalyr_util.COMPRESSION_TYPE_TO_VALID_LEVELS[",
            "            compression_type",
            "        ]",
            "",
            "        if compression_level < valid_level_min or compression_level > valid_level_max:",
            "            self.__config.put(",
            "                \"compression_level\",",
            "                scalyr_util.COMPRESSION_TYPE_TO_DEFAULT_LEVEL[compression_type],",
            "            )",
            "",
            "    def __get_config_or_environment_val(",
            "        self, config_object, param_name, param_type, env_aware, custom_env_name",
            "    ):",
            "        \"\"\"Returns a type-converted config param value or if not found, a matching environment value.",
            "",
            "        If the environment value is returned, it is also written into the config_object.",
            "",
            "        Currently only handles the following types (str, int, bool, float, JsonObject, JsonArray).",
            "        Also validates that environment variables can be correctly converted into the primitive type.",
            "",
            "        Both upper-case and lower-case versions of the environment variable will be checked.",
            "",
            "        @param config_object: The JsonObject config containing the field as a key",
            "        @param param_name: Parameter name",
            "        @param param_type: Parameter type",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param custom_env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name. Both upper and lower case versions are tried.",
            "            Note: A non-empty value also automatically implies env_aware as True, regardless of it's value.",
            "",
            "        @return A python object representing the config param (or environment) value or None",
            "        @raises",
            "            JsonConversionException: if the config value or env value cannot be correctly converted.",
            "            TypeError: if the param_type is not supported.",
            "        \"\"\"",
            "        if param_type == int:",
            "            config_val = config_object.get_int(param_name, none_if_missing=True)",
            "        elif param_type == bool:",
            "            config_val = config_object.get_bool(param_name, none_if_missing=True)",
            "        elif param_type == float:",
            "            config_val = config_object.get_float(param_name, none_if_missing=True)",
            "        elif param_type == six.text_type:",
            "            config_val = config_object.get_string(param_name, none_if_missing=True)",
            "        elif param_type == JsonObject:",
            "            config_val = config_object.get_json_object(param_name, none_if_missing=True)",
            "        elif param_type == JsonArray:",
            "            config_val = config_object.get_json_array(param_name, none_if_missing=True)",
            "        elif param_type in (ArrayOfStrings, SpaceAndCommaSeparatedArrayOfStrings):",
            "            # ArrayOfStrings are extracted from config file as JsonArray",
            "            # (but extracted from the environment different from JsonArray)",
            "            config_val = config_object.get_json_array(param_name, none_if_missing=True)",
            "        else:",
            "            raise TypeError(",
            "                \"Unsupported environment variable conversion type %s (param name = %s)\"",
            "                % (param_type, param_name)",
            "            )",
            "",
            "        if not env_aware:",
            "            if not custom_env_name:",
            "                return config_val",
            "",
            "        self._environment_aware_map[param_name] = custom_env_name or (",
            "            \"SCALYR_%s\" % param_name.upper()",
            "        )",
            "",
            "        env_val = get_config_from_env(",
            "            param_name,",
            "            custom_env_name=custom_env_name,",
            "            convert_to=param_type,",
            "            logger=self.__logger,",
            "            param_val=config_val,",
            "        )",
            "",
            "        # Not set in environment",
            "        if env_val is None:",
            "            return config_val",
            "",
            "        # Config file value wins if set",
            "        if config_val is not None:",
            "            return config_val",
            "",
            "        config_object.update({param_name: env_val})",
            "        return env_val",
            "",
            "    def __verify_logs_and_monitors_configs_and_apply_defaults(self, config, file_path):",
            "        \"\"\"Verifies the contents of the 'logs' and 'monitors' fields and updates missing fields with defaults.",
            "",
            "        This will verify and possible update the json arrays holding the 'logs' and 'monitor's configuration.",
            "        If any of the fields in those arrays or their contained elements do not meet their type requirement,",
            "        an exception will be raised.  If any of the fields are not present, then config will be updated with",
            "        the appropriate default values.",
            "",
            "        @param config: The main JsonObject configuration object.",
            "        @param file_path: The file that was read to retrieve the config object. This is used in error reporting.",
            "        \"\"\"",
            "        description = 'in configuration file \"%s\"' % file_path",
            "        self.__verify_or_set_optional_array(config, \"logs\", description)",
            "        self.__verify_or_set_optional_array(config, \"journald_logs\", description)",
            "        self.__verify_or_set_optional_array(config, \"k8s_logs\", description)",
            "        self.__verify_or_set_optional_array(config, \"monitors\", description)",
            "",
            "        i = 0",
            "        for log_entry in config.get_json_array(\"logs\"):",
            "            self.__verify_log_entry_and_set_defaults(",
            "                log_entry, config_file_path=file_path, entry_index=i",
            "            )",
            "            i += 1",
            "",
            "        i = 0",
            "        for log_entry in config.get_json_array(\"journald_logs\"):",
            "            self.__verify_log_entry_with_key_and_set_defaults(",
            "                log_entry,",
            "                key=\"journald_unit\",",
            "                config_file_path=file_path,",
            "                entry_index=i,",
            "            )",
            "            i += 1",
            "",
            "        i = 0",
            "        for log_entry in config.get_json_array(\"k8s_logs\"):",
            "            self.__verify_k8s_log_entry_and_set_defaults(",
            "                log_entry, config_file_path=file_path, entry_index=i,",
            "            )",
            "            i += 1",
            "",
            "        i = 0",
            "        for monitor_entry in config.get_json_array(\"monitors\"):",
            "            self.__verify_monitor_entry_and_set_defaults(",
            "                monitor_entry, file_path=file_path, entry_index=i",
            "            )",
            "            i += 1",
            "",
            "    def __verify_k8s_log_entry_and_set_defaults(",
            "        self, log_entry, description=None, config_file_path=None, entry_index=None,",
            "    ):",
            "        \"\"\"Verifies that the configuration for the specified k8s log entry.",
            "",
            "        A \"k8s_log\" entry can have one of multiple keys defined `k8s_pod_glob`, `k8s_namespace_glob`,",
            "        or `k8s_container_glob` which is a string containing a glob pattern to match against.",
            "",
            "        By default each of these values is `*` which matches against everything.  Users can",
            "        set these fields to limit the log configuration to specific pods, namespaces and containers.",
            "",
            "        Only the first matching config will be applied to any give log.  Users should make sure to",
            "        place more specific matching rules before more general ones.",
            "",
            "        Also verify the rest of the log config meets the required log config criteria and sets any defaults.",
            "",
            "        Raises an exception if it does not.",
            "",
            "        @param log_entry: The JsonObject holding the configuration for a log.",
            "        @param description: A human-readable description of where the log entry came from to use in error messages. If",
            "            none is given, then both file_path and entry_index must be set.",
            "        @param config_file_path: The path for the file from where the configuration was read. Used to generate the",
            "            description if none was given.",
            "        @param entry_index: The index of the entry in the 'logs' json array. Used to generate the description if none",
            "            was given.",
            "        \"\"\"",
            "",
            "        self.__verify_or_set_optional_string(",
            "            log_entry, \"k8s_pod_glob\", \"*\", description",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry, \"k8s_namespace_glob\", \"*\", description",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry, \"k8s_container_glob\", \"*\", description",
            "        )",
            "",
            "        self.__verify_log_entry_with_key_and_set_defaults(",
            "            log_entry,",
            "            None,",
            "            description=description,",
            "            config_file_path=config_file_path,",
            "            entry_index=entry_index,",
            "            # k8s log config defaults are applied later when adding containers to the copying manager",
            "            # so don't set them here",
            "            apply_defaults=False,",
            "            logs_field=\"k8s_logs\",",
            "        )",
            "",
            "    def __verify_log_entry_and_set_defaults(",
            "        self, log_entry, description=None, config_file_path=None, entry_index=None",
            "    ):",
            "        \"\"\"Verifies that the configuration for the specified log has a key called 'path' and meets all",
            "        the required criteria and sets any defaults.",
            "",
            "        Raises an exception if it does not.",
            "",
            "        @param log_entry: The JsonObject holding the configuration for a log.",
            "        @param description: A human-readable description of where the log entry came from to use in error messages. If",
            "            none is given, then both file_path and entry_index must be set.",
            "        @param config_file_path: The path for the file from where the configuration was read. Used to generate the",
            "            description if none was given.",
            "        @param entry_index: The index of the entry in the 'logs' json array. Used to generate the description if none",
            "            was given.",
            "        \"\"\"",
            "        # Make sure the log_enty has a path and the path is absolute.",
            "        path = log_entry.get_string(\"path\", none_if_missing=True)",
            "        if path and not os.path.isabs(path):",
            "            log_entry.put(\"path\", os.path.join(self.agent_log_path, path))",
            "        self.__verify_log_entry_with_key_and_set_defaults(",
            "            log_entry,",
            "            \"path\",",
            "            description=description,",
            "            config_file_path=config_file_path,",
            "            entry_index=entry_index,",
            "        )",
            "",
            "    def __verify_log_entry_with_key_and_set_defaults(",
            "        self,",
            "        log_entry,",
            "        key=None,",
            "        description=None,",
            "        config_file_path=None,",
            "        entry_index=None,",
            "        apply_defaults=True,",
            "        logs_field=\"logs\",",
            "    ):",
            "        \"\"\"Verifies that the configuration for the specified log meets all the required criteria and sets any defaults.",
            "",
            "        Raises an exception if it does not.",
            "",
            "        @param log_entry: The JsonObject holding the configuration for a log.",
            "        @param key: A key that must exist in the log entry.  The key is used to identify logs e.g. by path, container name etc",
            "            If `None` then no checking is done",
            "        @param description: A human-readable description of where the log entry came from to use in error messages. If",
            "            none is given, then both file_path and entry_index must be set.",
            "        @param config_file_path: The path for the file from where the configuration was read. Used to generate the",
            "            description if none was given.",
            "        @param entry_index: The index of the entry in the 'logs' json array. Used to generate the description if none",
            "            was given.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param logs_field: The name of the field used for log configs",
            "        \"\"\"",
            "        no_description_given = description is None",
            "        if no_description_given:",
            "            description = (",
            "                'the entry with index=%i in the \"%s\" array in configuration file \"%s\"'",
            "                % (entry_index, logs_field, config_file_path)",
            "            )",
            "        log = None",
            "        if key is not None:",
            "            # Verify it has a `key` entry that is a string.",
            "            self.__verify_required_string(log_entry, key, description)",
            "            log = log_entry.get_string(key)",
            "",
            "        if log is not None and no_description_given:",
            "            description = (",
            "                'the entry for \"%s\" in the \"%s\" array in configuration file \"%s\"'",
            "                % (log, logs_field, config_file_path)",
            "            )",
            "",
            "        self.__verify_or_set_optional_array_of_strings(",
            "            log_entry, \"exclude\", [], description, apply_defaults=apply_defaults,",
            "        )",
            "",
            "        # If a parser was specified, make sure it is a string.",
            "        if \"parser\" in log_entry:",
            "            self.__verify_or_set_optional_string(",
            "                log_entry,",
            "                \"parser\",",
            "                \"ignored\",",
            "                description,",
            "                apply_defaults=apply_defaults,",
            "            )",
            "",
            "        self.__verify_or_set_optional_attributes(log_entry, \"attributes\", description)",
            "",
            "        self.__verify_or_set_optional_array(log_entry, \"lineGroupers\", description)",
            "        i = 0",
            "        for element in log_entry.get_json_array(\"lineGroupers\"):",
            "            element_description = (",
            "                'the entry with index=%i in the \"lineGroupers\" array in ' % i",
            "            )",
            "            element_description += description",
            "",
            "            self.__verify_required_string(element, \"start\", element_description)",
            "            self.__verify_contains_exactly_one_string_out_of(",
            "                element,",
            "                [\"continueThrough\", \"continuePast\", \"haltBefore\", \"haltWith\"],",
            "                description,",
            "            )",
            "            i += 1",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            log_entry,",
            "            \"copy_from_start\",",
            "            False,",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            log_entry, \"parse_lines_as_json\", None, description, apply_defaults=False",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry,",
            "            \"parse_format\",",
            "            \"raw\",",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry,",
            "            \"json_message_field\",",
            "            \"log\",",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry,",
            "            \"json_timestamp_field\",",
            "            \"time\",",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            log_entry,",
            "            \"ignore_stale_files\",",
            "            False,",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            log_entry,",
            "            \"staleness_threshold_secs\",",
            "            5 * 60,",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            log_entry,",
            "            \"minimum_scan_interval\",",
            "            None,",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "",
            "        # Verify that if it has a sampling_rules array, then it is an array of json objects.",
            "        self.__verify_or_set_optional_array(log_entry, \"sampling_rules\", description)",
            "        i = 0",
            "        for element in log_entry.get_json_array(\"sampling_rules\"):",
            "            element_description = (",
            "                'the entry with index=%i in the \"sampling_rules\" array in ' % i",
            "            )",
            "            element_description += description",
            "            self.__verify_required_regexp(",
            "                element, \"match_expression\", element_description",
            "            )",
            "            self.__verify_required_percentage(",
            "                element, \"sampling_rate\", element_description",
            "            )",
            "            i += 1",
            "",
            "        # Verify that if it has a redaction_rules array, then it is an array of json objects.",
            "        self.__verify_or_set_optional_array(log_entry, \"redaction_rules\", description)",
            "        i = 0",
            "        for element in log_entry.get_json_array(\"redaction_rules\"):",
            "            element_description = (",
            "                'the entry with index=%i in the \"redaction_rules\" array in ' % i",
            "            )",
            "            element_description += description",
            "",
            "            self.__verify_required_regexp(",
            "                element, \"match_expression\", element_description",
            "            )",
            "            self.__verify_or_set_optional_string(",
            "                element, \"replacement\", \"\", element_description",
            "            )",
            "            i += 1",
            "",
            "        # We support the parser definition being at the top-level of the log config object, but we really need to",
            "        # put it in the attributes.",
            "        if \"parser\" in log_entry:",
            "            # noinspection PyTypeChecker",
            "            log_entry[\"attributes\"][\"parser\"] = log_entry[\"parser\"]",
            "",
            "    def __verify_monitor_entry_and_set_defaults(",
            "        self, monitor_entry, context_description=None, file_path=None, entry_index=None",
            "    ):",
            "        \"\"\"Verifies that the config for the specified monitor meets all the required criteria and sets any defaults.",
            "",
            "        Raises an exception if it does not.",
            "",
            "        @param monitor_entry: The JsonObject holding the configuration for a monitor.",
            "        @param file_path: The path for the file from where the configuration was read. Used to report errors to user.",
            "        @param entry_index: The index of the entry in the 'monitors' json array. Used to report errors to user.",
            "        \"\"\"",
            "        # Verify that it has a module name",
            "        if context_description is None:",
            "            description = (",
            "                'the entry with index=%i in the \"monitors\" array in configuration file \"%s\"'",
            "                % (entry_index, file_path)",
            "            )",
            "        else:",
            "            description = context_description",
            "",
            "        self.__verify_required_string(monitor_entry, \"module\", description)",
            "",
            "        module_name = monitor_entry.get_string(\"module\")",
            "",
            "        if context_description is None:",
            "            description = (",
            "                'the entry for module \"%s\" in the \"monitors\" array in configuration file \"%s\"'",
            "                % (module_name, file_path)",
            "            )",
            "        else:",
            "            description = context_description",
            "",
            "        # Verify that if it has a log_name field, it is a string.",
            "        self.__verify_or_set_optional_string(",
            "            monitor_entry, \"log_path\", module_name + \".log\", description",
            "        )",
            "",
            "    def __merge_server_attributes(self, fragment_file_path, config_fragment, config):",
            "        \"\"\"Merges the contents of the server attribute read from a configuration fragment to the main config object.",
            "",
            "        @param fragment_file_path: The path of the file from which the fragment was read. Used for error messages.",
            "        @param config_fragment: The JsonObject in the fragment file containing the 'server_attributes' field.",
            "        @param config: The main config object also containing a server_attributes field. The contents of the one from",
            "            config_fragment will be merged into this one.",
            "        \"\"\"",
            "        self.__verify_or_set_optional_attributes(",
            "            config_fragment,",
            "            \"server_attributes\",",
            "            'the configuration fragment at \"%s\"' % fragment_file_path,",
            "        )",
            "        source = config_fragment[\"server_attributes\"]",
            "        destination = config[\"server_attributes\"]",
            "        for k in source:",
            "            destination[k] = source[k]",
            "",
            "    def __verify_required_string(self, config_object, field, config_description):",
            "        \"\"\"Verifies that config_object has the required field and it can be converted to a string.",
            "",
            "        Raises an exception otherwise.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        \"\"\"",
            "        try:",
            "            config_object.get_string(field)",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The field \"%s\" is not a string.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"notString\",",
            "            )",
            "        except JsonMissingFieldException:",
            "            raise BadConfiguration(",
            "                'The required field \"%s\" is missing.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"missingRequired\",",
            "            )",
            "",
            "    def __verify_contains_exactly_one_string_out_of(",
            "        self, config_object, fields, config_description",
            "    ):",
            "        \"\"\"Verifies that config_object has exactly one of the named fields and it can be converted to a string.",
            "",
            "        Raises an exception otherwise.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param fields: A list of field names to check in the config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @return: The name of the found key, or None",
            "        \"\"\"",
            "        count = 0",
            "        result = None",
            "        for field in fields:",
            "            try:",
            "                value = config_object.get_string(field, none_if_missing=True)",
            "                if value is not None:",
            "                    result = field",
            "                    count += 1",
            "            except JsonConversionException:",
            "                raise BadConfiguration(",
            "                    'The field \"%s\" is not a string.  Error is in %s'",
            "                    % (field, config_description),",
            "                    field,",
            "                    \"notString\",",
            "                )",
            "        if count == 0:",
            "            raise BadConfiguration(",
            "                'A required field is missing.  Object must contain one of \"%s\".  Error is in %s'",
            "                % (six.text_type(fields), config_description),",
            "                field,",
            "                \"missingRequired\",",
            "            )",
            "        elif count > 1:",
            "            raise BadConfiguration(",
            "                'A required field has too many options.  Object must contain only one of \"%s\".  Error is in %s'",
            "                % (six.text_type(fields), config_description),",
            "                field,",
            "                \"missingRequired\",",
            "            )",
            "        return result",
            "",
            "    def __verify_or_set_optional_string(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "        valid_values=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is a string if present, otherwise sets default.",
            "",
            "        Raises an exception if the existing field cannot be converted to a string.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: The value to set in config_object for field if it currently has no value.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        @param valid_values: Optional list with valid values for this string.",
            "        \"\"\"",
            "        try:",
            "            value = self.__get_config_or_environment_val(",
            "                config_object, field, six.text_type, env_aware, env_name",
            "            )",
            "",
            "            if value is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, default_value)",
            "                return",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for field \"%s\" is not a string.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"notString\",",
            "            )",
            "",
            "        if value is not None and valid_values and value not in valid_values:",
            "            raise BadConfiguration(",
            "                'Got invalid value \"%s\" for field \"%s\". Valid values are: %s'",
            "                % (value, field, \", \".join(valid_values)),",
            "                field,",
            "                \"invalidValue\",",
            "            )",
            "",
            "    def __verify_or_set_optional_int(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object can be converted to an int if present, otherwise",
            "        sets default.",
            "",
            "        Raises an exception if the existing field cannot be converted to an int.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: The value to set in config_object for field if it currently has no value.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            value = self.__get_config_or_environment_val(",
            "                config_object, field, int, env_aware, env_name",
            "            )",
            "",
            "            if value is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, default_value)",
            "                return",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for field \"%s\" is not an int.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"notInt\",",
            "            )",
            "",
            "    def __verify_or_set_optional_float(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object can be converted to a float if present, otherwise",
            "        sets default.",
            "",
            "        Raises an exception if the existing field cannot be converted to a float.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: The value to set in config_object for field if it currently has no value.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            value = self.__get_config_or_environment_val(",
            "                config_object, field, float, env_aware, env_name",
            "            )",
            "",
            "            if value is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, default_value)",
            "                return",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for field \"%s\" is not an float.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"notFloat\",",
            "            )",
            "",
            "    def __verify_or_set_optional_attributes(",
            "        self,",
            "        config_object,",
            "        field,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is a json object if present, otherwise sets to empty",
            "        object.",
            "",
            "        Raises an exception if the existing field is not a json object or if any of its values cannot be converted",
            "        to a string.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            json_object = self.__get_config_or_environment_val(",
            "                config_object, field, JsonObject, env_aware, env_name",
            "            )",
            "",
            "            if json_object is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, JsonObject())",
            "                return",
            "",
            "            for key in json_object.keys():",
            "                try:",
            "                    json_object.get_string(key)",
            "                except JsonConversionException:",
            "                    raise BadConfiguration(",
            "                        'The value for field \"%s\" in the json object for \"%s\" is not a '",
            "                        \"string.  Error is in %s\" % (key, field, config_description),",
            "                        field,",
            "                        \"notString\",",
            "                    )",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for the field \"%s\" is not a json object.  '",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notJsonObject\",",
            "            )",
            "",
            "    def __verify_or_set_optional_bool(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is a boolean if present, otherwise sets default.",
            "",
            "        Raises an exception if the existing field cannot be converted to a boolean.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: The value to set in config_object for field if it currently has no value.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            value = self.__get_config_or_environment_val(",
            "                config_object, field, bool, env_aware, env_name",
            "            )",
            "",
            "            if value is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, default_value)",
            "                return",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for the required field \"%s\" is not a boolean.  '",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notBoolean\",",
            "            )",
            "",
            "    def __verify_or_set_optional_array(",
            "        self,",
            "        config_object,",
            "        field,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is an array of json objects if present, otherwise sets",
            "        to empty array.",
            "",
            "        Raises an exception if the existing field is not a json array or if any of its elements are not json objects.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            json_array = self.__get_config_or_environment_val(",
            "                config_object, field, JsonArray, env_aware, env_name",
            "            )",
            "",
            "            if json_array is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, JsonArray())",
            "                return",
            "",
            "            index = 0",
            "            for x in json_array:",
            "                if not isinstance(x, JsonObject):",
            "                    raise BadConfiguration(",
            "                        \"The element at index=%i is not a json object as required in the array \"",
            "                        'field \"%s (%s, %s)\".  Error is in %s'",
            "                        % (index, field, type(x), six.text_type(x), config_description),",
            "                        field,",
            "                        \"notJsonObject\",",
            "                    )",
            "                index += 1",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for the required field \"%s\" is not an array.  '",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notJsonArray\",",
            "            )",
            "",
            "    def __verify_or_set_optional_array_of_strings(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        separators=[\",\"],",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is an array of strings if present, otherwise sets",
            "        to empty array.",
            "",
            "        Raises an exception if the existing field is not a json array or if any of its elements are not strings/unicode.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: Default values (array of strings)",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param separators: list of allowed separators (An entry of None represents \"any whitespace\")",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        # 2->TODO Python3 can not sort None values",
            "        separators.sort(key=lambda s: s if s is not None else \"\")",
            "        # For legacy reasons, must support space-separated array of strings",
            "        cls = ArrayOfStrings",
            "        if separators == [None, \",\"]:",
            "            cls = SpaceAndCommaSeparatedArrayOfStrings",
            "        try:",
            "            array_of_strings = self.__get_config_or_environment_val(",
            "                config_object, field, cls, env_aware, env_name",
            "            )",
            "",
            "            if array_of_strings is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, cls(values=default_value))",
            "                return",
            "",
            "            index = 0",
            "            for x in array_of_strings:",
            "                if not isinstance(x, six.string_types):",
            "                    raise BadConfiguration(",
            "                        \"The element at index=%i is not a string or unicode object as required in the array \"",
            "                        'field \"%s\".  Error is in %s'",
            "                        % (index, field, config_description),",
            "                        field,",
            "                        \"notStringOrUnicode\",",
            "                    )",
            "                index += 1",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for the required field \"%s\" is not an array.  '",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notJsonArray\",",
            "            )",
            "",
            "    def __verify_required_regexp(self, config_object, field, config_description):",
            "        \"\"\"Verifies that config_object has the specified field and it can be parsed as a regular expression, otherwise",
            "        raises an exception.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        \"\"\"",
            "",
            "        try:",
            "            value = config_object.get_string(field, none_if_missing=True)",
            "",
            "            if value is not None:",
            "                re.compile(value)",
            "                return",
            "        except Exception:",
            "            raise BadConfiguration(",
            "                'The value for required field \"%s\" has a value that cannot be parsed as '",
            "                \"string regular expression (using python syntax).  \"",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notRegexp\",",
            "            )",
            "",
            "        raise BadConfiguration(",
            "            'The required regular expression field \"%s\" is missing.  Error is in %s'",
            "            % (field, config_description),",
            "            field,",
            "            \"missingRequired\",",
            "        )",
            "",
            "    def __verify_required_percentage(self, config_object, field, config_description):",
            "        \"\"\"Verifies that config_object has the specified field and it can be it is a number between 0 and 1, otherwise",
            "        raises an exception.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        \"\"\"",
            "        try:",
            "            value = config_object.get_float(field, none_if_missing=True)",
            "",
            "            if value is None:",
            "                raise BadConfiguration(",
            "                    'The required percentage field \"%s\" is missing.  Error is in %s'",
            "                    % (field, config_description),",
            "                    field,",
            "                    \"missingRequired\",",
            "                )",
            "            elif value < 0 or value > 1:",
            "                raise BadConfiguration(",
            "                    'The required percentage field \"%s\" has a value \"%s\" that is not a number '",
            "                    \"between 0 and 1 inclusive.  Error is in %s\"",
            "                    % (field, value, config_description),",
            "                    field,",
            "                    \"notPercentage\",",
            "                )",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The required field \"%s\" has a value that cannot be parsed as a number between 0 '",
            "                \"and 1 inclusive.  Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notNumber\",",
            "            )",
            "",
            "    def __get_config(self):",
            "        if self.__last_error is not None:",
            "            raise BadConfiguration(self.__last_error, \"fake\", \"fake\")",
            "        return self.__config",
            "",
            "    def __perform_substitutions(self, source_config):",
            "        \"\"\"Rewrites the content of the source_config to reflect the values in the `import_vars` array.",
            "",
            "        @param source_config:  The configuration to rewrite, represented as key/value pairs.",
            "        @type source_config: JsonObject",
            "        \"\"\"",
            "        substitutions = import_shell_variables(source_config=source_config)",
            "        if len(substitutions) > 0:",
            "            perform_object_substitution(",
            "                object_value=source_config, substitutions=substitutions",
            "            )",
            "",
            "",
            "\"\"\"",
            "Utility functions related to shell variable handling and substition.",
            "",
            "NOTE: Those functions are intentionally not defined inside \"__perform_substitutions\" to avoid memory",
            "leaks.",
            "\"\"\"",
            "",
            "",
            "def import_shell_variables(source_config):",
            "    \"\"\"Creates a dict mapping variables listed in the `import_vars` field of `source_config` to their",
            "    values from the environment.",
            "    \"\"\"",
            "    result = dict()",
            "    if \"import_vars\" in source_config:",
            "        for entry in source_config.get_json_array(\"import_vars\"):",
            "            # Allow for an entry of the form { var: \"foo\", default: \"bar\"}",
            "            if isinstance(entry, JsonObject):",
            "                var_name = entry[\"var\"]",
            "                default_value = entry[\"default\"]",
            "            else:",
            "                var_name = entry",
            "                default_value = \"\"",
            "",
            "            # 2->TODO in python2 os.environ returns 'str' type. Convert it to unicode.",
            "            var_value = os_environ_unicode.get(var_name)",
            "            if var_value:",
            "                result[var_name] = var_value",
            "            else:",
            "                result[var_name] = default_value",
            "    return result",
            "",
            "",
            "def perform_generic_substitution(value, substitutions):",
            "    \"\"\"Takes a given JSON value and performs the appropriate substitution.",
            "",
            "    This method will return a non-None value if the value has to be replaced with the returned value.",
            "    Otherwise, this will attempt to perform in-place substitutions.",
            "",
            "    For str, unicode, it substitutes the variables and returns the result.  For",
            "    container objects, it does the recursive substitution.",
            "",
            "    @param value: The JSON value",
            "    @type value: Any valid element of a JsonObject",
            "    @return: The value that should replace the original, if any.  If no replacement is necessary, returns None",
            "    \"\"\"",
            "    result = None",
            "    value_type = type(value)",
            "",
            "    if value_type is six.text_type and \"$\" in value:",
            "        result = perform_str_substitution(value, substitutions=substitutions)",
            "    elif isinstance(value, JsonObject):",
            "        perform_object_substitution(value, substitutions=substitutions)",
            "    elif isinstance(value, JsonArray):",
            "        perform_array_substitution(value, substitutions=substitutions)",
            "    return result",
            "",
            "",
            "def perform_object_substitution(object_value, substitutions):",
            "    \"\"\"Performs the in-place substitution for a JsonObject.",
            "",
            "    @param object_value: The object to perform substitutions on.",
            "    @type object_value: JsonObject",
            "    \"\"\"",
            "    # We collect the new values and apply them later to avoid messing up the iteration.",
            "    new_values = {}",
            "    for (key, value) in six.iteritems(object_value):",
            "        replace_value = perform_generic_substitution(value, substitutions=substitutions)",
            "        if replace_value is not None:",
            "            new_values[key] = replace_value",
            "",
            "    for (key, value) in six.iteritems(new_values):",
            "        object_value[key] = value",
            "",
            "",
            "def perform_str_substitution(str_value, substitutions):",
            "    \"\"\"Performs substitutions on the given string.",
            "",
            "    @param str_value: The input string.",
            "    @type str_value: str or unicode",
            "    @return: The resulting value after substitution.",
            "    @rtype: str or unicode",
            "    \"\"\"",
            "    result = str_value",
            "    for (var_name, value) in six.iteritems(substitutions):",
            "        result = result.replace(\"$%s\" % var_name, value)",
            "    return result",
            "",
            "",
            "def perform_array_substitution(array_value, substitutions):",
            "    \"\"\"Perform substitutions on the JsonArray.",
            "",
            "    @param array_value: The array",
            "    @type array_value: JsonArray",
            "    \"\"\"",
            "    for i in range(len(array_value)):",
            "        replace_value = perform_generic_substitution(",
            "            array_value[i], substitutions=substitutions",
            "        )",
            "        if replace_value is not None:",
            "            array_value[i] = replace_value"
        ],
        "afterPatchFile": [
            "# Copyright 2014 Scalyr Inc.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ------------------------------------------------------------------------",
            "#",
            "#",
            "# author: Steven Czerwinski <czerwin@scalyr.com>",
            "",
            "from __future__ import unicode_literals",
            "from __future__ import absolute_import",
            "",
            "__author__ = \"czerwin@scalyr.com\"",
            "",
            "import os",
            "import re",
            "import socket",
            "import time",
            "import logging",
            "",
            "import six",
            "import six.moves.urllib.parse",
            "from six.moves import range",
            "",
            "import scalyr_agent.util as scalyr_util",
            "",
            "from scalyr_agent.json_lib import JsonConversionException, JsonMissingFieldException",
            "from scalyr_agent.json_lib.objects import (",
            "    JsonObject,",
            "    JsonArray,",
            "    ArrayOfStrings,",
            "    SpaceAndCommaSeparatedArrayOfStrings,",
            ")",
            "from scalyr_agent.monitor_utils.blocking_rate_limiter import BlockingRateLimiter",
            "from scalyr_agent.util import JsonReadFileException",
            "from scalyr_agent.config_util import BadConfiguration, get_config_from_env",
            "",
            "from scalyr_agent.__scalyr__ import get_install_root",
            "from scalyr_agent.compat import os_environ_unicode",
            "from scalyr_agent import compat",
            "",
            "",
            "class Configuration(object):",
            "    \"\"\"Encapsulates the results of a single read of the configuration file.",
            "",
            "    An instance of this object can be used to read and validate the configuration file.  It supports",
            "    reading the main contents of the configuration, as well as configuration fragments in the 'configs.d'",
            "    directory.  It handles merging the contents of the configuration files together and filling in any default",
            "    values for fields not set.  You can also use the equivalent method to determine if two instances of this",
            "    object have the same configuration content.",
            "",
            "    You may also use environment variable substitution in any string value in the configuration file.  You just",
            "    need to define the ``import_vars`` configuration field to be a list of variable names to import from the",
            "    shell and then use the $VAR_NAME in any string field.  Each entry in ``import_vars`` may also be a dict with",
            "    two entries: ``var`` for the name of the environment variable and ``default`` for the value to use if the",
            "    the environment variable is not set or is empty.",
            "",
            "    This also handles reporting status information about the configuration state, including what time it was",
            "    read and what error (if any) was raised.",
            "",
            "    Note:",
            "    UNDOCUMENTED_CONFIG: These are undocumented config params, meaning they are not described in the public online docs",
            "        in order to simplify the mental model.  In rare cases, a customer may need to tune these params under direct",
            "        guidance from support.",
            "    \"\"\"",
            "",
            "    DEFAULT_K8S_IGNORE_NAMESPACES = [\"kube-system\"]",
            "    DEFAULT_K8S_INCLUDE_NAMESPACES = [\"*\"]",
            "",
            "    def __init__(self, file_path, default_paths, logger, extra_config_dir=None):",
            "        # Captures all environment aware variables for testing purposes",
            "        self._environment_aware_map = {}",
            "        self.__file_path = os.path.abspath(file_path)",
            "        # Paths for additional configuration files that were read (from the config directory).",
            "        self.__additional_paths = []",
            "        # The JsonObject holding the contents of the configuration file, along with any default",
            "        # values filled in.",
            "        self.__config = None",
            "        # The number of seconds past epoch when the file was read.",
            "        self.__read_time = None",
            "        # The exception, if any, that was raised when the state was read.  This will be a BadConfiguration exception.",
            "        self.__last_error = None",
            "        # The log configuration objects from the configuration file.  This does not include logs required by",
            "        # the monitors.",
            "        self.__log_configs = []",
            "        # Optional configuration for journald monitor logging",
            "        self.__journald_log_configs = []",
            "        # Optional configuration for k8s monitor logging",
            "        self.__k8s_log_configs = []",
            "        # The monitor configuration objects from the configuration file.  This does not include monitors that",
            "        # are created by default by the platform.",
            "        self.__monitor_configs = []",
            "",
            "        # The DefaultPaths object that specifies the default paths for things like the data and log directory",
            "        # based on platform.",
            "        self.__default_paths = default_paths",
            "",
            "        # FIX THESE:",
            "        # Add documentation, verify, etc.",
            "        self.max_retry_time = 15 * 60",
            "        self.max_allowed_checkpoint_age = 15 * 60",
            "",
            "        # An additional directory to look for config snippets",
            "        self.__extra_config_directory = extra_config_dir",
            "",
            "        self.__logger = logger",
            "",
            "    def parse(self):",
            "        self.__read_time = time.time()",
            "",
            "        try:",
            "            try:",
            "                # First read the file.  This makes sure it exists and can be parsed.",
            "                self.__config = scalyr_util.read_config_file_as_json(self.__file_path)",
            "",
            "                # What implicit entries do we need to add?  metric monitor, agent.log, and then logs from all monitors.",
            "            except JsonReadFileException as e:",
            "                raise BadConfiguration(six.text_type(e), None, \"fileParseError\")",
            "",
            "            # Import any requested variables from the shell and use them for substitutions.",
            "            self.__perform_substitutions(self.__config)",
            "",
            "            # get initial list of already seen config keys (we need to do this before",
            "            # defaults have been applied)",
            "            already_seen = {}",
            "            for k in self.__config.keys():",
            "                already_seen[k] = self.__file_path",
            "",
            "            self.__verify_main_config_and_apply_defaults(",
            "                self.__config, self.__file_path",
            "            )",
            "            api_key, api_config_file = self.__check_field(",
            "                \"api_key\", self.__config, self.__file_path",
            "            )",
            "            scalyr_server, scalyr_server_config_file = self.__check_field(",
            "                \"scalyr_server\", self.__config, self.__file_path",
            "            )",
            "            self.__verify_logs_and_monitors_configs_and_apply_defaults(",
            "                self.__config, self.__file_path",
            "            )",
            "",
            "            # these variables are allowed to appear in multiple files",
            "            allowed_multiple_keys = (",
            "                \"import_vars\",",
            "                \"logs\",",
            "                \"journald_logs\",",
            "                \"k8s_logs\",",
            "                \"monitors\",",
            "                \"server_attributes\",",
            "            )",
            "",
            "            # Get any configuration snippets in the config directory",
            "            extra_config = self.__list_files(self.config_directory)",
            "",
            "            # Plus any configuration snippets in the additional config directory",
            "            extra_config.extend(self.__list_files(self.extra_config_directory))",
            "",
            "            # Now, look for any additional configuration in the config fragment directory.",
            "            for fp in extra_config:",
            "                self.__additional_paths.append(fp)",
            "                content = scalyr_util.read_config_file_as_json(fp)",
            "                for k in content.keys():",
            "                    if k not in allowed_multiple_keys:",
            "                        if k in already_seen:",
            "                            self.__last_error = BadConfiguration(",
            "                                'Configuration fragment file \"%s\" redefines the config key \"%s\", first seen in \"%s\". '",
            "                                \"The only config items that can be defined in multiple config files are: %s.\"",
            "                                % (fp, k, already_seen[k], allowed_multiple_keys),",
            "                                k,",
            "                                \"multipleKeys\",",
            "                            )",
            "                            raise self.__last_error",
            "                        else:",
            "                            already_seen[k] = fp",
            "",
            "                self.__perform_substitutions(content)",
            "                self.__verify_main_config(content, self.__file_path)",
            "                self.__verify_logs_and_monitors_configs_and_apply_defaults(content, fp)",
            "",
            "                for (key, value) in six.iteritems(content):",
            "                    if key not in allowed_multiple_keys:",
            "                        self.__config.put(key, value)",
            "",
            "                self.__add_elements_from_array(\"logs\", content, self.__config)",
            "                self.__add_elements_from_array(\"journald_logs\", content, self.__config)",
            "                self.__add_elements_from_array(\"k8s_logs\", content, self.__config)",
            "                self.__add_elements_from_array(\"monitors\", content, self.__config)",
            "                self.__merge_server_attributes(fp, content, self.__config)",
            "",
            "            self.__set_api_key(self.__config, api_key)",
            "            if scalyr_server is not None:",
            "                self.__config.put(\"scalyr_server\", scalyr_server)",
            "            self.__verify_or_set_optional_string(",
            "                self.__config,",
            "                \"scalyr_server\",",
            "                \"https://agent.scalyr.com\",",
            "                \"configuration file %s\" % self.__file_path,",
            "                env_name=\"SCALYR_SERVER\",",
            "            )",
            "",
            "            self.__config[\"raw_scalyr_server\"] = self.__config[\"scalyr_server\"]",
            "",
            "            # force https unless otherwise instructed not to",
            "            if not self.__config[\"allow_http\"]:",
            "                server = self.__config[\"scalyr_server\"].strip()",
            "                https_server = server",
            "",
            "                parts = six.moves.urllib.parse.urlparse(server)",
            "",
            "                # use index-based addressing for 2.4 compatibility",
            "                scheme = parts[0]",
            "",
            "                if not scheme:",
            "                    https_server = \"https://\" + server",
            "                elif scheme == \"http\":",
            "                    https_server = re.sub(\"^http://\", \"https://\", server)",
            "",
            "                if https_server != server:",
            "                    self.__config[\"scalyr_server\"] = https_server",
            "",
            "            # Set defaults based on `max_send_rate_enforcement` value",
            "            if (",
            "                not self.__config[\"disable_max_send_rate_enforcement_overrides\"]",
            "                and not self.__config[\"max_send_rate_enforcement\"] == \"legacy\"",
            "            ):",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"max_allowed_request_size\", 1024 * 1024",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"pipeline_threshold\", 1.1",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"min_request_spacing_interval\", 1.0",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"max_request_spacing_interval\", 5.0",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"max_log_offset_size\", 5 * 1024 * 1024",
            "                )",
            "                self._warn_of_override_due_to_rate_enforcement(",
            "                    \"max_existing_log_offset_size\", 100 * 1024 * 1024",
            "                )",
            "",
            "                self.__config[\"max_allowed_request_size\"] = 5900000",
            "                self.__config[\"pipeline_threshold\"] = 0",
            "                self.__config[\"min_request_spacing_interval\"] = 0.0",
            "                self.__config[\"max_request_spacing_interval\"] = 5.0",
            "                self.__config[\"max_log_offset_size\"] = 200000000",
            "                self.__config[\"max_existing_log_offset_size\"] = 200000000",
            "",
            "            # Parse `max_send_rate_enforcement`",
            "            if (",
            "                self.__config[\"max_send_rate_enforcement\"] != \"unlimited\"",
            "                and self.__config[\"max_send_rate_enforcement\"] != \"legacy\"",
            "            ):",
            "                try:",
            "                    self.__config[",
            "                        \"parsed_max_send_rate_enforcement\"",
            "                    ] = scalyr_util.parse_data_rate_string(",
            "                        self.__config[\"max_send_rate_enforcement\"]",
            "                    )",
            "                except ValueError as e:",
            "                    raise BadConfiguration(",
            "                        six.text_type(e), \"max_send_rate_enforcement\", \"notDataRate\"",
            "                    )",
            "",
            "            # Add in 'serverHost' to server_attributes if it is not set.  We must do this after merging any",
            "            # server attributes from the config fragments.",
            "            if \"serverHost\" not in self.server_attributes:",
            "                self.__config[\"server_attributes\"][",
            "                    \"serverHost\"",
            "                ] = self.__get_default_hostname()",
            "",
            "            # Add in implicit entry to collect the log generated by this agent.",
            "            agent_log = None",
            "            if self.implicit_agent_log_collection:",
            "                config = JsonObject(path=\"agent.log\", parser=\"scalyrAgentLog\")",
            "                self.__verify_log_entry_and_set_defaults(",
            "                    config, description=\"implicit rule\"",
            "                )",
            "                agent_log = config",
            "",
            "            self.__log_configs = list(self.__config.get_json_array(\"logs\"))",
            "            if agent_log is not None:",
            "                self.__log_configs.append(agent_log)",
            "",
            "            self.__journald_log_configs = list(",
            "                self.__config.get_json_array(\"journald_logs\")",
            "            )",
            "",
            "            self.__k8s_log_configs = list(self.__config.get_json_array(\"k8s_logs\"))",
            "",
            "            # add in the profile log if we have enabled profiling",
            "            if self.enable_profiling:",
            "                profile_config = JsonObject(",
            "                    path=self.profile_log_name,",
            "                    copy_from_start=True,",
            "                    staleness_threshold_secs=20 * 60,",
            "                    parser=\"scalyrAgentProfiling\",",
            "                )",
            "                self.__verify_log_entry_and_set_defaults(",
            "                    profile_config, description=\"CPU profile log config\"",
            "                )",
            "                self.__log_configs.append(profile_config)",
            "",
            "            self.__monitor_configs = list(self.__config.get_json_array(\"monitors\"))",
            "        except BadConfiguration as e:",
            "            self.__last_error = e",
            "            raise e",
            "",
            "    def _warn_of_override_due_to_rate_enforcement(self, config_option, default):",
            "        if self.__config[config_option] != default:",
            "            self.__logger.warn(",
            "                \"Configured option %s is being overridden due to max_send_rate_enforcement setting.\"",
            "                % config_option,",
            "                limit_once_per_x_secs=86400,",
            "                limit_key=\"max_send_rate_enforcement_override\",",
            "            )",
            "",
            "    def apply_config(self):",
            "        \"\"\"",
            "        Apply global configuration object based on the configuration values.",
            "",
            "        At this point this only applies to the JSON library which is used.",
            "        \"\"\"",
            "        if not self.__config:",
            "            # parse() hasn't been called yet. We should probably throw here",
            "            return",
            "",
            "        # Set json library based on the config value. If \"auto\" is provided this means we use",
            "        # default behavior which is try to use ujson and if that's not available fall back to",
            "        # stdlib json",
            "        json_library = self.json_library",
            "        current_json_library = scalyr_util.get_json_lib()",
            "",
            "        if json_library != \"auto\" and json_library != current_json_library:",
            "            self.__logger.debug(",
            "                'Changing JSON library from \"%s\" to \"%s\"'",
            "                % (current_json_library, json_library)",
            "            )",
            "            scalyr_util.set_json_lib(json_library)",
            "",
            "    def print_useful_settings(self, other_config=None):",
            "        \"\"\"",
            "        Prints various useful configuration settings to the agent log, so we have a record",
            "        in the log of the settings that are currently in use.",
            "",
            "        @param other_config: Another configuration option.  If not None, this function will",
            "        only print configuration options that are different between the two objects.",
            "        \"\"\"",
            "",
            "        options = [",
            "            \"verify_server_certificate\",",
            "            \"ca_cert_path\",",
            "            \"compression_type\",",
            "            \"compression_level\",",
            "            \"pipeline_threshold\",",
            "            \"max_send_rate_enforcement\",",
            "            \"disable_max_send_rate_enforcement_overrides\",",
            "            \"min_allowed_request_size\",",
            "            \"max_allowed_request_size\",",
            "            \"min_request_spacing_interval\",",
            "            \"max_request_spacing_interval\",",
            "            \"read_page_size\",",
            "            \"max_line_size\",",
            "            \"internal_parse_max_line_size\",",
            "            \"line_completion_wait_time\",",
            "            \"max_log_offset_size\",",
            "            \"max_existing_log_offset_size\",",
            "        ]",
            "",
            "        # get options (if any) from the other configuration object",
            "        other_options = None",
            "        if other_config is not None:",
            "            other_options = {}",
            "            for option in options:",
            "                other_options[option] = getattr(other_config, option, None)",
            "",
            "        first = True",
            "        for option in options:",
            "            value = getattr(self, option, None)",
            "            print_value = False",
            "",
            "            # check to see if we should be printing this option which will will",
            "            # be True if other_config is None or if the other_config had a setting",
            "            # that was different from our current setting",
            "            if other_config is None:",
            "                print_value = True",
            "            elif (",
            "                other_options is not None",
            "                and option in other_options",
            "                and other_options[option] != value",
            "            ):",
            "                print_value = True",
            "",
            "            if print_value:",
            "                # if this is the first option we are printing, output a header",
            "                if first:",
            "                    self.__logger.info(\"Configuration settings\")",
            "                    first = False",
            "",
            "                self.__logger.info(\"\\t%s: %s\" % (option, value))",
            "",
            "    def __get_default_hostname(self):",
            "        \"\"\"Returns the default hostname for this host.",
            "        @return: The default hostname for this host.",
            "        @rtype: str",
            "        \"\"\"",
            "        result = six.ensure_text(socket.gethostname())",
            "        if result is not None and self.strip_domain_from_default_server_host:",
            "            result = result.split(\".\")[0]",
            "        return result",
            "",
            "    def parse_log_config(",
            "        self,",
            "        log_config,",
            "        default_parser=None,",
            "        context_description=\"uncategorized log entry\",",
            "    ):",
            "        \"\"\"Parses a given configuration stanza for a log file and returns the complete config with all the",
            "        default values filled in.",
            "",
            "        This is useful for parsing a log configuration entry that was not originally included in the configuration",
            "        but whose contents still must be verified.  For example, a log configuration entry from a monitor.",
            "",
            "        @param log_config: The configuration entry for the log.",
            "        @param default_parser: If the configuration entry does not have a ``parser`` entry, set this as the default.",
            "        @param context_description: The context of where this entry came from, used when creating an error message",
            "            for the user.",
            "",
            "        @type log_config: dict|JsonObject",
            "        @type default_parser: str",
            "        @type context_description: str",
            "",
            "        @return: The full configuration for the log.",
            "        @rtype: JsonObject",
            "        \"\"\"",
            "        if type(log_config) is dict:",
            "            log_config = JsonObject(content=log_config)",
            "",
            "        log_config = log_config.copy()",
            "",
            "        if default_parser is not None:",
            "            self.__verify_or_set_optional_string(",
            "                log_config, \"parser\", default_parser, context_description",
            "            )",
            "        self.__verify_log_entry_and_set_defaults(",
            "            log_config, description=context_description",
            "        )",
            "",
            "        return log_config",
            "",
            "    def parse_monitor_config(",
            "        self, monitor_config, context_description=\"uncategorized monitor entry\"",
            "    ):",
            "        \"\"\"Parses a given monitor configuration entry and returns the complete config with all default values",
            "        filled in.",
            "",
            "        This is useful for parsing a monitor configuration entry that was not originally included in the",
            "        configuration but whose contents still must be verified.  For example, a default monitor supplied by the",
            "        platform.",
            "",
            "        @param monitor_config: The configuration entry for the monitor.",
            "        @param context_description: The context of where this entry came from, used when creating an error message",
            "            for the user.",
            "",
            "        @type monitor_config: dict|JsonObject",
            "        @type context_description: str",
            "",
            "        @return: The full configuration for the monitor.",
            "        @rtype: JsonObject",
            "        \"\"\"",
            "        if type(monitor_config) is dict:",
            "            monitor_config = JsonObject(content=monitor_config)",
            "",
            "        monitor_config = monitor_config.copy()",
            "",
            "        self.__verify_monitor_entry_and_set_defaults(",
            "            monitor_config, context_description=context_description",
            "        )",
            "",
            "        return monitor_config",
            "",
            "    # k8s cache options",
            "    @property",
            "    def k8s_ignore_namespaces(self):",
            "        return self.__get_config().get_json_array(\"k8s_ignore_namespaces\")",
            "",
            "    @property",
            "    def k8s_include_namespaces(self):",
            "        return self.__get_config().get_json_array(\"k8s_include_namespaces\")",
            "",
            "    @property",
            "    def k8s_api_url(self):",
            "        return self.__get_config().get_string(\"k8s_api_url\")",
            "",
            "    @property",
            "    def k8s_verify_api_queries(self):",
            "        return self.__get_config().get_bool(\"k8s_verify_api_queries\")",
            "",
            "    @property",
            "    def k8s_verify_kubelet_queries(self):",
            "        return self.__get_config().get_bool(\"k8s_verify_kubelet_queries\")",
            "",
            "    @property",
            "    def k8s_kubelet_ca_cert(self):",
            "        return self.__get_config().get_string(\"k8s_kubelet_ca_cert\")",
            "",
            "    @property",
            "    def k8s_cache_query_timeout_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_query_timeout_secs\")",
            "",
            "    @property",
            "    def k8s_cache_expiry_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_expiry_secs\")",
            "",
            "    @property",
            "    def k8s_cache_expiry_fuzz_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_expiry_fuzz_secs\")",
            "",
            "    @property",
            "    def k8s_cache_start_fuzz_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_start_fuzz_secs\")",
            "",
            "    @property",
            "    def k8s_cache_purge_secs(self):",
            "        return self.__get_config().get_int(\"k8s_cache_purge_secs\")",
            "",
            "    @property",
            "    def k8s_service_account_cert(self):",
            "        return self.__get_config().get_string(\"k8s_service_account_cert\")",
            "",
            "    @property",
            "    def k8s_service_account_token(self):",
            "        return self.__get_config().get_string(\"k8s_service_account_token\")",
            "",
            "    @property",
            "    def k8s_service_account_namespace(self):",
            "        return self.__get_config().get_string(\"k8s_service_account_namespace\")",
            "",
            "    @property",
            "    def k8s_log_api_responses(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_bool(\"k8s_log_api_responses\")",
            "",
            "    @property",
            "    def k8s_log_api_exclude_200s(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_bool(\"k8s_log_api_exclude_200s\")",
            "",
            "    @property",
            "    def k8s_log_api_min_response_len(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_log_api_min_response_len\")",
            "",
            "    @property",
            "    def k8s_log_api_min_latency(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_log_api_min_latency\")",
            "",
            "    @property",
            "    def k8s_log_api_ratelimit_interval(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_log_api_ratelimit_interval\")",
            "",
            "    @property",
            "    def k8s_controlled_warmer_max_attempts(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_controlled_warmer_max_attempts\")",
            "",
            "    @property",
            "    def k8s_controlled_warmer_max_query_retries(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_controlled_warmer_max_query_retries\")",
            "",
            "    @property",
            "    def k8s_controlled_warmer_blacklist_time(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_controlled_warmer_blacklist_time\")",
            "",
            "    @property",
            "    def k8s_events_disable(self):",
            "        return self.__get_config().get_bool(\"k8s_events_disable\")",
            "",
            "    @property",
            "    def k8s_ratelimit_cluster_num_agents(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_ratelimit_cluster_num_agents\")",
            "",
            "    @property",
            "    def k8s_ratelimit_cluster_rps_init(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_cluster_rps_init\")",
            "",
            "    @property",
            "    def k8s_ratelimit_cluster_rps_min(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_cluster_rps_min\")",
            "",
            "    @property",
            "    def k8s_ratelimit_cluster_rps_max(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_cluster_rps_max\")",
            "",
            "    @property",
            "    def k8s_ratelimit_consecutive_increase_threshold(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(",
            "            \"k8s_ratelimit_consecutive_increase_threshold\"",
            "        )",
            "",
            "    @property",
            "    def k8s_ratelimit_strategy(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_string(\"k8s_ratelimit_strategy\")",
            "",
            "    @property",
            "    def k8s_ratelimit_increase_factor(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_increase_factor\")",
            "",
            "    @property",
            "    def k8s_ratelimit_backoff_factor(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_float(\"k8s_ratelimit_backoff_factor\")",
            "",
            "    @property",
            "    def k8s_ratelimit_max_concurrency(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_int(\"k8s_ratelimit_max_concurrency\")",
            "",
            "    @property",
            "    def enforce_monotonic_timestamps(self):",
            "        # UNDOCUMENTED_CONFIG",
            "        return self.__get_config().get_bool(\"enforce_monotonic_timestamps\")",
            "",
            "    @property",
            "    def include_raw_timestamp_field(self):",
            "        \"\"\"If True, adds an attribute called `raw_timestamp` to all events parsed using the",
            "        parse_as_json or parse_as_cri features.  This field will be set to the timestamp included in the JSON or",
            "        CRI line.  When parsing Docker or K8s logs, this represents the timestamp of the log",
            "        message as recorded by those systems.\"\"\"",
            "        return self.__get_config().get_bool(\"include_raw_timestamp_field\")",
            "",
            "    @property",
            "    def enable_profiling(self):",
            "        return self.__get_config().get_bool(\"enable_profiling\")",
            "",
            "    @property",
            "    def max_profile_interval_minutes(self):",
            "        return self.__get_config().get_int(\"max_profile_interval_minutes\")",
            "",
            "    @property",
            "    def profile_duration_minutes(self):",
            "        return self.__get_config().get_int(\"profile_duration_minutes\")",
            "",
            "    @property",
            "    def profile_clock(self):",
            "        return self.__get_config().get_string(\"profile_clock\")",
            "",
            "    @property",
            "    def profile_log_name(self):",
            "        return self.__get_config().get_string(\"profile_log_name\")",
            "",
            "    @property",
            "    def memory_profile_log_name(self):",
            "        return self.__get_config().get_string(\"memory_profile_log_name\")",
            "",
            "    @property",
            "    def disable_logfile_addevents_format(self):",
            "        return self.__get_config().get_bool(\"disable_logfile_addevents_format\")",
            "",
            "    @property",
            "    def json_library(self):",
            "        return self.__get_config().get_string(\"json_library\")",
            "",
            "    # Debug leak flags",
            "    @property",
            "    def disable_send_requests(self):",
            "        return self.__get_config().get_bool(\"disable_send_requests\")",
            "",
            "    # Debug leak flags",
            "    @property",
            "    def disable_monitor_threads(self):",
            "        return self.__get_config().get_bool(\"disable_leak_monitor_threads\")",
            "",
            "    @property",
            "    def disable_monitors_creation(self):",
            "        return self.__get_config().get_bool(\"disable_leak_monitors_creation\")",
            "",
            "    @property",
            "    def disable_new_file_matches(self):",
            "        return self.__get_config().get_bool(\"disable_leak_new_file_matches\")",
            "",
            "    @property",
            "    def disable_scan_for_new_bytes(self):",
            "        return self.__get_config().get_bool(\"disable_leak_scan_for_new_bytes\")",
            "",
            "    @property",
            "    def disable_processing_new_bytes(self):",
            "        return self.__get_config().get_bool(\"disable_leak_processing_new_bytes\")",
            "",
            "    @property",
            "    def disable_copying_thread(self):",
            "        return self.__get_config().get_bool(\"disable_leak_copying_thread\")",
            "",
            "    @property",
            "    def disable_overall_stats(self):",
            "        return self.__get_config().get_bool(\"disable_leak_overall_stats\")",
            "",
            "    @property",
            "    def disable_bandwidth_stats(self):",
            "        return self.__get_config().get_bool(\"disable_leak_bandwidth_stats\")",
            "",
            "    @property",
            "    def disable_copy_manager_stats(self):",
            "        return self.__get_config().get_bool(\"disable_copy_manager_stats\")",
            "",
            "    @property",
            "    def disable_update_debug_log_level(self):",
            "        return self.__get_config().get_bool(\"disable_leak_update_debug_log_level\")",
            "",
            "    @property",
            "    def enable_gc_stats(self):",
            "        return self.__get_config().get_bool(\"enable_gc_stats\")",
            "",
            "    @property",
            "    def disable_all_config_updates(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_all_config_updates\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_verify_config(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_verify_config\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_config_equivalence_check(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_config_equivalence_check\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_verify_can_write_to_logs(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_verify_can_write_to_logs\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_config_reload(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_config_reload\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def config_change_check_interval(self):",
            "        return self.__get_config().get_float(\"config_change_check_interval\")",
            "",
            "    @property",
            "    def overall_stats_log_interval(self):",
            "        return self.__get_config().get_float(\"overall_stats_log_interval\")",
            "",
            "    @property",
            "    def copying_manager_stats_log_interval(self):",
            "        return self.__get_config().get_float(\"copying_manager_stats_log_interval\")",
            "",
            "    @property",
            "    def bandwidth_stats_log_interval(self):",
            "        return self.__get_config().get_float(\"bandwidth_stats_log_interval\")",
            "",
            "    @property",
            "    def user_agent_refresh_interval(self):",
            "        return self.__get_config().get_float(\"user_agent_refresh_interval\")",
            "",
            "    @property",
            "    def garbage_collect_interval(self):",
            "        return self.__get_config().get_int(\"garbage_collect_interval\")",
            "",
            "    @property",
            "    def disable_verify_config_create_monitors_manager(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_verify_config_create_monitors_manager\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_verify_config_create_copying_manager(self):",
            "        return self.__get_config().get_int(",
            "            \"disable_leak_verify_config_create_copying_manager\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def disable_verify_config_cache_config(self):",
            "        return self.__get_config().get_bool(\"disable_leak_verify_config_cache_config\")",
            "",
            "    # end Debug leak flags",
            "",
            "    @property",
            "    def read_time(self):",
            "        \"\"\"Returns the time this configuration file was read.\"\"\"",
            "        return self.__read_time",
            "",
            "    @property",
            "    def file_path(self):",
            "        \"\"\"Returns the time this path of the file that was read for the configuration.\"\"\"",
            "        return self.__file_path",
            "",
            "    @property",
            "    def additional_file_paths(self):",
            "        \"\"\"Returns a list of the paths for the additional files from the configuration directory that were read.\"\"\"",
            "        return self.__additional_paths",
            "",
            "    @property",
            "    def last_error(self):",
            "        \"\"\"Returns the error seen (if any) while processing the configuration.\"\"\"",
            "        return self.__last_error",
            "",
            "    @property",
            "    def log_configs(self):",
            "        \"\"\"Returns the list of configuration entries for all the logs specified in the configuration file.",
            "",
            "        Note, this does not include logs required by monitors.  It is only logs explicitly listed in the configuration",
            "        file and possible the agent log as well.",
            "",
            "        @rtype list<JsonObject>\"\"\"",
            "        return self.__log_configs",
            "",
            "    @property",
            "    def journald_log_configs(self):",
            "        \"\"\"Returns the list of configuration entries for all the journald loggers specified in the configuration file.",
            "",
            "        @rtype list<JsonObject>\"\"\"",
            "        return self.__journald_log_configs",
            "",
            "    @property",
            "    def k8s_log_configs(self):",
            "        \"\"\"Returns the list of configuration entries for all the k8s loggers specified in the configuration file.",
            "",
            "        @rtype list<JsonObject>\"\"\"",
            "        return self.__k8s_log_configs",
            "",
            "    @property",
            "    def monitor_configs(self):",
            "        \"\"\"Returns the list of configuration entries for all monitores specified in the configuration file.",
            "",
            "        Note, this does not include default monitors for the platform.  It is only monitors explicitly listed in the",
            "        configuration file.",
            "",
            "        @rtype list<JsonObject>\"\"\"",
            "        return self.__monitor_configs",
            "",
            "    @property",
            "    def agent_data_path(self):",
            "        \"\"\"Returns the configuration value for 'agent_data_path'.\"\"\"",
            "        return self.__get_config().get_string(\"agent_data_path\")",
            "",
            "    @property",
            "    def agent_log_path(self):",
            "        \"\"\"Returns the configuration value for 'agent_log_path'.\"\"\"",
            "        return self.__get_config().get_string(\"agent_log_path\")",
            "",
            "    @property",
            "    def additional_monitor_module_paths(self):",
            "        \"\"\"Returns the configuration value for 'additional_monitor_module_paths'.\"\"\"",
            "        return self.__get_config().get_string(\"additional_monitor_module_paths\")",
            "",
            "    @property",
            "    def api_key(self):",
            "        \"\"\"Returns the configuration value for 'api_key'.\"\"\"",
            "        return self.__get_config().get_string(\"api_key\")",
            "",
            "    @property",
            "    def scalyr_server(self):",
            "        \"\"\"Returns the configuration value for 'scalyr_server'.\"\"\"",
            "        return self.__get_config().get_string(\"scalyr_server\")",
            "",
            "    @property",
            "    def raw_scalyr_server(self):",
            "        \"\"\"Returns the configuration value for 'raw_scalyr_server'.\"\"\"",
            "        return self.__get_config().get_string(\"raw_scalyr_server\")",
            "",
            "    @property",
            "    def check_remote_if_no_tty(self):",
            "        \"\"\"Returns the configuration value for `check_remote_if_no_tty`\"\"\"",
            "        return self.__get_config().get_bool(\"check_remote_if_no_tty\")",
            "",
            "    @property",
            "    def server_attributes(self):",
            "        \"\"\"Returns the configuration value for 'server_attributes'.\"\"\"",
            "        return self.__get_config().get_json_object(\"server_attributes\")",
            "",
            "    @property",
            "    def implicit_agent_log_collection(self):",
            "        \"\"\"Returns the configuration value for 'implicit_agent_log_collection'.\"\"\"",
            "        return self.__get_config().get_bool(\"implicit_agent_log_collection\")",
            "",
            "    @property",
            "    def implicit_metric_monitor(self):",
            "        \"\"\"Returns the configuration value for 'implicit_metric_monitor'.\"\"\"",
            "        return self.__get_config().get_bool(\"implicit_metric_monitor\")",
            "",
            "    @property",
            "    def implicit_agent_process_metrics_monitor(self):",
            "        \"\"\"Returns the configuration value for 'implicit_agent_process_metrics_monitor'.\"\"\"",
            "        return self.__get_config().get_bool(\"implicit_agent_process_metrics_monitor\")",
            "",
            "    @property",
            "    def use_unsafe_debugging(self):",
            "        \"\"\"Returns the configuration value for 'unsafe_debugging'.",
            "",
            "        Note, this should be used with extreme care.  It allows arbitrary commands to be executed by any local",
            "        user on the system as the user running the agent.\"\"\"",
            "        return self.__get_config().get_bool(\"use_unsafe_debugging\")",
            "",
            "    @property",
            "    def copying_thread_profile_interval(self):",
            "        \"\"\"Returns the interval (in seconds) between outputs of the profiling for the copying thread.",
            "        This should be zero unless you are profiling the copying thread.",
            "        \"\"\"",
            "        return self.__get_config().get_int(\"copying_thread_profile_interval\")",
            "",
            "    @property",
            "    def copying_thread_profile_output_path(self):",
            "        \"\"\"Returns the path prefix for writing all profiling dumps for the copying thread, when",
            "        ``copying_thread_profile_interval`` is greater than zero.",
            "        @return:",
            "        @rtype:",
            "        \"\"\"",
            "        return self.__get_config().get_string(\"copying_thread_profile_output_path\")",
            "",
            "    @property",
            "    def config_directory(self):",
            "        \"\"\"Returns the configuration value for 'config_directory', resolved to full path if necessary.\"\"\"",
            "        config_directory = self.__get_config().get_string(\"config_directory\")",
            "",
            "        # The configuration directory's path is relative to the the directory this configuration",
            "        # file is stored in.",
            "        return self.__resolve_absolute_path(",
            "            config_directory, self.__get_parent_directory(self.__file_path)",
            "        )",
            "",
            "    @property",
            "    def config_directory_raw(self):",
            "        \"\"\"Returns the configuration value for 'config_directory', as recorded in the configuration file.\"\"\"",
            "        return self.__get_config().get_string(\"config_directory\")",
            "",
            "    @property",
            "    def parsed_max_send_rate_enforcement(self):",
            "        \"\"\"Returns the configuration value for 'max_send_rate_enforcement' in bytes per second if not `unlimited` or `legacy`.\"\"\"",
            "        return self.__get_config().get_float(",
            "            \"parsed_max_send_rate_enforcement\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def max_send_rate_enforcement(self):",
            "        \"\"\"Returns the raw value for 'max_send_rate_enforcement'.\"\"\"",
            "        return self.__get_config().get_string(\"max_send_rate_enforcement\")",
            "",
            "    @property",
            "    def disable_max_send_rate_enforcement_overrides(self):",
            "        \"\"\"Returns the configuration value for 'disable_max_send_rate_enforcement_overrides'.\"\"\"",
            "        return self.__get_config().get_bool(",
            "            \"disable_max_send_rate_enforcement_overrides\"",
            "        )",
            "",
            "    @property",
            "    def extra_config_directory(self):",
            "        \"\"\"Returns the configuration value for `extra_config_directory`, resolved to full path if",
            "        necessary.  \"\"\"",
            "",
            "        # If `extra_config_directory` is a relative path, then it will be relative",
            "        # to the directory containing the main config file",
            "        if self.__extra_config_directory is None:",
            "            return None",
            "",
            "        return self.__resolve_absolute_path(",
            "            self.__extra_config_directory,",
            "            self.__get_parent_directory(self.__file_path),",
            "        )",
            "",
            "    @property",
            "    def extra_config_directory_raw(self):",
            "        \"\"\"Returns the configuration value for 'extra_config_directory'.\"\"\"",
            "        return self.__extra_config_directory",
            "",
            "    @property",
            "    def max_allowed_request_size(self):",
            "        \"\"\"Returns the configuration value for 'max_allowed_request_size'.\"\"\"",
            "        return self.__get_config().get_int(\"max_allowed_request_size\")",
            "",
            "    @property",
            "    def min_allowed_request_size(self):",
            "        \"\"\"Returns the configuration value for 'min_allowed_request_size'.\"\"\"",
            "        return self.__get_config().get_int(\"min_allowed_request_size\")",
            "",
            "    @property",
            "    def min_request_spacing_interval(self):",
            "        \"\"\"Returns the configuration value for 'min_request_spacing_interval'.\"\"\"",
            "        return self.__get_config().get_float(\"min_request_spacing_interval\")",
            "",
            "    @property",
            "    def max_request_spacing_interval(self):",
            "        \"\"\"Returns the configuration value for 'max_request_spacing_interval'.\"\"\"",
            "        return self.__get_config().get_float(\"max_request_spacing_interval\")",
            "",
            "    @property",
            "    def max_error_request_spacing_interval(self):",
            "        \"\"\"Returns the configuration value for 'max_error_request_spacing_interval'.\"\"\"",
            "        return self.__get_config().get_float(\"max_error_request_spacing_interval\")",
            "",
            "    @property",
            "    def low_water_bytes_sent(self):",
            "        \"\"\"Returns the configuration value for 'low_water_bytes_sent'.\"\"\"",
            "        return self.__get_config().get_int(\"low_water_bytes_sent\")",
            "",
            "    @property",
            "    def low_water_request_spacing_adjustment(self):",
            "        \"\"\"Returns the configuration value for 'low_water_request_spacing_adjustment'.\"\"\"",
            "        return self.__get_config().get_float(\"low_water_request_spacing_adjustment\")",
            "",
            "    @property",
            "    def high_water_bytes_sent(self):",
            "        \"\"\"Returns the configuration value for 'high_water_bytes_sent'.\"\"\"",
            "        return self.__get_config().get_int(\"high_water_bytes_sent\")",
            "",
            "    @property",
            "    def high_water_request_spacing_adjustment(self):",
            "        \"\"\"Returns the configuration value for 'high_water_request_spacing_adjustment'.\"\"\"",
            "        return self.__get_config().get_float(\"high_water_request_spacing_adjustment\")",
            "",
            "    @property",
            "    def max_new_log_detection_time(self):",
            "        \"\"\"Returns the configuration value for 'max_new_log_detection_time'.\"\"\"",
            "        return self.__get_config().get_float(\"max_new_log_detection_time\")",
            "",
            "    @property",
            "    def failure_request_spacing_adjustment(self):",
            "        \"\"\"Returns the configuration value for 'failure_request_spacing_adjustment'.\"\"\"",
            "        return self.__get_config().get_float(\"failure_request_spacing_adjustment\")",
            "",
            "    @property",
            "    def request_too_large_adjustment(self):",
            "        \"\"\"Returns the configuration value for 'request_too_large_adjustment'.\"\"\"",
            "        return self.__get_config().get_float(\"request_too_large_adjustment\")",
            "",
            "    @property",
            "    def request_deadline(self):",
            "        \"\"\"Returns the configuration value for 'request_deadline'.\"\"\"",
            "        return self.__get_config().get_float(\"request_deadline\")",
            "",
            "    @property",
            "    def debug_level(self):",
            "        \"\"\"Returns the configuration value for 'debug_level'.\"\"\"",
            "        return self.__get_config().get_int(\"debug_level\")",
            "",
            "    @property",
            "    def stdout_severity(self):",
            "        \"\"\"Returns the configuration value for 'stdout_severity'.",
            "        Only used when running in no-fork mode.",
            "        \"\"\"",
            "        return self.__get_config().get_string(\"stdout_severity\").upper()",
            "",
            "    @property",
            "    def ca_cert_path(self):",
            "        \"\"\"Returns the configuration value for 'ca_cert_path'.\"\"\"",
            "        return self.__get_config().get_string(\"ca_cert_path\")",
            "",
            "    @property",
            "    def compression_type(self):",
            "        \"\"\"Returns the configuration value for 'compression_type'.\"\"\"",
            "        return self.__get_config().get_string(\"compression_type\", none_if_missing=True)",
            "",
            "    @property",
            "    def compression_level(self):",
            "        \"\"\"Returns the configuration value for 'compression_level'.\"\"\"",
            "        return self.__get_config().get_int(\"compression_level\", default_value=9)",
            "",
            "    @property",
            "    def use_requests_lib(self):",
            "        \"\"\"Returns the configuration value for 'use_requests_lib'.\"\"\"",
            "        return self.__get_config().get_bool(\"use_requests_lib\")",
            "",
            "    @property",
            "    def use_tlslite(self):",
            "        \"\"\"Returns the configuration value for 'use_tlslite'.\"\"\"",
            "        return self.__get_config().get_bool(\"use_tlslite\")",
            "",
            "    @property",
            "    def network_proxies(self):",
            "        \"\"\"Returns the proxy map created by the 'https_proxy' and 'http_proxy' configuration variables, or",
            "        None if neither of those is set.",
            "        \"\"\"",
            "        https_proxy = self.__get_config().get_string(",
            "            \"https_proxy\", none_if_missing=True",
            "        )",
            "        http_proxy = self.__get_config().get_string(\"http_proxy\", none_if_missing=True)",
            "        if https_proxy is None and http_proxy is None:",
            "            return None",
            "        result = {}",
            "        if https_proxy is not None:",
            "            result[\"https\"] = https_proxy",
            "        if http_proxy is not None:",
            "            result[\"http\"] = http_proxy",
            "        return result",
            "",
            "    @property",
            "    def global_monitor_sample_interval(self):",
            "        \"\"\"Returns the configuration value for 'global_monitor_sample_interval'.\"\"\"",
            "        return self.__get_config().get_float(\"global_monitor_sample_interval\")",
            "",
            "    @property",
            "    def full_checkpoint_interval(self):",
            "        \"\"\"Returns the configuration value for 'full_checkpoint_interval_in_seconds'.\"\"\"",
            "        return self.__get_config().get_int(\"full_checkpoint_interval_in_seconds\")",
            "",
            "    @property",
            "    def minimum_scan_interval(self):",
            "        \"\"\"Returns the configuration value for 'minimum_scan_interval'.\"\"\"",
            "        return self.__get_config().get_int(",
            "            \"minimum_scan_interval\", none_if_missing=True",
            "        )",
            "",
            "    @property",
            "    def close_old_files_duration_in_seconds(self):",
            "        \"\"\"Returns the configuration value for 'close_old_files_duration_in_seconds'.\"\"\"",
            "        return self.__get_config().get_int(\"close_old_files_duration_in_seconds\")",
            "",
            "    @property",
            "    def max_line_size(self):",
            "        \"\"\"Returns the configuration value for 'max_line_size'.\"\"\"",
            "        return self.__get_config().get_int(\"max_line_size\")",
            "",
            "    @property",
            "    def line_completion_wait_time(self):",
            "        \"\"\"Returns the configuration value for 'line_completion_wait_time'.\"\"\"",
            "        return self.__get_config().get_float(\"line_completion_wait_time\")",
            "",
            "    @property",
            "    def internal_parse_max_line_size(self):",
            "        \"\"\"Returns the configuration value for 'internal_parse_max_line_size'.\"\"\"",
            "        return self.__get_config().get_int(\"internal_parse_max_line_size\")",
            "",
            "    @property",
            "    def max_log_offset_size(self):",
            "        \"\"\"Returns the configuration value for 'max_log_offset_size'.\"\"\"",
            "        return self.__get_config().get_int(\"max_log_offset_size\")",
            "",
            "    @property",
            "    def max_existing_log_offset_size(self):",
            "        \"\"\"Returns the configuration value for 'max_existing_log_offset_size'.\"\"\"",
            "        return self.__get_config().get_int(\"max_existing_log_offset_size\")",
            "",
            "    @property",
            "    def max_sequence_number(self):",
            "        \"\"\"Returns the maximum sequence number\"\"\"",
            "        return self.__get_config().get_int(\"max_sequence_number\")",
            "",
            "    @property",
            "    def read_page_size(self):",
            "        \"\"\"Returns the configuration value for 'read_page_size'.\"\"\"",
            "        return self.__get_config().get_int(\"read_page_size\")",
            "",
            "    @property",
            "    def log_deletion_delay(self):",
            "        \"\"\"Returns the configuration value for 'log_deletion_delay'.\"\"\"",
            "        return self.__get_config().get_float(\"log_deletion_delay\")",
            "",
            "    @property",
            "    def log_rotation_max_bytes(self):",
            "        \"\"\"Returns the configuration value for 'log_rotation_max_bytes'.\"\"\"",
            "        return self.__get_config().get_int(\"log_rotation_max_bytes\")",
            "",
            "    @property",
            "    def log_rotation_backup_count(self):",
            "        \"\"\"Returns the configuration value for 'log_rotation_backup_count'.\"\"\"",
            "        return self.__get_config().get_int(\"log_rotation_backup_count\")",
            "",
            "    @property",
            "    def copy_staleness_threshold(self):",
            "        \"\"\"Returns the configuration value for 'copy_staleness_threshold'.\"\"\"",
            "        return self.__get_config().get_float(\"copy_staleness_threshold\")",
            "",
            "    @property",
            "    def debug_init(self):",
            "        \"\"\"Returns the configuration value for 'debug_init'.\"\"\"",
            "        return self.__get_config().get_bool(\"debug_init\")",
            "",
            "    @property",
            "    def pidfile_advanced_reuse_guard(self):",
            "        \"\"\"Returns the configuration value for 'pidfile_advanced_reuse_guard'.\"\"\"",
            "        return self.__get_config().get_bool(\"pidfile_advanced_reuse_guard\")",
            "",
            "    @property",
            "    def verify_server_certificate(self):",
            "        \"\"\"Returns the configuration value for 'verify_server_certificate'.\"\"\"",
            "        return self.__get_config().get_bool(\"verify_server_certificate\")",
            "",
            "    @property",
            "    def pipeline_threshold(self):",
            "        \"\"\"Returns the percentage an add events request must be of the maximum allowed request size to",
            "        trigger pipelining the next add events request.",
            "        \"\"\"",
            "        return self.__get_config().get_float(\"pipeline_threshold\")",
            "",
            "    @property",
            "    def strip_domain_from_default_server_host(self):",
            "        \"\"\"Returns whether or not we should remove the domain name from the default server host that is used to",
            "        identify the source of the logs from this agent.  For example, if the hostname is `foo.scalyr.com`, setting",
            "        this field to `true` will result in `foo` being used as the reported `serverHost` name.",
            "",
            "        This only applies if you do not set an explicit `serverHost` attribute in the server attributes.",
            "        \"\"\"",
            "        return self.__get_config().get_bool(\"strip_domain_from_default_server_host\")",
            "",
            "    @property",
            "    def healthy_max_time_since_last_copy_attempt(self):",
            "        \"\"\"Returns the max amount of time since the last copy attempt to consider the agent 'healthy' for",
            "        the purpose of a health check using `status -v` or `-H`. This copy attempt need not be successful, since this is",
            "        just to check that the agent is healthy and should not reflect server side errors.",
            "        \"\"\"",
            "        return self.__get_config().get_float(\"healthy_max_time_since_last_copy_attempt\")",
            "",
            "    def equivalent(self, other, exclude_debug_level=False):",
            "        \"\"\"Returns true if other contains the same configuration information as this object.",
            "",
            "        This is different than an '_eq_' method because this comparison ignores some of the fields",
            "        such as what times the files were read at.  Also, it compares the final results of the configuration,",
            "        after defaults have been applied.",
            "",
            "        @param exclude_debug_level: If True, will also ignore the values for 'debug_level' when doing comparison.",
            "        \"\"\"",
            "        if self.__last_error != other.__last_error:",
            "            return False",
            "",
            "        original_debug_level = None",
            "        try:",
            "            # If we are ignoring debug level, then we do a little hack here where we just put the value for",
            "            # this config into other's config.. and then just put the original value back after we've done the",
            "            # comparison.",
            "            if exclude_debug_level:",
            "                original_debug_level = other.__config.get(\"debug_level\")",
            "                other.__config.put(\"debug_level\", self.__config.get(\"debug_level\"))",
            "",
            "            if self.__config != other.__config:",
            "                return False",
            "            return True",
            "        finally:",
            "            if original_debug_level is not None:",
            "                other.__config.put(\"debug_level\", original_debug_level)",
            "",
            "    @staticmethod",
            "    def get_extra_config_dir(extra_config_dir):",
            "        \"\"\"",
            "        Returns the value for the additional config directory - either from the value passed",
            "        in, or from the environment variable `SCALYR_EXTRA_CONFIG_DIR`.",
            "",
            "        @param extra_config_dir: the additinal configuration directory.  If this value is",
            "            None, then the environment variable `SCALYR_EXTRA_CONFIG_DIR` is read for the result",
            "        \"\"\"",
            "        result = extra_config_dir",
            "        if extra_config_dir is None:",
            "            result = compat.os_getenv_unicode(\"SCALYR_EXTRA_CONFIG_DIR\")",
            "        return result",
            "",
            "    @staticmethod",
            "    def default_ca_cert_path():",
            "        \"\"\"Returns the default configuration file path for the agent.\"\"\"",
            "        # TODO:  Support more platforms.",
            "        return Configuration.__resolve_to_install_location(\"certs\", \"ca_certs.crt\")",
            "",
            "    @property",
            "    def intermediate_certs_path(self):",
            "        \"\"\"Returns the intermediate certs path.\"\"\"",
            "        return Configuration.__resolve_to_install_location(",
            "            \"certs\", \"intermediate_certs.pem\"",
            "        )",
            "",
            "    @staticmethod",
            "    def __resolve_to_install_location(*paths):",
            "        \"\"\"Returns the absolute path created by joining the specified intermediate paths to",
            "        the install location for this package.",
            "",
            "        @param paths: The file components of the desired path. There can be multiple, starting with the outer directory",
            "            first and finishing with the last file.",
            "        \"\"\"",
            "        result = get_install_root()",
            "        for path in paths:",
            "            result = os.path.join(result, path)",
            "        return result",
            "",
            "    def __get_parent_directory(self, file_path):",
            "        \"\"\"Returns the directory containing the specified file.",
            "",
            "        @param file_path: The absolute file path.",
            "",
            "        @return: The absolute path for the parent directory.\"\"\"",
            "        return os.path.dirname(file_path)",
            "",
            "    def __resolve_absolute_path(self, file_path, working_directory):",
            "        \"\"\"Returns the full path for the specified file.",
            "",
            "        If the specified file path is relative, then working_directory is used to resolve the relative path.",
            "        This function does not do any existence checks on either file_path or working_directory.",
            "",
            "        @param file_path: The path of the file.",
            "        @param working_directory: The directory to use to resolve relative file paths.",
            "",
            "        @return: The absolute path for the specified file.",
            "        \"\"\"",
            "        if os.path.isabs(file_path):",
            "            return file_path",
            "",
            "        return os.path.join(working_directory, file_path)",
            "",
            "    def __list_files(self, directory_path):",
            "        \"\"\"Returns a list of the files ending in .json for the specified directory.",
            "",
            "        This only returns files in the directory.  Also, if the directory does not exist",
            "        or cannot be read, an empty list is returned.",
            "",
            "        @param directory_path: The path of the directory.",
            "",
            "        @return: If the directory exists and can be read, the list of files ending in .json (not directories).",
            "        \"\"\"",
            "        result = []",
            "        if directory_path is None:",
            "            return result",
            "        if not os.path.isdir(directory_path):",
            "            return result",
            "        if not os.access(directory_path, os.R_OK):",
            "            return result",
            "",
            "        for f in sorted(os.listdir(directory_path)):",
            "            if f.endswith(\".json\"):",
            "                full_path = os.path.join(directory_path, f)",
            "                if os.path.isfile(full_path):",
            "                    result.append(full_path)",
            "        return result",
            "",
            "    def __add_elements_from_array(self, field, source_json, destination_json):",
            "        \"\"\"Appends any elements in the JsonArray in source_json to destination_json.",
            "",
            "        @param field: The name of the field containing the JsonArray.",
            "        @param source_json: The JsonObject containing the JsonArray from which to retrieve elements.",
            "        @param destination_json: The JsonObject to which the elements should be added (in the JsonArray named field.",
            "        \"\"\"",
            "        destination_array = destination_json.get_json_array(field)",
            "        for element in source_json.get_json_array(field):",
            "            destination_array.add(element)",
            "",
            "    def __set_api_key(self, config, api_key):",
            "        \"\"\"",
            "        Sets the api_key of the config, and throws errors if there are any problems",
            "        \"\"\"",
            "        if api_key:",
            "            config.put(\"api_key\", api_key)",
            "",
            "        if \"api_key\" not in config:",
            "            raise BadConfiguration(",
            "                'The configuration file is missing the required field \"api_key\" that '",
            "                \"sets the authentication key to use when writing logs to Scalyr.  Please update \"",
            "                \"the config file with a Write Logs key from https://www.scalyr.com/keys\",",
            "                \"api_key\",",
            "                \"missingApiKey\",",
            "            )",
            "",
            "        if config.get_string(\"api_key\") == \"\":",
            "            raise BadConfiguration(",
            "                \"The configuration file contains an empty string for the required field \"",
            "                '\"api_key\" that sets the authentication key to use when writing logs to Scalyr. '",
            "                \"Please update the config file with a Write Logs key from https://www.scalyr.com/keys\",",
            "                \"api_key\",",
            "                \"emptyApiKey\",",
            "            )",
            "",
            "    def __check_field(",
            "        self,",
            "        field,",
            "        config,",
            "        file_path,",
            "        previous_value=None,",
            "        previous_config_file=None,",
            "        error_code=None,",
            "    ):",
            "        \"\"\"",
            "        Checks to see if the config contains a value for `field` , and if so verifies that it is a valid string.",
            "",
            "        @param field:  The name of the field to check.",
            "        @param config:  The contents of the config.",
            "        @param file_path:  The file path to the config.",
            "        @param previous_value:  If this field has been already defined in another config, the value it was given.",
            "        @param previous_config_file:  If not None, the path to a config file that has already set this field.  If this",
            "            is not None and this config does define the field, a `BadConfiguration` exception is raised.",
            "        @param error_code:  The error code to return if it is detected the field has been set in multiple config files.",
            "        @return the field's value and file_path if the field is found, else return None and None",
            "",
            "        \"\"\"",
            "",
            "        description = 'configuration file \"%s\"' % file_path",
            "",
            "        if field in config:",
            "            self.__verify_required_string(config, field, description)",
            "            result_key = config.get_string(field)",
            "            result_file = file_path",
            "",
            "            if previous_config_file is not None:",
            "                raise BadConfiguration(",
            "                    'The configuration file \"%s\" contains an \"%s\" value, but that field has already been set in '",
            "                    '\"%s\".  Please ensure that the \"%s\" value is set only once'",
            "                    % (file_path, field, previous_config_file, field),",
            "                    field,",
            "                    error_code,",
            "                )",
            "",
            "            return result_key, result_file",
            "        return previous_value, previous_config_file",
            "",
            "    def __verify_main_config(self, config, file_path):",
            "        self.__verify_main_config_and_apply_defaults(",
            "            config, file_path, apply_defaults=False",
            "        )",
            "",
            "    def __verify_main_config_and_apply_defaults(",
            "        self, config, file_path, apply_defaults=True",
            "    ):",
            "        \"\"\"Verifies the contents of the configuration object and updates missing fields with defaults.",
            "",
            "        This will verify and possibly update all the fields in the configuration file except for",
            "        the 'logs' and 'monitors' json arrays.  If any of the fields do not meet their type requirement,",
            "        an exception will be raised.  If any of the fields are not present, then config will be updated with",
            "        the appropriate default value.",
            "",
            "        @param config: The main JsonObject configuration object.",
            "        @param file_path: The file that was read to retrieve the config object. This is used in error reporting.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "    \"\"\"",
            "        description = 'configuration file \"%s\"' % file_path",
            "",
            "        self.__verify_or_set_optional_string(",
            "            config, \"api_key\", \"\", description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"allow_http\", False, description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"check_remote_if_no_tty\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"compression_type\",",
            "            \"deflate\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "            valid_values=scalyr_util.SUPPORTED_COMPRESSION_ALGORITHMS,",
            "        )",
            "        self.__verify_compression_type(self.compression_type)",
            "",
            "        # NOTE: If not explicitly specified by the user, we use compression algorithm specific",
            "        # default value",
            "        default_compression_level = scalyr_util.COMPRESSION_TYPE_TO_DEFAULT_LEVEL[",
            "            self.compression_type",
            "        ]",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"compression_level\",",
            "            default_compression_level,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_compression_level(self.compression_level)",
            "",
            "        self.__verify_or_set_optional_attributes(",
            "            config, \"server_attributes\", description, apply_defaults, env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"agent_log_path\",",
            "            self.__default_paths.agent_log_path,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"agent_data_path\",",
            "            self.__default_paths.agent_data_path,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"additional_monitor_module_paths\",",
            "            \"\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"config_directory\",",
            "            \"agent.d\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"json_library\",",
            "            \"auto\",",
            "            \"JSON serialization and deserializarion library to use. Valid options are auto, json, ujson and orjson\",",
            "            apply_defaults,",
            "            env_aware=True,",
            "            valid_values=[\"auto\", \"json\", \"ujson\", \"orjson\"],",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"implicit_agent_log_collection\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"implicit_metric_monitor\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"implicit_agent_process_metrics_monitor\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"use_unsafe_debugging\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"copying_thread_profile_interval\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"copying_thread_profile_output_path\",",
            "            \"/tmp/copying_thread_profiles_\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"global_monitor_sample_interval\",",
            "            30.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"close_old_files_duration_in_seconds\",",
            "            60 * 60 * 1,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"full_checkpoint_interval_in_seconds\",",
            "            60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"max_send_rate_enforcement\",",
            "            \"unlimited\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_max_send_rate_enforcement_overrides\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_allowed_request_size\",",
            "            1 * 1024 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"min_allowed_request_size\",",
            "            100 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"min_request_spacing_interval\",",
            "            1.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"max_request_spacing_interval\",",
            "            5.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"max_error_request_spacing_interval\",",
            "            30.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"minimum_scan_interval\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"low_water_bytes_sent\",",
            "            20 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"low_water_request_spacing_adjustment\",",
            "            1.5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"high_water_bytes_sent\",",
            "            100 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"high_water_request_spacing_adjustment\",",
            "            0.6,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"failure_request_spacing_adjustment\",",
            "            1.5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"request_too_large_adjustment\",",
            "            0.5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"max_new_log_detection_time\",",
            "            60.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # These parameters are used in log_processing.py to govern how logs are copied.",
            "",
            "        # The maximum allowed size for a line when reading from a log file.",
            "        # We do not strictly enforce this -- some lines returned by LogFileIterator may be",
            "        # longer than this due to some edge cases.",
            "        self.__verify_or_set_optional_int(",
            "            config, \"max_line_size\", 49900, description, apply_defaults, env_aware=True",
            "        )",
            "",
            "        # The number of seconds we are willing to wait when encountering a log line at the end of a log file that does",
            "        # not currently end in a new line (referred to as a partial line).  It could be that the full line just hasn't",
            "        # made it all the way to disk yet.  After this time though, we will just return the bytes as a line",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"line_completion_wait_time\",",
            "            5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The maximum negative offset relative to the end of a previously unseen log the log file",
            "        # iterator is allowed to become.  If bytes are not being read quickly enough, then",
            "        # the iterator will automatically advance so that it is no more than this length",
            "        # to the end of the file.  This is essentially the maximum bytes a new log file",
            "        # is allowed to be caught up when used in copying logs to Scalyr.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_log_offset_size\",",
            "            5 * 1024 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The maximum negative offset relative to the end of an existing log the log file",
            "        # iterator is allowed to become.  If bytes are not being read quickly enough, then",
            "        # the iterator will automatically advance so that it is no more than this length",
            "        # to the end of the file.  This is essentially the maximum bytes an existing log file",
            "        # is allowed to be caught up when used in copying logs to Scalyr.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_existing_log_offset_size\",",
            "            100 * 1024 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The maximum sequence number for a given sequence",
            "        # The sequence number is typically the total number of bytes read from a given file",
            "        # (across restarts and log rotations), and it resets to zero (and begins a new sequence)",
            "        # for each file once the current sequence_number exceeds this value",
            "        # defaults to 1 TB",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_sequence_number\",",
            "            1024 ** 4,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The number of bytes to read from a file at a time into the buffer.  This must",
            "        # always be greater than the MAX_LINE_SIZE",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"read_page_size\",",
            "            64 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"internal_parse_max_line_size\",",
            "            config.get_int(\"read_page_size\", 64 * 1024),",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The minimum time we wait for a log file to reappear on a file system after it has been removed before",
            "        # we consider it deleted.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"log_deletion_delay\",",
            "            10 * 60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # How many log rotations to do",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"log_rotation_backup_count\",",
            "            2,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The size of each log rotation file",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"log_rotation_max_bytes\",",
            "            20 * 1024 * 1024,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # The percentage of the maximum message size a message (max_allowed_request_size) has to be to trigger",
            "        # pipelining the next add events request.  This intentionally set to 110% to prevent it from being used unless",
            "        # explicitly requested.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"pipeline_threshold\",",
            "            1.1,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # If we have noticed that new bytes have appeared in a file but we do not read them before this threshold",
            "        # is exceeded, then we consider those bytes to be stale and just skip to reading from the end to get the",
            "        # freshest bytes.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"copy_staleness_threshold\",",
            "            15 * 60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"debug_init\", False, description, apply_defaults, env_aware=True",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"pidfile_advanced_reuse_guard\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"strip_domain_from_default_server_host\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"healthy_max_time_since_last_copy_attempt\",",
            "            60.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config, \"debug_level\", 0, description, apply_defaults, env_aware=True",
            "        )",
            "        debug_level = config.get_int(\"debug_level\", apply_defaults)",
            "        if debug_level < 0 or debug_level > 5:",
            "            raise BadConfiguration(",
            "                \"The debug level must be between 0 and 5 inclusive\",",
            "                \"debug_level\",",
            "                \"badDebugLevel\",",
            "            )",
            "",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"stdout_severity\",",
            "            \"NOTSET\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        stdout_severity = config.get_string(\"stdout_severity\", default_value=\"NOTSET\")",
            "        if not hasattr(logging, stdout_severity.upper()):",
            "            raise BadConfiguration(",
            "                \"The stdout severity must be a valid logging level name\",",
            "                \"stdout_severity\",",
            "                \"badStdoutSeverity\",",
            "            )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"request_deadline\",",
            "            60.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"ca_cert_path\",",
            "            Configuration.default_ca_cert_path(),",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"use_requests_lib\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"use_tlslite\", False, description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"verify_server_certificate\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config, \"http_proxy\", None, description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config, \"https_proxy\", None, description, apply_defaults, env_aware=True",
            "        )",
            "        self.__verify_or_set_optional_array_of_strings(",
            "            config,",
            "            \"k8s_ignore_namespaces\",",
            "            Configuration.DEFAULT_K8S_IGNORE_NAMESPACES,",
            "            description,",
            "            apply_defaults,",
            "            separators=[None, \",\"],",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_array_of_strings(",
            "            config,",
            "            \"k8s_include_namespaces\",",
            "            Configuration.DEFAULT_K8S_INCLUDE_NAMESPACES,",
            "            description,",
            "            apply_defaults,",
            "            separators=[None, \",\"],",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_api_url\",",
            "            \"https://kubernetes.default\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_verify_api_queries\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_verify_kubelet_queries\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_kubelet_ca_cert\",",
            "            \"/run/secrets/kubernetes.io/serviceaccount/ca.crt\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_query_timeout_secs\",",
            "            20,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_expiry_secs\",",
            "            30,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_expiry_fuzz_secs\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_start_fuzz_secs\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_cache_purge_secs\",",
            "            300,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_service_account_cert\",",
            "            \"/run/secrets/kubernetes.io/serviceaccount/ca.crt\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_service_account_token\",",
            "            \"/var/run/secrets/kubernetes.io/serviceaccount/token\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_service_account_namespace\",",
            "            \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # Whether to log api responses to agent_debug.log",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_log_api_responses\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # If set to True, do not log successes (response code 2xx)",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_log_api_exclude_200s\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # Minimum response length of api responses to be logged.  Responses smaller than this limit are not logged.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_log_api_min_response_len\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # Minimum latency of responses to be logged.  Responses faster than this limit are not logged.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_log_api_min_latency\",",
            "            0.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # If positive, api calls with the same path will be rate-limited to a message every interval seconds",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_log_api_ratelimit_interval\",",
            "            0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # TODO-163 : make other settings more aggressive",
            "",
            "        # Optional (defaults to 3). The number of times the warmer will retry a query to warm a pod before giving up and",
            "        # classifying it as a Temporary Error",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_controlled_warmer_max_query_retries\",",
            "            3,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # Optional (defaults to 5). The maximum number of Temporary Errors that may occur when warming a pod's entry,",
            "        # before the warmer blacklists it.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_controlled_warmer_max_attempts\",",
            "            5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # Optional (defaults to 300). When a pod is blacklisted, how many secs it must wait until it is",
            "        # tried again for warming.",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_controlled_warmer_blacklist_time\",",
            "            300,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"k8s_events_disable\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # Agent-wide k8s rate limiter settings",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_ratelimit_cluster_num_agents\",",
            "            1,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_cluster_rps_init\",",
            "            1000.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_cluster_rps_min\",",
            "            1.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_cluster_rps_max\",",
            "            1e9,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_ratelimit_consecutive_increase_threshold\",",
            "            5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"k8s_ratelimit_strategy\",",
            "            BlockingRateLimiter.STRATEGY_MULTIPLY,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_increase_factor\",",
            "            2.0,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"k8s_ratelimit_backoff_factor\",",
            "            0.5,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"k8s_ratelimit_max_concurrency\",",
            "            1,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_send_requests\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"enforce_monotonic_timestamps\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"include_raw_timestamp_field\",",
            "            True,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"enable_profiling\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"max_profile_interval_minutes\",",
            "            60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"profile_duration_minutes\",",
            "            2,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"profile_clock\",",
            "            \"random\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"profile_log_name\",",
            "            \"agent.callgrind\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            config,",
            "            \"memory_profile_log_name\",",
            "            \"agent.meminfo\",",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # AGENT-263: controls sending in the new format or not as a safety in case it is broken somewhere in the chain",
            "        # TODO: Remove this in a future release once we are more certain that it works",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_logfile_addevents_format\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        # Debug leak flags",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_monitor_threads\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_monitors_creation\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_new_file_matches\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_leak_scan_for_new_bytes\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_leak_processing_new_bytes\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_copying_thread\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_overall_stats\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_leak_bandwidth_stats\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"disable_copy_manager_stats\", False, description, apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_leak_update_debug_log_level\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config, \"enable_gc_stats\", False, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config, \"disable_leak_all_config_updates\", None, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config, \"disable_leak_verify_config\", None, description, apply_defaults",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"disable_leak_config_equivalence_check\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"disable_leak_verify_can_write_to_logs\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config, \"disable_leak_config_reload\", None, description, apply_defaults",
            "        )",
            "",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"config_change_check_interval\",",
            "            30,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # How often to capture and log overall agent stats (in seconds).",
            "        # NOTE: This values must be >= config_change_check_interval.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"overall_stats_log_interval\",",
            "            600,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # How often to capture and log copying manager agent stats (in seconds).",
            "        # NOTE: This values must be >= config_change_check_interval.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"copying_manager_stats_log_interval\",",
            "            300,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        # How often to capture and log bandwidth related stats (in seconds).",
            "        # NOTE: This values must be >= config_change_check_interval.",
            "        self.__verify_or_set_optional_float(",
            "            config,",
            "            \"bandwidth_stats_log_interval\",",
            "            60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"user_agent_refresh_interval\",",
            "            60,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"garbage_collect_interval\",",
            "            300,",
            "            description,",
            "            apply_defaults,",
            "            env_aware=True,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"disable_leak_verify_config_create_monitors_manager\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_int(",
            "            config,",
            "            \"disable_leak_verify_config_create_copying_manager\",",
            "            None,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            config,",
            "            \"disable_leak_verify_config_cache_config\",",
            "            False,",
            "            description,",
            "            apply_defaults,",
            "        )",
            "",
            "    def __verify_compression_type(self, compression_type):",
            "        \"\"\"",
            "        Verify that the library for the specified compression type (algorithm) is available.",
            "        \"\"\"",
            "        library_name = scalyr_util.COMPRESSION_TYPE_TO_PYTHON_LIBRARY.get(",
            "            compression_type, \"unknown\"",
            "        )",
            "",
            "        try:",
            "            _, _ = scalyr_util.get_compress_and_decompress_func(compression_type)",
            "        except (ImportError, ValueError) as e:",
            "            msg = (",
            "                'Failed to set compression type to \"%s\". Make sure that the corresponding Python '",
            "                \"library is available. You can install it using this command:\\n\\npip install %s\\n\\n \"",
            "                \"Original error: %s\" % (compression_type, library_name, str(e))",
            "            )",
            "            raise BadConfiguration(msg, \"compression_type\", \"invalidCompressionType\")",
            "",
            "    def __verify_compression_level(self, compression_level):",
            "        \"\"\"",
            "        Verify that the provided compression level is valid for the configured compression type.",
            "",
            "        If it's not, we use a default value for that compression algorithm. Keep in mind that this",
            "        behavior is there for backward compatibility reasons, otherwise it would be better to just",
            "        throw in such scenario",
            "        \"\"\"",
            "        compression_type = self.compression_type",
            "",
            "        valid_level_min, valid_level_max = scalyr_util.COMPRESSION_TYPE_TO_VALID_LEVELS[",
            "            compression_type",
            "        ]",
            "",
            "        if compression_level < valid_level_min or compression_level > valid_level_max:",
            "            self.__config.put(",
            "                \"compression_level\",",
            "                scalyr_util.COMPRESSION_TYPE_TO_DEFAULT_LEVEL[compression_type],",
            "            )",
            "",
            "    def __get_config_or_environment_val(",
            "        self, config_object, param_name, param_type, env_aware, custom_env_name",
            "    ):",
            "        \"\"\"Returns a type-converted config param value or if not found, a matching environment value.",
            "",
            "        If the environment value is returned, it is also written into the config_object.",
            "",
            "        Currently only handles the following types (str, int, bool, float, JsonObject, JsonArray).",
            "        Also validates that environment variables can be correctly converted into the primitive type.",
            "",
            "        Both upper-case and lower-case versions of the environment variable will be checked.",
            "",
            "        @param config_object: The JsonObject config containing the field as a key",
            "        @param param_name: Parameter name",
            "        @param param_type: Parameter type",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param custom_env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name. Both upper and lower case versions are tried.",
            "            Note: A non-empty value also automatically implies env_aware as True, regardless of it's value.",
            "",
            "        @return A python object representing the config param (or environment) value or None",
            "        @raises",
            "            JsonConversionException: if the config value or env value cannot be correctly converted.",
            "            TypeError: if the param_type is not supported.",
            "        \"\"\"",
            "        if param_type == int:",
            "            config_val = config_object.get_int(param_name, none_if_missing=True)",
            "        elif param_type == bool:",
            "            config_val = config_object.get_bool(param_name, none_if_missing=True)",
            "        elif param_type == float:",
            "            config_val = config_object.get_float(param_name, none_if_missing=True)",
            "        elif param_type == six.text_type:",
            "            config_val = config_object.get_string(param_name, none_if_missing=True)",
            "        elif param_type == JsonObject:",
            "            config_val = config_object.get_json_object(param_name, none_if_missing=True)",
            "        elif param_type == JsonArray:",
            "            config_val = config_object.get_json_array(param_name, none_if_missing=True)",
            "        elif param_type in (ArrayOfStrings, SpaceAndCommaSeparatedArrayOfStrings):",
            "            # ArrayOfStrings are extracted from config file as JsonArray",
            "            # (but extracted from the environment different from JsonArray)",
            "            config_val = config_object.get_json_array(param_name, none_if_missing=True)",
            "        else:",
            "            raise TypeError(",
            "                \"Unsupported environment variable conversion type %s (param name = %s)\"",
            "                % (param_type, param_name)",
            "            )",
            "",
            "        if not env_aware:",
            "            if not custom_env_name:",
            "                return config_val",
            "",
            "        self._environment_aware_map[param_name] = custom_env_name or (",
            "            \"SCALYR_%s\" % param_name.upper()",
            "        )",
            "",
            "        env_val = get_config_from_env(",
            "            param_name,",
            "            custom_env_name=custom_env_name,",
            "            convert_to=param_type,",
            "            logger=self.__logger,",
            "            param_val=config_val,",
            "        )",
            "",
            "        # Not set in environment",
            "        if env_val is None:",
            "            return config_val",
            "",
            "        # Config file value wins if set",
            "        if config_val is not None:",
            "            return config_val",
            "",
            "        config_object.update({param_name: env_val})",
            "        return env_val",
            "",
            "    def __verify_logs_and_monitors_configs_and_apply_defaults(self, config, file_path):",
            "        \"\"\"Verifies the contents of the 'logs' and 'monitors' fields and updates missing fields with defaults.",
            "",
            "        This will verify and possible update the json arrays holding the 'logs' and 'monitor's configuration.",
            "        If any of the fields in those arrays or their contained elements do not meet their type requirement,",
            "        an exception will be raised.  If any of the fields are not present, then config will be updated with",
            "        the appropriate default values.",
            "",
            "        @param config: The main JsonObject configuration object.",
            "        @param file_path: The file that was read to retrieve the config object. This is used in error reporting.",
            "        \"\"\"",
            "        description = 'in configuration file \"%s\"' % file_path",
            "        self.__verify_or_set_optional_array(config, \"logs\", description)",
            "        self.__verify_or_set_optional_array(config, \"journald_logs\", description)",
            "        self.__verify_or_set_optional_array(config, \"k8s_logs\", description)",
            "        self.__verify_or_set_optional_array(config, \"monitors\", description)",
            "",
            "        i = 0",
            "        for log_entry in config.get_json_array(\"logs\"):",
            "            self.__verify_log_entry_and_set_defaults(",
            "                log_entry, config_file_path=file_path, entry_index=i",
            "            )",
            "            i += 1",
            "",
            "        i = 0",
            "        for log_entry in config.get_json_array(\"journald_logs\"):",
            "            self.__verify_log_entry_with_key_and_set_defaults(",
            "                log_entry,",
            "                key=\"journald_unit\",",
            "                config_file_path=file_path,",
            "                entry_index=i,",
            "            )",
            "            i += 1",
            "",
            "        i = 0",
            "        for log_entry in config.get_json_array(\"k8s_logs\"):",
            "            self.__verify_k8s_log_entry_and_set_defaults(",
            "                log_entry, config_file_path=file_path, entry_index=i,",
            "            )",
            "            i += 1",
            "",
            "        i = 0",
            "        for monitor_entry in config.get_json_array(\"monitors\"):",
            "            self.__verify_monitor_entry_and_set_defaults(",
            "                monitor_entry, file_path=file_path, entry_index=i",
            "            )",
            "            i += 1",
            "",
            "    def __verify_k8s_log_entry_and_set_defaults(",
            "        self, log_entry, description=None, config_file_path=None, entry_index=None,",
            "    ):",
            "        \"\"\"Verifies that the configuration for the specified k8s log entry.",
            "",
            "        A \"k8s_log\" entry can have one of multiple keys defined `k8s_pod_glob`, `k8s_namespace_glob`,",
            "        or `k8s_container_glob` which is a string containing a glob pattern to match against.",
            "",
            "        By default each of these values is `*` which matches against everything.  Users can",
            "        set these fields to limit the log configuration to specific pods, namespaces and containers.",
            "",
            "        Only the first matching config will be applied to any give log.  Users should make sure to",
            "        place more specific matching rules before more general ones.",
            "",
            "        Also verify the rest of the log config meets the required log config criteria and sets any defaults.",
            "",
            "        Raises an exception if it does not.",
            "",
            "        @param log_entry: The JsonObject holding the configuration for a log.",
            "        @param description: A human-readable description of where the log entry came from to use in error messages. If",
            "            none is given, then both file_path and entry_index must be set.",
            "        @param config_file_path: The path for the file from where the configuration was read. Used to generate the",
            "            description if none was given.",
            "        @param entry_index: The index of the entry in the 'logs' json array. Used to generate the description if none",
            "            was given.",
            "        \"\"\"",
            "",
            "        self.__verify_or_set_optional_string(",
            "            log_entry, \"k8s_pod_glob\", \"*\", description",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry, \"k8s_namespace_glob\", \"*\", description",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry, \"k8s_container_glob\", \"*\", description",
            "        )",
            "",
            "        self.__verify_log_entry_with_key_and_set_defaults(",
            "            log_entry,",
            "            None,",
            "            description=description,",
            "            config_file_path=config_file_path,",
            "            entry_index=entry_index,",
            "            # k8s log config defaults are applied later when adding containers to the copying manager",
            "            # so don't set them here",
            "            apply_defaults=False,",
            "            logs_field=\"k8s_logs\",",
            "        )",
            "",
            "    def __verify_log_entry_and_set_defaults(",
            "        self, log_entry, description=None, config_file_path=None, entry_index=None",
            "    ):",
            "        \"\"\"Verifies that the configuration for the specified log has a key called 'path' and meets all",
            "        the required criteria and sets any defaults.",
            "",
            "        Raises an exception if it does not.",
            "",
            "        @param log_entry: The JsonObject holding the configuration for a log.",
            "        @param description: A human-readable description of where the log entry came from to use in error messages. If",
            "            none is given, then both file_path and entry_index must be set.",
            "        @param config_file_path: The path for the file from where the configuration was read. Used to generate the",
            "            description if none was given.",
            "        @param entry_index: The index of the entry in the 'logs' json array. Used to generate the description if none",
            "            was given.",
            "        \"\"\"",
            "        # Make sure the log_enty has a path and the path is absolute.",
            "        path = log_entry.get_string(\"path\", none_if_missing=True)",
            "        if path and not os.path.isabs(path):",
            "            log_entry.put(\"path\", os.path.join(self.agent_log_path, path))",
            "        self.__verify_log_entry_with_key_and_set_defaults(",
            "            log_entry,",
            "            \"path\",",
            "            description=description,",
            "            config_file_path=config_file_path,",
            "            entry_index=entry_index,",
            "        )",
            "",
            "    def __verify_log_entry_with_key_and_set_defaults(",
            "        self,",
            "        log_entry,",
            "        key=None,",
            "        description=None,",
            "        config_file_path=None,",
            "        entry_index=None,",
            "        apply_defaults=True,",
            "        logs_field=\"logs\",",
            "    ):",
            "        \"\"\"Verifies that the configuration for the specified log meets all the required criteria and sets any defaults.",
            "",
            "        Raises an exception if it does not.",
            "",
            "        @param log_entry: The JsonObject holding the configuration for a log.",
            "        @param key: A key that must exist in the log entry.  The key is used to identify logs e.g. by path, container name etc",
            "            If `None` then no checking is done",
            "        @param description: A human-readable description of where the log entry came from to use in error messages. If",
            "            none is given, then both file_path and entry_index must be set.",
            "        @param config_file_path: The path for the file from where the configuration was read. Used to generate the",
            "            description if none was given.",
            "        @param entry_index: The index of the entry in the 'logs' json array. Used to generate the description if none",
            "            was given.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param logs_field: The name of the field used for log configs",
            "        \"\"\"",
            "        no_description_given = description is None",
            "        if no_description_given:",
            "            description = (",
            "                'the entry with index=%i in the \"%s\" array in configuration file \"%s\"'",
            "                % (entry_index, logs_field, config_file_path)",
            "            )",
            "        log = None",
            "        if key is not None:",
            "            # Verify it has a `key` entry that is a string.",
            "            self.__verify_required_string(log_entry, key, description)",
            "            log = log_entry.get_string(key)",
            "",
            "        if log is not None and no_description_given:",
            "            description = (",
            "                'the entry for \"%s\" in the \"%s\" array in configuration file \"%s\"'",
            "                % (log, logs_field, config_file_path)",
            "            )",
            "",
            "        self.__verify_or_set_optional_array_of_strings(",
            "            log_entry, \"exclude\", [], description, apply_defaults=apply_defaults,",
            "        )",
            "",
            "        # If a parser was specified, make sure it is a string.",
            "        if \"parser\" in log_entry:",
            "            self.__verify_or_set_optional_string(",
            "                log_entry,",
            "                \"parser\",",
            "                \"ignored\",",
            "                description,",
            "                apply_defaults=apply_defaults,",
            "            )",
            "",
            "        self.__verify_or_set_optional_attributes(log_entry, \"attributes\", description)",
            "",
            "        self.__verify_or_set_optional_array(log_entry, \"lineGroupers\", description)",
            "        i = 0",
            "        for element in log_entry.get_json_array(\"lineGroupers\"):",
            "            element_description = (",
            "                'the entry with index=%i in the \"lineGroupers\" array in ' % i",
            "            )",
            "            element_description += description",
            "",
            "            self.__verify_required_string(element, \"start\", element_description)",
            "            self.__verify_contains_exactly_one_string_out_of(",
            "                element,",
            "                [\"continueThrough\", \"continuePast\", \"haltBefore\", \"haltWith\"],",
            "                description,",
            "            )",
            "            i += 1",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            log_entry,",
            "            \"copy_from_start\",",
            "            False,",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_bool(",
            "            log_entry, \"parse_lines_as_json\", None, description, apply_defaults=False",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry,",
            "            \"parse_format\",",
            "            \"raw\",",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry,",
            "            \"json_message_field\",",
            "            \"log\",",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_string(",
            "            log_entry,",
            "            \"json_timestamp_field\",",
            "            \"time\",",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "",
            "        self.__verify_or_set_optional_bool(",
            "            log_entry,",
            "            \"ignore_stale_files\",",
            "            False,",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "        self.__verify_or_set_optional_float(",
            "            log_entry,",
            "            \"staleness_threshold_secs\",",
            "            5 * 60,",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "",
            "        self.__verify_or_set_optional_int(",
            "            log_entry,",
            "            \"minimum_scan_interval\",",
            "            None,",
            "            description,",
            "            apply_defaults=apply_defaults,",
            "        )",
            "",
            "        # Verify that if it has a sampling_rules array, then it is an array of json objects.",
            "        self.__verify_or_set_optional_array(log_entry, \"sampling_rules\", description)",
            "        i = 0",
            "        for element in log_entry.get_json_array(\"sampling_rules\"):",
            "            element_description = (",
            "                'the entry with index=%i in the \"sampling_rules\" array in ' % i",
            "            )",
            "            element_description += description",
            "            self.__verify_required_regexp(",
            "                element, \"match_expression\", element_description",
            "            )",
            "            self.__verify_required_percentage(",
            "                element, \"sampling_rate\", element_description",
            "            )",
            "            i += 1",
            "",
            "        # Verify that if it has a redaction_rules array, then it is an array of json objects.",
            "        self.__verify_or_set_optional_array(log_entry, \"redaction_rules\", description)",
            "        i = 0",
            "        for element in log_entry.get_json_array(\"redaction_rules\"):",
            "            element_description = (",
            "                'the entry with index=%i in the \"redaction_rules\" array in ' % i",
            "            )",
            "            element_description += description",
            "",
            "            self.__verify_required_regexp(",
            "                element, \"match_expression\", element_description",
            "            )",
            "            self.__verify_or_set_optional_string(",
            "                element, \"replacement\", \"\", element_description",
            "            )",
            "            i += 1",
            "",
            "        # We support the parser definition being at the top-level of the log config object, but we really need to",
            "        # put it in the attributes.",
            "        if \"parser\" in log_entry:",
            "            # noinspection PyTypeChecker",
            "            log_entry[\"attributes\"][\"parser\"] = log_entry[\"parser\"]",
            "",
            "    def __verify_monitor_entry_and_set_defaults(",
            "        self, monitor_entry, context_description=None, file_path=None, entry_index=None",
            "    ):",
            "        \"\"\"Verifies that the config for the specified monitor meets all the required criteria and sets any defaults.",
            "",
            "        Raises an exception if it does not.",
            "",
            "        @param monitor_entry: The JsonObject holding the configuration for a monitor.",
            "        @param file_path: The path for the file from where the configuration was read. Used to report errors to user.",
            "        @param entry_index: The index of the entry in the 'monitors' json array. Used to report errors to user.",
            "        \"\"\"",
            "        # Verify that it has a module name",
            "        if context_description is None:",
            "            description = (",
            "                'the entry with index=%i in the \"monitors\" array in configuration file \"%s\"'",
            "                % (entry_index, file_path)",
            "            )",
            "        else:",
            "            description = context_description",
            "",
            "        self.__verify_required_string(monitor_entry, \"module\", description)",
            "",
            "        module_name = monitor_entry.get_string(\"module\")",
            "",
            "        if context_description is None:",
            "            description = (",
            "                'the entry for module \"%s\" in the \"monitors\" array in configuration file \"%s\"'",
            "                % (module_name, file_path)",
            "            )",
            "        else:",
            "            description = context_description",
            "",
            "        # Verify that if it has a log_name field, it is a string.",
            "        self.__verify_or_set_optional_string(",
            "            monitor_entry, \"log_path\", module_name + \".log\", description",
            "        )",
            "",
            "    def __merge_server_attributes(self, fragment_file_path, config_fragment, config):",
            "        \"\"\"Merges the contents of the server attribute read from a configuration fragment to the main config object.",
            "",
            "        @param fragment_file_path: The path of the file from which the fragment was read. Used for error messages.",
            "        @param config_fragment: The JsonObject in the fragment file containing the 'server_attributes' field.",
            "        @param config: The main config object also containing a server_attributes field. The contents of the one from",
            "            config_fragment will be merged into this one.",
            "        \"\"\"",
            "        self.__verify_or_set_optional_attributes(",
            "            config_fragment,",
            "            \"server_attributes\",",
            "            'the configuration fragment at \"%s\"' % fragment_file_path,",
            "        )",
            "        source = config_fragment[\"server_attributes\"]",
            "        destination = config[\"server_attributes\"]",
            "        for k in source:",
            "            destination[k] = source[k]",
            "",
            "    def __verify_required_string(self, config_object, field, config_description):",
            "        \"\"\"Verifies that config_object has the required field and it can be converted to a string.",
            "",
            "        Raises an exception otherwise.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        \"\"\"",
            "        try:",
            "            config_object.get_string(field)",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The field \"%s\" is not a string.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"notString\",",
            "            )",
            "        except JsonMissingFieldException:",
            "            raise BadConfiguration(",
            "                'The required field \"%s\" is missing.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"missingRequired\",",
            "            )",
            "",
            "    def __verify_contains_exactly_one_string_out_of(",
            "        self, config_object, fields, config_description",
            "    ):",
            "        \"\"\"Verifies that config_object has exactly one of the named fields and it can be converted to a string.",
            "",
            "        Raises an exception otherwise.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param fields: A list of field names to check in the config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @return: The name of the found key, or None",
            "        \"\"\"",
            "        count = 0",
            "        result = None",
            "        for field in fields:",
            "            try:",
            "                value = config_object.get_string(field, none_if_missing=True)",
            "                if value is not None:",
            "                    result = field",
            "                    count += 1",
            "            except JsonConversionException:",
            "                raise BadConfiguration(",
            "                    'The field \"%s\" is not a string.  Error is in %s'",
            "                    % (field, config_description),",
            "                    field,",
            "                    \"notString\",",
            "                )",
            "        if count == 0:",
            "            raise BadConfiguration(",
            "                'A required field is missing.  Object must contain one of \"%s\".  Error is in %s'",
            "                % (six.text_type(fields), config_description),",
            "                field,",
            "                \"missingRequired\",",
            "            )",
            "        elif count > 1:",
            "            raise BadConfiguration(",
            "                'A required field has too many options.  Object must contain only one of \"%s\".  Error is in %s'",
            "                % (six.text_type(fields), config_description),",
            "                field,",
            "                \"missingRequired\",",
            "            )",
            "        return result",
            "",
            "    def __verify_or_set_optional_string(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "        valid_values=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is a string if present, otherwise sets default.",
            "",
            "        Raises an exception if the existing field cannot be converted to a string.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: The value to set in config_object for field if it currently has no value.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        @param valid_values: Optional list with valid values for this string.",
            "        \"\"\"",
            "        try:",
            "            value = self.__get_config_or_environment_val(",
            "                config_object, field, six.text_type, env_aware, env_name",
            "            )",
            "",
            "            if value is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, default_value)",
            "                return",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for field \"%s\" is not a string.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"notString\",",
            "            )",
            "",
            "        if value is not None and valid_values and value not in valid_values:",
            "            raise BadConfiguration(",
            "                'Got invalid value \"%s\" for field \"%s\". Valid values are: %s'",
            "                % (value, field, \", \".join(valid_values)),",
            "                field,",
            "                \"invalidValue\",",
            "            )",
            "",
            "    def __verify_or_set_optional_int(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object can be converted to an int if present, otherwise",
            "        sets default.",
            "",
            "        Raises an exception if the existing field cannot be converted to an int.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: The value to set in config_object for field if it currently has no value.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            value = self.__get_config_or_environment_val(",
            "                config_object, field, int, env_aware, env_name",
            "            )",
            "",
            "            if value is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, default_value)",
            "                return",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for field \"%s\" is not an int.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"notInt\",",
            "            )",
            "",
            "    def __verify_or_set_optional_float(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object can be converted to a float if present, otherwise",
            "        sets default.",
            "",
            "        Raises an exception if the existing field cannot be converted to a float.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: The value to set in config_object for field if it currently has no value.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            value = self.__get_config_or_environment_val(",
            "                config_object, field, float, env_aware, env_name",
            "            )",
            "",
            "            if value is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, default_value)",
            "                return",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for field \"%s\" is not an float.  Error is in %s'",
            "                % (field, config_description),",
            "                field,",
            "                \"notFloat\",",
            "            )",
            "",
            "    def __verify_or_set_optional_attributes(",
            "        self,",
            "        config_object,",
            "        field,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is a json object if present, otherwise sets to empty",
            "        object.",
            "",
            "        Raises an exception if the existing field is not a json object or if any of its values cannot be converted",
            "        to a string.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            json_object = self.__get_config_or_environment_val(",
            "                config_object, field, JsonObject, env_aware, env_name",
            "            )",
            "",
            "            if json_object is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, JsonObject())",
            "                return",
            "",
            "            for key in json_object.keys():",
            "                try:",
            "                    json_object.get_string(key)",
            "                except JsonConversionException:",
            "                    raise BadConfiguration(",
            "                        'The value for field \"%s\" in the json object for \"%s\" is not a '",
            "                        \"string.  Error is in %s\" % (key, field, config_description),",
            "                        field,",
            "                        \"notString\",",
            "                    )",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for the field \"%s\" is not a json object.  '",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notJsonObject\",",
            "            )",
            "",
            "    def __verify_or_set_optional_bool(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is a boolean if present, otherwise sets default.",
            "",
            "        Raises an exception if the existing field cannot be converted to a boolean.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: The value to set in config_object for field if it currently has no value.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            value = self.__get_config_or_environment_val(",
            "                config_object, field, bool, env_aware, env_name",
            "            )",
            "",
            "            if value is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, default_value)",
            "                return",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for the required field \"%s\" is not a boolean.  '",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notBoolean\",",
            "            )",
            "",
            "    def __verify_or_set_optional_array(",
            "        self,",
            "        config_object,",
            "        field,",
            "        config_description,",
            "        apply_defaults=True,",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is an array of json objects if present, otherwise sets",
            "        to empty array.",
            "",
            "        Raises an exception if the existing field is not a json array or if any of its elements are not json objects.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        try:",
            "            json_array = self.__get_config_or_environment_val(",
            "                config_object, field, JsonArray, env_aware, env_name",
            "            )",
            "",
            "            if json_array is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, JsonArray())",
            "                return",
            "",
            "            index = 0",
            "            for x in json_array:",
            "                if not isinstance(x, JsonObject):",
            "                    raise BadConfiguration(",
            "                        \"The element at index=%i is not a json object as required in the array \"",
            "                        'field \"%s (%s, %s)\".  Error is in %s'",
            "                        % (index, field, type(x), six.text_type(x), config_description),",
            "                        field,",
            "                        \"notJsonObject\",",
            "                    )",
            "                index += 1",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for the required field \"%s\" is not an array.  '",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notJsonArray\",",
            "            )",
            "",
            "    def __verify_or_set_optional_array_of_strings(",
            "        self,",
            "        config_object,",
            "        field,",
            "        default_value,",
            "        config_description,",
            "        apply_defaults=True,",
            "        separators=[\",\"],",
            "        env_aware=False,",
            "        env_name=None,",
            "    ):",
            "        \"\"\"Verifies that the specified field in config_object is an array of strings if present, otherwise sets",
            "        to empty array.",
            "",
            "        Raises an exception if the existing field is not a json array or if any of its elements are not strings/unicode.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param default_value: Default values (array of strings)",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        @param apply_defaults: If true, apply default values for any missing fields.  If false do not set values",
            "            for any fields missing from the config.",
            "        @param separators: list of allowed separators (An entry of None represents \"any whitespace\")",
            "        @param env_aware: If True and not defined in config file, look for presence of environment variable.",
            "        @param env_name: If provided, will use this name to lookup the environment variable.  Otherwise, use",
            "            scalyr_<field> as the environment variable name.",
            "        \"\"\"",
            "        # 2->TODO Python3 can not sort None values",
            "        separators.sort(key=lambda s: s if s is not None else \"\")",
            "        # For legacy reasons, must support space-separated array of strings",
            "        cls = ArrayOfStrings",
            "        if separators == [None, \",\"]:",
            "            cls = SpaceAndCommaSeparatedArrayOfStrings",
            "        try:",
            "            array_of_strings = self.__get_config_or_environment_val(",
            "                config_object, field, cls, env_aware, env_name",
            "            )",
            "",
            "            if array_of_strings is None:",
            "                if apply_defaults:",
            "                    config_object.put(field, cls(values=default_value))",
            "                return",
            "",
            "            index = 0",
            "            for x in array_of_strings:",
            "                if not isinstance(x, six.string_types):",
            "                    raise BadConfiguration(",
            "                        \"The element at index=%i is not a string or unicode object as required in the array \"",
            "                        'field \"%s\".  Error is in %s'",
            "                        % (index, field, config_description),",
            "                        field,",
            "                        \"notStringOrUnicode\",",
            "                    )",
            "                index += 1",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The value for the required field \"%s\" is not an array.  '",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notJsonArray\",",
            "            )",
            "",
            "    def __verify_required_regexp(self, config_object, field, config_description):",
            "        \"\"\"Verifies that config_object has the specified field and it can be parsed as a regular expression, otherwise",
            "        raises an exception.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        \"\"\"",
            "",
            "        try:",
            "            value = config_object.get_string(field, none_if_missing=True)",
            "",
            "            if value is not None:",
            "                re.compile(value)",
            "                return",
            "        except Exception:",
            "            raise BadConfiguration(",
            "                'The value for required field \"%s\" has a value that cannot be parsed as '",
            "                \"string regular expression (using python syntax).  \"",
            "                \"Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notRegexp\",",
            "            )",
            "",
            "        raise BadConfiguration(",
            "            'The required regular expression field \"%s\" is missing.  Error is in %s'",
            "            % (field, config_description),",
            "            field,",
            "            \"missingRequired\",",
            "        )",
            "",
            "    def __verify_required_percentage(self, config_object, field, config_description):",
            "        \"\"\"Verifies that config_object has the specified field and it can be it is a number between 0 and 1, otherwise",
            "        raises an exception.",
            "",
            "        @param config_object: The JsonObject containing the configuration information.",
            "        @param field: The name of the field to check in config_object.",
            "        @param config_description: A description of where the configuration object was sourced from to be used in the",
            "            error reporting to the user.",
            "        \"\"\"",
            "        try:",
            "            value = config_object.get_float(field, none_if_missing=True)",
            "",
            "            if value is None:",
            "                raise BadConfiguration(",
            "                    'The required percentage field \"%s\" is missing.  Error is in %s'",
            "                    % (field, config_description),",
            "                    field,",
            "                    \"missingRequired\",",
            "                )",
            "            elif value < 0 or value > 1:",
            "                raise BadConfiguration(",
            "                    'The required percentage field \"%s\" has a value \"%s\" that is not a number '",
            "                    \"between 0 and 1 inclusive.  Error is in %s\"",
            "                    % (field, value, config_description),",
            "                    field,",
            "                    \"notPercentage\",",
            "                )",
            "",
            "        except JsonConversionException:",
            "            raise BadConfiguration(",
            "                'The required field \"%s\" has a value that cannot be parsed as a number between 0 '",
            "                \"and 1 inclusive.  Error is in %s\" % (field, config_description),",
            "                field,",
            "                \"notNumber\",",
            "            )",
            "",
            "    def __get_config(self):",
            "        if self.__last_error is not None:",
            "            raise BadConfiguration(self.__last_error, \"fake\", \"fake\")",
            "        return self.__config",
            "",
            "    def __perform_substitutions(self, source_config):",
            "        \"\"\"Rewrites the content of the source_config to reflect the values in the `import_vars` array.",
            "",
            "        @param source_config:  The configuration to rewrite, represented as key/value pairs.",
            "        @type source_config: JsonObject",
            "        \"\"\"",
            "        substitutions = import_shell_variables(source_config=source_config)",
            "        if len(substitutions) > 0:",
            "            perform_object_substitution(",
            "                object_value=source_config, substitutions=substitutions",
            "            )",
            "",
            "",
            "\"\"\"",
            "Utility functions related to shell variable handling and substition.",
            "",
            "NOTE: Those functions are intentionally not defined inside \"__perform_substitutions\" to avoid memory",
            "leaks.",
            "\"\"\"",
            "",
            "",
            "def import_shell_variables(source_config):",
            "    \"\"\"Creates a dict mapping variables listed in the `import_vars` field of `source_config` to their",
            "    values from the environment.",
            "    \"\"\"",
            "    result = dict()",
            "    if \"import_vars\" in source_config:",
            "        for entry in source_config.get_json_array(\"import_vars\"):",
            "            # Allow for an entry of the form { var: \"foo\", default: \"bar\"}",
            "            if isinstance(entry, JsonObject):",
            "                var_name = entry[\"var\"]",
            "                default_value = entry[\"default\"]",
            "            else:",
            "                var_name = entry",
            "                default_value = \"\"",
            "",
            "            # 2->TODO in python2 os.environ returns 'str' type. Convert it to unicode.",
            "            var_value = os_environ_unicode.get(var_name)",
            "            if var_value:",
            "                result[var_name] = var_value",
            "            else:",
            "                result[var_name] = default_value",
            "    return result",
            "",
            "",
            "def perform_generic_substitution(value, substitutions):",
            "    \"\"\"Takes a given JSON value and performs the appropriate substitution.",
            "",
            "    This method will return a non-None value if the value has to be replaced with the returned value.",
            "    Otherwise, this will attempt to perform in-place substitutions.",
            "",
            "    For str, unicode, it substitutes the variables and returns the result.  For",
            "    container objects, it does the recursive substitution.",
            "",
            "    @param value: The JSON value",
            "    @type value: Any valid element of a JsonObject",
            "    @return: The value that should replace the original, if any.  If no replacement is necessary, returns None",
            "    \"\"\"",
            "    result = None",
            "    value_type = type(value)",
            "",
            "    if value_type is six.text_type and \"$\" in value:",
            "        result = perform_str_substitution(value, substitutions=substitutions)",
            "    elif isinstance(value, JsonObject):",
            "        perform_object_substitution(value, substitutions=substitutions)",
            "    elif isinstance(value, JsonArray):",
            "        perform_array_substitution(value, substitutions=substitutions)",
            "    return result",
            "",
            "",
            "def perform_object_substitution(object_value, substitutions):",
            "    \"\"\"Performs the in-place substitution for a JsonObject.",
            "",
            "    @param object_value: The object to perform substitutions on.",
            "    @type object_value: JsonObject",
            "    \"\"\"",
            "    # We collect the new values and apply them later to avoid messing up the iteration.",
            "    new_values = {}",
            "    for (key, value) in six.iteritems(object_value):",
            "        replace_value = perform_generic_substitution(value, substitutions=substitutions)",
            "        if replace_value is not None:",
            "            new_values[key] = replace_value",
            "",
            "    for (key, value) in six.iteritems(new_values):",
            "        object_value[key] = value",
            "",
            "",
            "def perform_str_substitution(str_value, substitutions):",
            "    \"\"\"Performs substitutions on the given string.",
            "",
            "    @param str_value: The input string.",
            "    @type str_value: str or unicode",
            "    @return: The resulting value after substitution.",
            "    @rtype: str or unicode",
            "    \"\"\"",
            "    result = str_value",
            "    for (var_name, value) in six.iteritems(substitutions):",
            "        result = result.replace(\"$%s\" % var_name, value)",
            "    return result",
            "",
            "",
            "def perform_array_substitution(array_value, substitutions):",
            "    \"\"\"Perform substitutions on the JsonArray.",
            "",
            "    @param array_value: The array",
            "    @type array_value: JsonArray",
            "    \"\"\"",
            "    for i in range(len(array_value)):",
            "        replace_value = perform_generic_substitution(",
            "            array_value[i], substitutions=substitutions",
            "        )",
            "        if replace_value is not None:",
            "            array_value[i] = replace_value"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "scalyr_agent.configuration.Configuration.print_useful_settings.other_options",
            "scalyr_agent.configuration.Configuration.print_useful_settings.options",
            "src.pyload.core"
        ]
    }
}