{
    "changedetectionio/model/Watch.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 89,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "         if ready_url.startswith('source:'):"
            },
            "2": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "             ready_url=ready_url.replace('source:', '')"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+        # Also double check it after any Jinja2 formatting just incase"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+        if not is_safe_url(ready_url):"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+            return 'DISABLED'"
            },
            "7": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "         return ready_url"
            },
            "8": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 97,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "     def clear_watch(self):"
            }
        },
        "frontPatchFile": [
            "from changedetectionio.strtobool import strtobool",
            "from changedetectionio.safe_jinja import render as jinja_render",
            "from . import watch_base",
            "import os",
            "import re",
            "from pathlib import Path",
            "from loguru import logger",
            "",
            "from ..html_tools import TRANSLATE_WHITESPACE_TABLE",
            "",
            "# Allowable protocols, protects against javascript: etc",
            "# file:// is further checked by ALLOW_FILE_URI",
            "SAFE_PROTOCOL_REGEX='^(http|https|ftp|file):'",
            "",
            "minimum_seconds_recheck_time = int(os.getenv('MINIMUM_SECONDS_RECHECK_TIME', 3))",
            "mtable = {'seconds': 1, 'minutes': 60, 'hours': 3600, 'days': 86400, 'weeks': 86400 * 7}",
            "",
            "",
            "def is_safe_url(test_url):",
            "    # See https://github.com/dgtlmoon/changedetection.io/issues/1358",
            "",
            "    # Remove 'source:' prefix so we dont get 'source:javascript:' etc",
            "    # 'source:' is a valid way to tell us to return the source",
            "",
            "    r = re.compile(re.escape('source:'), re.IGNORECASE)",
            "    test_url = r.sub('', test_url)",
            "",
            "    pattern = re.compile(os.getenv('SAFE_PROTOCOL_REGEX', SAFE_PROTOCOL_REGEX), re.IGNORECASE)",
            "    if not pattern.match(test_url.strip()):",
            "        return False",
            "",
            "    return True",
            "",
            "",
            "class model(watch_base):",
            "    __newest_history_key = None",
            "    __history_n = 0",
            "    jitter_seconds = 0",
            "",
            "    def __init__(self, *arg, **kw):",
            "        self.__datastore_path = kw.get('datastore_path')",
            "        if kw.get('datastore_path'):",
            "            del kw['datastore_path']",
            "        super(model, self).__init__(*arg, **kw)",
            "        if kw.get('default'):",
            "            self.update(kw['default'])",
            "            del kw['default']",
            "",
            "        if self.get('default'):",
            "            del self['default']",
            "",
            "        # Be sure the cached timestamp is ready",
            "        bump = self.history",
            "",
            "    @property",
            "    def viewed(self):",
            "        # Don't return viewed when last_viewed is 0 and newest_key is 0",
            "        if int(self['last_viewed']) and int(self['last_viewed']) >= int(self.newest_history_key) :",
            "            return True",
            "",
            "        return False",
            "",
            "    def ensure_data_dir_exists(self):",
            "        if not os.path.isdir(self.watch_data_dir):",
            "            logger.debug(f\"> Creating data dir {self.watch_data_dir}\")",
            "            os.mkdir(self.watch_data_dir)",
            "",
            "    @property",
            "    def link(self):",
            "",
            "        url = self.get('url', '')",
            "        if not is_safe_url(url):",
            "            return 'DISABLED'",
            "",
            "        ready_url = url",
            "        if '{%' in url or '{{' in url:",
            "            # Jinja2 available in URLs along with https://pypi.org/project/jinja2-time/",
            "            try:",
            "                ready_url = jinja_render(template_str=url)",
            "            except Exception as e:",
            "                logger.critical(f\"Invalid URL template for: '{url}' - {str(e)}\")",
            "                from flask import (",
            "                    flash, Markup, url_for",
            "                )",
            "                message = Markup('<a href=\"{}#general\">The URL {} is invalid and cannot be used, click to edit</a>'.format(",
            "                    url_for('edit_page', uuid=self.get('uuid')), self.get('url', '')))",
            "                flash(message, 'error')",
            "                return ''",
            "",
            "        if ready_url.startswith('source:'):",
            "            ready_url=ready_url.replace('source:', '')",
            "        return ready_url",
            "",
            "    def clear_watch(self):",
            "        import pathlib",
            "",
            "        # JSON Data, Screenshots, Textfiles (history index and snapshots), HTML in the future etc",
            "        for item in pathlib.Path(str(self.watch_data_dir)).rglob(\"*.*\"):",
            "            os.unlink(item)",
            "",
            "        # Force the attr to recalculate",
            "        bump = self.history",
            "",
            "        # Do this last because it will trigger a recheck due to last_checked being zero",
            "        self.update({",
            "            'browser_steps_last_error_step': None,",
            "            'check_count': 0,",
            "            'fetch_time': 0.0,",
            "            'has_ldjson_price_data': None,",
            "            'last_checked': 0,",
            "            'last_error': False,",
            "            'last_notification_error': False,",
            "            'last_viewed': 0,",
            "            'previous_md5': False,",
            "            'previous_md5_before_filters': False,",
            "            'remote_server_reply': None,",
            "            'track_ldjson_price_data': None",
            "        })",
            "        return",
            "",
            "    @property",
            "    def is_source_type_url(self):",
            "        return self.get('url', '').startswith('source:')",
            "",
            "    @property",
            "    def get_fetch_backend(self):",
            "        \"\"\"",
            "        Like just using the `fetch_backend` key but there could be some logic",
            "        :return:",
            "        \"\"\"",
            "        # Maybe also if is_image etc?",
            "        # This is because chrome/playwright wont render the PDF in the browser and we will just fetch it and use pdf2html to see the text.",
            "        if self.is_pdf:",
            "            return 'html_requests'",
            "",
            "        return self.get('fetch_backend')",
            "",
            "    @property",
            "    def is_pdf(self):",
            "        # content_type field is set in the future",
            "        # https://github.com/dgtlmoon/changedetection.io/issues/1392",
            "        # Not sure the best logic here",
            "        return self.get('url', '').lower().endswith('.pdf') or 'pdf' in self.get('content_type', '').lower()",
            "",
            "    @property",
            "    def label(self):",
            "        # Used for sorting",
            "        return self.get('title') if self.get('title') else self.get('url')",
            "",
            "    @property",
            "    def last_changed(self):",
            "        # last_changed will be the newest snapshot, but when we have just one snapshot, it should be 0",
            "        if self.__history_n <= 1:",
            "            return 0",
            "        if self.__newest_history_key:",
            "            return int(self.__newest_history_key)",
            "        return 0",
            "",
            "    @property",
            "    def history_n(self):",
            "        return self.__history_n",
            "",
            "    @property",
            "    def history(self):",
            "        \"\"\"History index is just a text file as a list",
            "            {watch-uuid}/history.txt",
            "",
            "            contains a list like",
            "",
            "            {epoch-time},{filename}\\n",
            "",
            "            We read in this list as the history information",
            "",
            "        \"\"\"",
            "        tmp_history = {}",
            "",
            "        # In the case we are only using the watch for processing without history",
            "        if not self.watch_data_dir:",
            "            return []",
            "",
            "        # Read the history file as a dict",
            "        fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        if os.path.isfile(fname):",
            "            logger.debug(f\"Reading watch history index for {self.get('uuid')}\")",
            "            with open(fname, \"r\") as f:",
            "                for i in f.readlines():",
            "                    if ',' in i:",
            "                        k, v = i.strip().split(',', 2)",
            "",
            "                        # The index history could contain a relative path, so we need to make the fullpath",
            "                        # so that python can read it",
            "                        if not '/' in v and not '\\'' in v:",
            "                            v = os.path.join(self.watch_data_dir, v)",
            "                        else:",
            "                            # It's possible that they moved the datadir on older versions",
            "                            # So the snapshot exists but is in a different path",
            "                            snapshot_fname = v.split('/')[-1]",
            "                            proposed_new_path = os.path.join(self.watch_data_dir, snapshot_fname)",
            "                            if not os.path.exists(v) and os.path.exists(proposed_new_path):",
            "                                v = proposed_new_path",
            "",
            "                        tmp_history[k] = v",
            "",
            "        if len(tmp_history):",
            "            self.__newest_history_key = list(tmp_history.keys())[-1]",
            "        else:",
            "            self.__newest_history_key = None",
            "",
            "        self.__history_n = len(tmp_history)",
            "",
            "        return tmp_history",
            "",
            "    @property",
            "    def has_history(self):",
            "        fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        return os.path.isfile(fname)",
            "",
            "    @property",
            "    def has_browser_steps(self):",
            "        has_browser_steps = self.get('browser_steps') and list(filter(",
            "            lambda s: (s['operation'] and len(s['operation']) and s['operation'] != 'Choose one' and s['operation'] != 'Goto site'),",
            "            self.get('browser_steps')))",
            "",
            "        return has_browser_steps",
            "",
            "    @property",
            "    def has_restock_info(self):",
            "        if self.get('restock') and self['restock'].get('in_stock') != None:",
            "                return True",
            "",
            "        return False",
            "",
            "    # Returns the newest key, but if theres only 1 record, then it's counted as not being new, so return 0.",
            "    @property",
            "    def newest_history_key(self):",
            "        if self.__newest_history_key is not None:",
            "            return self.__newest_history_key",
            "",
            "        if len(self.history) <= 1:",
            "            return 0",
            "",
            "",
            "        bump = self.history",
            "        return self.__newest_history_key",
            "",
            "    # Given an arbitrary timestamp, find the closest next key",
            "    # For example, last_viewed = 1000 so it should return the next 1001 timestamp",
            "    #",
            "    # used for the [diff] button so it can preset a smarter from_version",
            "    @property",
            "    def get_next_snapshot_key_to_last_viewed(self):",
            "",
            "        \"\"\"Unfortunately for now timestamp is stored as string key\"\"\"",
            "        keys = list(self.history.keys())",
            "        if not keys:",
            "            return None",
            "",
            "        last_viewed = int(self.get('last_viewed'))",
            "        prev_k = keys[0]",
            "        sorted_keys = sorted(keys, key=lambda x: int(x))",
            "        sorted_keys.reverse()",
            "",
            "        # When the 'last viewed' timestamp is greater than the newest snapshot, return second last",
            "        if last_viewed > int(sorted_keys[0]):",
            "            return sorted_keys[1]",
            "",
            "        for k in sorted_keys:",
            "            if int(k) < last_viewed:",
            "                if prev_k == sorted_keys[0]:",
            "                    # Return the second last one so we dont recommend the same version compares itself",
            "                    return sorted_keys[1]",
            "",
            "                return prev_k",
            "            prev_k = k",
            "",
            "        return keys[0]",
            "",
            "    def get_history_snapshot(self, timestamp):",
            "        import brotli",
            "        filepath = self.history[timestamp]",
            "",
            "        # See if a brotli versions exists and switch to that",
            "        if not filepath.endswith('.br') and os.path.isfile(f\"{filepath}.br\"):",
            "            filepath = f\"{filepath}.br\"",
            "",
            "        # OR in the backup case that the .br does not exist, but the plain one does",
            "        if filepath.endswith('.br') and not os.path.isfile(filepath):",
            "            if os.path.isfile(filepath.replace('.br', '')):",
            "                filepath = filepath.replace('.br', '')",
            "",
            "        if filepath.endswith('.br'):",
            "            # Brotli doesnt have a fileheader to detect it, so we rely on filename",
            "            # https://www.rfc-editor.org/rfc/rfc7932",
            "            with open(filepath, 'rb') as f:",
            "                return(brotli.decompress(f.read()).decode('utf-8'))",
            "",
            "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
            "            return f.read()",
            "",
            "    # Save some text file to the appropriate path and bump the history",
            "    # result_obj from fetch_site_status.run()",
            "    def save_history_text(self, contents, timestamp, snapshot_id):",
            "        import brotli",
            "",
            "        logger.trace(f\"{self.get('uuid')} - Updating history.txt with timestamp {timestamp}\")",
            "",
            "        self.ensure_data_dir_exists()",
            "",
            "        threshold = int(os.getenv('SNAPSHOT_BROTLI_COMPRESSION_THRESHOLD', 1024))",
            "        skip_brotli = strtobool(os.getenv('DISABLE_BROTLI_TEXT_SNAPSHOT', 'False'))",
            "",
            "        if not skip_brotli and len(contents) > threshold:",
            "            snapshot_fname = f\"{snapshot_id}.txt.br\"",
            "            dest = os.path.join(self.watch_data_dir, snapshot_fname)",
            "            if not os.path.exists(dest):",
            "                with open(dest, 'wb') as f:",
            "                    f.write(brotli.compress(contents.encode('utf-8'), mode=brotli.MODE_TEXT))",
            "        else:",
            "            snapshot_fname = f\"{snapshot_id}.txt\"",
            "            dest = os.path.join(self.watch_data_dir, snapshot_fname)",
            "            if not os.path.exists(dest):",
            "                with open(dest, 'wb') as f:",
            "                    f.write(contents.encode('utf-8'))",
            "",
            "        # Append to index",
            "        # @todo check last char was \\n",
            "        index_fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        with open(index_fname, 'a') as f:",
            "            f.write(\"{},{}\\n\".format(timestamp, snapshot_fname))",
            "            f.close()",
            "",
            "        self.__newest_history_key = timestamp",
            "        self.__history_n += 1",
            "",
            "        # @todo bump static cache of the last timestamp so we dont need to examine the file to set a proper ''viewed'' status",
            "        return snapshot_fname",
            "",
            "    @property",
            "    @property",
            "    def has_empty_checktime(self):",
            "        # using all() + dictionary comprehension",
            "        # Check if all values are 0 in dictionary",
            "        res = all(x == None or x == False or x==0 for x in self.get('time_between_check', {}).values())",
            "        return res",
            "",
            "    def threshold_seconds(self):",
            "        seconds = 0",
            "        for m, n in mtable.items():",
            "            x = self.get('time_between_check', {}).get(m, None)",
            "            if x:",
            "                seconds += x * n",
            "        return seconds",
            "",
            "    # Iterate over all history texts and see if something new exists",
            "    # Always applying .strip() to start/end but optionally replace any other whitespace",
            "    def lines_contain_something_unique_compared_to_history(self, lines: list, ignore_whitespace=False):",
            "        local_lines = []",
            "        if lines:",
            "            if ignore_whitespace:",
            "                if isinstance(lines[0], str): # Can be either str or bytes depending on what was on the disk",
            "                    local_lines = set([l.translate(TRANSLATE_WHITESPACE_TABLE).lower() for l in lines])",
            "                else:",
            "                    local_lines = set([l.decode('utf-8').translate(TRANSLATE_WHITESPACE_TABLE).lower() for l in lines])",
            "            else:",
            "                if isinstance(lines[0], str): # Can be either str or bytes depending on what was on the disk",
            "                    local_lines = set([l.strip().lower() for l in lines])",
            "                else:",
            "                    local_lines = set([l.decode('utf-8').strip().lower() for l in lines])",
            "",
            "",
            "        # Compare each lines (set) against each history text file (set) looking for something new..",
            "        existing_history = set({})",
            "        for k, v in self.history.items():",
            "            content = self.get_history_snapshot(k)",
            "",
            "            if ignore_whitespace:",
            "                alist = set([line.translate(TRANSLATE_WHITESPACE_TABLE).lower() for line in content.splitlines()])",
            "            else:",
            "                alist = set([line.strip().lower() for line in content.splitlines()])",
            "",
            "            existing_history = existing_history.union(alist)",
            "",
            "        # Check that everything in local_lines(new stuff) already exists in existing_history - it should",
            "        # if not, something new happened",
            "        return not local_lines.issubset(existing_history)",
            "",
            "    def get_screenshot(self):",
            "        fname = os.path.join(self.watch_data_dir, \"last-screenshot.png\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "",
            "        # False is not an option for AppRise, must be type None",
            "        return None",
            "",
            "    def __get_file_ctime(self, filename):",
            "        fname = os.path.join(self.watch_data_dir, filename)",
            "        if os.path.isfile(fname):",
            "            return int(os.path.getmtime(fname))",
            "        return False",
            "",
            "    @property",
            "    def error_text_ctime(self):",
            "        return self.__get_file_ctime('last-error.txt')",
            "",
            "    @property",
            "    def snapshot_text_ctime(self):",
            "        if self.history_n==0:",
            "            return False",
            "",
            "        timestamp = list(self.history.keys())[-1]",
            "        return int(timestamp)",
            "",
            "    @property",
            "    def snapshot_screenshot_ctime(self):",
            "        return self.__get_file_ctime('last-screenshot.png')",
            "",
            "    @property",
            "    def snapshot_error_screenshot_ctime(self):",
            "        return self.__get_file_ctime('last-error-screenshot.png')",
            "",
            "    @property",
            "    def watch_data_dir(self):",
            "        # The base dir of the watch data",
            "        return os.path.join(self.__datastore_path, self['uuid']) if self.__datastore_path else None",
            "",
            "    def get_error_text(self):",
            "        \"\"\"Return the text saved from a previous request that resulted in a non-200 error\"\"\"",
            "        fname = os.path.join(self.watch_data_dir, \"last-error.txt\")",
            "        if os.path.isfile(fname):",
            "            with open(fname, 'r') as f:",
            "                return f.read()",
            "        return False",
            "",
            "    def get_error_snapshot(self):",
            "        \"\"\"Return path to the screenshot that resulted in a non-200 error\"\"\"",
            "        fname = os.path.join(self.watch_data_dir, \"last-error-screenshot.png\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "        return False",
            "",
            "",
            "    def pause(self):",
            "        self['paused'] = True",
            "",
            "    def unpause(self):",
            "        self['paused'] = False",
            "",
            "    def toggle_pause(self):",
            "        self['paused'] ^= True",
            "",
            "    def mute(self):",
            "        self['notification_muted'] = True",
            "",
            "    def unmute(self):",
            "        self['notification_muted'] = False",
            "",
            "    def toggle_mute(self):",
            "        self['notification_muted'] ^= True",
            "",
            "    def extra_notification_token_values(self):",
            "        # Used for providing extra tokens",
            "        # return {'widget': 555}",
            "        return {}",
            "",
            "    def extra_notification_token_placeholder_info(self):",
            "        # Used for providing extra tokens",
            "        # return [('widget', \"Get widget amounts\")]",
            "        return []",
            "",
            "",
            "    def extract_regex_from_all_history(self, regex):",
            "        import csv",
            "        import re",
            "        import datetime",
            "        csv_output_filename = False",
            "        csv_writer = False",
            "        f = None",
            "",
            "        # self.history will be keyed with the full path",
            "        for k, fname in self.history.items():",
            "            if os.path.isfile(fname):",
            "                if True:",
            "                    contents = self.get_history_snapshot(k)",
            "                    res = re.findall(regex, contents, re.MULTILINE)",
            "                    if res:",
            "                        if not csv_writer:",
            "                            # A file on the disk can be transferred much faster via flask than a string reply",
            "                            csv_output_filename = 'report.csv'",
            "                            f = open(os.path.join(self.watch_data_dir, csv_output_filename), 'w')",
            "                            # @todo some headers in the future",
            "                            #fieldnames = ['Epoch seconds', 'Date']",
            "                            csv_writer = csv.writer(f,",
            "                                                    delimiter=',',",
            "                                                    quotechar='\"',",
            "                                                    quoting=csv.QUOTE_MINIMAL,",
            "                                                    #fieldnames=fieldnames",
            "                                                    )",
            "                            csv_writer.writerow(['Epoch seconds', 'Date'])",
            "                            # csv_writer.writeheader()",
            "",
            "                        date_str = datetime.datetime.fromtimestamp(int(k)).strftime('%Y-%m-%d %H:%M:%S')",
            "                        for r in res:",
            "                            row = [k, date_str]",
            "                            if isinstance(r, str):",
            "                                row.append(r)",
            "                            else:",
            "                                row+=r",
            "                            csv_writer.writerow(row)",
            "",
            "        if f:",
            "            f.close()",
            "",
            "        return csv_output_filename",
            "",
            "",
            "    def has_special_diff_filter_options_set(self):",
            "",
            "        # All False - nothing would be done, so act like it's not processable",
            "        if not self.get('filter_text_added', True) and not self.get('filter_text_replaced', True) and not self.get('filter_text_removed', True):",
            "            return False",
            "",
            "        # Or one is set",
            "        if not self.get('filter_text_added', True) or not self.get('filter_text_replaced', True) or not self.get('filter_text_removed', True):",
            "            return True",
            "",
            "        # None is set",
            "        return False",
            "",
            "    def save_error_text(self, contents):",
            "        self.ensure_data_dir_exists()",
            "        target_path = os.path.join(self.watch_data_dir, \"last-error.txt\")",
            "        with open(target_path, 'w') as f:",
            "            f.write(contents)",
            "",
            "    def save_xpath_data(self, data, as_error=False):",
            "        import json",
            "",
            "        if as_error:",
            "            target_path = os.path.join(self.watch_data_dir, \"elements-error.json\")",
            "        else:",
            "            target_path = os.path.join(self.watch_data_dir, \"elements.json\")",
            "",
            "        self.ensure_data_dir_exists()",
            "",
            "        with open(target_path, 'w') as f:",
            "            f.write(json.dumps(data))",
            "            f.close()",
            "",
            "    # Save as PNG, PNG is larger but better for doing visual diff in the future",
            "    def save_screenshot(self, screenshot: bytes, as_error=False):",
            "",
            "        if as_error:",
            "            target_path = os.path.join(self.watch_data_dir, \"last-error-screenshot.png\")",
            "        else:",
            "            target_path = os.path.join(self.watch_data_dir, \"last-screenshot.png\")",
            "",
            "        self.ensure_data_dir_exists()",
            "",
            "        with open(target_path, 'wb') as f:",
            "            f.write(screenshot)",
            "            f.close()",
            "",
            "",
            "    def get_last_fetched_text_before_filters(self):",
            "        import brotli",
            "        filepath = os.path.join(self.watch_data_dir, 'last-fetched.br')",
            "",
            "        if not os.path.isfile(filepath):",
            "            # If a previous attempt doesnt yet exist, just snarf the previous snapshot instead",
            "            dates = list(self.history.keys())",
            "            if len(dates):",
            "                return self.get_history_snapshot(dates[-1])",
            "            else:",
            "                return ''",
            "",
            "        with open(filepath, 'rb') as f:",
            "            return(brotli.decompress(f.read()).decode('utf-8'))",
            "",
            "    def save_last_text_fetched_before_filters(self, contents):",
            "        import brotli",
            "        filepath = os.path.join(self.watch_data_dir, 'last-fetched.br')",
            "        with open(filepath, 'wb') as f:",
            "            f.write(brotli.compress(contents, mode=brotli.MODE_TEXT))",
            "",
            "    def save_last_fetched_html(self, timestamp, contents):",
            "        import brotli",
            "",
            "        self.ensure_data_dir_exists()",
            "        snapshot_fname = f\"{timestamp}.html.br\"",
            "        filepath = os.path.join(self.watch_data_dir, snapshot_fname)",
            "",
            "        with open(filepath, 'wb') as f:",
            "            contents = contents.encode('utf-8') if isinstance(contents, str) else contents",
            "            try:",
            "                f.write(brotli.compress(contents))",
            "            except Exception as e:",
            "                logger.warning(f\"{self.get('uuid')} - Unable to compress snapshot, saving as raw data to {filepath}\")",
            "                logger.warning(e)",
            "                f.write(contents)",
            "",
            "        self._prune_last_fetched_html_snapshots()",
            "",
            "    def get_fetched_html(self, timestamp):",
            "        import brotli",
            "",
            "        snapshot_fname = f\"{timestamp}.html.br\"",
            "        filepath = os.path.join(self.watch_data_dir, snapshot_fname)",
            "        if os.path.isfile(filepath):",
            "            with open(filepath, 'rb') as f:",
            "                return (brotli.decompress(f.read()).decode('utf-8'))",
            "",
            "        return False",
            "",
            "",
            "    def _prune_last_fetched_html_snapshots(self):",
            "",
            "        dates = list(self.history.keys())",
            "        dates.reverse()",
            "",
            "        for index, timestamp in enumerate(dates):",
            "            snapshot_fname = f\"{timestamp}.html.br\"",
            "            filepath = os.path.join(self.watch_data_dir, snapshot_fname)",
            "",
            "            # Keep only the first 2",
            "            if index > 1 and os.path.isfile(filepath):",
            "                os.remove(filepath)",
            "",
            "",
            "    @property",
            "    def get_browsersteps_available_screenshots(self):",
            "        \"For knowing which screenshots are available to show the user in BrowserSteps UI\"",
            "        available = []",
            "        for f in Path(self.watch_data_dir).glob('step_before-*.jpeg'):",
            "            step_n=re.search(r'step_before-(\\d+)', f.name)",
            "            if step_n:",
            "                available.append(step_n.group(1))",
            "        return available"
        ],
        "afterPatchFile": [
            "from changedetectionio.strtobool import strtobool",
            "from changedetectionio.safe_jinja import render as jinja_render",
            "from . import watch_base",
            "import os",
            "import re",
            "from pathlib import Path",
            "from loguru import logger",
            "",
            "from ..html_tools import TRANSLATE_WHITESPACE_TABLE",
            "",
            "# Allowable protocols, protects against javascript: etc",
            "# file:// is further checked by ALLOW_FILE_URI",
            "SAFE_PROTOCOL_REGEX='^(http|https|ftp|file):'",
            "",
            "minimum_seconds_recheck_time = int(os.getenv('MINIMUM_SECONDS_RECHECK_TIME', 3))",
            "mtable = {'seconds': 1, 'minutes': 60, 'hours': 3600, 'days': 86400, 'weeks': 86400 * 7}",
            "",
            "",
            "def is_safe_url(test_url):",
            "    # See https://github.com/dgtlmoon/changedetection.io/issues/1358",
            "",
            "    # Remove 'source:' prefix so we dont get 'source:javascript:' etc",
            "    # 'source:' is a valid way to tell us to return the source",
            "",
            "    r = re.compile(re.escape('source:'), re.IGNORECASE)",
            "    test_url = r.sub('', test_url)",
            "",
            "    pattern = re.compile(os.getenv('SAFE_PROTOCOL_REGEX', SAFE_PROTOCOL_REGEX), re.IGNORECASE)",
            "    if not pattern.match(test_url.strip()):",
            "        return False",
            "",
            "    return True",
            "",
            "",
            "class model(watch_base):",
            "    __newest_history_key = None",
            "    __history_n = 0",
            "    jitter_seconds = 0",
            "",
            "    def __init__(self, *arg, **kw):",
            "        self.__datastore_path = kw.get('datastore_path')",
            "        if kw.get('datastore_path'):",
            "            del kw['datastore_path']",
            "        super(model, self).__init__(*arg, **kw)",
            "        if kw.get('default'):",
            "            self.update(kw['default'])",
            "            del kw['default']",
            "",
            "        if self.get('default'):",
            "            del self['default']",
            "",
            "        # Be sure the cached timestamp is ready",
            "        bump = self.history",
            "",
            "    @property",
            "    def viewed(self):",
            "        # Don't return viewed when last_viewed is 0 and newest_key is 0",
            "        if int(self['last_viewed']) and int(self['last_viewed']) >= int(self.newest_history_key) :",
            "            return True",
            "",
            "        return False",
            "",
            "    def ensure_data_dir_exists(self):",
            "        if not os.path.isdir(self.watch_data_dir):",
            "            logger.debug(f\"> Creating data dir {self.watch_data_dir}\")",
            "            os.mkdir(self.watch_data_dir)",
            "",
            "    @property",
            "    def link(self):",
            "",
            "        url = self.get('url', '')",
            "        if not is_safe_url(url):",
            "            return 'DISABLED'",
            "",
            "        ready_url = url",
            "        if '{%' in url or '{{' in url:",
            "            # Jinja2 available in URLs along with https://pypi.org/project/jinja2-time/",
            "            try:",
            "                ready_url = jinja_render(template_str=url)",
            "            except Exception as e:",
            "                logger.critical(f\"Invalid URL template for: '{url}' - {str(e)}\")",
            "                from flask import (",
            "                    flash, Markup, url_for",
            "                )",
            "                message = Markup('<a href=\"{}#general\">The URL {} is invalid and cannot be used, click to edit</a>'.format(",
            "                    url_for('edit_page', uuid=self.get('uuid')), self.get('url', '')))",
            "                flash(message, 'error')",
            "                return ''",
            "",
            "        if ready_url.startswith('source:'):",
            "            ready_url=ready_url.replace('source:', '')",
            "",
            "        # Also double check it after any Jinja2 formatting just incase",
            "        if not is_safe_url(ready_url):",
            "            return 'DISABLED'",
            "        return ready_url",
            "",
            "    def clear_watch(self):",
            "        import pathlib",
            "",
            "        # JSON Data, Screenshots, Textfiles (history index and snapshots), HTML in the future etc",
            "        for item in pathlib.Path(str(self.watch_data_dir)).rglob(\"*.*\"):",
            "            os.unlink(item)",
            "",
            "        # Force the attr to recalculate",
            "        bump = self.history",
            "",
            "        # Do this last because it will trigger a recheck due to last_checked being zero",
            "        self.update({",
            "            'browser_steps_last_error_step': None,",
            "            'check_count': 0,",
            "            'fetch_time': 0.0,",
            "            'has_ldjson_price_data': None,",
            "            'last_checked': 0,",
            "            'last_error': False,",
            "            'last_notification_error': False,",
            "            'last_viewed': 0,",
            "            'previous_md5': False,",
            "            'previous_md5_before_filters': False,",
            "            'remote_server_reply': None,",
            "            'track_ldjson_price_data': None",
            "        })",
            "        return",
            "",
            "    @property",
            "    def is_source_type_url(self):",
            "        return self.get('url', '').startswith('source:')",
            "",
            "    @property",
            "    def get_fetch_backend(self):",
            "        \"\"\"",
            "        Like just using the `fetch_backend` key but there could be some logic",
            "        :return:",
            "        \"\"\"",
            "        # Maybe also if is_image etc?",
            "        # This is because chrome/playwright wont render the PDF in the browser and we will just fetch it and use pdf2html to see the text.",
            "        if self.is_pdf:",
            "            return 'html_requests'",
            "",
            "        return self.get('fetch_backend')",
            "",
            "    @property",
            "    def is_pdf(self):",
            "        # content_type field is set in the future",
            "        # https://github.com/dgtlmoon/changedetection.io/issues/1392",
            "        # Not sure the best logic here",
            "        return self.get('url', '').lower().endswith('.pdf') or 'pdf' in self.get('content_type', '').lower()",
            "",
            "    @property",
            "    def label(self):",
            "        # Used for sorting",
            "        return self.get('title') if self.get('title') else self.get('url')",
            "",
            "    @property",
            "    def last_changed(self):",
            "        # last_changed will be the newest snapshot, but when we have just one snapshot, it should be 0",
            "        if self.__history_n <= 1:",
            "            return 0",
            "        if self.__newest_history_key:",
            "            return int(self.__newest_history_key)",
            "        return 0",
            "",
            "    @property",
            "    def history_n(self):",
            "        return self.__history_n",
            "",
            "    @property",
            "    def history(self):",
            "        \"\"\"History index is just a text file as a list",
            "            {watch-uuid}/history.txt",
            "",
            "            contains a list like",
            "",
            "            {epoch-time},{filename}\\n",
            "",
            "            We read in this list as the history information",
            "",
            "        \"\"\"",
            "        tmp_history = {}",
            "",
            "        # In the case we are only using the watch for processing without history",
            "        if not self.watch_data_dir:",
            "            return []",
            "",
            "        # Read the history file as a dict",
            "        fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        if os.path.isfile(fname):",
            "            logger.debug(f\"Reading watch history index for {self.get('uuid')}\")",
            "            with open(fname, \"r\") as f:",
            "                for i in f.readlines():",
            "                    if ',' in i:",
            "                        k, v = i.strip().split(',', 2)",
            "",
            "                        # The index history could contain a relative path, so we need to make the fullpath",
            "                        # so that python can read it",
            "                        if not '/' in v and not '\\'' in v:",
            "                            v = os.path.join(self.watch_data_dir, v)",
            "                        else:",
            "                            # It's possible that they moved the datadir on older versions",
            "                            # So the snapshot exists but is in a different path",
            "                            snapshot_fname = v.split('/')[-1]",
            "                            proposed_new_path = os.path.join(self.watch_data_dir, snapshot_fname)",
            "                            if not os.path.exists(v) and os.path.exists(proposed_new_path):",
            "                                v = proposed_new_path",
            "",
            "                        tmp_history[k] = v",
            "",
            "        if len(tmp_history):",
            "            self.__newest_history_key = list(tmp_history.keys())[-1]",
            "        else:",
            "            self.__newest_history_key = None",
            "",
            "        self.__history_n = len(tmp_history)",
            "",
            "        return tmp_history",
            "",
            "    @property",
            "    def has_history(self):",
            "        fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        return os.path.isfile(fname)",
            "",
            "    @property",
            "    def has_browser_steps(self):",
            "        has_browser_steps = self.get('browser_steps') and list(filter(",
            "            lambda s: (s['operation'] and len(s['operation']) and s['operation'] != 'Choose one' and s['operation'] != 'Goto site'),",
            "            self.get('browser_steps')))",
            "",
            "        return has_browser_steps",
            "",
            "    @property",
            "    def has_restock_info(self):",
            "        if self.get('restock') and self['restock'].get('in_stock') != None:",
            "                return True",
            "",
            "        return False",
            "",
            "    # Returns the newest key, but if theres only 1 record, then it's counted as not being new, so return 0.",
            "    @property",
            "    def newest_history_key(self):",
            "        if self.__newest_history_key is not None:",
            "            return self.__newest_history_key",
            "",
            "        if len(self.history) <= 1:",
            "            return 0",
            "",
            "",
            "        bump = self.history",
            "        return self.__newest_history_key",
            "",
            "    # Given an arbitrary timestamp, find the closest next key",
            "    # For example, last_viewed = 1000 so it should return the next 1001 timestamp",
            "    #",
            "    # used for the [diff] button so it can preset a smarter from_version",
            "    @property",
            "    def get_next_snapshot_key_to_last_viewed(self):",
            "",
            "        \"\"\"Unfortunately for now timestamp is stored as string key\"\"\"",
            "        keys = list(self.history.keys())",
            "        if not keys:",
            "            return None",
            "",
            "        last_viewed = int(self.get('last_viewed'))",
            "        prev_k = keys[0]",
            "        sorted_keys = sorted(keys, key=lambda x: int(x))",
            "        sorted_keys.reverse()",
            "",
            "        # When the 'last viewed' timestamp is greater than the newest snapshot, return second last",
            "        if last_viewed > int(sorted_keys[0]):",
            "            return sorted_keys[1]",
            "",
            "        for k in sorted_keys:",
            "            if int(k) < last_viewed:",
            "                if prev_k == sorted_keys[0]:",
            "                    # Return the second last one so we dont recommend the same version compares itself",
            "                    return sorted_keys[1]",
            "",
            "                return prev_k",
            "            prev_k = k",
            "",
            "        return keys[0]",
            "",
            "    def get_history_snapshot(self, timestamp):",
            "        import brotli",
            "        filepath = self.history[timestamp]",
            "",
            "        # See if a brotli versions exists and switch to that",
            "        if not filepath.endswith('.br') and os.path.isfile(f\"{filepath}.br\"):",
            "            filepath = f\"{filepath}.br\"",
            "",
            "        # OR in the backup case that the .br does not exist, but the plain one does",
            "        if filepath.endswith('.br') and not os.path.isfile(filepath):",
            "            if os.path.isfile(filepath.replace('.br', '')):",
            "                filepath = filepath.replace('.br', '')",
            "",
            "        if filepath.endswith('.br'):",
            "            # Brotli doesnt have a fileheader to detect it, so we rely on filename",
            "            # https://www.rfc-editor.org/rfc/rfc7932",
            "            with open(filepath, 'rb') as f:",
            "                return(brotli.decompress(f.read()).decode('utf-8'))",
            "",
            "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:",
            "            return f.read()",
            "",
            "    # Save some text file to the appropriate path and bump the history",
            "    # result_obj from fetch_site_status.run()",
            "    def save_history_text(self, contents, timestamp, snapshot_id):",
            "        import brotli",
            "",
            "        logger.trace(f\"{self.get('uuid')} - Updating history.txt with timestamp {timestamp}\")",
            "",
            "        self.ensure_data_dir_exists()",
            "",
            "        threshold = int(os.getenv('SNAPSHOT_BROTLI_COMPRESSION_THRESHOLD', 1024))",
            "        skip_brotli = strtobool(os.getenv('DISABLE_BROTLI_TEXT_SNAPSHOT', 'False'))",
            "",
            "        if not skip_brotli and len(contents) > threshold:",
            "            snapshot_fname = f\"{snapshot_id}.txt.br\"",
            "            dest = os.path.join(self.watch_data_dir, snapshot_fname)",
            "            if not os.path.exists(dest):",
            "                with open(dest, 'wb') as f:",
            "                    f.write(brotli.compress(contents.encode('utf-8'), mode=brotli.MODE_TEXT))",
            "        else:",
            "            snapshot_fname = f\"{snapshot_id}.txt\"",
            "            dest = os.path.join(self.watch_data_dir, snapshot_fname)",
            "            if not os.path.exists(dest):",
            "                with open(dest, 'wb') as f:",
            "                    f.write(contents.encode('utf-8'))",
            "",
            "        # Append to index",
            "        # @todo check last char was \\n",
            "        index_fname = os.path.join(self.watch_data_dir, \"history.txt\")",
            "        with open(index_fname, 'a') as f:",
            "            f.write(\"{},{}\\n\".format(timestamp, snapshot_fname))",
            "            f.close()",
            "",
            "        self.__newest_history_key = timestamp",
            "        self.__history_n += 1",
            "",
            "        # @todo bump static cache of the last timestamp so we dont need to examine the file to set a proper ''viewed'' status",
            "        return snapshot_fname",
            "",
            "    @property",
            "    @property",
            "    def has_empty_checktime(self):",
            "        # using all() + dictionary comprehension",
            "        # Check if all values are 0 in dictionary",
            "        res = all(x == None or x == False or x==0 for x in self.get('time_between_check', {}).values())",
            "        return res",
            "",
            "    def threshold_seconds(self):",
            "        seconds = 0",
            "        for m, n in mtable.items():",
            "            x = self.get('time_between_check', {}).get(m, None)",
            "            if x:",
            "                seconds += x * n",
            "        return seconds",
            "",
            "    # Iterate over all history texts and see if something new exists",
            "    # Always applying .strip() to start/end but optionally replace any other whitespace",
            "    def lines_contain_something_unique_compared_to_history(self, lines: list, ignore_whitespace=False):",
            "        local_lines = []",
            "        if lines:",
            "            if ignore_whitespace:",
            "                if isinstance(lines[0], str): # Can be either str or bytes depending on what was on the disk",
            "                    local_lines = set([l.translate(TRANSLATE_WHITESPACE_TABLE).lower() for l in lines])",
            "                else:",
            "                    local_lines = set([l.decode('utf-8').translate(TRANSLATE_WHITESPACE_TABLE).lower() for l in lines])",
            "            else:",
            "                if isinstance(lines[0], str): # Can be either str or bytes depending on what was on the disk",
            "                    local_lines = set([l.strip().lower() for l in lines])",
            "                else:",
            "                    local_lines = set([l.decode('utf-8').strip().lower() for l in lines])",
            "",
            "",
            "        # Compare each lines (set) against each history text file (set) looking for something new..",
            "        existing_history = set({})",
            "        for k, v in self.history.items():",
            "            content = self.get_history_snapshot(k)",
            "",
            "            if ignore_whitespace:",
            "                alist = set([line.translate(TRANSLATE_WHITESPACE_TABLE).lower() for line in content.splitlines()])",
            "            else:",
            "                alist = set([line.strip().lower() for line in content.splitlines()])",
            "",
            "            existing_history = existing_history.union(alist)",
            "",
            "        # Check that everything in local_lines(new stuff) already exists in existing_history - it should",
            "        # if not, something new happened",
            "        return not local_lines.issubset(existing_history)",
            "",
            "    def get_screenshot(self):",
            "        fname = os.path.join(self.watch_data_dir, \"last-screenshot.png\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "",
            "        # False is not an option for AppRise, must be type None",
            "        return None",
            "",
            "    def __get_file_ctime(self, filename):",
            "        fname = os.path.join(self.watch_data_dir, filename)",
            "        if os.path.isfile(fname):",
            "            return int(os.path.getmtime(fname))",
            "        return False",
            "",
            "    @property",
            "    def error_text_ctime(self):",
            "        return self.__get_file_ctime('last-error.txt')",
            "",
            "    @property",
            "    def snapshot_text_ctime(self):",
            "        if self.history_n==0:",
            "            return False",
            "",
            "        timestamp = list(self.history.keys())[-1]",
            "        return int(timestamp)",
            "",
            "    @property",
            "    def snapshot_screenshot_ctime(self):",
            "        return self.__get_file_ctime('last-screenshot.png')",
            "",
            "    @property",
            "    def snapshot_error_screenshot_ctime(self):",
            "        return self.__get_file_ctime('last-error-screenshot.png')",
            "",
            "    @property",
            "    def watch_data_dir(self):",
            "        # The base dir of the watch data",
            "        return os.path.join(self.__datastore_path, self['uuid']) if self.__datastore_path else None",
            "",
            "    def get_error_text(self):",
            "        \"\"\"Return the text saved from a previous request that resulted in a non-200 error\"\"\"",
            "        fname = os.path.join(self.watch_data_dir, \"last-error.txt\")",
            "        if os.path.isfile(fname):",
            "            with open(fname, 'r') as f:",
            "                return f.read()",
            "        return False",
            "",
            "    def get_error_snapshot(self):",
            "        \"\"\"Return path to the screenshot that resulted in a non-200 error\"\"\"",
            "        fname = os.path.join(self.watch_data_dir, \"last-error-screenshot.png\")",
            "        if os.path.isfile(fname):",
            "            return fname",
            "        return False",
            "",
            "",
            "    def pause(self):",
            "        self['paused'] = True",
            "",
            "    def unpause(self):",
            "        self['paused'] = False",
            "",
            "    def toggle_pause(self):",
            "        self['paused'] ^= True",
            "",
            "    def mute(self):",
            "        self['notification_muted'] = True",
            "",
            "    def unmute(self):",
            "        self['notification_muted'] = False",
            "",
            "    def toggle_mute(self):",
            "        self['notification_muted'] ^= True",
            "",
            "    def extra_notification_token_values(self):",
            "        # Used for providing extra tokens",
            "        # return {'widget': 555}",
            "        return {}",
            "",
            "    def extra_notification_token_placeholder_info(self):",
            "        # Used for providing extra tokens",
            "        # return [('widget', \"Get widget amounts\")]",
            "        return []",
            "",
            "",
            "    def extract_regex_from_all_history(self, regex):",
            "        import csv",
            "        import re",
            "        import datetime",
            "        csv_output_filename = False",
            "        csv_writer = False",
            "        f = None",
            "",
            "        # self.history will be keyed with the full path",
            "        for k, fname in self.history.items():",
            "            if os.path.isfile(fname):",
            "                if True:",
            "                    contents = self.get_history_snapshot(k)",
            "                    res = re.findall(regex, contents, re.MULTILINE)",
            "                    if res:",
            "                        if not csv_writer:",
            "                            # A file on the disk can be transferred much faster via flask than a string reply",
            "                            csv_output_filename = 'report.csv'",
            "                            f = open(os.path.join(self.watch_data_dir, csv_output_filename), 'w')",
            "                            # @todo some headers in the future",
            "                            #fieldnames = ['Epoch seconds', 'Date']",
            "                            csv_writer = csv.writer(f,",
            "                                                    delimiter=',',",
            "                                                    quotechar='\"',",
            "                                                    quoting=csv.QUOTE_MINIMAL,",
            "                                                    #fieldnames=fieldnames",
            "                                                    )",
            "                            csv_writer.writerow(['Epoch seconds', 'Date'])",
            "                            # csv_writer.writeheader()",
            "",
            "                        date_str = datetime.datetime.fromtimestamp(int(k)).strftime('%Y-%m-%d %H:%M:%S')",
            "                        for r in res:",
            "                            row = [k, date_str]",
            "                            if isinstance(r, str):",
            "                                row.append(r)",
            "                            else:",
            "                                row+=r",
            "                            csv_writer.writerow(row)",
            "",
            "        if f:",
            "            f.close()",
            "",
            "        return csv_output_filename",
            "",
            "",
            "    def has_special_diff_filter_options_set(self):",
            "",
            "        # All False - nothing would be done, so act like it's not processable",
            "        if not self.get('filter_text_added', True) and not self.get('filter_text_replaced', True) and not self.get('filter_text_removed', True):",
            "            return False",
            "",
            "        # Or one is set",
            "        if not self.get('filter_text_added', True) or not self.get('filter_text_replaced', True) or not self.get('filter_text_removed', True):",
            "            return True",
            "",
            "        # None is set",
            "        return False",
            "",
            "    def save_error_text(self, contents):",
            "        self.ensure_data_dir_exists()",
            "        target_path = os.path.join(self.watch_data_dir, \"last-error.txt\")",
            "        with open(target_path, 'w') as f:",
            "            f.write(contents)",
            "",
            "    def save_xpath_data(self, data, as_error=False):",
            "        import json",
            "",
            "        if as_error:",
            "            target_path = os.path.join(self.watch_data_dir, \"elements-error.json\")",
            "        else:",
            "            target_path = os.path.join(self.watch_data_dir, \"elements.json\")",
            "",
            "        self.ensure_data_dir_exists()",
            "",
            "        with open(target_path, 'w') as f:",
            "            f.write(json.dumps(data))",
            "            f.close()",
            "",
            "    # Save as PNG, PNG is larger but better for doing visual diff in the future",
            "    def save_screenshot(self, screenshot: bytes, as_error=False):",
            "",
            "        if as_error:",
            "            target_path = os.path.join(self.watch_data_dir, \"last-error-screenshot.png\")",
            "        else:",
            "            target_path = os.path.join(self.watch_data_dir, \"last-screenshot.png\")",
            "",
            "        self.ensure_data_dir_exists()",
            "",
            "        with open(target_path, 'wb') as f:",
            "            f.write(screenshot)",
            "            f.close()",
            "",
            "",
            "    def get_last_fetched_text_before_filters(self):",
            "        import brotli",
            "        filepath = os.path.join(self.watch_data_dir, 'last-fetched.br')",
            "",
            "        if not os.path.isfile(filepath):",
            "            # If a previous attempt doesnt yet exist, just snarf the previous snapshot instead",
            "            dates = list(self.history.keys())",
            "            if len(dates):",
            "                return self.get_history_snapshot(dates[-1])",
            "            else:",
            "                return ''",
            "",
            "        with open(filepath, 'rb') as f:",
            "            return(brotli.decompress(f.read()).decode('utf-8'))",
            "",
            "    def save_last_text_fetched_before_filters(self, contents):",
            "        import brotli",
            "        filepath = os.path.join(self.watch_data_dir, 'last-fetched.br')",
            "        with open(filepath, 'wb') as f:",
            "            f.write(brotli.compress(contents, mode=brotli.MODE_TEXT))",
            "",
            "    def save_last_fetched_html(self, timestamp, contents):",
            "        import brotli",
            "",
            "        self.ensure_data_dir_exists()",
            "        snapshot_fname = f\"{timestamp}.html.br\"",
            "        filepath = os.path.join(self.watch_data_dir, snapshot_fname)",
            "",
            "        with open(filepath, 'wb') as f:",
            "            contents = contents.encode('utf-8') if isinstance(contents, str) else contents",
            "            try:",
            "                f.write(brotli.compress(contents))",
            "            except Exception as e:",
            "                logger.warning(f\"{self.get('uuid')} - Unable to compress snapshot, saving as raw data to {filepath}\")",
            "                logger.warning(e)",
            "                f.write(contents)",
            "",
            "        self._prune_last_fetched_html_snapshots()",
            "",
            "    def get_fetched_html(self, timestamp):",
            "        import brotli",
            "",
            "        snapshot_fname = f\"{timestamp}.html.br\"",
            "        filepath = os.path.join(self.watch_data_dir, snapshot_fname)",
            "        if os.path.isfile(filepath):",
            "            with open(filepath, 'rb') as f:",
            "                return (brotli.decompress(f.read()).decode('utf-8'))",
            "",
            "        return False",
            "",
            "",
            "    def _prune_last_fetched_html_snapshots(self):",
            "",
            "        dates = list(self.history.keys())",
            "        dates.reverse()",
            "",
            "        for index, timestamp in enumerate(dates):",
            "            snapshot_fname = f\"{timestamp}.html.br\"",
            "            filepath = os.path.join(self.watch_data_dir, snapshot_fname)",
            "",
            "            # Keep only the first 2",
            "            if index > 1 and os.path.isfile(filepath):",
            "                os.remove(filepath)",
            "",
            "",
            "    @property",
            "    def get_browsersteps_available_screenshots(self):",
            "        \"For knowing which screenshots are available to show the user in BrowserSteps UI\"",
            "        available = []",
            "        for f in Path(self.watch_data_dir).glob('step_before-*.jpeg'):",
            "            step_n=re.search(r'step_before-(\\d+)', f.name)",
            "            if step_n:",
            "                available.append(step_n.group(1))",
            "        return available"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "changedetectionio/processors/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "         from requests.structures import CaseInsensitiveDict"
            },
            "2": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # Protect against file:// access"
            },
            "4": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if re.search(r'^file://', self.watch.get('url', '').strip(), re.IGNORECASE):"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+        url = self.watch.link"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+        # Protect against file:// access, check the real \"link\" without any meta \"source:\" etc prepended."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+        if re.search(r'^file://', url, re.IGNORECASE):"
            },
            "9": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "             if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):"
            },
            "10": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "                 raise Exception("
            },
            "11": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "                     \"file:// type access is denied for security reasons.\""
            },
            "12": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "                 )"
            },
            "13": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        url = self.watch.link"
            },
            "15": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "16": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "         # Requests, playwright, other browser via wss:// etc, fetch_extra_something"
            },
            "17": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "         prefer_fetch_backend = self.watch.get('fetch_backend', 'system')"
            },
            "18": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "from abc import abstractmethod",
            "from changedetectionio.content_fetchers.base import Fetcher",
            "from changedetectionio.strtobool import strtobool",
            "from copy import deepcopy",
            "from loguru import logger",
            "import hashlib",
            "import importlib",
            "import inspect",
            "import os",
            "import pkgutil",
            "import re",
            "",
            "class difference_detection_processor():",
            "",
            "    browser_steps = None",
            "    datastore = None",
            "    fetcher = None",
            "    screenshot = None",
            "    watch = None",
            "    xpath_data = None",
            "    preferred_proxy = None",
            "",
            "    def __init__(self, *args, datastore, watch_uuid, **kwargs):",
            "        super().__init__(*args, **kwargs)",
            "        self.datastore = datastore",
            "        self.watch = deepcopy(self.datastore.data['watching'].get(watch_uuid))",
            "        # Generic fetcher that should be extended (requests, playwright etc)",
            "        self.fetcher = Fetcher()",
            "",
            "    def call_browser(self, preferred_proxy_id=None):",
            "",
            "        from requests.structures import CaseInsensitiveDict",
            "",
            "        # Protect against file:// access",
            "        if re.search(r'^file://', self.watch.get('url', '').strip(), re.IGNORECASE):",
            "            if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):",
            "                raise Exception(",
            "                    \"file:// type access is denied for security reasons.\"",
            "                )",
            "",
            "        url = self.watch.link",
            "",
            "        # Requests, playwright, other browser via wss:// etc, fetch_extra_something",
            "        prefer_fetch_backend = self.watch.get('fetch_backend', 'system')",
            "",
            "        # Proxy ID \"key\"",
            "        preferred_proxy_id = preferred_proxy_id if preferred_proxy_id else self.datastore.get_preferred_proxy_for_watch(uuid=self.watch.get('uuid'))",
            "",
            "        # Pluggable content self.fetcher",
            "        if not prefer_fetch_backend or prefer_fetch_backend == 'system':",
            "            prefer_fetch_backend = self.datastore.data['settings']['application'].get('fetch_backend')",
            "",
            "        # In the case that the preferred fetcher was a browser config with custom connection URL..",
            "        # @todo - on save watch, if its extra_browser_ then it should be obvious it will use playwright (like if its requests now..)",
            "        custom_browser_connection_url = None",
            "        if prefer_fetch_backend.startswith('extra_browser_'):",
            "            (t, key) = prefer_fetch_backend.split('extra_browser_')",
            "            connection = list(",
            "                filter(lambda s: (s['browser_name'] == key), self.datastore.data['settings']['requests'].get('extra_browsers', [])))",
            "            if connection:",
            "                prefer_fetch_backend = 'html_webdriver'",
            "                custom_browser_connection_url = connection[0].get('browser_connection_url')",
            "",
            "        # PDF should be html_requests because playwright will serve it up (so far) in a embedded page",
            "        # @todo https://github.com/dgtlmoon/changedetection.io/issues/2019",
            "        # @todo needs test to or a fix",
            "        if self.watch.is_pdf:",
            "           prefer_fetch_backend = \"html_requests\"",
            "",
            "        # Grab the right kind of 'fetcher', (playwright, requests, etc)",
            "        from changedetectionio import content_fetchers",
            "        if hasattr(content_fetchers, prefer_fetch_backend):",
            "            # @todo TEMPORARY HACK - SWITCH BACK TO PLAYWRIGHT FOR BROWSERSTEPS",
            "            if prefer_fetch_backend == 'html_webdriver' and self.watch.has_browser_steps:",
            "                # This is never supported in selenium anyway",
            "                logger.warning(\"Using playwright fetcher override for possible puppeteer request in browsersteps, because puppetteer:browser steps is incomplete.\")",
            "                from changedetectionio.content_fetchers.playwright import fetcher as playwright_fetcher",
            "                fetcher_obj = playwright_fetcher",
            "            else:",
            "                fetcher_obj = getattr(content_fetchers, prefer_fetch_backend)",
            "        else:",
            "            # What it referenced doesnt exist, Just use a default",
            "            fetcher_obj = getattr(content_fetchers, \"html_requests\")",
            "",
            "        proxy_url = None",
            "        if preferred_proxy_id:",
            "            # Custom browser endpoints should NOT have a proxy added",
            "            if not prefer_fetch_backend.startswith('extra_browser_'):",
            "                proxy_url = self.datastore.proxy_list.get(preferred_proxy_id).get('url')",
            "                logger.debug(f\"Selected proxy key '{preferred_proxy_id}' as proxy URL '{proxy_url}' for {url}\")",
            "            else:",
            "                logger.debug(f\"Skipping adding proxy data when custom Browser endpoint is specified. \")",
            "",
            "        # Now call the fetcher (playwright/requests/etc) with arguments that only a fetcher would need.",
            "        # When browser_connection_url is None, it method should default to working out whats the best defaults (os env vars etc)",
            "        self.fetcher = fetcher_obj(proxy_override=proxy_url,",
            "                                   custom_browser_connection_url=custom_browser_connection_url",
            "                                   )",
            "",
            "        if self.watch.has_browser_steps:",
            "            self.fetcher.browser_steps = self.watch.get('browser_steps', [])",
            "            self.fetcher.browser_steps_screenshot_path = os.path.join(self.datastore.datastore_path, self.watch.get('uuid'))",
            "",
            "        # Tweak the base config with the per-watch ones",
            "        from changedetectionio.safe_jinja import render as jinja_render",
            "        request_headers = CaseInsensitiveDict()",
            "",
            "        ua = self.datastore.data['settings']['requests'].get('default_ua')",
            "        if ua and ua.get(prefer_fetch_backend):",
            "            request_headers.update({'User-Agent': ua.get(prefer_fetch_backend)})",
            "",
            "        request_headers.update(self.watch.get('headers', {}))",
            "        request_headers.update(self.datastore.get_all_base_headers())",
            "        request_headers.update(self.datastore.get_all_headers_in_textfile_for_watch(uuid=self.watch.get('uuid')))",
            "",
            "        # https://github.com/psf/requests/issues/4525",
            "        # Requests doesnt yet support brotli encoding, so don't put 'br' here, be totally sure that the user cannot",
            "        # do this by accident.",
            "        if 'Accept-Encoding' in request_headers and \"br\" in request_headers['Accept-Encoding']:",
            "            request_headers['Accept-Encoding'] = request_headers['Accept-Encoding'].replace(', br', '')",
            "",
            "        for header_name in request_headers:",
            "            request_headers.update({header_name: jinja_render(template_str=request_headers.get(header_name))})",
            "",
            "        timeout = self.datastore.data['settings']['requests'].get('timeout')",
            "",
            "        request_body = self.watch.get('body')",
            "        if request_body:",
            "            request_body = jinja_render(template_str=self.watch.get('body'))",
            "        ",
            "        request_method = self.watch.get('method')",
            "        ignore_status_codes = self.watch.get('ignore_status_codes', False)",
            "",
            "        # Configurable per-watch or global extra delay before extracting text (for webDriver types)",
            "        system_webdriver_delay = self.datastore.data['settings']['application'].get('webdriver_delay', None)",
            "        if self.watch.get('webdriver_delay'):",
            "            self.fetcher.render_extract_delay = self.watch.get('webdriver_delay')",
            "        elif system_webdriver_delay is not None:",
            "            self.fetcher.render_extract_delay = system_webdriver_delay",
            "",
            "        if self.watch.get('webdriver_js_execute_code') is not None and self.watch.get('webdriver_js_execute_code').strip():",
            "            self.fetcher.webdriver_js_execute_code = self.watch.get('webdriver_js_execute_code')",
            "",
            "        # Requests for PDF's, images etc should be passwd the is_binary flag",
            "        is_binary = self.watch.is_pdf",
            "",
            "        # And here we go! call the right browser with browser-specific settings",
            "        empty_pages_are_a_change = self.datastore.data['settings']['application'].get('empty_pages_are_a_change', False)",
            "",
            "        self.fetcher.run(url=url,",
            "                         timeout=timeout,",
            "                         request_headers=request_headers,",
            "                         request_body=request_body,",
            "                         request_method=request_method,",
            "                         ignore_status_codes=ignore_status_codes,",
            "                         current_include_filters=self.watch.get('include_filters'),",
            "                         is_binary=is_binary,",
            "                         empty_pages_are_a_change=empty_pages_are_a_change",
            "                         )",
            "",
            "        #@todo .quit here could go on close object, so we can run JS if change-detected",
            "        self.fetcher.quit()",
            "",
            "        # After init, call run_changedetection() which will do the actual change-detection",
            "",
            "    @abstractmethod",
            "    def run_changedetection(self, watch):",
            "        update_obj = {'last_notification_error': False, 'last_error': False}",
            "        some_data = 'xxxxx'",
            "        update_obj[\"previous_md5\"] = hashlib.md5(some_data.encode('utf-8')).hexdigest()",
            "        changed_detected = False",
            "        return changed_detected, update_obj, ''.encode('utf-8')",
            "",
            "",
            "def find_sub_packages(package_name):",
            "    \"\"\"",
            "    Find all sub-packages within the given package.",
            "",
            "    :param package_name: The name of the base package to scan for sub-packages.",
            "    :return: A list of sub-package names.",
            "    \"\"\"",
            "    package = importlib.import_module(package_name)",
            "    return [name for _, name, is_pkg in pkgutil.iter_modules(package.__path__) if is_pkg]",
            "",
            "",
            "def find_processors():",
            "    \"\"\"",
            "    Find all subclasses of DifferenceDetectionProcessor in the specified package.",
            "",
            "    :param package_name: The name of the package to scan for processor modules.",
            "    :return: A list of (module, class) tuples.",
            "    \"\"\"",
            "    package_name = \"changedetectionio.processors\"  # Name of the current package/module",
            "",
            "    processors = []",
            "    sub_packages = find_sub_packages(package_name)",
            "",
            "    for sub_package in sub_packages:",
            "        module_name = f\"{package_name}.{sub_package}.processor\"",
            "        try:",
            "            module = importlib.import_module(module_name)",
            "",
            "            # Iterate through all classes in the module",
            "            for name, obj in inspect.getmembers(module, inspect.isclass):",
            "                if issubclass(obj, difference_detection_processor) and obj is not difference_detection_processor:",
            "                    processors.append((module, sub_package))",
            "        except (ModuleNotFoundError, ImportError) as e:",
            "            logger.warning(f\"Failed to import module {module_name}: {e} (find_processors())\")",
            "",
            "    return processors",
            "",
            "",
            "def get_parent_module(module):",
            "    module_name = module.__name__",
            "    if '.' not in module_name:",
            "        return None  # Top-level module has no parent",
            "    parent_module_name = module_name.rsplit('.', 1)[0]",
            "    try:",
            "        return importlib.import_module(parent_module_name)",
            "    except Exception as e:",
            "        pass",
            "",
            "    return False",
            "",
            "",
            "",
            "def get_custom_watch_obj_for_processor(processor_name):",
            "    from changedetectionio.model import Watch",
            "    watch_class = Watch.model",
            "    processor_classes = find_processors()",
            "    custom_watch_obj = next((tpl for tpl in processor_classes if tpl[1] == processor_name), None)",
            "    if custom_watch_obj:",
            "        # Parent of .processor.py COULD have its own Watch implementation",
            "        parent_module = get_parent_module(custom_watch_obj[0])",
            "        if hasattr(parent_module, 'Watch'):",
            "            watch_class = parent_module.Watch",
            "",
            "    return watch_class",
            "",
            "",
            "def available_processors():",
            "    \"\"\"",
            "    Get a list of processors by name and description for the UI elements",
            "    :return: A list :)",
            "    \"\"\"",
            "",
            "    processor_classes = find_processors()",
            "",
            "    available = []",
            "    for package, processor_class in processor_classes:",
            "        available.append((processor_class, package.name))",
            "",
            "    return available"
        ],
        "afterPatchFile": [
            "from abc import abstractmethod",
            "from changedetectionio.content_fetchers.base import Fetcher",
            "from changedetectionio.strtobool import strtobool",
            "from copy import deepcopy",
            "from loguru import logger",
            "import hashlib",
            "import importlib",
            "import inspect",
            "import os",
            "import pkgutil",
            "import re",
            "",
            "class difference_detection_processor():",
            "",
            "    browser_steps = None",
            "    datastore = None",
            "    fetcher = None",
            "    screenshot = None",
            "    watch = None",
            "    xpath_data = None",
            "    preferred_proxy = None",
            "",
            "    def __init__(self, *args, datastore, watch_uuid, **kwargs):",
            "        super().__init__(*args, **kwargs)",
            "        self.datastore = datastore",
            "        self.watch = deepcopy(self.datastore.data['watching'].get(watch_uuid))",
            "        # Generic fetcher that should be extended (requests, playwright etc)",
            "        self.fetcher = Fetcher()",
            "",
            "    def call_browser(self, preferred_proxy_id=None):",
            "",
            "        from requests.structures import CaseInsensitiveDict",
            "",
            "        url = self.watch.link",
            "",
            "        # Protect against file:// access, check the real \"link\" without any meta \"source:\" etc prepended.",
            "        if re.search(r'^file://', url, re.IGNORECASE):",
            "            if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):",
            "                raise Exception(",
            "                    \"file:// type access is denied for security reasons.\"",
            "                )",
            "",
            "        # Requests, playwright, other browser via wss:// etc, fetch_extra_something",
            "        prefer_fetch_backend = self.watch.get('fetch_backend', 'system')",
            "",
            "        # Proxy ID \"key\"",
            "        preferred_proxy_id = preferred_proxy_id if preferred_proxy_id else self.datastore.get_preferred_proxy_for_watch(uuid=self.watch.get('uuid'))",
            "",
            "        # Pluggable content self.fetcher",
            "        if not prefer_fetch_backend or prefer_fetch_backend == 'system':",
            "            prefer_fetch_backend = self.datastore.data['settings']['application'].get('fetch_backend')",
            "",
            "        # In the case that the preferred fetcher was a browser config with custom connection URL..",
            "        # @todo - on save watch, if its extra_browser_ then it should be obvious it will use playwright (like if its requests now..)",
            "        custom_browser_connection_url = None",
            "        if prefer_fetch_backend.startswith('extra_browser_'):",
            "            (t, key) = prefer_fetch_backend.split('extra_browser_')",
            "            connection = list(",
            "                filter(lambda s: (s['browser_name'] == key), self.datastore.data['settings']['requests'].get('extra_browsers', [])))",
            "            if connection:",
            "                prefer_fetch_backend = 'html_webdriver'",
            "                custom_browser_connection_url = connection[0].get('browser_connection_url')",
            "",
            "        # PDF should be html_requests because playwright will serve it up (so far) in a embedded page",
            "        # @todo https://github.com/dgtlmoon/changedetection.io/issues/2019",
            "        # @todo needs test to or a fix",
            "        if self.watch.is_pdf:",
            "           prefer_fetch_backend = \"html_requests\"",
            "",
            "        # Grab the right kind of 'fetcher', (playwright, requests, etc)",
            "        from changedetectionio import content_fetchers",
            "        if hasattr(content_fetchers, prefer_fetch_backend):",
            "            # @todo TEMPORARY HACK - SWITCH BACK TO PLAYWRIGHT FOR BROWSERSTEPS",
            "            if prefer_fetch_backend == 'html_webdriver' and self.watch.has_browser_steps:",
            "                # This is never supported in selenium anyway",
            "                logger.warning(\"Using playwright fetcher override for possible puppeteer request in browsersteps, because puppetteer:browser steps is incomplete.\")",
            "                from changedetectionio.content_fetchers.playwright import fetcher as playwright_fetcher",
            "                fetcher_obj = playwright_fetcher",
            "            else:",
            "                fetcher_obj = getattr(content_fetchers, prefer_fetch_backend)",
            "        else:",
            "            # What it referenced doesnt exist, Just use a default",
            "            fetcher_obj = getattr(content_fetchers, \"html_requests\")",
            "",
            "        proxy_url = None",
            "        if preferred_proxy_id:",
            "            # Custom browser endpoints should NOT have a proxy added",
            "            if not prefer_fetch_backend.startswith('extra_browser_'):",
            "                proxy_url = self.datastore.proxy_list.get(preferred_proxy_id).get('url')",
            "                logger.debug(f\"Selected proxy key '{preferred_proxy_id}' as proxy URL '{proxy_url}' for {url}\")",
            "            else:",
            "                logger.debug(f\"Skipping adding proxy data when custom Browser endpoint is specified. \")",
            "",
            "        # Now call the fetcher (playwright/requests/etc) with arguments that only a fetcher would need.",
            "        # When browser_connection_url is None, it method should default to working out whats the best defaults (os env vars etc)",
            "        self.fetcher = fetcher_obj(proxy_override=proxy_url,",
            "                                   custom_browser_connection_url=custom_browser_connection_url",
            "                                   )",
            "",
            "        if self.watch.has_browser_steps:",
            "            self.fetcher.browser_steps = self.watch.get('browser_steps', [])",
            "            self.fetcher.browser_steps_screenshot_path = os.path.join(self.datastore.datastore_path, self.watch.get('uuid'))",
            "",
            "        # Tweak the base config with the per-watch ones",
            "        from changedetectionio.safe_jinja import render as jinja_render",
            "        request_headers = CaseInsensitiveDict()",
            "",
            "        ua = self.datastore.data['settings']['requests'].get('default_ua')",
            "        if ua and ua.get(prefer_fetch_backend):",
            "            request_headers.update({'User-Agent': ua.get(prefer_fetch_backend)})",
            "",
            "        request_headers.update(self.watch.get('headers', {}))",
            "        request_headers.update(self.datastore.get_all_base_headers())",
            "        request_headers.update(self.datastore.get_all_headers_in_textfile_for_watch(uuid=self.watch.get('uuid')))",
            "",
            "        # https://github.com/psf/requests/issues/4525",
            "        # Requests doesnt yet support brotli encoding, so don't put 'br' here, be totally sure that the user cannot",
            "        # do this by accident.",
            "        if 'Accept-Encoding' in request_headers and \"br\" in request_headers['Accept-Encoding']:",
            "            request_headers['Accept-Encoding'] = request_headers['Accept-Encoding'].replace(', br', '')",
            "",
            "        for header_name in request_headers:",
            "            request_headers.update({header_name: jinja_render(template_str=request_headers.get(header_name))})",
            "",
            "        timeout = self.datastore.data['settings']['requests'].get('timeout')",
            "",
            "        request_body = self.watch.get('body')",
            "        if request_body:",
            "            request_body = jinja_render(template_str=self.watch.get('body'))",
            "        ",
            "        request_method = self.watch.get('method')",
            "        ignore_status_codes = self.watch.get('ignore_status_codes', False)",
            "",
            "        # Configurable per-watch or global extra delay before extracting text (for webDriver types)",
            "        system_webdriver_delay = self.datastore.data['settings']['application'].get('webdriver_delay', None)",
            "        if self.watch.get('webdriver_delay'):",
            "            self.fetcher.render_extract_delay = self.watch.get('webdriver_delay')",
            "        elif system_webdriver_delay is not None:",
            "            self.fetcher.render_extract_delay = system_webdriver_delay",
            "",
            "        if self.watch.get('webdriver_js_execute_code') is not None and self.watch.get('webdriver_js_execute_code').strip():",
            "            self.fetcher.webdriver_js_execute_code = self.watch.get('webdriver_js_execute_code')",
            "",
            "        # Requests for PDF's, images etc should be passwd the is_binary flag",
            "        is_binary = self.watch.is_pdf",
            "",
            "        # And here we go! call the right browser with browser-specific settings",
            "        empty_pages_are_a_change = self.datastore.data['settings']['application'].get('empty_pages_are_a_change', False)",
            "",
            "        self.fetcher.run(url=url,",
            "                         timeout=timeout,",
            "                         request_headers=request_headers,",
            "                         request_body=request_body,",
            "                         request_method=request_method,",
            "                         ignore_status_codes=ignore_status_codes,",
            "                         current_include_filters=self.watch.get('include_filters'),",
            "                         is_binary=is_binary,",
            "                         empty_pages_are_a_change=empty_pages_are_a_change",
            "                         )",
            "",
            "        #@todo .quit here could go on close object, so we can run JS if change-detected",
            "        self.fetcher.quit()",
            "",
            "        # After init, call run_changedetection() which will do the actual change-detection",
            "",
            "    @abstractmethod",
            "    def run_changedetection(self, watch):",
            "        update_obj = {'last_notification_error': False, 'last_error': False}",
            "        some_data = 'xxxxx'",
            "        update_obj[\"previous_md5\"] = hashlib.md5(some_data.encode('utf-8')).hexdigest()",
            "        changed_detected = False",
            "        return changed_detected, update_obj, ''.encode('utf-8')",
            "",
            "",
            "def find_sub_packages(package_name):",
            "    \"\"\"",
            "    Find all sub-packages within the given package.",
            "",
            "    :param package_name: The name of the base package to scan for sub-packages.",
            "    :return: A list of sub-package names.",
            "    \"\"\"",
            "    package = importlib.import_module(package_name)",
            "    return [name for _, name, is_pkg in pkgutil.iter_modules(package.__path__) if is_pkg]",
            "",
            "",
            "def find_processors():",
            "    \"\"\"",
            "    Find all subclasses of DifferenceDetectionProcessor in the specified package.",
            "",
            "    :param package_name: The name of the package to scan for processor modules.",
            "    :return: A list of (module, class) tuples.",
            "    \"\"\"",
            "    package_name = \"changedetectionio.processors\"  # Name of the current package/module",
            "",
            "    processors = []",
            "    sub_packages = find_sub_packages(package_name)",
            "",
            "    for sub_package in sub_packages:",
            "        module_name = f\"{package_name}.{sub_package}.processor\"",
            "        try:",
            "            module = importlib.import_module(module_name)",
            "",
            "            # Iterate through all classes in the module",
            "            for name, obj in inspect.getmembers(module, inspect.isclass):",
            "                if issubclass(obj, difference_detection_processor) and obj is not difference_detection_processor:",
            "                    processors.append((module, sub_package))",
            "        except (ModuleNotFoundError, ImportError) as e:",
            "            logger.warning(f\"Failed to import module {module_name}: {e} (find_processors())\")",
            "",
            "    return processors",
            "",
            "",
            "def get_parent_module(module):",
            "    module_name = module.__name__",
            "    if '.' not in module_name:",
            "        return None  # Top-level module has no parent",
            "    parent_module_name = module_name.rsplit('.', 1)[0]",
            "    try:",
            "        return importlib.import_module(parent_module_name)",
            "    except Exception as e:",
            "        pass",
            "",
            "    return False",
            "",
            "",
            "",
            "def get_custom_watch_obj_for_processor(processor_name):",
            "    from changedetectionio.model import Watch",
            "    watch_class = Watch.model",
            "    processor_classes = find_processors()",
            "    custom_watch_obj = next((tpl for tpl in processor_classes if tpl[1] == processor_name), None)",
            "    if custom_watch_obj:",
            "        # Parent of .processor.py COULD have its own Watch implementation",
            "        parent_module = get_parent_module(custom_watch_obj[0])",
            "        if hasattr(parent_module, 'Watch'):",
            "            watch_class = parent_module.Watch",
            "",
            "    return watch_class",
            "",
            "",
            "def available_processors():",
            "    \"\"\"",
            "    Get a list of processors by name and description for the UI elements",
            "    :return: A list :)",
            "    \"\"\"",
            "",
            "    processor_classes = find_processors()",
            "",
            "    available = []",
            "    for package, processor_class in processor_classes:",
            "        available.append((processor_class, package.name))",
            "",
            "    return available"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "34": [
                "difference_detection_processor",
                "call_browser"
            ],
            "35": [
                "difference_detection_processor",
                "call_browser"
            ],
            "41": [
                "difference_detection_processor",
                "call_browser"
            ],
            "42": [
                "difference_detection_processor",
                "call_browser"
            ]
        },
        "addLocation": []
    }
}