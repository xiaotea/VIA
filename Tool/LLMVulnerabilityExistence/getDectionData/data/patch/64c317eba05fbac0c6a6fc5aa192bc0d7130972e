{
    "libs/community/langchain_community/chains/graph_qa/arangodb.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "     # Specify the maximum amount of AQL Generation attempts that should be made"
            },
            "1": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "     max_aql_generation_attempts: int = 3"
            },
            "2": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 59,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "         return [self.input_key]"
            }
        },
        "frontPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    AQL_FIX_PROMPT,",
            "    AQL_GENERATION_PROMPT,",
            "    AQL_QA_PROMPT,",
            ")",
            "from langchain_community.graphs.arangodb_graph import ArangoGraph",
            "",
            "",
            "class ArangoGraphQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating AQL statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: ArangoGraph = Field(exclude=True)",
            "    aql_generation_chain: LLMChain",
            "    aql_fix_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    # Specifies the maximum number of AQL Query Results to return",
            "    top_k: int = 10",
            "",
            "    # Specifies the set of AQL Query Examples that promote few-shot-learning",
            "    aql_examples: str = \"\"",
            "",
            "    # Specify whether to return the AQL Query in the output dictionary",
            "    return_aql_query: bool = False",
            "",
            "    # Specify whether to return the AQL JSON Result in the output dictionary",
            "    return_aql_result: bool = False",
            "",
            "    # Specify the maximum amount of AQL Generation attempts that should be made",
            "    max_aql_generation_attempts: int = 3",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        return [self.output_key]",
            "",
            "    @property",
            "    def _chain_type(self) -> str:",
            "        return \"graph_aql_chain\"",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = AQL_QA_PROMPT,",
            "        aql_generation_prompt: BasePromptTemplate = AQL_GENERATION_PROMPT,",
            "        aql_fix_prompt: BasePromptTemplate = AQL_FIX_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> ArangoGraphQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        aql_generation_chain = LLMChain(llm=llm, prompt=aql_generation_prompt)",
            "        aql_fix_chain = LLMChain(llm=llm, prompt=aql_fix_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            aql_generation_chain=aql_generation_chain,",
            "            aql_fix_chain=aql_fix_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, Any]:",
            "        \"\"\"",
            "        Generate an AQL statement from user input, use it retrieve a response",
            "        from an ArangoDB Database instance, and respond to the user input",
            "        in natural language.",
            "",
            "        Users can modify the following ArangoGraphQAChain Class Variables:",
            "",
            "        :var top_k: The maximum number of AQL Query Results to return",
            "        :type top_k: int",
            "",
            "        :var aql_examples: A set of AQL Query Examples that are passed to",
            "            the AQL Generation Prompt Template to promote few-shot-learning.",
            "            Defaults to an empty string.",
            "        :type aql_examples: str",
            "",
            "        :var return_aql_query: Whether to return the AQL Query in the",
            "            output dictionary. Defaults to False.",
            "        :type return_aql_query: bool",
            "",
            "        :var return_aql_result: Whether to return the AQL Query in the",
            "            output dictionary. Defaults to False",
            "        :type return_aql_result: bool",
            "",
            "        :var max_aql_generation_attempts: The maximum amount of AQL",
            "            Generation attempts to be made prior to raising the last",
            "            AQL Query Execution Error. Defaults to 3.",
            "        :type max_aql_generation_attempts: int",
            "        \"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        user_input = inputs[self.input_key]",
            "",
            "        #########################",
            "        # Generate AQL Query #",
            "        aql_generation_output = self.aql_generation_chain.run(",
            "            {",
            "                \"adb_schema\": self.graph.schema,",
            "                \"aql_examples\": self.aql_examples,",
            "                \"user_input\": user_input,",
            "            },",
            "            callbacks=callbacks,",
            "        )",
            "        #########################",
            "",
            "        aql_query = \"\"",
            "        aql_error = \"\"",
            "        aql_result = None",
            "        aql_generation_attempt = 1",
            "",
            "        while (",
            "            aql_result is None",
            "            and aql_generation_attempt < self.max_aql_generation_attempts + 1",
            "        ):",
            "            #####################",
            "            # Extract AQL Query #",
            "            pattern = r\"```(?i:aql)?(.*?)```\"",
            "            matches = re.findall(pattern, aql_generation_output, re.DOTALL)",
            "            if not matches:",
            "                _run_manager.on_text(",
            "                    \"Invalid Response: \", end=\"\\n\", verbose=self.verbose",
            "                )",
            "                _run_manager.on_text(",
            "                    aql_generation_output, color=\"red\", end=\"\\n\", verbose=self.verbose",
            "                )",
            "                raise ValueError(f\"Response is Invalid: {aql_generation_output}\")",
            "",
            "            aql_query = matches[0]",
            "            #####################",
            "",
            "            _run_manager.on_text(",
            "                f\"AQL Query ({aql_generation_attempt}):\", verbose=self.verbose",
            "            )",
            "            _run_manager.on_text(",
            "                aql_query, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            #####################",
            "            # Execute AQL Query #",
            "            from arango import AQLQueryExecuteError",
            "",
            "            try:",
            "                aql_result = self.graph.query(aql_query, self.top_k)",
            "            except AQLQueryExecuteError as e:",
            "                aql_error = e.error_message",
            "",
            "                _run_manager.on_text(",
            "                    \"AQL Query Execution Error: \", end=\"\\n\", verbose=self.verbose",
            "                )",
            "                _run_manager.on_text(",
            "                    aql_error, color=\"yellow\", end=\"\\n\\n\", verbose=self.verbose",
            "                )",
            "",
            "                ########################",
            "                # Retry AQL Generation #",
            "                aql_generation_output = self.aql_fix_chain.run(",
            "                    {",
            "                        \"adb_schema\": self.graph.schema,",
            "                        \"aql_query\": aql_query,",
            "                        \"aql_error\": aql_error,",
            "                    },",
            "                    callbacks=callbacks,",
            "                )",
            "                ########################",
            "",
            "            #####################",
            "",
            "            aql_generation_attempt += 1",
            "",
            "        if aql_result is None:",
            "            m = f\"\"\"",
            "                Maximum amount of AQL Query Generation attempts reached.",
            "                Unable to execute the AQL Query due to the following error:",
            "                {aql_error}",
            "            \"\"\"",
            "            raise ValueError(m)",
            "",
            "        _run_manager.on_text(\"AQL Result:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            str(aql_result), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        ########################",
            "        # Interpret AQL Result #",
            "        result = self.qa_chain(",
            "            {",
            "                \"adb_schema\": self.graph.schema,",
            "                \"user_input\": user_input,",
            "                \"aql_query\": aql_query,",
            "                \"aql_result\": aql_result,",
            "            },",
            "            callbacks=callbacks,",
            "        )",
            "        ########################",
            "",
            "        # Return results #",
            "        result = {self.output_key: result[self.qa_chain.output_key]}",
            "",
            "        if self.return_aql_query:",
            "            result[\"aql_query\"] = aql_query",
            "",
            "        if self.return_aql_result:",
            "            result[\"aql_result\"] = aql_result",
            "",
            "        return result"
        ],
        "afterPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    AQL_FIX_PROMPT,",
            "    AQL_GENERATION_PROMPT,",
            "    AQL_QA_PROMPT,",
            ")",
            "from langchain_community.graphs.arangodb_graph import ArangoGraph",
            "",
            "",
            "class ArangoGraphQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating AQL statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: ArangoGraph = Field(exclude=True)",
            "    aql_generation_chain: LLMChain",
            "    aql_fix_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    # Specifies the maximum number of AQL Query Results to return",
            "    top_k: int = 10",
            "",
            "    # Specifies the set of AQL Query Examples that promote few-shot-learning",
            "    aql_examples: str = \"\"",
            "",
            "    # Specify whether to return the AQL Query in the output dictionary",
            "    return_aql_query: bool = False",
            "",
            "    # Specify whether to return the AQL JSON Result in the output dictionary",
            "    return_aql_result: bool = False",
            "",
            "    # Specify the maximum amount of AQL Generation attempts that should be made",
            "    max_aql_generation_attempts: int = 3",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        return [self.output_key]",
            "",
            "    @property",
            "    def _chain_type(self) -> str:",
            "        return \"graph_aql_chain\"",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = AQL_QA_PROMPT,",
            "        aql_generation_prompt: BasePromptTemplate = AQL_GENERATION_PROMPT,",
            "        aql_fix_prompt: BasePromptTemplate = AQL_FIX_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> ArangoGraphQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        aql_generation_chain = LLMChain(llm=llm, prompt=aql_generation_prompt)",
            "        aql_fix_chain = LLMChain(llm=llm, prompt=aql_fix_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            aql_generation_chain=aql_generation_chain,",
            "            aql_fix_chain=aql_fix_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, Any]:",
            "        \"\"\"",
            "        Generate an AQL statement from user input, use it retrieve a response",
            "        from an ArangoDB Database instance, and respond to the user input",
            "        in natural language.",
            "",
            "        Users can modify the following ArangoGraphQAChain Class Variables:",
            "",
            "        :var top_k: The maximum number of AQL Query Results to return",
            "        :type top_k: int",
            "",
            "        :var aql_examples: A set of AQL Query Examples that are passed to",
            "            the AQL Generation Prompt Template to promote few-shot-learning.",
            "            Defaults to an empty string.",
            "        :type aql_examples: str",
            "",
            "        :var return_aql_query: Whether to return the AQL Query in the",
            "            output dictionary. Defaults to False.",
            "        :type return_aql_query: bool",
            "",
            "        :var return_aql_result: Whether to return the AQL Query in the",
            "            output dictionary. Defaults to False",
            "        :type return_aql_result: bool",
            "",
            "        :var max_aql_generation_attempts: The maximum amount of AQL",
            "            Generation attempts to be made prior to raising the last",
            "            AQL Query Execution Error. Defaults to 3.",
            "        :type max_aql_generation_attempts: int",
            "        \"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        user_input = inputs[self.input_key]",
            "",
            "        #########################",
            "        # Generate AQL Query #",
            "        aql_generation_output = self.aql_generation_chain.run(",
            "            {",
            "                \"adb_schema\": self.graph.schema,",
            "                \"aql_examples\": self.aql_examples,",
            "                \"user_input\": user_input,",
            "            },",
            "            callbacks=callbacks,",
            "        )",
            "        #########################",
            "",
            "        aql_query = \"\"",
            "        aql_error = \"\"",
            "        aql_result = None",
            "        aql_generation_attempt = 1",
            "",
            "        while (",
            "            aql_result is None",
            "            and aql_generation_attempt < self.max_aql_generation_attempts + 1",
            "        ):",
            "            #####################",
            "            # Extract AQL Query #",
            "            pattern = r\"```(?i:aql)?(.*?)```\"",
            "            matches = re.findall(pattern, aql_generation_output, re.DOTALL)",
            "            if not matches:",
            "                _run_manager.on_text(",
            "                    \"Invalid Response: \", end=\"\\n\", verbose=self.verbose",
            "                )",
            "                _run_manager.on_text(",
            "                    aql_generation_output, color=\"red\", end=\"\\n\", verbose=self.verbose",
            "                )",
            "                raise ValueError(f\"Response is Invalid: {aql_generation_output}\")",
            "",
            "            aql_query = matches[0]",
            "            #####################",
            "",
            "            _run_manager.on_text(",
            "                f\"AQL Query ({aql_generation_attempt}):\", verbose=self.verbose",
            "            )",
            "            _run_manager.on_text(",
            "                aql_query, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            #####################",
            "            # Execute AQL Query #",
            "            from arango import AQLQueryExecuteError",
            "",
            "            try:",
            "                aql_result = self.graph.query(aql_query, self.top_k)",
            "            except AQLQueryExecuteError as e:",
            "                aql_error = e.error_message",
            "",
            "                _run_manager.on_text(",
            "                    \"AQL Query Execution Error: \", end=\"\\n\", verbose=self.verbose",
            "                )",
            "                _run_manager.on_text(",
            "                    aql_error, color=\"yellow\", end=\"\\n\\n\", verbose=self.verbose",
            "                )",
            "",
            "                ########################",
            "                # Retry AQL Generation #",
            "                aql_generation_output = self.aql_fix_chain.run(",
            "                    {",
            "                        \"adb_schema\": self.graph.schema,",
            "                        \"aql_query\": aql_query,",
            "                        \"aql_error\": aql_error,",
            "                    },",
            "                    callbacks=callbacks,",
            "                )",
            "                ########################",
            "",
            "            #####################",
            "",
            "            aql_generation_attempt += 1",
            "",
            "        if aql_result is None:",
            "            m = f\"\"\"",
            "                Maximum amount of AQL Query Generation attempts reached.",
            "                Unable to execute the AQL Query due to the following error:",
            "                {aql_error}",
            "            \"\"\"",
            "            raise ValueError(m)",
            "",
            "        _run_manager.on_text(\"AQL Result:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            str(aql_result), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        ########################",
            "        # Interpret AQL Result #",
            "        result = self.qa_chain(",
            "            {",
            "                \"adb_schema\": self.graph.schema,",
            "                \"user_input\": user_input,",
            "                \"aql_query\": aql_query,",
            "                \"aql_result\": aql_result,",
            "            },",
            "            callbacks=callbacks,",
            "        )",
            "        ########################",
            "",
            "        # Return results #",
            "        result = {self.output_key: result[self.qa_chain.output_key]}",
            "",
            "        if self.return_aql_query:",
            "            result[\"aql_query\"] = aql_query",
            "",
            "        if self.return_aql_result:",
            "            result[\"aql_result\"] = aql_result",
            "",
            "        return result"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.arangodb.ArangoGraphQAChain",
            "libs.community.langchain_community.chains.graph_qa.arangodb.ArangoGraphQAChain.from_llm",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default",
            "libs.community.langchain_community.chains.graph_qa.arangodb.ArangoGraphQAChain.self"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/cypher.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "     \"\"\"Optional cypher validation tool\"\"\""
            },
            "1": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "     use_function_response: bool = False"
            },
            "2": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "     \"\"\"Whether to wrap the database context as tool/function response\"\"\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+    "
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 193,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 213,
                "PatchRowcode": " "
            },
            "34": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            }
        },
        "frontPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional, Union",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.messages import (",
            "    AIMessage,",
            "    BaseMessage,",
            "    SystemMessage,",
            "    ToolMessage,",
            ")",
            "from langchain_core.output_parsers import StrOutputParser",
            "from langchain_core.prompts import (",
            "    BasePromptTemplate,",
            "    ChatPromptTemplate,",
            "    HumanMessagePromptTemplate,",
            "    MessagesPlaceholder,",
            ")",
            "from langchain_core.pydantic_v1 import Field",
            "from langchain_core.runnables import Runnable",
            "",
            "from langchain_community.chains.graph_qa.cypher_utils import (",
            "    CypherQueryCorrector,",
            "    Schema,",
            ")",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_GENERATION_PROMPT,",
            "    CYPHER_QA_PROMPT,",
            ")",
            "from langchain_community.graphs.graph_store import GraphStore",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "FUNCTION_RESPONSE_SYSTEM = \"\"\"You are an assistant that helps to form nice and human ",
            "understandable answers based on the provided information from tools.",
            "Do not add any other information that wasn't present in the tools, and use ",
            "very concise style in interpreting results!",
            "\"\"\"",
            "",
            "",
            "def extract_cypher(text: str) -> str:",
            "    \"\"\"Extract Cypher code from a text.",
            "",
            "    Args:",
            "        text: Text to extract Cypher code from.",
            "",
            "    Returns:",
            "        Cypher code extracted from the text.",
            "    \"\"\"",
            "    # The pattern to find Cypher code enclosed in triple backticks",
            "    pattern = r\"```(.*?)```\"",
            "",
            "    # Find all matches in the input text",
            "    matches = re.findall(pattern, text, re.DOTALL)",
            "",
            "    return matches[0] if matches else text",
            "",
            "",
            "def construct_schema(",
            "    structured_schema: Dict[str, Any],",
            "    include_types: List[str],",
            "    exclude_types: List[str],",
            ") -> str:",
            "    \"\"\"Filter the schema based on included or excluded types\"\"\"",
            "",
            "    def filter_func(x: str) -> bool:",
            "        return x in include_types if include_types else x not in exclude_types",
            "",
            "    filtered_schema: Dict[str, Any] = {",
            "        \"node_props\": {",
            "            k: v",
            "            for k, v in structured_schema.get(\"node_props\", {}).items()",
            "            if filter_func(k)",
            "        },",
            "        \"rel_props\": {",
            "            k: v",
            "            for k, v in structured_schema.get(\"rel_props\", {}).items()",
            "            if filter_func(k)",
            "        },",
            "        \"relationships\": [",
            "            r",
            "            for r in structured_schema.get(\"relationships\", [])",
            "            if all(filter_func(r[t]) for t in [\"start\", \"end\", \"type\"])",
            "        ],",
            "    }",
            "",
            "    # Format node properties",
            "    formatted_node_props = []",
            "    for label, properties in filtered_schema[\"node_props\"].items():",
            "        props_str = \", \".join(",
            "            [f\"{prop['property']}: {prop['type']}\" for prop in properties]",
            "        )",
            "        formatted_node_props.append(f\"{label} {{{props_str}}}\")",
            "",
            "    # Format relationship properties",
            "    formatted_rel_props = []",
            "    for rel_type, properties in filtered_schema[\"rel_props\"].items():",
            "        props_str = \", \".join(",
            "            [f\"{prop['property']}: {prop['type']}\" for prop in properties]",
            "        )",
            "        formatted_rel_props.append(f\"{rel_type} {{{props_str}}}\")",
            "",
            "    # Format relationships",
            "    formatted_rels = [",
            "        f\"(:{el['start']})-[:{el['type']}]->(:{el['end']})\"",
            "        for el in filtered_schema[\"relationships\"]",
            "    ]",
            "",
            "    return \"\\n\".join(",
            "        [",
            "            \"Node properties are the following:\",",
            "            \",\".join(formatted_node_props),",
            "            \"Relationship properties are the following:\",",
            "            \",\".join(formatted_rel_props),",
            "            \"The relationships are the following:\",",
            "            \",\".join(formatted_rels),",
            "        ]",
            "    )",
            "",
            "",
            "def get_function_response(",
            "    question: str, context: List[Dict[str, Any]]",
            ") -> List[BaseMessage]:",
            "    TOOL_ID = \"call_H7fABDuzEau48T10Qn0Lsh0D\"",
            "    messages = [",
            "        AIMessage(",
            "            content=\"\",",
            "            additional_kwargs={",
            "                \"tool_calls\": [",
            "                    {",
            "                        \"id\": TOOL_ID,",
            "                        \"function\": {",
            "                            \"arguments\": '{\"question\":\"' + question + '\"}',",
            "                            \"name\": \"GetInformation\",",
            "                        },",
            "                        \"type\": \"function\",",
            "                    }",
            "                ]",
            "            },",
            "        ),",
            "        ToolMessage(content=str(context), tool_call_id=TOOL_ID),",
            "    ]",
            "    return messages",
            "",
            "",
            "class GraphCypherQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating Cypher statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: GraphStore = Field(exclude=True)",
            "    cypher_generation_chain: LLMChain",
            "    qa_chain: Union[LLMChain, Runnable]",
            "    graph_schema: str",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 10",
            "    \"\"\"Number of results to return from the query\"\"\"",
            "    return_intermediate_steps: bool = False",
            "    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"",
            "    return_direct: bool = False",
            "    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"",
            "    cypher_query_corrector: Optional[CypherQueryCorrector] = None",
            "    \"\"\"Optional cypher validation tool\"\"\"",
            "    use_function_response: bool = False",
            "    \"\"\"Whether to wrap the database context as tool/function response\"\"\"",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @property",
            "    def _chain_type(self) -> str:",
            "        return \"graph_cypher_chain\"",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: Optional[BaseLanguageModel] = None,",
            "        *,",
            "        qa_prompt: Optional[BasePromptTemplate] = None,",
            "        cypher_prompt: Optional[BasePromptTemplate] = None,",
            "        cypher_llm: Optional[BaseLanguageModel] = None,",
            "        qa_llm: Optional[Union[BaseLanguageModel, Any]] = None,",
            "        exclude_types: List[str] = [],",
            "        include_types: List[str] = [],",
            "        validate_cypher: bool = False,",
            "        qa_llm_kwargs: Optional[Dict[str, Any]] = None,",
            "        cypher_llm_kwargs: Optional[Dict[str, Any]] = None,",
            "        use_function_response: bool = False,",
            "        function_response_system: str = FUNCTION_RESPONSE_SYSTEM,",
            "        **kwargs: Any,",
            "    ) -> GraphCypherQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "",
            "        if not cypher_llm and not llm:",
            "            raise ValueError(\"Either `llm` or `cypher_llm` parameters must be provided\")",
            "        if not qa_llm and not llm:",
            "            raise ValueError(\"Either `llm` or `qa_llm` parameters must be provided\")",
            "        if cypher_llm and qa_llm and llm:",
            "            raise ValueError(",
            "                \"You can specify up to two of 'cypher_llm', 'qa_llm'\"",
            "                \", and 'llm', but not all three simultaneously.\"",
            "            )",
            "        if cypher_prompt and cypher_llm_kwargs:",
            "            raise ValueError(",
            "                \"Specifying cypher_prompt and cypher_llm_kwargs together is\"",
            "                \" not allowed. Please pass prompt via cypher_llm_kwargs.\"",
            "            )",
            "        if qa_prompt and qa_llm_kwargs:",
            "            raise ValueError(",
            "                \"Specifying qa_prompt and qa_llm_kwargs together is\"",
            "                \" not allowed. Please pass prompt via qa_llm_kwargs.\"",
            "            )",
            "        use_qa_llm_kwargs = qa_llm_kwargs if qa_llm_kwargs is not None else {}",
            "        use_cypher_llm_kwargs = (",
            "            cypher_llm_kwargs if cypher_llm_kwargs is not None else {}",
            "        )",
            "        if \"prompt\" not in use_qa_llm_kwargs:",
            "            use_qa_llm_kwargs[\"prompt\"] = (",
            "                qa_prompt if qa_prompt is not None else CYPHER_QA_PROMPT",
            "            )",
            "        if \"prompt\" not in use_cypher_llm_kwargs:",
            "            use_cypher_llm_kwargs[\"prompt\"] = (",
            "                cypher_prompt if cypher_prompt is not None else CYPHER_GENERATION_PROMPT",
            "            )",
            "",
            "        qa_llm = qa_llm or llm",
            "        if use_function_response:",
            "            try:",
            "                qa_llm.bind_tools({})  # type: ignore[union-attr]",
            "                response_prompt = ChatPromptTemplate.from_messages(",
            "                    [",
            "                        SystemMessage(content=function_response_system),",
            "                        HumanMessagePromptTemplate.from_template(\"{question}\"),",
            "                        MessagesPlaceholder(variable_name=\"function_response\"),",
            "                    ]",
            "                )",
            "                qa_chain = response_prompt | qa_llm | StrOutputParser()  # type: ignore",
            "            except (NotImplementedError, AttributeError):",
            "                raise ValueError(\"Provided LLM does not support native tools/functions\")",
            "        else:",
            "            qa_chain = LLMChain(llm=qa_llm, **use_qa_llm_kwargs)  # type: ignore[arg-type]",
            "",
            "        cypher_generation_chain = LLMChain(",
            "            llm=cypher_llm or llm,  # type: ignore[arg-type]",
            "            **use_cypher_llm_kwargs,  # type: ignore[arg-type]",
            "        )",
            "",
            "        if exclude_types and include_types:",
            "            raise ValueError(",
            "                \"Either `exclude_types` or `include_types` \"",
            "                \"can be provided, but not both\"",
            "            )",
            "        graph_schema = construct_schema(",
            "            kwargs[\"graph\"].get_structured_schema, include_types, exclude_types",
            "        )",
            "",
            "        cypher_query_corrector = None",
            "        if validate_cypher:",
            "            corrector_schema = [",
            "                Schema(el[\"start\"], el[\"type\"], el[\"end\"])",
            "                for el in kwargs[\"graph\"].structured_schema.get(\"relationships\")",
            "            ]",
            "            cypher_query_corrector = CypherQueryCorrector(corrector_schema)",
            "",
            "        return cls(",
            "            graph_schema=graph_schema,",
            "            qa_chain=qa_chain,",
            "            cypher_generation_chain=cypher_generation_chain,",
            "            cypher_query_corrector=cypher_query_corrector,",
            "            use_function_response=use_function_response,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, Any]:",
            "        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "        args = {",
            "            \"question\": question,",
            "            \"schema\": self.graph_schema,",
            "        }",
            "        args.update(inputs)",
            "",
            "        intermediate_steps: List = []",
            "",
            "        generated_cypher = self.cypher_generation_chain.run(args, callbacks=callbacks)",
            "",
            "        # Extract Cypher code if it is wrapped in backticks",
            "        generated_cypher = extract_cypher(generated_cypher)",
            "",
            "        # Correct Cypher query if enabled",
            "        if self.cypher_query_corrector:",
            "            generated_cypher = self.cypher_query_corrector(generated_cypher)",
            "",
            "        _run_manager.on_text(\"Generated Cypher:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_cypher, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_cypher})",
            "",
            "        # Retrieve and limit the number of results",
            "        # Generated Cypher be null if query corrector identifies invalid schema",
            "        if generated_cypher:",
            "            context = self.graph.query(generated_cypher)[: self.top_k]",
            "        else:",
            "            context = []",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "            if self.use_function_response:",
            "                function_response = get_function_response(question, context)",
            "                final_result = self.qa_chain.invoke(  # type: ignore",
            "                    {\"question\": question, \"function_response\": function_response},",
            "                )",
            "            else:",
            "                result = self.qa_chain.invoke(  # type: ignore",
            "                    {\"question\": question, \"context\": context},",
            "                    callbacks=callbacks,",
            "                )",
            "                final_result = result[self.qa_chain.output_key]  # type: ignore",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result"
        ],
        "afterPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional, Union",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.messages import (",
            "    AIMessage,",
            "    BaseMessage,",
            "    SystemMessage,",
            "    ToolMessage,",
            ")",
            "from langchain_core.output_parsers import StrOutputParser",
            "from langchain_core.prompts import (",
            "    BasePromptTemplate,",
            "    ChatPromptTemplate,",
            "    HumanMessagePromptTemplate,",
            "    MessagesPlaceholder,",
            ")",
            "from langchain_core.pydantic_v1 import Field",
            "from langchain_core.runnables import Runnable",
            "",
            "from langchain_community.chains.graph_qa.cypher_utils import (",
            "    CypherQueryCorrector,",
            "    Schema,",
            ")",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_GENERATION_PROMPT,",
            "    CYPHER_QA_PROMPT,",
            ")",
            "from langchain_community.graphs.graph_store import GraphStore",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "FUNCTION_RESPONSE_SYSTEM = \"\"\"You are an assistant that helps to form nice and human ",
            "understandable answers based on the provided information from tools.",
            "Do not add any other information that wasn't present in the tools, and use ",
            "very concise style in interpreting results!",
            "\"\"\"",
            "",
            "",
            "def extract_cypher(text: str) -> str:",
            "    \"\"\"Extract Cypher code from a text.",
            "",
            "    Args:",
            "        text: Text to extract Cypher code from.",
            "",
            "    Returns:",
            "        Cypher code extracted from the text.",
            "    \"\"\"",
            "    # The pattern to find Cypher code enclosed in triple backticks",
            "    pattern = r\"```(.*?)```\"",
            "",
            "    # Find all matches in the input text",
            "    matches = re.findall(pattern, text, re.DOTALL)",
            "",
            "    return matches[0] if matches else text",
            "",
            "",
            "def construct_schema(",
            "    structured_schema: Dict[str, Any],",
            "    include_types: List[str],",
            "    exclude_types: List[str],",
            ") -> str:",
            "    \"\"\"Filter the schema based on included or excluded types\"\"\"",
            "",
            "    def filter_func(x: str) -> bool:",
            "        return x in include_types if include_types else x not in exclude_types",
            "",
            "    filtered_schema: Dict[str, Any] = {",
            "        \"node_props\": {",
            "            k: v",
            "            for k, v in structured_schema.get(\"node_props\", {}).items()",
            "            if filter_func(k)",
            "        },",
            "        \"rel_props\": {",
            "            k: v",
            "            for k, v in structured_schema.get(\"rel_props\", {}).items()",
            "            if filter_func(k)",
            "        },",
            "        \"relationships\": [",
            "            r",
            "            for r in structured_schema.get(\"relationships\", [])",
            "            if all(filter_func(r[t]) for t in [\"start\", \"end\", \"type\"])",
            "        ],",
            "    }",
            "",
            "    # Format node properties",
            "    formatted_node_props = []",
            "    for label, properties in filtered_schema[\"node_props\"].items():",
            "        props_str = \", \".join(",
            "            [f\"{prop['property']}: {prop['type']}\" for prop in properties]",
            "        )",
            "        formatted_node_props.append(f\"{label} {{{props_str}}}\")",
            "",
            "    # Format relationship properties",
            "    formatted_rel_props = []",
            "    for rel_type, properties in filtered_schema[\"rel_props\"].items():",
            "        props_str = \", \".join(",
            "            [f\"{prop['property']}: {prop['type']}\" for prop in properties]",
            "        )",
            "        formatted_rel_props.append(f\"{rel_type} {{{props_str}}}\")",
            "",
            "    # Format relationships",
            "    formatted_rels = [",
            "        f\"(:{el['start']})-[:{el['type']}]->(:{el['end']})\"",
            "        for el in filtered_schema[\"relationships\"]",
            "    ]",
            "",
            "    return \"\\n\".join(",
            "        [",
            "            \"Node properties are the following:\",",
            "            \",\".join(formatted_node_props),",
            "            \"Relationship properties are the following:\",",
            "            \",\".join(formatted_rel_props),",
            "            \"The relationships are the following:\",",
            "            \",\".join(formatted_rels),",
            "        ]",
            "    )",
            "",
            "",
            "def get_function_response(",
            "    question: str, context: List[Dict[str, Any]]",
            ") -> List[BaseMessage]:",
            "    TOOL_ID = \"call_H7fABDuzEau48T10Qn0Lsh0D\"",
            "    messages = [",
            "        AIMessage(",
            "            content=\"\",",
            "            additional_kwargs={",
            "                \"tool_calls\": [",
            "                    {",
            "                        \"id\": TOOL_ID,",
            "                        \"function\": {",
            "                            \"arguments\": '{\"question\":\"' + question + '\"}',",
            "                            \"name\": \"GetInformation\",",
            "                        },",
            "                        \"type\": \"function\",",
            "                    }",
            "                ]",
            "            },",
            "        ),",
            "        ToolMessage(content=str(context), tool_call_id=TOOL_ID),",
            "    ]",
            "    return messages",
            "",
            "",
            "class GraphCypherQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating Cypher statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: GraphStore = Field(exclude=True)",
            "    cypher_generation_chain: LLMChain",
            "    qa_chain: Union[LLMChain, Runnable]",
            "    graph_schema: str",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 10",
            "    \"\"\"Number of results to return from the query\"\"\"",
            "    return_intermediate_steps: bool = False",
            "    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"",
            "    return_direct: bool = False",
            "    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"",
            "    cypher_query_corrector: Optional[CypherQueryCorrector] = None",
            "    \"\"\"Optional cypher validation tool\"\"\"",
            "    use_function_response: bool = False",
            "    \"\"\"Whether to wrap the database context as tool/function response\"\"\"",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "    ",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @property",
            "    def _chain_type(self) -> str:",
            "        return \"graph_cypher_chain\"",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: Optional[BaseLanguageModel] = None,",
            "        *,",
            "        qa_prompt: Optional[BasePromptTemplate] = None,",
            "        cypher_prompt: Optional[BasePromptTemplate] = None,",
            "        cypher_llm: Optional[BaseLanguageModel] = None,",
            "        qa_llm: Optional[Union[BaseLanguageModel, Any]] = None,",
            "        exclude_types: List[str] = [],",
            "        include_types: List[str] = [],",
            "        validate_cypher: bool = False,",
            "        qa_llm_kwargs: Optional[Dict[str, Any]] = None,",
            "        cypher_llm_kwargs: Optional[Dict[str, Any]] = None,",
            "        use_function_response: bool = False,",
            "        function_response_system: str = FUNCTION_RESPONSE_SYSTEM,",
            "        **kwargs: Any,",
            "    ) -> GraphCypherQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "",
            "        if not cypher_llm and not llm:",
            "            raise ValueError(\"Either `llm` or `cypher_llm` parameters must be provided\")",
            "        if not qa_llm and not llm:",
            "            raise ValueError(\"Either `llm` or `qa_llm` parameters must be provided\")",
            "        if cypher_llm and qa_llm and llm:",
            "            raise ValueError(",
            "                \"You can specify up to two of 'cypher_llm', 'qa_llm'\"",
            "                \", and 'llm', but not all three simultaneously.\"",
            "            )",
            "        if cypher_prompt and cypher_llm_kwargs:",
            "            raise ValueError(",
            "                \"Specifying cypher_prompt and cypher_llm_kwargs together is\"",
            "                \" not allowed. Please pass prompt via cypher_llm_kwargs.\"",
            "            )",
            "        if qa_prompt and qa_llm_kwargs:",
            "            raise ValueError(",
            "                \"Specifying qa_prompt and qa_llm_kwargs together is\"",
            "                \" not allowed. Please pass prompt via qa_llm_kwargs.\"",
            "            )",
            "        use_qa_llm_kwargs = qa_llm_kwargs if qa_llm_kwargs is not None else {}",
            "        use_cypher_llm_kwargs = (",
            "            cypher_llm_kwargs if cypher_llm_kwargs is not None else {}",
            "        )",
            "        if \"prompt\" not in use_qa_llm_kwargs:",
            "            use_qa_llm_kwargs[\"prompt\"] = (",
            "                qa_prompt if qa_prompt is not None else CYPHER_QA_PROMPT",
            "            )",
            "        if \"prompt\" not in use_cypher_llm_kwargs:",
            "            use_cypher_llm_kwargs[\"prompt\"] = (",
            "                cypher_prompt if cypher_prompt is not None else CYPHER_GENERATION_PROMPT",
            "            )",
            "",
            "        qa_llm = qa_llm or llm",
            "        if use_function_response:",
            "            try:",
            "                qa_llm.bind_tools({})  # type: ignore[union-attr]",
            "                response_prompt = ChatPromptTemplate.from_messages(",
            "                    [",
            "                        SystemMessage(content=function_response_system),",
            "                        HumanMessagePromptTemplate.from_template(\"{question}\"),",
            "                        MessagesPlaceholder(variable_name=\"function_response\"),",
            "                    ]",
            "                )",
            "                qa_chain = response_prompt | qa_llm | StrOutputParser()  # type: ignore",
            "            except (NotImplementedError, AttributeError):",
            "                raise ValueError(\"Provided LLM does not support native tools/functions\")",
            "        else:",
            "            qa_chain = LLMChain(llm=qa_llm, **use_qa_llm_kwargs)  # type: ignore[arg-type]",
            "",
            "        cypher_generation_chain = LLMChain(",
            "            llm=cypher_llm or llm,  # type: ignore[arg-type]",
            "            **use_cypher_llm_kwargs,  # type: ignore[arg-type]",
            "        )",
            "",
            "        if exclude_types and include_types:",
            "            raise ValueError(",
            "                \"Either `exclude_types` or `include_types` \"",
            "                \"can be provided, but not both\"",
            "            )",
            "        graph_schema = construct_schema(",
            "            kwargs[\"graph\"].get_structured_schema, include_types, exclude_types",
            "        )",
            "",
            "        cypher_query_corrector = None",
            "        if validate_cypher:",
            "            corrector_schema = [",
            "                Schema(el[\"start\"], el[\"type\"], el[\"end\"])",
            "                for el in kwargs[\"graph\"].structured_schema.get(\"relationships\")",
            "            ]",
            "            cypher_query_corrector = CypherQueryCorrector(corrector_schema)",
            "",
            "        return cls(",
            "            graph_schema=graph_schema,",
            "            qa_chain=qa_chain,",
            "            cypher_generation_chain=cypher_generation_chain,",
            "            cypher_query_corrector=cypher_query_corrector,",
            "            use_function_response=use_function_response,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, Any]:",
            "        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "        args = {",
            "            \"question\": question,",
            "            \"schema\": self.graph_schema,",
            "        }",
            "        args.update(inputs)",
            "",
            "        intermediate_steps: List = []",
            "",
            "        generated_cypher = self.cypher_generation_chain.run(args, callbacks=callbacks)",
            "",
            "        # Extract Cypher code if it is wrapped in backticks",
            "        generated_cypher = extract_cypher(generated_cypher)",
            "",
            "        # Correct Cypher query if enabled",
            "        if self.cypher_query_corrector:",
            "            generated_cypher = self.cypher_query_corrector(generated_cypher)",
            "",
            "        _run_manager.on_text(\"Generated Cypher:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_cypher, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_cypher})",
            "",
            "        # Retrieve and limit the number of results",
            "        # Generated Cypher be null if query corrector identifies invalid schema",
            "        if generated_cypher:",
            "            context = self.graph.query(generated_cypher)[: self.top_k]",
            "        else:",
            "            context = []",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "            if self.use_function_response:",
            "                function_response = get_function_response(question, context)",
            "                final_result = self.qa_chain.invoke(  # type: ignore",
            "                    {\"question\": question, \"function_response\": function_response},",
            "                )",
            "            else:",
            "                result = self.qa_chain.invoke(  # type: ignore",
            "                    {\"question\": question, \"context\": context},",
            "                    callbacks=callbacks,",
            "                )",
            "                final_result = result[self.qa_chain.output_key]  # type: ignore",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.cypher.GraphCypherQAChain.self",
            "libs.community.langchain_community.chains.graph_qa.cypher.GraphCypherQAChain",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default",
            "libs.community.langchain_community.chains.graph_qa.cypher.GraphCypherQAChain.from_llm"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/falkordb.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "     return_direct: bool = False"
            },
            "1": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "     \"\"\"Whether or not to return the result of querying the graph directly.\"\"\""
            },
            "2": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 68,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "         \"\"\"Return the input keys."
            }
        },
        "frontPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_GENERATION_PROMPT,",
            "    CYPHER_QA_PROMPT,",
            ")",
            "from langchain_community.graphs import FalkorDBGraph",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "",
            "def extract_cypher(text: str) -> str:",
            "    \"\"\"",
            "    Extract Cypher code from a text.",
            "    Args:",
            "        text: Text to extract Cypher code from.",
            "",
            "    Returns:",
            "        Cypher code extracted from the text.",
            "    \"\"\"",
            "    # The pattern to find Cypher code enclosed in triple backticks",
            "    pattern = r\"```(.*?)```\"",
            "",
            "    # Find all matches in the input text",
            "    matches = re.findall(pattern, text, re.DOTALL)",
            "",
            "    return matches[0] if matches else text",
            "",
            "",
            "class FalkorDBQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating Cypher statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: FalkorDBGraph = Field(exclude=True)",
            "    cypher_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 10",
            "    \"\"\"Number of results to return from the query\"\"\"",
            "    return_intermediate_steps: bool = False",
            "    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"",
            "    return_direct: bool = False",
            "    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @property",
            "    def _chain_type(self) -> str:",
            "        return \"graph_cypher_chain\"",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        cypher_prompt: BasePromptTemplate = CYPHER_GENERATION_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> FalkorDBQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        cypher_generation_chain = LLMChain(llm=llm, prompt=cypher_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            cypher_generation_chain=cypher_generation_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, Any]:",
            "        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        intermediate_steps: List = []",
            "",
            "        generated_cypher = self.cypher_generation_chain.run(",
            "            {\"question\": question, \"schema\": self.graph.schema}, callbacks=callbacks",
            "        )",
            "",
            "        # Extract Cypher code if it is wrapped in backticks",
            "        generated_cypher = extract_cypher(generated_cypher)",
            "",
            "        _run_manager.on_text(\"Generated Cypher:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_cypher, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_cypher})",
            "",
            "        # Retrieve and limit the number of results",
            "        context = self.graph.query(generated_cypher)[: self.top_k]",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "",
            "            result = self.qa_chain(",
            "                {\"question\": question, \"context\": context},",
            "                callbacks=callbacks,",
            "            )",
            "            final_result = result[self.qa_chain.output_key]",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result"
        ],
        "afterPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_GENERATION_PROMPT,",
            "    CYPHER_QA_PROMPT,",
            ")",
            "from langchain_community.graphs import FalkorDBGraph",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "",
            "def extract_cypher(text: str) -> str:",
            "    \"\"\"",
            "    Extract Cypher code from a text.",
            "    Args:",
            "        text: Text to extract Cypher code from.",
            "",
            "    Returns:",
            "        Cypher code extracted from the text.",
            "    \"\"\"",
            "    # The pattern to find Cypher code enclosed in triple backticks",
            "    pattern = r\"```(.*?)```\"",
            "",
            "    # Find all matches in the input text",
            "    matches = re.findall(pattern, text, re.DOTALL)",
            "",
            "    return matches[0] if matches else text",
            "",
            "",
            "class FalkorDBQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating Cypher statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: FalkorDBGraph = Field(exclude=True)",
            "    cypher_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 10",
            "    \"\"\"Number of results to return from the query\"\"\"",
            "    return_intermediate_steps: bool = False",
            "    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"",
            "    return_direct: bool = False",
            "    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @property",
            "    def _chain_type(self) -> str:",
            "        return \"graph_cypher_chain\"",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        cypher_prompt: BasePromptTemplate = CYPHER_GENERATION_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> FalkorDBQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        cypher_generation_chain = LLMChain(llm=llm, prompt=cypher_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            cypher_generation_chain=cypher_generation_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, Any]:",
            "        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        intermediate_steps: List = []",
            "",
            "        generated_cypher = self.cypher_generation_chain.run(",
            "            {\"question\": question, \"schema\": self.graph.schema}, callbacks=callbacks",
            "        )",
            "",
            "        # Extract Cypher code if it is wrapped in backticks",
            "        generated_cypher = extract_cypher(generated_cypher)",
            "",
            "        _run_manager.on_text(\"Generated Cypher:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_cypher, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_cypher})",
            "",
            "        # Retrieve and limit the number of results",
            "        context = self.graph.query(generated_cypher)[: self.top_k]",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "",
            "            result = self.qa_chain(",
            "                {\"question\": question, \"context\": context},",
            "                callbacks=callbacks,",
            "            )",
            "            final_result = result[self.qa_chain.output_key]",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.falkordb.FalkorDBQAChain",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default",
            "libs.community.langchain_community.chains.graph_qa.falkordb.FalkorDBQAChain.self",
            "libs.community.langchain_community.chains.graph_qa.falkordb.FalkorDBQAChain.from_llm"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/gremlin.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "     return_direct: bool = False"
            },
            "1": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "     return_intermediate_steps: bool = False"
            },
            "2": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 65,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "         \"\"\"Input keys."
            }
        },
        "frontPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks.manager import CallbackManager, CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.prompts.prompt import PromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    GRAPHDB_SPARQL_FIX_TEMPLATE,",
            "    GREMLIN_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs import GremlinGraph",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "",
            "def extract_gremlin(text: str) -> str:",
            "    \"\"\"Extract Gremlin code from a text.",
            "",
            "    Args:",
            "        text: Text to extract Gremlin code from.",
            "",
            "    Returns:",
            "        Gremlin code extracted from the text.",
            "    \"\"\"",
            "    text = text.replace(\"`\", \"\")",
            "    if text.startswith(\"gremlin\"):",
            "        text = text[len(\"gremlin\") :]",
            "    return text.replace(\"\\n\", \"\")",
            "",
            "",
            "class GremlinQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating gremlin statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: GremlinGraph = Field(exclude=True)",
            "    gremlin_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    gremlin_fix_chain: LLMChain",
            "    max_fix_retries: int = 3",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 100",
            "    return_direct: bool = False",
            "    return_intermediate_steps: bool = False",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        gremlin_fix_prompt: BasePromptTemplate = PromptTemplate(",
            "            input_variables=[\"error_message\", \"generated_sparql\", \"schema\"],",
            "            template=GRAPHDB_SPARQL_FIX_TEMPLATE.replace(\"SPARQL\", \"Gremlin\").replace(",
            "                \"in Turtle format\", \"\"",
            "            ),",
            "        ),",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        gremlin_prompt: BasePromptTemplate = GREMLIN_GENERATION_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> GremlinQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        gremlin_generation_chain = LLMChain(llm=llm, prompt=gremlin_prompt)",
            "        gremlinl_fix_chain = LLMChain(llm=llm, prompt=gremlin_fix_prompt)",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            gremlin_generation_chain=gremlin_generation_chain,",
            "            gremlin_fix_chain=gremlinl_fix_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"Generate gremlin statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        intermediate_steps: List = []",
            "",
            "        chain_response = self.gremlin_generation_chain.invoke(",
            "            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "",
            "        generated_gremlin = extract_gremlin(",
            "            chain_response[self.gremlin_generation_chain.output_key]",
            "        )",
            "",
            "        _run_manager.on_text(\"Generated gremlin:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_gremlin, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_gremlin})",
            "",
            "        if generated_gremlin:",
            "            context = self.execute_with_retry(",
            "                _run_manager, callbacks, generated_gremlin",
            "            )[: self.top_k]",
            "        else:",
            "            context = []",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "",
            "            result = self.qa_chain.invoke(",
            "                {\"question\": question, \"context\": context},",
            "                callbacks=callbacks,",
            "            )",
            "            final_result = result[self.qa_chain.output_key]",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result",
            "",
            "    def execute_query(self, query: str) -> List[Any]:",
            "        try:",
            "            return self.graph.query(query)",
            "        except Exception as e:",
            "            if hasattr(e, \"status_message\"):",
            "                raise ValueError(e.status_message)",
            "            else:",
            "                raise ValueError(str(e))",
            "",
            "    def execute_with_retry(",
            "        self,",
            "        _run_manager: CallbackManagerForChainRun,",
            "        callbacks: CallbackManager,",
            "        generated_gremlin: str,",
            "    ) -> List[Any]:",
            "        try:",
            "            return self.execute_query(generated_gremlin)",
            "        except Exception as e:",
            "            retries = 0",
            "            error_message = str(e)",
            "            self.log_invalid_query(_run_manager, generated_gremlin, error_message)",
            "",
            "            while retries < self.max_fix_retries:",
            "                try:",
            "                    fix_chain_result = self.gremlin_fix_chain.invoke(",
            "                        {",
            "                            \"error_message\": error_message,",
            "                            # we are borrowing template from sparql",
            "                            \"generated_sparql\": generated_gremlin,",
            "                            \"schema\": self.schema,",
            "                        },",
            "                        callbacks=callbacks,",
            "                    )",
            "                    fixed_gremlin = fix_chain_result[self.gremlin_fix_chain.output_key]",
            "                    return self.execute_query(fixed_gremlin)",
            "                except Exception as e:",
            "                    retries += 1",
            "                    parse_exception = str(e)",
            "                    self.log_invalid_query(_run_manager, fixed_gremlin, parse_exception)",
            "",
            "        raise ValueError(\"The generated Gremlin query is invalid.\")",
            "",
            "    def log_invalid_query(",
            "        self,",
            "        _run_manager: CallbackManagerForChainRun,",
            "        generated_query: str,",
            "        error_message: str,",
            "    ) -> None:",
            "        _run_manager.on_text(\"Invalid Gremlin query: \", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_query, color=\"red\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        _run_manager.on_text(",
            "            \"Gremlin Query Parse Error: \", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        _run_manager.on_text(",
            "            error_message, color=\"red\", end=\"\\n\\n\", verbose=self.verbose",
            "        )"
        ],
        "afterPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks.manager import CallbackManager, CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.prompts.prompt import PromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    GRAPHDB_SPARQL_FIX_TEMPLATE,",
            "    GREMLIN_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs import GremlinGraph",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "",
            "def extract_gremlin(text: str) -> str:",
            "    \"\"\"Extract Gremlin code from a text.",
            "",
            "    Args:",
            "        text: Text to extract Gremlin code from.",
            "",
            "    Returns:",
            "        Gremlin code extracted from the text.",
            "    \"\"\"",
            "    text = text.replace(\"`\", \"\")",
            "    if text.startswith(\"gremlin\"):",
            "        text = text[len(\"gremlin\") :]",
            "    return text.replace(\"\\n\", \"\")",
            "",
            "",
            "class GremlinQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating gremlin statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: GremlinGraph = Field(exclude=True)",
            "    gremlin_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    gremlin_fix_chain: LLMChain",
            "    max_fix_retries: int = 3",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 100",
            "    return_direct: bool = False",
            "    return_intermediate_steps: bool = False",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        gremlin_fix_prompt: BasePromptTemplate = PromptTemplate(",
            "            input_variables=[\"error_message\", \"generated_sparql\", \"schema\"],",
            "            template=GRAPHDB_SPARQL_FIX_TEMPLATE.replace(\"SPARQL\", \"Gremlin\").replace(",
            "                \"in Turtle format\", \"\"",
            "            ),",
            "        ),",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        gremlin_prompt: BasePromptTemplate = GREMLIN_GENERATION_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> GremlinQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        gremlin_generation_chain = LLMChain(llm=llm, prompt=gremlin_prompt)",
            "        gremlinl_fix_chain = LLMChain(llm=llm, prompt=gremlin_fix_prompt)",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            gremlin_generation_chain=gremlin_generation_chain,",
            "            gremlin_fix_chain=gremlinl_fix_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"Generate gremlin statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        intermediate_steps: List = []",
            "",
            "        chain_response = self.gremlin_generation_chain.invoke(",
            "            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "",
            "        generated_gremlin = extract_gremlin(",
            "            chain_response[self.gremlin_generation_chain.output_key]",
            "        )",
            "",
            "        _run_manager.on_text(\"Generated gremlin:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_gremlin, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_gremlin})",
            "",
            "        if generated_gremlin:",
            "            context = self.execute_with_retry(",
            "                _run_manager, callbacks, generated_gremlin",
            "            )[: self.top_k]",
            "        else:",
            "            context = []",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "",
            "            result = self.qa_chain.invoke(",
            "                {\"question\": question, \"context\": context},",
            "                callbacks=callbacks,",
            "            )",
            "            final_result = result[self.qa_chain.output_key]",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result",
            "",
            "    def execute_query(self, query: str) -> List[Any]:",
            "        try:",
            "            return self.graph.query(query)",
            "        except Exception as e:",
            "            if hasattr(e, \"status_message\"):",
            "                raise ValueError(e.status_message)",
            "            else:",
            "                raise ValueError(str(e))",
            "",
            "    def execute_with_retry(",
            "        self,",
            "        _run_manager: CallbackManagerForChainRun,",
            "        callbacks: CallbackManager,",
            "        generated_gremlin: str,",
            "    ) -> List[Any]:",
            "        try:",
            "            return self.execute_query(generated_gremlin)",
            "        except Exception as e:",
            "            retries = 0",
            "            error_message = str(e)",
            "            self.log_invalid_query(_run_manager, generated_gremlin, error_message)",
            "",
            "            while retries < self.max_fix_retries:",
            "                try:",
            "                    fix_chain_result = self.gremlin_fix_chain.invoke(",
            "                        {",
            "                            \"error_message\": error_message,",
            "                            # we are borrowing template from sparql",
            "                            \"generated_sparql\": generated_gremlin,",
            "                            \"schema\": self.schema,",
            "                        },",
            "                        callbacks=callbacks,",
            "                    )",
            "                    fixed_gremlin = fix_chain_result[self.gremlin_fix_chain.output_key]",
            "                    return self.execute_query(fixed_gremlin)",
            "                except Exception as e:",
            "                    retries += 1",
            "                    parse_exception = str(e)",
            "                    self.log_invalid_query(_run_manager, fixed_gremlin, parse_exception)",
            "",
            "        raise ValueError(\"The generated Gremlin query is invalid.\")",
            "",
            "    def log_invalid_query(",
            "        self,",
            "        _run_manager: CallbackManagerForChainRun,",
            "        generated_query: str,",
            "        error_message: str,",
            "    ) -> None:",
            "        _run_manager.on_text(\"Invalid Gremlin query: \", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_query, color=\"red\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        _run_manager.on_text(",
            "            \"Gremlin Query Parse Error: \", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        _run_manager.on_text(",
            "            error_message, color=\"red\", end=\"\\n\\n\", verbose=self.verbose",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.gremlin.GremlinQAChain.self",
            "libs.community.langchain_community.chains.graph_qa.gremlin.GremlinQAChain.from_llm",
            "libs.community.langchain_community.chains.graph_qa.gremlin.GremlinQAChain",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/hugegraph.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "     input_key: str = \"query\"  #: :meta private:"
            },
            "1": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "     output_key: str = \"result\"  #: :meta private:"
            },
            "2": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "         \"\"\"Input keys."
            }
        },
        "frontPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    GREMLIN_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs.hugegraph import HugeGraph",
            "",
            "",
            "class HugeGraphQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating gremlin statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: HugeGraph = Field(exclude=True)",
            "    gremlin_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        gremlin_prompt: BasePromptTemplate = GREMLIN_GENERATION_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> HugeGraphQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        gremlin_generation_chain = LLMChain(llm=llm, prompt=gremlin_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            gremlin_generation_chain=gremlin_generation_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"Generate gremlin statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        generated_gremlin = self.gremlin_generation_chain.run(",
            "            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "",
            "        _run_manager.on_text(\"Generated gremlin:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_gremlin, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        context = self.graph.query(generated_gremlin)",
            "",
            "        _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        result = self.qa_chain(",
            "            {\"question\": question, \"context\": context},",
            "            callbacks=callbacks,",
            "        )",
            "        return {self.output_key: result[self.qa_chain.output_key]}"
        ],
        "afterPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    GREMLIN_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs.hugegraph import HugeGraph",
            "",
            "",
            "class HugeGraphQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating gremlin statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: HugeGraph = Field(exclude=True)",
            "    gremlin_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        gremlin_prompt: BasePromptTemplate = GREMLIN_GENERATION_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> HugeGraphQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        gremlin_generation_chain = LLMChain(llm=llm, prompt=gremlin_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            gremlin_generation_chain=gremlin_generation_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"Generate gremlin statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        generated_gremlin = self.gremlin_generation_chain.run(",
            "            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "",
            "        _run_manager.on_text(\"Generated gremlin:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_gremlin, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        context = self.graph.query(generated_gremlin)",
            "",
            "        _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        result = self.qa_chain(",
            "            {\"question\": question, \"context\": context},",
            "            callbacks=callbacks,",
            "        )",
            "        return {self.output_key: result[self.qa_chain.output_key]}"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.hugegraph.HugeGraphQAChain.from_llm",
            "libs.community.langchain_community.chains.graph_qa.hugegraph.HugeGraphQAChain.self",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default",
            "libs.community.langchain_community.chains.graph_qa.hugegraph.HugeGraphQAChain"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/kuzu.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "     input_key: str = \"query\"  #: :meta private:"
            },
            "1": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "     output_key: str = \"result\"  #: :meta private:"
            },
            "2": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 109,
                "PatchRowcode": "         \"\"\"Return the input keys."
            }
        },
        "frontPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    KUZU_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs.kuzu_graph import KuzuGraph",
            "",
            "",
            "def remove_prefix(text: str, prefix: str) -> str:",
            "    \"\"\"Remove a prefix from a text.",
            "",
            "    Args:",
            "        text: Text to remove the prefix from.",
            "        prefix: Prefix to remove from the text.",
            "",
            "    Returns:",
            "        Text with the prefix removed.",
            "    \"\"\"",
            "    if text.startswith(prefix):",
            "        return text[len(prefix) :]",
            "    return text",
            "",
            "",
            "def extract_cypher(text: str) -> str:",
            "    \"\"\"Extract Cypher code from a text.",
            "",
            "    Args:",
            "        text: Text to extract Cypher code from.",
            "",
            "    Returns:",
            "        Cypher code extracted from the text.",
            "    \"\"\"",
            "    # The pattern to find Cypher code enclosed in triple backticks",
            "    pattern = r\"```(.*?)```\"",
            "",
            "    # Find all matches in the input text",
            "    matches = re.findall(pattern, text, re.DOTALL)",
            "",
            "    return matches[0] if matches else text",
            "",
            "",
            "class KuzuQAChain(Chain):",
            "    \"\"\"Question-answering against a graph by generating Cypher statements for K\u00f9zu.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: KuzuGraph = Field(exclude=True)",
            "    cypher_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: Optional[BaseLanguageModel] = None,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        cypher_prompt: BasePromptTemplate = KUZU_GENERATION_PROMPT,",
            "        cypher_llm: Optional[BaseLanguageModel] = None,",
            "        qa_llm: Optional[BaseLanguageModel] = None,",
            "        **kwargs: Any,",
            "    ) -> KuzuQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        if not cypher_llm and not llm:",
            "            raise ValueError(\"Either `llm` or `cypher_llm` parameters must be provided\")",
            "        if not qa_llm and not llm:",
            "            raise ValueError(",
            "                \"Either `llm` or `qa_llm` parameters must be provided along with\"",
            "                \" `cypher_llm`\"",
            "            )",
            "        if cypher_llm and qa_llm and llm:",
            "            raise ValueError(",
            "                \"You can specify up to two of 'cypher_llm', 'qa_llm'\"",
            "                \", and 'llm', but not all three simultaneously.\"",
            "            )",
            "",
            "        qa_chain = LLMChain(",
            "            llm=qa_llm or llm,  # type: ignore[arg-type]",
            "            prompt=qa_prompt,",
            "        )",
            "        cypher_generation_chain = LLMChain(",
            "            llm=cypher_llm or llm,  # type: ignore[arg-type]",
            "            prompt=cypher_prompt,",
            "        )",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            cypher_generation_chain=cypher_generation_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        generated_cypher = self.cypher_generation_chain.run(",
            "            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "        # Extract Cypher code if it is wrapped in triple backticks",
            "        # with the language marker \"cypher\"",
            "        generated_cypher = remove_prefix(extract_cypher(generated_cypher), \"cypher\")",
            "",
            "        _run_manager.on_text(\"Generated Cypher:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_cypher, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        context = self.graph.query(generated_cypher)",
            "",
            "        _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        result = self.qa_chain(",
            "            {\"question\": question, \"context\": context},",
            "            callbacks=callbacks,",
            "        )",
            "        return {self.output_key: result[self.qa_chain.output_key]}"
        ],
        "afterPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    KUZU_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs.kuzu_graph import KuzuGraph",
            "",
            "",
            "def remove_prefix(text: str, prefix: str) -> str:",
            "    \"\"\"Remove a prefix from a text.",
            "",
            "    Args:",
            "        text: Text to remove the prefix from.",
            "        prefix: Prefix to remove from the text.",
            "",
            "    Returns:",
            "        Text with the prefix removed.",
            "    \"\"\"",
            "    if text.startswith(prefix):",
            "        return text[len(prefix) :]",
            "    return text",
            "",
            "",
            "def extract_cypher(text: str) -> str:",
            "    \"\"\"Extract Cypher code from a text.",
            "",
            "    Args:",
            "        text: Text to extract Cypher code from.",
            "",
            "    Returns:",
            "        Cypher code extracted from the text.",
            "    \"\"\"",
            "    # The pattern to find Cypher code enclosed in triple backticks",
            "    pattern = r\"```(.*?)```\"",
            "",
            "    # Find all matches in the input text",
            "    matches = re.findall(pattern, text, re.DOTALL)",
            "",
            "    return matches[0] if matches else text",
            "",
            "",
            "class KuzuQAChain(Chain):",
            "    \"\"\"Question-answering against a graph by generating Cypher statements for K\u00f9zu.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: KuzuGraph = Field(exclude=True)",
            "    cypher_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: Optional[BaseLanguageModel] = None,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        cypher_prompt: BasePromptTemplate = KUZU_GENERATION_PROMPT,",
            "        cypher_llm: Optional[BaseLanguageModel] = None,",
            "        qa_llm: Optional[BaseLanguageModel] = None,",
            "        **kwargs: Any,",
            "    ) -> KuzuQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        if not cypher_llm and not llm:",
            "            raise ValueError(\"Either `llm` or `cypher_llm` parameters must be provided\")",
            "        if not qa_llm and not llm:",
            "            raise ValueError(",
            "                \"Either `llm` or `qa_llm` parameters must be provided along with\"",
            "                \" `cypher_llm`\"",
            "            )",
            "        if cypher_llm and qa_llm and llm:",
            "            raise ValueError(",
            "                \"You can specify up to two of 'cypher_llm', 'qa_llm'\"",
            "                \", and 'llm', but not all three simultaneously.\"",
            "            )",
            "",
            "        qa_chain = LLMChain(",
            "            llm=qa_llm or llm,  # type: ignore[arg-type]",
            "            prompt=qa_prompt,",
            "        )",
            "        cypher_generation_chain = LLMChain(",
            "            llm=cypher_llm or llm,  # type: ignore[arg-type]",
            "            prompt=cypher_prompt,",
            "        )",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            cypher_generation_chain=cypher_generation_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        generated_cypher = self.cypher_generation_chain.run(",
            "            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "        # Extract Cypher code if it is wrapped in triple backticks",
            "        # with the language marker \"cypher\"",
            "        generated_cypher = remove_prefix(extract_cypher(generated_cypher), \"cypher\")",
            "",
            "        _run_manager.on_text(\"Generated Cypher:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_cypher, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        context = self.graph.query(generated_cypher)",
            "",
            "        _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        result = self.qa_chain(",
            "            {\"question\": question, \"context\": context},",
            "            callbacks=callbacks,",
            "        )",
            "        return {self.output_key: result[self.qa_chain.output_key]}"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.kuzu.KuzuQAChain",
            "libs.community.langchain_community.chains.graph_qa.kuzu.KuzuQAChain.from_llm",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default",
            "libs.community.langchain_community.chains.graph_qa.kuzu.KuzuQAChain.self"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/nebulagraph.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "     input_key: str = \"query\"  #: :meta private:"
            },
            "1": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "     output_key: str = \"result\"  #: :meta private:"
            },
            "2": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "         \"\"\"Return the input keys."
            }
        },
        "frontPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    NGQL_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs.nebula_graph import NebulaGraph",
            "",
            "",
            "class NebulaGraphQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating nGQL statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: NebulaGraph = Field(exclude=True)",
            "    ngql_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        ngql_prompt: BasePromptTemplate = NGQL_GENERATION_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> NebulaGraphQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        ngql_generation_chain = LLMChain(llm=llm, prompt=ngql_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            ngql_generation_chain=ngql_generation_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"Generate nGQL statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        generated_ngql = self.ngql_generation_chain.run(",
            "            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "",
            "        _run_manager.on_text(\"Generated nGQL:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_ngql, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        context = self.graph.query(generated_ngql)",
            "",
            "        _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        result = self.qa_chain(",
            "            {\"question\": question, \"context\": context},",
            "            callbacks=callbacks,",
            "        )",
            "        return {self.output_key: result[self.qa_chain.output_key]}"
        ],
        "afterPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    NGQL_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs.nebula_graph import NebulaGraph",
            "",
            "",
            "class NebulaGraphQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a graph by generating nGQL statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: NebulaGraph = Field(exclude=True)",
            "    ngql_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        ngql_prompt: BasePromptTemplate = NGQL_GENERATION_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> NebulaGraphQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        ngql_generation_chain = LLMChain(llm=llm, prompt=ngql_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            ngql_generation_chain=ngql_generation_chain,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"Generate nGQL statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        generated_ngql = self.ngql_generation_chain.run(",
            "            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "",
            "        _run_manager.on_text(\"Generated nGQL:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_ngql, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        context = self.graph.query(generated_ngql)",
            "",
            "        _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        result = self.qa_chain(",
            "            {\"question\": question, \"context\": context},",
            "            callbacks=callbacks,",
            "        )",
            "        return {self.output_key: result[self.qa_chain.output_key]}"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.nebulagraph.NebulaGraphQAChain",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default",
            "libs.community.langchain_community.chains.graph_qa.nebulagraph.NebulaGraphQAChain.self",
            "libs.community.langchain_community.chains.graph_qa.nebulagraph.NebulaGraphQAChain.from_llm"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/neptune_cypher.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "     extra_instructions: Optional[str] = None"
            },
            "1": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "     \"\"\"Extra instructions by the appended to the query generation prompt.\"\"\""
            },
            "2": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 122,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 143,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 144,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 145,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 146,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 147,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 148,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 149,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 150,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 152,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "         \"\"\"Return the input keys."
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain.chains.prompt_selector import ConditionalPromptSelector",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts.base import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    NEPTUNE_OPENCYPHER_GENERATION_PROMPT,",
            "    NEPTUNE_OPENCYPHER_GENERATION_SIMPLE_PROMPT,",
            ")",
            "from langchain_community.graphs import BaseNeptuneGraph",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "",
            "def trim_query(query: str) -> str:",
            "    \"\"\"Trim the query to only include Cypher keywords.\"\"\"",
            "    keywords = (",
            "        \"CALL\",",
            "        \"CREATE\",",
            "        \"DELETE\",",
            "        \"DETACH\",",
            "        \"LIMIT\",",
            "        \"MATCH\",",
            "        \"MERGE\",",
            "        \"OPTIONAL\",",
            "        \"ORDER\",",
            "        \"REMOVE\",",
            "        \"RETURN\",",
            "        \"SET\",",
            "        \"SKIP\",",
            "        \"UNWIND\",",
            "        \"WITH\",",
            "        \"WHERE\",",
            "        \"//\",",
            "    )",
            "",
            "    lines = query.split(\"\\n\")",
            "    new_query = \"\"",
            "",
            "    for line in lines:",
            "        if line.strip().upper().startswith(keywords):",
            "            new_query += line + \"\\n\"",
            "",
            "    return new_query",
            "",
            "",
            "def extract_cypher(text: str) -> str:",
            "    \"\"\"Extract Cypher code from text using Regex.\"\"\"",
            "    # The pattern to find Cypher code enclosed in triple backticks",
            "    pattern = r\"```(.*?)```\"",
            "",
            "    # Find all matches in the input text",
            "    matches = re.findall(pattern, text, re.DOTALL)",
            "",
            "    return matches[0] if matches else text",
            "",
            "",
            "def use_simple_prompt(llm: BaseLanguageModel) -> bool:",
            "    \"\"\"Decides whether to use the simple prompt\"\"\"",
            "    if llm._llm_type and \"anthropic\" in llm._llm_type:  # type: ignore",
            "        return True",
            "",
            "    # Bedrock anthropic",
            "    if hasattr(llm, \"model_id\") and \"anthropic\" in llm.model_id:  # type: ignore",
            "        return True",
            "",
            "    return False",
            "",
            "",
            "PROMPT_SELECTOR = ConditionalPromptSelector(",
            "    default_prompt=NEPTUNE_OPENCYPHER_GENERATION_PROMPT,",
            "    conditionals=[(use_simple_prompt, NEPTUNE_OPENCYPHER_GENERATION_SIMPLE_PROMPT)],",
            ")",
            "",
            "",
            "class NeptuneOpenCypherQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a Neptune graph",
            "    by generating openCypher statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "",
            "    Example:",
            "        .. code-block:: python",
            "",
            "        chain = NeptuneOpenCypherQAChain.from_llm(",
            "            llm=llm,",
            "            graph=graph",
            "        )",
            "        response = chain.run(query)",
            "    \"\"\"",
            "",
            "    graph: BaseNeptuneGraph = Field(exclude=True)",
            "    cypher_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 10",
            "    return_intermediate_steps: bool = False",
            "    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"",
            "    return_direct: bool = False",
            "    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"",
            "    extra_instructions: Optional[str] = None",
            "    \"\"\"Extra instructions by the appended to the query generation prompt.\"\"\"",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        cypher_prompt: Optional[BasePromptTemplate] = None,",
            "        extra_instructions: Optional[str] = None,",
            "        **kwargs: Any,",
            "    ) -> NeptuneOpenCypherQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "",
            "        _cypher_prompt = cypher_prompt or PROMPT_SELECTOR.get_prompt(llm)",
            "        cypher_generation_chain = LLMChain(llm=llm, prompt=_cypher_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            cypher_generation_chain=cypher_generation_chain,",
            "            extra_instructions=extra_instructions,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, Any]:",
            "        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        intermediate_steps: List = []",
            "",
            "        generated_cypher = self.cypher_generation_chain.run(",
            "            {",
            "                \"question\": question,",
            "                \"schema\": self.graph.get_schema,",
            "                \"extra_instructions\": self.extra_instructions or \"\",",
            "            },",
            "            callbacks=callbacks,",
            "        )",
            "",
            "        # Extract Cypher code if it is wrapped in backticks",
            "        generated_cypher = extract_cypher(generated_cypher)",
            "        generated_cypher = trim_query(generated_cypher)",
            "",
            "        _run_manager.on_text(\"Generated Cypher:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_cypher, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_cypher})",
            "",
            "        context = self.graph.query(generated_cypher)",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "",
            "            result = self.qa_chain(",
            "                {\"question\": question, \"context\": context},",
            "                callbacks=callbacks,",
            "            )",
            "            final_result = result[self.qa_chain.output_key]",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain.chains.prompt_selector import ConditionalPromptSelector",
            "from langchain_core.callbacks import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts.base import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    CYPHER_QA_PROMPT,",
            "    NEPTUNE_OPENCYPHER_GENERATION_PROMPT,",
            "    NEPTUNE_OPENCYPHER_GENERATION_SIMPLE_PROMPT,",
            ")",
            "from langchain_community.graphs import BaseNeptuneGraph",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "",
            "def trim_query(query: str) -> str:",
            "    \"\"\"Trim the query to only include Cypher keywords.\"\"\"",
            "    keywords = (",
            "        \"CALL\",",
            "        \"CREATE\",",
            "        \"DELETE\",",
            "        \"DETACH\",",
            "        \"LIMIT\",",
            "        \"MATCH\",",
            "        \"MERGE\",",
            "        \"OPTIONAL\",",
            "        \"ORDER\",",
            "        \"REMOVE\",",
            "        \"RETURN\",",
            "        \"SET\",",
            "        \"SKIP\",",
            "        \"UNWIND\",",
            "        \"WITH\",",
            "        \"WHERE\",",
            "        \"//\",",
            "    )",
            "",
            "    lines = query.split(\"\\n\")",
            "    new_query = \"\"",
            "",
            "    for line in lines:",
            "        if line.strip().upper().startswith(keywords):",
            "            new_query += line + \"\\n\"",
            "",
            "    return new_query",
            "",
            "",
            "def extract_cypher(text: str) -> str:",
            "    \"\"\"Extract Cypher code from text using Regex.\"\"\"",
            "    # The pattern to find Cypher code enclosed in triple backticks",
            "    pattern = r\"```(.*?)```\"",
            "",
            "    # Find all matches in the input text",
            "    matches = re.findall(pattern, text, re.DOTALL)",
            "",
            "    return matches[0] if matches else text",
            "",
            "",
            "def use_simple_prompt(llm: BaseLanguageModel) -> bool:",
            "    \"\"\"Decides whether to use the simple prompt\"\"\"",
            "    if llm._llm_type and \"anthropic\" in llm._llm_type:  # type: ignore",
            "        return True",
            "",
            "    # Bedrock anthropic",
            "    if hasattr(llm, \"model_id\") and \"anthropic\" in llm.model_id:  # type: ignore",
            "        return True",
            "",
            "    return False",
            "",
            "",
            "PROMPT_SELECTOR = ConditionalPromptSelector(",
            "    default_prompt=NEPTUNE_OPENCYPHER_GENERATION_PROMPT,",
            "    conditionals=[(use_simple_prompt, NEPTUNE_OPENCYPHER_GENERATION_SIMPLE_PROMPT)],",
            ")",
            "",
            "",
            "class NeptuneOpenCypherQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a Neptune graph",
            "    by generating openCypher statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "",
            "    Example:",
            "        .. code-block:: python",
            "",
            "        chain = NeptuneOpenCypherQAChain.from_llm(",
            "            llm=llm,",
            "            graph=graph",
            "        )",
            "        response = chain.run(query)",
            "    \"\"\"",
            "",
            "    graph: BaseNeptuneGraph = Field(exclude=True)",
            "    cypher_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 10",
            "    return_intermediate_steps: bool = False",
            "    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"",
            "    return_direct: bool = False",
            "    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"",
            "    extra_instructions: Optional[str] = None",
            "    \"\"\"Extra instructions by the appended to the query generation prompt.\"\"\"",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        \"\"\"Return the input keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        \"\"\"Return the output keys.",
            "",
            "        :meta private:",
            "        \"\"\"",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,",
            "        cypher_prompt: Optional[BasePromptTemplate] = None,",
            "        extra_instructions: Optional[str] = None,",
            "        **kwargs: Any,",
            "    ) -> NeptuneOpenCypherQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "",
            "        _cypher_prompt = cypher_prompt or PROMPT_SELECTOR.get_prompt(llm)",
            "        cypher_generation_chain = LLMChain(llm=llm, prompt=_cypher_prompt)",
            "",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            cypher_generation_chain=cypher_generation_chain,",
            "            extra_instructions=extra_instructions,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, Any]:",
            "        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        question = inputs[self.input_key]",
            "",
            "        intermediate_steps: List = []",
            "",
            "        generated_cypher = self.cypher_generation_chain.run(",
            "            {",
            "                \"question\": question,",
            "                \"schema\": self.graph.get_schema,",
            "                \"extra_instructions\": self.extra_instructions or \"\",",
            "            },",
            "            callbacks=callbacks,",
            "        )",
            "",
            "        # Extract Cypher code if it is wrapped in backticks",
            "        generated_cypher = extract_cypher(generated_cypher)",
            "        generated_cypher = trim_query(generated_cypher)",
            "",
            "        _run_manager.on_text(\"Generated Cypher:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_cypher, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_cypher})",
            "",
            "        context = self.graph.query(generated_cypher)",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "",
            "            result = self.qa_chain(",
            "                {\"question\": question, \"context\": context},",
            "                callbacks=callbacks,",
            "            )",
            "            final_result = result[self.qa_chain.output_key]",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain.self",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default",
            "libs.community.langchain_community.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain",
            "libs.community.langchain_community.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain.from_llm"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/neptune_sparql.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "     extra_instructions: Optional[str] = None"
            },
            "1": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "     \"\"\"Extra instructions by the appended to the query generation prompt.\"\"\""
            },
            "2": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 115,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 143,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 144,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 145,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 146,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 147,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 148,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 149,
                "PatchRowcode": "         return [self.input_key]"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Question answering over an RDF or OWL graph using SPARQL.",
            "\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks.manager import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts.base import BasePromptTemplate",
            "from langchain_core.prompts.prompt import PromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import SPARQL_QA_PROMPT",
            "from langchain_community.graphs import NeptuneRdfGraph",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "SPARQL_GENERATION_TEMPLATE = \"\"\"",
            "Task: Generate a SPARQL SELECT statement for querying a graph database.",
            "For instance, to find all email addresses of John Doe, the following ",
            "query in backticks would be suitable:",
            "```",
            "PREFIX foaf: <http://xmlns.com/foaf/0.1/>",
            "SELECT ?email",
            "WHERE {{",
            "    ?person foaf:name \"John Doe\" .",
            "    ?person foaf:mbox ?email .",
            "}}",
            "```",
            "Instructions:",
            "Use only the node types and properties provided in the schema.",
            "Do not use any node types and properties that are not explicitly provided.",
            "Include all necessary prefixes.",
            "",
            "Examples:",
            "",
            "Schema:",
            "{schema}",
            "Note: Be as concise as possible.",
            "Do not include any explanations or apologies in your responses.",
            "Do not respond to any questions that ask for anything else than ",
            "for you to construct a SPARQL query.",
            "Do not include any text except the SPARQL query generated.",
            "",
            "The question is:",
            "{prompt}\"\"\"",
            "",
            "SPARQL_GENERATION_PROMPT = PromptTemplate(",
            "    input_variables=[\"schema\", \"prompt\"], template=SPARQL_GENERATION_TEMPLATE",
            ")",
            "",
            "",
            "def extract_sparql(query: str) -> str:",
            "    \"\"\"Extract SPARQL code from a text.",
            "",
            "    Args:",
            "        query: Text to extract SPARQL code from.",
            "",
            "    Returns:",
            "        SPARQL code extracted from the text.",
            "    \"\"\"",
            "    query = query.strip()",
            "    querytoks = query.split(\"```\")",
            "    if len(querytoks) == 3:",
            "        query = querytoks[1]",
            "",
            "        if query.startswith(\"sparql\"):",
            "            query = query[6:]",
            "    elif query.startswith(\"<sparql>\") and query.endswith(\"</sparql>\"):",
            "        query = query[8:-9]",
            "    return query",
            "",
            "",
            "class NeptuneSparqlQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a Neptune graph",
            "    by generating SPARQL statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "",
            "    Example:",
            "        .. code-block:: python",
            "",
            "        chain = NeptuneSparqlQAChain.from_llm(",
            "            llm=llm,",
            "            graph=graph",
            "        )",
            "        response = chain.invoke(query)",
            "    \"\"\"",
            "",
            "    graph: NeptuneRdfGraph = Field(exclude=True)",
            "    sparql_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 10",
            "    return_intermediate_steps: bool = False",
            "    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"",
            "    return_direct: bool = False",
            "    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"",
            "    extra_instructions: Optional[str] = None",
            "    \"\"\"Extra instructions by the appended to the query generation prompt.\"\"\"",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = SPARQL_QA_PROMPT,",
            "        sparql_prompt: BasePromptTemplate = SPARQL_GENERATION_PROMPT,",
            "        examples: Optional[str] = None,",
            "        **kwargs: Any,",
            "    ) -> NeptuneSparqlQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        template_to_use = SPARQL_GENERATION_TEMPLATE",
            "        if examples:",
            "            template_to_use = template_to_use.replace(",
            "                \"Examples:\", \"Examples: \" + examples",
            "            )",
            "            sparql_prompt = PromptTemplate(",
            "                input_variables=[\"schema\", \"prompt\"], template=template_to_use",
            "            )",
            "        sparql_generation_chain = LLMChain(llm=llm, prompt=sparql_prompt)",
            "",
            "        return cls(  # type: ignore[call-arg]",
            "            qa_chain=qa_chain,",
            "            sparql_generation_chain=sparql_generation_chain,",
            "            examples=examples,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"",
            "        Generate SPARQL query, use it to retrieve a response from the gdb and answer",
            "        the question.",
            "        \"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        prompt = inputs[self.input_key]",
            "",
            "        intermediate_steps: List = []",
            "",
            "        generated_sparql = self.sparql_generation_chain.run(",
            "            {\"prompt\": prompt, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "",
            "        # Extract SPARQL",
            "        generated_sparql = extract_sparql(generated_sparql)",
            "",
            "        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_sparql, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_sparql})",
            "",
            "        context = self.graph.query(generated_sparql)",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "",
            "            result = self.qa_chain(",
            "                {\"prompt\": prompt, \"context\": context},",
            "                callbacks=callbacks,",
            "            )",
            "            final_result = result[self.qa_chain.output_key]",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Question answering over an RDF or OWL graph using SPARQL.",
            "\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import Any, Dict, List, Optional",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks.manager import CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts.base import BasePromptTemplate",
            "from langchain_core.prompts.prompt import PromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import SPARQL_QA_PROMPT",
            "from langchain_community.graphs import NeptuneRdfGraph",
            "",
            "INTERMEDIATE_STEPS_KEY = \"intermediate_steps\"",
            "",
            "SPARQL_GENERATION_TEMPLATE = \"\"\"",
            "Task: Generate a SPARQL SELECT statement for querying a graph database.",
            "For instance, to find all email addresses of John Doe, the following ",
            "query in backticks would be suitable:",
            "```",
            "PREFIX foaf: <http://xmlns.com/foaf/0.1/>",
            "SELECT ?email",
            "WHERE {{",
            "    ?person foaf:name \"John Doe\" .",
            "    ?person foaf:mbox ?email .",
            "}}",
            "```",
            "Instructions:",
            "Use only the node types and properties provided in the schema.",
            "Do not use any node types and properties that are not explicitly provided.",
            "Include all necessary prefixes.",
            "",
            "Examples:",
            "",
            "Schema:",
            "{schema}",
            "Note: Be as concise as possible.",
            "Do not include any explanations or apologies in your responses.",
            "Do not respond to any questions that ask for anything else than ",
            "for you to construct a SPARQL query.",
            "Do not include any text except the SPARQL query generated.",
            "",
            "The question is:",
            "{prompt}\"\"\"",
            "",
            "SPARQL_GENERATION_PROMPT = PromptTemplate(",
            "    input_variables=[\"schema\", \"prompt\"], template=SPARQL_GENERATION_TEMPLATE",
            ")",
            "",
            "",
            "def extract_sparql(query: str) -> str:",
            "    \"\"\"Extract SPARQL code from a text.",
            "",
            "    Args:",
            "        query: Text to extract SPARQL code from.",
            "",
            "    Returns:",
            "        SPARQL code extracted from the text.",
            "    \"\"\"",
            "    query = query.strip()",
            "    querytoks = query.split(\"```\")",
            "    if len(querytoks) == 3:",
            "        query = querytoks[1]",
            "",
            "        if query.startswith(\"sparql\"):",
            "            query = query[6:]",
            "    elif query.startswith(\"<sparql>\") and query.endswith(\"</sparql>\"):",
            "        query = query[8:-9]",
            "    return query",
            "",
            "",
            "class NeptuneSparqlQAChain(Chain):",
            "    \"\"\"Chain for question-answering against a Neptune graph",
            "    by generating SPARQL statements.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "",
            "    Example:",
            "        .. code-block:: python",
            "",
            "        chain = NeptuneSparqlQAChain.from_llm(",
            "            llm=llm,",
            "            graph=graph",
            "        )",
            "        response = chain.invoke(query)",
            "    \"\"\"",
            "",
            "    graph: NeptuneRdfGraph = Field(exclude=True)",
            "    sparql_generation_chain: LLMChain",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "    top_k: int = 10",
            "    return_intermediate_steps: bool = False",
            "    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"",
            "    return_direct: bool = False",
            "    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"",
            "    extra_instructions: Optional[str] = None",
            "    \"\"\"Extra instructions by the appended to the query generation prompt.\"\"\"",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        qa_prompt: BasePromptTemplate = SPARQL_QA_PROMPT,",
            "        sparql_prompt: BasePromptTemplate = SPARQL_GENERATION_PROMPT,",
            "        examples: Optional[str] = None,",
            "        **kwargs: Any,",
            "    ) -> NeptuneSparqlQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        template_to_use = SPARQL_GENERATION_TEMPLATE",
            "        if examples:",
            "            template_to_use = template_to_use.replace(",
            "                \"Examples:\", \"Examples: \" + examples",
            "            )",
            "            sparql_prompt = PromptTemplate(",
            "                input_variables=[\"schema\", \"prompt\"], template=template_to_use",
            "            )",
            "        sparql_generation_chain = LLMChain(llm=llm, prompt=sparql_prompt)",
            "",
            "        return cls(  # type: ignore[call-arg]",
            "            qa_chain=qa_chain,",
            "            sparql_generation_chain=sparql_generation_chain,",
            "            examples=examples,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"",
            "        Generate SPARQL query, use it to retrieve a response from the gdb and answer",
            "        the question.",
            "        \"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        prompt = inputs[self.input_key]",
            "",
            "        intermediate_steps: List = []",
            "",
            "        generated_sparql = self.sparql_generation_chain.run(",
            "            {\"prompt\": prompt, \"schema\": self.graph.get_schema}, callbacks=callbacks",
            "        )",
            "",
            "        # Extract SPARQL",
            "        generated_sparql = extract_sparql(generated_sparql)",
            "",
            "        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_sparql, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "        intermediate_steps.append({\"query\": generated_sparql})",
            "",
            "        context = self.graph.query(generated_sparql)",
            "",
            "        if self.return_direct:",
            "            final_result = context",
            "        else:",
            "            _run_manager.on_text(\"Full Context:\", end=\"\\n\", verbose=self.verbose)",
            "            _run_manager.on_text(",
            "                str(context), color=\"green\", end=\"\\n\", verbose=self.verbose",
            "            )",
            "",
            "            intermediate_steps.append({\"context\": context})",
            "",
            "            result = self.qa_chain(",
            "                {\"prompt\": prompt, \"context\": context},",
            "                callbacks=callbacks,",
            "            )",
            "            final_result = result[self.qa_chain.output_key]",
            "",
            "        chain_result: Dict[str, Any] = {self.output_key: final_result}",
            "        if self.return_intermediate_steps:",
            "            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps",
            "",
            "        return chain_result"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.neptune_sparql.NeptuneSparqlQAChain.from_llm",
            "libs.community.langchain_community.chains.graph_qa.neptune_sparql.NeptuneSparqlQAChain.self",
            "libs.community.langchain_community.chains.graph_qa.neptune_sparql.NeptuneSparqlQAChain",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default"
        ]
    },
    "libs/community/langchain_community/chains/graph_qa/ontotext_graphdb.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "     input_key: str = \"query\"  #: :meta private:"
            },
            "1": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "     output_key: str = \"result\"  #: :meta private:"
            },
            "2": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+    allow_dangerous_requests: bool = False"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+    *Security note*: Make sure that the database connection uses credentials"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+        that are narrowly-scoped to only include necessary permissions."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+        Failure to do so may result in data corruption or loss, since the calling"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+        code may attempt commands that would result in deletion, mutation"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+        of data if appropriately prompted or reading sensitive data if such"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+        data is present in the database."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+        The best way to guard against such negative outcomes is to (as appropriate)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+        limit the permissions granted to the credentials used with this tool."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+        See https://python.langchain.com/docs/security for more information."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+    def __init__(self, **kwargs: Any) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+        \"\"\"Initialize the chain.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+        super().__init__(**kwargs)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+        if self.allow_dangerous_requests is not True:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+            raise ValueError("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+                \"In order to use this chain, you must acknowledge that it can make \""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+                \"You must narrowly scope the permissions of the database connection \""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+                \"to only include necessary permissions. Failure to do so may result \""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+                \"in data corruption or loss or reading sensitive data if such data is \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+                \"present in the database.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+                \"Only use this chain if you understand the risks and have taken the \""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+                \"necessary precautions. \""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+                \"See https://python.langchain.com/docs/security for more information.\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+            )"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "     @property"
            },
            "35": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "     def input_keys(self) -> List[str]:"
            },
            "36": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 82,
                "PatchRowcode": "         return [self.input_key]"
            }
        },
        "frontPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import TYPE_CHECKING, Any, Dict, List, Optional",
            "",
            "if TYPE_CHECKING:",
            "    import rdflib",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks.manager import CallbackManager, CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts.base import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    GRAPHDB_QA_PROMPT,",
            "    GRAPHDB_SPARQL_FIX_PROMPT,",
            "    GRAPHDB_SPARQL_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs import OntotextGraphDBGraph",
            "",
            "",
            "class OntotextGraphDBQAChain(Chain):",
            "    \"\"\"Question-answering against Ontotext GraphDB",
            "       https://graphdb.ontotext.com/ by generating SPARQL queries.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: OntotextGraphDBGraph = Field(exclude=True)",
            "    sparql_generation_chain: LLMChain",
            "    sparql_fix_chain: LLMChain",
            "    max_fix_retries: int",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        sparql_generation_prompt: BasePromptTemplate = GRAPHDB_SPARQL_GENERATION_PROMPT,",
            "        sparql_fix_prompt: BasePromptTemplate = GRAPHDB_SPARQL_FIX_PROMPT,",
            "        max_fix_retries: int = 5,",
            "        qa_prompt: BasePromptTemplate = GRAPHDB_QA_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> OntotextGraphDBQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        sparql_generation_chain = LLMChain(llm=llm, prompt=sparql_generation_prompt)",
            "        sparql_fix_chain = LLMChain(llm=llm, prompt=sparql_fix_prompt)",
            "        max_fix_retries = max_fix_retries",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            sparql_generation_chain=sparql_generation_chain,",
            "            sparql_fix_chain=sparql_fix_chain,",
            "            max_fix_retries=max_fix_retries,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"",
            "        Generate a SPARQL query, use it to retrieve a response from GraphDB and answer",
            "        the question.",
            "        \"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        prompt = inputs[self.input_key]",
            "        ontology_schema = self.graph.get_schema",
            "",
            "        sparql_generation_chain_result = self.sparql_generation_chain.invoke(",
            "            {\"prompt\": prompt, \"schema\": ontology_schema}, callbacks=callbacks",
            "        )",
            "        generated_sparql = sparql_generation_chain_result[",
            "            self.sparql_generation_chain.output_key",
            "        ]",
            "",
            "        generated_sparql = self._get_prepared_sparql_query(",
            "            _run_manager, callbacks, generated_sparql, ontology_schema",
            "        )",
            "        query_results = self._execute_query(generated_sparql)",
            "",
            "        qa_chain_result = self.qa_chain.invoke(",
            "            {\"prompt\": prompt, \"context\": query_results}, callbacks=callbacks",
            "        )",
            "        result = qa_chain_result[self.qa_chain.output_key]",
            "        return {self.output_key: result}",
            "",
            "    def _get_prepared_sparql_query(",
            "        self,",
            "        _run_manager: CallbackManagerForChainRun,",
            "        callbacks: CallbackManager,",
            "        generated_sparql: str,",
            "        ontology_schema: str,",
            "    ) -> str:",
            "        try:",
            "            return self._prepare_sparql_query(_run_manager, generated_sparql)",
            "        except Exception as e:",
            "            retries = 0",
            "            error_message = str(e)",
            "            self._log_invalid_sparql_query(",
            "                _run_manager, generated_sparql, error_message",
            "            )",
            "",
            "            while retries < self.max_fix_retries:",
            "                try:",
            "                    sparql_fix_chain_result = self.sparql_fix_chain.invoke(",
            "                        {",
            "                            \"error_message\": error_message,",
            "                            \"generated_sparql\": generated_sparql,",
            "                            \"schema\": ontology_schema,",
            "                        },",
            "                        callbacks=callbacks,",
            "                    )",
            "                    generated_sparql = sparql_fix_chain_result[",
            "                        self.sparql_fix_chain.output_key",
            "                    ]",
            "                    return self._prepare_sparql_query(_run_manager, generated_sparql)",
            "                except Exception as e:",
            "                    retries += 1",
            "                    parse_exception = str(e)",
            "                    self._log_invalid_sparql_query(",
            "                        _run_manager, generated_sparql, parse_exception",
            "                    )",
            "",
            "        raise ValueError(\"The generated SPARQL query is invalid.\")",
            "",
            "    def _prepare_sparql_query(",
            "        self, _run_manager: CallbackManagerForChainRun, generated_sparql: str",
            "    ) -> str:",
            "        from rdflib.plugins.sparql import prepareQuery",
            "",
            "        prepareQuery(generated_sparql)",
            "        self._log_prepared_sparql_query(_run_manager, generated_sparql)",
            "        return generated_sparql",
            "",
            "    def _log_prepared_sparql_query(",
            "        self, _run_manager: CallbackManagerForChainRun, generated_query: str",
            "    ) -> None:",
            "        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_query, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "    def _log_invalid_sparql_query(",
            "        self,",
            "        _run_manager: CallbackManagerForChainRun,",
            "        generated_query: str,",
            "        error_message: str,",
            "    ) -> None:",
            "        _run_manager.on_text(\"Invalid SPARQL query: \", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_query, color=\"red\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        _run_manager.on_text(",
            "            \"SPARQL Query Parse Error: \", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        _run_manager.on_text(",
            "            error_message, color=\"red\", end=\"\\n\\n\", verbose=self.verbose",
            "        )",
            "",
            "    def _execute_query(self, query: str) -> List[rdflib.query.ResultRow]:",
            "        try:",
            "            return self.graph.query(query)",
            "        except Exception:",
            "            raise ValueError(\"Failed to execute the generated SPARQL query.\")"
        ],
        "afterPatchFile": [
            "\"\"\"Question answering over a graph.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "from typing import TYPE_CHECKING, Any, Dict, List, Optional",
            "",
            "if TYPE_CHECKING:",
            "    import rdflib",
            "",
            "from langchain.chains.base import Chain",
            "from langchain.chains.llm import LLMChain",
            "from langchain_core.callbacks.manager import CallbackManager, CallbackManagerForChainRun",
            "from langchain_core.language_models import BaseLanguageModel",
            "from langchain_core.prompts.base import BasePromptTemplate",
            "from langchain_core.pydantic_v1 import Field",
            "",
            "from langchain_community.chains.graph_qa.prompts import (",
            "    GRAPHDB_QA_PROMPT,",
            "    GRAPHDB_SPARQL_FIX_PROMPT,",
            "    GRAPHDB_SPARQL_GENERATION_PROMPT,",
            ")",
            "from langchain_community.graphs import OntotextGraphDBGraph",
            "",
            "",
            "class OntotextGraphDBQAChain(Chain):",
            "    \"\"\"Question-answering against Ontotext GraphDB",
            "       https://graphdb.ontotext.com/ by generating SPARQL queries.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    graph: OntotextGraphDBGraph = Field(exclude=True)",
            "    sparql_generation_chain: LLMChain",
            "    sparql_fix_chain: LLMChain",
            "    max_fix_retries: int",
            "    qa_chain: LLMChain",
            "    input_key: str = \"query\"  #: :meta private:",
            "    output_key: str = \"result\"  #: :meta private:",
            "",
            "    allow_dangerous_requests: bool = False",
            "    \"\"\"Forced user opt-in to acknowledge that the chain can make dangerous requests.",
            "",
            "    *Security note*: Make sure that the database connection uses credentials",
            "        that are narrowly-scoped to only include necessary permissions.",
            "        Failure to do so may result in data corruption or loss, since the calling",
            "        code may attempt commands that would result in deletion, mutation",
            "        of data if appropriately prompted or reading sensitive data if such",
            "        data is present in the database.",
            "        The best way to guard against such negative outcomes is to (as appropriate)",
            "        limit the permissions granted to the credentials used with this tool.",
            "",
            "        See https://python.langchain.com/docs/security for more information.",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"Initialize the chain.\"\"\"",
            "        super().__init__(**kwargs)",
            "        if self.allow_dangerous_requests is not True:",
            "            raise ValueError(",
            "                \"In order to use this chain, you must acknowledge that it can make \"",
            "                \"dangerous requests by setting `allow_dangerous_requests` to `True`.\"",
            "                \"You must narrowly scope the permissions of the database connection \"",
            "                \"to only include necessary permissions. Failure to do so may result \"",
            "                \"in data corruption or loss or reading sensitive data if such data is \"",
            "                \"present in the database.\"",
            "                \"Only use this chain if you understand the risks and have taken the \"",
            "                \"necessary precautions. \"",
            "                \"See https://python.langchain.com/docs/security for more information.\"",
            "            )",
            "",
            "    @property",
            "    def input_keys(self) -> List[str]:",
            "        return [self.input_key]",
            "",
            "    @property",
            "    def output_keys(self) -> List[str]:",
            "        _output_keys = [self.output_key]",
            "        return _output_keys",
            "",
            "    @classmethod",
            "    def from_llm(",
            "        cls,",
            "        llm: BaseLanguageModel,",
            "        *,",
            "        sparql_generation_prompt: BasePromptTemplate = GRAPHDB_SPARQL_GENERATION_PROMPT,",
            "        sparql_fix_prompt: BasePromptTemplate = GRAPHDB_SPARQL_FIX_PROMPT,",
            "        max_fix_retries: int = 5,",
            "        qa_prompt: BasePromptTemplate = GRAPHDB_QA_PROMPT,",
            "        **kwargs: Any,",
            "    ) -> OntotextGraphDBQAChain:",
            "        \"\"\"Initialize from LLM.\"\"\"",
            "        sparql_generation_chain = LLMChain(llm=llm, prompt=sparql_generation_prompt)",
            "        sparql_fix_chain = LLMChain(llm=llm, prompt=sparql_fix_prompt)",
            "        max_fix_retries = max_fix_retries",
            "        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)",
            "        return cls(",
            "            qa_chain=qa_chain,",
            "            sparql_generation_chain=sparql_generation_chain,",
            "            sparql_fix_chain=sparql_fix_chain,",
            "            max_fix_retries=max_fix_retries,",
            "            **kwargs,",
            "        )",
            "",
            "    def _call(",
            "        self,",
            "        inputs: Dict[str, Any],",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,",
            "    ) -> Dict[str, str]:",
            "        \"\"\"",
            "        Generate a SPARQL query, use it to retrieve a response from GraphDB and answer",
            "        the question.",
            "        \"\"\"",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()",
            "        callbacks = _run_manager.get_child()",
            "        prompt = inputs[self.input_key]",
            "        ontology_schema = self.graph.get_schema",
            "",
            "        sparql_generation_chain_result = self.sparql_generation_chain.invoke(",
            "            {\"prompt\": prompt, \"schema\": ontology_schema}, callbacks=callbacks",
            "        )",
            "        generated_sparql = sparql_generation_chain_result[",
            "            self.sparql_generation_chain.output_key",
            "        ]",
            "",
            "        generated_sparql = self._get_prepared_sparql_query(",
            "            _run_manager, callbacks, generated_sparql, ontology_schema",
            "        )",
            "        query_results = self._execute_query(generated_sparql)",
            "",
            "        qa_chain_result = self.qa_chain.invoke(",
            "            {\"prompt\": prompt, \"context\": query_results}, callbacks=callbacks",
            "        )",
            "        result = qa_chain_result[self.qa_chain.output_key]",
            "        return {self.output_key: result}",
            "",
            "    def _get_prepared_sparql_query(",
            "        self,",
            "        _run_manager: CallbackManagerForChainRun,",
            "        callbacks: CallbackManager,",
            "        generated_sparql: str,",
            "        ontology_schema: str,",
            "    ) -> str:",
            "        try:",
            "            return self._prepare_sparql_query(_run_manager, generated_sparql)",
            "        except Exception as e:",
            "            retries = 0",
            "            error_message = str(e)",
            "            self._log_invalid_sparql_query(",
            "                _run_manager, generated_sparql, error_message",
            "            )",
            "",
            "            while retries < self.max_fix_retries:",
            "                try:",
            "                    sparql_fix_chain_result = self.sparql_fix_chain.invoke(",
            "                        {",
            "                            \"error_message\": error_message,",
            "                            \"generated_sparql\": generated_sparql,",
            "                            \"schema\": ontology_schema,",
            "                        },",
            "                        callbacks=callbacks,",
            "                    )",
            "                    generated_sparql = sparql_fix_chain_result[",
            "                        self.sparql_fix_chain.output_key",
            "                    ]",
            "                    return self._prepare_sparql_query(_run_manager, generated_sparql)",
            "                except Exception as e:",
            "                    retries += 1",
            "                    parse_exception = str(e)",
            "                    self._log_invalid_sparql_query(",
            "                        _run_manager, generated_sparql, parse_exception",
            "                    )",
            "",
            "        raise ValueError(\"The generated SPARQL query is invalid.\")",
            "",
            "    def _prepare_sparql_query(",
            "        self, _run_manager: CallbackManagerForChainRun, generated_sparql: str",
            "    ) -> str:",
            "        from rdflib.plugins.sparql import prepareQuery",
            "",
            "        prepareQuery(generated_sparql)",
            "        self._log_prepared_sparql_query(_run_manager, generated_sparql)",
            "        return generated_sparql",
            "",
            "    def _log_prepared_sparql_query(",
            "        self, _run_manager: CallbackManagerForChainRun, generated_query: str",
            "    ) -> None:",
            "        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_query, color=\"green\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "",
            "    def _log_invalid_sparql_query(",
            "        self,",
            "        _run_manager: CallbackManagerForChainRun,",
            "        generated_query: str,",
            "        error_message: str,",
            "    ) -> None:",
            "        _run_manager.on_text(\"Invalid SPARQL query: \", end=\"\\n\", verbose=self.verbose)",
            "        _run_manager.on_text(",
            "            generated_query, color=\"red\", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        _run_manager.on_text(",
            "            \"SPARQL Query Parse Error: \", end=\"\\n\", verbose=self.verbose",
            "        )",
            "        _run_manager.on_text(",
            "            error_message, color=\"red\", end=\"\\n\\n\", verbose=self.verbose",
            "        )",
            "",
            "    def _execute_query(self, query: str) -> List[rdflib.query.ResultRow]:",
            "        try:",
            "            return self.graph.query(query)",
            "        except Exception:",
            "            raise ValueError(\"Failed to execute the generated SPARQL query.\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "libs.community.langchain_community.chains.graph_qa.ontotext_graphdb.OntotextGraphDBQAChain.self",
            "libs.community.langchain_community.chains.graph_qa.ontotext_graphdb.OntotextGraphDBQAChain",
            "django.contrib.auth.tests.test_views.PasswordResetTest.test_confirm_redirect_default",
            "libs.community.langchain_community.chains.graph_qa.ontotext_graphdb.OntotextGraphDBQAChain.from_llm"
        ]
    }
}