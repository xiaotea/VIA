{
    "glance/async_/flows/base_import.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "                                                'bfile': backing_file}"
            },
            "1": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "             raise RuntimeError(msg)"
            },
            "2": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 183,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+        try:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+            data_file = metadata['format-specific']['data']['data-file']"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+        except KeyError:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+            data_file = None"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+        if data_file is not None:"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+            msg = _(\"File %(path)s has invalid data-file \""
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+                    \"%(dfile)s, aborting.\") % {\"path\": path,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+                                               \"dfile\": data_file}"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+            raise RuntimeError(msg)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 193,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 194,
                "PatchRowcode": "         return path"
            },
            "14": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 195,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 196,
                "PatchRowcode": "     def revert(self, image_id, result, **kwargs):"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 OpenStack Foundation",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import json",
            "import os",
            "",
            "import glance_store as store_api",
            "from glance_store import backend",
            "from oslo_concurrency import processutils as putils",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_utils import encodeutils",
            "from oslo_utils import excutils",
            "from stevedore import named",
            "from taskflow.patterns import linear_flow as lf",
            "from taskflow import retry",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "from glance.async_ import utils",
            "from glance.common import exception",
            "from glance.common.scripts.image_import import main as image_import",
            "from glance.common.scripts import utils as script_utils",
            "from glance.i18n import _, _LE, _LI",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "CONF = cfg.CONF",
            "",
            "",
            "class _CreateImage(task.Task):",
            "",
            "    default_provides = 'image_id'",
            "",
            "    def __init__(self, task_id, task_type, task_repo, image_repo,",
            "                 image_factory):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.task_repo = task_repo",
            "        self.image_repo = image_repo",
            "        self.image_factory = image_factory",
            "        super(_CreateImage, self).__init__(",
            "            name='%s-CreateImage-%s' % (task_type, task_id))",
            "",
            "    def execute(self):",
            "        task = script_utils.get_task(self.task_repo, self.task_id)",
            "        if task is None:",
            "            return",
            "        task_input = script_utils.unpack_task_input(task)",
            "        image = image_import.create_image(",
            "            self.image_repo, self.image_factory,",
            "            task_input.get('image_properties'), self.task_id)",
            "",
            "        LOG.debug(\"Task %(task_id)s created image %(image_id)s\",",
            "                  {'task_id': task.task_id, 'image_id': image.image_id})",
            "        return image.image_id",
            "",
            "    def revert(self, *args, **kwargs):",
            "        # TODO(NiallBunting): Deleting the image like this could be considered",
            "        # a brute force way of reverting images. It may be worth checking if",
            "        # data has been written.",
            "        result = kwargs.get('result', None)",
            "        if result is not None:",
            "            if kwargs.get('flow_failures', None) is not None:",
            "                image = self.image_repo.get(result)",
            "                LOG.debug(\"Deleting image whilst reverting.\")",
            "                image.delete()",
            "                self.image_repo.remove(image)",
            "",
            "",
            "class _ImportToFS(task.Task):",
            "",
            "    default_provides = 'file_path'",
            "",
            "    def __init__(self, task_id, task_type, task_repo, uri):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.task_repo = task_repo",
            "        self.uri = uri",
            "        super(_ImportToFS, self).__init__(",
            "            name='%s-ImportToFS-%s' % (task_type, task_id))",
            "",
            "        # NOTE(abhishekk): Use reserved 'os_glance_tasks_store' for tasks,",
            "        # the else part will be removed once old way of configuring store",
            "        # is deprecated.",
            "        if CONF.enabled_backends:",
            "            self.store = store_api.get_store_from_store_identifier(",
            "                'os_glance_tasks_store')",
            "        else:",
            "            if CONF.task.work_dir is None:",
            "                msg = (_(\"%(task_id)s of %(task_type)s not configured \"",
            "                         \"properly. Missing work dir: %(work_dir)s\") %",
            "                       {'task_id': self.task_id,",
            "                        'task_type': self.task_type,",
            "                        'work_dir': CONF.task.work_dir})",
            "                raise exception.BadTaskConfiguration(msg)",
            "",
            "            self.store = self._build_store()",
            "",
            "    def _build_store(self):",
            "        # NOTE(flaper87): Due to the nice glance_store api (#sarcasm), we're",
            "        # forced to build our own config object, register the required options",
            "        # (and by required I mean *ALL* of them, even the ones we don't want),",
            "        # and create our own store instance by calling a private function.",
            "        # This is certainly unfortunate but it's the best we can do until the",
            "        # glance_store refactor is done. A good thing is that glance_store is",
            "        # under our team's management and it gates on Glance so changes to",
            "        # this API will (should?) break task's tests.",
            "        conf = cfg.ConfigOpts()",
            "        backend.register_opts(conf)",
            "        conf.set_override('filesystem_store_datadir',",
            "                          CONF.task.work_dir,",
            "                          group='glance_store')",
            "",
            "        # NOTE(flaper87): Do not even try to judge me for this... :(",
            "        # With the glance_store refactor, this code will change, until",
            "        # that happens, we don't have a better option and this is the",
            "        # least worst one, IMHO.",
            "        store = backend._load_store(conf, 'file')",
            "",
            "        if store is None:",
            "            msg = (_(\"%(task_id)s of %(task_type)s not configured \"",
            "                     \"properly. Could not load the filesystem store\") %",
            "                   {'task_id': self.task_id, 'task_type': self.task_type})",
            "            raise exception.BadTaskConfiguration(msg)",
            "",
            "        store.configure()",
            "        return store",
            "",
            "    def execute(self, image_id):",
            "        \"\"\"Create temp file into store and return path to it",
            "",
            "        :param image_id: Glance Image ID",
            "        \"\"\"",
            "        # NOTE(flaper87): We've decided to use a separate `work_dir` for",
            "        # this task - and tasks coming after this one - as a way to expect",
            "        # users to configure a local store for pre-import works on the image",
            "        # to happen.",
            "        #",
            "        # While using any path should be \"technically\" fine, it's not what",
            "        # we recommend as the best solution. For more details on this, please",
            "        # refer to the comment in the `_ImportToStore.execute` method.",
            "        data = script_utils.get_image_data_iter(self.uri)",
            "",
            "        path = self.store.add(image_id, data, 0, context=None)[0]",
            "",
            "        try:",
            "            # NOTE(flaper87): Consider moving this code to a common",
            "            # place that other tasks can consume as well.",
            "            stdout, stderr = putils.trycmd('qemu-img', 'info',",
            "                                           '--output=json', path,",
            "                                           prlimit=utils.QEMU_IMG_PROC_LIMITS,",
            "                                           log_errors=putils.LOG_ALL_ERRORS)",
            "        except OSError as exc:",
            "            with excutils.save_and_reraise_exception():",
            "                exc_message = encodeutils.exception_to_unicode(exc)",
            "                msg = _LE('Failed to execute security checks on the image '",
            "                          '%(task_id)s: %(exc)s')",
            "                LOG.error(msg, {'task_id': self.task_id, 'exc': exc_message})",
            "",
            "        metadata = json.loads(stdout)",
            "",
            "        backing_file = metadata.get('backing-filename')",
            "        if backing_file is not None:",
            "            msg = _(\"File %(path)s has invalid backing file \"",
            "                    \"%(bfile)s, aborting.\") % {'path': path,",
            "                                               'bfile': backing_file}",
            "            raise RuntimeError(msg)",
            "",
            "        return path",
            "",
            "    def revert(self, image_id, result, **kwargs):",
            "        if isinstance(result, failure.Failure):",
            "            LOG.exception(_LE('Task: %(task_id)s failed to import image '",
            "                              '%(image_id)s to the filesystem.'),",
            "                          {'task_id': self.task_id, 'image_id': image_id})",
            "            return",
            "",
            "        if os.path.exists(result.split(\"file://\")[-1]):",
            "            if CONF.enabled_backends:",
            "                store_api.delete(result, 'os_glance_tasks_store')",
            "            else:",
            "                store_api.delete_from_backend(result)",
            "",
            "",
            "class _DeleteFromFS(task.Task):",
            "",
            "    def __init__(self, task_id, task_type):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        super(_DeleteFromFS, self).__init__(",
            "            name='%s-DeleteFromFS-%s' % (task_type, task_id))",
            "",
            "    def execute(self, file_path):",
            "        \"\"\"Remove file from the backend",
            "",
            "        :param file_path: path to the file being deleted",
            "        \"\"\"",
            "        if CONF.enabled_backends:",
            "            store_api.delete(file_path, 'os_glance_tasks_store')",
            "        else:",
            "            store_api.delete_from_backend(file_path)",
            "",
            "",
            "class _ImportToStore(task.Task):",
            "",
            "    def __init__(self, task_id, task_type, image_repo, uri, backend):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.image_repo = image_repo",
            "        self.uri = uri",
            "        self.backend = backend",
            "        super(_ImportToStore, self).__init__(",
            "            name='%s-ImportToStore-%s' % (task_type, task_id))",
            "",
            "    def execute(self, image_id, file_path=None):",
            "        \"\"\"Bringing the introspected image to back end store",
            "",
            "        :param image_id: Glance Image ID",
            "        :param file_path: path to the image file",
            "        \"\"\"",
            "        # NOTE(flaper87): There are a couple of interesting bits in the",
            "        # interaction between this task and the `_ImportToFS` one. I'll try",
            "        # to cover them in this comment.",
            "        #",
            "        # NOTE(flaper87):",
            "        # `_ImportToFS` downloads the image to a dedicated `work_dir` which",
            "        # needs to be configured in advance (please refer to the config option",
            "        # docs for more info). The motivation behind this is also explained in",
            "        # the `_ImportToFS.execute` method.",
            "        #",
            "        # Due to the fact that we have an `_ImportToFS` task which downloads",
            "        # the image data already, we need to be as smart as we can in this task",
            "        # to avoid downloading the data several times and reducing the copy or",
            "        # write times. There are several scenarios where the interaction",
            "        # between this task and `_ImportToFS` could be improved. All these",
            "        # scenarios assume the `_ImportToFS` task has been executed before",
            "        # and/or in a more abstract scenario, that `file_path` is being",
            "        # provided.",
            "        #",
            "        # Scenario 1: FS Store is Remote, introspection enabled,",
            "        # conversion disabled",
            "        #",
            "        # In this scenario, the user would benefit from having the scratch path",
            "        # being the same path as the fs store. Only one write would happen and",
            "        # an extra read will happen in order to introspect the image. Note that",
            "        # this read is just for the image headers and not the entire file.",
            "        #",
            "        # Scenario 2: FS Store is remote, introspection enabled,",
            "        # conversion enabled",
            "        #",
            "        # In this scenario, the user would benefit from having a *local* store",
            "        # into which the image can be converted. This will require downloading",
            "        # the image locally, converting it and then copying the converted image",
            "        # to the remote store.",
            "        #",
            "        # Scenario 3: FS Store is local, introspection enabled,",
            "        # conversion disabled",
            "        # Scenario 4: FS Store is local, introspection enabled,",
            "        # conversion enabled",
            "        #",
            "        # In both these scenarios the user shouldn't care if the FS",
            "        # store path and the work dir are the same, therefore probably",
            "        # benefit, about the scratch path and the FS store being the",
            "        # same from a performance perspective. Space wise, regardless",
            "        # of the scenario, the user will have to account for it in",
            "        # advance.",
            "        #",
            "        # Lets get to it and identify the different scenarios in the",
            "        # implementation",
            "        image = self.image_repo.get(image_id)",
            "        image.status = 'saving'",
            "        self.image_repo.save(image)",
            "",
            "        # NOTE(flaper87): Let's dance... and fall",
            "        #",
            "        # Unfortunately, because of the way our domain layers work and",
            "        # the checks done in the FS store, we can't simply rename the file",
            "        # and set the location. To do that, we'd have to duplicate the logic",
            "        # of every and each of the domain factories (quota, location, etc)",
            "        # and we'd also need to hack the FS store to prevent it from raising",
            "        # a \"duplication path\" error. I'd rather have this task copying the",
            "        # image bits one more time than duplicating all that logic.",
            "        #",
            "        # Since I don't think this should be the definitive solution, I'm",
            "        # leaving the code below as a reference for what should happen here",
            "        # once the FS store and domain code will be able to handle this case.",
            "        #",
            "        # if file_path is None:",
            "        #    image_import.set_image_data(image, self.uri, None)",
            "        #    return",
            "",
            "        # NOTE(flaper87): Don't assume the image was stored in the",
            "        # work_dir. Think in the case this path was provided by another task.",
            "        # Also, lets try to neither assume things nor create \"logic\"",
            "        # dependencies between this task and `_ImportToFS`",
            "        #",
            "        # base_path = os.path.dirname(file_path.split(\"file://\")[-1])",
            "",
            "        # NOTE(flaper87): Hopefully just scenarios #3 and #4. I say",
            "        # hopefully because nothing prevents the user to use the same",
            "        # FS store path as a work dir",
            "        #",
            "        # image_path = os.path.join(base_path, image_id)",
            "        #",
            "        # if (base_path == CONF.glance_store.filesystem_store_datadir or",
            "        #      base_path in CONF.glance_store.filesystem_store_datadirs):",
            "        #     os.rename(file_path, image_path)",
            "        #",
            "        # image_import.set_image_data(image, image_path, None)",
            "        try:",
            "            image_import.set_image_data(image,",
            "                                        file_path or self.uri, self.task_id,",
            "                                        backend=self.backend)",
            "        except IOError as e:",
            "            msg = (_('Uploading the image failed due to: %(exc)s') %",
            "                   {'exc': encodeutils.exception_to_unicode(e)})",
            "            LOG.error(msg)",
            "            raise exception.UploadException(message=msg)",
            "        # NOTE(flaper87): We need to save the image again after the locations",
            "        # have been set in the image.",
            "        self.image_repo.save(image)",
            "",
            "",
            "class _SaveImage(task.Task):",
            "",
            "    def __init__(self, task_id, task_type, image_repo):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.image_repo = image_repo",
            "        super(_SaveImage, self).__init__(",
            "            name='%s-SaveImage-%s' % (task_type, task_id))",
            "",
            "    def execute(self, image_id):",
            "        \"\"\"Transition image status to active",
            "",
            "        :param image_id: Glance Image ID",
            "        \"\"\"",
            "        new_image = self.image_repo.get(image_id)",
            "        if new_image.status == 'saving':",
            "            # NOTE(flaper87): THIS IS WRONG!",
            "            # we should be doing atomic updates to avoid",
            "            # race conditions. This happens in other places",
            "            # too.",
            "            new_image.status = 'active'",
            "        self.image_repo.save(new_image)",
            "",
            "",
            "class _CompleteTask(task.Task):",
            "",
            "    def __init__(self, task_id, task_type, task_repo):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.task_repo = task_repo",
            "        super(_CompleteTask, self).__init__(",
            "            name='%s-CompleteTask-%s' % (task_type, task_id))",
            "",
            "    def execute(self, image_id):",
            "        \"\"\"Finishing the task flow",
            "",
            "        :param image_id: Glance Image ID",
            "        \"\"\"",
            "        task = script_utils.get_task(self.task_repo, self.task_id)",
            "        if task is None:",
            "            return",
            "        try:",
            "            task.succeed({'image_id': image_id})",
            "        except Exception as e:",
            "            # Note: The message string contains Error in it to indicate",
            "            # in the task.message that it's a error message for the user.",
            "",
            "            # TODO(nikhil): need to bring back save_and_reraise_exception when",
            "            # necessary",
            "            log_msg = _LE(\"Task ID %(task_id)s failed. Error: %(exc_type)s: \"",
            "                          \"%(e)s\")",
            "            LOG.exception(log_msg, {'exc_type': str(type(e)),",
            "                                    'e': encodeutils.exception_to_unicode(e),",
            "                                    'task_id': task.task_id})",
            "",
            "            err_msg = _(\"Error: %(exc_type)s: %(e)s\")",
            "            task.fail(err_msg % {'exc_type': str(type(e)),",
            "                                 'e': encodeutils.exception_to_unicode(e)})",
            "        finally:",
            "            self.task_repo.save(task)",
            "",
            "        LOG.info(_LI(\"%(task_id)s of %(task_type)s completed\"),",
            "                 {'task_id': self.task_id, 'task_type': self.task_type})",
            "",
            "",
            "def _get_import_flows(**kwargs):",
            "    # NOTE(flaper87): Until we have a better infrastructure to enable",
            "    # and disable tasks plugins, hard-code the tasks we know exist,",
            "    # instead of loading everything from the namespace. This guarantees",
            "    # both, the load order of these plugins and the fact that no random",
            "    # plugins will be added/loaded until we feel comfortable with this.",
            "    # Future patches will keep using NamedExtensionManager but they'll",
            "    # rely on a config option to control this process.",
            "    extensions = named.NamedExtensionManager('glance.flows.import',",
            "                                             names=['ovf_process',",
            "                                                    'convert',",
            "                                                    'introspect'],",
            "                                             name_order=True,",
            "                                             invoke_on_load=True,",
            "                                             invoke_kwds=kwargs)",
            "",
            "    for ext in extensions.extensions:",
            "        yield ext.obj",
            "",
            "",
            "def get_flow(**kwargs):",
            "    \"\"\"Return task flow",
            "",
            "    :param task_id: Task ID",
            "    :param task_type: Type of the task",
            "    :param task_repo: Task repo",
            "    :param image_repo: Image repository used",
            "    :param image_factory: Glance Image Factory",
            "    :param uri: uri for the image file",
            "    \"\"\"",
            "    task_id = kwargs.get('task_id')",
            "    task_type = kwargs.get('task_type')",
            "    task_repo = kwargs.get('task_repo')",
            "    image_repo = kwargs.get('image_repo')",
            "    image_factory = kwargs.get('image_factory')",
            "    uri = kwargs.get('uri')",
            "    backend = kwargs.get('backend')",
            "",
            "    flow = lf.Flow(task_type, retry=retry.AlwaysRevert()).add(",
            "        _CreateImage(task_id, task_type, task_repo, image_repo, image_factory))",
            "",
            "    import_to_store = _ImportToStore(task_id, task_type, image_repo, uri,",
            "                                     backend)",
            "",
            "    try:",
            "        # NOTE(flaper87): ImportToLocal and DeleteFromLocal shouldn't be here.",
            "        # Ideally, we should have the different import flows doing this for us",
            "        # and this function should clean up duplicated tasks. For example, say",
            "        # 2 flows need to have a local copy of the image - ImportToLocal - in",
            "        # order to be able to complete the task - i.e Introspect-. In that",
            "        # case, the introspect.get_flow call should add both, ImportToLocal and",
            "        # DeleteFromLocal, to the flow and this function will reduce the",
            "        # duplicated calls to those tasks by creating a linear flow that",
            "        # ensures those are called before the other tasks.  For now, I'm",
            "        # keeping them here, though.",
            "        limbo = lf.Flow(task_type).add(_ImportToFS(task_id,",
            "                                                   task_type,",
            "                                                   task_repo,",
            "                                                   uri))",
            "",
            "        for subflow in _get_import_flows(**kwargs):",
            "            limbo.add(subflow)",
            "",
            "        # NOTE(flaper87): We have hard-coded 2 tasks,",
            "        # if there aren't more than 2, it means that",
            "        # no subtask has been registered.",
            "        if len(limbo) > 1:",
            "            flow.add(limbo)",
            "",
            "            # NOTE(flaper87): Until this implementation gets smarter,",
            "            # make sure ImportToStore is called *after* the imported",
            "            # flow stages. If not, the image will be set to saving state",
            "            # invalidating tasks like Introspection or Convert.",
            "            flow.add(import_to_store)",
            "",
            "            # NOTE(flaper87): Since this is an \"optional\" task but required",
            "            # when `limbo` is executed, we're adding it in its own subflow",
            "            # to isolate it from the rest of the flow.",
            "            delete_flow = lf.Flow(task_type).add(_DeleteFromFS(task_id,",
            "                                                               task_type))",
            "            flow.add(delete_flow)",
            "        else:",
            "            flow.add(import_to_store)",
            "    except exception.BadTaskConfiguration as exc:",
            "        # NOTE(flaper87): If something goes wrong with the load of",
            "        # import tasks, make sure we go on.",
            "        LOG.error(_LE('Bad task configuration: %s'), exc.message)",
            "        flow.add(import_to_store)",
            "",
            "    flow.add(",
            "        _SaveImage(task_id, task_type, image_repo),",
            "        _CompleteTask(task_id, task_type, task_repo)",
            "    )",
            "    return flow"
        ],
        "afterPatchFile": [
            "# Copyright 2015 OpenStack Foundation",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import json",
            "import os",
            "",
            "import glance_store as store_api",
            "from glance_store import backend",
            "from oslo_concurrency import processutils as putils",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_utils import encodeutils",
            "from oslo_utils import excutils",
            "from stevedore import named",
            "from taskflow.patterns import linear_flow as lf",
            "from taskflow import retry",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "from glance.async_ import utils",
            "from glance.common import exception",
            "from glance.common.scripts.image_import import main as image_import",
            "from glance.common.scripts import utils as script_utils",
            "from glance.i18n import _, _LE, _LI",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "CONF = cfg.CONF",
            "",
            "",
            "class _CreateImage(task.Task):",
            "",
            "    default_provides = 'image_id'",
            "",
            "    def __init__(self, task_id, task_type, task_repo, image_repo,",
            "                 image_factory):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.task_repo = task_repo",
            "        self.image_repo = image_repo",
            "        self.image_factory = image_factory",
            "        super(_CreateImage, self).__init__(",
            "            name='%s-CreateImage-%s' % (task_type, task_id))",
            "",
            "    def execute(self):",
            "        task = script_utils.get_task(self.task_repo, self.task_id)",
            "        if task is None:",
            "            return",
            "        task_input = script_utils.unpack_task_input(task)",
            "        image = image_import.create_image(",
            "            self.image_repo, self.image_factory,",
            "            task_input.get('image_properties'), self.task_id)",
            "",
            "        LOG.debug(\"Task %(task_id)s created image %(image_id)s\",",
            "                  {'task_id': task.task_id, 'image_id': image.image_id})",
            "        return image.image_id",
            "",
            "    def revert(self, *args, **kwargs):",
            "        # TODO(NiallBunting): Deleting the image like this could be considered",
            "        # a brute force way of reverting images. It may be worth checking if",
            "        # data has been written.",
            "        result = kwargs.get('result', None)",
            "        if result is not None:",
            "            if kwargs.get('flow_failures', None) is not None:",
            "                image = self.image_repo.get(result)",
            "                LOG.debug(\"Deleting image whilst reverting.\")",
            "                image.delete()",
            "                self.image_repo.remove(image)",
            "",
            "",
            "class _ImportToFS(task.Task):",
            "",
            "    default_provides = 'file_path'",
            "",
            "    def __init__(self, task_id, task_type, task_repo, uri):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.task_repo = task_repo",
            "        self.uri = uri",
            "        super(_ImportToFS, self).__init__(",
            "            name='%s-ImportToFS-%s' % (task_type, task_id))",
            "",
            "        # NOTE(abhishekk): Use reserved 'os_glance_tasks_store' for tasks,",
            "        # the else part will be removed once old way of configuring store",
            "        # is deprecated.",
            "        if CONF.enabled_backends:",
            "            self.store = store_api.get_store_from_store_identifier(",
            "                'os_glance_tasks_store')",
            "        else:",
            "            if CONF.task.work_dir is None:",
            "                msg = (_(\"%(task_id)s of %(task_type)s not configured \"",
            "                         \"properly. Missing work dir: %(work_dir)s\") %",
            "                       {'task_id': self.task_id,",
            "                        'task_type': self.task_type,",
            "                        'work_dir': CONF.task.work_dir})",
            "                raise exception.BadTaskConfiguration(msg)",
            "",
            "            self.store = self._build_store()",
            "",
            "    def _build_store(self):",
            "        # NOTE(flaper87): Due to the nice glance_store api (#sarcasm), we're",
            "        # forced to build our own config object, register the required options",
            "        # (and by required I mean *ALL* of them, even the ones we don't want),",
            "        # and create our own store instance by calling a private function.",
            "        # This is certainly unfortunate but it's the best we can do until the",
            "        # glance_store refactor is done. A good thing is that glance_store is",
            "        # under our team's management and it gates on Glance so changes to",
            "        # this API will (should?) break task's tests.",
            "        conf = cfg.ConfigOpts()",
            "        backend.register_opts(conf)",
            "        conf.set_override('filesystem_store_datadir',",
            "                          CONF.task.work_dir,",
            "                          group='glance_store')",
            "",
            "        # NOTE(flaper87): Do not even try to judge me for this... :(",
            "        # With the glance_store refactor, this code will change, until",
            "        # that happens, we don't have a better option and this is the",
            "        # least worst one, IMHO.",
            "        store = backend._load_store(conf, 'file')",
            "",
            "        if store is None:",
            "            msg = (_(\"%(task_id)s of %(task_type)s not configured \"",
            "                     \"properly. Could not load the filesystem store\") %",
            "                   {'task_id': self.task_id, 'task_type': self.task_type})",
            "            raise exception.BadTaskConfiguration(msg)",
            "",
            "        store.configure()",
            "        return store",
            "",
            "    def execute(self, image_id):",
            "        \"\"\"Create temp file into store and return path to it",
            "",
            "        :param image_id: Glance Image ID",
            "        \"\"\"",
            "        # NOTE(flaper87): We've decided to use a separate `work_dir` for",
            "        # this task - and tasks coming after this one - as a way to expect",
            "        # users to configure a local store for pre-import works on the image",
            "        # to happen.",
            "        #",
            "        # While using any path should be \"technically\" fine, it's not what",
            "        # we recommend as the best solution. For more details on this, please",
            "        # refer to the comment in the `_ImportToStore.execute` method.",
            "        data = script_utils.get_image_data_iter(self.uri)",
            "",
            "        path = self.store.add(image_id, data, 0, context=None)[0]",
            "",
            "        try:",
            "            # NOTE(flaper87): Consider moving this code to a common",
            "            # place that other tasks can consume as well.",
            "            stdout, stderr = putils.trycmd('qemu-img', 'info',",
            "                                           '--output=json', path,",
            "                                           prlimit=utils.QEMU_IMG_PROC_LIMITS,",
            "                                           log_errors=putils.LOG_ALL_ERRORS)",
            "        except OSError as exc:",
            "            with excutils.save_and_reraise_exception():",
            "                exc_message = encodeutils.exception_to_unicode(exc)",
            "                msg = _LE('Failed to execute security checks on the image '",
            "                          '%(task_id)s: %(exc)s')",
            "                LOG.error(msg, {'task_id': self.task_id, 'exc': exc_message})",
            "",
            "        metadata = json.loads(stdout)",
            "",
            "        backing_file = metadata.get('backing-filename')",
            "        if backing_file is not None:",
            "            msg = _(\"File %(path)s has invalid backing file \"",
            "                    \"%(bfile)s, aborting.\") % {'path': path,",
            "                                               'bfile': backing_file}",
            "            raise RuntimeError(msg)",
            "",
            "        try:",
            "            data_file = metadata['format-specific']['data']['data-file']",
            "        except KeyError:",
            "            data_file = None",
            "        if data_file is not None:",
            "            msg = _(\"File %(path)s has invalid data-file \"",
            "                    \"%(dfile)s, aborting.\") % {\"path\": path,",
            "                                               \"dfile\": data_file}",
            "            raise RuntimeError(msg)",
            "",
            "        return path",
            "",
            "    def revert(self, image_id, result, **kwargs):",
            "        if isinstance(result, failure.Failure):",
            "            LOG.exception(_LE('Task: %(task_id)s failed to import image '",
            "                              '%(image_id)s to the filesystem.'),",
            "                          {'task_id': self.task_id, 'image_id': image_id})",
            "            return",
            "",
            "        if os.path.exists(result.split(\"file://\")[-1]):",
            "            if CONF.enabled_backends:",
            "                store_api.delete(result, 'os_glance_tasks_store')",
            "            else:",
            "                store_api.delete_from_backend(result)",
            "",
            "",
            "class _DeleteFromFS(task.Task):",
            "",
            "    def __init__(self, task_id, task_type):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        super(_DeleteFromFS, self).__init__(",
            "            name='%s-DeleteFromFS-%s' % (task_type, task_id))",
            "",
            "    def execute(self, file_path):",
            "        \"\"\"Remove file from the backend",
            "",
            "        :param file_path: path to the file being deleted",
            "        \"\"\"",
            "        if CONF.enabled_backends:",
            "            store_api.delete(file_path, 'os_glance_tasks_store')",
            "        else:",
            "            store_api.delete_from_backend(file_path)",
            "",
            "",
            "class _ImportToStore(task.Task):",
            "",
            "    def __init__(self, task_id, task_type, image_repo, uri, backend):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.image_repo = image_repo",
            "        self.uri = uri",
            "        self.backend = backend",
            "        super(_ImportToStore, self).__init__(",
            "            name='%s-ImportToStore-%s' % (task_type, task_id))",
            "",
            "    def execute(self, image_id, file_path=None):",
            "        \"\"\"Bringing the introspected image to back end store",
            "",
            "        :param image_id: Glance Image ID",
            "        :param file_path: path to the image file",
            "        \"\"\"",
            "        # NOTE(flaper87): There are a couple of interesting bits in the",
            "        # interaction between this task and the `_ImportToFS` one. I'll try",
            "        # to cover them in this comment.",
            "        #",
            "        # NOTE(flaper87):",
            "        # `_ImportToFS` downloads the image to a dedicated `work_dir` which",
            "        # needs to be configured in advance (please refer to the config option",
            "        # docs for more info). The motivation behind this is also explained in",
            "        # the `_ImportToFS.execute` method.",
            "        #",
            "        # Due to the fact that we have an `_ImportToFS` task which downloads",
            "        # the image data already, we need to be as smart as we can in this task",
            "        # to avoid downloading the data several times and reducing the copy or",
            "        # write times. There are several scenarios where the interaction",
            "        # between this task and `_ImportToFS` could be improved. All these",
            "        # scenarios assume the `_ImportToFS` task has been executed before",
            "        # and/or in a more abstract scenario, that `file_path` is being",
            "        # provided.",
            "        #",
            "        # Scenario 1: FS Store is Remote, introspection enabled,",
            "        # conversion disabled",
            "        #",
            "        # In this scenario, the user would benefit from having the scratch path",
            "        # being the same path as the fs store. Only one write would happen and",
            "        # an extra read will happen in order to introspect the image. Note that",
            "        # this read is just for the image headers and not the entire file.",
            "        #",
            "        # Scenario 2: FS Store is remote, introspection enabled,",
            "        # conversion enabled",
            "        #",
            "        # In this scenario, the user would benefit from having a *local* store",
            "        # into which the image can be converted. This will require downloading",
            "        # the image locally, converting it and then copying the converted image",
            "        # to the remote store.",
            "        #",
            "        # Scenario 3: FS Store is local, introspection enabled,",
            "        # conversion disabled",
            "        # Scenario 4: FS Store is local, introspection enabled,",
            "        # conversion enabled",
            "        #",
            "        # In both these scenarios the user shouldn't care if the FS",
            "        # store path and the work dir are the same, therefore probably",
            "        # benefit, about the scratch path and the FS store being the",
            "        # same from a performance perspective. Space wise, regardless",
            "        # of the scenario, the user will have to account for it in",
            "        # advance.",
            "        #",
            "        # Lets get to it and identify the different scenarios in the",
            "        # implementation",
            "        image = self.image_repo.get(image_id)",
            "        image.status = 'saving'",
            "        self.image_repo.save(image)",
            "",
            "        # NOTE(flaper87): Let's dance... and fall",
            "        #",
            "        # Unfortunately, because of the way our domain layers work and",
            "        # the checks done in the FS store, we can't simply rename the file",
            "        # and set the location. To do that, we'd have to duplicate the logic",
            "        # of every and each of the domain factories (quota, location, etc)",
            "        # and we'd also need to hack the FS store to prevent it from raising",
            "        # a \"duplication path\" error. I'd rather have this task copying the",
            "        # image bits one more time than duplicating all that logic.",
            "        #",
            "        # Since I don't think this should be the definitive solution, I'm",
            "        # leaving the code below as a reference for what should happen here",
            "        # once the FS store and domain code will be able to handle this case.",
            "        #",
            "        # if file_path is None:",
            "        #    image_import.set_image_data(image, self.uri, None)",
            "        #    return",
            "",
            "        # NOTE(flaper87): Don't assume the image was stored in the",
            "        # work_dir. Think in the case this path was provided by another task.",
            "        # Also, lets try to neither assume things nor create \"logic\"",
            "        # dependencies between this task and `_ImportToFS`",
            "        #",
            "        # base_path = os.path.dirname(file_path.split(\"file://\")[-1])",
            "",
            "        # NOTE(flaper87): Hopefully just scenarios #3 and #4. I say",
            "        # hopefully because nothing prevents the user to use the same",
            "        # FS store path as a work dir",
            "        #",
            "        # image_path = os.path.join(base_path, image_id)",
            "        #",
            "        # if (base_path == CONF.glance_store.filesystem_store_datadir or",
            "        #      base_path in CONF.glance_store.filesystem_store_datadirs):",
            "        #     os.rename(file_path, image_path)",
            "        #",
            "        # image_import.set_image_data(image, image_path, None)",
            "        try:",
            "            image_import.set_image_data(image,",
            "                                        file_path or self.uri, self.task_id,",
            "                                        backend=self.backend)",
            "        except IOError as e:",
            "            msg = (_('Uploading the image failed due to: %(exc)s') %",
            "                   {'exc': encodeutils.exception_to_unicode(e)})",
            "            LOG.error(msg)",
            "            raise exception.UploadException(message=msg)",
            "        # NOTE(flaper87): We need to save the image again after the locations",
            "        # have been set in the image.",
            "        self.image_repo.save(image)",
            "",
            "",
            "class _SaveImage(task.Task):",
            "",
            "    def __init__(self, task_id, task_type, image_repo):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.image_repo = image_repo",
            "        super(_SaveImage, self).__init__(",
            "            name='%s-SaveImage-%s' % (task_type, task_id))",
            "",
            "    def execute(self, image_id):",
            "        \"\"\"Transition image status to active",
            "",
            "        :param image_id: Glance Image ID",
            "        \"\"\"",
            "        new_image = self.image_repo.get(image_id)",
            "        if new_image.status == 'saving':",
            "            # NOTE(flaper87): THIS IS WRONG!",
            "            # we should be doing atomic updates to avoid",
            "            # race conditions. This happens in other places",
            "            # too.",
            "            new_image.status = 'active'",
            "        self.image_repo.save(new_image)",
            "",
            "",
            "class _CompleteTask(task.Task):",
            "",
            "    def __init__(self, task_id, task_type, task_repo):",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.task_repo = task_repo",
            "        super(_CompleteTask, self).__init__(",
            "            name='%s-CompleteTask-%s' % (task_type, task_id))",
            "",
            "    def execute(self, image_id):",
            "        \"\"\"Finishing the task flow",
            "",
            "        :param image_id: Glance Image ID",
            "        \"\"\"",
            "        task = script_utils.get_task(self.task_repo, self.task_id)",
            "        if task is None:",
            "            return",
            "        try:",
            "            task.succeed({'image_id': image_id})",
            "        except Exception as e:",
            "            # Note: The message string contains Error in it to indicate",
            "            # in the task.message that it's a error message for the user.",
            "",
            "            # TODO(nikhil): need to bring back save_and_reraise_exception when",
            "            # necessary",
            "            log_msg = _LE(\"Task ID %(task_id)s failed. Error: %(exc_type)s: \"",
            "                          \"%(e)s\")",
            "            LOG.exception(log_msg, {'exc_type': str(type(e)),",
            "                                    'e': encodeutils.exception_to_unicode(e),",
            "                                    'task_id': task.task_id})",
            "",
            "            err_msg = _(\"Error: %(exc_type)s: %(e)s\")",
            "            task.fail(err_msg % {'exc_type': str(type(e)),",
            "                                 'e': encodeutils.exception_to_unicode(e)})",
            "        finally:",
            "            self.task_repo.save(task)",
            "",
            "        LOG.info(_LI(\"%(task_id)s of %(task_type)s completed\"),",
            "                 {'task_id': self.task_id, 'task_type': self.task_type})",
            "",
            "",
            "def _get_import_flows(**kwargs):",
            "    # NOTE(flaper87): Until we have a better infrastructure to enable",
            "    # and disable tasks plugins, hard-code the tasks we know exist,",
            "    # instead of loading everything from the namespace. This guarantees",
            "    # both, the load order of these plugins and the fact that no random",
            "    # plugins will be added/loaded until we feel comfortable with this.",
            "    # Future patches will keep using NamedExtensionManager but they'll",
            "    # rely on a config option to control this process.",
            "    extensions = named.NamedExtensionManager('glance.flows.import',",
            "                                             names=['ovf_process',",
            "                                                    'convert',",
            "                                                    'introspect'],",
            "                                             name_order=True,",
            "                                             invoke_on_load=True,",
            "                                             invoke_kwds=kwargs)",
            "",
            "    for ext in extensions.extensions:",
            "        yield ext.obj",
            "",
            "",
            "def get_flow(**kwargs):",
            "    \"\"\"Return task flow",
            "",
            "    :param task_id: Task ID",
            "    :param task_type: Type of the task",
            "    :param task_repo: Task repo",
            "    :param image_repo: Image repository used",
            "    :param image_factory: Glance Image Factory",
            "    :param uri: uri for the image file",
            "    \"\"\"",
            "    task_id = kwargs.get('task_id')",
            "    task_type = kwargs.get('task_type')",
            "    task_repo = kwargs.get('task_repo')",
            "    image_repo = kwargs.get('image_repo')",
            "    image_factory = kwargs.get('image_factory')",
            "    uri = kwargs.get('uri')",
            "    backend = kwargs.get('backend')",
            "",
            "    flow = lf.Flow(task_type, retry=retry.AlwaysRevert()).add(",
            "        _CreateImage(task_id, task_type, task_repo, image_repo, image_factory))",
            "",
            "    import_to_store = _ImportToStore(task_id, task_type, image_repo, uri,",
            "                                     backend)",
            "",
            "    try:",
            "        # NOTE(flaper87): ImportToLocal and DeleteFromLocal shouldn't be here.",
            "        # Ideally, we should have the different import flows doing this for us",
            "        # and this function should clean up duplicated tasks. For example, say",
            "        # 2 flows need to have a local copy of the image - ImportToLocal - in",
            "        # order to be able to complete the task - i.e Introspect-. In that",
            "        # case, the introspect.get_flow call should add both, ImportToLocal and",
            "        # DeleteFromLocal, to the flow and this function will reduce the",
            "        # duplicated calls to those tasks by creating a linear flow that",
            "        # ensures those are called before the other tasks.  For now, I'm",
            "        # keeping them here, though.",
            "        limbo = lf.Flow(task_type).add(_ImportToFS(task_id,",
            "                                                   task_type,",
            "                                                   task_repo,",
            "                                                   uri))",
            "",
            "        for subflow in _get_import_flows(**kwargs):",
            "            limbo.add(subflow)",
            "",
            "        # NOTE(flaper87): We have hard-coded 2 tasks,",
            "        # if there aren't more than 2, it means that",
            "        # no subtask has been registered.",
            "        if len(limbo) > 1:",
            "            flow.add(limbo)",
            "",
            "            # NOTE(flaper87): Until this implementation gets smarter,",
            "            # make sure ImportToStore is called *after* the imported",
            "            # flow stages. If not, the image will be set to saving state",
            "            # invalidating tasks like Introspection or Convert.",
            "            flow.add(import_to_store)",
            "",
            "            # NOTE(flaper87): Since this is an \"optional\" task but required",
            "            # when `limbo` is executed, we're adding it in its own subflow",
            "            # to isolate it from the rest of the flow.",
            "            delete_flow = lf.Flow(task_type).add(_DeleteFromFS(task_id,",
            "                                                               task_type))",
            "            flow.add(delete_flow)",
            "        else:",
            "            flow.add(import_to_store)",
            "    except exception.BadTaskConfiguration as exc:",
            "        # NOTE(flaper87): If something goes wrong with the load of",
            "        # import tasks, make sure we go on.",
            "        LOG.error(_LE('Bad task configuration: %s'), exc.message)",
            "        flow.add(import_to_store)",
            "",
            "    flow.add(",
            "        _SaveImage(task_id, task_type, image_repo),",
            "        _CompleteTask(task_id, task_type, task_repo)",
            "    )",
            "    return flow"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "glance/async_/flows/plugins/image_conversion.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "             raise RuntimeError("
            },
            "1": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 122,
                "PatchRowcode": "                 'QCOW images with backing files are not allowed')"
            },
            "2": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 123,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+        try:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+            data_file = metadata['format-specific']['data']['data-file']"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+        except KeyError:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+            data_file = None"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+        if data_file is not None:"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+            raise RuntimeError("
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+                'QCOW images with data-file set are not allowed')"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": 132,
                "PatchRowcode": "         if metadata.get('format') == 'vmdk':"
            },
            "12": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "             create_type = metadata.get("
            },
            "13": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "                 'format-specific', {}).get("
            }
        },
        "frontPatchFile": [
            "# Copyright 2018 Red Hat, Inc.",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import json",
            "import os",
            "",
            "from oslo_concurrency import processutils as putils",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_utils import encodeutils",
            "from oslo_utils import excutils",
            "from taskflow.patterns import linear_flow as lf",
            "from taskflow import task",
            "",
            "from glance.async_ import utils",
            "from glance.i18n import _, _LI",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "conversion_plugin_opts = [",
            "    cfg.StrOpt('output_format',",
            "               default='raw',",
            "               choices=('qcow2', 'raw', 'vmdk'),",
            "               help=_(\"\"\"",
            "Desired output format for image conversion plugin.",
            "",
            "Provide a valid image format to which the conversion plugin",
            "will convert the image before storing it to the back-end.",
            "",
            "Note, if the Image Conversion plugin for image import is defined, users",
            "should only upload disk formats that are supported by `quemu-img` otherwise",
            "the conversion and import will fail.",
            "",
            "Possible values:",
            "    * qcow2",
            "    * raw",
            "    * vmdk",
            "",
            "Related Options:",
            "    * disk_formats",
            "\"\"\")),",
            "]",
            "",
            "CONF = cfg.CONF",
            "",
            "CONF.register_opts(conversion_plugin_opts, group='image_conversion')",
            "",
            "",
            "class _ConvertImage(task.Task):",
            "",
            "    default_provides = 'file_path'",
            "",
            "    def __init__(self, context, task_id, task_type, action_wrapper):",
            "        self.context = context",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.action_wrapper = action_wrapper",
            "        self.image_id = action_wrapper.image_id",
            "        self.dest_path = \"\"",
            "        self.python = CONF.wsgi.python_interpreter",
            "        super(_ConvertImage, self).__init__(",
            "            name='%s-Convert_Image-%s' % (task_type, task_id))",
            "",
            "    def execute(self, file_path, **kwargs):",
            "        with self.action_wrapper as action:",
            "            return self._execute(action, file_path, **kwargs)",
            "",
            "    def _execute(self, action, file_path, **kwargs):",
            "",
            "        target_format = CONF.image_conversion.output_format",
            "        # TODO(jokke): Once we support other schemas we need to take them into",
            "        # account and handle the paths here.",
            "        src_path = file_path.split('file://')[-1]",
            "        dest_path = \"%(path)s.%(target)s\" % {'path': src_path,",
            "                                             'target': target_format}",
            "        self.dest_path = dest_path",
            "",
            "        try:",
            "            stdout, stderr = putils.trycmd(\"qemu-img\", \"info\",",
            "                                           \"--output=json\",",
            "                                           src_path,",
            "                                           prlimit=utils.QEMU_IMG_PROC_LIMITS,",
            "                                           python_exec=self.python,",
            "                                           log_errors=putils.LOG_ALL_ERRORS,)",
            "        except OSError as exc:",
            "            with excutils.save_and_reraise_exception():",
            "                exc_message = encodeutils.exception_to_unicode(exc)",
            "                msg = (\"Failed to do introspection as part of image \"",
            "                       \"conversion for %(iid)s: %(err)s\")",
            "                LOG.error(msg, {'iid': self.image_id, 'err': exc_message})",
            "",
            "        if stderr:",
            "            raise RuntimeError(stderr)",
            "",
            "        metadata = json.loads(stdout)",
            "        try:",
            "            source_format = metadata['format']",
            "        except KeyError:",
            "            msg = (\"Failed to do introspection as part of image \"",
            "                   \"conversion for %(iid)s: Source format not reported\")",
            "            LOG.error(msg, {'iid': self.image_id})",
            "            raise RuntimeError(msg)",
            "",
            "        virtual_size = metadata.get('virtual-size', 0)",
            "        action.set_image_attribute(virtual_size=virtual_size)",
            "",
            "        if 'backing-filename' in metadata:",
            "            LOG.warning('Refusing to process QCOW image with a backing file')",
            "            raise RuntimeError(",
            "                'QCOW images with backing files are not allowed')",
            "",
            "        if metadata.get('format') == 'vmdk':",
            "            create_type = metadata.get(",
            "                'format-specific', {}).get(",
            "                    'data', {}).get('create-type')",
            "            allowed = CONF.image_format.vmdk_allowed_types",
            "            if not create_type:",
            "                raise RuntimeError(_('Unable to determine VMDK create-type'))",
            "            if not len(allowed):",
            "                LOG.warning(_('Refusing to process VMDK file as '",
            "                              'vmdk_allowed_types is empty'))",
            "                raise RuntimeError(_('Image is a VMDK, but no VMDK createType '",
            "                                     'is specified'))",
            "            if create_type not in allowed:",
            "                LOG.warning(_('Refusing to process VMDK file with create-type '",
            "                              'of %r which is not in allowed set of: %s'),",
            "                            create_type, ','.join(allowed))",
            "                raise RuntimeError(_('Invalid VMDK create-type specified'))",
            "",
            "        if source_format == target_format:",
            "            LOG.debug(\"Source is already in target format, \"",
            "                      \"not doing conversion for %s\", self.image_id)",
            "            return file_path",
            "",
            "        try:",
            "            stdout, stderr = putils.trycmd('qemu-img', 'convert',",
            "                                           '-f', source_format,",
            "                                           '-O', target_format,",
            "                                           src_path, dest_path,",
            "                                           log_errors=putils.LOG_ALL_ERRORS)",
            "        except OSError as exc:",
            "            with excutils.save_and_reraise_exception():",
            "                exc_message = encodeutils.exception_to_unicode(exc)",
            "                msg = \"Failed to do image conversion for %(iid)s: %(err)s\"",
            "                LOG.error(msg, {'iid': self.image_id, 'err': exc_message})",
            "",
            "        if stderr:",
            "            raise RuntimeError(stderr)",
            "",
            "        action.set_image_attribute(disk_format=target_format,",
            "                                   container_format='bare')",
            "        new_size = os.stat(dest_path).st_size",
            "        action.set_image_attribute(size=new_size)",
            "        LOG.info(_LI('Updated image %s size=%i disk_format=%s'),",
            "                 self.image_id, new_size, target_format)",
            "",
            "        os.remove(src_path)",
            "",
            "        return \"file://%s\" % dest_path",
            "",
            "    def revert(self, result=None, **kwargs):",
            "        # NOTE(flaper87): If result is None, it probably",
            "        # means this task failed. Otherwise, we would have",
            "        # a result from its execution.",
            "        if result is not None:",
            "            LOG.debug(\"Image conversion failed.\")",
            "            if os.path.exists(self.dest_path):",
            "                os.remove(self.dest_path)",
            "",
            "",
            "def get_flow(**kwargs):",
            "    \"\"\"Return task flow for no-op.",
            "",
            "    :param context: request context",
            "    :param task_id: Task ID.",
            "    :param task_type: Type of the task.",
            "    :param image_repo: Image repository used.",
            "    :param image_id: Image ID",
            "    :param action_wrapper: An api_image_import.ActionWrapper.",
            "    \"\"\"",
            "    context = kwargs.get('context')",
            "    task_id = kwargs.get('task_id')",
            "    task_type = kwargs.get('task_type')",
            "    action_wrapper = kwargs.get('action_wrapper')",
            "",
            "    return lf.Flow(task_type).add(",
            "        _ConvertImage(context, task_id, task_type, action_wrapper)",
            "    )"
        ],
        "afterPatchFile": [
            "# Copyright 2018 Red Hat, Inc.",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import json",
            "import os",
            "",
            "from oslo_concurrency import processutils as putils",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_utils import encodeutils",
            "from oslo_utils import excutils",
            "from taskflow.patterns import linear_flow as lf",
            "from taskflow import task",
            "",
            "from glance.async_ import utils",
            "from glance.i18n import _, _LI",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "conversion_plugin_opts = [",
            "    cfg.StrOpt('output_format',",
            "               default='raw',",
            "               choices=('qcow2', 'raw', 'vmdk'),",
            "               help=_(\"\"\"",
            "Desired output format for image conversion plugin.",
            "",
            "Provide a valid image format to which the conversion plugin",
            "will convert the image before storing it to the back-end.",
            "",
            "Note, if the Image Conversion plugin for image import is defined, users",
            "should only upload disk formats that are supported by `quemu-img` otherwise",
            "the conversion and import will fail.",
            "",
            "Possible values:",
            "    * qcow2",
            "    * raw",
            "    * vmdk",
            "",
            "Related Options:",
            "    * disk_formats",
            "\"\"\")),",
            "]",
            "",
            "CONF = cfg.CONF",
            "",
            "CONF.register_opts(conversion_plugin_opts, group='image_conversion')",
            "",
            "",
            "class _ConvertImage(task.Task):",
            "",
            "    default_provides = 'file_path'",
            "",
            "    def __init__(self, context, task_id, task_type, action_wrapper):",
            "        self.context = context",
            "        self.task_id = task_id",
            "        self.task_type = task_type",
            "        self.action_wrapper = action_wrapper",
            "        self.image_id = action_wrapper.image_id",
            "        self.dest_path = \"\"",
            "        self.python = CONF.wsgi.python_interpreter",
            "        super(_ConvertImage, self).__init__(",
            "            name='%s-Convert_Image-%s' % (task_type, task_id))",
            "",
            "    def execute(self, file_path, **kwargs):",
            "        with self.action_wrapper as action:",
            "            return self._execute(action, file_path, **kwargs)",
            "",
            "    def _execute(self, action, file_path, **kwargs):",
            "",
            "        target_format = CONF.image_conversion.output_format",
            "        # TODO(jokke): Once we support other schemas we need to take them into",
            "        # account and handle the paths here.",
            "        src_path = file_path.split('file://')[-1]",
            "        dest_path = \"%(path)s.%(target)s\" % {'path': src_path,",
            "                                             'target': target_format}",
            "        self.dest_path = dest_path",
            "",
            "        try:",
            "            stdout, stderr = putils.trycmd(\"qemu-img\", \"info\",",
            "                                           \"--output=json\",",
            "                                           src_path,",
            "                                           prlimit=utils.QEMU_IMG_PROC_LIMITS,",
            "                                           python_exec=self.python,",
            "                                           log_errors=putils.LOG_ALL_ERRORS,)",
            "        except OSError as exc:",
            "            with excutils.save_and_reraise_exception():",
            "                exc_message = encodeutils.exception_to_unicode(exc)",
            "                msg = (\"Failed to do introspection as part of image \"",
            "                       \"conversion for %(iid)s: %(err)s\")",
            "                LOG.error(msg, {'iid': self.image_id, 'err': exc_message})",
            "",
            "        if stderr:",
            "            raise RuntimeError(stderr)",
            "",
            "        metadata = json.loads(stdout)",
            "        try:",
            "            source_format = metadata['format']",
            "        except KeyError:",
            "            msg = (\"Failed to do introspection as part of image \"",
            "                   \"conversion for %(iid)s: Source format not reported\")",
            "            LOG.error(msg, {'iid': self.image_id})",
            "            raise RuntimeError(msg)",
            "",
            "        virtual_size = metadata.get('virtual-size', 0)",
            "        action.set_image_attribute(virtual_size=virtual_size)",
            "",
            "        if 'backing-filename' in metadata:",
            "            LOG.warning('Refusing to process QCOW image with a backing file')",
            "            raise RuntimeError(",
            "                'QCOW images with backing files are not allowed')",
            "",
            "        try:",
            "            data_file = metadata['format-specific']['data']['data-file']",
            "        except KeyError:",
            "            data_file = None",
            "        if data_file is not None:",
            "            raise RuntimeError(",
            "                'QCOW images with data-file set are not allowed')",
            "",
            "        if metadata.get('format') == 'vmdk':",
            "            create_type = metadata.get(",
            "                'format-specific', {}).get(",
            "                    'data', {}).get('create-type')",
            "            allowed = CONF.image_format.vmdk_allowed_types",
            "            if not create_type:",
            "                raise RuntimeError(_('Unable to determine VMDK create-type'))",
            "            if not len(allowed):",
            "                LOG.warning(_('Refusing to process VMDK file as '",
            "                              'vmdk_allowed_types is empty'))",
            "                raise RuntimeError(_('Image is a VMDK, but no VMDK createType '",
            "                                     'is specified'))",
            "            if create_type not in allowed:",
            "                LOG.warning(_('Refusing to process VMDK file with create-type '",
            "                              'of %r which is not in allowed set of: %s'),",
            "                            create_type, ','.join(allowed))",
            "                raise RuntimeError(_('Invalid VMDK create-type specified'))",
            "",
            "        if source_format == target_format:",
            "            LOG.debug(\"Source is already in target format, \"",
            "                      \"not doing conversion for %s\", self.image_id)",
            "            return file_path",
            "",
            "        try:",
            "            stdout, stderr = putils.trycmd('qemu-img', 'convert',",
            "                                           '-f', source_format,",
            "                                           '-O', target_format,",
            "                                           src_path, dest_path,",
            "                                           log_errors=putils.LOG_ALL_ERRORS)",
            "        except OSError as exc:",
            "            with excutils.save_and_reraise_exception():",
            "                exc_message = encodeutils.exception_to_unicode(exc)",
            "                msg = \"Failed to do image conversion for %(iid)s: %(err)s\"",
            "                LOG.error(msg, {'iid': self.image_id, 'err': exc_message})",
            "",
            "        if stderr:",
            "            raise RuntimeError(stderr)",
            "",
            "        action.set_image_attribute(disk_format=target_format,",
            "                                   container_format='bare')",
            "        new_size = os.stat(dest_path).st_size",
            "        action.set_image_attribute(size=new_size)",
            "        LOG.info(_LI('Updated image %s size=%i disk_format=%s'),",
            "                 self.image_id, new_size, target_format)",
            "",
            "        os.remove(src_path)",
            "",
            "        return \"file://%s\" % dest_path",
            "",
            "    def revert(self, result=None, **kwargs):",
            "        # NOTE(flaper87): If result is None, it probably",
            "        # means this task failed. Otherwise, we would have",
            "        # a result from its execution.",
            "        if result is not None:",
            "            LOG.debug(\"Image conversion failed.\")",
            "            if os.path.exists(self.dest_path):",
            "                os.remove(self.dest_path)",
            "",
            "",
            "def get_flow(**kwargs):",
            "    \"\"\"Return task flow for no-op.",
            "",
            "    :param context: request context",
            "    :param task_id: Task ID.",
            "    :param task_type: Type of the task.",
            "    :param image_repo: Image repository used.",
            "    :param image_id: Image ID",
            "    :param action_wrapper: An api_image_import.ActionWrapper.",
            "    \"\"\"",
            "    context = kwargs.get('context')",
            "    task_id = kwargs.get('task_id')",
            "    task_type = kwargs.get('task_type')",
            "    action_wrapper = kwargs.get('action_wrapper')",
            "",
            "    return lf.Flow(task_type).add(",
            "        _ConvertImage(context, task_id, task_type, action_wrapper)",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "glance.async_.flows.plugins.image_conversion._ConvertImage.execute"
        ]
    },
    "glance/tests/unit/async_/flows/plugins/test_image_conversion.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 184,
                "PatchRowcode": "             self.assertEqual('QCOW images with backing files are not allowed',"
            },
            "1": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "                              str(e))"
            },
            "2": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 186,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+    def test_image_convert_invalid_qcow_data_file(self):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+        data = {'format': 'qcow2',"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+                'format-specific': {"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+                    'data': {"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+                        'data-file': '/etc/hosts',"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+                    },"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 193,
                "PatchRowcode": "+                }}"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+        convert = self._setup_image_convert_info_fail()"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+        with mock.patch.object(processutils, 'execute') as exc_mock:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+            exc_mock.return_value = json.dumps(data), ''"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+            e = self.assertRaises(RuntimeError,"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+                                  convert.execute, 'file:///test/path.qcow')"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+            self.assertEqual('QCOW images with data-file set are not allowed',"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+                             str(e))"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 203,
                "PatchRowcode": "     def _test_image_convert_invalid_vmdk(self):"
            },
            "20": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 204,
                "PatchRowcode": "         data = {'format': 'vmdk',"
            },
            "21": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "                 'format-specific': {"
            }
        },
        "frontPatchFile": [
            "# Copyright 2018 RedHat, Inc.",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import json",
            "import os",
            "from unittest import mock",
            "",
            "import glance_store",
            "from oslo_concurrency import processutils",
            "from oslo_config import cfg",
            "",
            "import glance.async_.flows.api_image_import as import_flow",
            "import glance.async_.flows.plugins.image_conversion as image_conversion",
            "from glance.async_ import utils as async_utils",
            "from glance.common import utils",
            "from glance import domain",
            "from glance import gateway",
            "import glance.tests.utils as test_utils",
            "",
            "CONF = cfg.CONF",
            "",
            "",
            "UUID1 = 'c80a1a6c-bd1f-41c5-90ee-81afedb1d58d'",
            "TENANT1 = '6838eb7b-6ded-434a-882c-b344c77fe8df'",
            "",
            "",
            "class TestConvertImageTask(test_utils.BaseTestCase):",
            "",
            "    def setUp(self):",
            "        super(TestConvertImageTask, self).setUp()",
            "",
            "        glance_store.register_opts(CONF)",
            "        self.config(default_store='file',",
            "                    stores=['file', 'http'],",
            "                    filesystem_store_datadir=self.test_dir,",
            "                    group=\"glance_store\")",
            "        self.config(output_format='qcow2',",
            "                    group='image_conversion')",
            "        glance_store.create_stores(CONF)",
            "",
            "        self.work_dir = os.path.join(self.test_dir, 'work_dir')",
            "        utils.safe_mkdirs(self.work_dir)",
            "        self.config(work_dir=self.work_dir, group='task')",
            "",
            "        self.context = mock.MagicMock()",
            "        self.img_repo = mock.MagicMock()",
            "        self.task_repo = mock.MagicMock()",
            "        self.image_id = UUID1",
            "",
            "        self.gateway = gateway.Gateway()",
            "        self.task_factory = domain.TaskFactory()",
            "        self.img_factory = self.gateway.get_image_factory(self.context)",
            "        self.image = self.img_factory.new_image(image_id=self.image_id,",
            "                                                disk_format='raw',",
            "                                                container_format='bare')",
            "",
            "        task_input = {",
            "            \"import_from\": \"http://cloud.foo/image.raw\",",
            "            \"import_from_format\": \"raw\",",
            "            \"image_properties\": {'disk_format': 'raw',",
            "                                 'container_format': 'bare'}",
            "        }",
            "",
            "        task_ttl = CONF.task.task_time_to_live",
            "",
            "        self.task_type = 'import'",
            "        request_id = 'fake_request_id'",
            "        user_id = 'fake_user'",
            "        self.task = self.task_factory.new_task(self.task_type, TENANT1,",
            "                                               self.image_id, user_id,",
            "                                               request_id,",
            "                                               task_time_to_live=task_ttl,",
            "                                               task_input=task_input)",
            "",
            "        self.image.extra_properties = {",
            "            'os_glance_import_task': self.task.task_id}",
            "        self.wrapper = import_flow.ImportActionWrapper(self.img_repo,",
            "                                                       self.image_id,",
            "                                                       self.task.task_id)",
            "",
            "    @mock.patch.object(os, 'stat')",
            "    @mock.patch.object(os, 'remove')",
            "    def test_image_convert_success(self, mock_os_remove, mock_os_stat):",
            "        mock_os_remove.return_value = None",
            "        mock_os_stat.return_value.st_size = 123",
            "        image_convert = image_conversion._ConvertImage(self.context,",
            "                                                       self.task.task_id,",
            "                                                       self.task_type,",
            "                                                       self.wrapper)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "        image = mock.MagicMock(image_id=self.image_id, virtual_size=None,",
            "                               extra_properties={",
            "                                   'os_glance_import_task': self.task.task_id},",
            "                               disk_format='qcow2')",
            "        self.img_repo.get.return_value = image",
            "",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = (\"\", None)",
            "            with mock.patch.object(json, 'loads') as jloads_mock:",
            "                jloads_mock.return_value = {'format': 'raw',",
            "                                            'virtual-size': 456}",
            "                image_convert.execute('file:///test/path.raw')",
            "",
            "                # NOTE(hemanthm): Asserting that the source format is passed",
            "                # to qemu-utis to avoid inferring the image format. This",
            "                # shields us from an attack vector described at",
            "                # https://bugs.launchpad.net/glance/+bug/1449062/comments/72",
            "                self.assertIn('-f', exc_mock.call_args[0])",
            "                self.assertEqual(\"qcow2\", image.disk_format)",
            "",
            "        self.assertEqual('bare', image.container_format)",
            "        self.assertEqual('qcow2', image.disk_format)",
            "        self.assertEqual(456, image.virtual_size)",
            "        self.assertEqual(123, image.size)",
            "",
            "    def _setup_image_convert_info_fail(self):",
            "        image_convert = image_conversion._ConvertImage(self.context,",
            "                                                       self.task.task_id,",
            "                                                       self.task_type,",
            "                                                       self.wrapper)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "        image = mock.MagicMock(image_id=self.image_id, virtual_size=None,",
            "                               extra_properties={",
            "                                   'os_glance_import_task': self.task.task_id},",
            "                               disk_format='qcow2')",
            "        self.img_repo.get.return_value = image",
            "        return image_convert",
            "",
            "    def test_image_convert_fails_inspection(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.side_effect = OSError('fail')",
            "            self.assertRaises(OSError,",
            "                              convert.execute, 'file:///test/path.raw')",
            "            exc_mock.assert_called_once_with(",
            "                'qemu-img', 'info',",
            "                '--output=json',",
            "                '/test/path.raw',",
            "                prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                python_exec=convert.python,",
            "                log_errors=processutils.LOG_ALL_ERRORS)",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_inspection_reports_error(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = '', 'some error'",
            "            self.assertRaises(RuntimeError,",
            "                              convert.execute, 'file:///test/path.raw')",
            "            exc_mock.assert_called_once_with(",
            "                'qemu-img', 'info',",
            "                '--output=json',",
            "                '/test/path.raw',",
            "                prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                python_exec=convert.python,",
            "                log_errors=processutils.LOG_ALL_ERRORS)",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_invalid_qcow(self):",
            "        data = {'format': 'qcow2',",
            "                'backing-filename': '/etc/hosts'}",
            "",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = json.dumps(data), ''",
            "            e = self.assertRaises(RuntimeError,",
            "                                  convert.execute, 'file:///test/path.qcow')",
            "            self.assertEqual('QCOW images with backing files are not allowed',",
            "                             str(e))",
            "",
            "    def _test_image_convert_invalid_vmdk(self):",
            "        data = {'format': 'vmdk',",
            "                'format-specific': {",
            "                    'data': {",
            "                        'create-type': 'monolithicFlat',",
            "                    }}}",
            "",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = json.dumps(data), ''",
            "            convert.execute('file:///test/path.vmdk')",
            "",
            "    def test_image_convert_invalid_vmdk(self):",
            "        e = self.assertRaises(RuntimeError,",
            "                              self._test_image_convert_invalid_vmdk)",
            "        self.assertEqual('Invalid VMDK create-type specified', str(e))",
            "",
            "    def test_image_convert_valid_vmdk_no_types(self):",
            "        with mock.patch.object(CONF.image_format, 'vmdk_allowed_types',",
            "                               new=[]):",
            "            # We make it past the VMDK check and fail because our file",
            "            # does not exist",
            "            e = self.assertRaises(RuntimeError,",
            "                                  self._test_image_convert_invalid_vmdk)",
            "            self.assertEqual('Image is a VMDK, but no VMDK createType is '",
            "                             'specified', str(e))",
            "",
            "    def test_image_convert_valid_vmdk(self):",
            "        with mock.patch.object(CONF.image_format, 'vmdk_allowed_types',",
            "                               new=['monolithicSparse', 'monolithicFlat']):",
            "            # We make it past the VMDK check and fail because our file",
            "            # does not exist",
            "            self.assertRaises(FileNotFoundError,",
            "                              self._test_image_convert_invalid_vmdk)",
            "",
            "    def test_image_convert_fails(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.side_effect = [('{\"format\":\"raw\"}', ''),",
            "                                    OSError('convert_fail')]",
            "            self.assertRaises(OSError,",
            "                              convert.execute, 'file:///test/path.raw')",
            "            exc_mock.assert_has_calls(",
            "                [mock.call('qemu-img', 'info',",
            "                           '--output=json',",
            "                           '/test/path.raw',",
            "                           prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                           python_exec=convert.python,",
            "                           log_errors=processutils.LOG_ALL_ERRORS),",
            "                 mock.call('qemu-img', 'convert', '-f', 'raw', '-O', 'qcow2',",
            "                           '/test/path.raw', '/test/path.raw.qcow2',",
            "                           log_errors=processutils.LOG_ALL_ERRORS)])",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_reports_fail(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.side_effect = [('{\"format\":\"raw\"}', ''),",
            "                                    ('', 'some error')]",
            "            self.assertRaises(RuntimeError,",
            "                              convert.execute, 'file:///test/path.raw')",
            "            exc_mock.assert_has_calls(",
            "                [mock.call('qemu-img', 'info',",
            "                           '--output=json',",
            "                           '/test/path.raw',",
            "                           prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                           python_exec=convert.python,",
            "                           log_errors=processutils.LOG_ALL_ERRORS),",
            "                 mock.call('qemu-img', 'convert', '-f', 'raw', '-O', 'qcow2',",
            "                           '/test/path.raw', '/test/path.raw.qcow2',",
            "                           log_errors=processutils.LOG_ALL_ERRORS)])",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_fails_source_format(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = ('{}', '')",
            "            exc = self.assertRaises(RuntimeError,",
            "                                    convert.execute, 'file:///test/path.raw')",
            "            self.assertIn('Source format not reported', str(exc))",
            "            exc_mock.assert_called_once_with(",
            "                'qemu-img', 'info',",
            "                '--output=json',",
            "                '/test/path.raw',",
            "                prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                python_exec=convert.python,",
            "                log_errors=processutils.LOG_ALL_ERRORS)",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_same_format_does_nothing(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = (",
            "                '{\"format\": \"qcow2\", \"virtual-size\": 123}', '')",
            "            convert.execute('file:///test/path.qcow')",
            "            # Make sure we only called qemu-img for inspection, not conversion",
            "            exc_mock.assert_called_once_with(",
            "                'qemu-img', 'info',",
            "                '--output=json',",
            "                '/test/path.qcow',",
            "                prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                python_exec=convert.python,",
            "                log_errors=processutils.LOG_ALL_ERRORS)",
            "",
            "        # Make sure we set the virtual_size before we exited",
            "        image = self.img_repo.get.return_value",
            "        self.assertEqual(123, image.virtual_size)",
            "",
            "    @mock.patch.object(os, 'remove')",
            "    def test_image_convert_revert_success(self, mock_os_remove):",
            "        mock_os_remove.return_value = None",
            "        image_convert = image_conversion._ConvertImage(self.context,",
            "                                                       self.task.task_id,",
            "                                                       self.task_type,",
            "                                                       self.wrapper)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = (\"\", None)",
            "            with mock.patch.object(os.path, 'exists') as os_exists_mock:",
            "                os_exists_mock.return_value = True",
            "                image_convert.revert(result=mock.MagicMock())",
            "                self.assertEqual(1, mock_os_remove.call_count)",
            "",
            "    def test_image_convert_interpreter_configured(self):",
            "        # By default, wsgi.python_interpreter is None; if it is",
            "        # overridden, we should take the interpreter from config.",
            "        fake_interpreter = '/usr/bin/python2.7'",
            "        self.config(python_interpreter=fake_interpreter,",
            "                    group='wsgi')",
            "        convert = image_conversion._ConvertImage(self.context,",
            "                                                 self.task.task_id,",
            "                                                 self.task_type,",
            "                                                 self.wrapper)",
            "        self.assertEqual(fake_interpreter, convert.python)"
        ],
        "afterPatchFile": [
            "# Copyright 2018 RedHat, Inc.",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import json",
            "import os",
            "from unittest import mock",
            "",
            "import glance_store",
            "from oslo_concurrency import processutils",
            "from oslo_config import cfg",
            "",
            "import glance.async_.flows.api_image_import as import_flow",
            "import glance.async_.flows.plugins.image_conversion as image_conversion",
            "from glance.async_ import utils as async_utils",
            "from glance.common import utils",
            "from glance import domain",
            "from glance import gateway",
            "import glance.tests.utils as test_utils",
            "",
            "CONF = cfg.CONF",
            "",
            "",
            "UUID1 = 'c80a1a6c-bd1f-41c5-90ee-81afedb1d58d'",
            "TENANT1 = '6838eb7b-6ded-434a-882c-b344c77fe8df'",
            "",
            "",
            "class TestConvertImageTask(test_utils.BaseTestCase):",
            "",
            "    def setUp(self):",
            "        super(TestConvertImageTask, self).setUp()",
            "",
            "        glance_store.register_opts(CONF)",
            "        self.config(default_store='file',",
            "                    stores=['file', 'http'],",
            "                    filesystem_store_datadir=self.test_dir,",
            "                    group=\"glance_store\")",
            "        self.config(output_format='qcow2',",
            "                    group='image_conversion')",
            "        glance_store.create_stores(CONF)",
            "",
            "        self.work_dir = os.path.join(self.test_dir, 'work_dir')",
            "        utils.safe_mkdirs(self.work_dir)",
            "        self.config(work_dir=self.work_dir, group='task')",
            "",
            "        self.context = mock.MagicMock()",
            "        self.img_repo = mock.MagicMock()",
            "        self.task_repo = mock.MagicMock()",
            "        self.image_id = UUID1",
            "",
            "        self.gateway = gateway.Gateway()",
            "        self.task_factory = domain.TaskFactory()",
            "        self.img_factory = self.gateway.get_image_factory(self.context)",
            "        self.image = self.img_factory.new_image(image_id=self.image_id,",
            "                                                disk_format='raw',",
            "                                                container_format='bare')",
            "",
            "        task_input = {",
            "            \"import_from\": \"http://cloud.foo/image.raw\",",
            "            \"import_from_format\": \"raw\",",
            "            \"image_properties\": {'disk_format': 'raw',",
            "                                 'container_format': 'bare'}",
            "        }",
            "",
            "        task_ttl = CONF.task.task_time_to_live",
            "",
            "        self.task_type = 'import'",
            "        request_id = 'fake_request_id'",
            "        user_id = 'fake_user'",
            "        self.task = self.task_factory.new_task(self.task_type, TENANT1,",
            "                                               self.image_id, user_id,",
            "                                               request_id,",
            "                                               task_time_to_live=task_ttl,",
            "                                               task_input=task_input)",
            "",
            "        self.image.extra_properties = {",
            "            'os_glance_import_task': self.task.task_id}",
            "        self.wrapper = import_flow.ImportActionWrapper(self.img_repo,",
            "                                                       self.image_id,",
            "                                                       self.task.task_id)",
            "",
            "    @mock.patch.object(os, 'stat')",
            "    @mock.patch.object(os, 'remove')",
            "    def test_image_convert_success(self, mock_os_remove, mock_os_stat):",
            "        mock_os_remove.return_value = None",
            "        mock_os_stat.return_value.st_size = 123",
            "        image_convert = image_conversion._ConvertImage(self.context,",
            "                                                       self.task.task_id,",
            "                                                       self.task_type,",
            "                                                       self.wrapper)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "        image = mock.MagicMock(image_id=self.image_id, virtual_size=None,",
            "                               extra_properties={",
            "                                   'os_glance_import_task': self.task.task_id},",
            "                               disk_format='qcow2')",
            "        self.img_repo.get.return_value = image",
            "",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = (\"\", None)",
            "            with mock.patch.object(json, 'loads') as jloads_mock:",
            "                jloads_mock.return_value = {'format': 'raw',",
            "                                            'virtual-size': 456}",
            "                image_convert.execute('file:///test/path.raw')",
            "",
            "                # NOTE(hemanthm): Asserting that the source format is passed",
            "                # to qemu-utis to avoid inferring the image format. This",
            "                # shields us from an attack vector described at",
            "                # https://bugs.launchpad.net/glance/+bug/1449062/comments/72",
            "                self.assertIn('-f', exc_mock.call_args[0])",
            "                self.assertEqual(\"qcow2\", image.disk_format)",
            "",
            "        self.assertEqual('bare', image.container_format)",
            "        self.assertEqual('qcow2', image.disk_format)",
            "        self.assertEqual(456, image.virtual_size)",
            "        self.assertEqual(123, image.size)",
            "",
            "    def _setup_image_convert_info_fail(self):",
            "        image_convert = image_conversion._ConvertImage(self.context,",
            "                                                       self.task.task_id,",
            "                                                       self.task_type,",
            "                                                       self.wrapper)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "        image = mock.MagicMock(image_id=self.image_id, virtual_size=None,",
            "                               extra_properties={",
            "                                   'os_glance_import_task': self.task.task_id},",
            "                               disk_format='qcow2')",
            "        self.img_repo.get.return_value = image",
            "        return image_convert",
            "",
            "    def test_image_convert_fails_inspection(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.side_effect = OSError('fail')",
            "            self.assertRaises(OSError,",
            "                              convert.execute, 'file:///test/path.raw')",
            "            exc_mock.assert_called_once_with(",
            "                'qemu-img', 'info',",
            "                '--output=json',",
            "                '/test/path.raw',",
            "                prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                python_exec=convert.python,",
            "                log_errors=processutils.LOG_ALL_ERRORS)",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_inspection_reports_error(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = '', 'some error'",
            "            self.assertRaises(RuntimeError,",
            "                              convert.execute, 'file:///test/path.raw')",
            "            exc_mock.assert_called_once_with(",
            "                'qemu-img', 'info',",
            "                '--output=json',",
            "                '/test/path.raw',",
            "                prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                python_exec=convert.python,",
            "                log_errors=processutils.LOG_ALL_ERRORS)",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_invalid_qcow(self):",
            "        data = {'format': 'qcow2',",
            "                'backing-filename': '/etc/hosts'}",
            "",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = json.dumps(data), ''",
            "            e = self.assertRaises(RuntimeError,",
            "                                  convert.execute, 'file:///test/path.qcow')",
            "            self.assertEqual('QCOW images with backing files are not allowed',",
            "                             str(e))",
            "",
            "    def test_image_convert_invalid_qcow_data_file(self):",
            "        data = {'format': 'qcow2',",
            "                'format-specific': {",
            "                    'data': {",
            "                        'data-file': '/etc/hosts',",
            "                    },",
            "                }}",
            "",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = json.dumps(data), ''",
            "            e = self.assertRaises(RuntimeError,",
            "                                  convert.execute, 'file:///test/path.qcow')",
            "            self.assertEqual('QCOW images with data-file set are not allowed',",
            "                             str(e))",
            "",
            "    def _test_image_convert_invalid_vmdk(self):",
            "        data = {'format': 'vmdk',",
            "                'format-specific': {",
            "                    'data': {",
            "                        'create-type': 'monolithicFlat',",
            "                    }}}",
            "",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = json.dumps(data), ''",
            "            convert.execute('file:///test/path.vmdk')",
            "",
            "    def test_image_convert_invalid_vmdk(self):",
            "        e = self.assertRaises(RuntimeError,",
            "                              self._test_image_convert_invalid_vmdk)",
            "        self.assertEqual('Invalid VMDK create-type specified', str(e))",
            "",
            "    def test_image_convert_valid_vmdk_no_types(self):",
            "        with mock.patch.object(CONF.image_format, 'vmdk_allowed_types',",
            "                               new=[]):",
            "            # We make it past the VMDK check and fail because our file",
            "            # does not exist",
            "            e = self.assertRaises(RuntimeError,",
            "                                  self._test_image_convert_invalid_vmdk)",
            "            self.assertEqual('Image is a VMDK, but no VMDK createType is '",
            "                             'specified', str(e))",
            "",
            "    def test_image_convert_valid_vmdk(self):",
            "        with mock.patch.object(CONF.image_format, 'vmdk_allowed_types',",
            "                               new=['monolithicSparse', 'monolithicFlat']):",
            "            # We make it past the VMDK check and fail because our file",
            "            # does not exist",
            "            self.assertRaises(FileNotFoundError,",
            "                              self._test_image_convert_invalid_vmdk)",
            "",
            "    def test_image_convert_fails(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.side_effect = [('{\"format\":\"raw\"}', ''),",
            "                                    OSError('convert_fail')]",
            "            self.assertRaises(OSError,",
            "                              convert.execute, 'file:///test/path.raw')",
            "            exc_mock.assert_has_calls(",
            "                [mock.call('qemu-img', 'info',",
            "                           '--output=json',",
            "                           '/test/path.raw',",
            "                           prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                           python_exec=convert.python,",
            "                           log_errors=processutils.LOG_ALL_ERRORS),",
            "                 mock.call('qemu-img', 'convert', '-f', 'raw', '-O', 'qcow2',",
            "                           '/test/path.raw', '/test/path.raw.qcow2',",
            "                           log_errors=processutils.LOG_ALL_ERRORS)])",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_reports_fail(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.side_effect = [('{\"format\":\"raw\"}', ''),",
            "                                    ('', 'some error')]",
            "            self.assertRaises(RuntimeError,",
            "                              convert.execute, 'file:///test/path.raw')",
            "            exc_mock.assert_has_calls(",
            "                [mock.call('qemu-img', 'info',",
            "                           '--output=json',",
            "                           '/test/path.raw',",
            "                           prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                           python_exec=convert.python,",
            "                           log_errors=processutils.LOG_ALL_ERRORS),",
            "                 mock.call('qemu-img', 'convert', '-f', 'raw', '-O', 'qcow2',",
            "                           '/test/path.raw', '/test/path.raw.qcow2',",
            "                           log_errors=processutils.LOG_ALL_ERRORS)])",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_fails_source_format(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = ('{}', '')",
            "            exc = self.assertRaises(RuntimeError,",
            "                                    convert.execute, 'file:///test/path.raw')",
            "            self.assertIn('Source format not reported', str(exc))",
            "            exc_mock.assert_called_once_with(",
            "                'qemu-img', 'info',",
            "                '--output=json',",
            "                '/test/path.raw',",
            "                prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                python_exec=convert.python,",
            "                log_errors=processutils.LOG_ALL_ERRORS)",
            "        # Make sure we did not update the image",
            "        self.img_repo.save.assert_not_called()",
            "",
            "    def test_image_convert_same_format_does_nothing(self):",
            "        convert = self._setup_image_convert_info_fail()",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = (",
            "                '{\"format\": \"qcow2\", \"virtual-size\": 123}', '')",
            "            convert.execute('file:///test/path.qcow')",
            "            # Make sure we only called qemu-img for inspection, not conversion",
            "            exc_mock.assert_called_once_with(",
            "                'qemu-img', 'info',",
            "                '--output=json',",
            "                '/test/path.qcow',",
            "                prlimit=async_utils.QEMU_IMG_PROC_LIMITS,",
            "                python_exec=convert.python,",
            "                log_errors=processutils.LOG_ALL_ERRORS)",
            "",
            "        # Make sure we set the virtual_size before we exited",
            "        image = self.img_repo.get.return_value",
            "        self.assertEqual(123, image.virtual_size)",
            "",
            "    @mock.patch.object(os, 'remove')",
            "    def test_image_convert_revert_success(self, mock_os_remove):",
            "        mock_os_remove.return_value = None",
            "        image_convert = image_conversion._ConvertImage(self.context,",
            "                                                       self.task.task_id,",
            "                                                       self.task_type,",
            "                                                       self.wrapper)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        with mock.patch.object(processutils, 'execute') as exc_mock:",
            "            exc_mock.return_value = (\"\", None)",
            "            with mock.patch.object(os.path, 'exists') as os_exists_mock:",
            "                os_exists_mock.return_value = True",
            "                image_convert.revert(result=mock.MagicMock())",
            "                self.assertEqual(1, mock_os_remove.call_count)",
            "",
            "    def test_image_convert_interpreter_configured(self):",
            "        # By default, wsgi.python_interpreter is None; if it is",
            "        # overridden, we should take the interpreter from config.",
            "        fake_interpreter = '/usr/bin/python2.7'",
            "        self.config(python_interpreter=fake_interpreter,",
            "                    group='wsgi')",
            "        convert = image_conversion._ConvertImage(self.context,",
            "                                                 self.task.task_id,",
            "                                                 self.task_type,",
            "                                                 self.wrapper)",
            "        self.assertEqual(fake_interpreter, convert.python)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "glance.tests.unit.async_.flows.plugins.test_image_conversion.TestConvertImageTask.self"
        ]
    },
    "glance/tests/unit/async_/flows/test_import.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": 178,
                "PatchRowcode": "                 self.assertFalse(os.path.exists(tmp_image_path))"
            },
            "1": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 179,
                "PatchRowcode": "                 self.assertTrue(os.path.exists(image_path))"
            },
            "2": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 180,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+    def test_import_flow_invalid_data_file(self):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 182,
                "PatchRowcode": "+        self.config(engine_mode='serial',"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+                    group='taskflow_executor')"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+        img_factory = mock.MagicMock()"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+        executor = taskflow_executor.TaskExecutor("
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+            self.context,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+            self.task_repo,"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+            self.img_repo,"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+            img_factory)"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 193,
                "PatchRowcode": "+        self.task_repo.get.return_value = self.task"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+        def create_image(*args, **kwargs):"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+            kwargs['image_id'] = UUID1"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+            return self.img_factory.new_image(*args, **kwargs)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+        self.img_repo.get.return_value = self.image"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+        img_factory.new_image.side_effect = create_image"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+            with mock.patch.object(putils, 'trycmd') as tmock:"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+                out = json.dumps({'format-specific':"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+                                  {'data': {'data-file': 'somefile'}}})"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+                tmock.return_value = (out, '')"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+                e = self.assertRaises(RuntimeError,"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+                                      executor.begin_processing,"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+                                      self.task.task_id)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+                self.assertIn('somefile', str(e))"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 213,
                "PatchRowcode": "+"
            },
            "36": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "     def test_import_flow_revert_import_to_fs(self):"
            },
            "37": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "         self.config(engine_mode='serial', group='taskflow_executor')"
            },
            "38": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 216,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Red Hat, Inc.",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import io",
            "import json",
            "import os",
            "from unittest import mock",
            "import urllib",
            "",
            "import glance_store",
            "from oslo_concurrency import processutils as putils",
            "from oslo_config import cfg",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "import glance.async_.flows.base_import as import_flow",
            "from glance.async_ import taskflow_executor",
            "from glance.async_ import utils as async_utils",
            "from glance.common.scripts.image_import import main as image_import",
            "from glance.common.scripts import utils as script_utils",
            "from glance.common import utils",
            "from glance import context",
            "from glance import domain",
            "from glance import gateway",
            "import glance.tests.utils as test_utils",
            "",
            "CONF = cfg.CONF",
            "",
            "UUID1 = 'c80a1a6c-bd1f-41c5-90ee-81afedb1d58d'",
            "TENANT1 = '6838eb7b-6ded-434a-882c-b344c77fe8df'",
            "",
            "",
            "class _ErrorTask(task.Task):",
            "",
            "    def execute(self):",
            "        raise RuntimeError()",
            "",
            "",
            "class TestImportTask(test_utils.BaseTestCase):",
            "",
            "    def setUp(self):",
            "        super(TestImportTask, self).setUp()",
            "",
            "        glance_store.register_opts(CONF)",
            "        self.config(default_store='file',",
            "                    stores=['file', 'http'],",
            "                    filesystem_store_datadir=self.test_dir,",
            "                    group=\"glance_store\")",
            "        glance_store.create_stores(CONF)",
            "",
            "        self.work_dir = os.path.join(self.test_dir, 'work_dir')",
            "        utils.safe_mkdirs(self.work_dir)",
            "        self.config(work_dir=self.work_dir, group='task')",
            "",
            "        self.context = context.RequestContext(",
            "            user_id=TENANT1, project_id=TENANT1, overwrite=False",
            "        )",
            "        self.img_repo = mock.MagicMock()",
            "        self.task_repo = mock.MagicMock()",
            "",
            "        self.gateway = gateway.Gateway()",
            "        self.task_factory = domain.TaskFactory()",
            "        self.img_factory = self.gateway.get_image_factory(self.context)",
            "        self.image = self.img_factory.new_image(image_id=UUID1,",
            "                                                disk_format='qcow2',",
            "                                                container_format='bare')",
            "",
            "        task_input = {",
            "            \"import_from\": \"http://cloud.foo/image.qcow2\",",
            "            \"import_from_format\": \"qcow2\",",
            "            \"image_properties\": {'disk_format': 'qcow2',",
            "                                 'container_format': 'bare'}",
            "        }",
            "        task_ttl = CONF.task.task_time_to_live",
            "",
            "        self.task_type = 'import'",
            "        request_id = 'fake_request_id'",
            "        user_id = 'fake_user'",
            "        self.task = self.task_factory.new_task(self.task_type, TENANT1,",
            "                                               UUID1, user_id, request_id,",
            "                                               task_time_to_live=task_ttl,",
            "                                               task_input=task_input)",
            "",
            "    def _assert_qemu_process_limits(self, exec_mock):",
            "        # NOTE(hemanthm): Assert that process limits are being applied",
            "        # on \"qemu-img info\" calls. See bug #1449062 for more details.",
            "        kw_args = exec_mock.call_args[1]",
            "        self.assertIn('prlimit', kw_args)",
            "        self.assertEqual(async_utils.QEMU_IMG_PROC_LIMITS,",
            "                         kw_args.get('prlimit'))",
            "",
            "    def test_import_flow(self):",
            "        self.config(engine_mode='serial',",
            "                    group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                tmock.return_value = (json.dumps({",
            "                    'format': 'qcow2',",
            "                }), None)",
            "",
            "                executor.begin_processing(self.task.task_id)",
            "                image_path = os.path.join(self.test_dir, self.image.image_id)",
            "                tmp_image_path = os.path.join(self.work_dir,",
            "                                              \"%s.tasks_import\" % image_path)",
            "",
            "                self.assertFalse(os.path.exists(tmp_image_path))",
            "                self.assertTrue(os.path.exists(image_path))",
            "                self.assertEqual(1, len(list(self.image.locations)))",
            "                self.assertEqual(\"file://%s%s%s\" % (self.test_dir, os.sep,",
            "                                                    self.image.image_id),",
            "                                 self.image.locations[0]['url'])",
            "",
            "                self._assert_qemu_process_limits(tmock)",
            "",
            "    def test_import_flow_missing_work_dir(self):",
            "        self.config(engine_mode='serial', group='taskflow_executor')",
            "        self.config(work_dir=None, group='task')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(import_flow._ImportToFS, 'execute') as emk:",
            "                executor.begin_processing(self.task.task_id)",
            "                self.assertFalse(emk.called)",
            "",
            "                image_path = os.path.join(self.test_dir, self.image.image_id)",
            "                tmp_image_path = os.path.join(self.work_dir,",
            "                                              \"%s.tasks_import\" % image_path)",
            "                self.assertFalse(os.path.exists(tmp_image_path))",
            "                self.assertTrue(os.path.exists(image_path))",
            "",
            "    def test_import_flow_revert_import_to_fs(self):",
            "        self.config(engine_mode='serial', group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.side_effect = RuntimeError",
            "",
            "            with mock.patch.object(import_flow._ImportToFS, 'revert') as rmock:",
            "                self.assertRaises(RuntimeError,",
            "                                  executor.begin_processing, self.task.task_id)",
            "                self.assertTrue(rmock.called)",
            "                self.assertIsInstance(rmock.call_args[1]['result'],",
            "                                      failure.Failure)",
            "",
            "                image_path = os.path.join(self.test_dir, self.image.image_id)",
            "                tmp_image_path = os.path.join(self.work_dir,",
            "                                              \"%s.tasks_import\" % image_path)",
            "                self.assertFalse(os.path.exists(tmp_image_path))",
            "                # Note(sabari): The image should not have been uploaded to",
            "                # the store as the flow failed before ImportToStore Task.",
            "                self.assertFalse(os.path.exists(image_path))",
            "",
            "    def test_import_flow_backed_file_import_to_fs(self):",
            "        self.config(engine_mode='serial', group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                tmock.return_value = (json.dumps({",
            "                    'backing-filename': '/etc/password'",
            "                }), None)",
            "",
            "                with mock.patch.object(import_flow._ImportToFS,",
            "                                       'revert') as rmock:",
            "                    self.assertRaises(RuntimeError,",
            "                                      executor.begin_processing,",
            "                                      self.task.task_id)",
            "                    self.assertTrue(rmock.called)",
            "                    self.assertIsInstance(rmock.call_args[1]['result'],",
            "                                          failure.Failure)",
            "                    self._assert_qemu_process_limits(tmock)",
            "",
            "                    image_path = os.path.join(self.test_dir,",
            "                                              self.image.image_id)",
            "",
            "                    fname = \"%s.tasks_import\" % image_path",
            "                    tmp_image_path = os.path.join(self.work_dir, fname)",
            "",
            "                    self.assertFalse(os.path.exists(tmp_image_path))",
            "                    # Note(sabari): The image should not have been uploaded to",
            "                    # the store as the flow failed before ImportToStore Task.",
            "                    self.assertFalse(os.path.exists(image_path))",
            "",
            "    def test_import_flow_revert(self):",
            "        self.config(engine_mode='serial',",
            "                    group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                tmock.return_value = (json.dumps({",
            "                    'format': 'qcow2',",
            "                }), None)",
            "",
            "                with mock.patch.object(import_flow,",
            "                                       \"_get_import_flows\") as imock:",
            "                    imock.return_value = (x for x in [_ErrorTask()])",
            "                    self.assertRaises(RuntimeError,",
            "                                      executor.begin_processing,",
            "                                      self.task.task_id)",
            "",
            "                    self._assert_qemu_process_limits(tmock)",
            "",
            "                    image_path = os.path.join(self.test_dir,",
            "                                              self.image.image_id)",
            "                    tmp_image_path = os.path.join(self.work_dir,",
            "                                                  (\"%s.tasks_import\" %",
            "                                                   image_path))",
            "                    self.assertFalse(os.path.exists(tmp_image_path))",
            "",
            "                    # NOTE(flaper87): Eventually, we want this to be assertTrue",
            "                    # The current issue is there's no way to tell taskflow to",
            "                    # continue on failures. That is, revert the subflow but",
            "                    # keep executing the parent flow. Under",
            "                    # discussion/development.",
            "                    self.assertFalse(os.path.exists(image_path))",
            "",
            "    def test_import_flow_no_import_flows(self):",
            "        self.config(engine_mode='serial',",
            "                    group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(urllib.request, 'urlopen') as umock:",
            "            content = b\"TEST_IMAGE\"",
            "            umock.return_value = io.BytesIO(content)",
            "",
            "            with mock.patch.object(import_flow, \"_get_import_flows\") as imock:",
            "                imock.return_value = (x for x in [])",
            "                executor.begin_processing(self.task.task_id)",
            "                image_path = os.path.join(self.test_dir, self.image.image_id)",
            "                tmp_image_path = os.path.join(self.work_dir,",
            "                                              \"%s.tasks_import\" % image_path)",
            "                self.assertFalse(os.path.exists(tmp_image_path))",
            "                self.assertTrue(os.path.exists(image_path))",
            "                self.assertEqual(1, umock.call_count)",
            "",
            "                with open(image_path, 'rb') as ifile:",
            "                    self.assertEqual(content, ifile.read())",
            "",
            "    def test_create_image(self):",
            "        image_create = import_flow._CreateImage(self.task.task_id,",
            "                                                self.task_type,",
            "                                                self.task_repo,",
            "                                                self.img_repo,",
            "                                                self.img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "        with mock.patch.object(image_import, 'create_image') as ci_mock:",
            "            ci_mock.return_value = mock.Mock()",
            "            image_create.execute()",
            "",
            "            ci_mock.assert_called_once_with(self.img_repo,",
            "                                            self.img_factory,",
            "                                            {'container_format': 'bare',",
            "                                             'disk_format': 'qcow2'},",
            "                                            self.task.task_id)",
            "",
            "    def test_save_image(self):",
            "        save_image = import_flow._SaveImage(self.task.task_id,",
            "                                            self.task_type,",
            "                                            self.img_repo)",
            "",
            "        with mock.patch.object(self.img_repo, 'get') as get_mock:",
            "            image_id = mock.sentinel.image_id",
            "            image = mock.MagicMock(image_id=image_id, status='saving')",
            "            get_mock.return_value = image",
            "",
            "            with mock.patch.object(self.img_repo, 'save') as save_mock:",
            "                save_image.execute(image.image_id)",
            "                get_mock.assert_called_once_with(image_id)",
            "                save_mock.assert_called_once_with(image)",
            "                self.assertEqual('active', image.status)",
            "",
            "    def test_import_to_fs(self):",
            "        import_fs = import_flow._ImportToFS(self.task.task_id,",
            "                                            self.task_type,",
            "                                            self.task_repo,",
            "                                            'http://example.com/image.qcow2')",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            content = b\"test\"",
            "            dmock.return_value = [content]",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                tmock.return_value = (json.dumps({",
            "                    'format': 'qcow2',",
            "                }), None)",
            "",
            "                image_id = UUID1",
            "                path = import_fs.execute(image_id)",
            "                reader, size = glance_store.get_from_backend(path)",
            "                self.assertEqual(4, size)",
            "                self.assertEqual(content, b\"\".join(reader))",
            "",
            "                image_path = os.path.join(self.work_dir, image_id)",
            "                tmp_image_path = os.path.join(self.work_dir, image_path)",
            "                self.assertTrue(os.path.exists(tmp_image_path))",
            "                self._assert_qemu_process_limits(tmock)",
            "",
            "    def test_delete_from_fs(self):",
            "        delete_fs = import_flow._DeleteFromFS(self.task.task_id,",
            "                                              self.task_type)",
            "",
            "        data = [b\"test\"]",
            "",
            "        store = glance_store.get_store_from_scheme('file')",
            "        path = glance_store.store_add_to_backend(mock.sentinel.image_id, data,",
            "                                                 mock.sentinel.image_size,",
            "                                                 store, context=None)[0]",
            "",
            "        path_wo_scheme = path.split(\"file://\")[1]",
            "        self.assertTrue(os.path.exists(path_wo_scheme))",
            "        delete_fs.execute(path)",
            "        self.assertFalse(os.path.exists(path_wo_scheme))",
            "",
            "    def test_complete_task(self):",
            "        complete_task = import_flow._CompleteTask(self.task.task_id,",
            "                                                  self.task_type,",
            "                                                  self.task_repo)",
            "",
            "        image_id = mock.sentinel.image_id",
            "        image = mock.MagicMock(image_id=image_id)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "        with mock.patch.object(self.task, 'succeed') as succeed:",
            "            complete_task.execute(image.image_id)",
            "            succeed.assert_called_once_with({'image_id': image_id})"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Red Hat, Inc.",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import io",
            "import json",
            "import os",
            "from unittest import mock",
            "import urllib",
            "",
            "import glance_store",
            "from oslo_concurrency import processutils as putils",
            "from oslo_config import cfg",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "import glance.async_.flows.base_import as import_flow",
            "from glance.async_ import taskflow_executor",
            "from glance.async_ import utils as async_utils",
            "from glance.common.scripts.image_import import main as image_import",
            "from glance.common.scripts import utils as script_utils",
            "from glance.common import utils",
            "from glance import context",
            "from glance import domain",
            "from glance import gateway",
            "import glance.tests.utils as test_utils",
            "",
            "CONF = cfg.CONF",
            "",
            "UUID1 = 'c80a1a6c-bd1f-41c5-90ee-81afedb1d58d'",
            "TENANT1 = '6838eb7b-6ded-434a-882c-b344c77fe8df'",
            "",
            "",
            "class _ErrorTask(task.Task):",
            "",
            "    def execute(self):",
            "        raise RuntimeError()",
            "",
            "",
            "class TestImportTask(test_utils.BaseTestCase):",
            "",
            "    def setUp(self):",
            "        super(TestImportTask, self).setUp()",
            "",
            "        glance_store.register_opts(CONF)",
            "        self.config(default_store='file',",
            "                    stores=['file', 'http'],",
            "                    filesystem_store_datadir=self.test_dir,",
            "                    group=\"glance_store\")",
            "        glance_store.create_stores(CONF)",
            "",
            "        self.work_dir = os.path.join(self.test_dir, 'work_dir')",
            "        utils.safe_mkdirs(self.work_dir)",
            "        self.config(work_dir=self.work_dir, group='task')",
            "",
            "        self.context = context.RequestContext(",
            "            user_id=TENANT1, project_id=TENANT1, overwrite=False",
            "        )",
            "        self.img_repo = mock.MagicMock()",
            "        self.task_repo = mock.MagicMock()",
            "",
            "        self.gateway = gateway.Gateway()",
            "        self.task_factory = domain.TaskFactory()",
            "        self.img_factory = self.gateway.get_image_factory(self.context)",
            "        self.image = self.img_factory.new_image(image_id=UUID1,",
            "                                                disk_format='qcow2',",
            "                                                container_format='bare')",
            "",
            "        task_input = {",
            "            \"import_from\": \"http://cloud.foo/image.qcow2\",",
            "            \"import_from_format\": \"qcow2\",",
            "            \"image_properties\": {'disk_format': 'qcow2',",
            "                                 'container_format': 'bare'}",
            "        }",
            "        task_ttl = CONF.task.task_time_to_live",
            "",
            "        self.task_type = 'import'",
            "        request_id = 'fake_request_id'",
            "        user_id = 'fake_user'",
            "        self.task = self.task_factory.new_task(self.task_type, TENANT1,",
            "                                               UUID1, user_id, request_id,",
            "                                               task_time_to_live=task_ttl,",
            "                                               task_input=task_input)",
            "",
            "    def _assert_qemu_process_limits(self, exec_mock):",
            "        # NOTE(hemanthm): Assert that process limits are being applied",
            "        # on \"qemu-img info\" calls. See bug #1449062 for more details.",
            "        kw_args = exec_mock.call_args[1]",
            "        self.assertIn('prlimit', kw_args)",
            "        self.assertEqual(async_utils.QEMU_IMG_PROC_LIMITS,",
            "                         kw_args.get('prlimit'))",
            "",
            "    def test_import_flow(self):",
            "        self.config(engine_mode='serial',",
            "                    group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                tmock.return_value = (json.dumps({",
            "                    'format': 'qcow2',",
            "                }), None)",
            "",
            "                executor.begin_processing(self.task.task_id)",
            "                image_path = os.path.join(self.test_dir, self.image.image_id)",
            "                tmp_image_path = os.path.join(self.work_dir,",
            "                                              \"%s.tasks_import\" % image_path)",
            "",
            "                self.assertFalse(os.path.exists(tmp_image_path))",
            "                self.assertTrue(os.path.exists(image_path))",
            "                self.assertEqual(1, len(list(self.image.locations)))",
            "                self.assertEqual(\"file://%s%s%s\" % (self.test_dir, os.sep,",
            "                                                    self.image.image_id),",
            "                                 self.image.locations[0]['url'])",
            "",
            "                self._assert_qemu_process_limits(tmock)",
            "",
            "    def test_import_flow_missing_work_dir(self):",
            "        self.config(engine_mode='serial', group='taskflow_executor')",
            "        self.config(work_dir=None, group='task')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(import_flow._ImportToFS, 'execute') as emk:",
            "                executor.begin_processing(self.task.task_id)",
            "                self.assertFalse(emk.called)",
            "",
            "                image_path = os.path.join(self.test_dir, self.image.image_id)",
            "                tmp_image_path = os.path.join(self.work_dir,",
            "                                              \"%s.tasks_import\" % image_path)",
            "                self.assertFalse(os.path.exists(tmp_image_path))",
            "                self.assertTrue(os.path.exists(image_path))",
            "",
            "    def test_import_flow_invalid_data_file(self):",
            "        self.config(engine_mode='serial',",
            "                    group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                out = json.dumps({'format-specific':",
            "                                  {'data': {'data-file': 'somefile'}}})",
            "                tmock.return_value = (out, '')",
            "                e = self.assertRaises(RuntimeError,",
            "                                      executor.begin_processing,",
            "                                      self.task.task_id)",
            "                self.assertIn('somefile', str(e))",
            "",
            "    def test_import_flow_revert_import_to_fs(self):",
            "        self.config(engine_mode='serial', group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.side_effect = RuntimeError",
            "",
            "            with mock.patch.object(import_flow._ImportToFS, 'revert') as rmock:",
            "                self.assertRaises(RuntimeError,",
            "                                  executor.begin_processing, self.task.task_id)",
            "                self.assertTrue(rmock.called)",
            "                self.assertIsInstance(rmock.call_args[1]['result'],",
            "                                      failure.Failure)",
            "",
            "                image_path = os.path.join(self.test_dir, self.image.image_id)",
            "                tmp_image_path = os.path.join(self.work_dir,",
            "                                              \"%s.tasks_import\" % image_path)",
            "                self.assertFalse(os.path.exists(tmp_image_path))",
            "                # Note(sabari): The image should not have been uploaded to",
            "                # the store as the flow failed before ImportToStore Task.",
            "                self.assertFalse(os.path.exists(image_path))",
            "",
            "    def test_import_flow_backed_file_import_to_fs(self):",
            "        self.config(engine_mode='serial', group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                tmock.return_value = (json.dumps({",
            "                    'backing-filename': '/etc/password'",
            "                }), None)",
            "",
            "                with mock.patch.object(import_flow._ImportToFS,",
            "                                       'revert') as rmock:",
            "                    self.assertRaises(RuntimeError,",
            "                                      executor.begin_processing,",
            "                                      self.task.task_id)",
            "                    self.assertTrue(rmock.called)",
            "                    self.assertIsInstance(rmock.call_args[1]['result'],",
            "                                          failure.Failure)",
            "                    self._assert_qemu_process_limits(tmock)",
            "",
            "                    image_path = os.path.join(self.test_dir,",
            "                                              self.image.image_id)",
            "",
            "                    fname = \"%s.tasks_import\" % image_path",
            "                    tmp_image_path = os.path.join(self.work_dir, fname)",
            "",
            "                    self.assertFalse(os.path.exists(tmp_image_path))",
            "                    # Note(sabari): The image should not have been uploaded to",
            "                    # the store as the flow failed before ImportToStore Task.",
            "                    self.assertFalse(os.path.exists(image_path))",
            "",
            "    def test_import_flow_revert(self):",
            "        self.config(engine_mode='serial',",
            "                    group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            dmock.return_value = io.BytesIO(b\"TEST_IMAGE\")",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                tmock.return_value = (json.dumps({",
            "                    'format': 'qcow2',",
            "                }), None)",
            "",
            "                with mock.patch.object(import_flow,",
            "                                       \"_get_import_flows\") as imock:",
            "                    imock.return_value = (x for x in [_ErrorTask()])",
            "                    self.assertRaises(RuntimeError,",
            "                                      executor.begin_processing,",
            "                                      self.task.task_id)",
            "",
            "                    self._assert_qemu_process_limits(tmock)",
            "",
            "                    image_path = os.path.join(self.test_dir,",
            "                                              self.image.image_id)",
            "                    tmp_image_path = os.path.join(self.work_dir,",
            "                                                  (\"%s.tasks_import\" %",
            "                                                   image_path))",
            "                    self.assertFalse(os.path.exists(tmp_image_path))",
            "",
            "                    # NOTE(flaper87): Eventually, we want this to be assertTrue",
            "                    # The current issue is there's no way to tell taskflow to",
            "                    # continue on failures. That is, revert the subflow but",
            "                    # keep executing the parent flow. Under",
            "                    # discussion/development.",
            "                    self.assertFalse(os.path.exists(image_path))",
            "",
            "    def test_import_flow_no_import_flows(self):",
            "        self.config(engine_mode='serial',",
            "                    group='taskflow_executor')",
            "",
            "        img_factory = mock.MagicMock()",
            "",
            "        executor = taskflow_executor.TaskExecutor(",
            "            self.context,",
            "            self.task_repo,",
            "            self.img_repo,",
            "            img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "",
            "        def create_image(*args, **kwargs):",
            "            kwargs['image_id'] = UUID1",
            "            return self.img_factory.new_image(*args, **kwargs)",
            "",
            "        self.img_repo.get.return_value = self.image",
            "        img_factory.new_image.side_effect = create_image",
            "",
            "        with mock.patch.object(urllib.request, 'urlopen') as umock:",
            "            content = b\"TEST_IMAGE\"",
            "            umock.return_value = io.BytesIO(content)",
            "",
            "            with mock.patch.object(import_flow, \"_get_import_flows\") as imock:",
            "                imock.return_value = (x for x in [])",
            "                executor.begin_processing(self.task.task_id)",
            "                image_path = os.path.join(self.test_dir, self.image.image_id)",
            "                tmp_image_path = os.path.join(self.work_dir,",
            "                                              \"%s.tasks_import\" % image_path)",
            "                self.assertFalse(os.path.exists(tmp_image_path))",
            "                self.assertTrue(os.path.exists(image_path))",
            "                self.assertEqual(1, umock.call_count)",
            "",
            "                with open(image_path, 'rb') as ifile:",
            "                    self.assertEqual(content, ifile.read())",
            "",
            "    def test_create_image(self):",
            "        image_create = import_flow._CreateImage(self.task.task_id,",
            "                                                self.task_type,",
            "                                                self.task_repo,",
            "                                                self.img_repo,",
            "                                                self.img_factory)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "        with mock.patch.object(image_import, 'create_image') as ci_mock:",
            "            ci_mock.return_value = mock.Mock()",
            "            image_create.execute()",
            "",
            "            ci_mock.assert_called_once_with(self.img_repo,",
            "                                            self.img_factory,",
            "                                            {'container_format': 'bare',",
            "                                             'disk_format': 'qcow2'},",
            "                                            self.task.task_id)",
            "",
            "    def test_save_image(self):",
            "        save_image = import_flow._SaveImage(self.task.task_id,",
            "                                            self.task_type,",
            "                                            self.img_repo)",
            "",
            "        with mock.patch.object(self.img_repo, 'get') as get_mock:",
            "            image_id = mock.sentinel.image_id",
            "            image = mock.MagicMock(image_id=image_id, status='saving')",
            "            get_mock.return_value = image",
            "",
            "            with mock.patch.object(self.img_repo, 'save') as save_mock:",
            "                save_image.execute(image.image_id)",
            "                get_mock.assert_called_once_with(image_id)",
            "                save_mock.assert_called_once_with(image)",
            "                self.assertEqual('active', image.status)",
            "",
            "    def test_import_to_fs(self):",
            "        import_fs = import_flow._ImportToFS(self.task.task_id,",
            "                                            self.task_type,",
            "                                            self.task_repo,",
            "                                            'http://example.com/image.qcow2')",
            "",
            "        with mock.patch.object(script_utils, 'get_image_data_iter') as dmock:",
            "            content = b\"test\"",
            "            dmock.return_value = [content]",
            "",
            "            with mock.patch.object(putils, 'trycmd') as tmock:",
            "                tmock.return_value = (json.dumps({",
            "                    'format': 'qcow2',",
            "                }), None)",
            "",
            "                image_id = UUID1",
            "                path = import_fs.execute(image_id)",
            "                reader, size = glance_store.get_from_backend(path)",
            "                self.assertEqual(4, size)",
            "                self.assertEqual(content, b\"\".join(reader))",
            "",
            "                image_path = os.path.join(self.work_dir, image_id)",
            "                tmp_image_path = os.path.join(self.work_dir, image_path)",
            "                self.assertTrue(os.path.exists(tmp_image_path))",
            "                self._assert_qemu_process_limits(tmock)",
            "",
            "    def test_delete_from_fs(self):",
            "        delete_fs = import_flow._DeleteFromFS(self.task.task_id,",
            "                                              self.task_type)",
            "",
            "        data = [b\"test\"]",
            "",
            "        store = glance_store.get_store_from_scheme('file')",
            "        path = glance_store.store_add_to_backend(mock.sentinel.image_id, data,",
            "                                                 mock.sentinel.image_size,",
            "                                                 store, context=None)[0]",
            "",
            "        path_wo_scheme = path.split(\"file://\")[1]",
            "        self.assertTrue(os.path.exists(path_wo_scheme))",
            "        delete_fs.execute(path)",
            "        self.assertFalse(os.path.exists(path_wo_scheme))",
            "",
            "    def test_complete_task(self):",
            "        complete_task = import_flow._CompleteTask(self.task.task_id,",
            "                                                  self.task_type,",
            "                                                  self.task_repo)",
            "",
            "        image_id = mock.sentinel.image_id",
            "        image = mock.MagicMock(image_id=image_id)",
            "",
            "        self.task_repo.get.return_value = self.task",
            "        with mock.patch.object(self.task, 'succeed') as succeed:",
            "            complete_task.execute(image.image_id)",
            "            succeed.assert_called_once_with({'image_id': image_id})"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "glance.tests.unit.async_.flows.test_import.TestImportTask.self"
        ]
    }
}