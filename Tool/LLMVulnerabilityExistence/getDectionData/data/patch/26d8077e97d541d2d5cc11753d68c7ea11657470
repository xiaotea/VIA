{
    "superset/connectors/sqla/models.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " import numpy as np"
            },
            "1": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " import pandas as pd"
            },
            "2": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " import sqlalchemy as sa"
            },
            "3": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import sqlparse"
            },
            "4": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from flask import escape, Markup"
            },
            "5": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " from flask_appbuilder import Model"
            },
            "6": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " from flask_appbuilder.security.sqla.models import User"
            },
            "7": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "     ExploreMixin,"
            },
            "8": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "     ImportExportMixin,"
            },
            "9": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "     QueryResult,"
            },
            "10": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    QueryStringExtended,"
            },
            "11": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "     validate_adhoc_subquery,"
            },
            "12": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 107,
                "PatchRowcode": " )"
            },
            "13": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 108,
                "PatchRowcode": " from superset.models.slice import Slice"
            },
            "14": {
                "beforePatchRowNumber": 1099,
                "afterPatchRowNumber": 1097,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 1100,
                "afterPatchRowNumber": 1098,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 1101,
                "afterPatchRowNumber": 1099,
                "PatchRowcode": " class SqlaTable("
            },
            "17": {
                "beforePatchRowNumber": 1102,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Model, BaseDatasource, ExploreMixin"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1100,
                "PatchRowcode": "+    Model,"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1101,
                "PatchRowcode": "+    BaseDatasource,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1102,
                "PatchRowcode": "+    ExploreMixin,"
            },
            "21": {
                "beforePatchRowNumber": 1103,
                "afterPatchRowNumber": 1103,
                "PatchRowcode": " ):  # pylint: disable=too-many-public-methods"
            },
            "22": {
                "beforePatchRowNumber": 1104,
                "afterPatchRowNumber": 1104,
                "PatchRowcode": "     \"\"\"An ORM object for SqlAlchemy table references\"\"\""
            },
            "23": {
                "beforePatchRowNumber": 1105,
                "afterPatchRowNumber": 1105,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 1413,
                "afterPatchRowNumber": 1413,
                "PatchRowcode": "     def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:"
            },
            "25": {
                "beforePatchRowNumber": 1414,
                "afterPatchRowNumber": 1414,
                "PatchRowcode": "         return get_template_processor(table=self, database=self.database, **kwargs)"
            },
            "26": {
                "beforePatchRowNumber": 1415,
                "afterPatchRowNumber": 1415,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": 1416,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def get_query_str_extended("
            },
            "28": {
                "beforePatchRowNumber": 1417,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self,"
            },
            "29": {
                "beforePatchRowNumber": 1418,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        query_obj: QueryObjectDict,"
            },
            "30": {
                "beforePatchRowNumber": 1419,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        mutate: bool = True,"
            },
            "31": {
                "beforePatchRowNumber": 1420,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ) -> QueryStringExtended:"
            },
            "32": {
                "beforePatchRowNumber": 1421,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sqlaq = self.get_sqla_query(**query_obj)"
            },
            "33": {
                "beforePatchRowNumber": 1422,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sql = self.database.compile_sqla_query(sqlaq.sqla_query)"
            },
            "34": {
                "beforePatchRowNumber": 1423,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sql = self._apply_cte(sql, sqlaq.cte)"
            },
            "35": {
                "beforePatchRowNumber": 1424,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sql = sqlparse.format(sql, reindent=True)"
            },
            "36": {
                "beforePatchRowNumber": 1425,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if mutate:"
            },
            "37": {
                "beforePatchRowNumber": 1426,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            sql = self.mutate_query_from_config(sql)"
            },
            "38": {
                "beforePatchRowNumber": 1427,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return QueryStringExtended("
            },
            "39": {
                "beforePatchRowNumber": 1428,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            applied_template_filters=sqlaq.applied_template_filters,"
            },
            "40": {
                "beforePatchRowNumber": 1429,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            applied_filter_columns=sqlaq.applied_filter_columns,"
            },
            "41": {
                "beforePatchRowNumber": 1430,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            rejected_filter_columns=sqlaq.rejected_filter_columns,"
            },
            "42": {
                "beforePatchRowNumber": 1431,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            labels_expected=sqlaq.labels_expected,"
            },
            "43": {
                "beforePatchRowNumber": 1432,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            prequeries=sqlaq.prequeries,"
            },
            "44": {
                "beforePatchRowNumber": 1433,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            sql=sql,"
            },
            "45": {
                "beforePatchRowNumber": 1434,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "46": {
                "beforePatchRowNumber": 1435,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "47": {
                "beforePatchRowNumber": 1436,
                "afterPatchRowNumber": 1416,
                "PatchRowcode": "     def get_query_str(self, query_obj: QueryObjectDict) -> str:"
            },
            "48": {
                "beforePatchRowNumber": 1437,
                "afterPatchRowNumber": 1417,
                "PatchRowcode": "         query_str_ext = self.get_query_str_extended(query_obj)"
            },
            "49": {
                "beforePatchRowNumber": 1438,
                "afterPatchRowNumber": 1418,
                "PatchRowcode": "         all_queries = query_str_ext.prequeries + [query_str_ext.sql]"
            },
            "50": {
                "beforePatchRowNumber": 1474,
                "afterPatchRowNumber": 1454,
                "PatchRowcode": " "
            },
            "51": {
                "beforePatchRowNumber": 1475,
                "afterPatchRowNumber": 1455,
                "PatchRowcode": "         return from_clause, cte"
            },
            "52": {
                "beforePatchRowNumber": 1476,
                "afterPatchRowNumber": 1456,
                "PatchRowcode": " "
            },
            "53": {
                "beforePatchRowNumber": 1477,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def get_rendered_sql("
            },
            "54": {
                "beforePatchRowNumber": 1478,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self, template_processor: BaseTemplateProcessor | None = None"
            },
            "55": {
                "beforePatchRowNumber": 1479,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ) -> str:"
            },
            "56": {
                "beforePatchRowNumber": 1480,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "57": {
                "beforePatchRowNumber": 1481,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Render sql with template engine (Jinja)."
            },
            "58": {
                "beforePatchRowNumber": 1482,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "59": {
                "beforePatchRowNumber": 1483,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "60": {
                "beforePatchRowNumber": 1484,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sql = self.sql"
            },
            "61": {
                "beforePatchRowNumber": 1485,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if template_processor:"
            },
            "62": {
                "beforePatchRowNumber": 1486,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            try:"
            },
            "63": {
                "beforePatchRowNumber": 1487,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                sql = template_processor.process_template(sql)"
            },
            "64": {
                "beforePatchRowNumber": 1488,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            except TemplateError as ex:"
            },
            "65": {
                "beforePatchRowNumber": 1489,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                raise QueryObjectValidationError("
            },
            "66": {
                "beforePatchRowNumber": 1490,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    _("
            },
            "67": {
                "beforePatchRowNumber": 1491,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        \"Error while rendering virtual dataset query: %(msg)s\","
            },
            "68": {
                "beforePatchRowNumber": 1492,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        msg=ex.message,"
            },
            "69": {
                "beforePatchRowNumber": 1493,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    )"
            },
            "70": {
                "beforePatchRowNumber": 1494,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                ) from ex"
            },
            "71": {
                "beforePatchRowNumber": 1495,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)"
            },
            "72": {
                "beforePatchRowNumber": 1496,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not sql:"
            },
            "73": {
                "beforePatchRowNumber": 1497,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))"
            },
            "74": {
                "beforePatchRowNumber": 1498,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if len(sqlparse.split(sql)) > 1:"
            },
            "75": {
                "beforePatchRowNumber": 1499,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise QueryObjectValidationError("
            },
            "76": {
                "beforePatchRowNumber": 1500,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                _(\"Virtual dataset query cannot consist of multiple statements\")"
            },
            "77": {
                "beforePatchRowNumber": 1501,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "78": {
                "beforePatchRowNumber": 1502,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return sql"
            },
            "79": {
                "beforePatchRowNumber": 1503,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "80": {
                "beforePatchRowNumber": 1504,
                "afterPatchRowNumber": 1457,
                "PatchRowcode": "     def adhoc_metric_to_sqla("
            },
            "81": {
                "beforePatchRowNumber": 1505,
                "afterPatchRowNumber": 1458,
                "PatchRowcode": "         self,"
            },
            "82": {
                "beforePatchRowNumber": 1506,
                "afterPatchRowNumber": 1459,
                "PatchRowcode": "         metric: AdhocMetric,"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import builtins",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from dataclasses import dataclass, field",
            "from datetime import datetime, timedelta",
            "from json.decoder import JSONDecodeError",
            "from typing import Any, Callable, cast",
            "",
            "import dateutil.parser",
            "import numpy as np",
            "import pandas as pd",
            "import sqlalchemy as sa",
            "import sqlparse",
            "from flask import escape, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import (",
            "    and_,",
            "    Boolean,",
            "    Column,",
            "    DateTime,",
            "    Enum,",
            "    ForeignKey,",
            "    inspect,",
            "    Integer,",
            "    or_,",
            "    String,",
            "    Table,",
            "    Text,",
            "    update,",
            ")",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.ext.declarative import declared_attr",
            "from sqlalchemy.ext.hybrid import hybrid_property",
            "from sqlalchemy.orm import (",
            "    backref,",
            "    foreign,",
            "    Mapped,",
            "    Query,",
            "    reconstructor,",
            "    relationship,",
            "    RelationshipProperty,",
            ")",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.schema import UniqueConstraint",
            "from sqlalchemy.sql import column, ColumnElement, literal_column, table",
            "from sqlalchemy.sql.elements import ColumnClause, TextClause",
            "from sqlalchemy.sql.expression import Label, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "",
            "from superset import app, db, is_feature_enabled, security_manager",
            "from superset.commands.dataset.exceptions import DatasetNotFoundError",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.connectors.sqla.utils import (",
            "    get_columns_description,",
            "    get_physical_table_metadata,",
            "    get_virtual_table_metadata,",
            ")",
            "from superset.constants import EMPTY_STRING, NULL_STRING",
            "from superset.db_engine_specs.base import BaseEngineSpec, TimestampExpression",
            "from superset.exceptions import (",
            "    ColumnNotFoundException,",
            "    DatasetInvalidPermissionEvaluationException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetGenericDBErrorException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.jinja_context import (",
            "    BaseTemplateProcessor,",
            "    ExtraCache,",
            "    get_template_processor,",
            ")",
            "from superset.models.annotations import Annotation",
            "from superset.models.core import Database",
            "from superset.models.helpers import (",
            "    AuditMixinNullable,",
            "    CertificationMixin,",
            "    ExploreMixin,",
            "    ImportExportMixin,",
            "    QueryResult,",
            "    QueryStringExtended,",
            "    validate_adhoc_subquery,",
            ")",
            "from superset.models.slice import Slice",
            "from superset.sql_parse import ParsedQuery, sanitize_clause",
            "from superset.superset_typing import (",
            "    AdhocColumn,",
            "    AdhocMetric,",
            "    FilterValue,",
            "    FilterValues,",
            "    Metric,",
            "    QueryObjectDict,",
            "    ResultSetColumnType,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.backports import StrEnum",
            "from superset.utils.core import GenericDataType, MediumText",
            "",
            "config = app.config",
            "metadata = Model.metadata  # pylint: disable=no-member",
            "logger = logging.getLogger(__name__)",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "",
            "# a non-exhaustive set of additive metrics",
            "ADDITIVE_METRIC_TYPES = {",
            "    \"count\",",
            "    \"sum\",",
            "    \"doubleSum\",",
            "}",
            "ADDITIVE_METRIC_TYPES_LOWER = {op.lower() for op in ADDITIVE_METRIC_TYPES}",
            "",
            "",
            "@dataclass",
            "class MetadataResult:",
            "    added: list[str] = field(default_factory=list)",
            "    removed: list[str] = field(default_factory=list)",
            "    modified: list[str] = field(default_factory=list)",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "METRIC_FORM_DATA_PARAMS = [",
            "    \"metric\",",
            "    \"metric_2\",",
            "    \"metrics\",",
            "    \"metrics_b\",",
            "    \"percent_metrics\",",
            "    \"secondary_metric\",",
            "    \"size\",",
            "    \"timeseries_limit_metric\",",
            "    \"x\",",
            "    \"y\",",
            "]",
            "",
            "COLUMN_FORM_DATA_PARAMS = [",
            "    \"all_columns\",",
            "    \"all_columns_x\",",
            "    \"columns\",",
            "    \"entity\",",
            "    \"groupby\",",
            "    \"order_by_cols\",",
            "    \"series\",",
            "]",
            "",
            "",
            "class DatasourceKind(StrEnum):",
            "    VIRTUAL = \"virtual\"",
            "    PHYSICAL = \"physical\"",
            "",
            "",
            "class BaseDatasource(",
            "    AuditMixinNullable, ImportExportMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"A common interface to objects that are queryable",
            "    (tables and datasources)\"\"\"",
            "",
            "    # ---------------------------------------------------------------",
            "    # class attributes to define when deriving BaseDatasource",
            "    # ---------------------------------------------------------------",
            "    __tablename__: str | None = None  # {connector_name}_datasource",
            "    baselink: str | None = None  # url portion pointing to ModelView endpoint",
            "",
            "    owner_class: User | None = None",
            "",
            "    # Used to do code highlighting when displaying the query in the UI",
            "    query_language: str | None = None",
            "",
            "    # Only some datasources support Row Level Security",
            "    is_rls_supported: bool = False",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        # can be a Column or a property pointing to one",
            "        raise NotImplementedError()",
            "",
            "    # ---------------------------------------------------------------",
            "",
            "    # Columns",
            "    id = Column(Integer, primary_key=True)",
            "    description = Column(Text)",
            "    default_endpoint = Column(Text)",
            "    is_featured = Column(Boolean, default=False)  # TODO deprecating",
            "    filter_select_enabled = Column(Boolean, default=True)",
            "    offset = Column(Integer, default=0)",
            "    cache_timeout = Column(Integer)",
            "    params = Column(String(1000))",
            "    perm = Column(String(1000))",
            "    schema_perm = Column(String(1000))",
            "    is_managed_externally = Column(Boolean, nullable=False, default=False)",
            "    external_url = Column(Text, nullable=True)",
            "",
            "    sql: str | None = None",
            "    owners: list[User]",
            "    update_from_object_fields: list[str]",
            "",
            "    extra_import_fields = [\"is_managed_externally\", \"external_url\"]",
            "",
            "    @property",
            "    def kind(self) -> DatasourceKind:",
            "        return DatasourceKind.VIRTUAL if self.sql else DatasourceKind.PHYSICAL",
            "",
            "    @property",
            "    def owners_data(self) -> list[dict[str, Any]]:",
            "        return [",
            "            {",
            "                \"first_name\": o.first_name,",
            "                \"last_name\": o.last_name,",
            "                \"username\": o.username,",
            "                \"id\": o.id,",
            "            }",
            "            for o in self.owners",
            "        ]",
            "",
            "    @property",
            "    def is_virtual(self) -> bool:",
            "        return self.kind == DatasourceKind.VIRTUAL",
            "",
            "    @declared_attr",
            "    def slices(self) -> RelationshipProperty:",
            "        return relationship(",
            "            \"Slice\",",
            "            overlaps=\"table\",",
            "            primaryjoin=lambda: and_(",
            "                foreign(Slice.datasource_id) == self.id,",
            "                foreign(Slice.datasource_type) == self.type,",
            "            ),",
            "        )",
            "",
            "    columns: list[TableColumn] = []",
            "    metrics: list[SqlMetric] = []",
            "",
            "    @property",
            "    def type(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        \"\"\"Unique id across datasource types\"\"\"",
            "        return f\"{self.id}__{self.type}\"",
            "",
            "    @property",
            "    def column_names(self) -> list[str]:",
            "        return sorted([c.column_name for c in self.columns], key=lambda x: x or \"\")",
            "",
            "    @property",
            "    def columns_types(self) -> dict[str, str]:",
            "        return {c.column_name: c.type for c in self.columns}",
            "",
            "    @property",
            "    def main_dttm_col(self) -> str:",
            "        return \"timestamp\"",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def connection(self) -> str | None:",
            "        \"\"\"String representing the context of the Datasource\"\"\"",
            "        return None",
            "",
            "    @property",
            "    def schema(self) -> str | None:",
            "        \"\"\"String representing the schema of the Datasource (if it applies)\"\"\"",
            "        return None",
            "",
            "    @property",
            "    def filterable_column_names(self) -> list[str]:",
            "        return sorted([c.column_name for c in self.columns if c.filterable])",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        return []",
            "",
            "    @property",
            "    def url(self) -> str:",
            "        return f\"/{self.baselink}/edit/{self.id}\"",
            "",
            "    @property",
            "    def explore_url(self) -> str:",
            "        if self.default_endpoint:",
            "            return self.default_endpoint",
            "        return f\"/explore/?datasource_type={self.type}&datasource_id={self.id}\"",
            "",
            "    @property",
            "    def column_formats(self) -> dict[str, str | None]:",
            "        return {m.metric_name: m.d3format for m in self.metrics if m.d3format}",
            "",
            "    @property",
            "    def currency_formats(self) -> dict[str, dict[str, str | None] | None]:",
            "        return {m.metric_name: m.currency_json for m in self.metrics if m.currency_json}",
            "",
            "    def add_missing_metrics(self, metrics: list[SqlMetric]) -> None:",
            "        existing_metrics = {m.metric_name for m in self.metrics}",
            "        for metric in metrics:",
            "            if metric.metric_name not in existing_metrics:",
            "                metric.table_id = self.id",
            "                self.metrics.append(metric)",
            "",
            "    @property",
            "    def short_data(self) -> dict[str, Any]:",
            "        \"\"\"Data representation of the datasource sent to the frontend\"\"\"",
            "        return {",
            "            \"edit_url\": self.url,",
            "            \"id\": self.id,",
            "            \"uid\": self.uid,",
            "            \"schema\": self.schema,",
            "            \"name\": self.name,",
            "            \"type\": self.type,",
            "            \"connection\": self.connection,",
            "            \"creator\": str(self.created_by),",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        pass",
            "",
            "    @property",
            "    def order_by_choices(self) -> list[tuple[str, str]]:",
            "        choices = []",
            "        # self.column_names return sorted column_names",
            "        for column_name in self.column_names:",
            "            column_name = str(column_name or \"\")",
            "            choices.append(",
            "                (json.dumps([column_name, True]), f\"{column_name} \" + __(\"[asc]\"))",
            "            )",
            "            choices.append(",
            "                (json.dumps([column_name, False]), f\"{column_name} \" + __(\"[desc]\"))",
            "            )",
            "        return choices",
            "",
            "    @property",
            "    def verbose_map(self) -> dict[str, str]:",
            "        verb_map = {\"__timestamp\": \"Time\"}",
            "        verb_map.update(",
            "            {o.metric_name: o.verbose_name or o.metric_name for o in self.metrics}",
            "        )",
            "        verb_map.update(",
            "            {o.column_name: o.verbose_name or o.column_name for o in self.columns}",
            "        )",
            "        return verb_map",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        \"\"\"Data representation of the datasource sent to the frontend\"\"\"",
            "        return {",
            "            # simple fields",
            "            \"id\": self.id,",
            "            \"uid\": self.uid,",
            "            \"column_formats\": self.column_formats,",
            "            \"currency_formats\": self.currency_formats,",
            "            \"description\": self.description,",
            "            \"database\": self.database.data,  # pylint: disable=no-member",
            "            \"default_endpoint\": self.default_endpoint,",
            "            \"filter_select\": self.filter_select_enabled,  # TODO deprecate",
            "            \"filter_select_enabled\": self.filter_select_enabled,",
            "            \"name\": self.name,",
            "            \"datasource_name\": self.datasource_name,",
            "            \"table_name\": self.datasource_name,",
            "            \"type\": self.type,",
            "            \"schema\": self.schema,",
            "            \"offset\": self.offset,",
            "            \"cache_timeout\": self.cache_timeout,",
            "            \"params\": self.params,",
            "            \"perm\": self.perm,",
            "            \"edit_url\": self.url,",
            "            # sqla-specific",
            "            \"sql\": self.sql,",
            "            # one to many",
            "            \"columns\": [o.data for o in self.columns],",
            "            \"metrics\": [o.data for o in self.metrics],",
            "            # TODO deprecate, move logic to JS",
            "            \"order_by_choices\": self.order_by_choices,",
            "            \"owners\": [owner.id for owner in self.owners],",
            "            \"verbose_map\": self.verbose_map,",
            "            \"select_star\": self.select_star,",
            "        }",
            "",
            "    def data_for_slices(  # pylint: disable=too-many-locals",
            "        self, slices: list[Slice]",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        The representation of the datasource containing only the required data",
            "        to render the provided slices.",
            "",
            "        Used to reduce the payload when loading a dashboard.",
            "        \"\"\"",
            "        data = self.data",
            "        metric_names = set()",
            "        column_names = set()",
            "        for slc in slices:",
            "            form_data = slc.form_data",
            "            # pull out all required metrics from the form_data",
            "            for metric_param in METRIC_FORM_DATA_PARAMS:",
            "                for metric in utils.as_list(form_data.get(metric_param) or []):",
            "                    metric_names.add(utils.get_metric_name(metric))",
            "                    if utils.is_adhoc_metric(metric):",
            "                        column_ = metric.get(\"column\") or {}",
            "                        if column_name := column_.get(\"column_name\"):",
            "                            column_names.add(column_name)",
            "",
            "            # Columns used in query filters",
            "            column_names.update(",
            "                filter_[\"subject\"]",
            "                for filter_ in form_data.get(\"adhoc_filters\") or []",
            "                if filter_.get(\"clause\") == \"WHERE\" and filter_.get(\"subject\")",
            "            )",
            "",
            "            # columns used by Filter Box",
            "            column_names.update(",
            "                filter_config[\"column\"]",
            "                for filter_config in form_data.get(\"filter_configs\") or []",
            "                if \"column\" in filter_config",
            "            )",
            "",
            "            # for legacy dashboard imports which have the wrong query_context in them",
            "            try:",
            "                query_context = slc.get_query_context()",
            "            except DatasetNotFoundError:",
            "                query_context = None",
            "",
            "            # legacy charts don't have query_context charts",
            "            if query_context:",
            "                column_names.update(",
            "                    [",
            "                        utils.get_column_name(column_)",
            "                        for query in query_context.queries",
            "                        for column_ in query.columns",
            "                    ]",
            "                    or []",
            "                )",
            "            else:",
            "                _columns = [",
            "                    utils.get_column_name(column_)",
            "                    if utils.is_adhoc_column(column_)",
            "                    else column_",
            "                    for column_param in COLUMN_FORM_DATA_PARAMS",
            "                    for column_ in utils.as_list(form_data.get(column_param) or [])",
            "                ]",
            "                column_names.update(_columns)",
            "",
            "        filtered_metrics = [",
            "            metric",
            "            for metric in data[\"metrics\"]",
            "            if metric[\"metric_name\"] in metric_names",
            "        ]",
            "",
            "        filtered_columns: list[Column] = []",
            "        column_types: set[GenericDataType] = set()",
            "        for column_ in data[\"columns\"]:",
            "            generic_type = column_.get(\"type_generic\")",
            "            if generic_type is not None:",
            "                column_types.add(generic_type)",
            "            if column_[\"column_name\"] in column_names:",
            "                filtered_columns.append(column_)",
            "",
            "        data[\"column_types\"] = list(column_types)",
            "        del data[\"description\"]",
            "        data.update({\"metrics\": filtered_metrics})",
            "        data.update({\"columns\": filtered_columns})",
            "        verbose_map = {\"__timestamp\": \"Time\"}",
            "        verbose_map.update(",
            "            {",
            "                metric[\"metric_name\"]: metric[\"verbose_name\"] or metric[\"metric_name\"]",
            "                for metric in filtered_metrics",
            "            }",
            "        )",
            "        verbose_map.update(",
            "            {",
            "                column_[\"column_name\"]: column_[\"verbose_name\"]",
            "                or column_[\"column_name\"]",
            "                for column_ in filtered_columns",
            "            }",
            "        )",
            "        data[\"verbose_map\"] = verbose_map",
            "",
            "        return data",
            "",
            "    @staticmethod",
            "    def filter_values_handler(  # pylint: disable=too-many-arguments",
            "        values: FilterValues | None,",
            "        operator: str,",
            "        target_generic_type: GenericDataType,",
            "        target_native_type: str | None = None,",
            "        is_list_target: bool = False,",
            "        db_engine_spec: builtins.type[BaseEngineSpec] | None = None,",
            "        db_extra: dict[str, Any] | None = None,",
            "    ) -> FilterValues | None:",
            "        if values is None:",
            "            return None",
            "",
            "        def handle_single_value(value: FilterValue | None) -> FilterValue | None:",
            "            if operator == utils.FilterOperator.TEMPORAL_RANGE:",
            "                return value",
            "            if (",
            "                isinstance(value, (float, int))",
            "                and target_generic_type == utils.GenericDataType.TEMPORAL",
            "                and target_native_type is not None",
            "                and db_engine_spec is not None",
            "            ):",
            "                value = db_engine_spec.convert_dttm(",
            "                    target_type=target_native_type,",
            "                    dttm=datetime.utcfromtimestamp(value / 1000),",
            "                    db_extra=db_extra,",
            "                )",
            "                value = literal_column(value)",
            "            if isinstance(value, str):",
            "                value = value.strip(\"\\t\\n\")",
            "",
            "                if (",
            "                    target_generic_type == utils.GenericDataType.NUMERIC",
            "                    and operator",
            "                    not in {",
            "                        utils.FilterOperator.ILIKE,",
            "                        utils.FilterOperator.LIKE,",
            "                    }",
            "                ):",
            "                    # For backwards compatibility and edge cases",
            "                    # where a column data type might have changed",
            "                    return utils.cast_to_num(value)",
            "                if value == NULL_STRING:",
            "                    return None",
            "                if value == EMPTY_STRING:",
            "                    return \"\"",
            "            if target_generic_type == utils.GenericDataType.BOOLEAN:",
            "                return utils.cast_to_boolean(value)",
            "            return value",
            "",
            "        if isinstance(values, (list, tuple)):",
            "            values = [handle_single_value(v) for v in values]  # type: ignore",
            "        else:",
            "            values = handle_single_value(values)",
            "        if is_list_target and not isinstance(values, (tuple, list)):",
            "            values = [values]  # type: ignore",
            "        elif not is_list_target and isinstance(values, (tuple, list)):",
            "            values = values[0] if values else None",
            "        return values",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        \"\"\"Returns column information from the external system\"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        \"\"\"Returns a query as a string",
            "",
            "        This is used to be displayed to the user so that they can",
            "        understand what is taking place behind the scene\"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        \"\"\"Executes the query and returns a dataframe",
            "",
            "        query_obj is a dictionary representing Superset's query interface.",
            "        Should return a ``superset.models.helpers.QueryResult``",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry",
            "",
            "    def get_column(self, column_name: str | None) -> TableColumn | None:",
            "        if not column_name:",
            "            return None",
            "        for col in self.columns:",
            "            if col.column_name == column_name:",
            "                return col",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_fk_many_from_list(",
            "        object_list: list[Any],",
            "        fkmany: list[Column],",
            "        fkmany_class: builtins.type[TableColumn | SqlMetric],",
            "        key_attr: str,",
            "    ) -> list[Column]:",
            "        \"\"\"Update ORM one-to-many list from object list",
            "",
            "        Used for syncing metrics and columns using the same code\"\"\"",
            "",
            "        object_dict = {o.get(key_attr): o for o in object_list}",
            "",
            "        # delete fks that have been removed",
            "        fkmany = [o for o in fkmany if getattr(o, key_attr) in object_dict]",
            "",
            "        # sync existing fks",
            "        for fk in fkmany:",
            "            obj = object_dict.get(getattr(fk, key_attr))",
            "            if obj:",
            "                for attr in fkmany_class.update_from_object_fields:",
            "                    setattr(fk, attr, obj.get(attr))",
            "",
            "        # create new fks",
            "        new_fks = []",
            "        orm_keys = [getattr(o, key_attr) for o in fkmany]",
            "        for obj in object_list:",
            "            key = obj.get(key_attr)",
            "            if key not in orm_keys:",
            "                del obj[\"id\"]",
            "                orm_kwargs = {}",
            "                for k in obj:",
            "                    if k in fkmany_class.update_from_object_fields and k in obj:",
            "                        orm_kwargs[k] = obj[k]",
            "                new_obj = fkmany_class(**orm_kwargs)",
            "                new_fks.append(new_obj)",
            "        fkmany += new_fks",
            "        return fkmany",
            "",
            "    def update_from_object(self, obj: dict[str, Any]) -> None:",
            "        \"\"\"Update datasource from a data structure",
            "",
            "        The UI's table editor crafts a complex data structure that",
            "        contains most of the datasource's properties as well as",
            "        an array of metrics and columns objects. This method",
            "        receives the object from the UI and syncs the datasource to",
            "        match it. Since the fields are different for the different",
            "        connectors, the implementation uses ``update_from_object_fields``",
            "        which can be defined for each connector and",
            "        defines which fields should be synced\"\"\"",
            "        for attr in self.update_from_object_fields:",
            "            setattr(self, attr, obj.get(attr))",
            "",
            "        self.owners = obj.get(\"owners\", [])",
            "",
            "        # Syncing metrics",
            "        metrics = (",
            "            self.get_fk_many_from_list(",
            "                obj[\"metrics\"], self.metrics, SqlMetric, \"metric_name\"",
            "            )",
            "            if \"metrics\" in obj",
            "            else []",
            "        )",
            "        self.metrics = metrics",
            "",
            "        # Syncing columns",
            "        self.columns = (",
            "            self.get_fk_many_from_list(",
            "                obj[\"columns\"], self.columns, TableColumn, \"column_name\"",
            "            )",
            "            if \"columns\" in obj",
            "            else []",
            "        )",
            "",
            "    def get_extra_cache_keys(",
            "        self, query_obj: QueryObjectDict  # pylint: disable=unused-argument",
            "    ) -> list[Hashable]:",
            "        \"\"\"If a datasource needs to provide additional keys for calculation of",
            "        cache keys, those can be provided via this method",
            "",
            "        :param query_obj: The dict representation of a query object",
            "        :return: list of keys",
            "        \"\"\"",
            "        return []",
            "",
            "    def __hash__(self) -> int:",
            "        return hash(self.uid)",
            "",
            "    def __eq__(self, other: object) -> bool:",
            "        if not isinstance(other, BaseDatasource):",
            "            return NotImplemented",
            "        return self.uid == other.uid",
            "",
            "    def raise_for_access(self) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        security_manager.raise_for_access(datasource=self)",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls, datasource_name: str, schema: str, database_name: str",
            "    ) -> BaseDatasource | None:",
            "        raise NotImplementedError()",
            "",
            "",
            "class AnnotationDatasource(BaseDatasource):",
            "    \"\"\"Dummy object so we can query annotations using 'Viz' objects just like",
            "    regular datasources.",
            "    \"\"\"",
            "",
            "    cache_timeout = 0",
            "    changed_on = None",
            "    type = \"annotation\"",
            "    column_names = [",
            "        \"created_on\",",
            "        \"changed_on\",",
            "        \"id\",",
            "        \"start_dttm\",",
            "        \"end_dttm\",",
            "        \"layer_id\",",
            "        \"short_descr\",",
            "        \"long_descr\",",
            "        \"json_metadata\",",
            "        \"created_by_fk\",",
            "        \"changed_by_fk\",",
            "    ]",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        error_message = None",
            "        qry = db.session.query(Annotation)",
            "        qry = qry.filter(Annotation.layer_id == query_obj[\"filter\"][0][\"val\"])",
            "        if query_obj[\"from_dttm\"]:",
            "            qry = qry.filter(Annotation.start_dttm >= query_obj[\"from_dttm\"])",
            "        if query_obj[\"to_dttm\"]:",
            "            qry = qry.filter(Annotation.end_dttm <= query_obj[\"to_dttm\"])",
            "        status = QueryStatus.SUCCESS",
            "        try:",
            "            df = pd.read_sql_query(qry.statement, db.engine)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.exception(ex)",
            "            error_message = utils.error_msg_from_exception(ex)",
            "        return QueryResult(",
            "            status=status,",
            "            df=df,",
            "            duration=timedelta(0),",
            "            query=\"\",",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        raise NotImplementedError()",
            "",
            "    def values_for_column(self, column_name: str, limit: int = 10000) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "",
            "class TableColumn(AuditMixinNullable, ImportExportMixin, CertificationMixin, Model):",
            "",
            "    \"\"\"ORM object for table columns, each table can have multiple columns\"\"\"",
            "",
            "    __tablename__ = \"table_columns\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"column_name\"),)",
            "",
            "    id = Column(Integer, primary_key=True)",
            "    column_name = Column(String(255), nullable=False)",
            "    verbose_name = Column(String(1024))",
            "    is_active = Column(Boolean, default=True)",
            "    type = Column(Text)",
            "    advanced_data_type = Column(String(255))",
            "    groupby = Column(Boolean, default=True)",
            "    filterable = Column(Boolean, default=True)",
            "    description = Column(MediumText())",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    is_dttm = Column(Boolean, default=False)",
            "    expression = Column(MediumText())",
            "    python_date_format = Column(String(255))",
            "    extra = Column(Text)",
            "",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"columns\",",
            "    )",
            "",
            "    export_fields = [",
            "        \"table_id\",",
            "        \"column_name\",",
            "        \"verbose_name\",",
            "        \"is_dttm\",",
            "        \"is_active\",",
            "        \"type\",",
            "        \"advanced_data_type\",",
            "        \"groupby\",",
            "        \"filterable\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"python_date_format\",",
            "        \"extra\",",
            "    ]",
            "",
            "    update_from_object_fields = [s for s in export_fields if s not in (\"table_id\",)]",
            "    export_parent = \"table\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object.",
            "",
            "        Historically a TableColumn object (from an ORM perspective) was tightly bound to",
            "        a SqlaTable object, however with the introduction of the Query datasource this",
            "        is no longer true, i.e., the SqlaTable relationship is optional.",
            "",
            "        Now the TableColumn is either directly associated with the Database object (",
            "        which is unknown to the ORM) or indirectly via the SqlaTable object (courtesy of",
            "        the ORM) depending on the context.",
            "        \"\"\"",
            "",
            "        self._database: Database | None = kwargs.pop(\"database\", None)",
            "        super().__init__(**kwargs)",
            "",
            "    @reconstructor",
            "    def init_on_load(self) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object when invoked via the SQLAlchemy ORM.",
            "        \"\"\"",
            "",
            "        self._database = None",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.column_name)",
            "",
            "    @property",
            "    def is_boolean(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a boolean datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.BOOLEAN",
            "",
            "    @property",
            "    def is_numeric(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a numeric datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.NUMERIC",
            "",
            "    @property",
            "    def is_string(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a string datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.STRING",
            "",
            "    @property",
            "    def is_temporal(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a temporal datatype. If column has been set as",
            "        temporal/non-temporal (`is_dttm` is True or False respectively), return that",
            "        value. This usually happens during initial metadata fetching or when a column",
            "        is manually set as temporal (for this `python_date_format` needs to be set).",
            "        \"\"\"",
            "        if self.is_dttm is not None:",
            "            return self.is_dttm",
            "        return self.type_generic == GenericDataType.TEMPORAL",
            "",
            "    @property",
            "    def database(self) -> Database:",
            "        return self.table.database if self.table else self._database",
            "",
            "    @property",
            "    def db_engine_spec(self) -> builtins.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @property",
            "    def type_generic(self) -> utils.GenericDataType | None:",
            "        if self.is_dttm:",
            "            return GenericDataType.TEMPORAL",
            "",
            "        return (",
            "            column_spec.generic_type",
            "            if (",
            "                column_spec := self.db_engine_spec.get_column_spec(",
            "                    self.type,",
            "                    db_extra=self.db_extra,",
            "                )",
            "            )",
            "            else None",
            "        )",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        col = self.database.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    @property",
            "    def datasource(self) -> RelationshipProperty:",
            "        return self.table",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        time_grain: str | None,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TimestampExpression | Label:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "",
            "        pdf = self.python_date_format",
            "        is_epoch = pdf in (\"epoch_s\", \"epoch_ms\")",
            "        column_spec = self.db_engine_spec.get_column_spec(",
            "            self.type, db_extra=self.db_extra",
            "        )",
            "        type_ = column_spec.sqla_type if column_spec else DateTime",
            "        if not self.expression and not time_grain and not is_epoch:",
            "            sqla_col = column(self.column_name, type_=type_)",
            "            return self.database.make_sqla_column_compatible(sqla_col, label)",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, pdf, time_grain)",
            "        return self.database.make_sqla_column_compatible(time_expr, label)",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"advanced_data_type\",",
            "            \"certification_details\",",
            "            \"certified_by\",",
            "            \"column_name\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"filterable\",",
            "            \"groupby\",",
            "            \"id\",",
            "            \"is_certified\",",
            "            \"is_dttm\",",
            "            \"python_date_format\",",
            "            \"type\",",
            "            \"type_generic\",",
            "            \"verbose_name\",",
            "            \"warning_markdown\",",
            "        )",
            "",
            "        return {s: getattr(self, s) for s in attrs if hasattr(self, s)}",
            "",
            "",
            "class SqlMetric(AuditMixinNullable, ImportExportMixin, CertificationMixin, Model):",
            "",
            "    \"\"\"ORM object for metrics, each table can have multiple metrics\"\"\"",
            "",
            "    __tablename__ = \"sql_metrics\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"metric_name\"),)",
            "",
            "    id = Column(Integer, primary_key=True)",
            "    metric_name = Column(String(255), nullable=False)",
            "    verbose_name = Column(String(1024))",
            "    metric_type = Column(String(32))",
            "    description = Column(MediumText())",
            "    d3format = Column(String(128))",
            "    currency = Column(String(128))",
            "    warning_text = Column(Text)",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    expression = Column(MediumText(), nullable=False)",
            "    extra = Column(Text)",
            "",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"metrics\",",
            "    )",
            "",
            "    export_fields = [",
            "        \"metric_name\",",
            "        \"verbose_name\",",
            "        \"metric_type\",",
            "        \"table_id\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"d3format\",",
            "        \"currency\",",
            "        \"extra\",",
            "        \"warning_text\",",
            "    ]",
            "    update_from_object_fields = list(s for s in export_fields if s != \"table_id\")",
            "    export_parent = \"table\"",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.metric_name)",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.metric_name",
            "        expression = self.expression",
            "        if template_processor:",
            "            expression = template_processor.process_template(expression)",
            "",
            "        sqla_col: ColumnClause = literal_column(expression)",
            "        return self.table.database.make_sqla_column_compatible(sqla_col, label)",
            "",
            "    @property",
            "    def perm(self) -> str | None:",
            "        return (",
            "            (\"{parent_name}.[{obj.metric_name}](id:{obj.id})\").format(",
            "                obj=self, parent_name=self.table.full_name",
            "            )",
            "            if self.table",
            "            else None",
            "        )",
            "",
            "    def get_perm(self) -> str | None:",
            "        return self.perm",
            "",
            "    @property",
            "    def currency_json(self) -> dict[str, str | None] | None:",
            "        try:",
            "            return json.loads(self.currency or \"{}\") or None",
            "        except (TypeError, JSONDecodeError) as exc:",
            "            logger.error(",
            "                \"Unable to load currency json: %r. Leaving empty.\", exc, exc_info=True",
            "            )",
            "            return None",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"certification_details\",",
            "            \"certified_by\",",
            "            \"currency\",",
            "            \"d3format\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"id\",",
            "            \"is_certified\",",
            "            \"metric_name\",",
            "            \"warning_markdown\",",
            "            \"warning_text\",",
            "            \"verbose_name\",",
            "        )",
            "",
            "        return {s: getattr(self, s) for s in attrs}",
            "",
            "",
            "sqlatable_user = Table(",
            "    \"sqlatable_user\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"user_id\", Integer, ForeignKey(\"ab_user.id\", ondelete=\"CASCADE\")),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\")),",
            ")",
            "",
            "",
            "def _process_sql_expression(",
            "    expression: str | None,",
            "    database_id: int,",
            "    schema: str,",
            "    template_processor: BaseTemplateProcessor | None = None,",
            ") -> str | None:",
            "    if template_processor and expression:",
            "        expression = template_processor.process_template(expression)",
            "    if expression:",
            "        try:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            expression = sanitize_clause(expression)",
            "        except (QueryClauseValidationException, SupersetSecurityException) as ex:",
            "            raise QueryObjectValidationError(ex.message) from ex",
            "    return expression",
            "",
            "",
            "class SqlaTable(",
            "    Model, BaseDatasource, ExploreMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"An ORM object for SqlAlchemy table references\"\"\"",
            "",
            "    type = \"table\"",
            "    query_language = \"sql\"",
            "    is_rls_supported = True",
            "    columns: Mapped[list[TableColumn]] = relationship(",
            "        TableColumn,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metrics: Mapped[list[SqlMetric]] = relationship(",
            "        SqlMetric,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metric_class = SqlMetric",
            "    column_class = TableColumn",
            "    owner_class = security_manager.user_model",
            "",
            "    __tablename__ = \"tables\"",
            "",
            "    # Note this uniqueness constraint is not part of the physical schema, i.e., it does",
            "    # not exist in the migrations, but is required by `import_from_dict` to ensure the",
            "    # correct filters are applied in order to identify uniqueness.",
            "    #",
            "    # The reason it does not physically exist is MySQL, PostgreSQL, etc. have a",
            "    # different interpretation of uniqueness when it comes to NULL which is problematic",
            "    # given the schema is optional.",
            "    __table_args__ = (UniqueConstraint(\"database_id\", \"schema\", \"table_name\"),)",
            "",
            "    table_name = Column(String(250), nullable=False)",
            "    main_dttm_col = Column(String(250))",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=False)",
            "    fetch_values_predicate = Column(Text)",
            "    owners = relationship(owner_class, secondary=sqlatable_user, backref=\"tables\")",
            "    database: Database = relationship(",
            "        \"Database\",",
            "        backref=backref(\"tables\", cascade=\"all, delete-orphan\"),",
            "        foreign_keys=[database_id],",
            "    )",
            "    schema = Column(String(255))",
            "    sql = Column(MediumText())",
            "    is_sqllab_view = Column(Boolean, default=False)",
            "    template_params = Column(Text)",
            "    extra = Column(Text)",
            "    normalize_columns = Column(Boolean, default=False)",
            "    always_filter_main_dttm = Column(Boolean, default=False)",
            "",
            "    baselink = \"tablemodelview\"",
            "",
            "    export_fields = [",
            "        \"table_name\",",
            "        \"main_dttm_col\",",
            "        \"description\",",
            "        \"default_endpoint\",",
            "        \"database_id\",",
            "        \"offset\",",
            "        \"cache_timeout\",",
            "        \"schema\",",
            "        \"sql\",",
            "        \"params\",",
            "        \"template_params\",",
            "        \"filter_select_enabled\",",
            "        \"fetch_values_predicate\",",
            "        \"extra\",",
            "        \"normalize_columns\",",
            "        \"always_filter_main_dttm\",",
            "    ]",
            "    update_from_object_fields = [f for f in export_fields if f != \"database_id\"]",
            "    export_parent = \"database\"",
            "    export_children = [\"metrics\", \"columns\"]",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "",
            "    def __repr__(self) -> str:  # pylint: disable=invalid-repr-returned",
            "        return self.name",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: str | None) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    @property",
            "    def db_engine_spec(self) -> __builtins__.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if not self.changed_by:",
            "            return \"\"",
            "        return str(self.changed_by)",
            "",
            "    @property",
            "    def connection(self) -> str:",
            "        return str(self.database)",
            "",
            "    @property",
            "    def description_markeddown(self) -> str:",
            "        return utils.markdown(self.description)",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        return self.table_name",
            "",
            "    @property",
            "    def datasource_type(self) -> str:",
            "        return self.type",
            "",
            "    @property",
            "    def database_name(self) -> str:",
            "        return self.database.name",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls,",
            "        datasource_name: str,",
            "        schema: str | None,",
            "        database_name: str,",
            "    ) -> SqlaTable | None:",
            "        schema = schema or None",
            "        query = (",
            "            db.session.query(cls)",
            "            .join(Database)",
            "            .filter(cls.table_name == datasource_name)",
            "            .filter(Database.database_name == database_name)",
            "        )",
            "        # Handling schema being '' or None, which is easier to handle",
            "        # in python than in the SQLA query in a multi-dialect way",
            "        for tbl in query.all():",
            "            if schema == (tbl.schema or None):",
            "                return tbl",
            "        return None",
            "",
            "    @property",
            "    def link(self) -> Markup:",
            "        name = escape(self.name)",
            "        anchor = f'<a target=\"_blank\" href=\"{self.explore_url}\">{name}</a>'",
            "        return Markup(anchor)",
            "",
            "    def get_schema_perm(self) -> str | None:",
            "        \"\"\"Returns schema permission if present, database one otherwise.\"\"\"",
            "        return security_manager.get_schema_perm(self.database, self.schema)",
            "",
            "    def get_perm(self) -> str:",
            "        \"\"\"",
            "        Return this dataset permission name",
            "        :return: dataset permission name",
            "        :raises DatasetInvalidPermissionEvaluationException: When database is missing",
            "        \"\"\"",
            "        if self.database is None:",
            "            raise DatasetInvalidPermissionEvaluationException()",
            "        return f\"[{self.database}].[{self.table_name}](id:{self.id})\"",
            "",
            "    @hybrid_property",
            "    def name(self) -> str:  # pylint: disable=invalid-overridden-method",
            "        return self.schema + \".\" + self.table_name if self.schema else self.table_name",
            "",
            "    @property",
            "    def full_name(self) -> str:",
            "        return utils.get_datasource_full_name(",
            "            self.database, self.table_name, schema=self.schema",
            "        )",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        l = [c.column_name for c in self.columns if c.is_dttm]",
            "        if self.main_dttm_col and self.main_dttm_col not in l:",
            "            l.append(self.main_dttm_col)",
            "        return l",
            "",
            "    @property",
            "    def num_cols(self) -> list[str]:",
            "        return [c.column_name for c in self.columns if c.is_numeric]",
            "",
            "    @property",
            "    def any_dttm_col(self) -> str | None:",
            "        cols = self.dttm_cols",
            "        return cols[0] if cols else None",
            "",
            "    @property",
            "    def html(self) -> str:",
            "        df = pd.DataFrame((c.column_name, c.type) for c in self.columns)",
            "        df.columns = [\"field\", \"type\"]",
            "        return df.to_html(",
            "            index=False,",
            "            classes=(\"dataframe table table-striped table-bordered \" \"table-condensed\"),",
            "        )",
            "",
            "    @property",
            "    def sql_url(self) -> str:",
            "        return self.database.sql_url + \"?table_name=\" + str(self.table_name)",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        # todo(yongjie): create a physical table column type in a separate PR",
            "        if self.sql:",
            "            return get_virtual_table_metadata(dataset=self)",
            "        return get_physical_table_metadata(",
            "            database=self.database,",
            "            table_name=self.table_name,",
            "            schema_name=self.schema,",
            "            normalize_columns=self.normalize_columns,",
            "        )",
            "",
            "    @property",
            "    def time_column_grains(self) -> dict[str, Any]:",
            "        return {",
            "            \"time_columns\": self.dttm_cols,",
            "            \"time_grains\": [grain.name for grain in self.database.grains()],",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        # show_cols and latest_partition set to false to avoid",
            "        # the expensive cost of inspecting the DB",
            "        return self.database.select_star(",
            "            self.table_name, schema=self.schema, show_cols=False, latest_partition=False",
            "        )",
            "",
            "    @property",
            "    def health_check_message(self) -> str | None:",
            "        check = config[\"DATASET_HEALTH_CHECK\"]",
            "        return check(self) if check else None",
            "",
            "    @property",
            "    def granularity_sqla(self) -> list[tuple[Any, Any]]:",
            "        return utils.choicify(self.dttm_cols)",
            "",
            "    @property",
            "    def time_grain_sqla(self) -> list[tuple[Any, Any]]:",
            "        return [(g.duration, g.name) for g in self.database.grains() or []]",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        data_ = super().data",
            "        if self.type == \"table\":",
            "            data_[\"granularity_sqla\"] = self.granularity_sqla",
            "            data_[\"time_grain_sqla\"] = self.time_grain_sqla",
            "            data_[\"main_dttm_col\"] = self.main_dttm_col",
            "            data_[\"fetch_values_predicate\"] = self.fetch_values_predicate",
            "            data_[\"template_params\"] = self.template_params",
            "            data_[\"is_sqllab_view\"] = self.is_sqllab_view",
            "            data_[\"health_check_message\"] = self.health_check_message",
            "            data_[\"extra\"] = self.extra",
            "            data_[\"owners\"] = self.owners_data",
            "            data_[\"always_filter_main_dttm\"] = self.always_filter_main_dttm",
            "            data_[\"normalize_columns\"] = self.normalize_columns",
            "        return data_",
            "",
            "    @property",
            "    def extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TextClause:",
            "        fetch_values_predicate = self.fetch_values_predicate",
            "        if template_processor:",
            "            fetch_values_predicate = template_processor.process_template(",
            "                fetch_values_predicate",
            "            )",
            "        try:",
            "            return self.text(fetch_values_predicate)",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in fetch values predicate: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        sql_query_mutator = config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        return get_template_processor(table=self, database=self.database, **kwargs)",
            "",
            "    def get_query_str_extended(",
            "        self,",
            "        query_obj: QueryObjectDict,",
            "        mutate: bool = True,",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        sql = sqlparse.format(sql, reindent=True)",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def get_sqla_table(self) -> TableClause:",
            "        tbl = table(self.table_name)",
            "        if self.schema:",
            "            tbl.schema = self.schema",
            "        return tbl",
            "",
            "    def get_from_clause(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> tuple[TableClause | Alias, str | None]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "        if not self.is_virtual:",
            "            return self.get_sqla_table(), None",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql, engine=self.db_engine_spec.engine)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def get_rendered_sql(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        if len(sqlparse.split(sql)) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "        return sql",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            table_column: TableColumn | None = columns_by_name.get(column_name)",
            "            if table_column:",
            "                sqla_column = table_column.get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "            else:",
            "                sqla_column = column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = _process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    def adhoc_column_to_sqla(  # pylint: disable=too-many-locals",
            "        self,",
            "        col: AdhocColumn,",
            "        force_type_check: bool = False,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc column into a sqlalchemy column.",
            "",
            "        :param col: Adhoc column definition",
            "        :param force_type_check: Should the column type be checked in the db.",
            "               This is needed to validate if a filter with an adhoc column",
            "               is applicable.",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        label = utils.get_column_name(col)",
            "        expression = _process_sql_expression(",
            "            expression=col[\"sqlExpression\"],",
            "            database_id=self.database_id,",
            "            schema=self.schema,",
            "            template_processor=template_processor,",
            "        )",
            "        time_grain = col.get(\"timeGrain\")",
            "        has_timegrain = col.get(\"columnType\") == \"BASE_AXIS\" and time_grain",
            "        is_dttm = False",
            "        pdf = None",
            "        if col_in_metadata := self.get_column(expression):",
            "            sqla_column = col_in_metadata.get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "            is_dttm = col_in_metadata.is_temporal",
            "            pdf = col_in_metadata.python_date_format",
            "        else:",
            "            sqla_column = literal_column(expression)",
            "            if has_timegrain or force_type_check:",
            "                try:",
            "                    # probe adhoc column type",
            "                    tbl, _ = self.get_from_clause(template_processor)",
            "                    qry = sa.select([sqla_column]).limit(1).select_from(tbl)",
            "                    sql = self.database.compile_sqla_query(qry)",
            "                    col_desc = get_columns_description(self.database, self.schema, sql)",
            "                    if not col_desc:",
            "                        raise SupersetGenericDBErrorException(\"Column not found\")",
            "                    is_dttm = col_desc[0][\"is_dttm\"]  # type: ignore",
            "                except SupersetGenericDBErrorException as ex:",
            "                    raise ColumnNotFoundException(message=str(ex)) from ex",
            "",
            "        if is_dttm and has_timegrain:",
            "            sqla_column = self.db_engine_spec.get_timestamp_expr(",
            "                col=sqla_column,",
            "                pdf=pdf,",
            "                time_grain=time_grain,",
            "            )",
            "        return self.make_sqla_column_compatible(sqla_column, label)",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[int | str, list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, SqlMetric],",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> str | int | float | bool | Text:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if column_.type and column_.is_temporal and isinstance(value, str):",
            "            sql = self.db_engine_spec.convert_dttm(",
            "                column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "            )",
            "",
            "            if sql:",
            "                value = self.text(sql)",
            "",
            "        return value",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> pd.DataFrame | None:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_sqla_table_object(self) -> Table:",
            "        return self.database.get_table(self.table_name, schema=self.schema)",
            "",
            "    def fetch_metadata(self, commit: bool = True) -> MetadataResult:",
            "        \"\"\"",
            "        Fetches the metadata for the table and merges it in",
            "",
            "        :param commit: should the changes be committed or not.",
            "        :return: Tuple with lists of added, removed and modified column names.",
            "        \"\"\"",
            "        new_columns = self.external_metadata()",
            "        metrics = [",
            "            SqlMetric(**metric)",
            "            for metric in self.database.get_metrics(self.table_name, self.schema)",
            "        ]",
            "        any_date_col = None",
            "        db_engine_spec = self.db_engine_spec",
            "",
            "        # If no `self.id`, then this is a new table, no need to fetch columns",
            "        # from db.  Passing in `self.id` to query will actually automatically",
            "        # generate a new id, which can be tricky during certain transactions.",
            "        old_columns = (",
            "            (",
            "                db.session.query(TableColumn)",
            "                .filter(TableColumn.table_id == self.id)",
            "                .all()",
            "            )",
            "            if self.id",
            "            else self.columns",
            "        )",
            "",
            "        old_columns_by_name: dict[str, TableColumn] = {",
            "            col.column_name: col for col in old_columns",
            "        }",
            "        results = MetadataResult(",
            "            removed=[",
            "                col",
            "                for col in old_columns_by_name",
            "                if col not in {col[\"column_name\"] for col in new_columns}",
            "            ]",
            "        )",
            "",
            "        # clear old columns before adding modified columns back",
            "        columns = []",
            "        for col in new_columns:",
            "            old_column = old_columns_by_name.pop(col[\"column_name\"], None)",
            "            if not old_column:",
            "                results.added.append(col[\"column_name\"])",
            "                new_column = TableColumn(",
            "                    column_name=col[\"column_name\"],",
            "                    type=col[\"type\"],",
            "                    table=self,",
            "                )",
            "                new_column.is_dttm = new_column.is_temporal",
            "                db_engine_spec.alter_new_orm_column(new_column)",
            "            else:",
            "                new_column = old_column",
            "                if new_column.type != col[\"type\"]:",
            "                    results.modified.append(col[\"column_name\"])",
            "                new_column.type = col[\"type\"]",
            "                new_column.expression = \"\"",
            "            new_column.groupby = True",
            "            new_column.filterable = True",
            "            columns.append(new_column)",
            "            if not any_date_col and new_column.is_temporal:",
            "                any_date_col = col[\"column_name\"]",
            "",
            "        # add back calculated (virtual) columns",
            "        columns.extend([col for col in old_columns if col.expression])",
            "        self.columns = columns",
            "",
            "        if not self.main_dttm_col:",
            "            self.main_dttm_col = any_date_col",
            "        self.add_missing_metrics(metrics)",
            "",
            "        # Apply config supplied mutations.",
            "        config[\"SQLA_TABLE_MUTATOR\"](self)",
            "",
            "        db.session.merge(self)",
            "        if commit:",
            "            db.session.commit()",
            "        return results",
            "",
            "    @classmethod",
            "    def query_datasources_by_name(",
            "        cls,",
            "        database: Database,",
            "        datasource_name: str,",
            "        schema: str | None = None,",
            "    ) -> list[SqlaTable]:",
            "        query = (",
            "            db.session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter_by(table_name=datasource_name)",
            "        )",
            "        if schema:",
            "            query = query.filter_by(schema=schema)",
            "        return query.all()",
            "",
            "    @classmethod",
            "    def query_datasources_by_permissions(  # pylint: disable=invalid-name",
            "        cls,",
            "        database: Database,",
            "        permissions: set[str],",
            "        schema_perms: set[str],",
            "    ) -> list[SqlaTable]:",
            "        # TODO(hughhhh): add unit test",
            "        return (",
            "            db.session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter(",
            "                or_(",
            "                    SqlaTable.perm.in_(permissions),",
            "                    SqlaTable.schema_perm.in_(schema_perms),",
            "                )",
            "            )",
            "            .all()",
            "        )",
            "",
            "    @classmethod",
            "    def get_eager_sqlatable_datasource(cls, datasource_id: int) -> SqlaTable:",
            "        \"\"\"Returns SqlaTable with columns and metrics.\"\"\"",
            "        return (",
            "            db.session.query(cls)",
            "            .options(",
            "                sa.orm.subqueryload(cls.columns),",
            "                sa.orm.subqueryload(cls.metrics),",
            "            )",
            "            .filter_by(id=datasource_id)",
            "            .one()",
            "        )",
            "",
            "    @classmethod",
            "    def get_all_datasources(cls) -> list[SqlaTable]:",
            "        qry = db.session.query(cls)",
            "        qry = cls.default_query(qry)",
            "        return qry.all()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry.filter_by(is_sqllab_view=False)",
            "",
            "    def has_extra_cache_key_calls(self, query_obj: QueryObjectDict) -> bool:",
            "        \"\"\"",
            "        Detects the presence of calls to `ExtraCache` methods in items in query_obj that",
            "        can be templated. If any are present, the query must be evaluated to extract",
            "        additional keys for the cache key. This method is needed to avoid executing the",
            "        template code unnecessarily, as it may contain expensive calls, e.g. to extract",
            "        the latest partition of a database.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: True if there are call(s) to an `ExtraCache` method, False otherwise",
            "        \"\"\"",
            "        templatable_statements: list[str] = []",
            "        if self.sql:",
            "            templatable_statements.append(self.sql)",
            "        if self.fetch_values_predicate:",
            "            templatable_statements.append(self.fetch_values_predicate)",
            "        extras = query_obj.get(\"extras\", {})",
            "        if \"where\" in extras:",
            "            templatable_statements.append(extras[\"where\"])",
            "        if \"having\" in extras:",
            "            templatable_statements.append(extras[\"having\"])",
            "        if self.is_rls_supported:",
            "            templatable_statements += [",
            "                f.clause for f in security_manager.get_rls_filters(self)",
            "            ]",
            "        for statement in templatable_statements:",
            "            if ExtraCache.regex.search(statement):",
            "                return True",
            "        return False",
            "",
            "    def get_extra_cache_keys(self, query_obj: QueryObjectDict) -> list[Hashable]:",
            "        \"\"\"",
            "        The cache key of a SqlaTable needs to consider any keys added by the parent",
            "        class and any keys added via `ExtraCache`.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: The extra cache keys",
            "        \"\"\"",
            "        extra_cache_keys = super().get_extra_cache_keys(query_obj)",
            "        if self.has_extra_cache_key_calls(query_obj):",
            "            sqla_query = self.get_sqla_query(**query_obj)",
            "            extra_cache_keys += sqla_query.extra_cache_keys",
            "        return extra_cache_keys",
            "",
            "    @property",
            "    def quote_identifier(self) -> Callable[[str], str]:",
            "        return self.database.quote_identifier",
            "",
            "    @staticmethod",
            "    def before_update(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Note this listener is called when any fields are being updated",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        :raises Exception: If the target table is not unique",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_before_update(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def update_column(  # pylint: disable=unused-argument",
            "        mapper: Mapper, connection: Connection, target: SqlMetric | TableColumn",
            "    ) -> None:",
            "        \"\"\"",
            "        :param mapper: Unused.",
            "        :param connection: Unused.",
            "        :param target: The metric or column that was updated.",
            "        \"\"\"",
            "        session = inspect(target).session  # pylint: disable=disallowed-name",
            "",
            "        # Forces an update to the table's changed_on value when a metric or column on the",
            "        # table is updated. This busts the cache key for all charts that use the table.",
            "        session.execute(update(SqlaTable).where(SqlaTable.id == target.table.id))",
            "",
            "    @staticmethod",
            "    def after_insert(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after insert",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_after_insert(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def after_delete(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        sqla_table: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after delete",
            "        \"\"\"",
            "        security_manager.dataset_after_delete(mapper, connection, sqla_table)",
            "",
            "    def load_database(self: SqlaTable) -> None:",
            "        # somehow the database attribute is not loaded on access",
            "        if self.database_id and (",
            "            not self.database or self.database.id != self.database_id",
            "        ):",
            "            session = inspect(self).session  # pylint: disable=disallowed-name",
            "            self.database = session.query(Database).filter_by(id=self.database_id).one()",
            "",
            "",
            "sa.event.listen(SqlaTable, \"before_update\", SqlaTable.before_update)",
            "sa.event.listen(SqlaTable, \"after_insert\", SqlaTable.after_insert)",
            "sa.event.listen(SqlaTable, \"after_delete\", SqlaTable.after_delete)",
            "sa.event.listen(SqlMetric, \"after_update\", SqlaTable.update_column)",
            "sa.event.listen(TableColumn, \"after_update\", SqlaTable.update_column)",
            "",
            "RLSFilterRoles = Table(",
            "    \"rls_filter_roles\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"role_id\", Integer, ForeignKey(\"ab_role.id\"), nullable=False),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "RLSFilterTables = Table(",
            "    \"rls_filter_tables\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\")),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "",
            "class RowLevelSecurityFilter(Model, AuditMixinNullable):",
            "    \"\"\"",
            "    Custom where clauses attached to Tables and Roles.",
            "    \"\"\"",
            "",
            "    __tablename__ = \"row_level_security_filters\"",
            "    id = Column(Integer, primary_key=True)",
            "    name = Column(String(255), unique=True, nullable=False)",
            "    description = Column(Text)",
            "    filter_type = Column(",
            "        Enum(*[filter_type.value for filter_type in utils.RowLevelSecurityFilterType])",
            "    )",
            "    group_key = Column(String(255), nullable=True)",
            "    roles = relationship(",
            "        security_manager.role_model,",
            "        secondary=RLSFilterRoles,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    tables = relationship(",
            "        SqlaTable,",
            "        overlaps=\"table\",",
            "        secondary=RLSFilterTables,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    clause = Column(MediumText(), nullable=False)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import builtins",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from dataclasses import dataclass, field",
            "from datetime import datetime, timedelta",
            "from json.decoder import JSONDecodeError",
            "from typing import Any, Callable, cast",
            "",
            "import dateutil.parser",
            "import numpy as np",
            "import pandas as pd",
            "import sqlalchemy as sa",
            "from flask import escape, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import (",
            "    and_,",
            "    Boolean,",
            "    Column,",
            "    DateTime,",
            "    Enum,",
            "    ForeignKey,",
            "    inspect,",
            "    Integer,",
            "    or_,",
            "    String,",
            "    Table,",
            "    Text,",
            "    update,",
            ")",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.ext.declarative import declared_attr",
            "from sqlalchemy.ext.hybrid import hybrid_property",
            "from sqlalchemy.orm import (",
            "    backref,",
            "    foreign,",
            "    Mapped,",
            "    Query,",
            "    reconstructor,",
            "    relationship,",
            "    RelationshipProperty,",
            ")",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.schema import UniqueConstraint",
            "from sqlalchemy.sql import column, ColumnElement, literal_column, table",
            "from sqlalchemy.sql.elements import ColumnClause, TextClause",
            "from sqlalchemy.sql.expression import Label, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "",
            "from superset import app, db, is_feature_enabled, security_manager",
            "from superset.commands.dataset.exceptions import DatasetNotFoundError",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.connectors.sqla.utils import (",
            "    get_columns_description,",
            "    get_physical_table_metadata,",
            "    get_virtual_table_metadata,",
            ")",
            "from superset.constants import EMPTY_STRING, NULL_STRING",
            "from superset.db_engine_specs.base import BaseEngineSpec, TimestampExpression",
            "from superset.exceptions import (",
            "    ColumnNotFoundException,",
            "    DatasetInvalidPermissionEvaluationException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetGenericDBErrorException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.jinja_context import (",
            "    BaseTemplateProcessor,",
            "    ExtraCache,",
            "    get_template_processor,",
            ")",
            "from superset.models.annotations import Annotation",
            "from superset.models.core import Database",
            "from superset.models.helpers import (",
            "    AuditMixinNullable,",
            "    CertificationMixin,",
            "    ExploreMixin,",
            "    ImportExportMixin,",
            "    QueryResult,",
            "    validate_adhoc_subquery,",
            ")",
            "from superset.models.slice import Slice",
            "from superset.sql_parse import ParsedQuery, sanitize_clause",
            "from superset.superset_typing import (",
            "    AdhocColumn,",
            "    AdhocMetric,",
            "    FilterValue,",
            "    FilterValues,",
            "    Metric,",
            "    QueryObjectDict,",
            "    ResultSetColumnType,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.backports import StrEnum",
            "from superset.utils.core import GenericDataType, MediumText",
            "",
            "config = app.config",
            "metadata = Model.metadata  # pylint: disable=no-member",
            "logger = logging.getLogger(__name__)",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "",
            "# a non-exhaustive set of additive metrics",
            "ADDITIVE_METRIC_TYPES = {",
            "    \"count\",",
            "    \"sum\",",
            "    \"doubleSum\",",
            "}",
            "ADDITIVE_METRIC_TYPES_LOWER = {op.lower() for op in ADDITIVE_METRIC_TYPES}",
            "",
            "",
            "@dataclass",
            "class MetadataResult:",
            "    added: list[str] = field(default_factory=list)",
            "    removed: list[str] = field(default_factory=list)",
            "    modified: list[str] = field(default_factory=list)",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "METRIC_FORM_DATA_PARAMS = [",
            "    \"metric\",",
            "    \"metric_2\",",
            "    \"metrics\",",
            "    \"metrics_b\",",
            "    \"percent_metrics\",",
            "    \"secondary_metric\",",
            "    \"size\",",
            "    \"timeseries_limit_metric\",",
            "    \"x\",",
            "    \"y\",",
            "]",
            "",
            "COLUMN_FORM_DATA_PARAMS = [",
            "    \"all_columns\",",
            "    \"all_columns_x\",",
            "    \"columns\",",
            "    \"entity\",",
            "    \"groupby\",",
            "    \"order_by_cols\",",
            "    \"series\",",
            "]",
            "",
            "",
            "class DatasourceKind(StrEnum):",
            "    VIRTUAL = \"virtual\"",
            "    PHYSICAL = \"physical\"",
            "",
            "",
            "class BaseDatasource(",
            "    AuditMixinNullable, ImportExportMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"A common interface to objects that are queryable",
            "    (tables and datasources)\"\"\"",
            "",
            "    # ---------------------------------------------------------------",
            "    # class attributes to define when deriving BaseDatasource",
            "    # ---------------------------------------------------------------",
            "    __tablename__: str | None = None  # {connector_name}_datasource",
            "    baselink: str | None = None  # url portion pointing to ModelView endpoint",
            "",
            "    owner_class: User | None = None",
            "",
            "    # Used to do code highlighting when displaying the query in the UI",
            "    query_language: str | None = None",
            "",
            "    # Only some datasources support Row Level Security",
            "    is_rls_supported: bool = False",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        # can be a Column or a property pointing to one",
            "        raise NotImplementedError()",
            "",
            "    # ---------------------------------------------------------------",
            "",
            "    # Columns",
            "    id = Column(Integer, primary_key=True)",
            "    description = Column(Text)",
            "    default_endpoint = Column(Text)",
            "    is_featured = Column(Boolean, default=False)  # TODO deprecating",
            "    filter_select_enabled = Column(Boolean, default=True)",
            "    offset = Column(Integer, default=0)",
            "    cache_timeout = Column(Integer)",
            "    params = Column(String(1000))",
            "    perm = Column(String(1000))",
            "    schema_perm = Column(String(1000))",
            "    is_managed_externally = Column(Boolean, nullable=False, default=False)",
            "    external_url = Column(Text, nullable=True)",
            "",
            "    sql: str | None = None",
            "    owners: list[User]",
            "    update_from_object_fields: list[str]",
            "",
            "    extra_import_fields = [\"is_managed_externally\", \"external_url\"]",
            "",
            "    @property",
            "    def kind(self) -> DatasourceKind:",
            "        return DatasourceKind.VIRTUAL if self.sql else DatasourceKind.PHYSICAL",
            "",
            "    @property",
            "    def owners_data(self) -> list[dict[str, Any]]:",
            "        return [",
            "            {",
            "                \"first_name\": o.first_name,",
            "                \"last_name\": o.last_name,",
            "                \"username\": o.username,",
            "                \"id\": o.id,",
            "            }",
            "            for o in self.owners",
            "        ]",
            "",
            "    @property",
            "    def is_virtual(self) -> bool:",
            "        return self.kind == DatasourceKind.VIRTUAL",
            "",
            "    @declared_attr",
            "    def slices(self) -> RelationshipProperty:",
            "        return relationship(",
            "            \"Slice\",",
            "            overlaps=\"table\",",
            "            primaryjoin=lambda: and_(",
            "                foreign(Slice.datasource_id) == self.id,",
            "                foreign(Slice.datasource_type) == self.type,",
            "            ),",
            "        )",
            "",
            "    columns: list[TableColumn] = []",
            "    metrics: list[SqlMetric] = []",
            "",
            "    @property",
            "    def type(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        \"\"\"Unique id across datasource types\"\"\"",
            "        return f\"{self.id}__{self.type}\"",
            "",
            "    @property",
            "    def column_names(self) -> list[str]:",
            "        return sorted([c.column_name for c in self.columns], key=lambda x: x or \"\")",
            "",
            "    @property",
            "    def columns_types(self) -> dict[str, str]:",
            "        return {c.column_name: c.type for c in self.columns}",
            "",
            "    @property",
            "    def main_dttm_col(self) -> str:",
            "        return \"timestamp\"",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def connection(self) -> str | None:",
            "        \"\"\"String representing the context of the Datasource\"\"\"",
            "        return None",
            "",
            "    @property",
            "    def schema(self) -> str | None:",
            "        \"\"\"String representing the schema of the Datasource (if it applies)\"\"\"",
            "        return None",
            "",
            "    @property",
            "    def filterable_column_names(self) -> list[str]:",
            "        return sorted([c.column_name for c in self.columns if c.filterable])",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        return []",
            "",
            "    @property",
            "    def url(self) -> str:",
            "        return f\"/{self.baselink}/edit/{self.id}\"",
            "",
            "    @property",
            "    def explore_url(self) -> str:",
            "        if self.default_endpoint:",
            "            return self.default_endpoint",
            "        return f\"/explore/?datasource_type={self.type}&datasource_id={self.id}\"",
            "",
            "    @property",
            "    def column_formats(self) -> dict[str, str | None]:",
            "        return {m.metric_name: m.d3format for m in self.metrics if m.d3format}",
            "",
            "    @property",
            "    def currency_formats(self) -> dict[str, dict[str, str | None] | None]:",
            "        return {m.metric_name: m.currency_json for m in self.metrics if m.currency_json}",
            "",
            "    def add_missing_metrics(self, metrics: list[SqlMetric]) -> None:",
            "        existing_metrics = {m.metric_name for m in self.metrics}",
            "        for metric in metrics:",
            "            if metric.metric_name not in existing_metrics:",
            "                metric.table_id = self.id",
            "                self.metrics.append(metric)",
            "",
            "    @property",
            "    def short_data(self) -> dict[str, Any]:",
            "        \"\"\"Data representation of the datasource sent to the frontend\"\"\"",
            "        return {",
            "            \"edit_url\": self.url,",
            "            \"id\": self.id,",
            "            \"uid\": self.uid,",
            "            \"schema\": self.schema,",
            "            \"name\": self.name,",
            "            \"type\": self.type,",
            "            \"connection\": self.connection,",
            "            \"creator\": str(self.created_by),",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        pass",
            "",
            "    @property",
            "    def order_by_choices(self) -> list[tuple[str, str]]:",
            "        choices = []",
            "        # self.column_names return sorted column_names",
            "        for column_name in self.column_names:",
            "            column_name = str(column_name or \"\")",
            "            choices.append(",
            "                (json.dumps([column_name, True]), f\"{column_name} \" + __(\"[asc]\"))",
            "            )",
            "            choices.append(",
            "                (json.dumps([column_name, False]), f\"{column_name} \" + __(\"[desc]\"))",
            "            )",
            "        return choices",
            "",
            "    @property",
            "    def verbose_map(self) -> dict[str, str]:",
            "        verb_map = {\"__timestamp\": \"Time\"}",
            "        verb_map.update(",
            "            {o.metric_name: o.verbose_name or o.metric_name for o in self.metrics}",
            "        )",
            "        verb_map.update(",
            "            {o.column_name: o.verbose_name or o.column_name for o in self.columns}",
            "        )",
            "        return verb_map",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        \"\"\"Data representation of the datasource sent to the frontend\"\"\"",
            "        return {",
            "            # simple fields",
            "            \"id\": self.id,",
            "            \"uid\": self.uid,",
            "            \"column_formats\": self.column_formats,",
            "            \"currency_formats\": self.currency_formats,",
            "            \"description\": self.description,",
            "            \"database\": self.database.data,  # pylint: disable=no-member",
            "            \"default_endpoint\": self.default_endpoint,",
            "            \"filter_select\": self.filter_select_enabled,  # TODO deprecate",
            "            \"filter_select_enabled\": self.filter_select_enabled,",
            "            \"name\": self.name,",
            "            \"datasource_name\": self.datasource_name,",
            "            \"table_name\": self.datasource_name,",
            "            \"type\": self.type,",
            "            \"schema\": self.schema,",
            "            \"offset\": self.offset,",
            "            \"cache_timeout\": self.cache_timeout,",
            "            \"params\": self.params,",
            "            \"perm\": self.perm,",
            "            \"edit_url\": self.url,",
            "            # sqla-specific",
            "            \"sql\": self.sql,",
            "            # one to many",
            "            \"columns\": [o.data for o in self.columns],",
            "            \"metrics\": [o.data for o in self.metrics],",
            "            # TODO deprecate, move logic to JS",
            "            \"order_by_choices\": self.order_by_choices,",
            "            \"owners\": [owner.id for owner in self.owners],",
            "            \"verbose_map\": self.verbose_map,",
            "            \"select_star\": self.select_star,",
            "        }",
            "",
            "    def data_for_slices(  # pylint: disable=too-many-locals",
            "        self, slices: list[Slice]",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        The representation of the datasource containing only the required data",
            "        to render the provided slices.",
            "",
            "        Used to reduce the payload when loading a dashboard.",
            "        \"\"\"",
            "        data = self.data",
            "        metric_names = set()",
            "        column_names = set()",
            "        for slc in slices:",
            "            form_data = slc.form_data",
            "            # pull out all required metrics from the form_data",
            "            for metric_param in METRIC_FORM_DATA_PARAMS:",
            "                for metric in utils.as_list(form_data.get(metric_param) or []):",
            "                    metric_names.add(utils.get_metric_name(metric))",
            "                    if utils.is_adhoc_metric(metric):",
            "                        column_ = metric.get(\"column\") or {}",
            "                        if column_name := column_.get(\"column_name\"):",
            "                            column_names.add(column_name)",
            "",
            "            # Columns used in query filters",
            "            column_names.update(",
            "                filter_[\"subject\"]",
            "                for filter_ in form_data.get(\"adhoc_filters\") or []",
            "                if filter_.get(\"clause\") == \"WHERE\" and filter_.get(\"subject\")",
            "            )",
            "",
            "            # columns used by Filter Box",
            "            column_names.update(",
            "                filter_config[\"column\"]",
            "                for filter_config in form_data.get(\"filter_configs\") or []",
            "                if \"column\" in filter_config",
            "            )",
            "",
            "            # for legacy dashboard imports which have the wrong query_context in them",
            "            try:",
            "                query_context = slc.get_query_context()",
            "            except DatasetNotFoundError:",
            "                query_context = None",
            "",
            "            # legacy charts don't have query_context charts",
            "            if query_context:",
            "                column_names.update(",
            "                    [",
            "                        utils.get_column_name(column_)",
            "                        for query in query_context.queries",
            "                        for column_ in query.columns",
            "                    ]",
            "                    or []",
            "                )",
            "            else:",
            "                _columns = [",
            "                    utils.get_column_name(column_)",
            "                    if utils.is_adhoc_column(column_)",
            "                    else column_",
            "                    for column_param in COLUMN_FORM_DATA_PARAMS",
            "                    for column_ in utils.as_list(form_data.get(column_param) or [])",
            "                ]",
            "                column_names.update(_columns)",
            "",
            "        filtered_metrics = [",
            "            metric",
            "            for metric in data[\"metrics\"]",
            "            if metric[\"metric_name\"] in metric_names",
            "        ]",
            "",
            "        filtered_columns: list[Column] = []",
            "        column_types: set[GenericDataType] = set()",
            "        for column_ in data[\"columns\"]:",
            "            generic_type = column_.get(\"type_generic\")",
            "            if generic_type is not None:",
            "                column_types.add(generic_type)",
            "            if column_[\"column_name\"] in column_names:",
            "                filtered_columns.append(column_)",
            "",
            "        data[\"column_types\"] = list(column_types)",
            "        del data[\"description\"]",
            "        data.update({\"metrics\": filtered_metrics})",
            "        data.update({\"columns\": filtered_columns})",
            "        verbose_map = {\"__timestamp\": \"Time\"}",
            "        verbose_map.update(",
            "            {",
            "                metric[\"metric_name\"]: metric[\"verbose_name\"] or metric[\"metric_name\"]",
            "                for metric in filtered_metrics",
            "            }",
            "        )",
            "        verbose_map.update(",
            "            {",
            "                column_[\"column_name\"]: column_[\"verbose_name\"]",
            "                or column_[\"column_name\"]",
            "                for column_ in filtered_columns",
            "            }",
            "        )",
            "        data[\"verbose_map\"] = verbose_map",
            "",
            "        return data",
            "",
            "    @staticmethod",
            "    def filter_values_handler(  # pylint: disable=too-many-arguments",
            "        values: FilterValues | None,",
            "        operator: str,",
            "        target_generic_type: GenericDataType,",
            "        target_native_type: str | None = None,",
            "        is_list_target: bool = False,",
            "        db_engine_spec: builtins.type[BaseEngineSpec] | None = None,",
            "        db_extra: dict[str, Any] | None = None,",
            "    ) -> FilterValues | None:",
            "        if values is None:",
            "            return None",
            "",
            "        def handle_single_value(value: FilterValue | None) -> FilterValue | None:",
            "            if operator == utils.FilterOperator.TEMPORAL_RANGE:",
            "                return value",
            "            if (",
            "                isinstance(value, (float, int))",
            "                and target_generic_type == utils.GenericDataType.TEMPORAL",
            "                and target_native_type is not None",
            "                and db_engine_spec is not None",
            "            ):",
            "                value = db_engine_spec.convert_dttm(",
            "                    target_type=target_native_type,",
            "                    dttm=datetime.utcfromtimestamp(value / 1000),",
            "                    db_extra=db_extra,",
            "                )",
            "                value = literal_column(value)",
            "            if isinstance(value, str):",
            "                value = value.strip(\"\\t\\n\")",
            "",
            "                if (",
            "                    target_generic_type == utils.GenericDataType.NUMERIC",
            "                    and operator",
            "                    not in {",
            "                        utils.FilterOperator.ILIKE,",
            "                        utils.FilterOperator.LIKE,",
            "                    }",
            "                ):",
            "                    # For backwards compatibility and edge cases",
            "                    # where a column data type might have changed",
            "                    return utils.cast_to_num(value)",
            "                if value == NULL_STRING:",
            "                    return None",
            "                if value == EMPTY_STRING:",
            "                    return \"\"",
            "            if target_generic_type == utils.GenericDataType.BOOLEAN:",
            "                return utils.cast_to_boolean(value)",
            "            return value",
            "",
            "        if isinstance(values, (list, tuple)):",
            "            values = [handle_single_value(v) for v in values]  # type: ignore",
            "        else:",
            "            values = handle_single_value(values)",
            "        if is_list_target and not isinstance(values, (tuple, list)):",
            "            values = [values]  # type: ignore",
            "        elif not is_list_target and isinstance(values, (tuple, list)):",
            "            values = values[0] if values else None",
            "        return values",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        \"\"\"Returns column information from the external system\"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        \"\"\"Returns a query as a string",
            "",
            "        This is used to be displayed to the user so that they can",
            "        understand what is taking place behind the scene\"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        \"\"\"Executes the query and returns a dataframe",
            "",
            "        query_obj is a dictionary representing Superset's query interface.",
            "        Should return a ``superset.models.helpers.QueryResult``",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry",
            "",
            "    def get_column(self, column_name: str | None) -> TableColumn | None:",
            "        if not column_name:",
            "            return None",
            "        for col in self.columns:",
            "            if col.column_name == column_name:",
            "                return col",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_fk_many_from_list(",
            "        object_list: list[Any],",
            "        fkmany: list[Column],",
            "        fkmany_class: builtins.type[TableColumn | SqlMetric],",
            "        key_attr: str,",
            "    ) -> list[Column]:",
            "        \"\"\"Update ORM one-to-many list from object list",
            "",
            "        Used for syncing metrics and columns using the same code\"\"\"",
            "",
            "        object_dict = {o.get(key_attr): o for o in object_list}",
            "",
            "        # delete fks that have been removed",
            "        fkmany = [o for o in fkmany if getattr(o, key_attr) in object_dict]",
            "",
            "        # sync existing fks",
            "        for fk in fkmany:",
            "            obj = object_dict.get(getattr(fk, key_attr))",
            "            if obj:",
            "                for attr in fkmany_class.update_from_object_fields:",
            "                    setattr(fk, attr, obj.get(attr))",
            "",
            "        # create new fks",
            "        new_fks = []",
            "        orm_keys = [getattr(o, key_attr) for o in fkmany]",
            "        for obj in object_list:",
            "            key = obj.get(key_attr)",
            "            if key not in orm_keys:",
            "                del obj[\"id\"]",
            "                orm_kwargs = {}",
            "                for k in obj:",
            "                    if k in fkmany_class.update_from_object_fields and k in obj:",
            "                        orm_kwargs[k] = obj[k]",
            "                new_obj = fkmany_class(**orm_kwargs)",
            "                new_fks.append(new_obj)",
            "        fkmany += new_fks",
            "        return fkmany",
            "",
            "    def update_from_object(self, obj: dict[str, Any]) -> None:",
            "        \"\"\"Update datasource from a data structure",
            "",
            "        The UI's table editor crafts a complex data structure that",
            "        contains most of the datasource's properties as well as",
            "        an array of metrics and columns objects. This method",
            "        receives the object from the UI and syncs the datasource to",
            "        match it. Since the fields are different for the different",
            "        connectors, the implementation uses ``update_from_object_fields``",
            "        which can be defined for each connector and",
            "        defines which fields should be synced\"\"\"",
            "        for attr in self.update_from_object_fields:",
            "            setattr(self, attr, obj.get(attr))",
            "",
            "        self.owners = obj.get(\"owners\", [])",
            "",
            "        # Syncing metrics",
            "        metrics = (",
            "            self.get_fk_many_from_list(",
            "                obj[\"metrics\"], self.metrics, SqlMetric, \"metric_name\"",
            "            )",
            "            if \"metrics\" in obj",
            "            else []",
            "        )",
            "        self.metrics = metrics",
            "",
            "        # Syncing columns",
            "        self.columns = (",
            "            self.get_fk_many_from_list(",
            "                obj[\"columns\"], self.columns, TableColumn, \"column_name\"",
            "            )",
            "            if \"columns\" in obj",
            "            else []",
            "        )",
            "",
            "    def get_extra_cache_keys(",
            "        self, query_obj: QueryObjectDict  # pylint: disable=unused-argument",
            "    ) -> list[Hashable]:",
            "        \"\"\"If a datasource needs to provide additional keys for calculation of",
            "        cache keys, those can be provided via this method",
            "",
            "        :param query_obj: The dict representation of a query object",
            "        :return: list of keys",
            "        \"\"\"",
            "        return []",
            "",
            "    def __hash__(self) -> int:",
            "        return hash(self.uid)",
            "",
            "    def __eq__(self, other: object) -> bool:",
            "        if not isinstance(other, BaseDatasource):",
            "            return NotImplemented",
            "        return self.uid == other.uid",
            "",
            "    def raise_for_access(self) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        security_manager.raise_for_access(datasource=self)",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls, datasource_name: str, schema: str, database_name: str",
            "    ) -> BaseDatasource | None:",
            "        raise NotImplementedError()",
            "",
            "",
            "class AnnotationDatasource(BaseDatasource):",
            "    \"\"\"Dummy object so we can query annotations using 'Viz' objects just like",
            "    regular datasources.",
            "    \"\"\"",
            "",
            "    cache_timeout = 0",
            "    changed_on = None",
            "    type = \"annotation\"",
            "    column_names = [",
            "        \"created_on\",",
            "        \"changed_on\",",
            "        \"id\",",
            "        \"start_dttm\",",
            "        \"end_dttm\",",
            "        \"layer_id\",",
            "        \"short_descr\",",
            "        \"long_descr\",",
            "        \"json_metadata\",",
            "        \"created_by_fk\",",
            "        \"changed_by_fk\",",
            "    ]",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        error_message = None",
            "        qry = db.session.query(Annotation)",
            "        qry = qry.filter(Annotation.layer_id == query_obj[\"filter\"][0][\"val\"])",
            "        if query_obj[\"from_dttm\"]:",
            "            qry = qry.filter(Annotation.start_dttm >= query_obj[\"from_dttm\"])",
            "        if query_obj[\"to_dttm\"]:",
            "            qry = qry.filter(Annotation.end_dttm <= query_obj[\"to_dttm\"])",
            "        status = QueryStatus.SUCCESS",
            "        try:",
            "            df = pd.read_sql_query(qry.statement, db.engine)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.exception(ex)",
            "            error_message = utils.error_msg_from_exception(ex)",
            "        return QueryResult(",
            "            status=status,",
            "            df=df,",
            "            duration=timedelta(0),",
            "            query=\"\",",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        raise NotImplementedError()",
            "",
            "    def values_for_column(self, column_name: str, limit: int = 10000) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "",
            "class TableColumn(AuditMixinNullable, ImportExportMixin, CertificationMixin, Model):",
            "",
            "    \"\"\"ORM object for table columns, each table can have multiple columns\"\"\"",
            "",
            "    __tablename__ = \"table_columns\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"column_name\"),)",
            "",
            "    id = Column(Integer, primary_key=True)",
            "    column_name = Column(String(255), nullable=False)",
            "    verbose_name = Column(String(1024))",
            "    is_active = Column(Boolean, default=True)",
            "    type = Column(Text)",
            "    advanced_data_type = Column(String(255))",
            "    groupby = Column(Boolean, default=True)",
            "    filterable = Column(Boolean, default=True)",
            "    description = Column(MediumText())",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    is_dttm = Column(Boolean, default=False)",
            "    expression = Column(MediumText())",
            "    python_date_format = Column(String(255))",
            "    extra = Column(Text)",
            "",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"columns\",",
            "    )",
            "",
            "    export_fields = [",
            "        \"table_id\",",
            "        \"column_name\",",
            "        \"verbose_name\",",
            "        \"is_dttm\",",
            "        \"is_active\",",
            "        \"type\",",
            "        \"advanced_data_type\",",
            "        \"groupby\",",
            "        \"filterable\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"python_date_format\",",
            "        \"extra\",",
            "    ]",
            "",
            "    update_from_object_fields = [s for s in export_fields if s not in (\"table_id\",)]",
            "    export_parent = \"table\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object.",
            "",
            "        Historically a TableColumn object (from an ORM perspective) was tightly bound to",
            "        a SqlaTable object, however with the introduction of the Query datasource this",
            "        is no longer true, i.e., the SqlaTable relationship is optional.",
            "",
            "        Now the TableColumn is either directly associated with the Database object (",
            "        which is unknown to the ORM) or indirectly via the SqlaTable object (courtesy of",
            "        the ORM) depending on the context.",
            "        \"\"\"",
            "",
            "        self._database: Database | None = kwargs.pop(\"database\", None)",
            "        super().__init__(**kwargs)",
            "",
            "    @reconstructor",
            "    def init_on_load(self) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object when invoked via the SQLAlchemy ORM.",
            "        \"\"\"",
            "",
            "        self._database = None",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.column_name)",
            "",
            "    @property",
            "    def is_boolean(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a boolean datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.BOOLEAN",
            "",
            "    @property",
            "    def is_numeric(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a numeric datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.NUMERIC",
            "",
            "    @property",
            "    def is_string(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a string datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.STRING",
            "",
            "    @property",
            "    def is_temporal(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a temporal datatype. If column has been set as",
            "        temporal/non-temporal (`is_dttm` is True or False respectively), return that",
            "        value. This usually happens during initial metadata fetching or when a column",
            "        is manually set as temporal (for this `python_date_format` needs to be set).",
            "        \"\"\"",
            "        if self.is_dttm is not None:",
            "            return self.is_dttm",
            "        return self.type_generic == GenericDataType.TEMPORAL",
            "",
            "    @property",
            "    def database(self) -> Database:",
            "        return self.table.database if self.table else self._database",
            "",
            "    @property",
            "    def db_engine_spec(self) -> builtins.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @property",
            "    def type_generic(self) -> utils.GenericDataType | None:",
            "        if self.is_dttm:",
            "            return GenericDataType.TEMPORAL",
            "",
            "        return (",
            "            column_spec.generic_type",
            "            if (",
            "                column_spec := self.db_engine_spec.get_column_spec(",
            "                    self.type,",
            "                    db_extra=self.db_extra,",
            "                )",
            "            )",
            "            else None",
            "        )",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        col = self.database.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    @property",
            "    def datasource(self) -> RelationshipProperty:",
            "        return self.table",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        time_grain: str | None,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TimestampExpression | Label:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "",
            "        pdf = self.python_date_format",
            "        is_epoch = pdf in (\"epoch_s\", \"epoch_ms\")",
            "        column_spec = self.db_engine_spec.get_column_spec(",
            "            self.type, db_extra=self.db_extra",
            "        )",
            "        type_ = column_spec.sqla_type if column_spec else DateTime",
            "        if not self.expression and not time_grain and not is_epoch:",
            "            sqla_col = column(self.column_name, type_=type_)",
            "            return self.database.make_sqla_column_compatible(sqla_col, label)",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, pdf, time_grain)",
            "        return self.database.make_sqla_column_compatible(time_expr, label)",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"advanced_data_type\",",
            "            \"certification_details\",",
            "            \"certified_by\",",
            "            \"column_name\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"filterable\",",
            "            \"groupby\",",
            "            \"id\",",
            "            \"is_certified\",",
            "            \"is_dttm\",",
            "            \"python_date_format\",",
            "            \"type\",",
            "            \"type_generic\",",
            "            \"verbose_name\",",
            "            \"warning_markdown\",",
            "        )",
            "",
            "        return {s: getattr(self, s) for s in attrs if hasattr(self, s)}",
            "",
            "",
            "class SqlMetric(AuditMixinNullable, ImportExportMixin, CertificationMixin, Model):",
            "",
            "    \"\"\"ORM object for metrics, each table can have multiple metrics\"\"\"",
            "",
            "    __tablename__ = \"sql_metrics\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"metric_name\"),)",
            "",
            "    id = Column(Integer, primary_key=True)",
            "    metric_name = Column(String(255), nullable=False)",
            "    verbose_name = Column(String(1024))",
            "    metric_type = Column(String(32))",
            "    description = Column(MediumText())",
            "    d3format = Column(String(128))",
            "    currency = Column(String(128))",
            "    warning_text = Column(Text)",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    expression = Column(MediumText(), nullable=False)",
            "    extra = Column(Text)",
            "",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"metrics\",",
            "    )",
            "",
            "    export_fields = [",
            "        \"metric_name\",",
            "        \"verbose_name\",",
            "        \"metric_type\",",
            "        \"table_id\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"d3format\",",
            "        \"currency\",",
            "        \"extra\",",
            "        \"warning_text\",",
            "    ]",
            "    update_from_object_fields = list(s for s in export_fields if s != \"table_id\")",
            "    export_parent = \"table\"",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.metric_name)",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.metric_name",
            "        expression = self.expression",
            "        if template_processor:",
            "            expression = template_processor.process_template(expression)",
            "",
            "        sqla_col: ColumnClause = literal_column(expression)",
            "        return self.table.database.make_sqla_column_compatible(sqla_col, label)",
            "",
            "    @property",
            "    def perm(self) -> str | None:",
            "        return (",
            "            (\"{parent_name}.[{obj.metric_name}](id:{obj.id})\").format(",
            "                obj=self, parent_name=self.table.full_name",
            "            )",
            "            if self.table",
            "            else None",
            "        )",
            "",
            "    def get_perm(self) -> str | None:",
            "        return self.perm",
            "",
            "    @property",
            "    def currency_json(self) -> dict[str, str | None] | None:",
            "        try:",
            "            return json.loads(self.currency or \"{}\") or None",
            "        except (TypeError, JSONDecodeError) as exc:",
            "            logger.error(",
            "                \"Unable to load currency json: %r. Leaving empty.\", exc, exc_info=True",
            "            )",
            "            return None",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"certification_details\",",
            "            \"certified_by\",",
            "            \"currency\",",
            "            \"d3format\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"id\",",
            "            \"is_certified\",",
            "            \"metric_name\",",
            "            \"warning_markdown\",",
            "            \"warning_text\",",
            "            \"verbose_name\",",
            "        )",
            "",
            "        return {s: getattr(self, s) for s in attrs}",
            "",
            "",
            "sqlatable_user = Table(",
            "    \"sqlatable_user\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"user_id\", Integer, ForeignKey(\"ab_user.id\", ondelete=\"CASCADE\")),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\")),",
            ")",
            "",
            "",
            "def _process_sql_expression(",
            "    expression: str | None,",
            "    database_id: int,",
            "    schema: str,",
            "    template_processor: BaseTemplateProcessor | None = None,",
            ") -> str | None:",
            "    if template_processor and expression:",
            "        expression = template_processor.process_template(expression)",
            "    if expression:",
            "        try:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            expression = sanitize_clause(expression)",
            "        except (QueryClauseValidationException, SupersetSecurityException) as ex:",
            "            raise QueryObjectValidationError(ex.message) from ex",
            "    return expression",
            "",
            "",
            "class SqlaTable(",
            "    Model,",
            "    BaseDatasource,",
            "    ExploreMixin,",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"An ORM object for SqlAlchemy table references\"\"\"",
            "",
            "    type = \"table\"",
            "    query_language = \"sql\"",
            "    is_rls_supported = True",
            "    columns: Mapped[list[TableColumn]] = relationship(",
            "        TableColumn,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metrics: Mapped[list[SqlMetric]] = relationship(",
            "        SqlMetric,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metric_class = SqlMetric",
            "    column_class = TableColumn",
            "    owner_class = security_manager.user_model",
            "",
            "    __tablename__ = \"tables\"",
            "",
            "    # Note this uniqueness constraint is not part of the physical schema, i.e., it does",
            "    # not exist in the migrations, but is required by `import_from_dict` to ensure the",
            "    # correct filters are applied in order to identify uniqueness.",
            "    #",
            "    # The reason it does not physically exist is MySQL, PostgreSQL, etc. have a",
            "    # different interpretation of uniqueness when it comes to NULL which is problematic",
            "    # given the schema is optional.",
            "    __table_args__ = (UniqueConstraint(\"database_id\", \"schema\", \"table_name\"),)",
            "",
            "    table_name = Column(String(250), nullable=False)",
            "    main_dttm_col = Column(String(250))",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=False)",
            "    fetch_values_predicate = Column(Text)",
            "    owners = relationship(owner_class, secondary=sqlatable_user, backref=\"tables\")",
            "    database: Database = relationship(",
            "        \"Database\",",
            "        backref=backref(\"tables\", cascade=\"all, delete-orphan\"),",
            "        foreign_keys=[database_id],",
            "    )",
            "    schema = Column(String(255))",
            "    sql = Column(MediumText())",
            "    is_sqllab_view = Column(Boolean, default=False)",
            "    template_params = Column(Text)",
            "    extra = Column(Text)",
            "    normalize_columns = Column(Boolean, default=False)",
            "    always_filter_main_dttm = Column(Boolean, default=False)",
            "",
            "    baselink = \"tablemodelview\"",
            "",
            "    export_fields = [",
            "        \"table_name\",",
            "        \"main_dttm_col\",",
            "        \"description\",",
            "        \"default_endpoint\",",
            "        \"database_id\",",
            "        \"offset\",",
            "        \"cache_timeout\",",
            "        \"schema\",",
            "        \"sql\",",
            "        \"params\",",
            "        \"template_params\",",
            "        \"filter_select_enabled\",",
            "        \"fetch_values_predicate\",",
            "        \"extra\",",
            "        \"normalize_columns\",",
            "        \"always_filter_main_dttm\",",
            "    ]",
            "    update_from_object_fields = [f for f in export_fields if f != \"database_id\"]",
            "    export_parent = \"database\"",
            "    export_children = [\"metrics\", \"columns\"]",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "",
            "    def __repr__(self) -> str:  # pylint: disable=invalid-repr-returned",
            "        return self.name",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: str | None) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    @property",
            "    def db_engine_spec(self) -> __builtins__.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if not self.changed_by:",
            "            return \"\"",
            "        return str(self.changed_by)",
            "",
            "    @property",
            "    def connection(self) -> str:",
            "        return str(self.database)",
            "",
            "    @property",
            "    def description_markeddown(self) -> str:",
            "        return utils.markdown(self.description)",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        return self.table_name",
            "",
            "    @property",
            "    def datasource_type(self) -> str:",
            "        return self.type",
            "",
            "    @property",
            "    def database_name(self) -> str:",
            "        return self.database.name",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls,",
            "        datasource_name: str,",
            "        schema: str | None,",
            "        database_name: str,",
            "    ) -> SqlaTable | None:",
            "        schema = schema or None",
            "        query = (",
            "            db.session.query(cls)",
            "            .join(Database)",
            "            .filter(cls.table_name == datasource_name)",
            "            .filter(Database.database_name == database_name)",
            "        )",
            "        # Handling schema being '' or None, which is easier to handle",
            "        # in python than in the SQLA query in a multi-dialect way",
            "        for tbl in query.all():",
            "            if schema == (tbl.schema or None):",
            "                return tbl",
            "        return None",
            "",
            "    @property",
            "    def link(self) -> Markup:",
            "        name = escape(self.name)",
            "        anchor = f'<a target=\"_blank\" href=\"{self.explore_url}\">{name}</a>'",
            "        return Markup(anchor)",
            "",
            "    def get_schema_perm(self) -> str | None:",
            "        \"\"\"Returns schema permission if present, database one otherwise.\"\"\"",
            "        return security_manager.get_schema_perm(self.database, self.schema)",
            "",
            "    def get_perm(self) -> str:",
            "        \"\"\"",
            "        Return this dataset permission name",
            "        :return: dataset permission name",
            "        :raises DatasetInvalidPermissionEvaluationException: When database is missing",
            "        \"\"\"",
            "        if self.database is None:",
            "            raise DatasetInvalidPermissionEvaluationException()",
            "        return f\"[{self.database}].[{self.table_name}](id:{self.id})\"",
            "",
            "    @hybrid_property",
            "    def name(self) -> str:  # pylint: disable=invalid-overridden-method",
            "        return self.schema + \".\" + self.table_name if self.schema else self.table_name",
            "",
            "    @property",
            "    def full_name(self) -> str:",
            "        return utils.get_datasource_full_name(",
            "            self.database, self.table_name, schema=self.schema",
            "        )",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        l = [c.column_name for c in self.columns if c.is_dttm]",
            "        if self.main_dttm_col and self.main_dttm_col not in l:",
            "            l.append(self.main_dttm_col)",
            "        return l",
            "",
            "    @property",
            "    def num_cols(self) -> list[str]:",
            "        return [c.column_name for c in self.columns if c.is_numeric]",
            "",
            "    @property",
            "    def any_dttm_col(self) -> str | None:",
            "        cols = self.dttm_cols",
            "        return cols[0] if cols else None",
            "",
            "    @property",
            "    def html(self) -> str:",
            "        df = pd.DataFrame((c.column_name, c.type) for c in self.columns)",
            "        df.columns = [\"field\", \"type\"]",
            "        return df.to_html(",
            "            index=False,",
            "            classes=(\"dataframe table table-striped table-bordered \" \"table-condensed\"),",
            "        )",
            "",
            "    @property",
            "    def sql_url(self) -> str:",
            "        return self.database.sql_url + \"?table_name=\" + str(self.table_name)",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        # todo(yongjie): create a physical table column type in a separate PR",
            "        if self.sql:",
            "            return get_virtual_table_metadata(dataset=self)",
            "        return get_physical_table_metadata(",
            "            database=self.database,",
            "            table_name=self.table_name,",
            "            schema_name=self.schema,",
            "            normalize_columns=self.normalize_columns,",
            "        )",
            "",
            "    @property",
            "    def time_column_grains(self) -> dict[str, Any]:",
            "        return {",
            "            \"time_columns\": self.dttm_cols,",
            "            \"time_grains\": [grain.name for grain in self.database.grains()],",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        # show_cols and latest_partition set to false to avoid",
            "        # the expensive cost of inspecting the DB",
            "        return self.database.select_star(",
            "            self.table_name, schema=self.schema, show_cols=False, latest_partition=False",
            "        )",
            "",
            "    @property",
            "    def health_check_message(self) -> str | None:",
            "        check = config[\"DATASET_HEALTH_CHECK\"]",
            "        return check(self) if check else None",
            "",
            "    @property",
            "    def granularity_sqla(self) -> list[tuple[Any, Any]]:",
            "        return utils.choicify(self.dttm_cols)",
            "",
            "    @property",
            "    def time_grain_sqla(self) -> list[tuple[Any, Any]]:",
            "        return [(g.duration, g.name) for g in self.database.grains() or []]",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        data_ = super().data",
            "        if self.type == \"table\":",
            "            data_[\"granularity_sqla\"] = self.granularity_sqla",
            "            data_[\"time_grain_sqla\"] = self.time_grain_sqla",
            "            data_[\"main_dttm_col\"] = self.main_dttm_col",
            "            data_[\"fetch_values_predicate\"] = self.fetch_values_predicate",
            "            data_[\"template_params\"] = self.template_params",
            "            data_[\"is_sqllab_view\"] = self.is_sqllab_view",
            "            data_[\"health_check_message\"] = self.health_check_message",
            "            data_[\"extra\"] = self.extra",
            "            data_[\"owners\"] = self.owners_data",
            "            data_[\"always_filter_main_dttm\"] = self.always_filter_main_dttm",
            "            data_[\"normalize_columns\"] = self.normalize_columns",
            "        return data_",
            "",
            "    @property",
            "    def extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TextClause:",
            "        fetch_values_predicate = self.fetch_values_predicate",
            "        if template_processor:",
            "            fetch_values_predicate = template_processor.process_template(",
            "                fetch_values_predicate",
            "            )",
            "        try:",
            "            return self.text(fetch_values_predicate)",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in fetch values predicate: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        sql_query_mutator = config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        return get_template_processor(table=self, database=self.database, **kwargs)",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def get_sqla_table(self) -> TableClause:",
            "        tbl = table(self.table_name)",
            "        if self.schema:",
            "            tbl.schema = self.schema",
            "        return tbl",
            "",
            "    def get_from_clause(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> tuple[TableClause | Alias, str | None]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "        if not self.is_virtual:",
            "            return self.get_sqla_table(), None",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql, engine=self.db_engine_spec.engine)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            table_column: TableColumn | None = columns_by_name.get(column_name)",
            "            if table_column:",
            "                sqla_column = table_column.get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "            else:",
            "                sqla_column = column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = _process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    def adhoc_column_to_sqla(  # pylint: disable=too-many-locals",
            "        self,",
            "        col: AdhocColumn,",
            "        force_type_check: bool = False,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc column into a sqlalchemy column.",
            "",
            "        :param col: Adhoc column definition",
            "        :param force_type_check: Should the column type be checked in the db.",
            "               This is needed to validate if a filter with an adhoc column",
            "               is applicable.",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        label = utils.get_column_name(col)",
            "        expression = _process_sql_expression(",
            "            expression=col[\"sqlExpression\"],",
            "            database_id=self.database_id,",
            "            schema=self.schema,",
            "            template_processor=template_processor,",
            "        )",
            "        time_grain = col.get(\"timeGrain\")",
            "        has_timegrain = col.get(\"columnType\") == \"BASE_AXIS\" and time_grain",
            "        is_dttm = False",
            "        pdf = None",
            "        if col_in_metadata := self.get_column(expression):",
            "            sqla_column = col_in_metadata.get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "            is_dttm = col_in_metadata.is_temporal",
            "            pdf = col_in_metadata.python_date_format",
            "        else:",
            "            sqla_column = literal_column(expression)",
            "            if has_timegrain or force_type_check:",
            "                try:",
            "                    # probe adhoc column type",
            "                    tbl, _ = self.get_from_clause(template_processor)",
            "                    qry = sa.select([sqla_column]).limit(1).select_from(tbl)",
            "                    sql = self.database.compile_sqla_query(qry)",
            "                    col_desc = get_columns_description(self.database, self.schema, sql)",
            "                    if not col_desc:",
            "                        raise SupersetGenericDBErrorException(\"Column not found\")",
            "                    is_dttm = col_desc[0][\"is_dttm\"]  # type: ignore",
            "                except SupersetGenericDBErrorException as ex:",
            "                    raise ColumnNotFoundException(message=str(ex)) from ex",
            "",
            "        if is_dttm and has_timegrain:",
            "            sqla_column = self.db_engine_spec.get_timestamp_expr(",
            "                col=sqla_column,",
            "                pdf=pdf,",
            "                time_grain=time_grain,",
            "            )",
            "        return self.make_sqla_column_compatible(sqla_column, label)",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[int | str, list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, SqlMetric],",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> str | int | float | bool | Text:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if column_.type and column_.is_temporal and isinstance(value, str):",
            "            sql = self.db_engine_spec.convert_dttm(",
            "                column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "            )",
            "",
            "            if sql:",
            "                value = self.text(sql)",
            "",
            "        return value",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> pd.DataFrame | None:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_sqla_table_object(self) -> Table:",
            "        return self.database.get_table(self.table_name, schema=self.schema)",
            "",
            "    def fetch_metadata(self, commit: bool = True) -> MetadataResult:",
            "        \"\"\"",
            "        Fetches the metadata for the table and merges it in",
            "",
            "        :param commit: should the changes be committed or not.",
            "        :return: Tuple with lists of added, removed and modified column names.",
            "        \"\"\"",
            "        new_columns = self.external_metadata()",
            "        metrics = [",
            "            SqlMetric(**metric)",
            "            for metric in self.database.get_metrics(self.table_name, self.schema)",
            "        ]",
            "        any_date_col = None",
            "        db_engine_spec = self.db_engine_spec",
            "",
            "        # If no `self.id`, then this is a new table, no need to fetch columns",
            "        # from db.  Passing in `self.id` to query will actually automatically",
            "        # generate a new id, which can be tricky during certain transactions.",
            "        old_columns = (",
            "            (",
            "                db.session.query(TableColumn)",
            "                .filter(TableColumn.table_id == self.id)",
            "                .all()",
            "            )",
            "            if self.id",
            "            else self.columns",
            "        )",
            "",
            "        old_columns_by_name: dict[str, TableColumn] = {",
            "            col.column_name: col for col in old_columns",
            "        }",
            "        results = MetadataResult(",
            "            removed=[",
            "                col",
            "                for col in old_columns_by_name",
            "                if col not in {col[\"column_name\"] for col in new_columns}",
            "            ]",
            "        )",
            "",
            "        # clear old columns before adding modified columns back",
            "        columns = []",
            "        for col in new_columns:",
            "            old_column = old_columns_by_name.pop(col[\"column_name\"], None)",
            "            if not old_column:",
            "                results.added.append(col[\"column_name\"])",
            "                new_column = TableColumn(",
            "                    column_name=col[\"column_name\"],",
            "                    type=col[\"type\"],",
            "                    table=self,",
            "                )",
            "                new_column.is_dttm = new_column.is_temporal",
            "                db_engine_spec.alter_new_orm_column(new_column)",
            "            else:",
            "                new_column = old_column",
            "                if new_column.type != col[\"type\"]:",
            "                    results.modified.append(col[\"column_name\"])",
            "                new_column.type = col[\"type\"]",
            "                new_column.expression = \"\"",
            "            new_column.groupby = True",
            "            new_column.filterable = True",
            "            columns.append(new_column)",
            "            if not any_date_col and new_column.is_temporal:",
            "                any_date_col = col[\"column_name\"]",
            "",
            "        # add back calculated (virtual) columns",
            "        columns.extend([col for col in old_columns if col.expression])",
            "        self.columns = columns",
            "",
            "        if not self.main_dttm_col:",
            "            self.main_dttm_col = any_date_col",
            "        self.add_missing_metrics(metrics)",
            "",
            "        # Apply config supplied mutations.",
            "        config[\"SQLA_TABLE_MUTATOR\"](self)",
            "",
            "        db.session.merge(self)",
            "        if commit:",
            "            db.session.commit()",
            "        return results",
            "",
            "    @classmethod",
            "    def query_datasources_by_name(",
            "        cls,",
            "        database: Database,",
            "        datasource_name: str,",
            "        schema: str | None = None,",
            "    ) -> list[SqlaTable]:",
            "        query = (",
            "            db.session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter_by(table_name=datasource_name)",
            "        )",
            "        if schema:",
            "            query = query.filter_by(schema=schema)",
            "        return query.all()",
            "",
            "    @classmethod",
            "    def query_datasources_by_permissions(  # pylint: disable=invalid-name",
            "        cls,",
            "        database: Database,",
            "        permissions: set[str],",
            "        schema_perms: set[str],",
            "    ) -> list[SqlaTable]:",
            "        # TODO(hughhhh): add unit test",
            "        return (",
            "            db.session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter(",
            "                or_(",
            "                    SqlaTable.perm.in_(permissions),",
            "                    SqlaTable.schema_perm.in_(schema_perms),",
            "                )",
            "            )",
            "            .all()",
            "        )",
            "",
            "    @classmethod",
            "    def get_eager_sqlatable_datasource(cls, datasource_id: int) -> SqlaTable:",
            "        \"\"\"Returns SqlaTable with columns and metrics.\"\"\"",
            "        return (",
            "            db.session.query(cls)",
            "            .options(",
            "                sa.orm.subqueryload(cls.columns),",
            "                sa.orm.subqueryload(cls.metrics),",
            "            )",
            "            .filter_by(id=datasource_id)",
            "            .one()",
            "        )",
            "",
            "    @classmethod",
            "    def get_all_datasources(cls) -> list[SqlaTable]:",
            "        qry = db.session.query(cls)",
            "        qry = cls.default_query(qry)",
            "        return qry.all()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry.filter_by(is_sqllab_view=False)",
            "",
            "    def has_extra_cache_key_calls(self, query_obj: QueryObjectDict) -> bool:",
            "        \"\"\"",
            "        Detects the presence of calls to `ExtraCache` methods in items in query_obj that",
            "        can be templated. If any are present, the query must be evaluated to extract",
            "        additional keys for the cache key. This method is needed to avoid executing the",
            "        template code unnecessarily, as it may contain expensive calls, e.g. to extract",
            "        the latest partition of a database.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: True if there are call(s) to an `ExtraCache` method, False otherwise",
            "        \"\"\"",
            "        templatable_statements: list[str] = []",
            "        if self.sql:",
            "            templatable_statements.append(self.sql)",
            "        if self.fetch_values_predicate:",
            "            templatable_statements.append(self.fetch_values_predicate)",
            "        extras = query_obj.get(\"extras\", {})",
            "        if \"where\" in extras:",
            "            templatable_statements.append(extras[\"where\"])",
            "        if \"having\" in extras:",
            "            templatable_statements.append(extras[\"having\"])",
            "        if self.is_rls_supported:",
            "            templatable_statements += [",
            "                f.clause for f in security_manager.get_rls_filters(self)",
            "            ]",
            "        for statement in templatable_statements:",
            "            if ExtraCache.regex.search(statement):",
            "                return True",
            "        return False",
            "",
            "    def get_extra_cache_keys(self, query_obj: QueryObjectDict) -> list[Hashable]:",
            "        \"\"\"",
            "        The cache key of a SqlaTable needs to consider any keys added by the parent",
            "        class and any keys added via `ExtraCache`.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: The extra cache keys",
            "        \"\"\"",
            "        extra_cache_keys = super().get_extra_cache_keys(query_obj)",
            "        if self.has_extra_cache_key_calls(query_obj):",
            "            sqla_query = self.get_sqla_query(**query_obj)",
            "            extra_cache_keys += sqla_query.extra_cache_keys",
            "        return extra_cache_keys",
            "",
            "    @property",
            "    def quote_identifier(self) -> Callable[[str], str]:",
            "        return self.database.quote_identifier",
            "",
            "    @staticmethod",
            "    def before_update(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Note this listener is called when any fields are being updated",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        :raises Exception: If the target table is not unique",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_before_update(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def update_column(  # pylint: disable=unused-argument",
            "        mapper: Mapper, connection: Connection, target: SqlMetric | TableColumn",
            "    ) -> None:",
            "        \"\"\"",
            "        :param mapper: Unused.",
            "        :param connection: Unused.",
            "        :param target: The metric or column that was updated.",
            "        \"\"\"",
            "        session = inspect(target).session  # pylint: disable=disallowed-name",
            "",
            "        # Forces an update to the table's changed_on value when a metric or column on the",
            "        # table is updated. This busts the cache key for all charts that use the table.",
            "        session.execute(update(SqlaTable).where(SqlaTable.id == target.table.id))",
            "",
            "    @staticmethod",
            "    def after_insert(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after insert",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_after_insert(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def after_delete(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        sqla_table: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after delete",
            "        \"\"\"",
            "        security_manager.dataset_after_delete(mapper, connection, sqla_table)",
            "",
            "    def load_database(self: SqlaTable) -> None:",
            "        # somehow the database attribute is not loaded on access",
            "        if self.database_id and (",
            "            not self.database or self.database.id != self.database_id",
            "        ):",
            "            session = inspect(self).session  # pylint: disable=disallowed-name",
            "            self.database = session.query(Database).filter_by(id=self.database_id).one()",
            "",
            "",
            "sa.event.listen(SqlaTable, \"before_update\", SqlaTable.before_update)",
            "sa.event.listen(SqlaTable, \"after_insert\", SqlaTable.after_insert)",
            "sa.event.listen(SqlaTable, \"after_delete\", SqlaTable.after_delete)",
            "sa.event.listen(SqlMetric, \"after_update\", SqlaTable.update_column)",
            "sa.event.listen(TableColumn, \"after_update\", SqlaTable.update_column)",
            "",
            "RLSFilterRoles = Table(",
            "    \"rls_filter_roles\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"role_id\", Integer, ForeignKey(\"ab_role.id\"), nullable=False),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "RLSFilterTables = Table(",
            "    \"rls_filter_tables\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\")),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "",
            "class RowLevelSecurityFilter(Model, AuditMixinNullable):",
            "    \"\"\"",
            "    Custom where clauses attached to Tables and Roles.",
            "    \"\"\"",
            "",
            "    __tablename__ = \"row_level_security_filters\"",
            "    id = Column(Integer, primary_key=True)",
            "    name = Column(String(255), unique=True, nullable=False)",
            "    description = Column(Text)",
            "    filter_type = Column(",
            "        Enum(*[filter_type.value for filter_type in utils.RowLevelSecurityFilterType])",
            "    )",
            "    group_key = Column(String(255), nullable=True)",
            "    roles = relationship(",
            "        security_manager.role_model,",
            "        secondary=RLSFilterRoles,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    tables = relationship(",
            "        SqlaTable,",
            "        overlaps=\"table\",",
            "        secondary=RLSFilterTables,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    clause = Column(MediumText(), nullable=False)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "36": [],
            "107": [],
            "1102": [
                "SqlaTable"
            ],
            "1416": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1417": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1418": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1419": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1420": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1421": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1422": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1423": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1424": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1425": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1426": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1427": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1428": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1429": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1430": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1431": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1432": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1433": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1434": [
                "SqlaTable",
                "get_query_str_extended"
            ],
            "1435": [
                "SqlaTable"
            ],
            "1477": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1478": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1479": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1480": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1481": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1482": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1483": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1484": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1485": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1486": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1487": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1488": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1489": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1490": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1491": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1492": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1493": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1494": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1495": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1496": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1497": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1498": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1499": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1500": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1501": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1502": [
                "SqlaTable",
                "get_rendered_sql"
            ],
            "1503": [
                "SqlaTable"
            ]
        },
        "addLocation": []
    },
    "superset/db_engine_specs/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 59,
                "PatchRowcode": " from superset.constants import TimeGrain as TimeGrainConstants"
            },
            "1": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 60,
                "PatchRowcode": " from superset.databases.utils import make_url_safe"
            },
            "2": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " from superset.errors import ErrorLevel, SupersetError, SupersetErrorType"
            },
            "3": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from superset.sql_parse import ParsedQuery, Table"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+from superset.sql_parse import ParsedQuery, SQLScript, Table"
            },
            "5": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 63,
                "PatchRowcode": " from superset.superset_typing import ResultSetColumnType, SQLAColumnType"
            },
            "6": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " from superset.utils import core as utils"
            },
            "7": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 65,
                "PatchRowcode": " from superset.utils.core import ColumnSpec, GenericDataType"
            },
            "8": {
                "beforePatchRowNumber": 1448,
                "afterPatchRowNumber": 1448,
                "PatchRowcode": "                 qry = partition_query"
            },
            "9": {
                "beforePatchRowNumber": 1449,
                "afterPatchRowNumber": 1449,
                "PatchRowcode": "         sql = database.compile_sqla_query(qry)"
            },
            "10": {
                "beforePatchRowNumber": 1450,
                "afterPatchRowNumber": 1450,
                "PatchRowcode": "         if indent:"
            },
            "11": {
                "beforePatchRowNumber": 1451,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            sql = sqlparse.format(sql, reindent=True)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1451,
                "PatchRowcode": "+            sql = SQLScript(sql, engine=cls.engine).format()"
            },
            "13": {
                "beforePatchRowNumber": 1452,
                "afterPatchRowNumber": 1452,
                "PatchRowcode": "         return sql"
            },
            "14": {
                "beforePatchRowNumber": 1453,
                "afterPatchRowNumber": 1453,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 1454,
                "afterPatchRowNumber": 1454,
                "PatchRowcode": "     @classmethod"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "",
            "from __future__ import annotations",
            "",
            "import json",
            "import logging",
            "import re",
            "from datetime import datetime",
            "from re import Match, Pattern",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    cast,",
            "    ContextManager,",
            "    NamedTuple,",
            "    TYPE_CHECKING,",
            "    TypedDict,",
            "    Union,",
            ")",
            "",
            "import pandas as pd",
            "import sqlparse",
            "from apispec import APISpec",
            "from apispec.ext.marshmallow import MarshmallowPlugin",
            "from deprecation import deprecated",
            "from flask import current_app",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from marshmallow import fields, Schema",
            "from marshmallow.validate import Range",
            "from sqlalchemy import column, select, types",
            "from sqlalchemy.engine.base import Engine",
            "from sqlalchemy.engine.interfaces import Compiled, Dialect",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.ext.compiler import compiles",
            "from sqlalchemy.sql import literal_column, quoted_name, text",
            "from sqlalchemy.sql.expression import ColumnClause, Select, TextAsFrom, TextClause",
            "from sqlalchemy.types import TypeEngine",
            "from sqlparse.tokens import CTE",
            "",
            "from superset import security_manager, sql_parse",
            "from superset.constants import TimeGrain as TimeGrainConstants",
            "from superset.databases.utils import make_url_safe",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.sql_parse import ParsedQuery, Table",
            "from superset.superset_typing import ResultSetColumnType, SQLAColumnType",
            "from superset.utils import core as utils",
            "from superset.utils.core import ColumnSpec, GenericDataType",
            "from superset.utils.hashing import md5_sha_from_str",
            "from superset.utils.network import is_hostname_valid, is_port_open",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import TableColumn",
            "    from superset.models.core import Database",
            "    from superset.models.sql_lab import Query",
            "",
            "ColumnTypeMapping = tuple[",
            "    Pattern[str],",
            "    Union[TypeEngine, Callable[[Match[str]], TypeEngine]],",
            "    GenericDataType,",
            "]",
            "",
            "logger = logging.getLogger()",
            "",
            "",
            "def convert_inspector_columns(cols: list[SQLAColumnType]) -> list[ResultSetColumnType]:",
            "    result_set_columns: list[ResultSetColumnType] = []",
            "    for col in cols:",
            "        result_set_columns.append({\"column_name\": col.get(\"name\"), **col})  # type: ignore",
            "    return result_set_columns",
            "",
            "",
            "class TimeGrain(NamedTuple):",
            "    name: str  # TODO: redundant field, remove",
            "    label: str",
            "    function: str",
            "    duration: str | None",
            "",
            "",
            "builtin_time_grains: dict[str | None, str] = {",
            "    TimeGrainConstants.SECOND: __(\"Second\"),",
            "    TimeGrainConstants.FIVE_SECONDS: __(\"5 second\"),",
            "    TimeGrainConstants.THIRTY_SECONDS: __(\"30 second\"),",
            "    TimeGrainConstants.MINUTE: __(\"Minute\"),",
            "    TimeGrainConstants.FIVE_MINUTES: __(\"5 minute\"),",
            "    TimeGrainConstants.TEN_MINUTES: __(\"10 minute\"),",
            "    TimeGrainConstants.FIFTEEN_MINUTES: __(\"15 minute\"),",
            "    TimeGrainConstants.THIRTY_MINUTES: __(\"30 minute\"),",
            "    TimeGrainConstants.HOUR: __(\"Hour\"),",
            "    TimeGrainConstants.SIX_HOURS: __(\"6 hour\"),",
            "    TimeGrainConstants.DAY: __(\"Day\"),",
            "    TimeGrainConstants.WEEK: __(\"Week\"),",
            "    TimeGrainConstants.MONTH: __(\"Month\"),",
            "    TimeGrainConstants.QUARTER: __(\"Quarter\"),",
            "    TimeGrainConstants.YEAR: __(\"Year\"),",
            "    TimeGrainConstants.WEEK_STARTING_SUNDAY: __(\"Week starting Sunday\"),",
            "    TimeGrainConstants.WEEK_STARTING_MONDAY: __(\"Week starting Monday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SATURDAY: __(\"Week ending Saturday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SUNDAY: __(\"Week ending Sunday\"),",
            "}",
            "",
            "",
            "class TimestampExpression(",
            "    ColumnClause",
            "):  # pylint: disable=abstract-method, too-many-ancestors",
            "    def __init__(self, expr: str, col: ColumnClause, **kwargs: Any) -> None:",
            "        \"\"\"Sqlalchemy class that can be used to render native column elements respecting",
            "        engine-specific quoting rules as part of a string-based expression.",
            "",
            "        :param expr: Sql expression with '{col}' denoting the locations where the col",
            "        object will be rendered.",
            "        :param col: the target column",
            "        \"\"\"",
            "        super().__init__(expr, **kwargs)",
            "        self.col = col",
            "",
            "    @property",
            "    def _constructor(self) -> ColumnClause:",
            "        # Needed to ensure that the column label is rendered correctly when",
            "        # proxied to the outer query.",
            "        # See https://github.com/sqlalchemy/sqlalchemy/issues/4730",
            "        return ColumnClause",
            "",
            "",
            "@compiles(TimestampExpression)",
            "def compile_timegrain_expression(",
            "    element: TimestampExpression, compiler: Compiled, **kwargs: Any",
            ") -> str:",
            "    return element.name.replace(\"{col}\", compiler.process(element.col, **kwargs))",
            "",
            "",
            "class LimitMethod:  # pylint: disable=too-few-public-methods",
            "    \"\"\"Enum the ways that limits can be applied\"\"\"",
            "",
            "    FETCH_MANY = \"fetch_many\"",
            "    WRAP_SQL = \"wrap_sql\"",
            "    FORCE_LIMIT = \"force_limit\"",
            "",
            "",
            "class MetricType(TypedDict, total=False):",
            "    \"\"\"",
            "    Type for metrics return by `get_metrics`.",
            "    \"\"\"",
            "",
            "    metric_name: str",
            "    expression: str",
            "    verbose_name: str | None",
            "    metric_type: str | None",
            "    description: str | None",
            "    d3format: str | None",
            "    currency: str | None",
            "    warning_text: str | None",
            "    extra: str | None",
            "",
            "",
            "class BaseEngineSpec:  # pylint: disable=too-many-public-methods",
            "    \"\"\"Abstract class for database engine specific configurations",
            "",
            "    Attributes:",
            "        allows_alias_to_source_column: Whether the engine is able to pick the",
            "                                       source column for aggregation clauses",
            "                                       used in ORDER BY when a column in SELECT",
            "                                       has an alias that is the same as a source",
            "                                       column.",
            "        allows_hidden_orderby_agg:     Whether the engine allows ORDER BY to",
            "                                       directly use aggregation clauses, without",
            "                                       having to add the same aggregation in SELECT.",
            "    \"\"\"",
            "",
            "    engine_name: str | None = None  # for user messages, overridden in child classes",
            "",
            "    # These attributes map the DB engine spec to one or more SQLAlchemy dialects/drivers;",
            "    # see the ``supports_url`` and ``supports_backend`` methods below.",
            "    engine = \"base\"  # str as defined in sqlalchemy.engine.engine",
            "    engine_aliases: set[str] = set()",
            "    drivers: dict[str, str] = {}",
            "    default_driver: str | None = None",
            "",
            "    # placeholder with the SQLAlchemy URI template",
            "    sqlalchemy_uri_placeholder = (",
            "        \"engine+driver://user:password@host:port/dbname[?key=value&key=value...]\"",
            "    )",
            "",
            "    disable_ssh_tunneling = False",
            "",
            "    _date_trunc_functions: dict[str, str] = {}",
            "    _time_grain_expressions: dict[str | None, str] = {}",
            "    _default_column_type_mappings: tuple[ColumnTypeMapping, ...] = (",
            "        (",
            "            re.compile(r\"^string\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^n((var)?char|text)\", re.IGNORECASE),",
            "            types.UnicodeText(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(var)?char\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(tiny|medium|long)?text\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallint\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^int(eger)?\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigint\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^long\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^decimal\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^numeric\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^float\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^double\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^real\", re.IGNORECASE),",
            "            types.REAL,",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallserial\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^serial\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigserial\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^money\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^timestamp\", re.IGNORECASE),",
            "            types.TIMESTAMP(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^datetime\", re.IGNORECASE),",
            "            types.DateTime(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^date\", re.IGNORECASE),",
            "            types.Date(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^time\", re.IGNORECASE),",
            "            types.Time(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^interval\", re.IGNORECASE),",
            "            types.Interval(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^bool(ean)?\", re.IGNORECASE),",
            "            types.Boolean(),",
            "            GenericDataType.BOOLEAN,",
            "        ),",
            "    )",
            "    # engine-specific type mappings to check prior to the defaults",
            "    column_type_mappings: tuple[ColumnTypeMapping, ...] = ()",
            "",
            "    # type-specific functions to mutate values received from the database.",
            "    # Needed on certain databases that return values in an unexpected format",
            "    column_type_mutators: dict[TypeEngine, Callable[[Any], Any]] = {}",
            "",
            "    # Does database support join-free timeslot grouping",
            "    time_groupby_inline = False",
            "    limit_method = LimitMethod.FORCE_LIMIT",
            "    allows_joins = True",
            "    allows_subqueries = True",
            "    allows_alias_in_select = True",
            "    allows_alias_in_orderby = True",
            "    allows_sql_comments = True",
            "    allows_escaped_colons = True",
            "",
            "    # Whether ORDER BY clause can use aliases created in SELECT",
            "    # that are the same as a source column",
            "    allows_alias_to_source_column = True",
            "",
            "    # Whether ORDER BY clause must appear in SELECT",
            "    # if True, then it doesn't have to.",
            "    allows_hidden_orderby_agg = True",
            "",
            "    # Whether ORDER BY clause can use sql calculated expression",
            "    # if True, use alias of select column for `order by`",
            "    # the True is safely for most database",
            "    # But for backward compatibility, False by default",
            "    allows_hidden_cc_in_orderby = False",
            "",
            "    # Whether allow CTE as subquery or regular CTE",
            "    # If True, then it will allow  in subquery ,",
            "    # if False it will allow as regular CTE",
            "    allows_cte_in_subquery = True",
            "    # Define alias for CTE",
            "    cte_alias = \"__cte\"",
            "    # Whether allow LIMIT clause in the SQL",
            "    # If True, then the database engine is allowed for LIMIT clause",
            "    # If False, then the database engine is allowed for TOP clause",
            "    allow_limit_clause = True",
            "    # This set will give keywords for select statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    select_keywords: set[str] = {\"SELECT\"}",
            "    # This set will give the keywords for data limit statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    top_keywords: set[str] = {\"TOP\"}",
            "    # A set of disallowed connection query parameters by driver name",
            "    disallow_uri_query_params: dict[str, set[str]] = {}",
            "    # A Dict of query parameters that will always be used on every connection",
            "    # by driver name",
            "    enforce_uri_query_params: dict[str, dict[str, Any]] = {}",
            "",
            "    force_column_alias_quotes = False",
            "    arraysize = 0",
            "    max_column_name_length: int | None = None",
            "    try_remove_schema_from_table_name = True  # pylint: disable=invalid-name",
            "    run_multiple_statements_as_one = False",
            "    custom_errors: dict[",
            "        Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]",
            "    ] = {}",
            "",
            "    # Whether the engine supports file uploads",
            "    # if True, database will be listed as option in the upload file form",
            "    supports_file_upload = True",
            "",
            "    # Is the DB engine spec able to change the default schema? This requires implementing",
            "    # a custom `adjust_engine_params` method.",
            "    supports_dynamic_schema = False",
            "",
            "    # Does the DB support catalogs? A catalog here is a group of schemas, and has",
            "    # different names depending on the DB: BigQuery calles it a \"project\", Postgres calls",
            "    # it a \"database\", Trino calls it a \"catalog\", etc.",
            "    supports_catalog = False",
            "",
            "    # Can the catalog be changed on a per-query basis?",
            "    supports_dynamic_catalog = False",
            "",
            "    @classmethod",
            "    def get_allows_alias_in_select(",
            "        cls, database: Database  # pylint: disable=unused-argument",
            "    ) -> bool:",
            "        \"\"\"",
            "        Method for dynamic `allows_alias_in_select`.",
            "",
            "        In Dremio this atribute is version-dependent, so Superset needs to inspect the",
            "        database configuration in order to determine it. This method allows engine-specs",
            "        to define dynamic values for the attribute.",
            "        \"\"\"",
            "        return cls.allows_alias_in_select",
            "",
            "    @classmethod",
            "    def supports_url(cls, url: URL) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy URL.",
            "",
            "        As an example, if a given DB engine spec has:",
            "",
            "            class PostgresDBEngineSpec:",
            "                engine = \"postgresql\"",
            "                engine_aliases = \"postgres\"",
            "                drivers = {",
            "                    \"psycopg2\": \"The default Postgres driver\",",
            "                    \"asyncpg\": \"An asynchronous Postgres driver\",",
            "                }",
            "",
            "        It would be used for all the following SQLAlchemy URIs:",
            "",
            "            - postgres://user:password@host/db",
            "            - postgresql://user:password@host/db",
            "            - postgres+asyncpg://user:password@host/db",
            "            - postgres+psycopg2://user:password@host/db",
            "            - postgresql+asyncpg://user:password@host/db",
            "            - postgresql+psycopg2://user:password@host/db",
            "",
            "        Note that SQLAlchemy has a default driver even if one is not specified:",
            "",
            "            >>> from sqlalchemy.engine.url import make_url",
            "            >>> make_url('postgres://').get_driver_name()",
            "            'psycopg2'",
            "",
            "        \"\"\"",
            "        backend = url.get_backend_name()",
            "        driver = url.get_driver_name()",
            "        return cls.supports_backend(backend, driver)",
            "",
            "    @classmethod",
            "    def supports_backend(cls, backend: str, driver: str | None = None) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy backend/driver.",
            "        \"\"\"",
            "        # check the backend first",
            "        if backend != cls.engine and backend not in cls.engine_aliases:",
            "            return False",
            "",
            "        # originally DB engine specs didn't declare any drivers and the check was made",
            "        # only on the engine; if that's the case, ignore the driver for backwards",
            "        # compatibility",
            "        if not cls.drivers or driver is None:",
            "            return True",
            "",
            "        return driver in cls.drivers",
            "",
            "    @classmethod",
            "    def get_default_schema(cls, database: Database) -> str | None:",
            "        \"\"\"",
            "        Return the default schema in a given database.",
            "        \"\"\"",
            "        with database.get_inspector_with_context() as inspector:",
            "            return inspector.default_schema_name",
            "",
            "    @classmethod",
            "    def get_schema_from_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        sqlalchemy_uri: URL,",
            "        connect_args: dict[str, Any],",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the schema configured in a SQLALchemy URI and connection arguments, if any.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def get_default_schema_for_query(",
            "        cls,",
            "        database: Database,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the default schema for a given query.",
            "",
            "        This is used to determine the schema of tables that aren't fully qualified, eg:",
            "",
            "            SELECT * FROM foo;",
            "",
            "        In the example above, the schema where the `foo` table lives depends on a few",
            "        factors:",
            "",
            "            1. For DB engine specs that allow dynamically changing the schema based on the",
            "               query we should use the query schema.",
            "            2. For DB engine specs that don't support dynamically changing the schema and",
            "               have the schema hardcoded in the SQLAlchemy URI we should use the schema",
            "               from the URI.",
            "            3. For DB engine specs that don't connect to a specific schema and can't",
            "               change it dynamically we need to probe the database for the default schema.",
            "",
            "        Determining the correct schema is crucial for managing access to data, so please",
            "        make sure you understand this logic when working on a new DB engine spec.",
            "        \"\"\"",
            "        # dynamic schema varies on a per-query basis",
            "        if cls.supports_dynamic_schema:",
            "            return query.schema",
            "",
            "        # check if the schema is stored in the SQLAlchemy URI or connection arguments",
            "        try:",
            "            connect_args = database.get_extra()[\"engine_params\"][\"connect_args\"]",
            "        except KeyError:",
            "            connect_args = {}",
            "        sqlalchemy_uri = make_url_safe(database.sqlalchemy_uri)",
            "        if schema := cls.get_schema_from_engine_params(sqlalchemy_uri, connect_args):",
            "            return schema",
            "",
            "        # return the default schema of the database",
            "        return cls.get_default_schema(database)",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific exceptions into",
            "        Superset DBAPI exceptions",
            "",
            "        Note: On python 3.9 this method can be changed to a classmethod property",
            "        without the need of implementing a metaclass type",
            "",
            "        :return: A map of driver specific exception to superset custom exceptions",
            "        \"\"\"",
            "        return {}",
            "",
            "    @classmethod",
            "    def parse_error_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific parser method",
            "",
            "        :return: An Exception with a parsed string off the original exception",
            "        \"\"\"",
            "        return exception",
            "",
            "    @classmethod",
            "    def get_dbapi_mapped_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Get a superset custom DBAPI exception from the driver specific exception.",
            "",
            "        Override if the engine needs to perform extra changes to the exception, for",
            "        example change the exception message or implement custom more complex logic",
            "",
            "        :param exception: The driver specific exception",
            "        :return: Superset custom DBAPI exception",
            "        \"\"\"",
            "        new_exception = cls.get_dbapi_exception_mapping().get(type(exception))",
            "        if not new_exception:",
            "            return cls.parse_error_exception(exception)",
            "        return new_exception(str(exception))",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(  # pylint: disable=unused-argument",
            "        cls,",
            "        extra: dict[str, Any],",
            "    ) -> bool:",
            "        return False",
            "",
            "    @classmethod",
            "    def get_text_clause(cls, clause: str) -> TextClause:",
            "        \"\"\"",
            "        SQLAlchemy wrapper to ensure text clauses are escaped properly",
            "",
            "        :param clause: string clause with potentially unescaped characters",
            "        :return: text clause with escaped characters",
            "        \"\"\"",
            "        if cls.allows_escaped_colons:",
            "            clause = clause.replace(\":\", \"\\\\:\")",
            "        return text(clause)",
            "",
            "    @classmethod",
            "    def get_engine(",
            "        cls,",
            "        database: Database,",
            "        schema: str | None = None,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> ContextManager[Engine]:",
            "        \"\"\"",
            "        Return an engine context manager.",
            "",
            "            >>> with DBEngineSpec.get_engine(database, schema, source) as engine:",
            "            ...     connection = engine.connect()",
            "            ...     connection.execute(sql)",
            "",
            "        \"\"\"",
            "        return database.get_sqla_engine_with_context(schema=schema, source=source)",
            "",
            "    @classmethod",
            "    def get_timestamp_expr(",
            "        cls,",
            "        col: ColumnClause,",
            "        pdf: str | None,",
            "        time_grain: str | None,",
            "    ) -> TimestampExpression:",
            "        \"\"\"",
            "        Construct a TimestampExpression to be used in a SQLAlchemy query.",
            "",
            "        :param col: Target column for the TimestampExpression",
            "        :param pdf: date format (seconds or milliseconds)",
            "        :param time_grain: time grain, e.g. P1Y for 1 year",
            "        :return: TimestampExpression object",
            "        \"\"\"",
            "        if time_grain:",
            "            type_ = str(getattr(col, \"type\", \"\"))",
            "            time_expr = cls.get_time_grain_expressions().get(time_grain)",
            "            if not time_expr:",
            "                raise NotImplementedError(",
            "                    f\"No grain spec for {time_grain} for database {cls.engine}\"",
            "                )",
            "            if type_ and \"{func}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{func}\", date_trunc_function)",
            "            if type_ and \"{type}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{type}\", type_)",
            "        else:",
            "            time_expr = \"{col}\"",
            "",
            "        # if epoch, translate to DATE using db specific conf",
            "        if pdf == \"epoch_s\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_to_dttm())",
            "        elif pdf == \"epoch_ms\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_ms_to_dttm())",
            "",
            "        return TimestampExpression(time_expr, col, type_=col.type)",
            "",
            "    @classmethod",
            "    def get_time_grains(cls) -> tuple[TimeGrain, ...]:",
            "        \"\"\"",
            "        Generate a tuple of supported time grains.",
            "",
            "        :return: All time grains supported by the engine",
            "        \"\"\"",
            "",
            "        ret_list = []",
            "        time_grains = builtin_time_grains.copy()",
            "        time_grains.update(current_app.config[\"TIME_GRAIN_ADDONS\"])",
            "        for duration, func in cls.get_time_grain_expressions().items():",
            "            if duration in time_grains:",
            "                name = time_grains[duration]",
            "                ret_list.append(TimeGrain(name, _(name), func, duration))",
            "        return tuple(ret_list)",
            "",
            "    @classmethod",
            "    def _sort_time_grains(",
            "        cls, val: tuple[str | None, str], index: int",
            "    ) -> float | int | str:",
            "        \"\"\"",
            "        Return an ordered time-based value of a portion of a time grain",
            "        for sorting",
            "        Values are expected to be either None or start with P or PT",
            "        Have a numerical value in the middle and end with",
            "        a value for the time interval",
            "        It can also start or end with epoch start time denoting a range",
            "        i.e, week beginning or ending with a day",
            "        \"\"\"",
            "        pos = {",
            "            \"FIRST\": 0,",
            "            \"SECOND\": 1,",
            "            \"THIRD\": 2,",
            "            \"LAST\": 3,",
            "        }",
            "",
            "        if val[0] is None:",
            "            return pos[\"FIRST\"]",
            "",
            "        prog = re.compile(r\"(.*\\/)?(P|PT)([0-9\\.]+)(S|M|H|D|W|M|Y)(\\/.*)?\")",
            "        result = prog.match(val[0])",
            "",
            "        # for any time grains that don't match the format, put them at the end",
            "        if result is None:",
            "            return pos[\"LAST\"]",
            "",
            "        second_minute_hour = [\"S\", \"M\", \"H\"]",
            "        day_week_month_year = [\"D\", \"W\", \"M\", \"Y\"]",
            "        is_less_than_day = result.group(2) == \"PT\"",
            "        interval = result.group(4)",
            "        epoch_time_start_string = result.group(1) or result.group(5)",
            "        has_starting_or_ending = bool(len(epoch_time_start_string or \"\"))",
            "",
            "        def sort_day_week() -> int:",
            "            if has_starting_or_ending:",
            "                return pos[\"LAST\"]",
            "            if is_less_than_day:",
            "                return pos[\"SECOND\"]",
            "            return pos[\"THIRD\"]",
            "",
            "        def sort_interval() -> float:",
            "            if is_less_than_day:",
            "                return second_minute_hour.index(interval)",
            "            return day_week_month_year.index(interval)",
            "",
            "        # 0: all \"PT\" values should come before \"P\" values (i.e, PT10M)",
            "        # 1: order values within the above arrays (\"D\" before \"W\")",
            "        # 2: sort by numeric value (PT10M before PT15M)",
            "        # 3: sort by any week starting/ending values",
            "        plist = {",
            "            0: sort_day_week(),",
            "            1: pos[\"SECOND\"] if is_less_than_day else pos[\"THIRD\"],",
            "            2: sort_interval(),",
            "            3: float(result.group(3)),",
            "        }",
            "",
            "        return plist.get(index, 0)",
            "",
            "    @classmethod",
            "    def get_time_grain_expressions(cls) -> dict[str | None, str]:",
            "        \"\"\"",
            "        Return a dict of all supported time grains including any potential added grains",
            "        but excluding any potentially disabled grains in the config file.",
            "",
            "        :return: All time grain expressions supported by the engine",
            "        \"\"\"",
            "        # TODO: use @memoize decorator or similar to avoid recomputation on every call",
            "        time_grain_expressions = cls._time_grain_expressions.copy()",
            "        grain_addon_expressions = current_app.config[\"TIME_GRAIN_ADDON_EXPRESSIONS\"]",
            "        time_grain_expressions.update(grain_addon_expressions.get(cls.engine, {}))",
            "        denylist: list[str] = current_app.config[\"TIME_GRAIN_DENYLIST\"]",
            "        for key in denylist:",
            "            time_grain_expressions.pop(key, None)",
            "",
            "        return dict(",
            "            sorted(",
            "                time_grain_expressions.items(),",
            "                key=lambda x: (",
            "                    cls._sort_time_grains(x, 0),",
            "                    cls._sort_time_grains(x, 1),",
            "                    cls._sort_time_grains(x, 2),",
            "                    cls._sort_time_grains(x, 3),",
            "                ),",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "",
            "        :param cursor: Cursor instance",
            "        :param limit: Maximum number of rows to be returned by the cursor",
            "        :return: Result of query",
            "        \"\"\"",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            if cls.limit_method == LimitMethod.FETCH_MANY and limit:",
            "                return cursor.fetchmany(limit)",
            "            data = cursor.fetchall()",
            "            description = cursor.description or []",
            "            # Create a mapping between column name and a mutator function to normalize",
            "            # values with. The first two items in the description row are",
            "            # the column name and type.",
            "            column_mutators = {",
            "                row[0]: func",
            "                for row in description",
            "                if (",
            "                    func := cls.column_type_mutators.get(",
            "                        type(cls.get_sqla_column_type(cls.get_datatype(row[1])))",
            "                    )",
            "                )",
            "            }",
            "            if column_mutators:",
            "                indexes = {row[0]: idx for idx, row in enumerate(description)}",
            "                for row_idx, row in enumerate(data):",
            "                    new_row = list(row)",
            "                    for col, func in column_mutators.items():",
            "                        col_idx = indexes[col]",
            "                        new_row[col_idx] = func(row[col_idx])",
            "                    data[row_idx] = tuple(new_row)",
            "",
            "            return data",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def expand_data(",
            "        cls, columns: list[ResultSetColumnType], data: list[dict[Any, Any]]",
            "    ) -> tuple[",
            "        list[ResultSetColumnType], list[dict[Any, Any]], list[ResultSetColumnType]",
            "    ]:",
            "        \"\"\"",
            "        Some engines support expanding nested fields. See implementation in Presto",
            "        spec for details.",
            "",
            "        :param columns: columns selected in the query",
            "        :param data: original data set",
            "        :return: list of all columns(selected columns and their nested fields),",
            "                 expanded data set, listed of nested fields",
            "        \"\"\"",
            "        return columns, data, []",
            "",
            "    @classmethod",
            "    def alter_new_orm_column(cls, orm_col: TableColumn) -> None:",
            "        \"\"\"Allow altering default column attributes when first detected/added",
            "",
            "        For instance special column like `__time` for Druid can be",
            "        set to is_dttm=True. Note that this only gets called when new",
            "        columns are detected/created\"\"\"",
            "        # TODO: Fix circular import caused by importing TableColumn",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (seconds) to datetime that can be used in a",
            "        query. The reference column should be denoted as `{col}` in the return",
            "        expression, e.g. \"FROM_UNIXTIME({col})\"",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @classmethod",
            "    def epoch_ms_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (milliseconds) to datetime that can be used",
            "        in a query.",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        return cls.epoch_to_dttm().replace(\"{col}\", \"({col}/1000)\")",
            "",
            "    @classmethod",
            "    def get_datatype(cls, type_code: Any) -> str | None:",
            "        \"\"\"",
            "        Change column type code from cursor description to string representation.",
            "",
            "        :param type_code: Type code from cursor description",
            "        :return: String representation of type code",
            "        \"\"\"",
            "        if isinstance(type_code, str) and type_code != \"\":",
            "            return type_code.upper()",
            "        return None",
            "",
            "    @classmethod",
            "    @deprecated(deprecated_in=\"3.0\")",
            "    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Normalizes indexes for more consistency across db engines",
            "",
            "        noop by default",
            "",
            "        :param indexes: Raw indexes as returned by SQLAlchemy",
            "        :return: cleaner, more aligned index definition",
            "        \"\"\"",
            "        return indexes",
            "",
            "    @classmethod",
            "    def extra_table_metadata(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        schema_name: str | None,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Returns engine-specific table metadata",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name",
            "        :param schema_name: Schema name",
            "        :return: Engine-specific table metadata",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        return {}",
            "",
            "    @classmethod",
            "    def apply_limit_to_sql(",
            "        cls, sql: str, limit: int, database: Database, force: bool = False",
            "    ) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a LIMIT clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param database: Database instance",
            "        :return: SQL query with limit clause",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        if cls.limit_method == LimitMethod.WRAP_SQL:",
            "            sql = sql.strip(\"\\t\\n ;\")",
            "            qry = (",
            "                select(\"*\")",
            "                .select_from(TextAsFrom(text(sql), [\"*\"]).alias(\"inner_qry\"))",
            "                .limit(limit)",
            "            )",
            "            return database.compile_sqla_query(qry)",
            "",
            "        if cls.limit_method == LimitMethod.FORCE_LIMIT:",
            "            parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "            sql = parsed_query.set_or_update_query_limit(limit, force=force)",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def apply_top_to_sql(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a TOP clause",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param sql: SQL query",
            "        :return: SQL query with top clause",
            "        \"\"\"",
            "",
            "        cte = None",
            "        sql_remainder = None",
            "        sql = sql.strip(\" \\t\\n;\")",
            "        sql_statement = sqlparse.format(sql, strip_comments=True)",
            "        query_limit: int | None = sql_parse.extract_top_from_query(",
            "            sql_statement, cls.top_keywords",
            "        )",
            "        if not limit:",
            "            final_limit = query_limit",
            "        elif int(query_limit or 0) < limit and query_limit is not None:",
            "            final_limit = query_limit",
            "        else:",
            "            final_limit = limit",
            "        if not cls.allows_cte_in_subquery:",
            "            cte, sql_remainder = sql_parse.get_cte_remainder_query(sql_statement)",
            "        if cte:",
            "            str_statement = str(sql_remainder)",
            "            cte = cte + \"\\n\"",
            "        else:",
            "            cte = \"\"",
            "            str_statement = str(sql)",
            "        str_statement = str_statement.replace(\"\\n\", \" \").replace(\"\\r\", \"\")",
            "",
            "        tokens = str_statement.rstrip().split(\" \")",
            "        tokens = [token for token in tokens if token]",
            "        if cls.top_not_in_sql(str_statement):",
            "            selects = [",
            "                i",
            "                for i, word in enumerate(tokens)",
            "                if word.upper() in cls.select_keywords",
            "            ]",
            "            first_select = selects[0]",
            "            if tokens[first_select + 1].upper() == \"DISTINCT\":",
            "                first_select += 1",
            "",
            "            tokens.insert(first_select + 1, \"TOP\")",
            "            tokens.insert(first_select + 2, str(final_limit))",
            "",
            "        next_is_limit_token = False",
            "        new_tokens = []",
            "",
            "        for token in tokens:",
            "            if token in cls.top_keywords:",
            "                next_is_limit_token = True",
            "            elif next_is_limit_token:",
            "                if token.isdigit():",
            "                    token = str(final_limit)",
            "                    next_is_limit_token = False",
            "            new_tokens.append(token)",
            "        sql = \" \".join(new_tokens)",
            "        return cte + sql",
            "",
            "    @classmethod",
            "    def top_not_in_sql(cls, sql: str) -> bool:",
            "        for top_word in cls.top_keywords:",
            "            if top_word.upper() in sql.upper():",
            "                return False",
            "        return True",
            "",
            "    @classmethod",
            "    def get_limit_from_sql(cls, sql: str) -> int | None:",
            "        \"\"\"",
            "        Extract limit from SQL query",
            "",
            "        :param sql: SQL query",
            "        :return: Value of limit clause in query",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.limit",
            "",
            "    @classmethod",
            "    def set_or_update_query_limit(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Create a query based on original query but with new limit clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: New limit to insert/replace into query",
            "        :return: Query with new limit",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.set_or_update_query_limit(limit)",
            "",
            "    @classmethod",
            "    def get_cte_query(cls, sql: str) -> str | None:",
            "        \"\"\"",
            "        Convert the input CTE based SQL to the SQL for virtual table conversion",
            "",
            "        :param sql: SQL query",
            "        :return: CTE with the main select query aliased as `__cte`",
            "",
            "        \"\"\"",
            "        if not cls.allows_cte_in_subquery:",
            "            stmt = sqlparse.parse(sql)[0]",
            "",
            "            # The first meaningful token for CTE will be with WITH",
            "            idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)",
            "            if not (token and token.ttype == CTE):",
            "                return None",
            "            idx, token = stmt.token_next(idx)",
            "            idx = stmt.token_index(token) + 1",
            "",
            "            # extract rest of the SQLs after CTE",
            "            remainder = \"\".join(str(token) for token in stmt.tokens[idx:]).strip()",
            "            return f\"WITH {token.value},\\n{cls.cte_alias} AS (\\n{remainder}\\n)\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def df_to_sql(",
            "        cls,",
            "        database: Database,",
            "        table: Table,",
            "        df: pd.DataFrame,",
            "        to_sql_kwargs: dict[str, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Upload data from a Pandas DataFrame to a database.",
            "",
            "        For regular engines this calls the `pandas.DataFrame.to_sql` method. Can be",
            "        overridden for engines that don't work well with this method, e.g. Hive and",
            "        BigQuery.",
            "",
            "        Note this method does not create metadata for the table.",
            "",
            "        :param database: The database to upload the data to",
            "        :param table: The table to upload the data to",
            "        :param df: The dataframe with data to be uploaded",
            "        :param to_sql_kwargs: The kwargs to be passed to pandas.DataFrame.to_sql` method",
            "        \"\"\"",
            "",
            "        to_sql_kwargs[\"name\"] = table.table",
            "",
            "        if table.schema:",
            "            # Only add schema when it is preset and non-empty.",
            "            to_sql_kwargs[\"schema\"] = table.schema",
            "",
            "        with cls.get_engine(database) as engine:",
            "            if engine.dialect.supports_multivalues_insert:",
            "                to_sql_kwargs[\"method\"] = \"multi\"",
            "",
            "            df.to_sql(con=engine, **to_sql_kwargs)",
            "",
            "    @classmethod",
            "    def convert_dttm(  # pylint: disable=unused-argument",
            "        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Convert a Python `datetime` object to a SQL expression.",
            "",
            "        :param target_type: The target type of expression",
            "        :param dttm: The datetime object",
            "        :param db_extra: The database extra object",
            "        :return: The SQL expression",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def handle_cursor(cls, cursor: Any, query: Query) -> None:",
            "        \"\"\"Handle a live cursor between the execute and fetchall calls",
            "",
            "        The flow works without this method doing anything, but it allows",
            "        for handling the cursor and updating progress information in the",
            "        query object\"\"\"",
            "        # TODO: Fix circular import error caused by importing sql_lab.Query",
            "",
            "    @classmethod",
            "    def execute_with_cursor(cls, cursor: Any, sql: str, query: Query) -> None:",
            "        \"\"\"",
            "        Trigger execution of a query and handle the resulting cursor.",
            "",
            "        For most implementations this just makes calls to `execute` and",
            "        `handle_cursor` consecutively, but in some engines (e.g. Trino) we may",
            "        need to handle client limitations such as lack of async support and",
            "        perform a more complicated operation to get information from the cursor",
            "        in a timely manner and facilitate operations such as query stop",
            "        \"\"\"",
            "        logger.debug(\"Query %d: Running query: %s\", query.id, sql)",
            "        cls.execute(cursor, sql, async_=True)",
            "        logger.debug(\"Query %d: Handling cursor\", query.id)",
            "        cls.handle_cursor(cursor, query)",
            "",
            "    @classmethod",
            "    def extract_error_message(cls, ex: Exception) -> str:",
            "        return f\"{cls.engine} error: {cls._extract_error_message(ex)}\"",
            "",
            "    @classmethod",
            "    def _extract_error_message(cls, ex: Exception) -> str:",
            "        \"\"\"Extract error message for queries\"\"\"",
            "        return utils.error_msg_from_exception(ex)",
            "",
            "    @classmethod",
            "    def extract_errors(",
            "        cls, ex: Exception, context: dict[str, Any] | None = None",
            "    ) -> list[SupersetError]:",
            "        raw_message = cls._extract_error_message(ex)",
            "",
            "        context = context or {}",
            "        for regex, (message, error_type, extra) in cls.custom_errors.items():",
            "            if match := regex.search(raw_message):",
            "                params = {**context, **match.groupdict()}",
            "                extra[\"engine_name\"] = cls.engine_name",
            "                return [",
            "                    SupersetError(",
            "                        error_type=error_type,",
            "                        message=message % params,",
            "                        level=ErrorLevel.ERROR,",
            "                        extra=extra,",
            "                    )",
            "                ]",
            "",
            "        return [",
            "            SupersetError(",
            "                error_type=SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "                message=cls._extract_error_message(ex),",
            "                level=ErrorLevel.ERROR,",
            "                extra={\"engine_name\": cls.engine_name},",
            "            )",
            "        ]",
            "",
            "    @classmethod",
            "    def adjust_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        uri: URL,",
            "        connect_args: dict[str, Any],",
            "        catalog: str | None = None,",
            "        schema: str | None = None,",
            "    ) -> tuple[URL, dict[str, Any]]:",
            "        \"\"\"",
            "        Return a new URL and ``connect_args`` for a specific catalog/schema.",
            "",
            "        This is used in SQL Lab, allowing users to select a schema from the list of",
            "        schemas available in a given database, and have the query run with that schema as",
            "        the default one.",
            "",
            "        For some databases (like MySQL, Presto, Snowflake) this requires modifying the",
            "        SQLAlchemy URI before creating the connection. For others (like Postgres), it",
            "        requires additional parameters in ``connect_args`` or running pre-session",
            "        queries with ``set`` parameters.",
            "",
            "        When a DB engine spec implements this method or ``get_prequeries`` (see below) it",
            "        should also have the attribute ``supports_dynamic_schema`` set to true, so that",
            "        Superset knows in which schema a given query is running in order to enforce",
            "        permissions (see #23385 and #23401).",
            "",
            "        Currently, changing the catalog is not supported. The method accepts a catalog so",
            "        that when catalog support is added to Superset the interface remains the same.",
            "        This is important because DB engine specs can be installed from 3rd party",
            "        packages, so we want to keep these methods as stable as possible.",
            "        \"\"\"",
            "        return uri, {",
            "            **connect_args,",
            "            **cls.enforce_uri_query_params.get(uri.get_driver_name(), {}),",
            "        }",
            "",
            "    @classmethod",
            "    def get_prequeries(",
            "        cls,",
            "        catalog: str | None = None,  # pylint: disable=unused-argument",
            "        schema: str | None = None,  # pylint: disable=unused-argument",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return pre-session queries.",
            "",
            "        These are currently used as an alternative to ``adjust_engine_params`` for",
            "        databases where the selected schema cannot be specified in the SQLAlchemy URI or",
            "        connection arguments.",
            "",
            "        For example, in order to specify a default schema in RDS we need to run a query",
            "        at the beginning of the session:",
            "",
            "            sql> set search_path = my_schema;",
            "",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def patch(cls) -> None:",
            "        \"\"\"",
            "        TODO: Improve docstring and refactor implementation in Hive",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def get_catalog_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get all catalogs from database.",
            "",
            "        This needs to be implemented per database, since SQLAlchemy doesn't offer an",
            "        abstraction.",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def get_schema_names(cls, inspector: Inspector) -> list[str]:",
            "        \"\"\"",
            "        Get all schemas from database",
            "",
            "        :param inspector: SqlAlchemy inspector",
            "        :return: All schemas in the database",
            "        \"\"\"",
            "        return sorted(inspector.get_schema_names())",
            "",
            "    @classmethod",
            "    def get_table_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the real table names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The physical table names",
            "        \"\"\"",
            "",
            "        try:",
            "            tables = set(inspector.get_table_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            tables = {re.sub(f\"^{schema}\\\\.\", \"\", table) for table in tables}",
            "        return tables",
            "",
            "    @classmethod",
            "    def get_view_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the view names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The view names",
            "        \"\"\"",
            "",
            "        try:",
            "            views = set(inspector.get_view_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            views = {re.sub(f\"^{schema}\\\\.\", \"\", view) for view in views}",
            "        return views",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: Database,  # pylint: disable=unused-argument",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "",
            "        return inspector.get_indexes(table_name, schema)",
            "",
            "    @classmethod",
            "    def get_table_comment(",
            "        cls, inspector: Inspector, table_name: str, schema: str | None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Get comment of table from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :return: comment of table",
            "        \"\"\"",
            "        comment = None",
            "        try:",
            "            comment = inspector.get_table_comment(table_name, schema)",
            "            comment = comment.get(\"text\") if isinstance(comment, dict) else None",
            "        except NotImplementedError:",
            "            # It's expected that some dialects don't implement the comment method",
            "            pass",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.error(\"Unexpected error while fetching table comment\", exc_info=True)",
            "            logger.exception(ex)",
            "        return comment",
            "",
            "    @classmethod",
            "    def get_columns(  # pylint: disable=unused-argument",
            "        cls,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "        options: dict[str, Any] | None = None,",
            "    ) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        Get all columns from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :param options: Extra options to customise the display of columns in",
            "                        some databases",
            "        :return: All columns in table",
            "        \"\"\"",
            "        return convert_inspector_columns(",
            "            cast(list[SQLAColumnType], inspector.get_columns(table_name, schema))",
            "        )",
            "",
            "    @classmethod",
            "    def get_metrics(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[MetricType]:",
            "        \"\"\"",
            "        Get all metrics from a given schema and table.",
            "        \"\"\"",
            "        return [",
            "            {",
            "                \"metric_name\": \"count\",",
            "                \"verbose_name\": \"COUNT(*)\",",
            "                \"metric_type\": \"count\",",
            "                \"expression\": \"COUNT(*)\",",
            "            }",
            "        ]",
            "",
            "    @classmethod",
            "    def where_latest_partition(  # pylint: disable=too-many-arguments,unused-argument",
            "        cls,",
            "        table_name: str,",
            "        schema: str | None,",
            "        database: Database,",
            "        query: Select,",
            "        columns: list[ResultSetColumnType] | None = None,",
            "    ) -> Select | None:",
            "        \"\"\"",
            "        Add a where clause to a query to reference only the most recent partition",
            "",
            "        :param table_name: Table name",
            "        :param schema: Schema name",
            "        :param database: Database instance",
            "        :param query: SqlAlchemy query",
            "        :param columns: List of TableColumns",
            "        :return: SqlAlchemy query with additional where clause referencing the latest",
            "        partition",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database, TableColumn",
            "        return None",
            "",
            "    @classmethod",
            "    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:",
            "        return [",
            "            literal_column(query_as)",
            "            if (query_as := c.get(\"query_as\"))",
            "            else column(c[\"column_name\"])",
            "            for c in cols",
            "        ]",
            "",
            "    @classmethod",
            "    def select_star(  # pylint: disable=too-many-arguments,too-many-locals",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        engine: Engine,",
            "        schema: str | None = None,",
            "        limit: int = 100,",
            "        show_cols: bool = False,",
            "        indent: bool = True,",
            "        latest_partition: bool = True,",
            "        cols: list[ResultSetColumnType] | None = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Generate a \"SELECT * from [schema.]table_name\" query with appropriate limit.",
            "",
            "        WARNING: expects only unquoted table and schema names.",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name, unquoted",
            "        :param engine: SqlAlchemy Engine instance",
            "        :param schema: Schema, unquoted",
            "        :param limit: limit to impose on query",
            "        :param show_cols: Show columns in query; otherwise use \"*\"",
            "        :param indent: Add indentation to query",
            "        :param latest_partition: Only query the latest partition",
            "        :param cols: Columns to include in query",
            "        :return: SQL query",
            "        \"\"\"",
            "        # pylint: disable=redefined-outer-name",
            "        fields: str | list[Any] = \"*\"",
            "        cols = cols or []",
            "        if (show_cols or latest_partition) and not cols:",
            "            cols = database.get_columns(table_name, schema)",
            "",
            "        if show_cols:",
            "            fields = cls._get_fields(cols)",
            "        quote = engine.dialect.identifier_preparer.quote",
            "        quote_schema = engine.dialect.identifier_preparer.quote_schema",
            "        if schema:",
            "            full_table_name = quote_schema(schema) + \".\" + quote(table_name)",
            "        else:",
            "            full_table_name = quote(table_name)",
            "",
            "        qry = select(fields).select_from(text(full_table_name))",
            "",
            "        if limit and cls.allow_limit_clause:",
            "            qry = qry.limit(limit)",
            "        if latest_partition:",
            "            partition_query = cls.where_latest_partition(",
            "                table_name, schema, database, qry, columns=cols",
            "            )",
            "            if partition_query is not None:",
            "                qry = partition_query",
            "        sql = database.compile_sqla_query(qry)",
            "        if indent:",
            "            sql = sqlparse.format(sql, reindent=True)",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        \"\"\"",
            "        Generate a SQL query that estimates the cost of a given statement.",
            "",
            "        :param statement: A single SQL statement",
            "        :param cursor: Cursor instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        \"\"\"",
            "        Format cost estimate.",
            "",
            "        :param raw_cost: Raw estimate from `estimate_query_cost`",
            "        :return: Human readable cost estimate",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def process_statement(cls, statement: str, database: Database) -> str:",
            "        \"\"\"",
            "        Process a SQL statement by stripping and mutating it.",
            "",
            "        :param statement: A single SQL statement",
            "        :param database: Database instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        parsed_query = ParsedQuery(statement, engine=cls.engine)",
            "        sql = parsed_query.stripped()",
            "        sql_query_mutator = current_app.config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = current_app.config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=database,",
            "            )",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_query_cost(",
            "        cls,",
            "        database: Database,",
            "        schema: str,",
            "        sql: str,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Estimate the cost of a multiple statement SQL query.",
            "",
            "        :param database: Database instance",
            "        :param schema: Database schema",
            "        :param sql: SQL query with possibly multiple statements",
            "        :param source: Source of the query (eg, \"sql_lab\")",
            "        \"\"\"",
            "        extra = database.get_extra() or {}",
            "        if not cls.get_allow_cost_estimate(extra):",
            "            raise Exception(  # pylint: disable=broad-exception-raised",
            "                \"Database does not support cost estimation\"",
            "            )",
            "",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        statements = parsed_query.get_statements()",
            "",
            "        costs = []",
            "        with database.get_raw_connection(schema=schema, source=source) as conn:",
            "            cursor = conn.cursor()",
            "            for statement in statements:",
            "                processed_statement = cls.process_statement(statement, database)",
            "                costs.append(cls.estimate_statement_cost(processed_statement, cursor))",
            "",
            "        return costs",
            "",
            "    @classmethod",
            "    def get_url_for_impersonation(",
            "        cls, url: URL, impersonate_user: bool, username: str | None",
            "    ) -> URL:",
            "        \"\"\"",
            "        Return a modified URL with the username set.",
            "",
            "        :param url: SQLAlchemy URL object",
            "        :param impersonate_user: Flag indicating if impersonation is enabled",
            "        :param username: Effective username",
            "        \"\"\"",
            "        if impersonate_user and username is not None:",
            "            url = url.set(username=username)",
            "",
            "        return url",
            "",
            "    @classmethod",
            "    def update_impersonation_config(",
            "        cls,",
            "        connect_args: dict[str, Any],",
            "        uri: str,",
            "        username: str | None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a configuration dictionary",
            "        that can set the correct properties for impersonating users",
            "",
            "        :param connect_args: config to be updated",
            "        :param uri: URI",
            "        :param username: Effective username",
            "        :return: None",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def execute(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: str,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"",
            "        Execute a SQL query",
            "",
            "        :param cursor: Cursor instance",
            "        :param query: Query to execute",
            "        :param kwargs: kwargs to be passed to cursor.execute()",
            "        :return:",
            "        \"\"\"",
            "        if not cls.allows_sql_comments:",
            "            query = sql_parse.strip_comments_from_sql(query, engine=cls.engine)",
            "",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            cursor.execute(query)",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def make_label_compatible(cls, label: str) -> str | quoted_name:",
            "        \"\"\"",
            "        Conditionally mutate and/or quote a sqlalchemy expression label. If",
            "        force_column_alias_quotes is set to True, return the label as a",
            "        sqlalchemy.sql.elements.quoted_name object to ensure that the select query",
            "        and query results have same case. Otherwise, return the mutated label as a",
            "        regular string. If maximum supported column name length is exceeded,",
            "        generate a truncated label by calling truncate_label().",
            "",
            "        :param label: expected expression label/alias",
            "        :return: conditionally mutated label supported by the db engine",
            "        \"\"\"",
            "        label_mutated = cls._mutate_label(label)",
            "        if (",
            "            cls.max_column_name_length",
            "            and len(label_mutated) > cls.max_column_name_length",
            "        ):",
            "            label_mutated = cls._truncate_label(label)",
            "        if cls.force_column_alias_quotes:",
            "            label_mutated = quoted_name(label_mutated, True)",
            "        return label_mutated",
            "",
            "    @classmethod",
            "    def get_column_types(",
            "        cls,",
            "        column_type: str | None,",
            "    ) -> tuple[TypeEngine, GenericDataType] | None:",
            "        \"\"\"",
            "        Return a sqlalchemy native column type and generic data type that corresponds",
            "        to the column type defined in the data source (return None to use default type",
            "        inferred by SQLAlchemy). Override `column_type_mappings` for specific needs",
            "        (see MSSQL for example of NCHAR/NVARCHAR handling).",
            "",
            "        :param column_type: Column type returned by inspector",
            "        :return: SQLAlchemy and generic Superset column types",
            "        \"\"\"",
            "        if not column_type:",
            "            return None",
            "",
            "        for regex, sqla_type, generic_type in (",
            "            cls.column_type_mappings + cls._default_column_type_mappings",
            "        ):",
            "            match = regex.match(column_type)",
            "            if not match:",
            "                continue",
            "            if callable(sqla_type):",
            "                return sqla_type(match), generic_type",
            "            return sqla_type, generic_type",
            "        return None",
            "",
            "    @staticmethod",
            "    def _mutate_label(label: str) -> str:",
            "        \"\"\"",
            "        Most engines support mixed case aliases that can include numbers",
            "        and special characters, like commas, parentheses etc. For engines that",
            "        have restrictions on what types of aliases are supported, this method",
            "        can be overridden to ensure that labels conform to the engine's",
            "        limitations. Mutated labels should be deterministic (input label A always",
            "        yields output label X) and unique (input labels A and B don't yield the same",
            "        output label X).",
            "",
            "        :param label: Preferred expression label",
            "        :return: Conditionally mutated label",
            "        \"\"\"",
            "        return label",
            "",
            "    @classmethod",
            "    def _truncate_label(cls, label: str) -> str:",
            "        \"\"\"",
            "        In the case that a label exceeds the max length supported by the engine,",
            "        this method is used to construct a deterministic and unique label based on",
            "        the original label. By default, this returns a md5 hash of the original label,",
            "        conditionally truncated if the length of the hash exceeds the max column length",
            "        of the engine.",
            "",
            "        :param label: Expected expression label",
            "        :return: Truncated label",
            "        \"\"\"",
            "        label = md5_sha_from_str(label)",
            "        # truncate hash if it exceeds max length",
            "        if cls.max_column_name_length and len(label) > cls.max_column_name_length:",
            "            label = label[: cls.max_column_name_length]",
            "        return label",
            "",
            "    @classmethod",
            "    def column_datatype_to_string(",
            "        cls, sqla_column_type: TypeEngine, dialect: Dialect",
            "    ) -> str:",
            "        \"\"\"",
            "        Convert sqlalchemy column type to string representation.",
            "        By default, removes collation and character encoding info to avoid",
            "        unnecessarily long datatypes.",
            "",
            "        :param sqla_column_type: SqlAlchemy column type",
            "        :param dialect: Sqlalchemy dialect",
            "        :return: Compiled column type",
            "        \"\"\"",
            "        sqla_column_type = sqla_column_type.copy()",
            "        if hasattr(sqla_column_type, \"collation\"):",
            "            sqla_column_type.collation = None",
            "        if hasattr(sqla_column_type, \"charset\"):",
            "            sqla_column_type.charset = None",
            "        return sqla_column_type.compile(dialect=dialect).upper()",
            "",
            "    @classmethod",
            "    def get_function_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get a list of function names that are able to be called on the database.",
            "        Used for SQL Lab autocomplete.",
            "",
            "        :param database: The database to get functions for",
            "        :return: A list of function names useable in the database",
            "        \"\"\"",
            "        return []",
            "",
            "    @staticmethod",
            "    def pyodbc_rows_to_tuples(data: list[Any]) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "        Convert pyodbc.Row objects from `fetch_data` to tuples.",
            "",
            "        :param data: List of tuples or pyodbc.Row objects",
            "        :return: List of tuples",
            "        \"\"\"",
            "        if data and type(data[0]).__name__ == \"Row\":",
            "            data = [tuple(row) for row in data]",
            "        return data",
            "",
            "    @staticmethod",
            "    def mutate_db_for_connection_test(  # pylint: disable=unused-argument",
            "        database: Database,",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require passing additional parameters for validating database",
            "        connections. This method makes it possible to mutate the database instance prior",
            "        to testing if a connection is ok.",
            "",
            "        :param database: instance to be mutated",
            "        \"\"\"",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        Some databases require adding elements to connection parameters,",
            "        like passing certificates to `extra`. This can be done here.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        \"\"\"",
            "        extra: dict[str, Any] = {}",
            "        if database.extra:",
            "            try:",
            "                extra = json.loads(database.extra)",
            "            except json.JSONDecodeError as ex:",
            "                logger.error(ex, exc_info=True)",
            "                raise ex",
            "        return extra",
            "",
            "    @staticmethod",
            "    def update_params_from_encrypted_extra(  # pylint: disable=invalid-name",
            "        database: Database, params: dict[str, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require some sensitive information which do not conform to",
            "        the username:password syntax normally used by SQLAlchemy.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :param params: params to be updated",
            "        \"\"\"",
            "        if not database.encrypted_extra:",
            "            return",
            "        try:",
            "            encrypted_extra = json.loads(database.encrypted_extra)",
            "            params.update(encrypted_extra)",
            "        except json.JSONDecodeError as ex:",
            "            logger.error(ex, exc_info=True)",
            "            raise ex",
            "",
            "    @classmethod",
            "    def is_readonly_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"Pessimistic readonly, 100% sure statement won't mutate anything\"\"\"",
            "        return (",
            "            parsed_query.is_select()",
            "            or parsed_query.is_explain()",
            "            or parsed_query.is_show()",
            "        )",
            "",
            "    @classmethod",
            "    def is_select_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"",
            "        Determine if the statement should be considered as SELECT statement.",
            "        Some query dialects do not contain \"SELECT\" word in queries (eg. Kusto)",
            "        \"\"\"",
            "        return parsed_query.is_select()",
            "",
            "    @classmethod",
            "    def get_column_spec(  # pylint: disable=unused-argument",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> ColumnSpec | None:",
            "        \"\"\"",
            "        Get generic type related specs regarding a native column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        if col_types := cls.get_column_types(native_type):",
            "            column_type, generic_type = col_types",
            "            is_dttm = generic_type == GenericDataType.TEMPORAL",
            "            return ColumnSpec(",
            "                sqla_type=column_type, generic_type=generic_type, is_dttm=is_dttm",
            "            )",
            "        return None",
            "",
            "    @classmethod",
            "    def get_sqla_column_type(",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> TypeEngine | None:",
            "        \"\"\"",
            "        Converts native database type to sqlalchemy column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        column_spec = cls.get_column_spec(",
            "            native_type=native_type,",
            "            db_extra=db_extra,",
            "            source=source,",
            "        )",
            "        return column_spec.sqla_type if column_spec else None",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def prepare_cancel_query(cls, query: Query) -> None:",
            "        \"\"\"",
            "        Some databases may acquire the query cancelation id after the query",
            "        cancelation request has been received. For those cases, the db engine spec",
            "        can record the cancelation intent so that the query can either be stopped",
            "        prior to execution, or canceled once the query id is acquired.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def has_implicit_cancel(cls) -> bool:",
            "        \"\"\"",
            "        Return True if the live cursor handles the implicit cancelation of the query,",
            "        False otherwise.",
            "",
            "        :return: Whether the live cursor implicitly cancels the query",
            "        :see: handle_cursor",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def get_cancel_query_id(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Select identifiers from the database engine that uniquely identifies the",
            "        queries to cancel. The identifier is typically a session id, process id",
            "        or similar.",
            "",
            "        :param cursor: Cursor instance in which the query will be executed",
            "        :param query: Query instance",
            "        :return: Query identifier",
            "        \"\"\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def cancel_query(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "        cancel_query_id: str,",
            "    ) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Value returned by get_cancel_query_payload or set in",
            "        other life-cycle methods of the query",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def parse_sql(cls, sql: str) -> list[str]:",
            "        return [str(s).strip(\" ;\") for s in sqlparse.parse(sql)]",
            "",
            "    @classmethod",
            "    def get_impersonation_key(cls, user: User | None) -> Any:",
            "        \"\"\"",
            "        Construct an impersonation key, by default it's the given username.",
            "",
            "        :param user: logged-in user",
            "",
            "        :returns: username if given user is not null",
            "        \"\"\"",
            "        return user.username if user else None",
            "",
            "    @classmethod",
            "    def mask_encrypted_extra(cls, encrypted_extra: str | None) -> str | None:",
            "        \"\"\"",
            "        Mask ``encrypted_extra``.",
            "",
            "        This is used to remove any sensitive data in ``encrypted_extra`` when presenting",
            "        it to the user. For example, a private key might be replaced with a masked value",
            "        \"XXXXXXXXXX\". If the masked value is changed the corresponding entry is updated,",
            "        otherwise the old value is used (see ``unmask_encrypted_extra`` below).",
            "        \"\"\"",
            "        return encrypted_extra",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def unmask_encrypted_extra(cls, old: str | None, new: str | None) -> str | None:",
            "        \"\"\"",
            "        Remove masks from ``encrypted_extra``.",
            "",
            "        This method allows reusing existing values from the current encrypted extra on",
            "        updates. It's useful for reusing masked passwords, allowing keys to be updated",
            "        without having to provide sensitive data to the client.",
            "        \"\"\"",
            "        return new",
            "",
            "    @classmethod",
            "    def get_public_information(cls) -> dict[str, Any]:",
            "        \"\"\"",
            "        Construct a Dict with properties we want to expose.",
            "",
            "        :returns: Dict with properties of our class like supports_file_upload",
            "        and disable_ssh_tunneling",
            "        \"\"\"",
            "        return {",
            "            \"supports_file_upload\": cls.supports_file_upload,",
            "            \"disable_ssh_tunneling\": cls.disable_ssh_tunneling,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_database_uri(cls, sqlalchemy_uri: URL) -> None:",
            "        \"\"\"",
            "        Validates a database SQLAlchemy URI per engine spec.",
            "        Use this to implement a final validation for unwanted connection configuration",
            "",
            "        :param sqlalchemy_uri:",
            "        \"\"\"",
            "        if existing_disallowed := cls.disallow_uri_query_params.get(",
            "            sqlalchemy_uri.get_driver_name(), set()",
            "        ).intersection(sqlalchemy_uri.query):",
            "            raise ValueError(f\"Forbidden query parameter(s): {existing_disallowed}\")",
            "",
            "    @classmethod",
            "    def denormalize_name(cls, dialect: Dialect, name: str) -> str:",
            "        if (",
            "            hasattr(dialect, \"requires_name_normalize\")",
            "            and dialect.requires_name_normalize",
            "        ):",
            "            return dialect.denormalize_name(name)",
            "",
            "        return name",
            "",
            "",
            "# schema for adding a database by providing parameters instead of the",
            "# full SQLAlchemy URI",
            "class BasicParametersSchema(Schema):",
            "    username = fields.String(",
            "        required=True, allow_none=True, metadata={\"description\": __(\"Username\")}",
            "    )",
            "    password = fields.String(allow_none=True, metadata={\"description\": __(\"Password\")})",
            "    host = fields.String(",
            "        required=True, metadata={\"description\": __(\"Hostname or IP address\")}",
            "    )",
            "    port = fields.Integer(",
            "        required=True,",
            "        metadata={\"description\": __(\"Database port\")},",
            "        validate=Range(min=0, max=2**16, max_inclusive=False),",
            "    )",
            "    database = fields.String(",
            "        required=True, metadata={\"description\": __(\"Database name\")}",
            "    )",
            "    query = fields.Dict(",
            "        keys=fields.Str(),",
            "        values=fields.Raw(),",
            "        metadata={\"description\": __(\"Additional parameters\")},",
            "    )",
            "    encryption = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an encrypted connection to the database\")},",
            "    )",
            "    ssh = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an ssh tunnel connection to the database\")},",
            "    )",
            "",
            "",
            "class BasicParametersType(TypedDict, total=False):",
            "    username: str | None",
            "    password: str | None",
            "    host: str",
            "    port: int",
            "    database: str",
            "    query: dict[str, Any]",
            "    encryption: bool",
            "",
            "",
            "class BasicPropertiesType(TypedDict):",
            "    parameters: BasicParametersType",
            "",
            "",
            "class BasicParametersMixin:",
            "    \"\"\"",
            "    Mixin for configuring DB engine specs via a dictionary.",
            "",
            "    With this mixin the SQLAlchemy engine can be configured through",
            "    individual parameters, instead of the full SQLAlchemy URI. This",
            "    mixin is for the most common pattern of URI:",
            "",
            "        engine+driver://user:password@host:port/dbname[?key=value&key=value...]",
            "",
            "    \"\"\"",
            "",
            "    # schema describing the parameters used to configure the DB",
            "    parameters_schema = BasicParametersSchema()",
            "",
            "    # recommended driver name for the DB engine spec",
            "    default_driver = \"\"",
            "",
            "    # query parameter to enable encryption in the database connection",
            "    # for Postgres this would be `{\"sslmode\": \"verify-ca\"}`, eg.",
            "    encryption_parameters: dict[str, str] = {}",
            "",
            "    @classmethod",
            "    def build_sqlalchemy_uri(  # pylint: disable=unused-argument",
            "        cls,",
            "        parameters: BasicParametersType,",
            "        encrypted_extra: dict[str, str] | None = None,",
            "    ) -> str:",
            "        # make a copy so that we don't update the original",
            "        query = parameters.get(\"query\", {}).copy()",
            "        if parameters.get(\"encryption\"):",
            "            if not cls.encryption_parameters:",
            "                raise Exception(  # pylint: disable=broad-exception-raised",
            "                    \"Unable to build a URL with encryption enabled\"",
            "                )",
            "            query.update(cls.encryption_parameters)",
            "",
            "        return str(",
            "            URL.create(",
            "                f\"{cls.engine}+{cls.default_driver}\".rstrip(\"+\"),  # type: ignore",
            "                username=parameters.get(\"username\"),",
            "                password=parameters.get(\"password\"),",
            "                host=parameters[\"host\"],",
            "                port=parameters[\"port\"],",
            "                database=parameters[\"database\"],",
            "                query=query,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def get_parameters_from_uri(  # pylint: disable=unused-argument",
            "        cls, uri: str, encrypted_extra: dict[str, Any] | None = None",
            "    ) -> BasicParametersType:",
            "        url = make_url_safe(uri)",
            "        query = {",
            "            key: value",
            "            for (key, value) in url.query.items()",
            "            if (key, value) not in cls.encryption_parameters.items()",
            "        }",
            "        encryption = all(",
            "            item in url.query.items() for item in cls.encryption_parameters.items()",
            "        )",
            "        return {",
            "            \"username\": url.username,",
            "            \"password\": url.password,",
            "            \"host\": url.host,",
            "            \"port\": url.port,",
            "            \"database\": url.database,",
            "            \"query\": query,",
            "            \"encryption\": encryption,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_parameters(",
            "        cls, properties: BasicPropertiesType",
            "    ) -> list[SupersetError]:",
            "        \"\"\"",
            "        Validates any number of parameters, for progressive validation.",
            "",
            "        If only the hostname is present it will check if the name is resolvable. As more",
            "        parameters are present in the request, more validation is done.",
            "        \"\"\"",
            "        errors: list[SupersetError] = []",
            "",
            "        required = {\"host\", \"port\", \"username\", \"database\"}",
            "        parameters = properties.get(\"parameters\", {})",
            "        present = {key for key in parameters if parameters.get(key, ())}",
            "",
            "        if missing := sorted(required - present):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=f'One or more parameters are missing: {\", \".join(missing)}',",
            "                    error_type=SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR,",
            "                    level=ErrorLevel.WARNING,",
            "                    extra={\"missing\": missing},",
            "                ),",
            "            )",
            "",
            "        host = parameters.get(\"host\", None)",
            "        if not host:",
            "            return errors",
            "        if not is_hostname_valid(host):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The hostname provided can't be resolved.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"host\"]},",
            "                ),",
            "            )",
            "            return errors",
            "",
            "        port = parameters.get(\"port\", None)",
            "        if not port:",
            "            return errors",
            "        try:",
            "            port = int(port)",
            "        except (ValueError, TypeError):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"Port must be a valid integer.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        if not (isinstance(port, int) and 0 <= port < 2**16):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=(",
            "                        \"The port must be an integer between 0 and 65535 \"",
            "                        \"(inclusive).\"",
            "                    ),",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        elif not is_port_open(host, port):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The port is closed.\",",
            "                    error_type=SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "",
            "        return errors",
            "",
            "    @classmethod",
            "    def parameters_json_schema(cls) -> Any:",
            "        \"\"\"",
            "        Return configuration parameters as OpenAPI.",
            "        \"\"\"",
            "        if not cls.parameters_schema:",
            "            return None",
            "",
            "        spec = APISpec(",
            "            title=\"Database Parameters\",",
            "            version=\"1.0.0\",",
            "            openapi_version=\"3.0.2\",",
            "            plugins=[MarshmallowPlugin()],",
            "        )",
            "        spec.components.schema(cls.__name__, schema=cls.parameters_schema)",
            "        return spec.to_dict()[\"components\"][\"schemas\"][cls.__name__]"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "",
            "from __future__ import annotations",
            "",
            "import json",
            "import logging",
            "import re",
            "from datetime import datetime",
            "from re import Match, Pattern",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    cast,",
            "    ContextManager,",
            "    NamedTuple,",
            "    TYPE_CHECKING,",
            "    TypedDict,",
            "    Union,",
            ")",
            "",
            "import pandas as pd",
            "import sqlparse",
            "from apispec import APISpec",
            "from apispec.ext.marshmallow import MarshmallowPlugin",
            "from deprecation import deprecated",
            "from flask import current_app",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from marshmallow import fields, Schema",
            "from marshmallow.validate import Range",
            "from sqlalchemy import column, select, types",
            "from sqlalchemy.engine.base import Engine",
            "from sqlalchemy.engine.interfaces import Compiled, Dialect",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.ext.compiler import compiles",
            "from sqlalchemy.sql import literal_column, quoted_name, text",
            "from sqlalchemy.sql.expression import ColumnClause, Select, TextAsFrom, TextClause",
            "from sqlalchemy.types import TypeEngine",
            "from sqlparse.tokens import CTE",
            "",
            "from superset import security_manager, sql_parse",
            "from superset.constants import TimeGrain as TimeGrainConstants",
            "from superset.databases.utils import make_url_safe",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.sql_parse import ParsedQuery, SQLScript, Table",
            "from superset.superset_typing import ResultSetColumnType, SQLAColumnType",
            "from superset.utils import core as utils",
            "from superset.utils.core import ColumnSpec, GenericDataType",
            "from superset.utils.hashing import md5_sha_from_str",
            "from superset.utils.network import is_hostname_valid, is_port_open",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import TableColumn",
            "    from superset.models.core import Database",
            "    from superset.models.sql_lab import Query",
            "",
            "ColumnTypeMapping = tuple[",
            "    Pattern[str],",
            "    Union[TypeEngine, Callable[[Match[str]], TypeEngine]],",
            "    GenericDataType,",
            "]",
            "",
            "logger = logging.getLogger()",
            "",
            "",
            "def convert_inspector_columns(cols: list[SQLAColumnType]) -> list[ResultSetColumnType]:",
            "    result_set_columns: list[ResultSetColumnType] = []",
            "    for col in cols:",
            "        result_set_columns.append({\"column_name\": col.get(\"name\"), **col})  # type: ignore",
            "    return result_set_columns",
            "",
            "",
            "class TimeGrain(NamedTuple):",
            "    name: str  # TODO: redundant field, remove",
            "    label: str",
            "    function: str",
            "    duration: str | None",
            "",
            "",
            "builtin_time_grains: dict[str | None, str] = {",
            "    TimeGrainConstants.SECOND: __(\"Second\"),",
            "    TimeGrainConstants.FIVE_SECONDS: __(\"5 second\"),",
            "    TimeGrainConstants.THIRTY_SECONDS: __(\"30 second\"),",
            "    TimeGrainConstants.MINUTE: __(\"Minute\"),",
            "    TimeGrainConstants.FIVE_MINUTES: __(\"5 minute\"),",
            "    TimeGrainConstants.TEN_MINUTES: __(\"10 minute\"),",
            "    TimeGrainConstants.FIFTEEN_MINUTES: __(\"15 minute\"),",
            "    TimeGrainConstants.THIRTY_MINUTES: __(\"30 minute\"),",
            "    TimeGrainConstants.HOUR: __(\"Hour\"),",
            "    TimeGrainConstants.SIX_HOURS: __(\"6 hour\"),",
            "    TimeGrainConstants.DAY: __(\"Day\"),",
            "    TimeGrainConstants.WEEK: __(\"Week\"),",
            "    TimeGrainConstants.MONTH: __(\"Month\"),",
            "    TimeGrainConstants.QUARTER: __(\"Quarter\"),",
            "    TimeGrainConstants.YEAR: __(\"Year\"),",
            "    TimeGrainConstants.WEEK_STARTING_SUNDAY: __(\"Week starting Sunday\"),",
            "    TimeGrainConstants.WEEK_STARTING_MONDAY: __(\"Week starting Monday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SATURDAY: __(\"Week ending Saturday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SUNDAY: __(\"Week ending Sunday\"),",
            "}",
            "",
            "",
            "class TimestampExpression(",
            "    ColumnClause",
            "):  # pylint: disable=abstract-method, too-many-ancestors",
            "    def __init__(self, expr: str, col: ColumnClause, **kwargs: Any) -> None:",
            "        \"\"\"Sqlalchemy class that can be used to render native column elements respecting",
            "        engine-specific quoting rules as part of a string-based expression.",
            "",
            "        :param expr: Sql expression with '{col}' denoting the locations where the col",
            "        object will be rendered.",
            "        :param col: the target column",
            "        \"\"\"",
            "        super().__init__(expr, **kwargs)",
            "        self.col = col",
            "",
            "    @property",
            "    def _constructor(self) -> ColumnClause:",
            "        # Needed to ensure that the column label is rendered correctly when",
            "        # proxied to the outer query.",
            "        # See https://github.com/sqlalchemy/sqlalchemy/issues/4730",
            "        return ColumnClause",
            "",
            "",
            "@compiles(TimestampExpression)",
            "def compile_timegrain_expression(",
            "    element: TimestampExpression, compiler: Compiled, **kwargs: Any",
            ") -> str:",
            "    return element.name.replace(\"{col}\", compiler.process(element.col, **kwargs))",
            "",
            "",
            "class LimitMethod:  # pylint: disable=too-few-public-methods",
            "    \"\"\"Enum the ways that limits can be applied\"\"\"",
            "",
            "    FETCH_MANY = \"fetch_many\"",
            "    WRAP_SQL = \"wrap_sql\"",
            "    FORCE_LIMIT = \"force_limit\"",
            "",
            "",
            "class MetricType(TypedDict, total=False):",
            "    \"\"\"",
            "    Type for metrics return by `get_metrics`.",
            "    \"\"\"",
            "",
            "    metric_name: str",
            "    expression: str",
            "    verbose_name: str | None",
            "    metric_type: str | None",
            "    description: str | None",
            "    d3format: str | None",
            "    currency: str | None",
            "    warning_text: str | None",
            "    extra: str | None",
            "",
            "",
            "class BaseEngineSpec:  # pylint: disable=too-many-public-methods",
            "    \"\"\"Abstract class for database engine specific configurations",
            "",
            "    Attributes:",
            "        allows_alias_to_source_column: Whether the engine is able to pick the",
            "                                       source column for aggregation clauses",
            "                                       used in ORDER BY when a column in SELECT",
            "                                       has an alias that is the same as a source",
            "                                       column.",
            "        allows_hidden_orderby_agg:     Whether the engine allows ORDER BY to",
            "                                       directly use aggregation clauses, without",
            "                                       having to add the same aggregation in SELECT.",
            "    \"\"\"",
            "",
            "    engine_name: str | None = None  # for user messages, overridden in child classes",
            "",
            "    # These attributes map the DB engine spec to one or more SQLAlchemy dialects/drivers;",
            "    # see the ``supports_url`` and ``supports_backend`` methods below.",
            "    engine = \"base\"  # str as defined in sqlalchemy.engine.engine",
            "    engine_aliases: set[str] = set()",
            "    drivers: dict[str, str] = {}",
            "    default_driver: str | None = None",
            "",
            "    # placeholder with the SQLAlchemy URI template",
            "    sqlalchemy_uri_placeholder = (",
            "        \"engine+driver://user:password@host:port/dbname[?key=value&key=value...]\"",
            "    )",
            "",
            "    disable_ssh_tunneling = False",
            "",
            "    _date_trunc_functions: dict[str, str] = {}",
            "    _time_grain_expressions: dict[str | None, str] = {}",
            "    _default_column_type_mappings: tuple[ColumnTypeMapping, ...] = (",
            "        (",
            "            re.compile(r\"^string\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^n((var)?char|text)\", re.IGNORECASE),",
            "            types.UnicodeText(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(var)?char\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(tiny|medium|long)?text\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallint\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^int(eger)?\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigint\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^long\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^decimal\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^numeric\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^float\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^double\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^real\", re.IGNORECASE),",
            "            types.REAL,",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallserial\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^serial\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigserial\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^money\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^timestamp\", re.IGNORECASE),",
            "            types.TIMESTAMP(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^datetime\", re.IGNORECASE),",
            "            types.DateTime(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^date\", re.IGNORECASE),",
            "            types.Date(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^time\", re.IGNORECASE),",
            "            types.Time(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^interval\", re.IGNORECASE),",
            "            types.Interval(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^bool(ean)?\", re.IGNORECASE),",
            "            types.Boolean(),",
            "            GenericDataType.BOOLEAN,",
            "        ),",
            "    )",
            "    # engine-specific type mappings to check prior to the defaults",
            "    column_type_mappings: tuple[ColumnTypeMapping, ...] = ()",
            "",
            "    # type-specific functions to mutate values received from the database.",
            "    # Needed on certain databases that return values in an unexpected format",
            "    column_type_mutators: dict[TypeEngine, Callable[[Any], Any]] = {}",
            "",
            "    # Does database support join-free timeslot grouping",
            "    time_groupby_inline = False",
            "    limit_method = LimitMethod.FORCE_LIMIT",
            "    allows_joins = True",
            "    allows_subqueries = True",
            "    allows_alias_in_select = True",
            "    allows_alias_in_orderby = True",
            "    allows_sql_comments = True",
            "    allows_escaped_colons = True",
            "",
            "    # Whether ORDER BY clause can use aliases created in SELECT",
            "    # that are the same as a source column",
            "    allows_alias_to_source_column = True",
            "",
            "    # Whether ORDER BY clause must appear in SELECT",
            "    # if True, then it doesn't have to.",
            "    allows_hidden_orderby_agg = True",
            "",
            "    # Whether ORDER BY clause can use sql calculated expression",
            "    # if True, use alias of select column for `order by`",
            "    # the True is safely for most database",
            "    # But for backward compatibility, False by default",
            "    allows_hidden_cc_in_orderby = False",
            "",
            "    # Whether allow CTE as subquery or regular CTE",
            "    # If True, then it will allow  in subquery ,",
            "    # if False it will allow as regular CTE",
            "    allows_cte_in_subquery = True",
            "    # Define alias for CTE",
            "    cte_alias = \"__cte\"",
            "    # Whether allow LIMIT clause in the SQL",
            "    # If True, then the database engine is allowed for LIMIT clause",
            "    # If False, then the database engine is allowed for TOP clause",
            "    allow_limit_clause = True",
            "    # This set will give keywords for select statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    select_keywords: set[str] = {\"SELECT\"}",
            "    # This set will give the keywords for data limit statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    top_keywords: set[str] = {\"TOP\"}",
            "    # A set of disallowed connection query parameters by driver name",
            "    disallow_uri_query_params: dict[str, set[str]] = {}",
            "    # A Dict of query parameters that will always be used on every connection",
            "    # by driver name",
            "    enforce_uri_query_params: dict[str, dict[str, Any]] = {}",
            "",
            "    force_column_alias_quotes = False",
            "    arraysize = 0",
            "    max_column_name_length: int | None = None",
            "    try_remove_schema_from_table_name = True  # pylint: disable=invalid-name",
            "    run_multiple_statements_as_one = False",
            "    custom_errors: dict[",
            "        Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]",
            "    ] = {}",
            "",
            "    # Whether the engine supports file uploads",
            "    # if True, database will be listed as option in the upload file form",
            "    supports_file_upload = True",
            "",
            "    # Is the DB engine spec able to change the default schema? This requires implementing",
            "    # a custom `adjust_engine_params` method.",
            "    supports_dynamic_schema = False",
            "",
            "    # Does the DB support catalogs? A catalog here is a group of schemas, and has",
            "    # different names depending on the DB: BigQuery calles it a \"project\", Postgres calls",
            "    # it a \"database\", Trino calls it a \"catalog\", etc.",
            "    supports_catalog = False",
            "",
            "    # Can the catalog be changed on a per-query basis?",
            "    supports_dynamic_catalog = False",
            "",
            "    @classmethod",
            "    def get_allows_alias_in_select(",
            "        cls, database: Database  # pylint: disable=unused-argument",
            "    ) -> bool:",
            "        \"\"\"",
            "        Method for dynamic `allows_alias_in_select`.",
            "",
            "        In Dremio this atribute is version-dependent, so Superset needs to inspect the",
            "        database configuration in order to determine it. This method allows engine-specs",
            "        to define dynamic values for the attribute.",
            "        \"\"\"",
            "        return cls.allows_alias_in_select",
            "",
            "    @classmethod",
            "    def supports_url(cls, url: URL) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy URL.",
            "",
            "        As an example, if a given DB engine spec has:",
            "",
            "            class PostgresDBEngineSpec:",
            "                engine = \"postgresql\"",
            "                engine_aliases = \"postgres\"",
            "                drivers = {",
            "                    \"psycopg2\": \"The default Postgres driver\",",
            "                    \"asyncpg\": \"An asynchronous Postgres driver\",",
            "                }",
            "",
            "        It would be used for all the following SQLAlchemy URIs:",
            "",
            "            - postgres://user:password@host/db",
            "            - postgresql://user:password@host/db",
            "            - postgres+asyncpg://user:password@host/db",
            "            - postgres+psycopg2://user:password@host/db",
            "            - postgresql+asyncpg://user:password@host/db",
            "            - postgresql+psycopg2://user:password@host/db",
            "",
            "        Note that SQLAlchemy has a default driver even if one is not specified:",
            "",
            "            >>> from sqlalchemy.engine.url import make_url",
            "            >>> make_url('postgres://').get_driver_name()",
            "            'psycopg2'",
            "",
            "        \"\"\"",
            "        backend = url.get_backend_name()",
            "        driver = url.get_driver_name()",
            "        return cls.supports_backend(backend, driver)",
            "",
            "    @classmethod",
            "    def supports_backend(cls, backend: str, driver: str | None = None) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy backend/driver.",
            "        \"\"\"",
            "        # check the backend first",
            "        if backend != cls.engine and backend not in cls.engine_aliases:",
            "            return False",
            "",
            "        # originally DB engine specs didn't declare any drivers and the check was made",
            "        # only on the engine; if that's the case, ignore the driver for backwards",
            "        # compatibility",
            "        if not cls.drivers or driver is None:",
            "            return True",
            "",
            "        return driver in cls.drivers",
            "",
            "    @classmethod",
            "    def get_default_schema(cls, database: Database) -> str | None:",
            "        \"\"\"",
            "        Return the default schema in a given database.",
            "        \"\"\"",
            "        with database.get_inspector_with_context() as inspector:",
            "            return inspector.default_schema_name",
            "",
            "    @classmethod",
            "    def get_schema_from_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        sqlalchemy_uri: URL,",
            "        connect_args: dict[str, Any],",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the schema configured in a SQLALchemy URI and connection arguments, if any.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def get_default_schema_for_query(",
            "        cls,",
            "        database: Database,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the default schema for a given query.",
            "",
            "        This is used to determine the schema of tables that aren't fully qualified, eg:",
            "",
            "            SELECT * FROM foo;",
            "",
            "        In the example above, the schema where the `foo` table lives depends on a few",
            "        factors:",
            "",
            "            1. For DB engine specs that allow dynamically changing the schema based on the",
            "               query we should use the query schema.",
            "            2. For DB engine specs that don't support dynamically changing the schema and",
            "               have the schema hardcoded in the SQLAlchemy URI we should use the schema",
            "               from the URI.",
            "            3. For DB engine specs that don't connect to a specific schema and can't",
            "               change it dynamically we need to probe the database for the default schema.",
            "",
            "        Determining the correct schema is crucial for managing access to data, so please",
            "        make sure you understand this logic when working on a new DB engine spec.",
            "        \"\"\"",
            "        # dynamic schema varies on a per-query basis",
            "        if cls.supports_dynamic_schema:",
            "            return query.schema",
            "",
            "        # check if the schema is stored in the SQLAlchemy URI or connection arguments",
            "        try:",
            "            connect_args = database.get_extra()[\"engine_params\"][\"connect_args\"]",
            "        except KeyError:",
            "            connect_args = {}",
            "        sqlalchemy_uri = make_url_safe(database.sqlalchemy_uri)",
            "        if schema := cls.get_schema_from_engine_params(sqlalchemy_uri, connect_args):",
            "            return schema",
            "",
            "        # return the default schema of the database",
            "        return cls.get_default_schema(database)",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific exceptions into",
            "        Superset DBAPI exceptions",
            "",
            "        Note: On python 3.9 this method can be changed to a classmethod property",
            "        without the need of implementing a metaclass type",
            "",
            "        :return: A map of driver specific exception to superset custom exceptions",
            "        \"\"\"",
            "        return {}",
            "",
            "    @classmethod",
            "    def parse_error_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific parser method",
            "",
            "        :return: An Exception with a parsed string off the original exception",
            "        \"\"\"",
            "        return exception",
            "",
            "    @classmethod",
            "    def get_dbapi_mapped_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Get a superset custom DBAPI exception from the driver specific exception.",
            "",
            "        Override if the engine needs to perform extra changes to the exception, for",
            "        example change the exception message or implement custom more complex logic",
            "",
            "        :param exception: The driver specific exception",
            "        :return: Superset custom DBAPI exception",
            "        \"\"\"",
            "        new_exception = cls.get_dbapi_exception_mapping().get(type(exception))",
            "        if not new_exception:",
            "            return cls.parse_error_exception(exception)",
            "        return new_exception(str(exception))",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(  # pylint: disable=unused-argument",
            "        cls,",
            "        extra: dict[str, Any],",
            "    ) -> bool:",
            "        return False",
            "",
            "    @classmethod",
            "    def get_text_clause(cls, clause: str) -> TextClause:",
            "        \"\"\"",
            "        SQLAlchemy wrapper to ensure text clauses are escaped properly",
            "",
            "        :param clause: string clause with potentially unescaped characters",
            "        :return: text clause with escaped characters",
            "        \"\"\"",
            "        if cls.allows_escaped_colons:",
            "            clause = clause.replace(\":\", \"\\\\:\")",
            "        return text(clause)",
            "",
            "    @classmethod",
            "    def get_engine(",
            "        cls,",
            "        database: Database,",
            "        schema: str | None = None,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> ContextManager[Engine]:",
            "        \"\"\"",
            "        Return an engine context manager.",
            "",
            "            >>> with DBEngineSpec.get_engine(database, schema, source) as engine:",
            "            ...     connection = engine.connect()",
            "            ...     connection.execute(sql)",
            "",
            "        \"\"\"",
            "        return database.get_sqla_engine_with_context(schema=schema, source=source)",
            "",
            "    @classmethod",
            "    def get_timestamp_expr(",
            "        cls,",
            "        col: ColumnClause,",
            "        pdf: str | None,",
            "        time_grain: str | None,",
            "    ) -> TimestampExpression:",
            "        \"\"\"",
            "        Construct a TimestampExpression to be used in a SQLAlchemy query.",
            "",
            "        :param col: Target column for the TimestampExpression",
            "        :param pdf: date format (seconds or milliseconds)",
            "        :param time_grain: time grain, e.g. P1Y for 1 year",
            "        :return: TimestampExpression object",
            "        \"\"\"",
            "        if time_grain:",
            "            type_ = str(getattr(col, \"type\", \"\"))",
            "            time_expr = cls.get_time_grain_expressions().get(time_grain)",
            "            if not time_expr:",
            "                raise NotImplementedError(",
            "                    f\"No grain spec for {time_grain} for database {cls.engine}\"",
            "                )",
            "            if type_ and \"{func}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{func}\", date_trunc_function)",
            "            if type_ and \"{type}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{type}\", type_)",
            "        else:",
            "            time_expr = \"{col}\"",
            "",
            "        # if epoch, translate to DATE using db specific conf",
            "        if pdf == \"epoch_s\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_to_dttm())",
            "        elif pdf == \"epoch_ms\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_ms_to_dttm())",
            "",
            "        return TimestampExpression(time_expr, col, type_=col.type)",
            "",
            "    @classmethod",
            "    def get_time_grains(cls) -> tuple[TimeGrain, ...]:",
            "        \"\"\"",
            "        Generate a tuple of supported time grains.",
            "",
            "        :return: All time grains supported by the engine",
            "        \"\"\"",
            "",
            "        ret_list = []",
            "        time_grains = builtin_time_grains.copy()",
            "        time_grains.update(current_app.config[\"TIME_GRAIN_ADDONS\"])",
            "        for duration, func in cls.get_time_grain_expressions().items():",
            "            if duration in time_grains:",
            "                name = time_grains[duration]",
            "                ret_list.append(TimeGrain(name, _(name), func, duration))",
            "        return tuple(ret_list)",
            "",
            "    @classmethod",
            "    def _sort_time_grains(",
            "        cls, val: tuple[str | None, str], index: int",
            "    ) -> float | int | str:",
            "        \"\"\"",
            "        Return an ordered time-based value of a portion of a time grain",
            "        for sorting",
            "        Values are expected to be either None or start with P or PT",
            "        Have a numerical value in the middle and end with",
            "        a value for the time interval",
            "        It can also start or end with epoch start time denoting a range",
            "        i.e, week beginning or ending with a day",
            "        \"\"\"",
            "        pos = {",
            "            \"FIRST\": 0,",
            "            \"SECOND\": 1,",
            "            \"THIRD\": 2,",
            "            \"LAST\": 3,",
            "        }",
            "",
            "        if val[0] is None:",
            "            return pos[\"FIRST\"]",
            "",
            "        prog = re.compile(r\"(.*\\/)?(P|PT)([0-9\\.]+)(S|M|H|D|W|M|Y)(\\/.*)?\")",
            "        result = prog.match(val[0])",
            "",
            "        # for any time grains that don't match the format, put them at the end",
            "        if result is None:",
            "            return pos[\"LAST\"]",
            "",
            "        second_minute_hour = [\"S\", \"M\", \"H\"]",
            "        day_week_month_year = [\"D\", \"W\", \"M\", \"Y\"]",
            "        is_less_than_day = result.group(2) == \"PT\"",
            "        interval = result.group(4)",
            "        epoch_time_start_string = result.group(1) or result.group(5)",
            "        has_starting_or_ending = bool(len(epoch_time_start_string or \"\"))",
            "",
            "        def sort_day_week() -> int:",
            "            if has_starting_or_ending:",
            "                return pos[\"LAST\"]",
            "            if is_less_than_day:",
            "                return pos[\"SECOND\"]",
            "            return pos[\"THIRD\"]",
            "",
            "        def sort_interval() -> float:",
            "            if is_less_than_day:",
            "                return second_minute_hour.index(interval)",
            "            return day_week_month_year.index(interval)",
            "",
            "        # 0: all \"PT\" values should come before \"P\" values (i.e, PT10M)",
            "        # 1: order values within the above arrays (\"D\" before \"W\")",
            "        # 2: sort by numeric value (PT10M before PT15M)",
            "        # 3: sort by any week starting/ending values",
            "        plist = {",
            "            0: sort_day_week(),",
            "            1: pos[\"SECOND\"] if is_less_than_day else pos[\"THIRD\"],",
            "            2: sort_interval(),",
            "            3: float(result.group(3)),",
            "        }",
            "",
            "        return plist.get(index, 0)",
            "",
            "    @classmethod",
            "    def get_time_grain_expressions(cls) -> dict[str | None, str]:",
            "        \"\"\"",
            "        Return a dict of all supported time grains including any potential added grains",
            "        but excluding any potentially disabled grains in the config file.",
            "",
            "        :return: All time grain expressions supported by the engine",
            "        \"\"\"",
            "        # TODO: use @memoize decorator or similar to avoid recomputation on every call",
            "        time_grain_expressions = cls._time_grain_expressions.copy()",
            "        grain_addon_expressions = current_app.config[\"TIME_GRAIN_ADDON_EXPRESSIONS\"]",
            "        time_grain_expressions.update(grain_addon_expressions.get(cls.engine, {}))",
            "        denylist: list[str] = current_app.config[\"TIME_GRAIN_DENYLIST\"]",
            "        for key in denylist:",
            "            time_grain_expressions.pop(key, None)",
            "",
            "        return dict(",
            "            sorted(",
            "                time_grain_expressions.items(),",
            "                key=lambda x: (",
            "                    cls._sort_time_grains(x, 0),",
            "                    cls._sort_time_grains(x, 1),",
            "                    cls._sort_time_grains(x, 2),",
            "                    cls._sort_time_grains(x, 3),",
            "                ),",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "",
            "        :param cursor: Cursor instance",
            "        :param limit: Maximum number of rows to be returned by the cursor",
            "        :return: Result of query",
            "        \"\"\"",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            if cls.limit_method == LimitMethod.FETCH_MANY and limit:",
            "                return cursor.fetchmany(limit)",
            "            data = cursor.fetchall()",
            "            description = cursor.description or []",
            "            # Create a mapping between column name and a mutator function to normalize",
            "            # values with. The first two items in the description row are",
            "            # the column name and type.",
            "            column_mutators = {",
            "                row[0]: func",
            "                for row in description",
            "                if (",
            "                    func := cls.column_type_mutators.get(",
            "                        type(cls.get_sqla_column_type(cls.get_datatype(row[1])))",
            "                    )",
            "                )",
            "            }",
            "            if column_mutators:",
            "                indexes = {row[0]: idx for idx, row in enumerate(description)}",
            "                for row_idx, row in enumerate(data):",
            "                    new_row = list(row)",
            "                    for col, func in column_mutators.items():",
            "                        col_idx = indexes[col]",
            "                        new_row[col_idx] = func(row[col_idx])",
            "                    data[row_idx] = tuple(new_row)",
            "",
            "            return data",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def expand_data(",
            "        cls, columns: list[ResultSetColumnType], data: list[dict[Any, Any]]",
            "    ) -> tuple[",
            "        list[ResultSetColumnType], list[dict[Any, Any]], list[ResultSetColumnType]",
            "    ]:",
            "        \"\"\"",
            "        Some engines support expanding nested fields. See implementation in Presto",
            "        spec for details.",
            "",
            "        :param columns: columns selected in the query",
            "        :param data: original data set",
            "        :return: list of all columns(selected columns and their nested fields),",
            "                 expanded data set, listed of nested fields",
            "        \"\"\"",
            "        return columns, data, []",
            "",
            "    @classmethod",
            "    def alter_new_orm_column(cls, orm_col: TableColumn) -> None:",
            "        \"\"\"Allow altering default column attributes when first detected/added",
            "",
            "        For instance special column like `__time` for Druid can be",
            "        set to is_dttm=True. Note that this only gets called when new",
            "        columns are detected/created\"\"\"",
            "        # TODO: Fix circular import caused by importing TableColumn",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (seconds) to datetime that can be used in a",
            "        query. The reference column should be denoted as `{col}` in the return",
            "        expression, e.g. \"FROM_UNIXTIME({col})\"",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @classmethod",
            "    def epoch_ms_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (milliseconds) to datetime that can be used",
            "        in a query.",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        return cls.epoch_to_dttm().replace(\"{col}\", \"({col}/1000)\")",
            "",
            "    @classmethod",
            "    def get_datatype(cls, type_code: Any) -> str | None:",
            "        \"\"\"",
            "        Change column type code from cursor description to string representation.",
            "",
            "        :param type_code: Type code from cursor description",
            "        :return: String representation of type code",
            "        \"\"\"",
            "        if isinstance(type_code, str) and type_code != \"\":",
            "            return type_code.upper()",
            "        return None",
            "",
            "    @classmethod",
            "    @deprecated(deprecated_in=\"3.0\")",
            "    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Normalizes indexes for more consistency across db engines",
            "",
            "        noop by default",
            "",
            "        :param indexes: Raw indexes as returned by SQLAlchemy",
            "        :return: cleaner, more aligned index definition",
            "        \"\"\"",
            "        return indexes",
            "",
            "    @classmethod",
            "    def extra_table_metadata(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        schema_name: str | None,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Returns engine-specific table metadata",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name",
            "        :param schema_name: Schema name",
            "        :return: Engine-specific table metadata",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        return {}",
            "",
            "    @classmethod",
            "    def apply_limit_to_sql(",
            "        cls, sql: str, limit: int, database: Database, force: bool = False",
            "    ) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a LIMIT clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param database: Database instance",
            "        :return: SQL query with limit clause",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        if cls.limit_method == LimitMethod.WRAP_SQL:",
            "            sql = sql.strip(\"\\t\\n ;\")",
            "            qry = (",
            "                select(\"*\")",
            "                .select_from(TextAsFrom(text(sql), [\"*\"]).alias(\"inner_qry\"))",
            "                .limit(limit)",
            "            )",
            "            return database.compile_sqla_query(qry)",
            "",
            "        if cls.limit_method == LimitMethod.FORCE_LIMIT:",
            "            parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "            sql = parsed_query.set_or_update_query_limit(limit, force=force)",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def apply_top_to_sql(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a TOP clause",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param sql: SQL query",
            "        :return: SQL query with top clause",
            "        \"\"\"",
            "",
            "        cte = None",
            "        sql_remainder = None",
            "        sql = sql.strip(\" \\t\\n;\")",
            "        sql_statement = sqlparse.format(sql, strip_comments=True)",
            "        query_limit: int | None = sql_parse.extract_top_from_query(",
            "            sql_statement, cls.top_keywords",
            "        )",
            "        if not limit:",
            "            final_limit = query_limit",
            "        elif int(query_limit or 0) < limit and query_limit is not None:",
            "            final_limit = query_limit",
            "        else:",
            "            final_limit = limit",
            "        if not cls.allows_cte_in_subquery:",
            "            cte, sql_remainder = sql_parse.get_cte_remainder_query(sql_statement)",
            "        if cte:",
            "            str_statement = str(sql_remainder)",
            "            cte = cte + \"\\n\"",
            "        else:",
            "            cte = \"\"",
            "            str_statement = str(sql)",
            "        str_statement = str_statement.replace(\"\\n\", \" \").replace(\"\\r\", \"\")",
            "",
            "        tokens = str_statement.rstrip().split(\" \")",
            "        tokens = [token for token in tokens if token]",
            "        if cls.top_not_in_sql(str_statement):",
            "            selects = [",
            "                i",
            "                for i, word in enumerate(tokens)",
            "                if word.upper() in cls.select_keywords",
            "            ]",
            "            first_select = selects[0]",
            "            if tokens[first_select + 1].upper() == \"DISTINCT\":",
            "                first_select += 1",
            "",
            "            tokens.insert(first_select + 1, \"TOP\")",
            "            tokens.insert(first_select + 2, str(final_limit))",
            "",
            "        next_is_limit_token = False",
            "        new_tokens = []",
            "",
            "        for token in tokens:",
            "            if token in cls.top_keywords:",
            "                next_is_limit_token = True",
            "            elif next_is_limit_token:",
            "                if token.isdigit():",
            "                    token = str(final_limit)",
            "                    next_is_limit_token = False",
            "            new_tokens.append(token)",
            "        sql = \" \".join(new_tokens)",
            "        return cte + sql",
            "",
            "    @classmethod",
            "    def top_not_in_sql(cls, sql: str) -> bool:",
            "        for top_word in cls.top_keywords:",
            "            if top_word.upper() in sql.upper():",
            "                return False",
            "        return True",
            "",
            "    @classmethod",
            "    def get_limit_from_sql(cls, sql: str) -> int | None:",
            "        \"\"\"",
            "        Extract limit from SQL query",
            "",
            "        :param sql: SQL query",
            "        :return: Value of limit clause in query",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.limit",
            "",
            "    @classmethod",
            "    def set_or_update_query_limit(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Create a query based on original query but with new limit clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: New limit to insert/replace into query",
            "        :return: Query with new limit",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.set_or_update_query_limit(limit)",
            "",
            "    @classmethod",
            "    def get_cte_query(cls, sql: str) -> str | None:",
            "        \"\"\"",
            "        Convert the input CTE based SQL to the SQL for virtual table conversion",
            "",
            "        :param sql: SQL query",
            "        :return: CTE with the main select query aliased as `__cte`",
            "",
            "        \"\"\"",
            "        if not cls.allows_cte_in_subquery:",
            "            stmt = sqlparse.parse(sql)[0]",
            "",
            "            # The first meaningful token for CTE will be with WITH",
            "            idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)",
            "            if not (token and token.ttype == CTE):",
            "                return None",
            "            idx, token = stmt.token_next(idx)",
            "            idx = stmt.token_index(token) + 1",
            "",
            "            # extract rest of the SQLs after CTE",
            "            remainder = \"\".join(str(token) for token in stmt.tokens[idx:]).strip()",
            "            return f\"WITH {token.value},\\n{cls.cte_alias} AS (\\n{remainder}\\n)\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def df_to_sql(",
            "        cls,",
            "        database: Database,",
            "        table: Table,",
            "        df: pd.DataFrame,",
            "        to_sql_kwargs: dict[str, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Upload data from a Pandas DataFrame to a database.",
            "",
            "        For regular engines this calls the `pandas.DataFrame.to_sql` method. Can be",
            "        overridden for engines that don't work well with this method, e.g. Hive and",
            "        BigQuery.",
            "",
            "        Note this method does not create metadata for the table.",
            "",
            "        :param database: The database to upload the data to",
            "        :param table: The table to upload the data to",
            "        :param df: The dataframe with data to be uploaded",
            "        :param to_sql_kwargs: The kwargs to be passed to pandas.DataFrame.to_sql` method",
            "        \"\"\"",
            "",
            "        to_sql_kwargs[\"name\"] = table.table",
            "",
            "        if table.schema:",
            "            # Only add schema when it is preset and non-empty.",
            "            to_sql_kwargs[\"schema\"] = table.schema",
            "",
            "        with cls.get_engine(database) as engine:",
            "            if engine.dialect.supports_multivalues_insert:",
            "                to_sql_kwargs[\"method\"] = \"multi\"",
            "",
            "            df.to_sql(con=engine, **to_sql_kwargs)",
            "",
            "    @classmethod",
            "    def convert_dttm(  # pylint: disable=unused-argument",
            "        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Convert a Python `datetime` object to a SQL expression.",
            "",
            "        :param target_type: The target type of expression",
            "        :param dttm: The datetime object",
            "        :param db_extra: The database extra object",
            "        :return: The SQL expression",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def handle_cursor(cls, cursor: Any, query: Query) -> None:",
            "        \"\"\"Handle a live cursor between the execute and fetchall calls",
            "",
            "        The flow works without this method doing anything, but it allows",
            "        for handling the cursor and updating progress information in the",
            "        query object\"\"\"",
            "        # TODO: Fix circular import error caused by importing sql_lab.Query",
            "",
            "    @classmethod",
            "    def execute_with_cursor(cls, cursor: Any, sql: str, query: Query) -> None:",
            "        \"\"\"",
            "        Trigger execution of a query and handle the resulting cursor.",
            "",
            "        For most implementations this just makes calls to `execute` and",
            "        `handle_cursor` consecutively, but in some engines (e.g. Trino) we may",
            "        need to handle client limitations such as lack of async support and",
            "        perform a more complicated operation to get information from the cursor",
            "        in a timely manner and facilitate operations such as query stop",
            "        \"\"\"",
            "        logger.debug(\"Query %d: Running query: %s\", query.id, sql)",
            "        cls.execute(cursor, sql, async_=True)",
            "        logger.debug(\"Query %d: Handling cursor\", query.id)",
            "        cls.handle_cursor(cursor, query)",
            "",
            "    @classmethod",
            "    def extract_error_message(cls, ex: Exception) -> str:",
            "        return f\"{cls.engine} error: {cls._extract_error_message(ex)}\"",
            "",
            "    @classmethod",
            "    def _extract_error_message(cls, ex: Exception) -> str:",
            "        \"\"\"Extract error message for queries\"\"\"",
            "        return utils.error_msg_from_exception(ex)",
            "",
            "    @classmethod",
            "    def extract_errors(",
            "        cls, ex: Exception, context: dict[str, Any] | None = None",
            "    ) -> list[SupersetError]:",
            "        raw_message = cls._extract_error_message(ex)",
            "",
            "        context = context or {}",
            "        for regex, (message, error_type, extra) in cls.custom_errors.items():",
            "            if match := regex.search(raw_message):",
            "                params = {**context, **match.groupdict()}",
            "                extra[\"engine_name\"] = cls.engine_name",
            "                return [",
            "                    SupersetError(",
            "                        error_type=error_type,",
            "                        message=message % params,",
            "                        level=ErrorLevel.ERROR,",
            "                        extra=extra,",
            "                    )",
            "                ]",
            "",
            "        return [",
            "            SupersetError(",
            "                error_type=SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "                message=cls._extract_error_message(ex),",
            "                level=ErrorLevel.ERROR,",
            "                extra={\"engine_name\": cls.engine_name},",
            "            )",
            "        ]",
            "",
            "    @classmethod",
            "    def adjust_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        uri: URL,",
            "        connect_args: dict[str, Any],",
            "        catalog: str | None = None,",
            "        schema: str | None = None,",
            "    ) -> tuple[URL, dict[str, Any]]:",
            "        \"\"\"",
            "        Return a new URL and ``connect_args`` for a specific catalog/schema.",
            "",
            "        This is used in SQL Lab, allowing users to select a schema from the list of",
            "        schemas available in a given database, and have the query run with that schema as",
            "        the default one.",
            "",
            "        For some databases (like MySQL, Presto, Snowflake) this requires modifying the",
            "        SQLAlchemy URI before creating the connection. For others (like Postgres), it",
            "        requires additional parameters in ``connect_args`` or running pre-session",
            "        queries with ``set`` parameters.",
            "",
            "        When a DB engine spec implements this method or ``get_prequeries`` (see below) it",
            "        should also have the attribute ``supports_dynamic_schema`` set to true, so that",
            "        Superset knows in which schema a given query is running in order to enforce",
            "        permissions (see #23385 and #23401).",
            "",
            "        Currently, changing the catalog is not supported. The method accepts a catalog so",
            "        that when catalog support is added to Superset the interface remains the same.",
            "        This is important because DB engine specs can be installed from 3rd party",
            "        packages, so we want to keep these methods as stable as possible.",
            "        \"\"\"",
            "        return uri, {",
            "            **connect_args,",
            "            **cls.enforce_uri_query_params.get(uri.get_driver_name(), {}),",
            "        }",
            "",
            "    @classmethod",
            "    def get_prequeries(",
            "        cls,",
            "        catalog: str | None = None,  # pylint: disable=unused-argument",
            "        schema: str | None = None,  # pylint: disable=unused-argument",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return pre-session queries.",
            "",
            "        These are currently used as an alternative to ``adjust_engine_params`` for",
            "        databases where the selected schema cannot be specified in the SQLAlchemy URI or",
            "        connection arguments.",
            "",
            "        For example, in order to specify a default schema in RDS we need to run a query",
            "        at the beginning of the session:",
            "",
            "            sql> set search_path = my_schema;",
            "",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def patch(cls) -> None:",
            "        \"\"\"",
            "        TODO: Improve docstring and refactor implementation in Hive",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def get_catalog_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get all catalogs from database.",
            "",
            "        This needs to be implemented per database, since SQLAlchemy doesn't offer an",
            "        abstraction.",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def get_schema_names(cls, inspector: Inspector) -> list[str]:",
            "        \"\"\"",
            "        Get all schemas from database",
            "",
            "        :param inspector: SqlAlchemy inspector",
            "        :return: All schemas in the database",
            "        \"\"\"",
            "        return sorted(inspector.get_schema_names())",
            "",
            "    @classmethod",
            "    def get_table_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the real table names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The physical table names",
            "        \"\"\"",
            "",
            "        try:",
            "            tables = set(inspector.get_table_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            tables = {re.sub(f\"^{schema}\\\\.\", \"\", table) for table in tables}",
            "        return tables",
            "",
            "    @classmethod",
            "    def get_view_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the view names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The view names",
            "        \"\"\"",
            "",
            "        try:",
            "            views = set(inspector.get_view_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            views = {re.sub(f\"^{schema}\\\\.\", \"\", view) for view in views}",
            "        return views",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: Database,  # pylint: disable=unused-argument",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "",
            "        return inspector.get_indexes(table_name, schema)",
            "",
            "    @classmethod",
            "    def get_table_comment(",
            "        cls, inspector: Inspector, table_name: str, schema: str | None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Get comment of table from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :return: comment of table",
            "        \"\"\"",
            "        comment = None",
            "        try:",
            "            comment = inspector.get_table_comment(table_name, schema)",
            "            comment = comment.get(\"text\") if isinstance(comment, dict) else None",
            "        except NotImplementedError:",
            "            # It's expected that some dialects don't implement the comment method",
            "            pass",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.error(\"Unexpected error while fetching table comment\", exc_info=True)",
            "            logger.exception(ex)",
            "        return comment",
            "",
            "    @classmethod",
            "    def get_columns(  # pylint: disable=unused-argument",
            "        cls,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "        options: dict[str, Any] | None = None,",
            "    ) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        Get all columns from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :param options: Extra options to customise the display of columns in",
            "                        some databases",
            "        :return: All columns in table",
            "        \"\"\"",
            "        return convert_inspector_columns(",
            "            cast(list[SQLAColumnType], inspector.get_columns(table_name, schema))",
            "        )",
            "",
            "    @classmethod",
            "    def get_metrics(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[MetricType]:",
            "        \"\"\"",
            "        Get all metrics from a given schema and table.",
            "        \"\"\"",
            "        return [",
            "            {",
            "                \"metric_name\": \"count\",",
            "                \"verbose_name\": \"COUNT(*)\",",
            "                \"metric_type\": \"count\",",
            "                \"expression\": \"COUNT(*)\",",
            "            }",
            "        ]",
            "",
            "    @classmethod",
            "    def where_latest_partition(  # pylint: disable=too-many-arguments,unused-argument",
            "        cls,",
            "        table_name: str,",
            "        schema: str | None,",
            "        database: Database,",
            "        query: Select,",
            "        columns: list[ResultSetColumnType] | None = None,",
            "    ) -> Select | None:",
            "        \"\"\"",
            "        Add a where clause to a query to reference only the most recent partition",
            "",
            "        :param table_name: Table name",
            "        :param schema: Schema name",
            "        :param database: Database instance",
            "        :param query: SqlAlchemy query",
            "        :param columns: List of TableColumns",
            "        :return: SqlAlchemy query with additional where clause referencing the latest",
            "        partition",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database, TableColumn",
            "        return None",
            "",
            "    @classmethod",
            "    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:",
            "        return [",
            "            literal_column(query_as)",
            "            if (query_as := c.get(\"query_as\"))",
            "            else column(c[\"column_name\"])",
            "            for c in cols",
            "        ]",
            "",
            "    @classmethod",
            "    def select_star(  # pylint: disable=too-many-arguments,too-many-locals",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        engine: Engine,",
            "        schema: str | None = None,",
            "        limit: int = 100,",
            "        show_cols: bool = False,",
            "        indent: bool = True,",
            "        latest_partition: bool = True,",
            "        cols: list[ResultSetColumnType] | None = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Generate a \"SELECT * from [schema.]table_name\" query with appropriate limit.",
            "",
            "        WARNING: expects only unquoted table and schema names.",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name, unquoted",
            "        :param engine: SqlAlchemy Engine instance",
            "        :param schema: Schema, unquoted",
            "        :param limit: limit to impose on query",
            "        :param show_cols: Show columns in query; otherwise use \"*\"",
            "        :param indent: Add indentation to query",
            "        :param latest_partition: Only query the latest partition",
            "        :param cols: Columns to include in query",
            "        :return: SQL query",
            "        \"\"\"",
            "        # pylint: disable=redefined-outer-name",
            "        fields: str | list[Any] = \"*\"",
            "        cols = cols or []",
            "        if (show_cols or latest_partition) and not cols:",
            "            cols = database.get_columns(table_name, schema)",
            "",
            "        if show_cols:",
            "            fields = cls._get_fields(cols)",
            "        quote = engine.dialect.identifier_preparer.quote",
            "        quote_schema = engine.dialect.identifier_preparer.quote_schema",
            "        if schema:",
            "            full_table_name = quote_schema(schema) + \".\" + quote(table_name)",
            "        else:",
            "            full_table_name = quote(table_name)",
            "",
            "        qry = select(fields).select_from(text(full_table_name))",
            "",
            "        if limit and cls.allow_limit_clause:",
            "            qry = qry.limit(limit)",
            "        if latest_partition:",
            "            partition_query = cls.where_latest_partition(",
            "                table_name, schema, database, qry, columns=cols",
            "            )",
            "            if partition_query is not None:",
            "                qry = partition_query",
            "        sql = database.compile_sqla_query(qry)",
            "        if indent:",
            "            sql = SQLScript(sql, engine=cls.engine).format()",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        \"\"\"",
            "        Generate a SQL query that estimates the cost of a given statement.",
            "",
            "        :param statement: A single SQL statement",
            "        :param cursor: Cursor instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        \"\"\"",
            "        Format cost estimate.",
            "",
            "        :param raw_cost: Raw estimate from `estimate_query_cost`",
            "        :return: Human readable cost estimate",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def process_statement(cls, statement: str, database: Database) -> str:",
            "        \"\"\"",
            "        Process a SQL statement by stripping and mutating it.",
            "",
            "        :param statement: A single SQL statement",
            "        :param database: Database instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        parsed_query = ParsedQuery(statement, engine=cls.engine)",
            "        sql = parsed_query.stripped()",
            "        sql_query_mutator = current_app.config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = current_app.config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=database,",
            "            )",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_query_cost(",
            "        cls,",
            "        database: Database,",
            "        schema: str,",
            "        sql: str,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Estimate the cost of a multiple statement SQL query.",
            "",
            "        :param database: Database instance",
            "        :param schema: Database schema",
            "        :param sql: SQL query with possibly multiple statements",
            "        :param source: Source of the query (eg, \"sql_lab\")",
            "        \"\"\"",
            "        extra = database.get_extra() or {}",
            "        if not cls.get_allow_cost_estimate(extra):",
            "            raise Exception(  # pylint: disable=broad-exception-raised",
            "                \"Database does not support cost estimation\"",
            "            )",
            "",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        statements = parsed_query.get_statements()",
            "",
            "        costs = []",
            "        with database.get_raw_connection(schema=schema, source=source) as conn:",
            "            cursor = conn.cursor()",
            "            for statement in statements:",
            "                processed_statement = cls.process_statement(statement, database)",
            "                costs.append(cls.estimate_statement_cost(processed_statement, cursor))",
            "",
            "        return costs",
            "",
            "    @classmethod",
            "    def get_url_for_impersonation(",
            "        cls, url: URL, impersonate_user: bool, username: str | None",
            "    ) -> URL:",
            "        \"\"\"",
            "        Return a modified URL with the username set.",
            "",
            "        :param url: SQLAlchemy URL object",
            "        :param impersonate_user: Flag indicating if impersonation is enabled",
            "        :param username: Effective username",
            "        \"\"\"",
            "        if impersonate_user and username is not None:",
            "            url = url.set(username=username)",
            "",
            "        return url",
            "",
            "    @classmethod",
            "    def update_impersonation_config(",
            "        cls,",
            "        connect_args: dict[str, Any],",
            "        uri: str,",
            "        username: str | None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a configuration dictionary",
            "        that can set the correct properties for impersonating users",
            "",
            "        :param connect_args: config to be updated",
            "        :param uri: URI",
            "        :param username: Effective username",
            "        :return: None",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def execute(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: str,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"",
            "        Execute a SQL query",
            "",
            "        :param cursor: Cursor instance",
            "        :param query: Query to execute",
            "        :param kwargs: kwargs to be passed to cursor.execute()",
            "        :return:",
            "        \"\"\"",
            "        if not cls.allows_sql_comments:",
            "            query = sql_parse.strip_comments_from_sql(query, engine=cls.engine)",
            "",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            cursor.execute(query)",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def make_label_compatible(cls, label: str) -> str | quoted_name:",
            "        \"\"\"",
            "        Conditionally mutate and/or quote a sqlalchemy expression label. If",
            "        force_column_alias_quotes is set to True, return the label as a",
            "        sqlalchemy.sql.elements.quoted_name object to ensure that the select query",
            "        and query results have same case. Otherwise, return the mutated label as a",
            "        regular string. If maximum supported column name length is exceeded,",
            "        generate a truncated label by calling truncate_label().",
            "",
            "        :param label: expected expression label/alias",
            "        :return: conditionally mutated label supported by the db engine",
            "        \"\"\"",
            "        label_mutated = cls._mutate_label(label)",
            "        if (",
            "            cls.max_column_name_length",
            "            and len(label_mutated) > cls.max_column_name_length",
            "        ):",
            "            label_mutated = cls._truncate_label(label)",
            "        if cls.force_column_alias_quotes:",
            "            label_mutated = quoted_name(label_mutated, True)",
            "        return label_mutated",
            "",
            "    @classmethod",
            "    def get_column_types(",
            "        cls,",
            "        column_type: str | None,",
            "    ) -> tuple[TypeEngine, GenericDataType] | None:",
            "        \"\"\"",
            "        Return a sqlalchemy native column type and generic data type that corresponds",
            "        to the column type defined in the data source (return None to use default type",
            "        inferred by SQLAlchemy). Override `column_type_mappings` for specific needs",
            "        (see MSSQL for example of NCHAR/NVARCHAR handling).",
            "",
            "        :param column_type: Column type returned by inspector",
            "        :return: SQLAlchemy and generic Superset column types",
            "        \"\"\"",
            "        if not column_type:",
            "            return None",
            "",
            "        for regex, sqla_type, generic_type in (",
            "            cls.column_type_mappings + cls._default_column_type_mappings",
            "        ):",
            "            match = regex.match(column_type)",
            "            if not match:",
            "                continue",
            "            if callable(sqla_type):",
            "                return sqla_type(match), generic_type",
            "            return sqla_type, generic_type",
            "        return None",
            "",
            "    @staticmethod",
            "    def _mutate_label(label: str) -> str:",
            "        \"\"\"",
            "        Most engines support mixed case aliases that can include numbers",
            "        and special characters, like commas, parentheses etc. For engines that",
            "        have restrictions on what types of aliases are supported, this method",
            "        can be overridden to ensure that labels conform to the engine's",
            "        limitations. Mutated labels should be deterministic (input label A always",
            "        yields output label X) and unique (input labels A and B don't yield the same",
            "        output label X).",
            "",
            "        :param label: Preferred expression label",
            "        :return: Conditionally mutated label",
            "        \"\"\"",
            "        return label",
            "",
            "    @classmethod",
            "    def _truncate_label(cls, label: str) -> str:",
            "        \"\"\"",
            "        In the case that a label exceeds the max length supported by the engine,",
            "        this method is used to construct a deterministic and unique label based on",
            "        the original label. By default, this returns a md5 hash of the original label,",
            "        conditionally truncated if the length of the hash exceeds the max column length",
            "        of the engine.",
            "",
            "        :param label: Expected expression label",
            "        :return: Truncated label",
            "        \"\"\"",
            "        label = md5_sha_from_str(label)",
            "        # truncate hash if it exceeds max length",
            "        if cls.max_column_name_length and len(label) > cls.max_column_name_length:",
            "            label = label[: cls.max_column_name_length]",
            "        return label",
            "",
            "    @classmethod",
            "    def column_datatype_to_string(",
            "        cls, sqla_column_type: TypeEngine, dialect: Dialect",
            "    ) -> str:",
            "        \"\"\"",
            "        Convert sqlalchemy column type to string representation.",
            "        By default, removes collation and character encoding info to avoid",
            "        unnecessarily long datatypes.",
            "",
            "        :param sqla_column_type: SqlAlchemy column type",
            "        :param dialect: Sqlalchemy dialect",
            "        :return: Compiled column type",
            "        \"\"\"",
            "        sqla_column_type = sqla_column_type.copy()",
            "        if hasattr(sqla_column_type, \"collation\"):",
            "            sqla_column_type.collation = None",
            "        if hasattr(sqla_column_type, \"charset\"):",
            "            sqla_column_type.charset = None",
            "        return sqla_column_type.compile(dialect=dialect).upper()",
            "",
            "    @classmethod",
            "    def get_function_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get a list of function names that are able to be called on the database.",
            "        Used for SQL Lab autocomplete.",
            "",
            "        :param database: The database to get functions for",
            "        :return: A list of function names useable in the database",
            "        \"\"\"",
            "        return []",
            "",
            "    @staticmethod",
            "    def pyodbc_rows_to_tuples(data: list[Any]) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "        Convert pyodbc.Row objects from `fetch_data` to tuples.",
            "",
            "        :param data: List of tuples or pyodbc.Row objects",
            "        :return: List of tuples",
            "        \"\"\"",
            "        if data and type(data[0]).__name__ == \"Row\":",
            "            data = [tuple(row) for row in data]",
            "        return data",
            "",
            "    @staticmethod",
            "    def mutate_db_for_connection_test(  # pylint: disable=unused-argument",
            "        database: Database,",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require passing additional parameters for validating database",
            "        connections. This method makes it possible to mutate the database instance prior",
            "        to testing if a connection is ok.",
            "",
            "        :param database: instance to be mutated",
            "        \"\"\"",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        Some databases require adding elements to connection parameters,",
            "        like passing certificates to `extra`. This can be done here.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        \"\"\"",
            "        extra: dict[str, Any] = {}",
            "        if database.extra:",
            "            try:",
            "                extra = json.loads(database.extra)",
            "            except json.JSONDecodeError as ex:",
            "                logger.error(ex, exc_info=True)",
            "                raise ex",
            "        return extra",
            "",
            "    @staticmethod",
            "    def update_params_from_encrypted_extra(  # pylint: disable=invalid-name",
            "        database: Database, params: dict[str, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require some sensitive information which do not conform to",
            "        the username:password syntax normally used by SQLAlchemy.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :param params: params to be updated",
            "        \"\"\"",
            "        if not database.encrypted_extra:",
            "            return",
            "        try:",
            "            encrypted_extra = json.loads(database.encrypted_extra)",
            "            params.update(encrypted_extra)",
            "        except json.JSONDecodeError as ex:",
            "            logger.error(ex, exc_info=True)",
            "            raise ex",
            "",
            "    @classmethod",
            "    def is_readonly_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"Pessimistic readonly, 100% sure statement won't mutate anything\"\"\"",
            "        return (",
            "            parsed_query.is_select()",
            "            or parsed_query.is_explain()",
            "            or parsed_query.is_show()",
            "        )",
            "",
            "    @classmethod",
            "    def is_select_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"",
            "        Determine if the statement should be considered as SELECT statement.",
            "        Some query dialects do not contain \"SELECT\" word in queries (eg. Kusto)",
            "        \"\"\"",
            "        return parsed_query.is_select()",
            "",
            "    @classmethod",
            "    def get_column_spec(  # pylint: disable=unused-argument",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> ColumnSpec | None:",
            "        \"\"\"",
            "        Get generic type related specs regarding a native column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        if col_types := cls.get_column_types(native_type):",
            "            column_type, generic_type = col_types",
            "            is_dttm = generic_type == GenericDataType.TEMPORAL",
            "            return ColumnSpec(",
            "                sqla_type=column_type, generic_type=generic_type, is_dttm=is_dttm",
            "            )",
            "        return None",
            "",
            "    @classmethod",
            "    def get_sqla_column_type(",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> TypeEngine | None:",
            "        \"\"\"",
            "        Converts native database type to sqlalchemy column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        column_spec = cls.get_column_spec(",
            "            native_type=native_type,",
            "            db_extra=db_extra,",
            "            source=source,",
            "        )",
            "        return column_spec.sqla_type if column_spec else None",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def prepare_cancel_query(cls, query: Query) -> None:",
            "        \"\"\"",
            "        Some databases may acquire the query cancelation id after the query",
            "        cancelation request has been received. For those cases, the db engine spec",
            "        can record the cancelation intent so that the query can either be stopped",
            "        prior to execution, or canceled once the query id is acquired.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def has_implicit_cancel(cls) -> bool:",
            "        \"\"\"",
            "        Return True if the live cursor handles the implicit cancelation of the query,",
            "        False otherwise.",
            "",
            "        :return: Whether the live cursor implicitly cancels the query",
            "        :see: handle_cursor",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def get_cancel_query_id(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Select identifiers from the database engine that uniquely identifies the",
            "        queries to cancel. The identifier is typically a session id, process id",
            "        or similar.",
            "",
            "        :param cursor: Cursor instance in which the query will be executed",
            "        :param query: Query instance",
            "        :return: Query identifier",
            "        \"\"\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def cancel_query(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "        cancel_query_id: str,",
            "    ) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Value returned by get_cancel_query_payload or set in",
            "        other life-cycle methods of the query",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def parse_sql(cls, sql: str) -> list[str]:",
            "        return [str(s).strip(\" ;\") for s in sqlparse.parse(sql)]",
            "",
            "    @classmethod",
            "    def get_impersonation_key(cls, user: User | None) -> Any:",
            "        \"\"\"",
            "        Construct an impersonation key, by default it's the given username.",
            "",
            "        :param user: logged-in user",
            "",
            "        :returns: username if given user is not null",
            "        \"\"\"",
            "        return user.username if user else None",
            "",
            "    @classmethod",
            "    def mask_encrypted_extra(cls, encrypted_extra: str | None) -> str | None:",
            "        \"\"\"",
            "        Mask ``encrypted_extra``.",
            "",
            "        This is used to remove any sensitive data in ``encrypted_extra`` when presenting",
            "        it to the user. For example, a private key might be replaced with a masked value",
            "        \"XXXXXXXXXX\". If the masked value is changed the corresponding entry is updated,",
            "        otherwise the old value is used (see ``unmask_encrypted_extra`` below).",
            "        \"\"\"",
            "        return encrypted_extra",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def unmask_encrypted_extra(cls, old: str | None, new: str | None) -> str | None:",
            "        \"\"\"",
            "        Remove masks from ``encrypted_extra``.",
            "",
            "        This method allows reusing existing values from the current encrypted extra on",
            "        updates. It's useful for reusing masked passwords, allowing keys to be updated",
            "        without having to provide sensitive data to the client.",
            "        \"\"\"",
            "        return new",
            "",
            "    @classmethod",
            "    def get_public_information(cls) -> dict[str, Any]:",
            "        \"\"\"",
            "        Construct a Dict with properties we want to expose.",
            "",
            "        :returns: Dict with properties of our class like supports_file_upload",
            "        and disable_ssh_tunneling",
            "        \"\"\"",
            "        return {",
            "            \"supports_file_upload\": cls.supports_file_upload,",
            "            \"disable_ssh_tunneling\": cls.disable_ssh_tunneling,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_database_uri(cls, sqlalchemy_uri: URL) -> None:",
            "        \"\"\"",
            "        Validates a database SQLAlchemy URI per engine spec.",
            "        Use this to implement a final validation for unwanted connection configuration",
            "",
            "        :param sqlalchemy_uri:",
            "        \"\"\"",
            "        if existing_disallowed := cls.disallow_uri_query_params.get(",
            "            sqlalchemy_uri.get_driver_name(), set()",
            "        ).intersection(sqlalchemy_uri.query):",
            "            raise ValueError(f\"Forbidden query parameter(s): {existing_disallowed}\")",
            "",
            "    @classmethod",
            "    def denormalize_name(cls, dialect: Dialect, name: str) -> str:",
            "        if (",
            "            hasattr(dialect, \"requires_name_normalize\")",
            "            and dialect.requires_name_normalize",
            "        ):",
            "            return dialect.denormalize_name(name)",
            "",
            "        return name",
            "",
            "",
            "# schema for adding a database by providing parameters instead of the",
            "# full SQLAlchemy URI",
            "class BasicParametersSchema(Schema):",
            "    username = fields.String(",
            "        required=True, allow_none=True, metadata={\"description\": __(\"Username\")}",
            "    )",
            "    password = fields.String(allow_none=True, metadata={\"description\": __(\"Password\")})",
            "    host = fields.String(",
            "        required=True, metadata={\"description\": __(\"Hostname or IP address\")}",
            "    )",
            "    port = fields.Integer(",
            "        required=True,",
            "        metadata={\"description\": __(\"Database port\")},",
            "        validate=Range(min=0, max=2**16, max_inclusive=False),",
            "    )",
            "    database = fields.String(",
            "        required=True, metadata={\"description\": __(\"Database name\")}",
            "    )",
            "    query = fields.Dict(",
            "        keys=fields.Str(),",
            "        values=fields.Raw(),",
            "        metadata={\"description\": __(\"Additional parameters\")},",
            "    )",
            "    encryption = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an encrypted connection to the database\")},",
            "    )",
            "    ssh = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an ssh tunnel connection to the database\")},",
            "    )",
            "",
            "",
            "class BasicParametersType(TypedDict, total=False):",
            "    username: str | None",
            "    password: str | None",
            "    host: str",
            "    port: int",
            "    database: str",
            "    query: dict[str, Any]",
            "    encryption: bool",
            "",
            "",
            "class BasicPropertiesType(TypedDict):",
            "    parameters: BasicParametersType",
            "",
            "",
            "class BasicParametersMixin:",
            "    \"\"\"",
            "    Mixin for configuring DB engine specs via a dictionary.",
            "",
            "    With this mixin the SQLAlchemy engine can be configured through",
            "    individual parameters, instead of the full SQLAlchemy URI. This",
            "    mixin is for the most common pattern of URI:",
            "",
            "        engine+driver://user:password@host:port/dbname[?key=value&key=value...]",
            "",
            "    \"\"\"",
            "",
            "    # schema describing the parameters used to configure the DB",
            "    parameters_schema = BasicParametersSchema()",
            "",
            "    # recommended driver name for the DB engine spec",
            "    default_driver = \"\"",
            "",
            "    # query parameter to enable encryption in the database connection",
            "    # for Postgres this would be `{\"sslmode\": \"verify-ca\"}`, eg.",
            "    encryption_parameters: dict[str, str] = {}",
            "",
            "    @classmethod",
            "    def build_sqlalchemy_uri(  # pylint: disable=unused-argument",
            "        cls,",
            "        parameters: BasicParametersType,",
            "        encrypted_extra: dict[str, str] | None = None,",
            "    ) -> str:",
            "        # make a copy so that we don't update the original",
            "        query = parameters.get(\"query\", {}).copy()",
            "        if parameters.get(\"encryption\"):",
            "            if not cls.encryption_parameters:",
            "                raise Exception(  # pylint: disable=broad-exception-raised",
            "                    \"Unable to build a URL with encryption enabled\"",
            "                )",
            "            query.update(cls.encryption_parameters)",
            "",
            "        return str(",
            "            URL.create(",
            "                f\"{cls.engine}+{cls.default_driver}\".rstrip(\"+\"),  # type: ignore",
            "                username=parameters.get(\"username\"),",
            "                password=parameters.get(\"password\"),",
            "                host=parameters[\"host\"],",
            "                port=parameters[\"port\"],",
            "                database=parameters[\"database\"],",
            "                query=query,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def get_parameters_from_uri(  # pylint: disable=unused-argument",
            "        cls, uri: str, encrypted_extra: dict[str, Any] | None = None",
            "    ) -> BasicParametersType:",
            "        url = make_url_safe(uri)",
            "        query = {",
            "            key: value",
            "            for (key, value) in url.query.items()",
            "            if (key, value) not in cls.encryption_parameters.items()",
            "        }",
            "        encryption = all(",
            "            item in url.query.items() for item in cls.encryption_parameters.items()",
            "        )",
            "        return {",
            "            \"username\": url.username,",
            "            \"password\": url.password,",
            "            \"host\": url.host,",
            "            \"port\": url.port,",
            "            \"database\": url.database,",
            "            \"query\": query,",
            "            \"encryption\": encryption,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_parameters(",
            "        cls, properties: BasicPropertiesType",
            "    ) -> list[SupersetError]:",
            "        \"\"\"",
            "        Validates any number of parameters, for progressive validation.",
            "",
            "        If only the hostname is present it will check if the name is resolvable. As more",
            "        parameters are present in the request, more validation is done.",
            "        \"\"\"",
            "        errors: list[SupersetError] = []",
            "",
            "        required = {\"host\", \"port\", \"username\", \"database\"}",
            "        parameters = properties.get(\"parameters\", {})",
            "        present = {key for key in parameters if parameters.get(key, ())}",
            "",
            "        if missing := sorted(required - present):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=f'One or more parameters are missing: {\", \".join(missing)}',",
            "                    error_type=SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR,",
            "                    level=ErrorLevel.WARNING,",
            "                    extra={\"missing\": missing},",
            "                ),",
            "            )",
            "",
            "        host = parameters.get(\"host\", None)",
            "        if not host:",
            "            return errors",
            "        if not is_hostname_valid(host):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The hostname provided can't be resolved.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"host\"]},",
            "                ),",
            "            )",
            "            return errors",
            "",
            "        port = parameters.get(\"port\", None)",
            "        if not port:",
            "            return errors",
            "        try:",
            "            port = int(port)",
            "        except (ValueError, TypeError):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"Port must be a valid integer.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        if not (isinstance(port, int) and 0 <= port < 2**16):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=(",
            "                        \"The port must be an integer between 0 and 65535 \"",
            "                        \"(inclusive).\"",
            "                    ),",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        elif not is_port_open(host, port):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The port is closed.\",",
            "                    error_type=SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "",
            "        return errors",
            "",
            "    @classmethod",
            "    def parameters_json_schema(cls) -> Any:",
            "        \"\"\"",
            "        Return configuration parameters as OpenAPI.",
            "        \"\"\"",
            "        if not cls.parameters_schema:",
            "            return None",
            "",
            "        spec = APISpec(",
            "            title=\"Database Parameters\",",
            "            version=\"1.0.0\",",
            "            openapi_version=\"3.0.2\",",
            "            plugins=[MarshmallowPlugin()],",
            "        )",
            "        spec.components.schema(cls.__name__, schema=cls.parameters_schema)",
            "        return spec.to_dict()[\"components\"][\"schemas\"][cls.__name__]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "62": [],
            "1451": [
                "BaseEngineSpec",
                "select_star"
            ]
        },
        "addLocation": []
    },
    "superset/db_engine_specs/postgres.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from re import Pattern"
            },
            "1": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from typing import Any, TYPE_CHECKING"
            },
            "2": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import sqlparse"
            },
            "4": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from flask_babel import gettext as __"
            },
            "5": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from sqlalchemy.dialects.postgresql import DOUBLE_PRECISION, ENUM, JSON"
            },
            "6": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " from sqlalchemy.dialects.postgresql.base import PGInspector"
            },
            "7": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from superset.errors import ErrorLevel, SupersetError, SupersetErrorType"
            },
            "8": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " from superset.exceptions import SupersetException, SupersetSecurityException"
            },
            "9": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " from superset.models.sql_lab import Query"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+from superset.sql_parse import SQLScript"
            },
            "11": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " from superset.utils import core as utils"
            },
            "12": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " from superset.utils.core import GenericDataType"
            },
            "13": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": 281,
                "PatchRowcode": "         This method simply uses the parent method after checking that there are no"
            },
            "15": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": 282,
                "PatchRowcode": "         malicious path setting in the query."
            },
            "16": {
                "beforePatchRowNumber": 283,
                "afterPatchRowNumber": 283,
                "PatchRowcode": "         \"\"\""
            },
            "17": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sql = sqlparse.format(query.sql, strip_comments=True)"
            },
            "18": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if re.search(r\"set\\s+search_path\\s*=\", sql, re.IGNORECASE):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 284,
                "PatchRowcode": "+        script = SQLScript(query.sql, engine=cls.engine)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 285,
                "PatchRowcode": "+        settings = script.get_settings()"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 286,
                "PatchRowcode": "+        if \"search_path\" in settings:"
            },
            "22": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 287,
                "PatchRowcode": "             raise SupersetSecurityException("
            },
            "23": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": 288,
                "PatchRowcode": "                 SupersetError("
            },
            "24": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": 289,
                "PatchRowcode": "                     error_type=SupersetErrorType.QUERY_SECURITY_ACCESS_ERROR,"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "from __future__ import annotations",
            "",
            "import json",
            "import logging",
            "import re",
            "from datetime import datetime",
            "from re import Pattern",
            "from typing import Any, TYPE_CHECKING",
            "",
            "import sqlparse",
            "from flask_babel import gettext as __",
            "from sqlalchemy.dialects.postgresql import DOUBLE_PRECISION, ENUM, JSON",
            "from sqlalchemy.dialects.postgresql.base import PGInspector",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.types import Date, DateTime, String",
            "",
            "from superset.constants import TimeGrain",
            "from superset.db_engine_specs.base import BaseEngineSpec, BasicParametersMixin",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetException, SupersetSecurityException",
            "from superset.models.sql_lab import Query",
            "from superset.utils import core as utils",
            "from superset.utils.core import GenericDataType",
            "",
            "if TYPE_CHECKING:",
            "    from superset.models.core import Database  # pragma: no cover",
            "",
            "logger = logging.getLogger()",
            "",
            "",
            "# Regular expressions to catch custom errors",
            "CONNECTION_INVALID_USERNAME_REGEX = re.compile(",
            "    'role \"(?P<username>.*?)\" does not exist'",
            ")",
            "CONNECTION_INVALID_PASSWORD_REGEX = re.compile(",
            "    'password authentication failed for user \"(?P<username>.*?)\"'",
            ")",
            "CONNECTION_INVALID_PASSWORD_NEEDED_REGEX = re.compile(\"no password supplied\")",
            "CONNECTION_INVALID_HOSTNAME_REGEX = re.compile(",
            "    'could not translate host name \"(?P<hostname>.*?)\" to address: '",
            "    \"nodename nor servname provided, or not known\"",
            ")",
            "CONNECTION_PORT_CLOSED_REGEX = re.compile(",
            "    r\"could not connect to server: Connection refused\\s+Is the server \"",
            "    r'running on host \"(?P<hostname>.*?)\" (\\(.*?\\) )?and accepting\\s+TCP/IP '",
            "    r\"connections on port (?P<port>.*?)\\?\"",
            ")",
            "CONNECTION_HOST_DOWN_REGEX = re.compile(",
            "    r\"could not connect to server: (?P<reason>.*?)\\s+Is the server running on \"",
            "    r'host \"(?P<hostname>.*?)\" (\\(.*?\\) )?and accepting\\s+TCP/IP '",
            "    r\"connections on port (?P<port>.*?)\\?\"",
            ")",
            "CONNECTION_UNKNOWN_DATABASE_REGEX = re.compile(",
            "    'database \"(?P<database>.*?)\" does not exist'",
            ")",
            "COLUMN_DOES_NOT_EXIST_REGEX = re.compile(",
            "    r'postgresql error: column \"(?P<column_name>.+?)\" '",
            "    r\"does not exist\\s+LINE (?P<location>\\d+?)\"",
            ")",
            "",
            "SYNTAX_ERROR_REGEX = re.compile('syntax error at or near \"(?P<syntax_error>.*?)\"')",
            "",
            "",
            "def parse_options(connect_args: dict[str, Any]) -> dict[str, str]:",
            "    \"\"\"",
            "    Parse ``options`` from  ``connect_args`` into a dictionary.",
            "    \"\"\"",
            "    if not isinstance(connect_args.get(\"options\"), str):",
            "        return {}",
            "",
            "    tokens = (",
            "        tuple(token.strip() for token in option.strip().split(\"=\", 1))",
            "        for option in re.split(r\"-c\\s?\", connect_args[\"options\"])",
            "        if \"=\" in option",
            "    )",
            "",
            "    return {token[0]: token[1] for token in tokens}",
            "",
            "",
            "class PostgresBaseEngineSpec(BaseEngineSpec):",
            "    \"\"\"Abstract class for Postgres 'like' databases\"\"\"",
            "",
            "    engine = \"\"",
            "    engine_name = \"PostgreSQL\"",
            "",
            "    supports_catalog = True",
            "",
            "    _time_grain_expressions = {",
            "        None: \"{col}\",",
            "        TimeGrain.SECOND: \"DATE_TRUNC('second', {col})\",",
            "        TimeGrain.MINUTE: \"DATE_TRUNC('minute', {col})\",",
            "        TimeGrain.HOUR: \"DATE_TRUNC('hour', {col})\",",
            "        TimeGrain.DAY: \"DATE_TRUNC('day', {col})\",",
            "        TimeGrain.WEEK: \"DATE_TRUNC('week', {col})\",",
            "        TimeGrain.MONTH: \"DATE_TRUNC('month', {col})\",",
            "        TimeGrain.QUARTER: \"DATE_TRUNC('quarter', {col})\",",
            "        TimeGrain.YEAR: \"DATE_TRUNC('year', {col})\",",
            "    }",
            "",
            "    custom_errors: dict[Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]] = {",
            "        CONNECTION_INVALID_USERNAME_REGEX: (",
            "            __('The username \"%(username)s\" does not exist.'),",
            "            SupersetErrorType.CONNECTION_INVALID_USERNAME_ERROR,",
            "            {\"invalid\": [\"username\"]},",
            "        ),",
            "        CONNECTION_INVALID_PASSWORD_REGEX: (",
            "            __('The password provided for username \"%(username)s\" is incorrect.'),",
            "            SupersetErrorType.CONNECTION_INVALID_PASSWORD_ERROR,",
            "            {\"invalid\": [\"username\", \"password\"]},",
            "        ),",
            "        CONNECTION_INVALID_PASSWORD_NEEDED_REGEX: (",
            "            __(\"Please re-enter the password.\"),",
            "            SupersetErrorType.CONNECTION_ACCESS_DENIED_ERROR,",
            "            {\"invalid\": [\"password\"]},",
            "        ),",
            "        CONNECTION_INVALID_HOSTNAME_REGEX: (",
            "            __('The hostname \"%(hostname)s\" cannot be resolved.'),",
            "            SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,",
            "            {\"invalid\": [\"host\"]},",
            "        ),",
            "        CONNECTION_PORT_CLOSED_REGEX: (",
            "            __('Port %(port)s on hostname \"%(hostname)s\" refused the connection.'),",
            "            SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,",
            "            {\"invalid\": [\"host\", \"port\"]},",
            "        ),",
            "        CONNECTION_HOST_DOWN_REGEX: (",
            "            __(",
            "                'The host \"%(hostname)s\" might be down, and can\\'t be '",
            "                \"reached on port %(port)s.\"",
            "            ),",
            "            SupersetErrorType.CONNECTION_HOST_DOWN_ERROR,",
            "            {\"invalid\": [\"host\", \"port\"]},",
            "        ),",
            "        CONNECTION_UNKNOWN_DATABASE_REGEX: (",
            "            __('Unable to connect to database \"%(database)s\".'),",
            "            SupersetErrorType.CONNECTION_UNKNOWN_DATABASE_ERROR,",
            "            {\"invalid\": [\"database\"]},",
            "        ),",
            "        COLUMN_DOES_NOT_EXIST_REGEX: (",
            "            __(",
            "                'We can\\'t seem to resolve the column \"%(column_name)s\" at '",
            "                \"line %(location)s.\",",
            "            ),",
            "            SupersetErrorType.COLUMN_DOES_NOT_EXIST_ERROR,",
            "            {},",
            "        ),",
            "        SYNTAX_ERROR_REGEX: (",
            "            __(",
            "                \"Please check your query for syntax errors at or \"",
            "                'near \"%(syntax_error)s\". Then, try running your query again.'",
            "            ),",
            "            SupersetErrorType.SYNTAX_ERROR,",
            "            {},",
            "        ),",
            "    }",
            "",
            "    @classmethod",
            "    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:",
            "        if not cursor.description:",
            "            return []",
            "        return super().fetch_data(cursor, limit)",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        return \"(timestamp 'epoch' + {col} * interval '1 second')\"",
            "",
            "    @classmethod",
            "    def convert_dttm(",
            "        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None",
            "    ) -> str | None:",
            "        sqla_type = cls.get_sqla_column_type(target_type)",
            "",
            "        if isinstance(sqla_type, Date):",
            "            return f\"TO_DATE('{dttm.date().isoformat()}', 'YYYY-MM-DD')\"",
            "        if isinstance(sqla_type, DateTime):",
            "            dttm_formatted = dttm.isoformat(sep=\" \", timespec=\"microseconds\")",
            "            return f\"\"\"TO_TIMESTAMP('{dttm_formatted}', 'YYYY-MM-DD HH24:MI:SS.US')\"\"\"",
            "        return None",
            "",
            "",
            "class PostgresEngineSpec(BasicParametersMixin, PostgresBaseEngineSpec):",
            "    engine = \"postgresql\"",
            "    engine_aliases = {\"postgres\"}",
            "    supports_dynamic_schema = True",
            "",
            "    default_driver = \"psycopg2\"",
            "    sqlalchemy_uri_placeholder = (",
            "        \"postgresql://user:password@host:port/dbname[?key=value&key=value...]\"",
            "    )",
            "    # https://www.postgresql.org/docs/9.1/libpq-ssl.html#LIBQ-SSL-CERTIFICATES",
            "    encryption_parameters = {\"sslmode\": \"require\"}",
            "",
            "    max_column_name_length = 63",
            "    try_remove_schema_from_table_name = False",
            "",
            "    column_type_mappings = (",
            "        (",
            "            re.compile(r\"^double precision\", re.IGNORECASE),",
            "            DOUBLE_PRECISION(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^array.*\", re.IGNORECASE),",
            "            String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^json.*\", re.IGNORECASE),",
            "            JSON(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^enum.*\", re.IGNORECASE),",
            "            ENUM(),",
            "            GenericDataType.STRING,",
            "        ),",
            "    )",
            "",
            "    @classmethod",
            "    def get_schema_from_engine_params(",
            "        cls,",
            "        sqlalchemy_uri: URL,",
            "        connect_args: dict[str, Any],",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the configured schema.",
            "",
            "        While Postgres doesn't support connecting directly to a given schema, it allows",
            "        users to specify a \"search path\" that is used to resolve non-qualified table",
            "        names; this can be specified in the database ``connect_args``.",
            "",
            "        One important detail is that the search path can be a comma separated list of",
            "        schemas. While this is supported by the SQLAlchemy dialect, it shouldn't be used",
            "        in Superset because it breaks schema-level permissions, since it's impossible",
            "        to determine the schema for a non-qualified table in a query. In cases like",
            "        that we raise an exception.",
            "",
            "        Note that because the DB engine supports dynamic schema this method is never",
            "        called. It's left here as an implementation reference.",
            "        \"\"\"",
            "        options = parse_options(connect_args)",
            "        if search_path := options.get(\"search_path\"):",
            "            schemas = search_path.split(\",\")",
            "            if len(schemas) > 1:",
            "                raise Exception(  # pylint: disable=broad-exception-raised",
            "                    \"Multiple schemas are configured in the search path, which means \"",
            "                    \"Superset is unable to determine the schema of unqualified table \"",
            "                    \"names and enforce permissions.\"",
            "                )",
            "            return schemas[0]",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def get_default_schema_for_query(",
            "        cls,",
            "        database: Database,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the default schema for a given query.",
            "",
            "        This method simply uses the parent method after checking that there are no",
            "        malicious path setting in the query.",
            "        \"\"\"",
            "        sql = sqlparse.format(query.sql, strip_comments=True)",
            "        if re.search(r\"set\\s+search_path\\s*=\", sql, re.IGNORECASE):",
            "            raise SupersetSecurityException(",
            "                SupersetError(",
            "                    error_type=SupersetErrorType.QUERY_SECURITY_ACCESS_ERROR,",
            "                    message=__(",
            "                        \"Users are not allowed to set a search path for security reasons.\"",
            "                    ),",
            "                    level=ErrorLevel.ERROR,",
            "                )",
            "            )",
            "",
            "        return super().get_default_schema_for_query(database, query)",
            "",
            "    @classmethod",
            "    def get_prequeries(",
            "        cls,",
            "        catalog: str | None = None,",
            "        schema: str | None = None,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Set the search path to the specified schema.",
            "",
            "        This is important for two reasons: in SQL Lab it will allow queries to run in",
            "        the schema selected in the dropdown, resolving unqualified table names to the",
            "        expected schema.",
            "",
            "        But more importantly, in SQL Lab this is used to check if the user has access to",
            "        any tables with unqualified names. If the schema is not set by SQL Lab it could",
            "        be anything, and we would have to block users from running any queries",
            "        referencing tables without an explicit schema.",
            "        \"\"\"",
            "        return [f'set search_path = \"{schema}\"'] if schema else []",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(cls, extra: dict[str, Any]) -> bool:",
            "        return True",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        sql = f\"EXPLAIN {statement}\"",
            "        cursor.execute(sql)",
            "",
            "        result = cursor.fetchone()[0]",
            "        match = re.search(r\"cost=([\\d\\.]+)\\.\\.([\\d\\.]+)\", result)",
            "        if match:",
            "            return {",
            "                \"Start-up cost\": float(match.group(1)),",
            "                \"Total cost\": float(match.group(2)),",
            "            }",
            "",
            "        return {}",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        return [{k: str(v) for k, v in row.items()} for row in raw_cost]",
            "",
            "    @classmethod",
            "    def get_catalog_names(",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return all catalogs.",
            "",
            "        In Postgres, a catalog is called a \"database\".",
            "        \"\"\"",
            "        return sorted(",
            "            catalog",
            "            for (catalog,) in inspector.bind.execute(",
            "                \"\"\"",
            "SELECT datname FROM pg_database",
            "WHERE datistemplate = false;",
            "            \"\"\"",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def get_table_names(",
            "        cls, database: Database, inspector: PGInspector, schema: str | None",
            "    ) -> set[str]:",
            "        \"\"\"Need to consider foreign tables for PostgreSQL\"\"\"",
            "        return set(inspector.get_table_names(schema)) | set(",
            "            inspector.get_foreign_table_names(schema)",
            "        )",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        For Postgres, the path to a SSL certificate is placed in `connect_args`.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        :raises SupersetException: If database extra json payload is unparseable",
            "        \"\"\"",
            "        try:",
            "            extra = json.loads(database.extra or \"{}\")",
            "        except json.JSONDecodeError as ex:",
            "            raise SupersetException(\"Unable to parse database extras\") from ex",
            "",
            "        if database.server_cert:",
            "            engine_params = extra.get(\"engine_params\", {})",
            "            connect_args = engine_params.get(\"connect_args\", {})",
            "            connect_args[\"sslmode\"] = connect_args.get(\"sslmode\", \"verify-full\")",
            "            path = utils.create_ssl_cert_file(database.server_cert)",
            "            connect_args[\"sslrootcert\"] = path",
            "            engine_params[\"connect_args\"] = connect_args",
            "            extra[\"engine_params\"] = engine_params",
            "        return extra",
            "",
            "    @classmethod",
            "    def get_datatype(cls, type_code: Any) -> str | None:",
            "        # pylint: disable=import-outside-toplevel",
            "        from psycopg2.extensions import binary_types, string_types",
            "",
            "        types = binary_types.copy()",
            "        types.update(string_types)",
            "        if type_code in types:",
            "            return types[type_code].name",
            "        return None",
            "",
            "    @classmethod",
            "    def get_cancel_query_id(cls, cursor: Any, query: Query) -> str | None:",
            "        \"\"\"",
            "        Get Postgres PID that will be used to cancel all other running",
            "        queries in the same session.",
            "",
            "        :param cursor: Cursor instance in which the query will be executed",
            "        :param query: Query instance",
            "        :return: Postgres PID",
            "        \"\"\"",
            "        cursor.execute(\"SELECT pg_backend_pid()\")",
            "        row = cursor.fetchone()",
            "        return row[0]",
            "",
            "    @classmethod",
            "    def cancel_query(cls, cursor: Any, query: Query, cancel_query_id: str) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Postgres PID",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "        try:",
            "            cursor.execute(",
            "                \"SELECT pg_terminate_backend(pid) \"",
            "                \"FROM pg_stat_activity \"",
            "                f\"WHERE pid='{cancel_query_id}'\"",
            "            )",
            "        except Exception:  # pylint: disable=broad-except",
            "            return False",
            "",
            "        return True"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "from __future__ import annotations",
            "",
            "import json",
            "import logging",
            "import re",
            "from datetime import datetime",
            "from re import Pattern",
            "from typing import Any, TYPE_CHECKING",
            "",
            "from flask_babel import gettext as __",
            "from sqlalchemy.dialects.postgresql import DOUBLE_PRECISION, ENUM, JSON",
            "from sqlalchemy.dialects.postgresql.base import PGInspector",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.types import Date, DateTime, String",
            "",
            "from superset.constants import TimeGrain",
            "from superset.db_engine_specs.base import BaseEngineSpec, BasicParametersMixin",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetException, SupersetSecurityException",
            "from superset.models.sql_lab import Query",
            "from superset.sql_parse import SQLScript",
            "from superset.utils import core as utils",
            "from superset.utils.core import GenericDataType",
            "",
            "if TYPE_CHECKING:",
            "    from superset.models.core import Database  # pragma: no cover",
            "",
            "logger = logging.getLogger()",
            "",
            "",
            "# Regular expressions to catch custom errors",
            "CONNECTION_INVALID_USERNAME_REGEX = re.compile(",
            "    'role \"(?P<username>.*?)\" does not exist'",
            ")",
            "CONNECTION_INVALID_PASSWORD_REGEX = re.compile(",
            "    'password authentication failed for user \"(?P<username>.*?)\"'",
            ")",
            "CONNECTION_INVALID_PASSWORD_NEEDED_REGEX = re.compile(\"no password supplied\")",
            "CONNECTION_INVALID_HOSTNAME_REGEX = re.compile(",
            "    'could not translate host name \"(?P<hostname>.*?)\" to address: '",
            "    \"nodename nor servname provided, or not known\"",
            ")",
            "CONNECTION_PORT_CLOSED_REGEX = re.compile(",
            "    r\"could not connect to server: Connection refused\\s+Is the server \"",
            "    r'running on host \"(?P<hostname>.*?)\" (\\(.*?\\) )?and accepting\\s+TCP/IP '",
            "    r\"connections on port (?P<port>.*?)\\?\"",
            ")",
            "CONNECTION_HOST_DOWN_REGEX = re.compile(",
            "    r\"could not connect to server: (?P<reason>.*?)\\s+Is the server running on \"",
            "    r'host \"(?P<hostname>.*?)\" (\\(.*?\\) )?and accepting\\s+TCP/IP '",
            "    r\"connections on port (?P<port>.*?)\\?\"",
            ")",
            "CONNECTION_UNKNOWN_DATABASE_REGEX = re.compile(",
            "    'database \"(?P<database>.*?)\" does not exist'",
            ")",
            "COLUMN_DOES_NOT_EXIST_REGEX = re.compile(",
            "    r'postgresql error: column \"(?P<column_name>.+?)\" '",
            "    r\"does not exist\\s+LINE (?P<location>\\d+?)\"",
            ")",
            "",
            "SYNTAX_ERROR_REGEX = re.compile('syntax error at or near \"(?P<syntax_error>.*?)\"')",
            "",
            "",
            "def parse_options(connect_args: dict[str, Any]) -> dict[str, str]:",
            "    \"\"\"",
            "    Parse ``options`` from  ``connect_args`` into a dictionary.",
            "    \"\"\"",
            "    if not isinstance(connect_args.get(\"options\"), str):",
            "        return {}",
            "",
            "    tokens = (",
            "        tuple(token.strip() for token in option.strip().split(\"=\", 1))",
            "        for option in re.split(r\"-c\\s?\", connect_args[\"options\"])",
            "        if \"=\" in option",
            "    )",
            "",
            "    return {token[0]: token[1] for token in tokens}",
            "",
            "",
            "class PostgresBaseEngineSpec(BaseEngineSpec):",
            "    \"\"\"Abstract class for Postgres 'like' databases\"\"\"",
            "",
            "    engine = \"\"",
            "    engine_name = \"PostgreSQL\"",
            "",
            "    supports_catalog = True",
            "",
            "    _time_grain_expressions = {",
            "        None: \"{col}\",",
            "        TimeGrain.SECOND: \"DATE_TRUNC('second', {col})\",",
            "        TimeGrain.MINUTE: \"DATE_TRUNC('minute', {col})\",",
            "        TimeGrain.HOUR: \"DATE_TRUNC('hour', {col})\",",
            "        TimeGrain.DAY: \"DATE_TRUNC('day', {col})\",",
            "        TimeGrain.WEEK: \"DATE_TRUNC('week', {col})\",",
            "        TimeGrain.MONTH: \"DATE_TRUNC('month', {col})\",",
            "        TimeGrain.QUARTER: \"DATE_TRUNC('quarter', {col})\",",
            "        TimeGrain.YEAR: \"DATE_TRUNC('year', {col})\",",
            "    }",
            "",
            "    custom_errors: dict[Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]] = {",
            "        CONNECTION_INVALID_USERNAME_REGEX: (",
            "            __('The username \"%(username)s\" does not exist.'),",
            "            SupersetErrorType.CONNECTION_INVALID_USERNAME_ERROR,",
            "            {\"invalid\": [\"username\"]},",
            "        ),",
            "        CONNECTION_INVALID_PASSWORD_REGEX: (",
            "            __('The password provided for username \"%(username)s\" is incorrect.'),",
            "            SupersetErrorType.CONNECTION_INVALID_PASSWORD_ERROR,",
            "            {\"invalid\": [\"username\", \"password\"]},",
            "        ),",
            "        CONNECTION_INVALID_PASSWORD_NEEDED_REGEX: (",
            "            __(\"Please re-enter the password.\"),",
            "            SupersetErrorType.CONNECTION_ACCESS_DENIED_ERROR,",
            "            {\"invalid\": [\"password\"]},",
            "        ),",
            "        CONNECTION_INVALID_HOSTNAME_REGEX: (",
            "            __('The hostname \"%(hostname)s\" cannot be resolved.'),",
            "            SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,",
            "            {\"invalid\": [\"host\"]},",
            "        ),",
            "        CONNECTION_PORT_CLOSED_REGEX: (",
            "            __('Port %(port)s on hostname \"%(hostname)s\" refused the connection.'),",
            "            SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,",
            "            {\"invalid\": [\"host\", \"port\"]},",
            "        ),",
            "        CONNECTION_HOST_DOWN_REGEX: (",
            "            __(",
            "                'The host \"%(hostname)s\" might be down, and can\\'t be '",
            "                \"reached on port %(port)s.\"",
            "            ),",
            "            SupersetErrorType.CONNECTION_HOST_DOWN_ERROR,",
            "            {\"invalid\": [\"host\", \"port\"]},",
            "        ),",
            "        CONNECTION_UNKNOWN_DATABASE_REGEX: (",
            "            __('Unable to connect to database \"%(database)s\".'),",
            "            SupersetErrorType.CONNECTION_UNKNOWN_DATABASE_ERROR,",
            "            {\"invalid\": [\"database\"]},",
            "        ),",
            "        COLUMN_DOES_NOT_EXIST_REGEX: (",
            "            __(",
            "                'We can\\'t seem to resolve the column \"%(column_name)s\" at '",
            "                \"line %(location)s.\",",
            "            ),",
            "            SupersetErrorType.COLUMN_DOES_NOT_EXIST_ERROR,",
            "            {},",
            "        ),",
            "        SYNTAX_ERROR_REGEX: (",
            "            __(",
            "                \"Please check your query for syntax errors at or \"",
            "                'near \"%(syntax_error)s\". Then, try running your query again.'",
            "            ),",
            "            SupersetErrorType.SYNTAX_ERROR,",
            "            {},",
            "        ),",
            "    }",
            "",
            "    @classmethod",
            "    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:",
            "        if not cursor.description:",
            "            return []",
            "        return super().fetch_data(cursor, limit)",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        return \"(timestamp 'epoch' + {col} * interval '1 second')\"",
            "",
            "    @classmethod",
            "    def convert_dttm(",
            "        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None",
            "    ) -> str | None:",
            "        sqla_type = cls.get_sqla_column_type(target_type)",
            "",
            "        if isinstance(sqla_type, Date):",
            "            return f\"TO_DATE('{dttm.date().isoformat()}', 'YYYY-MM-DD')\"",
            "        if isinstance(sqla_type, DateTime):",
            "            dttm_formatted = dttm.isoformat(sep=\" \", timespec=\"microseconds\")",
            "            return f\"\"\"TO_TIMESTAMP('{dttm_formatted}', 'YYYY-MM-DD HH24:MI:SS.US')\"\"\"",
            "        return None",
            "",
            "",
            "class PostgresEngineSpec(BasicParametersMixin, PostgresBaseEngineSpec):",
            "    engine = \"postgresql\"",
            "    engine_aliases = {\"postgres\"}",
            "    supports_dynamic_schema = True",
            "",
            "    default_driver = \"psycopg2\"",
            "    sqlalchemy_uri_placeholder = (",
            "        \"postgresql://user:password@host:port/dbname[?key=value&key=value...]\"",
            "    )",
            "    # https://www.postgresql.org/docs/9.1/libpq-ssl.html#LIBQ-SSL-CERTIFICATES",
            "    encryption_parameters = {\"sslmode\": \"require\"}",
            "",
            "    max_column_name_length = 63",
            "    try_remove_schema_from_table_name = False",
            "",
            "    column_type_mappings = (",
            "        (",
            "            re.compile(r\"^double precision\", re.IGNORECASE),",
            "            DOUBLE_PRECISION(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^array.*\", re.IGNORECASE),",
            "            String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^json.*\", re.IGNORECASE),",
            "            JSON(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^enum.*\", re.IGNORECASE),",
            "            ENUM(),",
            "            GenericDataType.STRING,",
            "        ),",
            "    )",
            "",
            "    @classmethod",
            "    def get_schema_from_engine_params(",
            "        cls,",
            "        sqlalchemy_uri: URL,",
            "        connect_args: dict[str, Any],",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the configured schema.",
            "",
            "        While Postgres doesn't support connecting directly to a given schema, it allows",
            "        users to specify a \"search path\" that is used to resolve non-qualified table",
            "        names; this can be specified in the database ``connect_args``.",
            "",
            "        One important detail is that the search path can be a comma separated list of",
            "        schemas. While this is supported by the SQLAlchemy dialect, it shouldn't be used",
            "        in Superset because it breaks schema-level permissions, since it's impossible",
            "        to determine the schema for a non-qualified table in a query. In cases like",
            "        that we raise an exception.",
            "",
            "        Note that because the DB engine supports dynamic schema this method is never",
            "        called. It's left here as an implementation reference.",
            "        \"\"\"",
            "        options = parse_options(connect_args)",
            "        if search_path := options.get(\"search_path\"):",
            "            schemas = search_path.split(\",\")",
            "            if len(schemas) > 1:",
            "                raise Exception(  # pylint: disable=broad-exception-raised",
            "                    \"Multiple schemas are configured in the search path, which means \"",
            "                    \"Superset is unable to determine the schema of unqualified table \"",
            "                    \"names and enforce permissions.\"",
            "                )",
            "            return schemas[0]",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def get_default_schema_for_query(",
            "        cls,",
            "        database: Database,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the default schema for a given query.",
            "",
            "        This method simply uses the parent method after checking that there are no",
            "        malicious path setting in the query.",
            "        \"\"\"",
            "        script = SQLScript(query.sql, engine=cls.engine)",
            "        settings = script.get_settings()",
            "        if \"search_path\" in settings:",
            "            raise SupersetSecurityException(",
            "                SupersetError(",
            "                    error_type=SupersetErrorType.QUERY_SECURITY_ACCESS_ERROR,",
            "                    message=__(",
            "                        \"Users are not allowed to set a search path for security reasons.\"",
            "                    ),",
            "                    level=ErrorLevel.ERROR,",
            "                )",
            "            )",
            "",
            "        return super().get_default_schema_for_query(database, query)",
            "",
            "    @classmethod",
            "    def get_prequeries(",
            "        cls,",
            "        catalog: str | None = None,",
            "        schema: str | None = None,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Set the search path to the specified schema.",
            "",
            "        This is important for two reasons: in SQL Lab it will allow queries to run in",
            "        the schema selected in the dropdown, resolving unqualified table names to the",
            "        expected schema.",
            "",
            "        But more importantly, in SQL Lab this is used to check if the user has access to",
            "        any tables with unqualified names. If the schema is not set by SQL Lab it could",
            "        be anything, and we would have to block users from running any queries",
            "        referencing tables without an explicit schema.",
            "        \"\"\"",
            "        return [f'set search_path = \"{schema}\"'] if schema else []",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(cls, extra: dict[str, Any]) -> bool:",
            "        return True",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        sql = f\"EXPLAIN {statement}\"",
            "        cursor.execute(sql)",
            "",
            "        result = cursor.fetchone()[0]",
            "        match = re.search(r\"cost=([\\d\\.]+)\\.\\.([\\d\\.]+)\", result)",
            "        if match:",
            "            return {",
            "                \"Start-up cost\": float(match.group(1)),",
            "                \"Total cost\": float(match.group(2)),",
            "            }",
            "",
            "        return {}",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        return [{k: str(v) for k, v in row.items()} for row in raw_cost]",
            "",
            "    @classmethod",
            "    def get_catalog_names(",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return all catalogs.",
            "",
            "        In Postgres, a catalog is called a \"database\".",
            "        \"\"\"",
            "        return sorted(",
            "            catalog",
            "            for (catalog,) in inspector.bind.execute(",
            "                \"\"\"",
            "SELECT datname FROM pg_database",
            "WHERE datistemplate = false;",
            "            \"\"\"",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def get_table_names(",
            "        cls, database: Database, inspector: PGInspector, schema: str | None",
            "    ) -> set[str]:",
            "        \"\"\"Need to consider foreign tables for PostgreSQL\"\"\"",
            "        return set(inspector.get_table_names(schema)) | set(",
            "            inspector.get_foreign_table_names(schema)",
            "        )",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        For Postgres, the path to a SSL certificate is placed in `connect_args`.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        :raises SupersetException: If database extra json payload is unparseable",
            "        \"\"\"",
            "        try:",
            "            extra = json.loads(database.extra or \"{}\")",
            "        except json.JSONDecodeError as ex:",
            "            raise SupersetException(\"Unable to parse database extras\") from ex",
            "",
            "        if database.server_cert:",
            "            engine_params = extra.get(\"engine_params\", {})",
            "            connect_args = engine_params.get(\"connect_args\", {})",
            "            connect_args[\"sslmode\"] = connect_args.get(\"sslmode\", \"verify-full\")",
            "            path = utils.create_ssl_cert_file(database.server_cert)",
            "            connect_args[\"sslrootcert\"] = path",
            "            engine_params[\"connect_args\"] = connect_args",
            "            extra[\"engine_params\"] = engine_params",
            "        return extra",
            "",
            "    @classmethod",
            "    def get_datatype(cls, type_code: Any) -> str | None:",
            "        # pylint: disable=import-outside-toplevel",
            "        from psycopg2.extensions import binary_types, string_types",
            "",
            "        types = binary_types.copy()",
            "        types.update(string_types)",
            "        if type_code in types:",
            "            return types[type_code].name",
            "        return None",
            "",
            "    @classmethod",
            "    def get_cancel_query_id(cls, cursor: Any, query: Query) -> str | None:",
            "        \"\"\"",
            "        Get Postgres PID that will be used to cancel all other running",
            "        queries in the same session.",
            "",
            "        :param cursor: Cursor instance in which the query will be executed",
            "        :param query: Query instance",
            "        :return: Postgres PID",
            "        \"\"\"",
            "        cursor.execute(\"SELECT pg_backend_pid()\")",
            "        row = cursor.fetchone()",
            "        return row[0]",
            "",
            "    @classmethod",
            "    def cancel_query(cls, cursor: Any, query: Query, cancel_query_id: str) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Postgres PID",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "        try:",
            "            cursor.execute(",
            "                \"SELECT pg_terminate_backend(pid) \"",
            "                \"FROM pg_stat_activity \"",
            "                f\"WHERE pid='{cancel_query_id}'\"",
            "            )",
            "        except Exception:  # pylint: disable=broad-except",
            "            return False",
            "",
            "        return True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "27": [],
            "284": [
                "PostgresEngineSpec",
                "get_default_schema_for_query"
            ],
            "285": [
                "PostgresEngineSpec",
                "get_default_schema_for_query"
            ]
        },
        "addLocation": []
    },
    "superset/errors.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "     RESULTS_BACKEND_ERROR = \"RESULTS_BACKEND_ERROR\""
            },
            "1": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "     ASYNC_WORKERS_ERROR = \"ASYNC_WORKERS_ERROR\""
            },
            "2": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "     ADHOC_SUBQUERY_NOT_ALLOWED_ERROR = \"ADHOC_SUBQUERY_NOT_ALLOWED_ERROR\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+    INVALID_SQL_ERROR = \"INVALID_SQL_ERROR\""
            },
            "4": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 87,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 88,
                "PatchRowcode": "     # Generic errors"
            },
            "6": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "     GENERIC_COMMAND_ERROR = \"GENERIC_COMMAND_ERROR\""
            },
            "7": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 177,
                "PatchRowcode": "     SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR: [1020],"
            },
            "8": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": 178,
                "PatchRowcode": "     SupersetErrorType.INVALID_CTAS_QUERY_ERROR: [1023],"
            },
            "9": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": 179,
                "PatchRowcode": "     SupersetErrorType.INVALID_CVAS_QUERY_ERROR: [1024, 1025],"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 180,
                "PatchRowcode": "+    SupersetErrorType.INVALID_SQL_ERROR: [1003],"
            },
            "11": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "     SupersetErrorType.SQLLAB_TIMEOUT_ERROR: [1026, 1027],"
            },
            "12": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "     SupersetErrorType.OBJECT_DOES_NOT_EXIST_ERROR: [1029],"
            },
            "13": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 183,
                "PatchRowcode": "     SupersetErrorType.SYNTAX_ERROR: [1030],"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from dataclasses import dataclass",
            "from typing import Any, Optional",
            "",
            "from flask_babel import lazy_gettext as _",
            "",
            "from superset.utils.backports import StrEnum",
            "",
            "",
            "class SupersetErrorType(StrEnum):",
            "    \"\"\"",
            "    Types of errors that can exist within Superset.",
            "",
            "    Keep in sync with superset-frontend/src/components/ErrorMessage/types.ts",
            "    \"\"\"",
            "",
            "    # Frontend errors",
            "    FRONTEND_CSRF_ERROR = \"FRONTEND_CSRF_ERROR\"",
            "    FRONTEND_NETWORK_ERROR = \"FRONTEND_NETWORK_ERROR\"",
            "    FRONTEND_TIMEOUT_ERROR = \"FRONTEND_TIMEOUT_ERROR\"",
            "",
            "    # DB Engine errors",
            "    GENERIC_DB_ENGINE_ERROR = \"GENERIC_DB_ENGINE_ERROR\"",
            "    COLUMN_DOES_NOT_EXIST_ERROR = \"COLUMN_DOES_NOT_EXIST_ERROR\"",
            "    TABLE_DOES_NOT_EXIST_ERROR = \"TABLE_DOES_NOT_EXIST_ERROR\"",
            "    SCHEMA_DOES_NOT_EXIST_ERROR = \"SCHEMA_DOES_NOT_EXIST_ERROR\"",
            "    CONNECTION_INVALID_USERNAME_ERROR = \"CONNECTION_INVALID_USERNAME_ERROR\"",
            "    CONNECTION_INVALID_PASSWORD_ERROR = \"CONNECTION_INVALID_PASSWORD_ERROR\"",
            "    CONNECTION_INVALID_HOSTNAME_ERROR = \"CONNECTION_INVALID_HOSTNAME_ERROR\"",
            "    CONNECTION_PORT_CLOSED_ERROR = \"CONNECTION_PORT_CLOSED_ERROR\"",
            "    CONNECTION_INVALID_PORT_ERROR = \"CONNECTION_INVALID_PORT_ERROR\"",
            "    CONNECTION_HOST_DOWN_ERROR = \"CONNECTION_HOST_DOWN_ERROR\"",
            "    CONNECTION_ACCESS_DENIED_ERROR = \"CONNECTION_ACCESS_DENIED_ERROR\"",
            "    CONNECTION_UNKNOWN_DATABASE_ERROR = \"CONNECTION_UNKNOWN_DATABASE_ERROR\"",
            "    CONNECTION_DATABASE_PERMISSIONS_ERROR = \"CONNECTION_DATABASE_PERMISSIONS_ERROR\"",
            "    CONNECTION_MISSING_PARAMETERS_ERROR = \"CONNECTION_MISSING_PARAMETERS_ERROR\"",
            "    OBJECT_DOES_NOT_EXIST_ERROR = \"OBJECT_DOES_NOT_EXIST_ERROR\"",
            "    SYNTAX_ERROR = \"SYNTAX_ERROR\"",
            "    CONNECTION_DATABASE_TIMEOUT = \"CONNECTION_DATABASE_TIMEOUT\"",
            "",
            "    # Viz errors",
            "    VIZ_GET_DF_ERROR = \"VIZ_GET_DF_ERROR\"",
            "    UNKNOWN_DATASOURCE_TYPE_ERROR = \"UNKNOWN_DATASOURCE_TYPE_ERROR\"",
            "    FAILED_FETCHING_DATASOURCE_INFO_ERROR = \"FAILED_FETCHING_DATASOURCE_INFO_ERROR\"",
            "",
            "    # Security access errors",
            "    TABLE_SECURITY_ACCESS_ERROR = \"TABLE_SECURITY_ACCESS_ERROR\"",
            "    DATASOURCE_SECURITY_ACCESS_ERROR = \"DATASOURCE_SECURITY_ACCESS_ERROR\"",
            "    DATABASE_SECURITY_ACCESS_ERROR = \"DATABASE_SECURITY_ACCESS_ERROR\"",
            "    QUERY_SECURITY_ACCESS_ERROR = \"QUERY_SECURITY_ACCESS_ERROR\"",
            "    MISSING_OWNERSHIP_ERROR = \"MISSING_OWNERSHIP_ERROR\"",
            "    USER_ACTIVITY_SECURITY_ACCESS_ERROR = \"USER_ACTIVITY_SECURITY_ACCESS_ERROR\"",
            "    DASHBOARD_SECURITY_ACCESS_ERROR = \"DASHBOARD_SECURITY_ACCESS_ERROR\"",
            "    CHART_SECURITY_ACCESS_ERROR = \"CHART_SECURITY_ACCESS_ERROR\"",
            "",
            "    # Other errors",
            "    BACKEND_TIMEOUT_ERROR = \"BACKEND_TIMEOUT_ERROR\"",
            "    DATABASE_NOT_FOUND_ERROR = \"DATABASE_NOT_FOUND_ERROR\"",
            "",
            "    # Sql Lab errors",
            "    MISSING_TEMPLATE_PARAMS_ERROR = \"MISSING_TEMPLATE_PARAMS_ERROR\"",
            "    INVALID_TEMPLATE_PARAMS_ERROR = \"INVALID_TEMPLATE_PARAMS_ERROR\"",
            "    RESULTS_BACKEND_NOT_CONFIGURED_ERROR = \"RESULTS_BACKEND_NOT_CONFIGURED_ERROR\"",
            "    DML_NOT_ALLOWED_ERROR = \"DML_NOT_ALLOWED_ERROR\"",
            "    INVALID_CTAS_QUERY_ERROR = \"INVALID_CTAS_QUERY_ERROR\"",
            "    INVALID_CVAS_QUERY_ERROR = \"INVALID_CVAS_QUERY_ERROR\"",
            "    SQLLAB_TIMEOUT_ERROR = \"SQLLAB_TIMEOUT_ERROR\"",
            "    RESULTS_BACKEND_ERROR = \"RESULTS_BACKEND_ERROR\"",
            "    ASYNC_WORKERS_ERROR = \"ASYNC_WORKERS_ERROR\"",
            "    ADHOC_SUBQUERY_NOT_ALLOWED_ERROR = \"ADHOC_SUBQUERY_NOT_ALLOWED_ERROR\"",
            "",
            "    # Generic errors",
            "    GENERIC_COMMAND_ERROR = \"GENERIC_COMMAND_ERROR\"",
            "    GENERIC_BACKEND_ERROR = \"GENERIC_BACKEND_ERROR\"",
            "",
            "    # API errors",
            "    INVALID_PAYLOAD_FORMAT_ERROR = \"INVALID_PAYLOAD_FORMAT_ERROR\"",
            "    INVALID_PAYLOAD_SCHEMA_ERROR = \"INVALID_PAYLOAD_SCHEMA_ERROR\"",
            "    MARSHMALLOW_ERROR = \"MARSHMALLOW_ERROR\"",
            "",
            "    # Report errors",
            "    REPORT_NOTIFICATION_ERROR = \"REPORT_NOTIFICATION_ERROR\"",
            "",
            "",
            "ISSUE_CODES = {",
            "    1000: _(\"The datasource is too large to query.\"),",
            "    1001: _(\"The database is under an unusual load.\"),",
            "    1002: _(\"The database returned an unexpected error.\"),",
            "    1003: _(",
            "        \"There is a syntax error in the SQL query. \"",
            "        \"Perhaps there was a misspelling or a typo.\"",
            "    ),",
            "    1004: _(\"The column was deleted or renamed in the database.\"),",
            "    1005: _(\"The table was deleted or renamed in the database.\"),",
            "    1006: _(\"One or more parameters specified in the query are missing.\"),",
            "    1007: _(\"The hostname provided can't be resolved.\"),",
            "    1008: _(\"The port is closed.\"),",
            "    1009: _(\"The host might be down, and can't be reached on the provided port.\"),",
            "    1010: _(\"Superset encountered an error while running a command.\"),",
            "    1011: _(\"Superset encountered an unexpected error.\"),",
            "    1012: _(\"The username provided when connecting to a database is not valid.\"),",
            "    1013: _(\"The password provided when connecting to a database is not valid.\"),",
            "    1014: _(\"Either the username or the password is wrong.\"),",
            "    1015: _(\"Either the database is spelled incorrectly or does not exist.\"),",
            "    1016: _(\"The schema was deleted or renamed in the database.\"),",
            "    1017: _(\"User doesn't have the proper permissions.\"),",
            "    1018: _(\"One or more parameters needed to configure a database are missing.\"),",
            "    1019: _(\"The submitted payload has the incorrect format.\"),",
            "    1020: _(\"The submitted payload has the incorrect schema.\"),",
            "    1021: _(\"Results backend needed for asynchronous queries is not configured.\"),",
            "    1022: _(\"Database does not allow data manipulation.\"),",
            "    1023: _(",
            "        \"The CTAS (create table as select) doesn't have a \"",
            "        \"SELECT statement at the end. Please make sure your query has a \"",
            "        \"SELECT as its last statement. Then, try running your query again.\"",
            "    ),",
            "    1024: _(\"CVAS (create view as select) query has more than one statement.\"),",
            "    1025: _(\"CVAS (create view as select) query is not a SELECT statement.\"),",
            "    1026: _(\"Query is too complex and takes too long to run.\"),",
            "    1027: _(\"The database is currently running too many queries.\"),",
            "    1028: _(\"One or more parameters specified in the query are malformed.\"),",
            "    1029: _(\"The object does not exist in the given database.\"),",
            "    1030: _(\"The query has a syntax error.\"),",
            "    1031: _(\"The results backend no longer has the data from the query.\"),",
            "    1032: _(\"The query associated with the results was deleted.\"),",
            "    1033: _(",
            "        \"The results stored in the backend were stored in a \"",
            "        \"different format, and no longer can be deserialized.\"",
            "    ),",
            "    1034: _(\"The port number is invalid.\"),",
            "    1035: _(\"Failed to start remote query on a worker.\"),",
            "    1036: _(\"The database was deleted.\"),",
            "    1037: _(\"Custom SQL fields cannot contain sub-queries.\"),",
            "    1040: _(\"The submitted payload failed validation.\"),",
            "}",
            "",
            "",
            "ERROR_TYPES_TO_ISSUE_CODES_MAPPING = {",
            "    SupersetErrorType.ADHOC_SUBQUERY_NOT_ALLOWED_ERROR: [1037],",
            "    SupersetErrorType.BACKEND_TIMEOUT_ERROR: [1000, 1001],",
            "    SupersetErrorType.GENERIC_DB_ENGINE_ERROR: [1002],",
            "    SupersetErrorType.COLUMN_DOES_NOT_EXIST_ERROR: [1003, 1004],",
            "    SupersetErrorType.TABLE_DOES_NOT_EXIST_ERROR: [1003, 1005],",
            "    SupersetErrorType.SCHEMA_DOES_NOT_EXIST_ERROR: [1003, 1016],",
            "    SupersetErrorType.MISSING_TEMPLATE_PARAMS_ERROR: [1006],",
            "    SupersetErrorType.INVALID_TEMPLATE_PARAMS_ERROR: [1028],",
            "    SupersetErrorType.RESULTS_BACKEND_NOT_CONFIGURED_ERROR: [1021],",
            "    SupersetErrorType.DML_NOT_ALLOWED_ERROR: [1022],",
            "    SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR: [1007],",
            "    SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR: [1008],",
            "    SupersetErrorType.CONNECTION_HOST_DOWN_ERROR: [1009],",
            "    SupersetErrorType.GENERIC_COMMAND_ERROR: [1010],",
            "    SupersetErrorType.GENERIC_BACKEND_ERROR: [1011],",
            "    SupersetErrorType.CONNECTION_INVALID_USERNAME_ERROR: [1012],",
            "    SupersetErrorType.CONNECTION_INVALID_PASSWORD_ERROR: [1013],",
            "    SupersetErrorType.CONNECTION_ACCESS_DENIED_ERROR: [1014, 1015],",
            "    SupersetErrorType.CONNECTION_UNKNOWN_DATABASE_ERROR: [1015],",
            "    SupersetErrorType.CONNECTION_DATABASE_PERMISSIONS_ERROR: [1017],",
            "    SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR: [1018],",
            "    SupersetErrorType.INVALID_PAYLOAD_FORMAT_ERROR: [1019],",
            "    SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR: [1020],",
            "    SupersetErrorType.INVALID_CTAS_QUERY_ERROR: [1023],",
            "    SupersetErrorType.INVALID_CVAS_QUERY_ERROR: [1024, 1025],",
            "    SupersetErrorType.SQLLAB_TIMEOUT_ERROR: [1026, 1027],",
            "    SupersetErrorType.OBJECT_DOES_NOT_EXIST_ERROR: [1029],",
            "    SupersetErrorType.SYNTAX_ERROR: [1030],",
            "    SupersetErrorType.RESULTS_BACKEND_ERROR: [1031, 1032, 1033],",
            "    SupersetErrorType.CONNECTION_INVALID_PORT_ERROR: [1034],",
            "    SupersetErrorType.ASYNC_WORKERS_ERROR: [1035],",
            "    SupersetErrorType.DATABASE_NOT_FOUND_ERROR: [1011, 1036],",
            "    SupersetErrorType.CONNECTION_DATABASE_TIMEOUT: [1001, 1009],",
            "    SupersetErrorType.MARSHMALLOW_ERROR: [1040],",
            "}",
            "",
            "",
            "class ErrorLevel(StrEnum):",
            "    \"\"\"",
            "    Levels of errors that can exist within Superset.",
            "",
            "    Keep in sync with superset-frontend/src/components/ErrorMessage/types.ts",
            "    \"\"\"",
            "",
            "    INFO = \"info\"",
            "    WARNING = \"warning\"",
            "    ERROR = \"error\"",
            "",
            "",
            "@dataclass",
            "class SupersetError:",
            "    \"\"\"",
            "    An error that is returned to a client.",
            "    \"\"\"",
            "",
            "    message: str",
            "    error_type: SupersetErrorType",
            "    level: ErrorLevel",
            "    extra: Optional[dict[str, Any]] = None",
            "",
            "    def __post_init__(self) -> None:",
            "        \"\"\"",
            "        Mutates the extra params with user facing error codes that map to backend",
            "        errors.",
            "        \"\"\"",
            "        if issue_codes := ERROR_TYPES_TO_ISSUE_CODES_MAPPING.get(self.error_type):",
            "            self.extra = self.extra or {}",
            "            self.extra.update(",
            "                {",
            "                    \"issue_codes\": [",
            "                        {",
            "                            \"code\": issue_code,",
            "                            \"message\": (",
            "                                f\"Issue {issue_code} - {ISSUE_CODES[issue_code]}\"",
            "                            ),",
            "                        }",
            "                        for issue_code in issue_codes",
            "                    ]",
            "                }",
            "            )",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        rv = {\"message\": self.message, \"error_type\": self.error_type}",
            "        if self.extra:",
            "            rv[\"extra\"] = self.extra  # type: ignore",
            "        return rv"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from dataclasses import dataclass",
            "from typing import Any, Optional",
            "",
            "from flask_babel import lazy_gettext as _",
            "",
            "from superset.utils.backports import StrEnum",
            "",
            "",
            "class SupersetErrorType(StrEnum):",
            "    \"\"\"",
            "    Types of errors that can exist within Superset.",
            "",
            "    Keep in sync with superset-frontend/src/components/ErrorMessage/types.ts",
            "    \"\"\"",
            "",
            "    # Frontend errors",
            "    FRONTEND_CSRF_ERROR = \"FRONTEND_CSRF_ERROR\"",
            "    FRONTEND_NETWORK_ERROR = \"FRONTEND_NETWORK_ERROR\"",
            "    FRONTEND_TIMEOUT_ERROR = \"FRONTEND_TIMEOUT_ERROR\"",
            "",
            "    # DB Engine errors",
            "    GENERIC_DB_ENGINE_ERROR = \"GENERIC_DB_ENGINE_ERROR\"",
            "    COLUMN_DOES_NOT_EXIST_ERROR = \"COLUMN_DOES_NOT_EXIST_ERROR\"",
            "    TABLE_DOES_NOT_EXIST_ERROR = \"TABLE_DOES_NOT_EXIST_ERROR\"",
            "    SCHEMA_DOES_NOT_EXIST_ERROR = \"SCHEMA_DOES_NOT_EXIST_ERROR\"",
            "    CONNECTION_INVALID_USERNAME_ERROR = \"CONNECTION_INVALID_USERNAME_ERROR\"",
            "    CONNECTION_INVALID_PASSWORD_ERROR = \"CONNECTION_INVALID_PASSWORD_ERROR\"",
            "    CONNECTION_INVALID_HOSTNAME_ERROR = \"CONNECTION_INVALID_HOSTNAME_ERROR\"",
            "    CONNECTION_PORT_CLOSED_ERROR = \"CONNECTION_PORT_CLOSED_ERROR\"",
            "    CONNECTION_INVALID_PORT_ERROR = \"CONNECTION_INVALID_PORT_ERROR\"",
            "    CONNECTION_HOST_DOWN_ERROR = \"CONNECTION_HOST_DOWN_ERROR\"",
            "    CONNECTION_ACCESS_DENIED_ERROR = \"CONNECTION_ACCESS_DENIED_ERROR\"",
            "    CONNECTION_UNKNOWN_DATABASE_ERROR = \"CONNECTION_UNKNOWN_DATABASE_ERROR\"",
            "    CONNECTION_DATABASE_PERMISSIONS_ERROR = \"CONNECTION_DATABASE_PERMISSIONS_ERROR\"",
            "    CONNECTION_MISSING_PARAMETERS_ERROR = \"CONNECTION_MISSING_PARAMETERS_ERROR\"",
            "    OBJECT_DOES_NOT_EXIST_ERROR = \"OBJECT_DOES_NOT_EXIST_ERROR\"",
            "    SYNTAX_ERROR = \"SYNTAX_ERROR\"",
            "    CONNECTION_DATABASE_TIMEOUT = \"CONNECTION_DATABASE_TIMEOUT\"",
            "",
            "    # Viz errors",
            "    VIZ_GET_DF_ERROR = \"VIZ_GET_DF_ERROR\"",
            "    UNKNOWN_DATASOURCE_TYPE_ERROR = \"UNKNOWN_DATASOURCE_TYPE_ERROR\"",
            "    FAILED_FETCHING_DATASOURCE_INFO_ERROR = \"FAILED_FETCHING_DATASOURCE_INFO_ERROR\"",
            "",
            "    # Security access errors",
            "    TABLE_SECURITY_ACCESS_ERROR = \"TABLE_SECURITY_ACCESS_ERROR\"",
            "    DATASOURCE_SECURITY_ACCESS_ERROR = \"DATASOURCE_SECURITY_ACCESS_ERROR\"",
            "    DATABASE_SECURITY_ACCESS_ERROR = \"DATABASE_SECURITY_ACCESS_ERROR\"",
            "    QUERY_SECURITY_ACCESS_ERROR = \"QUERY_SECURITY_ACCESS_ERROR\"",
            "    MISSING_OWNERSHIP_ERROR = \"MISSING_OWNERSHIP_ERROR\"",
            "    USER_ACTIVITY_SECURITY_ACCESS_ERROR = \"USER_ACTIVITY_SECURITY_ACCESS_ERROR\"",
            "    DASHBOARD_SECURITY_ACCESS_ERROR = \"DASHBOARD_SECURITY_ACCESS_ERROR\"",
            "    CHART_SECURITY_ACCESS_ERROR = \"CHART_SECURITY_ACCESS_ERROR\"",
            "",
            "    # Other errors",
            "    BACKEND_TIMEOUT_ERROR = \"BACKEND_TIMEOUT_ERROR\"",
            "    DATABASE_NOT_FOUND_ERROR = \"DATABASE_NOT_FOUND_ERROR\"",
            "",
            "    # Sql Lab errors",
            "    MISSING_TEMPLATE_PARAMS_ERROR = \"MISSING_TEMPLATE_PARAMS_ERROR\"",
            "    INVALID_TEMPLATE_PARAMS_ERROR = \"INVALID_TEMPLATE_PARAMS_ERROR\"",
            "    RESULTS_BACKEND_NOT_CONFIGURED_ERROR = \"RESULTS_BACKEND_NOT_CONFIGURED_ERROR\"",
            "    DML_NOT_ALLOWED_ERROR = \"DML_NOT_ALLOWED_ERROR\"",
            "    INVALID_CTAS_QUERY_ERROR = \"INVALID_CTAS_QUERY_ERROR\"",
            "    INVALID_CVAS_QUERY_ERROR = \"INVALID_CVAS_QUERY_ERROR\"",
            "    SQLLAB_TIMEOUT_ERROR = \"SQLLAB_TIMEOUT_ERROR\"",
            "    RESULTS_BACKEND_ERROR = \"RESULTS_BACKEND_ERROR\"",
            "    ASYNC_WORKERS_ERROR = \"ASYNC_WORKERS_ERROR\"",
            "    ADHOC_SUBQUERY_NOT_ALLOWED_ERROR = \"ADHOC_SUBQUERY_NOT_ALLOWED_ERROR\"",
            "    INVALID_SQL_ERROR = \"INVALID_SQL_ERROR\"",
            "",
            "    # Generic errors",
            "    GENERIC_COMMAND_ERROR = \"GENERIC_COMMAND_ERROR\"",
            "    GENERIC_BACKEND_ERROR = \"GENERIC_BACKEND_ERROR\"",
            "",
            "    # API errors",
            "    INVALID_PAYLOAD_FORMAT_ERROR = \"INVALID_PAYLOAD_FORMAT_ERROR\"",
            "    INVALID_PAYLOAD_SCHEMA_ERROR = \"INVALID_PAYLOAD_SCHEMA_ERROR\"",
            "    MARSHMALLOW_ERROR = \"MARSHMALLOW_ERROR\"",
            "",
            "    # Report errors",
            "    REPORT_NOTIFICATION_ERROR = \"REPORT_NOTIFICATION_ERROR\"",
            "",
            "",
            "ISSUE_CODES = {",
            "    1000: _(\"The datasource is too large to query.\"),",
            "    1001: _(\"The database is under an unusual load.\"),",
            "    1002: _(\"The database returned an unexpected error.\"),",
            "    1003: _(",
            "        \"There is a syntax error in the SQL query. \"",
            "        \"Perhaps there was a misspelling or a typo.\"",
            "    ),",
            "    1004: _(\"The column was deleted or renamed in the database.\"),",
            "    1005: _(\"The table was deleted or renamed in the database.\"),",
            "    1006: _(\"One or more parameters specified in the query are missing.\"),",
            "    1007: _(\"The hostname provided can't be resolved.\"),",
            "    1008: _(\"The port is closed.\"),",
            "    1009: _(\"The host might be down, and can't be reached on the provided port.\"),",
            "    1010: _(\"Superset encountered an error while running a command.\"),",
            "    1011: _(\"Superset encountered an unexpected error.\"),",
            "    1012: _(\"The username provided when connecting to a database is not valid.\"),",
            "    1013: _(\"The password provided when connecting to a database is not valid.\"),",
            "    1014: _(\"Either the username or the password is wrong.\"),",
            "    1015: _(\"Either the database is spelled incorrectly or does not exist.\"),",
            "    1016: _(\"The schema was deleted or renamed in the database.\"),",
            "    1017: _(\"User doesn't have the proper permissions.\"),",
            "    1018: _(\"One or more parameters needed to configure a database are missing.\"),",
            "    1019: _(\"The submitted payload has the incorrect format.\"),",
            "    1020: _(\"The submitted payload has the incorrect schema.\"),",
            "    1021: _(\"Results backend needed for asynchronous queries is not configured.\"),",
            "    1022: _(\"Database does not allow data manipulation.\"),",
            "    1023: _(",
            "        \"The CTAS (create table as select) doesn't have a \"",
            "        \"SELECT statement at the end. Please make sure your query has a \"",
            "        \"SELECT as its last statement. Then, try running your query again.\"",
            "    ),",
            "    1024: _(\"CVAS (create view as select) query has more than one statement.\"),",
            "    1025: _(\"CVAS (create view as select) query is not a SELECT statement.\"),",
            "    1026: _(\"Query is too complex and takes too long to run.\"),",
            "    1027: _(\"The database is currently running too many queries.\"),",
            "    1028: _(\"One or more parameters specified in the query are malformed.\"),",
            "    1029: _(\"The object does not exist in the given database.\"),",
            "    1030: _(\"The query has a syntax error.\"),",
            "    1031: _(\"The results backend no longer has the data from the query.\"),",
            "    1032: _(\"The query associated with the results was deleted.\"),",
            "    1033: _(",
            "        \"The results stored in the backend were stored in a \"",
            "        \"different format, and no longer can be deserialized.\"",
            "    ),",
            "    1034: _(\"The port number is invalid.\"),",
            "    1035: _(\"Failed to start remote query on a worker.\"),",
            "    1036: _(\"The database was deleted.\"),",
            "    1037: _(\"Custom SQL fields cannot contain sub-queries.\"),",
            "    1040: _(\"The submitted payload failed validation.\"),",
            "}",
            "",
            "",
            "ERROR_TYPES_TO_ISSUE_CODES_MAPPING = {",
            "    SupersetErrorType.ADHOC_SUBQUERY_NOT_ALLOWED_ERROR: [1037],",
            "    SupersetErrorType.BACKEND_TIMEOUT_ERROR: [1000, 1001],",
            "    SupersetErrorType.GENERIC_DB_ENGINE_ERROR: [1002],",
            "    SupersetErrorType.COLUMN_DOES_NOT_EXIST_ERROR: [1003, 1004],",
            "    SupersetErrorType.TABLE_DOES_NOT_EXIST_ERROR: [1003, 1005],",
            "    SupersetErrorType.SCHEMA_DOES_NOT_EXIST_ERROR: [1003, 1016],",
            "    SupersetErrorType.MISSING_TEMPLATE_PARAMS_ERROR: [1006],",
            "    SupersetErrorType.INVALID_TEMPLATE_PARAMS_ERROR: [1028],",
            "    SupersetErrorType.RESULTS_BACKEND_NOT_CONFIGURED_ERROR: [1021],",
            "    SupersetErrorType.DML_NOT_ALLOWED_ERROR: [1022],",
            "    SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR: [1007],",
            "    SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR: [1008],",
            "    SupersetErrorType.CONNECTION_HOST_DOWN_ERROR: [1009],",
            "    SupersetErrorType.GENERIC_COMMAND_ERROR: [1010],",
            "    SupersetErrorType.GENERIC_BACKEND_ERROR: [1011],",
            "    SupersetErrorType.CONNECTION_INVALID_USERNAME_ERROR: [1012],",
            "    SupersetErrorType.CONNECTION_INVALID_PASSWORD_ERROR: [1013],",
            "    SupersetErrorType.CONNECTION_ACCESS_DENIED_ERROR: [1014, 1015],",
            "    SupersetErrorType.CONNECTION_UNKNOWN_DATABASE_ERROR: [1015],",
            "    SupersetErrorType.CONNECTION_DATABASE_PERMISSIONS_ERROR: [1017],",
            "    SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR: [1018],",
            "    SupersetErrorType.INVALID_PAYLOAD_FORMAT_ERROR: [1019],",
            "    SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR: [1020],",
            "    SupersetErrorType.INVALID_CTAS_QUERY_ERROR: [1023],",
            "    SupersetErrorType.INVALID_CVAS_QUERY_ERROR: [1024, 1025],",
            "    SupersetErrorType.INVALID_SQL_ERROR: [1003],",
            "    SupersetErrorType.SQLLAB_TIMEOUT_ERROR: [1026, 1027],",
            "    SupersetErrorType.OBJECT_DOES_NOT_EXIST_ERROR: [1029],",
            "    SupersetErrorType.SYNTAX_ERROR: [1030],",
            "    SupersetErrorType.RESULTS_BACKEND_ERROR: [1031, 1032, 1033],",
            "    SupersetErrorType.CONNECTION_INVALID_PORT_ERROR: [1034],",
            "    SupersetErrorType.ASYNC_WORKERS_ERROR: [1035],",
            "    SupersetErrorType.DATABASE_NOT_FOUND_ERROR: [1011, 1036],",
            "    SupersetErrorType.CONNECTION_DATABASE_TIMEOUT: [1001, 1009],",
            "    SupersetErrorType.MARSHMALLOW_ERROR: [1040],",
            "}",
            "",
            "",
            "class ErrorLevel(StrEnum):",
            "    \"\"\"",
            "    Levels of errors that can exist within Superset.",
            "",
            "    Keep in sync with superset-frontend/src/components/ErrorMessage/types.ts",
            "    \"\"\"",
            "",
            "    INFO = \"info\"",
            "    WARNING = \"warning\"",
            "    ERROR = \"error\"",
            "",
            "",
            "@dataclass",
            "class SupersetError:",
            "    \"\"\"",
            "    An error that is returned to a client.",
            "    \"\"\"",
            "",
            "    message: str",
            "    error_type: SupersetErrorType",
            "    level: ErrorLevel",
            "    extra: Optional[dict[str, Any]] = None",
            "",
            "    def __post_init__(self) -> None:",
            "        \"\"\"",
            "        Mutates the extra params with user facing error codes that map to backend",
            "        errors.",
            "        \"\"\"",
            "        if issue_codes := ERROR_TYPES_TO_ISSUE_CODES_MAPPING.get(self.error_type):",
            "            self.extra = self.extra or {}",
            "            self.extra.update(",
            "                {",
            "                    \"issue_codes\": [",
            "                        {",
            "                            \"code\": issue_code,",
            "                            \"message\": (",
            "                                f\"Issue {issue_code} - {ISSUE_CODES[issue_code]}\"",
            "                            ),",
            "                        }",
            "                        for issue_code in issue_codes",
            "                    ]",
            "                }",
            "            )",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        rv = {\"message\": self.message, \"error_type\": self.error_type}",
            "        if self.extra:",
            "            rv[\"extra\"] = self.extra  # type: ignore",
            "        return rv"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "superset.errors.SupersetErrorType.self"
        ]
    },
    "superset/exceptions.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": 295,
                "PatchRowcode": "             extra={\"messages\": exc.messages, \"payload\": payload},"
            },
            "1": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": 296,
                "PatchRowcode": "         )"
            },
            "2": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": 297,
                "PatchRowcode": "         super().__init__(error)"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 298,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 299,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 300,
                "PatchRowcode": "+class SupersetParseError(SupersetErrorException):"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 301,
                "PatchRowcode": "+    \"\"\""
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+    Exception to be raised when we fail to parse SQL."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+    \"\"\""
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 304,
                "PatchRowcode": "+"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 305,
                "PatchRowcode": "+    status = 422"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 306,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 307,
                "PatchRowcode": "+    def __init__(self, sql: str, engine: Optional[str] = None):"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 308,
                "PatchRowcode": "+        error = SupersetError("
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 309,
                "PatchRowcode": "+            message=_(\"The SQL is invalid and cannot be parsed.\"),"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 310,
                "PatchRowcode": "+            error_type=SupersetErrorType.INVALID_SQL_ERROR,"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 311,
                "PatchRowcode": "+            level=ErrorLevel.ERROR,"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 312,
                "PatchRowcode": "+            extra={\"sql\": sql, \"engine\": engine},"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 313,
                "PatchRowcode": "+        )"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 314,
                "PatchRowcode": "+        super().__init__(error)"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from collections import defaultdict",
            "from typing import Any, Optional",
            "",
            "from flask_babel import gettext as _",
            "from marshmallow import ValidationError",
            "",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "",
            "",
            "class SupersetException(Exception):",
            "    status = 500",
            "    message = \"\"",
            "",
            "    def __init__(",
            "        self,",
            "        message: str = \"\",",
            "        exception: Optional[Exception] = None,",
            "        error_type: Optional[SupersetErrorType] = None,",
            "    ) -> None:",
            "        if message:",
            "            self.message = message",
            "        self._exception = exception",
            "        self._error_type = error_type",
            "        super().__init__(self.message)",
            "",
            "    @property",
            "    def exception(self) -> Optional[Exception]:",
            "        return self._exception",
            "",
            "    @property",
            "    def error_type(self) -> Optional[SupersetErrorType]:",
            "        return self._error_type",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        rv = {}",
            "        if hasattr(self, \"message\"):",
            "            rv[\"message\"] = self.message",
            "        if self.error_type:",
            "            rv[\"error_type\"] = self.error_type",
            "        if self.exception is not None and hasattr(self.exception, \"to_dict\"):",
            "            rv = {**rv, **self.exception.to_dict()}",
            "        return rv",
            "",
            "",
            "class SupersetErrorException(SupersetException):",
            "    \"\"\"Exceptions with a single SupersetErrorType associated with them\"\"\"",
            "",
            "    def __init__(self, error: SupersetError, status: Optional[int] = None) -> None:",
            "        super().__init__(error.message)",
            "        self.error = error",
            "        if status is not None:",
            "            self.status = status",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return self.error.to_dict()",
            "",
            "",
            "class SupersetGenericErrorException(SupersetErrorException):",
            "    \"\"\"Exceptions that are too generic to have their own type\"\"\"",
            "",
            "    def __init__(self, message: str, status: Optional[int] = None) -> None:",
            "        super().__init__(",
            "            SupersetError(",
            "                message=message,",
            "                error_type=SupersetErrorType.GENERIC_BACKEND_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "        if status is not None:",
            "            self.status = status",
            "",
            "",
            "class SupersetErrorFromParamsException(SupersetErrorException):",
            "    \"\"\"Exceptions that pass in parameters to construct a SupersetError\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        error_type: SupersetErrorType,",
            "        message: str,",
            "        level: ErrorLevel,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            SupersetError(",
            "                error_type=error_type, message=message, level=level, extra=extra or {}",
            "            )",
            "        )",
            "",
            "",
            "class SupersetErrorsException(SupersetException):",
            "    \"\"\"Exceptions with multiple SupersetErrorType associated with them\"\"\"",
            "",
            "    def __init__(",
            "        self, errors: list[SupersetError], status: Optional[int] = None",
            "    ) -> None:",
            "        super().__init__(str(errors))",
            "        self.errors = errors",
            "        if status is not None:",
            "            self.status = status",
            "",
            "",
            "class SupersetSyntaxErrorException(SupersetErrorsException):",
            "    status = 422",
            "    error_type = SupersetErrorType.SYNTAX_ERROR",
            "",
            "    def __init__(self, errors: list[SupersetError]) -> None:",
            "        super().__init__(errors)",
            "",
            "",
            "class SupersetTimeoutException(SupersetErrorFromParamsException):",
            "    status = 408",
            "",
            "",
            "class SupersetGenericDBErrorException(SupersetErrorFromParamsException):",
            "    status = 400",
            "",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        level: ErrorLevel = ErrorLevel.ERROR,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "            message,",
            "            level,",
            "            extra,",
            "        )",
            "",
            "",
            "class SupersetTemplateParamsErrorException(SupersetErrorFromParamsException):",
            "    status = 400",
            "",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        error: SupersetErrorType,",
            "        level: ErrorLevel = ErrorLevel.ERROR,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            error,",
            "            message,",
            "            level,",
            "            extra,",
            "        )",
            "",
            "",
            "class SupersetSecurityException(SupersetErrorException):",
            "    status = 403",
            "",
            "    def __init__(",
            "        self, error: SupersetError, payload: Optional[dict[str, Any]] = None",
            "    ) -> None:",
            "        super().__init__(error)",
            "        self.payload = payload",
            "",
            "",
            "class SupersetVizException(SupersetErrorsException):",
            "    status = 400",
            "",
            "",
            "class NoDataException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class NullValueException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class SupersetTemplateException(SupersetException):",
            "    pass",
            "",
            "",
            "class SpatialException(SupersetException):",
            "    pass",
            "",
            "",
            "class CertificateException(SupersetException):",
            "    message = _(\"Invalid certificate\")",
            "",
            "",
            "class DatabaseNotFound(SupersetException):",
            "    status = 400",
            "",
            "",
            "class MissingUserContextException(SupersetException):",
            "    status = 422",
            "",
            "",
            "class QueryObjectValidationError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class AdvancedDataTypeResponseError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class InvalidPostProcessingError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class CacheLoadError(SupersetException):",
            "    status = 404",
            "",
            "",
            "class QueryClauseValidationException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class DashboardImportException(SupersetException):",
            "    pass",
            "",
            "",
            "class DatasetInvalidPermissionEvaluationException(SupersetException):",
            "    \"\"\"",
            "    When a dataset can't compute its permission name",
            "    \"\"\"",
            "",
            "",
            "class SerializationError(SupersetException):",
            "    pass",
            "",
            "",
            "class InvalidPayloadFormatError(SupersetErrorException):",
            "    status = 400",
            "",
            "    def __init__(self, message: str = \"Request payload has incorrect format\"):",
            "        error = SupersetError(",
            "            message=message,",
            "            error_type=SupersetErrorType.INVALID_PAYLOAD_FORMAT_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class InvalidPayloadSchemaError(SupersetErrorException):",
            "    status = 422",
            "",
            "    def __init__(self, error: ValidationError):",
            "        # dataclasses.asdict does not work with defaultdict, convert to dict",
            "        # https://bugs.python.org/issue35540",
            "        for k, v in error.messages.items():",
            "            if isinstance(v, defaultdict):",
            "                error.messages[k] = dict(v)",
            "        error = SupersetError(",
            "            message=\"An error happened when validating the request\",",
            "            error_type=SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"messages\": error.messages},",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class SupersetCancelQueryException(SupersetException):",
            "    status = 422",
            "",
            "",
            "class QueryNotFoundException(SupersetException):",
            "    status = 404",
            "",
            "",
            "class ColumnNotFoundException(SupersetException):",
            "    status = 404",
            "",
            "",
            "class SupersetMarshmallowValidationError(SupersetErrorException):",
            "    \"\"\"",
            "    Exception to be raised for Marshmallow validation errors.",
            "    \"\"\"",
            "",
            "    status = 422",
            "",
            "    def __init__(self, exc: ValidationError, payload: dict[str, Any]):",
            "        error = SupersetError(",
            "            message=_(\"The schema of the submitted payload is invalid.\"),",
            "            error_type=SupersetErrorType.MARSHMALLOW_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"messages\": exc.messages, \"payload\": payload},",
            "        )",
            "        super().__init__(error)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from collections import defaultdict",
            "from typing import Any, Optional",
            "",
            "from flask_babel import gettext as _",
            "from marshmallow import ValidationError",
            "",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "",
            "",
            "class SupersetException(Exception):",
            "    status = 500",
            "    message = \"\"",
            "",
            "    def __init__(",
            "        self,",
            "        message: str = \"\",",
            "        exception: Optional[Exception] = None,",
            "        error_type: Optional[SupersetErrorType] = None,",
            "    ) -> None:",
            "        if message:",
            "            self.message = message",
            "        self._exception = exception",
            "        self._error_type = error_type",
            "        super().__init__(self.message)",
            "",
            "    @property",
            "    def exception(self) -> Optional[Exception]:",
            "        return self._exception",
            "",
            "    @property",
            "    def error_type(self) -> Optional[SupersetErrorType]:",
            "        return self._error_type",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        rv = {}",
            "        if hasattr(self, \"message\"):",
            "            rv[\"message\"] = self.message",
            "        if self.error_type:",
            "            rv[\"error_type\"] = self.error_type",
            "        if self.exception is not None and hasattr(self.exception, \"to_dict\"):",
            "            rv = {**rv, **self.exception.to_dict()}",
            "        return rv",
            "",
            "",
            "class SupersetErrorException(SupersetException):",
            "    \"\"\"Exceptions with a single SupersetErrorType associated with them\"\"\"",
            "",
            "    def __init__(self, error: SupersetError, status: Optional[int] = None) -> None:",
            "        super().__init__(error.message)",
            "        self.error = error",
            "        if status is not None:",
            "            self.status = status",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return self.error.to_dict()",
            "",
            "",
            "class SupersetGenericErrorException(SupersetErrorException):",
            "    \"\"\"Exceptions that are too generic to have their own type\"\"\"",
            "",
            "    def __init__(self, message: str, status: Optional[int] = None) -> None:",
            "        super().__init__(",
            "            SupersetError(",
            "                message=message,",
            "                error_type=SupersetErrorType.GENERIC_BACKEND_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "        if status is not None:",
            "            self.status = status",
            "",
            "",
            "class SupersetErrorFromParamsException(SupersetErrorException):",
            "    \"\"\"Exceptions that pass in parameters to construct a SupersetError\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        error_type: SupersetErrorType,",
            "        message: str,",
            "        level: ErrorLevel,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            SupersetError(",
            "                error_type=error_type, message=message, level=level, extra=extra or {}",
            "            )",
            "        )",
            "",
            "",
            "class SupersetErrorsException(SupersetException):",
            "    \"\"\"Exceptions with multiple SupersetErrorType associated with them\"\"\"",
            "",
            "    def __init__(",
            "        self, errors: list[SupersetError], status: Optional[int] = None",
            "    ) -> None:",
            "        super().__init__(str(errors))",
            "        self.errors = errors",
            "        if status is not None:",
            "            self.status = status",
            "",
            "",
            "class SupersetSyntaxErrorException(SupersetErrorsException):",
            "    status = 422",
            "    error_type = SupersetErrorType.SYNTAX_ERROR",
            "",
            "    def __init__(self, errors: list[SupersetError]) -> None:",
            "        super().__init__(errors)",
            "",
            "",
            "class SupersetTimeoutException(SupersetErrorFromParamsException):",
            "    status = 408",
            "",
            "",
            "class SupersetGenericDBErrorException(SupersetErrorFromParamsException):",
            "    status = 400",
            "",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        level: ErrorLevel = ErrorLevel.ERROR,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "            message,",
            "            level,",
            "            extra,",
            "        )",
            "",
            "",
            "class SupersetTemplateParamsErrorException(SupersetErrorFromParamsException):",
            "    status = 400",
            "",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        error: SupersetErrorType,",
            "        level: ErrorLevel = ErrorLevel.ERROR,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            error,",
            "            message,",
            "            level,",
            "            extra,",
            "        )",
            "",
            "",
            "class SupersetSecurityException(SupersetErrorException):",
            "    status = 403",
            "",
            "    def __init__(",
            "        self, error: SupersetError, payload: Optional[dict[str, Any]] = None",
            "    ) -> None:",
            "        super().__init__(error)",
            "        self.payload = payload",
            "",
            "",
            "class SupersetVizException(SupersetErrorsException):",
            "    status = 400",
            "",
            "",
            "class NoDataException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class NullValueException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class SupersetTemplateException(SupersetException):",
            "    pass",
            "",
            "",
            "class SpatialException(SupersetException):",
            "    pass",
            "",
            "",
            "class CertificateException(SupersetException):",
            "    message = _(\"Invalid certificate\")",
            "",
            "",
            "class DatabaseNotFound(SupersetException):",
            "    status = 400",
            "",
            "",
            "class MissingUserContextException(SupersetException):",
            "    status = 422",
            "",
            "",
            "class QueryObjectValidationError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class AdvancedDataTypeResponseError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class InvalidPostProcessingError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class CacheLoadError(SupersetException):",
            "    status = 404",
            "",
            "",
            "class QueryClauseValidationException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class DashboardImportException(SupersetException):",
            "    pass",
            "",
            "",
            "class DatasetInvalidPermissionEvaluationException(SupersetException):",
            "    \"\"\"",
            "    When a dataset can't compute its permission name",
            "    \"\"\"",
            "",
            "",
            "class SerializationError(SupersetException):",
            "    pass",
            "",
            "",
            "class InvalidPayloadFormatError(SupersetErrorException):",
            "    status = 400",
            "",
            "    def __init__(self, message: str = \"Request payload has incorrect format\"):",
            "        error = SupersetError(",
            "            message=message,",
            "            error_type=SupersetErrorType.INVALID_PAYLOAD_FORMAT_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class InvalidPayloadSchemaError(SupersetErrorException):",
            "    status = 422",
            "",
            "    def __init__(self, error: ValidationError):",
            "        # dataclasses.asdict does not work with defaultdict, convert to dict",
            "        # https://bugs.python.org/issue35540",
            "        for k, v in error.messages.items():",
            "            if isinstance(v, defaultdict):",
            "                error.messages[k] = dict(v)",
            "        error = SupersetError(",
            "            message=\"An error happened when validating the request\",",
            "            error_type=SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"messages\": error.messages},",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class SupersetCancelQueryException(SupersetException):",
            "    status = 422",
            "",
            "",
            "class QueryNotFoundException(SupersetException):",
            "    status = 404",
            "",
            "",
            "class ColumnNotFoundException(SupersetException):",
            "    status = 404",
            "",
            "",
            "class SupersetMarshmallowValidationError(SupersetErrorException):",
            "    \"\"\"",
            "    Exception to be raised for Marshmallow validation errors.",
            "    \"\"\"",
            "",
            "    status = 422",
            "",
            "    def __init__(self, exc: ValidationError, payload: dict[str, Any]):",
            "        error = SupersetError(",
            "            message=_(\"The schema of the submitted payload is invalid.\"),",
            "            error_type=SupersetErrorType.MARSHMALLOW_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"messages\": exc.messages, \"payload\": payload},",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class SupersetParseError(SupersetErrorException):",
            "    \"\"\"",
            "    Exception to be raised when we fail to parse SQL.",
            "    \"\"\"",
            "",
            "    status = 422",
            "",
            "    def __init__(self, sql: str, engine: Optional[str] = None):",
            "        error = SupersetError(",
            "            message=_(\"The SQL is invalid and cannot be parsed.\"),",
            "            error_type=SupersetErrorType.INVALID_SQL_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"sql\": sql, \"engine\": engine},",
            "        )",
            "        super().__init__(error)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "mobsf.DynamicAnalyzer.views.ios.corellium_instance"
        ]
    },
    "superset/models/helpers.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "     ColumnNotFoundException,"
            },
            "1": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "     QueryClauseValidationException,"
            },
            "2": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "     QueryObjectValidationError,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+    SupersetParseError,"
            },
            "4": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "     SupersetSecurityException,"
            },
            "5": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 69,
                "PatchRowcode": " )"
            },
            "6": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " from superset.extensions import feature_flag_manager"
            },
            "7": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "     insert_rls_in_predicate,"
            },
            "8": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "     ParsedQuery,"
            },
            "9": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "     sanitize_clause,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+    SQLScript,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+    SQLStatement,"
            },
            "12": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 79,
                "PatchRowcode": " )"
            },
            "13": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " from superset.superset_typing import ("
            },
            "14": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "     AdhocMetric,"
            },
            "15": {
                "beforePatchRowNumber": 901,
                "afterPatchRowNumber": 904,
                "PatchRowcode": "         return sql"
            },
            "16": {
                "beforePatchRowNumber": 902,
                "afterPatchRowNumber": 905,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 903,
                "afterPatchRowNumber": 906,
                "PatchRowcode": "     def get_query_str_extended("
            },
            "18": {
                "beforePatchRowNumber": 904,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self, query_obj: QueryObjectDict, mutate: bool = True"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 907,
                "PatchRowcode": "+        self,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 908,
                "PatchRowcode": "+        query_obj: QueryObjectDict,"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 909,
                "PatchRowcode": "+        mutate: bool = True,"
            },
            "22": {
                "beforePatchRowNumber": 905,
                "afterPatchRowNumber": 910,
                "PatchRowcode": "     ) -> QueryStringExtended:"
            },
            "23": {
                "beforePatchRowNumber": 906,
                "afterPatchRowNumber": 911,
                "PatchRowcode": "         sqlaq = self.get_sqla_query(**query_obj)"
            },
            "24": {
                "beforePatchRowNumber": 907,
                "afterPatchRowNumber": 912,
                "PatchRowcode": "         sql = self.database.compile_sqla_query(sqlaq.sqla_query)"
            },
            "25": {
                "beforePatchRowNumber": 908,
                "afterPatchRowNumber": 913,
                "PatchRowcode": "         sql = self._apply_cte(sql, sqlaq.cte)"
            },
            "26": {
                "beforePatchRowNumber": 909,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sql = sqlparse.format(sql, reindent=True)"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 914,
                "PatchRowcode": "+        try:"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 915,
                "PatchRowcode": "+            sql = SQLStatement(sql, engine=self.db_engine_spec.engine).format()"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 916,
                "PatchRowcode": "+        except SupersetParseError:"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 917,
                "PatchRowcode": "+            logger.warning(\"Unable to parse SQL to format it, passing it as-is\")"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 918,
                "PatchRowcode": "+"
            },
            "32": {
                "beforePatchRowNumber": 910,
                "afterPatchRowNumber": 919,
                "PatchRowcode": "         if mutate:"
            },
            "33": {
                "beforePatchRowNumber": 911,
                "afterPatchRowNumber": 920,
                "PatchRowcode": "             sql = self.mutate_query_from_config(sql)"
            },
            "34": {
                "beforePatchRowNumber": 912,
                "afterPatchRowNumber": 921,
                "PatchRowcode": "         return QueryStringExtended("
            },
            "35": {
                "beforePatchRowNumber": 1054,
                "afterPatchRowNumber": 1063,
                "PatchRowcode": "         )"
            },
            "36": {
                "beforePatchRowNumber": 1055,
                "afterPatchRowNumber": 1064,
                "PatchRowcode": " "
            },
            "37": {
                "beforePatchRowNumber": 1056,
                "afterPatchRowNumber": 1065,
                "PatchRowcode": "     def get_rendered_sql("
            },
            "38": {
                "beforePatchRowNumber": 1057,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self, template_processor: Optional[BaseTemplateProcessor] = None"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1066,
                "PatchRowcode": "+        self,"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1067,
                "PatchRowcode": "+        template_processor: Optional[BaseTemplateProcessor] = None,"
            },
            "41": {
                "beforePatchRowNumber": 1058,
                "afterPatchRowNumber": 1068,
                "PatchRowcode": "     ) -> str:"
            },
            "42": {
                "beforePatchRowNumber": 1059,
                "afterPatchRowNumber": 1069,
                "PatchRowcode": "         \"\"\""
            },
            "43": {
                "beforePatchRowNumber": 1060,
                "afterPatchRowNumber": 1070,
                "PatchRowcode": "         Render sql with template engine (Jinja)."
            },
            "44": {
                "beforePatchRowNumber": 1071,
                "afterPatchRowNumber": 1081,
                "PatchRowcode": "                         msg=ex.message,"
            },
            "45": {
                "beforePatchRowNumber": 1072,
                "afterPatchRowNumber": 1082,
                "PatchRowcode": "                     )"
            },
            "46": {
                "beforePatchRowNumber": 1073,
                "afterPatchRowNumber": 1083,
                "PatchRowcode": "                 ) from ex"
            },
            "47": {
                "beforePatchRowNumber": 1074,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)"
            },
            "48": {
                "beforePatchRowNumber": 1075,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not sql:"
            },
            "49": {
                "beforePatchRowNumber": 1076,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))"
            },
            "50": {
                "beforePatchRowNumber": 1077,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if len(sqlparse.split(sql)) > 1:"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1084,
                "PatchRowcode": "+"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1085,
                "PatchRowcode": "+        script = SQLScript(sql.strip(\"\\t\\r\\n; \"), engine=self.db_engine_spec.engine)"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1086,
                "PatchRowcode": "+        if len(script.statements) > 1:"
            },
            "54": {
                "beforePatchRowNumber": 1078,
                "afterPatchRowNumber": 1087,
                "PatchRowcode": "             raise QueryObjectValidationError("
            },
            "55": {
                "beforePatchRowNumber": 1079,
                "afterPatchRowNumber": 1088,
                "PatchRowcode": "                 _(\"Virtual dataset query cannot consist of multiple statements\")"
            },
            "56": {
                "beforePatchRowNumber": 1080,
                "afterPatchRowNumber": 1089,
                "PatchRowcode": "             )"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1090,
                "PatchRowcode": "+"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1091,
                "PatchRowcode": "+        sql = script.statements[0].format(comments=False)"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1092,
                "PatchRowcode": "+        if not sql:"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1093,
                "PatchRowcode": "+            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))"
            },
            "61": {
                "beforePatchRowNumber": 1081,
                "afterPatchRowNumber": 1094,
                "PatchRowcode": "         return sql"
            },
            "62": {
                "beforePatchRowNumber": 1082,
                "afterPatchRowNumber": 1095,
                "PatchRowcode": " "
            },
            "63": {
                "beforePatchRowNumber": 1083,
                "afterPatchRowNumber": 1096,
                "PatchRowcode": "     def text(self, clause: str) -> TextClause:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "\"\"\"a collection of model-related helper classes and functions\"\"\"",
            "import builtins",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "import uuid",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from datetime import datetime, timedelta",
            "from json.decoder import JSONDecodeError",
            "from typing import Any, cast, NamedTuple, Optional, TYPE_CHECKING, Union",
            "",
            "import dateutil.parser",
            "import humanize",
            "import numpy as np",
            "import pandas as pd",
            "import pytz",
            "import sqlalchemy as sa",
            "import sqlparse",
            "import yaml",
            "from flask import escape, g, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.models.decorators import renders",
            "from flask_appbuilder.models.mixins import AuditMixin",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import and_, Column, or_, UniqueConstraint",
            "from sqlalchemy.exc import MultipleResultsFound",
            "from sqlalchemy.ext.declarative import declared_attr",
            "from sqlalchemy.orm import Mapper, validates",
            "from sqlalchemy.sql.elements import ColumnElement, literal_column, TextClause",
            "from sqlalchemy.sql.expression import Label, Select, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "from sqlalchemy_utils import UUIDType",
            "",
            "from superset import app, db, is_feature_enabled, security_manager",
            "from superset.advanced_data_type.types import AdvancedDataTypeResponse",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.common.utils.time_range_utils import get_since_until_from_time_range",
            "from superset.constants import EMPTY_STRING, NULL_STRING",
            "from superset.db_engine_specs.base import TimestampExpression",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    AdvancedDataTypeResponseError,",
            "    ColumnNotFoundException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetSecurityException,",
            ")",
            "from superset.extensions import feature_flag_manager",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.sql_parse import (",
            "    has_table_query,",
            "    insert_rls_in_predicate,",
            "    ParsedQuery,",
            "    sanitize_clause,",
            ")",
            "from superset.superset_typing import (",
            "    AdhocMetric,",
            "    Column as ColumnTyping,",
            "    FilterValue,",
            "    FilterValues,",
            "    Metric,",
            "    OrderBy,",
            "    QueryObjectDict,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.core import (",
            "    GenericDataType,",
            "    get_column_name,",
            "    get_user_id,",
            "    is_adhoc_column,",
            "    MediumText,",
            "    remove_duplicates,",
            ")",
            "from superset.utils.dates import datetime_to_epoch",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import SqlMetric, TableColumn",
            "    from superset.db_engine_specs import BaseEngineSpec",
            "    from superset.models.core import Database",
            "",
            "",
            "config = app.config",
            "logger = logging.getLogger(__name__)",
            "",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "SERIES_LIMIT_SUBQ_ALIAS = \"series_limit\"",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "",
            "",
            "def validate_adhoc_subquery(",
            "    sql: str,",
            "    database_id: int,",
            "    default_schema: str,",
            ") -> str:",
            "    \"\"\"",
            "    Check if adhoc SQL contains sub-queries or nested sub-queries with table.",
            "",
            "    If sub-queries are allowed, the adhoc SQL is modified to insert any applicable RLS",
            "    predicates to it.",
            "",
            "    :param sql: adhoc sql expression",
            "    :raise SupersetSecurityException if sql contains sub-queries or",
            "    nested sub-queries with table",
            "    \"\"\"",
            "    statements = []",
            "    for statement in sqlparse.parse(sql):",
            "        if has_table_query(statement):",
            "            if not is_feature_enabled(\"ALLOW_ADHOC_SUBQUERY\"):",
            "                raise SupersetSecurityException(",
            "                    SupersetError(",
            "                        error_type=SupersetErrorType.ADHOC_SUBQUERY_NOT_ALLOWED_ERROR,",
            "                        message=_(\"Custom SQL fields cannot contain sub-queries.\"),",
            "                        level=ErrorLevel.ERROR,",
            "                    )",
            "                )",
            "            statement = insert_rls_in_predicate(statement, database_id, default_schema)",
            "        statements.append(statement)",
            "",
            "    return \";\\n\".join(str(statement) for statement in statements)",
            "",
            "",
            "def json_to_dict(json_str: str) -> dict[Any, Any]:",
            "    if json_str:",
            "        val = re.sub(\",[ \\t\\r\\n]+}\", \"}\", json_str)",
            "        val = re.sub(\",[ \\t\\r\\n]+\\\\]\", \"]\", val)",
            "        return json.loads(val)",
            "",
            "    return {}",
            "",
            "",
            "def convert_uuids(obj: Any) -> Any:",
            "    \"\"\"",
            "    Convert UUID objects to str so we can use yaml.safe_dump",
            "    \"\"\"",
            "    if isinstance(obj, uuid.UUID):",
            "        return str(obj)",
            "",
            "    if isinstance(obj, list):",
            "        return [convert_uuids(el) for el in obj]",
            "",
            "    if isinstance(obj, dict):",
            "        return {k: convert_uuids(v) for k, v in obj.items()}",
            "",
            "    return obj",
            "",
            "",
            "class ImportExportMixin:",
            "    uuid = sa.Column(",
            "        UUIDType(binary=True), primary_key=False, unique=True, default=uuid.uuid4",
            "    )",
            "",
            "    export_parent: Optional[str] = None",
            "    # The name of the attribute",
            "    # with the SQL Alchemy back reference",
            "",
            "    export_children: list[str] = []",
            "    # List of (str) names of attributes",
            "    # with the SQL Alchemy forward references",
            "",
            "    export_fields: list[str] = []",
            "    # The names of the attributes",
            "    # that are available for import and export",
            "",
            "    extra_import_fields: list[str] = []",
            "    # Additional fields that should be imported,",
            "    # even though they were not exported",
            "",
            "    __mapper__: Mapper",
            "",
            "    @classmethod",
            "    def _unique_constraints(cls) -> list[set[str]]:",
            "        \"\"\"Get all (single column and multi column) unique constraints\"\"\"",
            "        unique = [",
            "            {c.name for c in u.columns}",
            "            for u in cls.__table_args__  # type: ignore",
            "            if isinstance(u, UniqueConstraint)",
            "        ]",
            "        unique.extend(",
            "            {c.name} for c in cls.__table__.columns if c.unique  # type: ignore",
            "        )",
            "        return unique",
            "",
            "    @classmethod",
            "    def parent_foreign_key_mappings(cls) -> dict[str, str]:",
            "        \"\"\"Get a mapping of foreign name to the local name of foreign keys\"\"\"",
            "        parent_rel = cls.__mapper__.relationships.get(cls.export_parent)",
            "        if parent_rel:",
            "            return {l.name: r.name for (l, r) in parent_rel.local_remote_pairs}",
            "        return {}",
            "",
            "    @classmethod",
            "    def export_schema(",
            "        cls, recursive: bool = True, include_parent_ref: bool = False",
            "    ) -> dict[str, Any]:",
            "        \"\"\"Export schema as a dictionary\"\"\"",
            "        parent_excludes = set()",
            "        if not include_parent_ref:",
            "            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)",
            "            if parent_ref:",
            "                parent_excludes = {column.name for column in parent_ref.local_columns}",
            "",
            "        def formatter(column: sa.Column) -> str:",
            "            return (",
            "                f\"{str(column.type)} Default ({column.default.arg})\"",
            "                if column.default",
            "                else str(column.type)",
            "            )",
            "",
            "        schema: dict[str, Any] = {",
            "            column.name: formatter(column)",
            "            for column in cls.__table__.columns  # type: ignore",
            "            if (column.name in cls.export_fields and column.name not in parent_excludes)",
            "        }",
            "        if recursive:",
            "            for column in cls.export_children:",
            "                child_class = cls.__mapper__.relationships[column].argument.class_",
            "                schema[column] = [",
            "                    child_class.export_schema(",
            "                        recursive=recursive, include_parent_ref=include_parent_ref",
            "                    )",
            "                ]",
            "        return schema",
            "",
            "    @classmethod",
            "    def import_from_dict(",
            "        # pylint: disable=too-many-arguments,too-many-branches,too-many-locals",
            "        cls,",
            "        dict_rep: dict[Any, Any],",
            "        parent: Optional[Any] = None,",
            "        recursive: bool = True,",
            "        sync: Optional[list[str]] = None,",
            "        allow_reparenting: bool = False,",
            "    ) -> Any:",
            "        \"\"\"Import obj from a dictionary\"\"\"",
            "        if sync is None:",
            "            sync = []",
            "        parent_refs = cls.parent_foreign_key_mappings()",
            "        export_fields = (",
            "            set(cls.export_fields)",
            "            | set(cls.extra_import_fields)",
            "            | set(parent_refs.keys())",
            "            | {\"uuid\"}",
            "        )",
            "        new_children = {c: dict_rep[c] for c in cls.export_children if c in dict_rep}",
            "        unique_constraints = cls._unique_constraints()",
            "",
            "        filters = []  # Using these filters to check if obj already exists",
            "",
            "        # Remove fields that should not get imported",
            "        for k in list(dict_rep):",
            "            if k not in export_fields and k not in parent_refs:",
            "                del dict_rep[k]",
            "",
            "        if not parent:",
            "            if cls.export_parent:",
            "                for prnt in parent_refs.keys():",
            "                    if prnt not in dict_rep:",
            "                        raise RuntimeError(f\"{cls.__name__}: Missing field {prnt}\")",
            "        else:",
            "            # Set foreign keys to parent obj",
            "            for k, v in parent_refs.items():",
            "                dict_rep[k] = getattr(parent, v)",
            "",
            "        if not allow_reparenting:",
            "            # Add filter for parent obj",
            "            filters.extend(",
            "                [getattr(cls, k) == dict_rep.get(k) for k in parent_refs.keys()]",
            "            )",
            "",
            "        # Add filter for unique constraints",
            "        ucs = [",
            "            and_(",
            "                *[",
            "                    getattr(cls, k) == dict_rep.get(k)",
            "                    for k in cs",
            "                    if dict_rep.get(k) is not None",
            "                ]",
            "            )",
            "            for cs in unique_constraints",
            "        ]",
            "        filters.append(or_(*ucs))",
            "",
            "        # Check if object already exists in DB, break if more than one is found",
            "        try:",
            "            obj_query = db.session.query(cls).filter(and_(*filters))",
            "            obj = obj_query.one_or_none()",
            "        except MultipleResultsFound as ex:",
            "            logger.error(",
            "                \"Error importing %s \\n %s \\n %s\",",
            "                cls.__name__,",
            "                str(obj_query),",
            "                yaml.safe_dump(dict_rep),",
            "                exc_info=True,",
            "            )",
            "            raise ex",
            "",
            "        if not obj:",
            "            is_new_obj = True",
            "            # Create new DB object",
            "            obj = cls(**dict_rep)",
            "            logger.info(\"Importing new %s %s\", obj.__tablename__, str(obj))",
            "            if cls.export_parent and parent:",
            "                setattr(obj, cls.export_parent, parent)",
            "            db.session.add(obj)",
            "        else:",
            "            is_new_obj = False",
            "            logger.info(\"Updating %s %s\", obj.__tablename__, str(obj))",
            "            # Update columns",
            "            for k, v in dict_rep.items():",
            "                setattr(obj, k, v)",
            "",
            "        # Recursively create children",
            "        if recursive:",
            "            for child in cls.export_children:",
            "                argument = cls.__mapper__.relationships[child].argument",
            "                child_class = (",
            "                    argument.class_ if hasattr(argument, \"class_\") else argument",
            "                )",
            "                added = []",
            "                for c_obj in new_children.get(child, []):",
            "                    added.append(",
            "                        child_class.import_from_dict(",
            "                            dict_rep=c_obj, parent=obj, sync=sync",
            "                        )",
            "                    )",
            "                # If children should get synced, delete the ones that did not",
            "                # get updated.",
            "                if child in sync and not is_new_obj:",
            "                    back_refs = child_class.parent_foreign_key_mappings()",
            "                    delete_filters = [",
            "                        getattr(child_class, k) == getattr(obj, back_refs.get(k))",
            "                        for k in back_refs.keys()",
            "                    ]",
            "                    to_delete = set(",
            "                        db.session.query(child_class).filter(and_(*delete_filters))",
            "                    ).difference(set(added))",
            "                    for o in to_delete:",
            "                        logger.info(\"Deleting %s %s\", child, str(obj))",
            "                        db.session.delete(o)",
            "",
            "        return obj",
            "",
            "    def export_to_dict(",
            "        self,",
            "        recursive: bool = True,",
            "        include_parent_ref: bool = False,",
            "        include_defaults: bool = False,",
            "        export_uuids: bool = False,",
            "    ) -> dict[Any, Any]:",
            "        \"\"\"Export obj to dictionary\"\"\"",
            "        export_fields = set(self.export_fields)",
            "        if export_uuids:",
            "            export_fields.add(\"uuid\")",
            "            if \"id\" in export_fields:",
            "                export_fields.remove(\"id\")",
            "",
            "        cls = self.__class__",
            "        parent_excludes = set()",
            "        if recursive and not include_parent_ref:",
            "            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)",
            "            if parent_ref:",
            "                parent_excludes = {c.name for c in parent_ref.local_columns}",
            "        dict_rep = {",
            "            c.name: getattr(self, c.name)",
            "            for c in cls.__table__.columns  # type: ignore",
            "            if (",
            "                c.name in export_fields",
            "                and c.name not in parent_excludes",
            "                and (",
            "                    include_defaults",
            "                    or (",
            "                        getattr(self, c.name) is not None",
            "                        and (not c.default or getattr(self, c.name) != c.default.arg)",
            "                    )",
            "                )",
            "            )",
            "        }",
            "",
            "        # sort according to export_fields using DSU (decorate, sort, undecorate)",
            "        order = {field: i for i, field in enumerate(self.export_fields)}",
            "        decorated_keys = [(order.get(k, len(order)), k) for k in dict_rep]",
            "        decorated_keys.sort()",
            "        dict_rep = {k: dict_rep[k] for _, k in decorated_keys}",
            "",
            "        if recursive:",
            "            for cld in self.export_children:",
            "                # sorting to make lists of children stable",
            "                dict_rep[cld] = sorted(",
            "                    [",
            "                        child.export_to_dict(",
            "                            recursive=recursive,",
            "                            include_parent_ref=include_parent_ref,",
            "                            include_defaults=include_defaults,",
            "                        )",
            "                        for child in getattr(self, cld)",
            "                    ],",
            "                    key=lambda k: sorted(str(k.items())),",
            "                )",
            "",
            "        return convert_uuids(dict_rep)",
            "",
            "    def override(self, obj: Any) -> None:",
            "        \"\"\"Overrides the plain fields of the dashboard.\"\"\"",
            "        for field in obj.__class__.export_fields:",
            "            setattr(self, field, getattr(obj, field))",
            "",
            "    def copy(self) -> Any:",
            "        \"\"\"Creates a copy of the dashboard without relationships.\"\"\"",
            "        new_obj = self.__class__()",
            "        new_obj.override(self)",
            "        return new_obj",
            "",
            "    def alter_params(self, **kwargs: Any) -> None:",
            "        params = self.params_dict",
            "        params.update(kwargs)",
            "        self.params = json.dumps(params)",
            "",
            "    def remove_params(self, param_to_remove: str) -> None:",
            "        params = self.params_dict",
            "        params.pop(param_to_remove, None)",
            "        self.params = json.dumps(params)",
            "",
            "    def reset_ownership(self) -> None:",
            "        \"\"\"object will belong to the user the current user\"\"\"",
            "        # make sure the object doesn't have relations to a user",
            "        # it will be filled by appbuilder on save",
            "        self.created_by = None",
            "        self.changed_by = None",
            "        # flask global context might not exist (in cli or tests for example)",
            "        self.owners = []",
            "        if g and hasattr(g, \"user\"):",
            "            self.owners = [g.user]",
            "",
            "    @property",
            "    def params_dict(self) -> dict[Any, Any]:",
            "        return json_to_dict(self.params)",
            "",
            "    @property",
            "    def template_params_dict(self) -> dict[Any, Any]:",
            "        return json_to_dict(self.template_params)  # type: ignore",
            "",
            "",
            "def _user(user: User) -> str:",
            "    if not user:",
            "        return \"\"",
            "    return escape(user)",
            "",
            "",
            "class AuditMixinNullable(AuditMixin):",
            "    \"\"\"Altering the AuditMixin to use nullable fields",
            "",
            "    Allows creating objects programmatically outside of CRUD",
            "    \"\"\"",
            "",
            "    created_on = sa.Column(sa.DateTime, default=datetime.now, nullable=True)",
            "    changed_on = sa.Column(",
            "        sa.DateTime, default=datetime.now, onupdate=datetime.now, nullable=True",
            "    )",
            "",
            "    @declared_attr",
            "    def created_by_fk(self) -> sa.Column:  # pylint: disable=arguments-renamed",
            "        return sa.Column(",
            "            sa.Integer,",
            "            sa.ForeignKey(\"ab_user.id\"),",
            "            default=get_user_id,",
            "            nullable=True,",
            "        )",
            "",
            "    @declared_attr",
            "    def changed_by_fk(self) -> sa.Column:  # pylint: disable=arguments-renamed",
            "        return sa.Column(",
            "            sa.Integer,",
            "            sa.ForeignKey(\"ab_user.id\"),",
            "            default=get_user_id,",
            "            onupdate=get_user_id,",
            "            nullable=True,",
            "        )",
            "",
            "    @property",
            "    def created_by_name(self) -> str:",
            "        if self.created_by:",
            "            return escape(f\"{self.created_by}\")",
            "        return \"\"",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if self.changed_by:",
            "            return escape(f\"{self.changed_by}\")",
            "        return \"\"",
            "",
            "    @renders(\"created_by\")",
            "    def creator(self) -> Union[Markup, str]:",
            "        return _user(self.created_by)",
            "",
            "    @property",
            "    def changed_by_(self) -> Union[Markup, str]:",
            "        return _user(self.changed_by)",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_(self) -> Markup:",
            "        return Markup(f'<span class=\"no-wrap\">{self.changed_on}</span>')",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_delta_humanized(self) -> str:",
            "        return self.changed_on_humanized",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_dttm(self) -> float:",
            "        return datetime_to_epoch(self.changed_on)",
            "",
            "    @renders(\"created_on\")",
            "    def created_on_delta_humanized(self) -> str:",
            "        return self.created_on_humanized",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_utc(self) -> str:",
            "        # Convert naive datetime to UTC",
            "        return self.changed_on.astimezone(pytz.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")",
            "",
            "    @property",
            "    def changed_on_humanized(self) -> str:",
            "        return humanize.naturaltime(datetime.now() - self.changed_on)",
            "",
            "    @property",
            "    def created_on_humanized(self) -> str:",
            "        return humanize.naturaltime(datetime.now() - self.created_on)",
            "",
            "    @renders(\"changed_on\")",
            "    def modified(self) -> Markup:",
            "        return Markup(f'<span class=\"no-wrap\">{self.changed_on_humanized}</span>')",
            "",
            "",
            "class QueryResult:  # pylint: disable=too-few-public-methods",
            "",
            "    \"\"\"Object returned by the query interface\"\"\"",
            "",
            "    def __init__(  # pylint: disable=too-many-arguments",
            "        self,",
            "        df: pd.DataFrame,",
            "        query: str,",
            "        duration: timedelta,",
            "        applied_template_filters: Optional[list[str]] = None,",
            "        applied_filter_columns: Optional[list[ColumnTyping]] = None,",
            "        rejected_filter_columns: Optional[list[ColumnTyping]] = None,",
            "        status: str = QueryStatus.SUCCESS,",
            "        error_message: Optional[str] = None,",
            "        errors: Optional[list[dict[str, Any]]] = None,",
            "        from_dttm: Optional[datetime] = None,",
            "        to_dttm: Optional[datetime] = None,",
            "    ) -> None:",
            "        self.df = df",
            "        self.query = query",
            "        self.duration = duration",
            "        self.applied_template_filters = applied_template_filters or []",
            "        self.applied_filter_columns = applied_filter_columns or []",
            "        self.rejected_filter_columns = rejected_filter_columns or []",
            "        self.status = status",
            "        self.error_message = error_message",
            "        self.errors = errors or []",
            "        self.from_dttm = from_dttm",
            "        self.to_dttm = to_dttm",
            "",
            "",
            "class ExtraJSONMixin:",
            "    \"\"\"Mixin to add an `extra` column (JSON) and utility methods\"\"\"",
            "",
            "    extra_json = sa.Column(MediumText(), default=\"{}\")",
            "",
            "    @property",
            "    def extra(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra_json or \"{}\") or {}",
            "        except (TypeError, JSONDecodeError) as exc:",
            "            logger.error(",
            "                \"Unable to load an extra json: %r. Leaving empty.\", exc, exc_info=True",
            "            )",
            "            return {}",
            "",
            "    @extra.setter",
            "    def extra(self, extras: dict[str, Any]) -> None:",
            "        self.extra_json = json.dumps(extras)",
            "",
            "    def set_extra_json_key(self, key: str, value: Any) -> None:",
            "        extra = self.extra",
            "        extra[key] = value",
            "        self.extra_json = json.dumps(extra)",
            "",
            "    @validates(\"extra_json\")",
            "    def ensure_extra_json_is_not_none(",
            "        self,",
            "        _: str,",
            "        value: Optional[dict[str, Any]],",
            "    ) -> Any:",
            "        if value is None:",
            "            return \"{}\"",
            "        return value",
            "",
            "",
            "class CertificationMixin:",
            "    \"\"\"Mixin to add extra certification fields\"\"\"",
            "",
            "    extra = sa.Column(sa.Text, default=\"{}\")",
            "",
            "    def get_extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    @property",
            "    def is_certified(self) -> bool:",
            "        return bool(self.get_extra_dict().get(\"certification\"))",
            "",
            "    @property",
            "    def certified_by(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"certification\", {}).get(\"certified_by\")",
            "",
            "    @property",
            "    def certification_details(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"certification\", {}).get(\"details\")",
            "",
            "    @property",
            "    def warning_markdown(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"warning_markdown\")",
            "",
            "",
            "def clone_model(",
            "    target: Model,",
            "    ignore: Optional[list[str]] = None,",
            "    keep_relations: Optional[list[str]] = None,",
            "    **kwargs: Any,",
            ") -> Model:",
            "    \"\"\"",
            "    Clone a SQLAlchemy model. By default will only clone naive column attributes.",
            "    To include relationship attributes, use `keep_relations`.",
            "    \"\"\"",
            "    ignore = ignore or []",
            "",
            "    table = target.__table__",
            "    primary_keys = table.primary_key.columns.keys()",
            "    data = {",
            "        attr: getattr(target, attr)",
            "        for attr in list(table.columns.keys()) + (keep_relations or [])",
            "        if attr not in primary_keys and attr not in ignore",
            "    }",
            "    data.update(kwargs)",
            "",
            "    return target.__class__(**data)",
            "",
            "",
            "# todo(hugh): centralize where this code lives",
            "class QueryStringExtended(NamedTuple):",
            "    applied_template_filters: Optional[list[str]]",
            "    applied_filter_columns: list[ColumnTyping]",
            "    rejected_filter_columns: list[ColumnTyping]",
            "    labels_expected: list[str]",
            "    prequeries: list[str]",
            "    sql: str",
            "",
            "",
            "class SqlaQuery(NamedTuple):",
            "    applied_template_filters: list[str]",
            "    applied_filter_columns: list[ColumnTyping]",
            "    rejected_filter_columns: list[ColumnTyping]",
            "    cte: Optional[str]",
            "    extra_cache_keys: list[Any]",
            "    labels_expected: list[str]",
            "    prequeries: list[str]",
            "    sqla_query: Select",
            "",
            "",
            "class ExploreMixin:  # pylint: disable=too-many-public-methods",
            "    \"\"\"",
            "    Allows any flask_appbuilder.Model (Query, Table, etc.)",
            "    to be used to power a chart inside /explore",
            "    \"\"\"",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "    fetch_values_predicate = None",
            "",
            "    @property",
            "    def type(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def db_extra(self) -> Optional[dict[str, Any]]:",
            "        raise NotImplementedError()",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def database_id(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def owners_data(self) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def metrics(self) -> list[Any]:",
            "        return []",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def is_rls_supported(self) -> bool:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def cache_timeout(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def column_names(self) -> list[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def offset(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def main_dttm_col(self) -> Optional[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def always_filter_main_dttm(self) -> Optional[bool]:",
            "        return False",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def db_engine_spec(self) -> builtins.type[\"BaseEngineSpec\"]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def database(self) -> \"Database\":",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def schema(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def sql(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def columns(self) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "    def get_extra_cache_keys(self, query_obj: dict[str, Any]) -> list[Hashable]:",
            "        raise NotImplementedError()",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        raise NotImplementedError()",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: Optional[  # pylint: disable=unused-argument",
            "            BaseTemplateProcessor",
            "        ] = None,",
            "    ) -> TextClause:",
            "        return self.fetch_values_predicate",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[Union[int, str], list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def _process_sql_expression(",
            "        self,",
            "        expression: Optional[str],",
            "        database_id: int,",
            "        schema: str,",
            "        template_processor: Optional[BaseTemplateProcessor],",
            "    ) -> Optional[str]:",
            "        if template_processor and expression:",
            "            expression = template_processor.process_template(expression)",
            "        if expression:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            try:",
            "                expression = sanitize_clause(expression)",
            "            except QueryClauseValidationException as ex:",
            "                raise QueryObjectValidationError(ex.message) from ex",
            "        return expression",
            "",
            "    def make_sqla_column_compatible(",
            "        self, sqla_col: ColumnElement, label: Optional[str] = None",
            "    ) -> ColumnElement:",
            "        \"\"\"Takes a sqlalchemy column object and adds label info if supported by engine.",
            "        :param sqla_col: sqlalchemy column instance",
            "        :param label: alias/label that column is expected to have",
            "        :return: either a sql alchemy column or label instance if supported by engine",
            "        \"\"\"",
            "        label_expected = label or sqla_col.name",
            "        db_engine_spec = self.db_engine_spec",
            "        # add quotes to tables",
            "        if db_engine_spec.get_allows_alias_in_select(self.database):",
            "            label = db_engine_spec.make_label_compatible(label_expected)",
            "            sqla_col = sqla_col.label(label)",
            "        sqla_col.key = label_expected",
            "        return sqla_col",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        if sql_query_mutator := config[\"SQL_QUERY_MUTATOR\"]:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: Optional[str]) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    def get_query_str_extended(",
            "        self, query_obj: QueryObjectDict, mutate: bool = True",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        sql = sqlparse.format(sql, reindent=True)",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "    ) -> Union[str, int, float, bool, str]:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if isinstance(column_, dict):",
            "            if (",
            "                column_.get(\"type\")",
            "                and column_.get(\"is_temporal\")",
            "                and isinstance(value, str)",
            "            ):",
            "                sql = self.db_engine_spec.convert_dttm(",
            "                    column_.get(\"type\"), dateutil.parser.parse(value), db_extra=None",
            "                )",
            "",
            "                if sql:",
            "                    value = self.db_engine_spec.get_text_clause(sql)",
            "        else:",
            "            if column_.type and column_.is_temporal and isinstance(value, str):",
            "                sql = self.db_engine_spec.convert_dttm(",
            "                    column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "                )",
            "",
            "                if sql:",
            "                    value = self.text(sql)",
            "        return value",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def exc_query(self, qry: Any) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(qry)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> Optional[pd.DataFrame]:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_rendered_sql(",
            "        self, template_processor: Optional[BaseTemplateProcessor] = None",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        if len(sqlparse.split(sql)) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "        return sql",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def get_from_clause(",
            "        self, template_processor: Optional[BaseTemplateProcessor] = None",
            "    ) -> tuple[Union[TableClause, Alias], Optional[str]]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql, engine=self.db_engine_spec.engine)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            sa.table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, \"TableColumn\"],  # pylint: disable=unused-argument",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            sqla_column = sa.column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = self._process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    @property",
            "    def template_params_dict(self) -> dict[Any, Any]:",
            "        return {}",
            "",
            "    @staticmethod",
            "    def filter_values_handler(  # pylint: disable=too-many-arguments",
            "        values: Optional[FilterValues],",
            "        operator: str,",
            "        target_generic_type: utils.GenericDataType,",
            "        target_native_type: Optional[str] = None,",
            "        is_list_target: bool = False,",
            "        db_engine_spec: Optional[",
            "            builtins.type[\"BaseEngineSpec\"]",
            "        ] = None,  # fix(hughhh): Optional[Type[BaseEngineSpec]]",
            "        db_extra: Optional[dict[str, Any]] = None,",
            "    ) -> Optional[FilterValues]:",
            "        if values is None:",
            "            return None",
            "",
            "        def handle_single_value(value: Optional[FilterValue]) -> Optional[FilterValue]:",
            "            if operator == utils.FilterOperator.TEMPORAL_RANGE:",
            "                return value",
            "            if (",
            "                isinstance(value, (float, int))",
            "                and target_generic_type == utils.GenericDataType.TEMPORAL",
            "                and target_native_type is not None",
            "                and db_engine_spec is not None",
            "            ):",
            "                value = db_engine_spec.convert_dttm(",
            "                    target_type=target_native_type,",
            "                    dttm=datetime.utcfromtimestamp(value / 1000),",
            "                    db_extra=db_extra,",
            "                )",
            "                value = literal_column(value)",
            "            if isinstance(value, str):",
            "                value = value.strip(\"\\t\\n\")",
            "",
            "                if (",
            "                    target_generic_type == utils.GenericDataType.NUMERIC",
            "                    and operator",
            "                    not in {",
            "                        utils.FilterOperator.ILIKE,",
            "                        utils.FilterOperator.LIKE,",
            "                    }",
            "                ):",
            "                    # For backwards compatibility and edge cases",
            "                    # where a column data type might have changed",
            "                    return utils.cast_to_num(value)",
            "                if value == NULL_STRING:",
            "                    return None",
            "                if value == EMPTY_STRING:",
            "                    return \"\"",
            "            if target_generic_type == utils.GenericDataType.BOOLEAN:",
            "                return utils.cast_to_boolean(value)",
            "            return value",
            "",
            "        if isinstance(values, (list, tuple)):",
            "            values = [handle_single_value(v) for v in values]  # type: ignore",
            "        else:",
            "            values = handle_single_value(values)",
            "        if is_list_target and not isinstance(values, (tuple, list)):",
            "            values = [values]  # type: ignore",
            "        elif not is_list_target and isinstance(values, (tuple, list)):",
            "            values = values[0] if values else None",
            "        return values",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, \"SqlMetric\"],",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def adhoc_column_to_sqla(",
            "        self,",
            "        col: \"AdhocColumn\",  # type: ignore",
            "        force_type_check: bool = False,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        raise NotImplementedError()",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def dttm_sql_literal(self, dttm: datetime, col: \"TableColumn\") -> str:",
            "        \"\"\"Convert datetime object to a SQL expression string\"\"\"",
            "",
            "        sql = (",
            "            self.db_engine_spec.convert_dttm(col.type, dttm, db_extra=self.db_extra)",
            "            if col.type",
            "            else None",
            "        )",
            "",
            "        if sql:",
            "            return sql",
            "",
            "        tf = col.python_date_format",
            "",
            "        # Fallback to the default format (if defined).",
            "        if not tf and self.db_extra:",
            "            tf = self.db_extra.get(\"python_date_format_by_column_name\", {}).get(",
            "                col.column_name",
            "            )",
            "",
            "        if tf:",
            "            if tf in {\"epoch_ms\", \"epoch_s\"}:",
            "                seconds_since_epoch = int(dttm.timestamp())",
            "                if tf == \"epoch_s\":",
            "                    return str(seconds_since_epoch)",
            "                return str(seconds_since_epoch * 1000)",
            "            return f\"'{dttm.strftime(tf)}'\"",
            "",
            "        return f\"\"\"'{dttm.strftime(\"%Y-%m-%d %H:%M:%S.%f\")}'\"\"\"",
            "",
            "    def get_time_filter(  # pylint: disable=too-many-arguments",
            "        self,",
            "        time_col: \"TableColumn\",",
            "        start_dttm: Optional[sa.DateTime],",
            "        end_dttm: Optional[sa.DateTime],",
            "        time_grain: Optional[str] = None,",
            "        label: Optional[str] = \"__time\",",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        col = (",
            "            time_col.get_timestamp_expression(",
            "                time_grain=time_grain,",
            "                label=label,",
            "                template_processor=template_processor,",
            "            )",
            "            if time_grain",
            "            else self.convert_tbl_column_to_sqla_col(",
            "                time_col, label=label, template_processor=template_processor",
            "            )",
            "        )",
            "",
            "        l = []",
            "        if start_dttm:",
            "            l.append(",
            "                col",
            "                >= self.db_engine_spec.get_text_clause(",
            "                    self.dttm_sql_literal(start_dttm, time_col)",
            "                )",
            "            )",
            "        if end_dttm:",
            "            l.append(",
            "                col",
            "                < self.db_engine_spec.get_text_clause(",
            "                    self.dttm_sql_literal(end_dttm, time_col)",
            "                )",
            "            )",
            "        return and_(*l)",
            "",
            "    def values_for_column(",
            "        self,",
            "        column_name: str,",
            "        limit: int = 10000,",
            "        denormalize_column: bool = False,",
            "    ) -> list[Any]:",
            "        # denormalize column name before querying for values",
            "        # unless disabled in the dataset configuration",
            "        db_dialect = self.database.get_dialect()",
            "        column_name_ = (",
            "            self.database.db_engine_spec.denormalize_name(db_dialect, column_name)",
            "            if denormalize_column",
            "            else column_name",
            "        )",
            "        cols = {col.column_name: col for col in self.columns}",
            "        target_col = cols[column_name_]",
            "        tp = self.get_template_processor()",
            "        tbl, cte = self.get_from_clause(tp)",
            "",
            "        qry = (",
            "            sa.select(",
            "                # The alias (label) here is important because some dialects will",
            "                # automatically add a random alias to the projection because of the",
            "                # call to DISTINCT; others will uppercase the column names. This",
            "                # gives us a deterministic column name in the dataframe.",
            "                [target_col.get_sqla_col(template_processor=tp).label(\"column_values\")]",
            "            )",
            "            .select_from(tbl)",
            "            .distinct()",
            "        )",
            "        if limit:",
            "            qry = qry.limit(limit)",
            "",
            "        if self.fetch_values_predicate:",
            "            qry = qry.where(self.get_fetch_values_predicate(template_processor=tp))",
            "",
            "        with self.database.get_sqla_engine_with_context() as engine:",
            "            sql = qry.compile(engine, compile_kwargs={\"literal_binds\": True})",
            "            sql = self._apply_cte(sql, cte)",
            "            sql = self.mutate_query_from_config(sql)",
            "",
            "            df = pd.read_sql_query(sql=sql, con=engine)",
            "            # replace NaN with None to ensure it can be serialized to JSON",
            "            df = df.replace({np.nan: None})",
            "            return df[\"column_values\"].to_list()",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        column: dict[str, Any],",
            "        time_grain: Optional[str],",
            "        label: Optional[str] = None,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Union[TimestampExpression, Label]:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param column: column object",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "        column_spec = self.db_engine_spec.get_column_spec(column.get(\"type\"))",
            "        type_ = column_spec.sqla_type if column_spec else sa.DateTime",
            "        col = sa.column(column.get(\"column_name\"), type_=type_)",
            "",
            "        if template_processor:",
            "            expression = template_processor.process_template(column[\"column_name\"])",
            "            col = sa.literal_column(expression, type_=type_)",
            "",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, None, time_grain)",
            "        return self.make_sqla_column_compatible(time_expr, label)",
            "",
            "    def convert_tbl_column_to_sqla_col(",
            "        self,",
            "        tbl_column: \"TableColumn\",",
            "        label: Optional[str] = None,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Column:",
            "        label = label or tbl_column.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := tbl_column.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = sa.column(tbl_column.column_name, type_=type_)",
            "        col = self.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    def get_sqla_query(  # pylint: disable=too-many-arguments,too-many-locals,too-many-branches,too-many-statements",
            "        self,",
            "        apply_fetch_values_predicate: bool = False,",
            "        columns: Optional[list[Column]] = None,",
            "        extras: Optional[dict[str, Any]] = None,",
            "        filter: Optional[  # pylint: disable=redefined-builtin",
            "            list[utils.QueryObjectFilterClause]",
            "        ] = None,",
            "        from_dttm: Optional[datetime] = None,",
            "        granularity: Optional[str] = None,",
            "        groupby: Optional[list[Column]] = None,",
            "        inner_from_dttm: Optional[datetime] = None,",
            "        inner_to_dttm: Optional[datetime] = None,",
            "        is_rowcount: bool = False,",
            "        is_timeseries: bool = True,",
            "        metrics: Optional[list[Metric]] = None,",
            "        orderby: Optional[list[OrderBy]] = None,",
            "        order_desc: bool = True,",
            "        to_dttm: Optional[datetime] = None,",
            "        series_columns: Optional[list[Column]] = None,",
            "        series_limit: Optional[int] = None,",
            "        series_limit_metric: Optional[Metric] = None,",
            "        row_limit: Optional[int] = None,",
            "        row_offset: Optional[int] = None,",
            "        timeseries_limit: Optional[int] = None,",
            "        timeseries_limit_metric: Optional[Metric] = None,",
            "        time_shift: Optional[str] = None,",
            "    ) -> SqlaQuery:",
            "        \"\"\"Querying any sqla table from this common interface\"\"\"",
            "        if granularity not in self.dttm_cols and granularity is not None:",
            "            granularity = self.main_dttm_col",
            "",
            "        extras = extras or {}",
            "        time_grain = extras.get(\"time_grain_sqla\")",
            "",
            "        template_kwargs = {",
            "            \"columns\": columns,",
            "            \"from_dttm\": from_dttm.isoformat() if from_dttm else None,",
            "            \"groupby\": groupby,",
            "            \"metrics\": metrics,",
            "            \"row_limit\": row_limit,",
            "            \"row_offset\": row_offset,",
            "            \"time_column\": granularity,",
            "            \"time_grain\": time_grain,",
            "            \"to_dttm\": to_dttm.isoformat() if to_dttm else None,",
            "            \"table_columns\": [col.column_name for col in self.columns],",
            "            \"filter\": filter,",
            "        }",
            "        columns = columns or []",
            "        groupby = groupby or []",
            "        rejected_adhoc_filters_columns: list[Union[str, ColumnTyping]] = []",
            "        applied_adhoc_filters_columns: list[Union[str, ColumnTyping]] = []",
            "        db_engine_spec = self.db_engine_spec",
            "        series_column_labels = [",
            "            db_engine_spec.make_label_compatible(column)",
            "            for column in utils.get_column_names(",
            "                columns=series_columns or [],",
            "            )",
            "        ]",
            "        # deprecated, to be removed in 2.0",
            "        if is_timeseries and timeseries_limit:",
            "            series_limit = timeseries_limit",
            "        series_limit_metric = series_limit_metric or timeseries_limit_metric",
            "        template_kwargs.update(self.template_params_dict)",
            "        extra_cache_keys: list[Any] = []",
            "        template_kwargs[\"extra_cache_keys\"] = extra_cache_keys",
            "        removed_filters: list[str] = []",
            "        applied_template_filters: list[str] = []",
            "        template_kwargs[\"removed_filters\"] = removed_filters",
            "        template_kwargs[\"applied_filters\"] = applied_template_filters",
            "        template_processor = self.get_template_processor(**template_kwargs)",
            "        prequeries: list[str] = []",
            "        orderby = orderby or []",
            "        need_groupby = bool(metrics is not None or groupby)",
            "        metrics = metrics or []",
            "",
            "        # For backward compatibility",
            "        if granularity not in self.dttm_cols and granularity is not None:",
            "            granularity = self.main_dttm_col",
            "",
            "        columns_by_name: dict[str, \"TableColumn\"] = {",
            "            col.column_name: col for col in self.columns",
            "        }",
            "",
            "        metrics_by_name: dict[str, \"SqlMetric\"] = {",
            "            m.metric_name: m for m in self.metrics",
            "        }",
            "",
            "        if not granularity and is_timeseries:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Datetime column not provided as part table configuration \"",
            "                    \"and is required by this type of chart\"",
            "                )",
            "            )",
            "        if not metrics and not columns and not groupby:",
            "            raise QueryObjectValidationError(_(\"Empty query?\"))",
            "",
            "        metrics_exprs: list[ColumnElement] = []",
            "        for metric in metrics:",
            "            if utils.is_adhoc_metric(metric):",
            "                assert isinstance(metric, dict)",
            "                metrics_exprs.append(",
            "                    self.adhoc_metric_to_sqla(",
            "                        metric=metric,",
            "                        columns_by_name=columns_by_name,",
            "                        template_processor=template_processor,",
            "                    )",
            "                )",
            "            elif isinstance(metric, str) and metric in metrics_by_name:",
            "                metrics_exprs.append(",
            "                    metrics_by_name[metric].get_sqla_col(",
            "                        template_processor=template_processor",
            "                    )",
            "                )",
            "            else:",
            "                raise QueryObjectValidationError(",
            "                    _(\"Metric '%(metric)s' does not exist\", metric=metric)",
            "                )",
            "",
            "        if metrics_exprs:",
            "            main_metric_expr = metrics_exprs[0]",
            "        else:",
            "            main_metric_expr, label = literal_column(\"COUNT(*)\"), \"ccount\"",
            "            main_metric_expr = self.make_sqla_column_compatible(main_metric_expr, label)",
            "",
            "        # To ensure correct handling of the ORDER BY labeling we need to reference the",
            "        # metric instance if defined in the SELECT clause.",
            "        # use the key of the ColumnClause for the expected label",
            "        metrics_exprs_by_label = {m.key: m for m in metrics_exprs}",
            "        metrics_exprs_by_expr = {str(m): m for m in metrics_exprs}",
            "",
            "        # Since orderby may use adhoc metrics, too; we need to process them first",
            "        orderby_exprs: list[ColumnElement] = []",
            "        for orig_col, ascending in orderby:",
            "            col: Union[AdhocMetric, ColumnElement] = orig_col",
            "            if isinstance(col, dict):",
            "                col = cast(AdhocMetric, col)",
            "                if col.get(\"sqlExpression\"):",
            "                    col[\"sqlExpression\"] = self._process_sql_expression(",
            "                        expression=col[\"sqlExpression\"],",
            "                        database_id=self.database_id,",
            "                        schema=self.schema,",
            "                        template_processor=template_processor,",
            "                    )",
            "                if utils.is_adhoc_metric(col):",
            "                    # add adhoc sort by column to columns_by_name if not exists",
            "                    col = self.adhoc_metric_to_sqla(col, columns_by_name)",
            "                    # if the adhoc metric has been defined before",
            "                    # use the existing instance.",
            "                    col = metrics_exprs_by_expr.get(str(col), col)",
            "                    need_groupby = True",
            "            elif col in columns_by_name:",
            "                col = self.convert_tbl_column_to_sqla_col(",
            "                    columns_by_name[col], template_processor=template_processor",
            "                )",
            "            elif col in metrics_exprs_by_label:",
            "                col = metrics_exprs_by_label[col]",
            "                need_groupby = True",
            "            elif col in metrics_by_name:",
            "                col = metrics_by_name[col].get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "                need_groupby = True",
            "",
            "            if isinstance(col, ColumnElement):",
            "                orderby_exprs.append(col)",
            "            else:",
            "                # Could not convert a column reference to valid ColumnElement",
            "                raise QueryObjectValidationError(",
            "                    _(\"Unknown column used in orderby: %(col)s\", col=orig_col)",
            "                )",
            "",
            "        select_exprs: list[Union[Column, Label]] = []",
            "        groupby_all_columns = {}",
            "        groupby_series_columns = {}",
            "",
            "        # filter out the pseudo column  __timestamp from columns",
            "        columns = [col for col in columns if col != utils.DTTM_ALIAS]",
            "        dttm_col = columns_by_name.get(granularity) if granularity else None",
            "",
            "        if need_groupby:",
            "            # dedup columns while preserving order",
            "            columns = groupby or columns",
            "            for selected in columns:",
            "                if isinstance(selected, str):",
            "                    # if groupby field/expr equals granularity field/expr",
            "                    if selected == granularity:",
            "                        table_col = columns_by_name[selected]",
            "                        outer = table_col.get_timestamp_expression(",
            "                            time_grain=time_grain,",
            "                            label=selected,",
            "                            template_processor=template_processor,",
            "                        )",
            "                    # if groupby field equals a selected column",
            "                    elif selected in columns_by_name:",
            "                        outer = self.convert_tbl_column_to_sqla_col(",
            "                            columns_by_name[selected],",
            "                            template_processor=template_processor,",
            "                        )",
            "                    else:",
            "                        selected = validate_adhoc_subquery(",
            "                            selected,",
            "                            self.database_id,",
            "                            self.schema,",
            "                        )",
            "                        outer = literal_column(f\"({selected})\")",
            "                        outer = self.make_sqla_column_compatible(outer, selected)",
            "                else:",
            "                    outer = self.adhoc_column_to_sqla(",
            "                        col=selected, template_processor=template_processor",
            "                    )",
            "                groupby_all_columns[outer.name] = outer",
            "                if (",
            "                    is_timeseries and not series_column_labels",
            "                ) or outer.name in series_column_labels:",
            "                    groupby_series_columns[outer.name] = outer",
            "                select_exprs.append(outer)",
            "        elif columns:",
            "            for selected in columns:",
            "                if is_adhoc_column(selected):",
            "                    _sql = selected[\"sqlExpression\"]",
            "                    _column_label = selected[\"label\"]",
            "                elif isinstance(selected, str):",
            "                    _sql = selected",
            "                    _column_label = selected",
            "",
            "                selected = validate_adhoc_subquery(",
            "                    _sql,",
            "                    self.database_id,",
            "                    self.schema,",
            "                )",
            "",
            "                select_exprs.append(",
            "                    self.convert_tbl_column_to_sqla_col(",
            "                        columns_by_name[selected], template_processor=template_processor",
            "                    )",
            "                    if isinstance(selected, str) and selected in columns_by_name",
            "                    else self.make_sqla_column_compatible(",
            "                        literal_column(selected), _column_label",
            "                    )",
            "                )",
            "            metrics_exprs = []",
            "",
            "        if granularity:",
            "            if granularity not in columns_by_name or not dttm_col:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        'Time column \"%(col)s\" does not exist in dataset',",
            "                        col=granularity,",
            "                    )",
            "                )",
            "            time_filters = []",
            "",
            "            if is_timeseries:",
            "                timestamp = dttm_col.get_timestamp_expression(",
            "                    time_grain=time_grain, template_processor=template_processor",
            "                )",
            "                # always put timestamp as the first column",
            "                select_exprs.insert(0, timestamp)",
            "                groupby_all_columns[timestamp.name] = timestamp",
            "",
            "            # Use main dttm column to support index with secondary dttm columns.",
            "            if (",
            "                self.always_filter_main_dttm",
            "                and self.main_dttm_col in self.dttm_cols",
            "                and self.main_dttm_col != dttm_col.column_name",
            "            ):",
            "                time_filters.append(",
            "                    self.get_time_filter(",
            "                        time_col=columns_by_name[self.main_dttm_col],",
            "                        start_dttm=from_dttm,",
            "                        end_dttm=to_dttm,",
            "                        template_processor=template_processor,",
            "                    )",
            "                )",
            "",
            "            time_filter_column = self.get_time_filter(",
            "                time_col=dttm_col,",
            "                start_dttm=from_dttm,",
            "                end_dttm=to_dttm,",
            "                template_processor=template_processor,",
            "            )",
            "            time_filters.append(time_filter_column)",
            "",
            "        # Always remove duplicates by column name, as sometimes `metrics_exprs`",
            "        # can have the same name as a groupby column (e.g. when users use",
            "        # raw columns as custom SQL adhoc metric).",
            "        select_exprs = remove_duplicates(",
            "            select_exprs + metrics_exprs, key=lambda x: x.name",
            "        )",
            "",
            "        # Expected output columns",
            "        labels_expected = [c.key for c in select_exprs]",
            "",
            "        # Order by columns are \"hidden\" columns, some databases require them",
            "        # always be present in SELECT if an aggregation function is used",
            "        if not db_engine_spec.allows_hidden_orderby_agg:",
            "            select_exprs = remove_duplicates(select_exprs + orderby_exprs)",
            "",
            "        qry = sa.select(select_exprs)",
            "",
            "        tbl, cte = self.get_from_clause(template_processor)",
            "",
            "        if groupby_all_columns:",
            "            qry = qry.group_by(*groupby_all_columns.values())",
            "",
            "        where_clause_and = []",
            "        having_clause_and = []",
            "",
            "        for flt in filter:  # type: ignore",
            "            if not all(flt.get(s) for s in [\"col\", \"op\"]):",
            "                continue",
            "            flt_col = flt[\"col\"]",
            "            val = flt.get(\"val\")",
            "            flt_grain = flt.get(\"grain\")",
            "            op = flt[\"op\"].upper()",
            "            col_obj: Optional[\"TableColumn\"] = None",
            "            sqla_col: Optional[Column] = None",
            "            if flt_col == utils.DTTM_ALIAS and is_timeseries and dttm_col:",
            "                col_obj = dttm_col",
            "            elif is_adhoc_column(flt_col):",
            "                try:",
            "                    sqla_col = self.adhoc_column_to_sqla(flt_col, force_type_check=True)",
            "                    applied_adhoc_filters_columns.append(flt_col)",
            "                except ColumnNotFoundException:",
            "                    rejected_adhoc_filters_columns.append(flt_col)",
            "                    continue",
            "            else:",
            "                col_obj = columns_by_name.get(cast(str, flt_col))",
            "            filter_grain = flt.get(\"grain\")",
            "",
            "            if get_column_name(flt_col) in removed_filters:",
            "                # Skip generating SQLA filter when the jinja template handles it.",
            "                continue",
            "",
            "            if col_obj or sqla_col is not None:",
            "                if sqla_col is not None:",
            "                    pass",
            "                elif col_obj and filter_grain:",
            "                    sqla_col = col_obj.get_timestamp_expression(",
            "                        time_grain=filter_grain, template_processor=template_processor",
            "                    )",
            "                elif col_obj:",
            "                    sqla_col = self.convert_tbl_column_to_sqla_col(",
            "                        tbl_column=col_obj, template_processor=template_processor",
            "                    )",
            "                col_type = col_obj.type if col_obj else None",
            "                col_spec = db_engine_spec.get_column_spec(native_type=col_type)",
            "                is_list_target = op in (",
            "                    utils.FilterOperator.IN.value,",
            "                    utils.FilterOperator.NOT_IN.value,",
            "                )",
            "",
            "                col_advanced_data_type = col_obj.advanced_data_type if col_obj else \"\"",
            "",
            "                if col_spec and not col_advanced_data_type:",
            "                    target_generic_type = col_spec.generic_type",
            "                else:",
            "                    target_generic_type = GenericDataType.STRING",
            "                eq = self.filter_values_handler(",
            "                    values=val,",
            "                    operator=op,",
            "                    target_generic_type=target_generic_type,",
            "                    target_native_type=col_type,",
            "                    is_list_target=is_list_target,",
            "                    db_engine_spec=db_engine_spec,",
            "                )",
            "                if (",
            "                    col_advanced_data_type != \"\"",
            "                    and feature_flag_manager.is_feature_enabled(",
            "                        \"ENABLE_ADVANCED_DATA_TYPES\"",
            "                    )",
            "                    and col_advanced_data_type in ADVANCED_DATA_TYPES",
            "                ):",
            "                    values = eq if is_list_target else [eq]  # type: ignore",
            "                    bus_resp: AdvancedDataTypeResponse = ADVANCED_DATA_TYPES[",
            "                        col_advanced_data_type",
            "                    ].translate_type(",
            "                        {",
            "                            \"type\": col_advanced_data_type,",
            "                            \"values\": values,",
            "                        }",
            "                    )",
            "                    if bus_resp[\"error_message\"]:",
            "                        raise AdvancedDataTypeResponseError(",
            "                            _(bus_resp[\"error_message\"])",
            "                        )",
            "",
            "                    where_clause_and.append(",
            "                        ADVANCED_DATA_TYPES[col_advanced_data_type].translate_filter(",
            "                            sqla_col, op, bus_resp[\"values\"]",
            "                        )",
            "                    )",
            "                elif is_list_target:",
            "                    assert isinstance(eq, (tuple, list))",
            "                    if len(eq) == 0:",
            "                        raise QueryObjectValidationError(",
            "                            _(\"Filter value list cannot be empty\")",
            "                        )",
            "                    if len(eq) > len(",
            "                        eq_without_none := [x for x in eq if x is not None]",
            "                    ):",
            "                        is_null_cond = sqla_col.is_(None)",
            "                        if eq:",
            "                            cond = or_(is_null_cond, sqla_col.in_(eq_without_none))",
            "                        else:",
            "                            cond = is_null_cond",
            "                    else:",
            "                        cond = sqla_col.in_(eq)",
            "                    if op == utils.FilterOperator.NOT_IN.value:",
            "                        cond = ~cond",
            "                    where_clause_and.append(cond)",
            "                elif op == utils.FilterOperator.IS_NULL.value:",
            "                    where_clause_and.append(sqla_col.is_(None))",
            "                elif op == utils.FilterOperator.IS_NOT_NULL.value:",
            "                    where_clause_and.append(sqla_col.isnot(None))",
            "                elif op == utils.FilterOperator.IS_TRUE.value:",
            "                    where_clause_and.append(sqla_col.is_(True))",
            "                elif op == utils.FilterOperator.IS_FALSE.value:",
            "                    where_clause_and.append(sqla_col.is_(False))",
            "                else:",
            "                    if (",
            "                        op",
            "                        not in {",
            "                            utils.FilterOperator.EQUALS.value,",
            "                            utils.FilterOperator.NOT_EQUALS.value,",
            "                        }",
            "                        and eq is None",
            "                    ):",
            "                        raise QueryObjectValidationError(",
            "                            _(",
            "                                \"Must specify a value for filters \"",
            "                                \"with comparison operators\"",
            "                            )",
            "                        )",
            "                    if op == utils.FilterOperator.EQUALS.value:",
            "                        where_clause_and.append(sqla_col == eq)",
            "                    elif op == utils.FilterOperator.NOT_EQUALS.value:",
            "                        where_clause_and.append(sqla_col != eq)",
            "                    elif op == utils.FilterOperator.GREATER_THAN.value:",
            "                        where_clause_and.append(sqla_col > eq)",
            "                    elif op == utils.FilterOperator.LESS_THAN.value:",
            "                        where_clause_and.append(sqla_col < eq)",
            "                    elif op == utils.FilterOperator.GREATER_THAN_OR_EQUALS.value:",
            "                        where_clause_and.append(sqla_col >= eq)",
            "                    elif op == utils.FilterOperator.LESS_THAN_OR_EQUALS.value:",
            "                        where_clause_and.append(sqla_col <= eq)",
            "                    elif op in {",
            "                        utils.FilterOperator.ILIKE.value,",
            "                        utils.FilterOperator.LIKE.value,",
            "                    }:",
            "                        if target_generic_type != GenericDataType.STRING:",
            "                            sqla_col = sa.cast(sqla_col, sa.String)",
            "",
            "                        if op == utils.FilterOperator.LIKE.value:",
            "                            where_clause_and.append(sqla_col.like(eq))",
            "                        else:",
            "                            where_clause_and.append(sqla_col.ilike(eq))",
            "                    elif (",
            "                        op == utils.FilterOperator.TEMPORAL_RANGE.value",
            "                        and isinstance(eq, str)",
            "                        and col_obj is not None",
            "                    ):",
            "                        _since, _until = get_since_until_from_time_range(",
            "                            time_range=eq,",
            "                            time_shift=time_shift,",
            "                            extras=extras,",
            "                        )",
            "                        where_clause_and.append(",
            "                            self.get_time_filter(",
            "                                time_col=col_obj,",
            "                                start_dttm=_since,",
            "                                end_dttm=_until,",
            "                                time_grain=flt_grain,",
            "                                label=sqla_col.key,",
            "                                template_processor=template_processor,",
            "                            )",
            "                        )",
            "                    else:",
            "                        raise QueryObjectValidationError(",
            "                            _(\"Invalid filter operation type: %(op)s\", op=op)",
            "                        )",
            "        where_clause_and += self.get_sqla_row_level_filters(template_processor)",
            "        if extras:",
            "            where = extras.get(\"where\")",
            "            if where:",
            "                try:",
            "                    where = template_processor.process_template(f\"({where})\")",
            "                except TemplateError as ex:",
            "                    raise QueryObjectValidationError(",
            "                        _(",
            "                            \"Error in jinja expression in WHERE clause: %(msg)s\",",
            "                            msg=ex.message,",
            "                        )",
            "                    ) from ex",
            "                where = self._process_sql_expression(",
            "                    expression=where,",
            "                    database_id=self.database_id,",
            "                    schema=self.schema,",
            "                    template_processor=template_processor,",
            "                )",
            "                where_clause_and += [self.text(where)]",
            "            having = extras.get(\"having\")",
            "            if having:",
            "                try:",
            "                    having = template_processor.process_template(f\"({having})\")",
            "                except TemplateError as ex:",
            "                    raise QueryObjectValidationError(",
            "                        _(",
            "                            \"Error in jinja expression in HAVING clause: %(msg)s\",",
            "                            msg=ex.message,",
            "                        )",
            "                    ) from ex",
            "                having = self._process_sql_expression(",
            "                    expression=having,",
            "                    database_id=self.database_id,",
            "                    schema=self.schema,",
            "                    template_processor=template_processor,",
            "                )",
            "                having_clause_and += [self.text(having)]",
            "",
            "        if apply_fetch_values_predicate and self.fetch_values_predicate:",
            "            qry = qry.where(",
            "                self.get_fetch_values_predicate(template_processor=template_processor)",
            "            )",
            "        if granularity:",
            "            qry = qry.where(and_(*(time_filters + where_clause_and)))",
            "        else:",
            "            qry = qry.where(and_(*where_clause_and))",
            "        qry = qry.having(and_(*having_clause_and))",
            "",
            "        self.make_orderby_compatible(select_exprs, orderby_exprs)",
            "",
            "        for col, (orig_col, ascending) in zip(orderby_exprs, orderby):",
            "            if not db_engine_spec.allows_alias_in_orderby and isinstance(col, Label):",
            "                # if engine does not allow using SELECT alias in ORDER BY",
            "                # revert to the underlying column",
            "                col = col.element",
            "",
            "            if (",
            "                db_engine_spec.get_allows_alias_in_select(self.database)",
            "                and db_engine_spec.allows_hidden_cc_in_orderby",
            "                and col.name in [select_col.name for select_col in select_exprs]",
            "            ):",
            "                with self.database.get_sqla_engine_with_context() as engine:",
            "                    quote = engine.dialect.identifier_preparer.quote",
            "                    col = literal_column(quote(col.name))",
            "            direction = sa.asc if ascending else sa.desc",
            "            qry = qry.order_by(direction(col))",
            "",
            "        if row_limit:",
            "            qry = qry.limit(row_limit)",
            "        if row_offset:",
            "            qry = qry.offset(row_offset)",
            "",
            "        if series_limit and groupby_series_columns:",
            "            if db_engine_spec.allows_joins and db_engine_spec.allows_subqueries:",
            "                # some sql dialects require for order by expressions",
            "                # to also be in the select clause -- others, e.g. vertica,",
            "                # require a unique inner alias",
            "                inner_main_metric_expr = self.make_sqla_column_compatible(",
            "                    main_metric_expr, \"mme_inner__\"",
            "                )",
            "                inner_groupby_exprs = []",
            "                inner_select_exprs = []",
            "                for gby_name, gby_obj in groupby_series_columns.items():",
            "                    inner = self.make_sqla_column_compatible(gby_obj, gby_name + \"__\")",
            "                    inner_groupby_exprs.append(inner)",
            "                    inner_select_exprs.append(inner)",
            "",
            "                inner_select_exprs += [inner_main_metric_expr]",
            "                subq = sa.select(inner_select_exprs).select_from(tbl)",
            "                inner_time_filter = []",
            "",
            "                if dttm_col and not db_engine_spec.time_groupby_inline:",
            "                    inner_time_filter = [",
            "                        self.get_time_filter(",
            "                            time_col=dttm_col,",
            "                            start_dttm=inner_from_dttm or from_dttm,",
            "                            end_dttm=inner_to_dttm or to_dttm,",
            "                            template_processor=template_processor,",
            "                        )",
            "                    ]",
            "                subq = subq.where(and_(*(where_clause_and + inner_time_filter)))",
            "                subq = subq.group_by(*inner_groupby_exprs)",
            "",
            "                ob = inner_main_metric_expr",
            "                if series_limit_metric:",
            "                    ob = self._get_series_orderby(",
            "                        series_limit_metric=series_limit_metric,",
            "                        metrics_by_name=metrics_by_name,",
            "                        columns_by_name=columns_by_name,",
            "                        template_processor=template_processor,",
            "                    )",
            "                direction = sa.desc if order_desc else sa.asc",
            "                subq = subq.order_by(direction(ob))",
            "                subq = subq.limit(series_limit)",
            "",
            "                on_clause = []",
            "                for gby_name, gby_obj in groupby_series_columns.items():",
            "                    # in this case the column name, not the alias, needs to be",
            "                    # conditionally mutated, as it refers to the column alias in",
            "                    # the inner query",
            "                    col_name = db_engine_spec.make_label_compatible(gby_name + \"__\")",
            "                    on_clause.append(gby_obj == sa.column(col_name))",
            "",
            "                tbl = tbl.join(subq.alias(SERIES_LIMIT_SUBQ_ALIAS), and_(*on_clause))",
            "            else:",
            "                if series_limit_metric:",
            "                    orderby = [",
            "                        (",
            "                            self._get_series_orderby(",
            "                                series_limit_metric=series_limit_metric,",
            "                                metrics_by_name=metrics_by_name,",
            "                                columns_by_name=columns_by_name,",
            "                                template_processor=template_processor,",
            "                            ),",
            "                            not order_desc,",
            "                        )",
            "                    ]",
            "",
            "                # run prequery to get top groups",
            "                prequery_obj = {",
            "                    \"is_timeseries\": False,",
            "                    \"row_limit\": series_limit,",
            "                    \"metrics\": metrics,",
            "                    \"granularity\": granularity,",
            "                    \"groupby\": groupby,",
            "                    \"from_dttm\": inner_from_dttm or from_dttm,",
            "                    \"to_dttm\": inner_to_dttm or to_dttm,",
            "                    \"filter\": filter,",
            "                    \"orderby\": orderby,",
            "                    \"extras\": extras,",
            "                    \"columns\": columns,",
            "                    \"order_desc\": True,",
            "                }",
            "",
            "                result = self.query(prequery_obj)",
            "                prequeries.append(result.query)",
            "                dimensions = [",
            "                    c",
            "                    for c in result.df.columns",
            "                    if c not in metrics and c in groupby_series_columns",
            "                ]",
            "                top_groups = self._get_top_groups(",
            "                    result.df, dimensions, groupby_series_columns, columns_by_name",
            "                )",
            "                qry = qry.where(top_groups)",
            "",
            "        qry = qry.select_from(tbl)",
            "",
            "        if is_rowcount:",
            "            if not db_engine_spec.allows_subqueries:",
            "                raise QueryObjectValidationError(",
            "                    _(\"Database does not support subqueries\")",
            "                )",
            "            label = \"rowcount\"",
            "            col = self.make_sqla_column_compatible(literal_column(\"COUNT(*)\"), label)",
            "            qry = sa.select([col]).select_from(qry.alias(\"rowcount_qry\"))",
            "            labels_expected = [label]",
            "",
            "        filter_columns = [flt.get(\"col\") for flt in filter] if filter else []",
            "        rejected_filter_columns = [",
            "            col",
            "            for col in filter_columns",
            "            if col",
            "            and not is_adhoc_column(col)",
            "            and col not in self.column_names",
            "            and col not in applied_template_filters",
            "        ] + rejected_adhoc_filters_columns",
            "",
            "        applied_filter_columns = [",
            "            col",
            "            for col in filter_columns",
            "            if col",
            "            and not is_adhoc_column(col)",
            "            and (col in self.column_names or col in applied_template_filters)",
            "        ] + applied_adhoc_filters_columns",
            "",
            "        return SqlaQuery(",
            "            applied_template_filters=applied_template_filters,",
            "            cte=cte,",
            "            applied_filter_columns=applied_filter_columns,",
            "            rejected_filter_columns=rejected_filter_columns,",
            "            extra_cache_keys=extra_cache_keys,",
            "            labels_expected=labels_expected,",
            "            sqla_query=qry,",
            "            prequeries=prequeries,",
            "        )"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "\"\"\"a collection of model-related helper classes and functions\"\"\"",
            "import builtins",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "import uuid",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from datetime import datetime, timedelta",
            "from json.decoder import JSONDecodeError",
            "from typing import Any, cast, NamedTuple, Optional, TYPE_CHECKING, Union",
            "",
            "import dateutil.parser",
            "import humanize",
            "import numpy as np",
            "import pandas as pd",
            "import pytz",
            "import sqlalchemy as sa",
            "import sqlparse",
            "import yaml",
            "from flask import escape, g, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.models.decorators import renders",
            "from flask_appbuilder.models.mixins import AuditMixin",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import and_, Column, or_, UniqueConstraint",
            "from sqlalchemy.exc import MultipleResultsFound",
            "from sqlalchemy.ext.declarative import declared_attr",
            "from sqlalchemy.orm import Mapper, validates",
            "from sqlalchemy.sql.elements import ColumnElement, literal_column, TextClause",
            "from sqlalchemy.sql.expression import Label, Select, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "from sqlalchemy_utils import UUIDType",
            "",
            "from superset import app, db, is_feature_enabled, security_manager",
            "from superset.advanced_data_type.types import AdvancedDataTypeResponse",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.common.utils.time_range_utils import get_since_until_from_time_range",
            "from superset.constants import EMPTY_STRING, NULL_STRING",
            "from superset.db_engine_specs.base import TimestampExpression",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    AdvancedDataTypeResponseError,",
            "    ColumnNotFoundException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetParseError,",
            "    SupersetSecurityException,",
            ")",
            "from superset.extensions import feature_flag_manager",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.sql_parse import (",
            "    has_table_query,",
            "    insert_rls_in_predicate,",
            "    ParsedQuery,",
            "    sanitize_clause,",
            "    SQLScript,",
            "    SQLStatement,",
            ")",
            "from superset.superset_typing import (",
            "    AdhocMetric,",
            "    Column as ColumnTyping,",
            "    FilterValue,",
            "    FilterValues,",
            "    Metric,",
            "    OrderBy,",
            "    QueryObjectDict,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.core import (",
            "    GenericDataType,",
            "    get_column_name,",
            "    get_user_id,",
            "    is_adhoc_column,",
            "    MediumText,",
            "    remove_duplicates,",
            ")",
            "from superset.utils.dates import datetime_to_epoch",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import SqlMetric, TableColumn",
            "    from superset.db_engine_specs import BaseEngineSpec",
            "    from superset.models.core import Database",
            "",
            "",
            "config = app.config",
            "logger = logging.getLogger(__name__)",
            "",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "SERIES_LIMIT_SUBQ_ALIAS = \"series_limit\"",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "",
            "",
            "def validate_adhoc_subquery(",
            "    sql: str,",
            "    database_id: int,",
            "    default_schema: str,",
            ") -> str:",
            "    \"\"\"",
            "    Check if adhoc SQL contains sub-queries or nested sub-queries with table.",
            "",
            "    If sub-queries are allowed, the adhoc SQL is modified to insert any applicable RLS",
            "    predicates to it.",
            "",
            "    :param sql: adhoc sql expression",
            "    :raise SupersetSecurityException if sql contains sub-queries or",
            "    nested sub-queries with table",
            "    \"\"\"",
            "    statements = []",
            "    for statement in sqlparse.parse(sql):",
            "        if has_table_query(statement):",
            "            if not is_feature_enabled(\"ALLOW_ADHOC_SUBQUERY\"):",
            "                raise SupersetSecurityException(",
            "                    SupersetError(",
            "                        error_type=SupersetErrorType.ADHOC_SUBQUERY_NOT_ALLOWED_ERROR,",
            "                        message=_(\"Custom SQL fields cannot contain sub-queries.\"),",
            "                        level=ErrorLevel.ERROR,",
            "                    )",
            "                )",
            "            statement = insert_rls_in_predicate(statement, database_id, default_schema)",
            "        statements.append(statement)",
            "",
            "    return \";\\n\".join(str(statement) for statement in statements)",
            "",
            "",
            "def json_to_dict(json_str: str) -> dict[Any, Any]:",
            "    if json_str:",
            "        val = re.sub(\",[ \\t\\r\\n]+}\", \"}\", json_str)",
            "        val = re.sub(\",[ \\t\\r\\n]+\\\\]\", \"]\", val)",
            "        return json.loads(val)",
            "",
            "    return {}",
            "",
            "",
            "def convert_uuids(obj: Any) -> Any:",
            "    \"\"\"",
            "    Convert UUID objects to str so we can use yaml.safe_dump",
            "    \"\"\"",
            "    if isinstance(obj, uuid.UUID):",
            "        return str(obj)",
            "",
            "    if isinstance(obj, list):",
            "        return [convert_uuids(el) for el in obj]",
            "",
            "    if isinstance(obj, dict):",
            "        return {k: convert_uuids(v) for k, v in obj.items()}",
            "",
            "    return obj",
            "",
            "",
            "class ImportExportMixin:",
            "    uuid = sa.Column(",
            "        UUIDType(binary=True), primary_key=False, unique=True, default=uuid.uuid4",
            "    )",
            "",
            "    export_parent: Optional[str] = None",
            "    # The name of the attribute",
            "    # with the SQL Alchemy back reference",
            "",
            "    export_children: list[str] = []",
            "    # List of (str) names of attributes",
            "    # with the SQL Alchemy forward references",
            "",
            "    export_fields: list[str] = []",
            "    # The names of the attributes",
            "    # that are available for import and export",
            "",
            "    extra_import_fields: list[str] = []",
            "    # Additional fields that should be imported,",
            "    # even though they were not exported",
            "",
            "    __mapper__: Mapper",
            "",
            "    @classmethod",
            "    def _unique_constraints(cls) -> list[set[str]]:",
            "        \"\"\"Get all (single column and multi column) unique constraints\"\"\"",
            "        unique = [",
            "            {c.name for c in u.columns}",
            "            for u in cls.__table_args__  # type: ignore",
            "            if isinstance(u, UniqueConstraint)",
            "        ]",
            "        unique.extend(",
            "            {c.name} for c in cls.__table__.columns if c.unique  # type: ignore",
            "        )",
            "        return unique",
            "",
            "    @classmethod",
            "    def parent_foreign_key_mappings(cls) -> dict[str, str]:",
            "        \"\"\"Get a mapping of foreign name to the local name of foreign keys\"\"\"",
            "        parent_rel = cls.__mapper__.relationships.get(cls.export_parent)",
            "        if parent_rel:",
            "            return {l.name: r.name for (l, r) in parent_rel.local_remote_pairs}",
            "        return {}",
            "",
            "    @classmethod",
            "    def export_schema(",
            "        cls, recursive: bool = True, include_parent_ref: bool = False",
            "    ) -> dict[str, Any]:",
            "        \"\"\"Export schema as a dictionary\"\"\"",
            "        parent_excludes = set()",
            "        if not include_parent_ref:",
            "            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)",
            "            if parent_ref:",
            "                parent_excludes = {column.name for column in parent_ref.local_columns}",
            "",
            "        def formatter(column: sa.Column) -> str:",
            "            return (",
            "                f\"{str(column.type)} Default ({column.default.arg})\"",
            "                if column.default",
            "                else str(column.type)",
            "            )",
            "",
            "        schema: dict[str, Any] = {",
            "            column.name: formatter(column)",
            "            for column in cls.__table__.columns  # type: ignore",
            "            if (column.name in cls.export_fields and column.name not in parent_excludes)",
            "        }",
            "        if recursive:",
            "            for column in cls.export_children:",
            "                child_class = cls.__mapper__.relationships[column].argument.class_",
            "                schema[column] = [",
            "                    child_class.export_schema(",
            "                        recursive=recursive, include_parent_ref=include_parent_ref",
            "                    )",
            "                ]",
            "        return schema",
            "",
            "    @classmethod",
            "    def import_from_dict(",
            "        # pylint: disable=too-many-arguments,too-many-branches,too-many-locals",
            "        cls,",
            "        dict_rep: dict[Any, Any],",
            "        parent: Optional[Any] = None,",
            "        recursive: bool = True,",
            "        sync: Optional[list[str]] = None,",
            "        allow_reparenting: bool = False,",
            "    ) -> Any:",
            "        \"\"\"Import obj from a dictionary\"\"\"",
            "        if sync is None:",
            "            sync = []",
            "        parent_refs = cls.parent_foreign_key_mappings()",
            "        export_fields = (",
            "            set(cls.export_fields)",
            "            | set(cls.extra_import_fields)",
            "            | set(parent_refs.keys())",
            "            | {\"uuid\"}",
            "        )",
            "        new_children = {c: dict_rep[c] for c in cls.export_children if c in dict_rep}",
            "        unique_constraints = cls._unique_constraints()",
            "",
            "        filters = []  # Using these filters to check if obj already exists",
            "",
            "        # Remove fields that should not get imported",
            "        for k in list(dict_rep):",
            "            if k not in export_fields and k not in parent_refs:",
            "                del dict_rep[k]",
            "",
            "        if not parent:",
            "            if cls.export_parent:",
            "                for prnt in parent_refs.keys():",
            "                    if prnt not in dict_rep:",
            "                        raise RuntimeError(f\"{cls.__name__}: Missing field {prnt}\")",
            "        else:",
            "            # Set foreign keys to parent obj",
            "            for k, v in parent_refs.items():",
            "                dict_rep[k] = getattr(parent, v)",
            "",
            "        if not allow_reparenting:",
            "            # Add filter for parent obj",
            "            filters.extend(",
            "                [getattr(cls, k) == dict_rep.get(k) for k in parent_refs.keys()]",
            "            )",
            "",
            "        # Add filter for unique constraints",
            "        ucs = [",
            "            and_(",
            "                *[",
            "                    getattr(cls, k) == dict_rep.get(k)",
            "                    for k in cs",
            "                    if dict_rep.get(k) is not None",
            "                ]",
            "            )",
            "            for cs in unique_constraints",
            "        ]",
            "        filters.append(or_(*ucs))",
            "",
            "        # Check if object already exists in DB, break if more than one is found",
            "        try:",
            "            obj_query = db.session.query(cls).filter(and_(*filters))",
            "            obj = obj_query.one_or_none()",
            "        except MultipleResultsFound as ex:",
            "            logger.error(",
            "                \"Error importing %s \\n %s \\n %s\",",
            "                cls.__name__,",
            "                str(obj_query),",
            "                yaml.safe_dump(dict_rep),",
            "                exc_info=True,",
            "            )",
            "            raise ex",
            "",
            "        if not obj:",
            "            is_new_obj = True",
            "            # Create new DB object",
            "            obj = cls(**dict_rep)",
            "            logger.info(\"Importing new %s %s\", obj.__tablename__, str(obj))",
            "            if cls.export_parent and parent:",
            "                setattr(obj, cls.export_parent, parent)",
            "            db.session.add(obj)",
            "        else:",
            "            is_new_obj = False",
            "            logger.info(\"Updating %s %s\", obj.__tablename__, str(obj))",
            "            # Update columns",
            "            for k, v in dict_rep.items():",
            "                setattr(obj, k, v)",
            "",
            "        # Recursively create children",
            "        if recursive:",
            "            for child in cls.export_children:",
            "                argument = cls.__mapper__.relationships[child].argument",
            "                child_class = (",
            "                    argument.class_ if hasattr(argument, \"class_\") else argument",
            "                )",
            "                added = []",
            "                for c_obj in new_children.get(child, []):",
            "                    added.append(",
            "                        child_class.import_from_dict(",
            "                            dict_rep=c_obj, parent=obj, sync=sync",
            "                        )",
            "                    )",
            "                # If children should get synced, delete the ones that did not",
            "                # get updated.",
            "                if child in sync and not is_new_obj:",
            "                    back_refs = child_class.parent_foreign_key_mappings()",
            "                    delete_filters = [",
            "                        getattr(child_class, k) == getattr(obj, back_refs.get(k))",
            "                        for k in back_refs.keys()",
            "                    ]",
            "                    to_delete = set(",
            "                        db.session.query(child_class).filter(and_(*delete_filters))",
            "                    ).difference(set(added))",
            "                    for o in to_delete:",
            "                        logger.info(\"Deleting %s %s\", child, str(obj))",
            "                        db.session.delete(o)",
            "",
            "        return obj",
            "",
            "    def export_to_dict(",
            "        self,",
            "        recursive: bool = True,",
            "        include_parent_ref: bool = False,",
            "        include_defaults: bool = False,",
            "        export_uuids: bool = False,",
            "    ) -> dict[Any, Any]:",
            "        \"\"\"Export obj to dictionary\"\"\"",
            "        export_fields = set(self.export_fields)",
            "        if export_uuids:",
            "            export_fields.add(\"uuid\")",
            "            if \"id\" in export_fields:",
            "                export_fields.remove(\"id\")",
            "",
            "        cls = self.__class__",
            "        parent_excludes = set()",
            "        if recursive and not include_parent_ref:",
            "            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)",
            "            if parent_ref:",
            "                parent_excludes = {c.name for c in parent_ref.local_columns}",
            "        dict_rep = {",
            "            c.name: getattr(self, c.name)",
            "            for c in cls.__table__.columns  # type: ignore",
            "            if (",
            "                c.name in export_fields",
            "                and c.name not in parent_excludes",
            "                and (",
            "                    include_defaults",
            "                    or (",
            "                        getattr(self, c.name) is not None",
            "                        and (not c.default or getattr(self, c.name) != c.default.arg)",
            "                    )",
            "                )",
            "            )",
            "        }",
            "",
            "        # sort according to export_fields using DSU (decorate, sort, undecorate)",
            "        order = {field: i for i, field in enumerate(self.export_fields)}",
            "        decorated_keys = [(order.get(k, len(order)), k) for k in dict_rep]",
            "        decorated_keys.sort()",
            "        dict_rep = {k: dict_rep[k] for _, k in decorated_keys}",
            "",
            "        if recursive:",
            "            for cld in self.export_children:",
            "                # sorting to make lists of children stable",
            "                dict_rep[cld] = sorted(",
            "                    [",
            "                        child.export_to_dict(",
            "                            recursive=recursive,",
            "                            include_parent_ref=include_parent_ref,",
            "                            include_defaults=include_defaults,",
            "                        )",
            "                        for child in getattr(self, cld)",
            "                    ],",
            "                    key=lambda k: sorted(str(k.items())),",
            "                )",
            "",
            "        return convert_uuids(dict_rep)",
            "",
            "    def override(self, obj: Any) -> None:",
            "        \"\"\"Overrides the plain fields of the dashboard.\"\"\"",
            "        for field in obj.__class__.export_fields:",
            "            setattr(self, field, getattr(obj, field))",
            "",
            "    def copy(self) -> Any:",
            "        \"\"\"Creates a copy of the dashboard without relationships.\"\"\"",
            "        new_obj = self.__class__()",
            "        new_obj.override(self)",
            "        return new_obj",
            "",
            "    def alter_params(self, **kwargs: Any) -> None:",
            "        params = self.params_dict",
            "        params.update(kwargs)",
            "        self.params = json.dumps(params)",
            "",
            "    def remove_params(self, param_to_remove: str) -> None:",
            "        params = self.params_dict",
            "        params.pop(param_to_remove, None)",
            "        self.params = json.dumps(params)",
            "",
            "    def reset_ownership(self) -> None:",
            "        \"\"\"object will belong to the user the current user\"\"\"",
            "        # make sure the object doesn't have relations to a user",
            "        # it will be filled by appbuilder on save",
            "        self.created_by = None",
            "        self.changed_by = None",
            "        # flask global context might not exist (in cli or tests for example)",
            "        self.owners = []",
            "        if g and hasattr(g, \"user\"):",
            "            self.owners = [g.user]",
            "",
            "    @property",
            "    def params_dict(self) -> dict[Any, Any]:",
            "        return json_to_dict(self.params)",
            "",
            "    @property",
            "    def template_params_dict(self) -> dict[Any, Any]:",
            "        return json_to_dict(self.template_params)  # type: ignore",
            "",
            "",
            "def _user(user: User) -> str:",
            "    if not user:",
            "        return \"\"",
            "    return escape(user)",
            "",
            "",
            "class AuditMixinNullable(AuditMixin):",
            "    \"\"\"Altering the AuditMixin to use nullable fields",
            "",
            "    Allows creating objects programmatically outside of CRUD",
            "    \"\"\"",
            "",
            "    created_on = sa.Column(sa.DateTime, default=datetime.now, nullable=True)",
            "    changed_on = sa.Column(",
            "        sa.DateTime, default=datetime.now, onupdate=datetime.now, nullable=True",
            "    )",
            "",
            "    @declared_attr",
            "    def created_by_fk(self) -> sa.Column:  # pylint: disable=arguments-renamed",
            "        return sa.Column(",
            "            sa.Integer,",
            "            sa.ForeignKey(\"ab_user.id\"),",
            "            default=get_user_id,",
            "            nullable=True,",
            "        )",
            "",
            "    @declared_attr",
            "    def changed_by_fk(self) -> sa.Column:  # pylint: disable=arguments-renamed",
            "        return sa.Column(",
            "            sa.Integer,",
            "            sa.ForeignKey(\"ab_user.id\"),",
            "            default=get_user_id,",
            "            onupdate=get_user_id,",
            "            nullable=True,",
            "        )",
            "",
            "    @property",
            "    def created_by_name(self) -> str:",
            "        if self.created_by:",
            "            return escape(f\"{self.created_by}\")",
            "        return \"\"",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if self.changed_by:",
            "            return escape(f\"{self.changed_by}\")",
            "        return \"\"",
            "",
            "    @renders(\"created_by\")",
            "    def creator(self) -> Union[Markup, str]:",
            "        return _user(self.created_by)",
            "",
            "    @property",
            "    def changed_by_(self) -> Union[Markup, str]:",
            "        return _user(self.changed_by)",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_(self) -> Markup:",
            "        return Markup(f'<span class=\"no-wrap\">{self.changed_on}</span>')",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_delta_humanized(self) -> str:",
            "        return self.changed_on_humanized",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_dttm(self) -> float:",
            "        return datetime_to_epoch(self.changed_on)",
            "",
            "    @renders(\"created_on\")",
            "    def created_on_delta_humanized(self) -> str:",
            "        return self.created_on_humanized",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_utc(self) -> str:",
            "        # Convert naive datetime to UTC",
            "        return self.changed_on.astimezone(pytz.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")",
            "",
            "    @property",
            "    def changed_on_humanized(self) -> str:",
            "        return humanize.naturaltime(datetime.now() - self.changed_on)",
            "",
            "    @property",
            "    def created_on_humanized(self) -> str:",
            "        return humanize.naturaltime(datetime.now() - self.created_on)",
            "",
            "    @renders(\"changed_on\")",
            "    def modified(self) -> Markup:",
            "        return Markup(f'<span class=\"no-wrap\">{self.changed_on_humanized}</span>')",
            "",
            "",
            "class QueryResult:  # pylint: disable=too-few-public-methods",
            "",
            "    \"\"\"Object returned by the query interface\"\"\"",
            "",
            "    def __init__(  # pylint: disable=too-many-arguments",
            "        self,",
            "        df: pd.DataFrame,",
            "        query: str,",
            "        duration: timedelta,",
            "        applied_template_filters: Optional[list[str]] = None,",
            "        applied_filter_columns: Optional[list[ColumnTyping]] = None,",
            "        rejected_filter_columns: Optional[list[ColumnTyping]] = None,",
            "        status: str = QueryStatus.SUCCESS,",
            "        error_message: Optional[str] = None,",
            "        errors: Optional[list[dict[str, Any]]] = None,",
            "        from_dttm: Optional[datetime] = None,",
            "        to_dttm: Optional[datetime] = None,",
            "    ) -> None:",
            "        self.df = df",
            "        self.query = query",
            "        self.duration = duration",
            "        self.applied_template_filters = applied_template_filters or []",
            "        self.applied_filter_columns = applied_filter_columns or []",
            "        self.rejected_filter_columns = rejected_filter_columns or []",
            "        self.status = status",
            "        self.error_message = error_message",
            "        self.errors = errors or []",
            "        self.from_dttm = from_dttm",
            "        self.to_dttm = to_dttm",
            "",
            "",
            "class ExtraJSONMixin:",
            "    \"\"\"Mixin to add an `extra` column (JSON) and utility methods\"\"\"",
            "",
            "    extra_json = sa.Column(MediumText(), default=\"{}\")",
            "",
            "    @property",
            "    def extra(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra_json or \"{}\") or {}",
            "        except (TypeError, JSONDecodeError) as exc:",
            "            logger.error(",
            "                \"Unable to load an extra json: %r. Leaving empty.\", exc, exc_info=True",
            "            )",
            "            return {}",
            "",
            "    @extra.setter",
            "    def extra(self, extras: dict[str, Any]) -> None:",
            "        self.extra_json = json.dumps(extras)",
            "",
            "    def set_extra_json_key(self, key: str, value: Any) -> None:",
            "        extra = self.extra",
            "        extra[key] = value",
            "        self.extra_json = json.dumps(extra)",
            "",
            "    @validates(\"extra_json\")",
            "    def ensure_extra_json_is_not_none(",
            "        self,",
            "        _: str,",
            "        value: Optional[dict[str, Any]],",
            "    ) -> Any:",
            "        if value is None:",
            "            return \"{}\"",
            "        return value",
            "",
            "",
            "class CertificationMixin:",
            "    \"\"\"Mixin to add extra certification fields\"\"\"",
            "",
            "    extra = sa.Column(sa.Text, default=\"{}\")",
            "",
            "    def get_extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    @property",
            "    def is_certified(self) -> bool:",
            "        return bool(self.get_extra_dict().get(\"certification\"))",
            "",
            "    @property",
            "    def certified_by(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"certification\", {}).get(\"certified_by\")",
            "",
            "    @property",
            "    def certification_details(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"certification\", {}).get(\"details\")",
            "",
            "    @property",
            "    def warning_markdown(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"warning_markdown\")",
            "",
            "",
            "def clone_model(",
            "    target: Model,",
            "    ignore: Optional[list[str]] = None,",
            "    keep_relations: Optional[list[str]] = None,",
            "    **kwargs: Any,",
            ") -> Model:",
            "    \"\"\"",
            "    Clone a SQLAlchemy model. By default will only clone naive column attributes.",
            "    To include relationship attributes, use `keep_relations`.",
            "    \"\"\"",
            "    ignore = ignore or []",
            "",
            "    table = target.__table__",
            "    primary_keys = table.primary_key.columns.keys()",
            "    data = {",
            "        attr: getattr(target, attr)",
            "        for attr in list(table.columns.keys()) + (keep_relations or [])",
            "        if attr not in primary_keys and attr not in ignore",
            "    }",
            "    data.update(kwargs)",
            "",
            "    return target.__class__(**data)",
            "",
            "",
            "# todo(hugh): centralize where this code lives",
            "class QueryStringExtended(NamedTuple):",
            "    applied_template_filters: Optional[list[str]]",
            "    applied_filter_columns: list[ColumnTyping]",
            "    rejected_filter_columns: list[ColumnTyping]",
            "    labels_expected: list[str]",
            "    prequeries: list[str]",
            "    sql: str",
            "",
            "",
            "class SqlaQuery(NamedTuple):",
            "    applied_template_filters: list[str]",
            "    applied_filter_columns: list[ColumnTyping]",
            "    rejected_filter_columns: list[ColumnTyping]",
            "    cte: Optional[str]",
            "    extra_cache_keys: list[Any]",
            "    labels_expected: list[str]",
            "    prequeries: list[str]",
            "    sqla_query: Select",
            "",
            "",
            "class ExploreMixin:  # pylint: disable=too-many-public-methods",
            "    \"\"\"",
            "    Allows any flask_appbuilder.Model (Query, Table, etc.)",
            "    to be used to power a chart inside /explore",
            "    \"\"\"",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "    fetch_values_predicate = None",
            "",
            "    @property",
            "    def type(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def db_extra(self) -> Optional[dict[str, Any]]:",
            "        raise NotImplementedError()",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def database_id(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def owners_data(self) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def metrics(self) -> list[Any]:",
            "        return []",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def is_rls_supported(self) -> bool:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def cache_timeout(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def column_names(self) -> list[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def offset(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def main_dttm_col(self) -> Optional[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def always_filter_main_dttm(self) -> Optional[bool]:",
            "        return False",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def db_engine_spec(self) -> builtins.type[\"BaseEngineSpec\"]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def database(self) -> \"Database\":",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def schema(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def sql(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def columns(self) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "    def get_extra_cache_keys(self, query_obj: dict[str, Any]) -> list[Hashable]:",
            "        raise NotImplementedError()",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        raise NotImplementedError()",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: Optional[  # pylint: disable=unused-argument",
            "            BaseTemplateProcessor",
            "        ] = None,",
            "    ) -> TextClause:",
            "        return self.fetch_values_predicate",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[Union[int, str], list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def _process_sql_expression(",
            "        self,",
            "        expression: Optional[str],",
            "        database_id: int,",
            "        schema: str,",
            "        template_processor: Optional[BaseTemplateProcessor],",
            "    ) -> Optional[str]:",
            "        if template_processor and expression:",
            "            expression = template_processor.process_template(expression)",
            "        if expression:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            try:",
            "                expression = sanitize_clause(expression)",
            "            except QueryClauseValidationException as ex:",
            "                raise QueryObjectValidationError(ex.message) from ex",
            "        return expression",
            "",
            "    def make_sqla_column_compatible(",
            "        self, sqla_col: ColumnElement, label: Optional[str] = None",
            "    ) -> ColumnElement:",
            "        \"\"\"Takes a sqlalchemy column object and adds label info if supported by engine.",
            "        :param sqla_col: sqlalchemy column instance",
            "        :param label: alias/label that column is expected to have",
            "        :return: either a sql alchemy column or label instance if supported by engine",
            "        \"\"\"",
            "        label_expected = label or sqla_col.name",
            "        db_engine_spec = self.db_engine_spec",
            "        # add quotes to tables",
            "        if db_engine_spec.get_allows_alias_in_select(self.database):",
            "            label = db_engine_spec.make_label_compatible(label_expected)",
            "            sqla_col = sqla_col.label(label)",
            "        sqla_col.key = label_expected",
            "        return sqla_col",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        if sql_query_mutator := config[\"SQL_QUERY_MUTATOR\"]:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: Optional[str]) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    def get_query_str_extended(",
            "        self,",
            "        query_obj: QueryObjectDict,",
            "        mutate: bool = True,",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        try:",
            "            sql = SQLStatement(sql, engine=self.db_engine_spec.engine).format()",
            "        except SupersetParseError:",
            "            logger.warning(\"Unable to parse SQL to format it, passing it as-is\")",
            "",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "    ) -> Union[str, int, float, bool, str]:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if isinstance(column_, dict):",
            "            if (",
            "                column_.get(\"type\")",
            "                and column_.get(\"is_temporal\")",
            "                and isinstance(value, str)",
            "            ):",
            "                sql = self.db_engine_spec.convert_dttm(",
            "                    column_.get(\"type\"), dateutil.parser.parse(value), db_extra=None",
            "                )",
            "",
            "                if sql:",
            "                    value = self.db_engine_spec.get_text_clause(sql)",
            "        else:",
            "            if column_.type and column_.is_temporal and isinstance(value, str):",
            "                sql = self.db_engine_spec.convert_dttm(",
            "                    column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "                )",
            "",
            "                if sql:",
            "                    value = self.text(sql)",
            "        return value",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def exc_query(self, qry: Any) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(qry)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> Optional[pd.DataFrame]:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_rendered_sql(",
            "        self,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "",
            "        script = SQLScript(sql.strip(\"\\t\\r\\n; \"), engine=self.db_engine_spec.engine)",
            "        if len(script.statements) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "",
            "        sql = script.statements[0].format(comments=False)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        return sql",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def get_from_clause(",
            "        self, template_processor: Optional[BaseTemplateProcessor] = None",
            "    ) -> tuple[Union[TableClause, Alias], Optional[str]]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql, engine=self.db_engine_spec.engine)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            sa.table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, \"TableColumn\"],  # pylint: disable=unused-argument",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            sqla_column = sa.column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = self._process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    @property",
            "    def template_params_dict(self) -> dict[Any, Any]:",
            "        return {}",
            "",
            "    @staticmethod",
            "    def filter_values_handler(  # pylint: disable=too-many-arguments",
            "        values: Optional[FilterValues],",
            "        operator: str,",
            "        target_generic_type: utils.GenericDataType,",
            "        target_native_type: Optional[str] = None,",
            "        is_list_target: bool = False,",
            "        db_engine_spec: Optional[",
            "            builtins.type[\"BaseEngineSpec\"]",
            "        ] = None,  # fix(hughhh): Optional[Type[BaseEngineSpec]]",
            "        db_extra: Optional[dict[str, Any]] = None,",
            "    ) -> Optional[FilterValues]:",
            "        if values is None:",
            "            return None",
            "",
            "        def handle_single_value(value: Optional[FilterValue]) -> Optional[FilterValue]:",
            "            if operator == utils.FilterOperator.TEMPORAL_RANGE:",
            "                return value",
            "            if (",
            "                isinstance(value, (float, int))",
            "                and target_generic_type == utils.GenericDataType.TEMPORAL",
            "                and target_native_type is not None",
            "                and db_engine_spec is not None",
            "            ):",
            "                value = db_engine_spec.convert_dttm(",
            "                    target_type=target_native_type,",
            "                    dttm=datetime.utcfromtimestamp(value / 1000),",
            "                    db_extra=db_extra,",
            "                )",
            "                value = literal_column(value)",
            "            if isinstance(value, str):",
            "                value = value.strip(\"\\t\\n\")",
            "",
            "                if (",
            "                    target_generic_type == utils.GenericDataType.NUMERIC",
            "                    and operator",
            "                    not in {",
            "                        utils.FilterOperator.ILIKE,",
            "                        utils.FilterOperator.LIKE,",
            "                    }",
            "                ):",
            "                    # For backwards compatibility and edge cases",
            "                    # where a column data type might have changed",
            "                    return utils.cast_to_num(value)",
            "                if value == NULL_STRING:",
            "                    return None",
            "                if value == EMPTY_STRING:",
            "                    return \"\"",
            "            if target_generic_type == utils.GenericDataType.BOOLEAN:",
            "                return utils.cast_to_boolean(value)",
            "            return value",
            "",
            "        if isinstance(values, (list, tuple)):",
            "            values = [handle_single_value(v) for v in values]  # type: ignore",
            "        else:",
            "            values = handle_single_value(values)",
            "        if is_list_target and not isinstance(values, (tuple, list)):",
            "            values = [values]  # type: ignore",
            "        elif not is_list_target and isinstance(values, (tuple, list)):",
            "            values = values[0] if values else None",
            "        return values",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, \"SqlMetric\"],",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def adhoc_column_to_sqla(",
            "        self,",
            "        col: \"AdhocColumn\",  # type: ignore",
            "        force_type_check: bool = False,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        raise NotImplementedError()",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def dttm_sql_literal(self, dttm: datetime, col: \"TableColumn\") -> str:",
            "        \"\"\"Convert datetime object to a SQL expression string\"\"\"",
            "",
            "        sql = (",
            "            self.db_engine_spec.convert_dttm(col.type, dttm, db_extra=self.db_extra)",
            "            if col.type",
            "            else None",
            "        )",
            "",
            "        if sql:",
            "            return sql",
            "",
            "        tf = col.python_date_format",
            "",
            "        # Fallback to the default format (if defined).",
            "        if not tf and self.db_extra:",
            "            tf = self.db_extra.get(\"python_date_format_by_column_name\", {}).get(",
            "                col.column_name",
            "            )",
            "",
            "        if tf:",
            "            if tf in {\"epoch_ms\", \"epoch_s\"}:",
            "                seconds_since_epoch = int(dttm.timestamp())",
            "                if tf == \"epoch_s\":",
            "                    return str(seconds_since_epoch)",
            "                return str(seconds_since_epoch * 1000)",
            "            return f\"'{dttm.strftime(tf)}'\"",
            "",
            "        return f\"\"\"'{dttm.strftime(\"%Y-%m-%d %H:%M:%S.%f\")}'\"\"\"",
            "",
            "    def get_time_filter(  # pylint: disable=too-many-arguments",
            "        self,",
            "        time_col: \"TableColumn\",",
            "        start_dttm: Optional[sa.DateTime],",
            "        end_dttm: Optional[sa.DateTime],",
            "        time_grain: Optional[str] = None,",
            "        label: Optional[str] = \"__time\",",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        col = (",
            "            time_col.get_timestamp_expression(",
            "                time_grain=time_grain,",
            "                label=label,",
            "                template_processor=template_processor,",
            "            )",
            "            if time_grain",
            "            else self.convert_tbl_column_to_sqla_col(",
            "                time_col, label=label, template_processor=template_processor",
            "            )",
            "        )",
            "",
            "        l = []",
            "        if start_dttm:",
            "            l.append(",
            "                col",
            "                >= self.db_engine_spec.get_text_clause(",
            "                    self.dttm_sql_literal(start_dttm, time_col)",
            "                )",
            "            )",
            "        if end_dttm:",
            "            l.append(",
            "                col",
            "                < self.db_engine_spec.get_text_clause(",
            "                    self.dttm_sql_literal(end_dttm, time_col)",
            "                )",
            "            )",
            "        return and_(*l)",
            "",
            "    def values_for_column(",
            "        self,",
            "        column_name: str,",
            "        limit: int = 10000,",
            "        denormalize_column: bool = False,",
            "    ) -> list[Any]:",
            "        # denormalize column name before querying for values",
            "        # unless disabled in the dataset configuration",
            "        db_dialect = self.database.get_dialect()",
            "        column_name_ = (",
            "            self.database.db_engine_spec.denormalize_name(db_dialect, column_name)",
            "            if denormalize_column",
            "            else column_name",
            "        )",
            "        cols = {col.column_name: col for col in self.columns}",
            "        target_col = cols[column_name_]",
            "        tp = self.get_template_processor()",
            "        tbl, cte = self.get_from_clause(tp)",
            "",
            "        qry = (",
            "            sa.select(",
            "                # The alias (label) here is important because some dialects will",
            "                # automatically add a random alias to the projection because of the",
            "                # call to DISTINCT; others will uppercase the column names. This",
            "                # gives us a deterministic column name in the dataframe.",
            "                [target_col.get_sqla_col(template_processor=tp).label(\"column_values\")]",
            "            )",
            "            .select_from(tbl)",
            "            .distinct()",
            "        )",
            "        if limit:",
            "            qry = qry.limit(limit)",
            "",
            "        if self.fetch_values_predicate:",
            "            qry = qry.where(self.get_fetch_values_predicate(template_processor=tp))",
            "",
            "        with self.database.get_sqla_engine_with_context() as engine:",
            "            sql = qry.compile(engine, compile_kwargs={\"literal_binds\": True})",
            "            sql = self._apply_cte(sql, cte)",
            "            sql = self.mutate_query_from_config(sql)",
            "",
            "            df = pd.read_sql_query(sql=sql, con=engine)",
            "            # replace NaN with None to ensure it can be serialized to JSON",
            "            df = df.replace({np.nan: None})",
            "            return df[\"column_values\"].to_list()",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        column: dict[str, Any],",
            "        time_grain: Optional[str],",
            "        label: Optional[str] = None,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Union[TimestampExpression, Label]:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param column: column object",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "        column_spec = self.db_engine_spec.get_column_spec(column.get(\"type\"))",
            "        type_ = column_spec.sqla_type if column_spec else sa.DateTime",
            "        col = sa.column(column.get(\"column_name\"), type_=type_)",
            "",
            "        if template_processor:",
            "            expression = template_processor.process_template(column[\"column_name\"])",
            "            col = sa.literal_column(expression, type_=type_)",
            "",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, None, time_grain)",
            "        return self.make_sqla_column_compatible(time_expr, label)",
            "",
            "    def convert_tbl_column_to_sqla_col(",
            "        self,",
            "        tbl_column: \"TableColumn\",",
            "        label: Optional[str] = None,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Column:",
            "        label = label or tbl_column.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := tbl_column.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = sa.column(tbl_column.column_name, type_=type_)",
            "        col = self.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    def get_sqla_query(  # pylint: disable=too-many-arguments,too-many-locals,too-many-branches,too-many-statements",
            "        self,",
            "        apply_fetch_values_predicate: bool = False,",
            "        columns: Optional[list[Column]] = None,",
            "        extras: Optional[dict[str, Any]] = None,",
            "        filter: Optional[  # pylint: disable=redefined-builtin",
            "            list[utils.QueryObjectFilterClause]",
            "        ] = None,",
            "        from_dttm: Optional[datetime] = None,",
            "        granularity: Optional[str] = None,",
            "        groupby: Optional[list[Column]] = None,",
            "        inner_from_dttm: Optional[datetime] = None,",
            "        inner_to_dttm: Optional[datetime] = None,",
            "        is_rowcount: bool = False,",
            "        is_timeseries: bool = True,",
            "        metrics: Optional[list[Metric]] = None,",
            "        orderby: Optional[list[OrderBy]] = None,",
            "        order_desc: bool = True,",
            "        to_dttm: Optional[datetime] = None,",
            "        series_columns: Optional[list[Column]] = None,",
            "        series_limit: Optional[int] = None,",
            "        series_limit_metric: Optional[Metric] = None,",
            "        row_limit: Optional[int] = None,",
            "        row_offset: Optional[int] = None,",
            "        timeseries_limit: Optional[int] = None,",
            "        timeseries_limit_metric: Optional[Metric] = None,",
            "        time_shift: Optional[str] = None,",
            "    ) -> SqlaQuery:",
            "        \"\"\"Querying any sqla table from this common interface\"\"\"",
            "        if granularity not in self.dttm_cols and granularity is not None:",
            "            granularity = self.main_dttm_col",
            "",
            "        extras = extras or {}",
            "        time_grain = extras.get(\"time_grain_sqla\")",
            "",
            "        template_kwargs = {",
            "            \"columns\": columns,",
            "            \"from_dttm\": from_dttm.isoformat() if from_dttm else None,",
            "            \"groupby\": groupby,",
            "            \"metrics\": metrics,",
            "            \"row_limit\": row_limit,",
            "            \"row_offset\": row_offset,",
            "            \"time_column\": granularity,",
            "            \"time_grain\": time_grain,",
            "            \"to_dttm\": to_dttm.isoformat() if to_dttm else None,",
            "            \"table_columns\": [col.column_name for col in self.columns],",
            "            \"filter\": filter,",
            "        }",
            "        columns = columns or []",
            "        groupby = groupby or []",
            "        rejected_adhoc_filters_columns: list[Union[str, ColumnTyping]] = []",
            "        applied_adhoc_filters_columns: list[Union[str, ColumnTyping]] = []",
            "        db_engine_spec = self.db_engine_spec",
            "        series_column_labels = [",
            "            db_engine_spec.make_label_compatible(column)",
            "            for column in utils.get_column_names(",
            "                columns=series_columns or [],",
            "            )",
            "        ]",
            "        # deprecated, to be removed in 2.0",
            "        if is_timeseries and timeseries_limit:",
            "            series_limit = timeseries_limit",
            "        series_limit_metric = series_limit_metric or timeseries_limit_metric",
            "        template_kwargs.update(self.template_params_dict)",
            "        extra_cache_keys: list[Any] = []",
            "        template_kwargs[\"extra_cache_keys\"] = extra_cache_keys",
            "        removed_filters: list[str] = []",
            "        applied_template_filters: list[str] = []",
            "        template_kwargs[\"removed_filters\"] = removed_filters",
            "        template_kwargs[\"applied_filters\"] = applied_template_filters",
            "        template_processor = self.get_template_processor(**template_kwargs)",
            "        prequeries: list[str] = []",
            "        orderby = orderby or []",
            "        need_groupby = bool(metrics is not None or groupby)",
            "        metrics = metrics or []",
            "",
            "        # For backward compatibility",
            "        if granularity not in self.dttm_cols and granularity is not None:",
            "            granularity = self.main_dttm_col",
            "",
            "        columns_by_name: dict[str, \"TableColumn\"] = {",
            "            col.column_name: col for col in self.columns",
            "        }",
            "",
            "        metrics_by_name: dict[str, \"SqlMetric\"] = {",
            "            m.metric_name: m for m in self.metrics",
            "        }",
            "",
            "        if not granularity and is_timeseries:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Datetime column not provided as part table configuration \"",
            "                    \"and is required by this type of chart\"",
            "                )",
            "            )",
            "        if not metrics and not columns and not groupby:",
            "            raise QueryObjectValidationError(_(\"Empty query?\"))",
            "",
            "        metrics_exprs: list[ColumnElement] = []",
            "        for metric in metrics:",
            "            if utils.is_adhoc_metric(metric):",
            "                assert isinstance(metric, dict)",
            "                metrics_exprs.append(",
            "                    self.adhoc_metric_to_sqla(",
            "                        metric=metric,",
            "                        columns_by_name=columns_by_name,",
            "                        template_processor=template_processor,",
            "                    )",
            "                )",
            "            elif isinstance(metric, str) and metric in metrics_by_name:",
            "                metrics_exprs.append(",
            "                    metrics_by_name[metric].get_sqla_col(",
            "                        template_processor=template_processor",
            "                    )",
            "                )",
            "            else:",
            "                raise QueryObjectValidationError(",
            "                    _(\"Metric '%(metric)s' does not exist\", metric=metric)",
            "                )",
            "",
            "        if metrics_exprs:",
            "            main_metric_expr = metrics_exprs[0]",
            "        else:",
            "            main_metric_expr, label = literal_column(\"COUNT(*)\"), \"ccount\"",
            "            main_metric_expr = self.make_sqla_column_compatible(main_metric_expr, label)",
            "",
            "        # To ensure correct handling of the ORDER BY labeling we need to reference the",
            "        # metric instance if defined in the SELECT clause.",
            "        # use the key of the ColumnClause for the expected label",
            "        metrics_exprs_by_label = {m.key: m for m in metrics_exprs}",
            "        metrics_exprs_by_expr = {str(m): m for m in metrics_exprs}",
            "",
            "        # Since orderby may use adhoc metrics, too; we need to process them first",
            "        orderby_exprs: list[ColumnElement] = []",
            "        for orig_col, ascending in orderby:",
            "            col: Union[AdhocMetric, ColumnElement] = orig_col",
            "            if isinstance(col, dict):",
            "                col = cast(AdhocMetric, col)",
            "                if col.get(\"sqlExpression\"):",
            "                    col[\"sqlExpression\"] = self._process_sql_expression(",
            "                        expression=col[\"sqlExpression\"],",
            "                        database_id=self.database_id,",
            "                        schema=self.schema,",
            "                        template_processor=template_processor,",
            "                    )",
            "                if utils.is_adhoc_metric(col):",
            "                    # add adhoc sort by column to columns_by_name if not exists",
            "                    col = self.adhoc_metric_to_sqla(col, columns_by_name)",
            "                    # if the adhoc metric has been defined before",
            "                    # use the existing instance.",
            "                    col = metrics_exprs_by_expr.get(str(col), col)",
            "                    need_groupby = True",
            "            elif col in columns_by_name:",
            "                col = self.convert_tbl_column_to_sqla_col(",
            "                    columns_by_name[col], template_processor=template_processor",
            "                )",
            "            elif col in metrics_exprs_by_label:",
            "                col = metrics_exprs_by_label[col]",
            "                need_groupby = True",
            "            elif col in metrics_by_name:",
            "                col = metrics_by_name[col].get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "                need_groupby = True",
            "",
            "            if isinstance(col, ColumnElement):",
            "                orderby_exprs.append(col)",
            "            else:",
            "                # Could not convert a column reference to valid ColumnElement",
            "                raise QueryObjectValidationError(",
            "                    _(\"Unknown column used in orderby: %(col)s\", col=orig_col)",
            "                )",
            "",
            "        select_exprs: list[Union[Column, Label]] = []",
            "        groupby_all_columns = {}",
            "        groupby_series_columns = {}",
            "",
            "        # filter out the pseudo column  __timestamp from columns",
            "        columns = [col for col in columns if col != utils.DTTM_ALIAS]",
            "        dttm_col = columns_by_name.get(granularity) if granularity else None",
            "",
            "        if need_groupby:",
            "            # dedup columns while preserving order",
            "            columns = groupby or columns",
            "            for selected in columns:",
            "                if isinstance(selected, str):",
            "                    # if groupby field/expr equals granularity field/expr",
            "                    if selected == granularity:",
            "                        table_col = columns_by_name[selected]",
            "                        outer = table_col.get_timestamp_expression(",
            "                            time_grain=time_grain,",
            "                            label=selected,",
            "                            template_processor=template_processor,",
            "                        )",
            "                    # if groupby field equals a selected column",
            "                    elif selected in columns_by_name:",
            "                        outer = self.convert_tbl_column_to_sqla_col(",
            "                            columns_by_name[selected],",
            "                            template_processor=template_processor,",
            "                        )",
            "                    else:",
            "                        selected = validate_adhoc_subquery(",
            "                            selected,",
            "                            self.database_id,",
            "                            self.schema,",
            "                        )",
            "                        outer = literal_column(f\"({selected})\")",
            "                        outer = self.make_sqla_column_compatible(outer, selected)",
            "                else:",
            "                    outer = self.adhoc_column_to_sqla(",
            "                        col=selected, template_processor=template_processor",
            "                    )",
            "                groupby_all_columns[outer.name] = outer",
            "                if (",
            "                    is_timeseries and not series_column_labels",
            "                ) or outer.name in series_column_labels:",
            "                    groupby_series_columns[outer.name] = outer",
            "                select_exprs.append(outer)",
            "        elif columns:",
            "            for selected in columns:",
            "                if is_adhoc_column(selected):",
            "                    _sql = selected[\"sqlExpression\"]",
            "                    _column_label = selected[\"label\"]",
            "                elif isinstance(selected, str):",
            "                    _sql = selected",
            "                    _column_label = selected",
            "",
            "                selected = validate_adhoc_subquery(",
            "                    _sql,",
            "                    self.database_id,",
            "                    self.schema,",
            "                )",
            "",
            "                select_exprs.append(",
            "                    self.convert_tbl_column_to_sqla_col(",
            "                        columns_by_name[selected], template_processor=template_processor",
            "                    )",
            "                    if isinstance(selected, str) and selected in columns_by_name",
            "                    else self.make_sqla_column_compatible(",
            "                        literal_column(selected), _column_label",
            "                    )",
            "                )",
            "            metrics_exprs = []",
            "",
            "        if granularity:",
            "            if granularity not in columns_by_name or not dttm_col:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        'Time column \"%(col)s\" does not exist in dataset',",
            "                        col=granularity,",
            "                    )",
            "                )",
            "            time_filters = []",
            "",
            "            if is_timeseries:",
            "                timestamp = dttm_col.get_timestamp_expression(",
            "                    time_grain=time_grain, template_processor=template_processor",
            "                )",
            "                # always put timestamp as the first column",
            "                select_exprs.insert(0, timestamp)",
            "                groupby_all_columns[timestamp.name] = timestamp",
            "",
            "            # Use main dttm column to support index with secondary dttm columns.",
            "            if (",
            "                self.always_filter_main_dttm",
            "                and self.main_dttm_col in self.dttm_cols",
            "                and self.main_dttm_col != dttm_col.column_name",
            "            ):",
            "                time_filters.append(",
            "                    self.get_time_filter(",
            "                        time_col=columns_by_name[self.main_dttm_col],",
            "                        start_dttm=from_dttm,",
            "                        end_dttm=to_dttm,",
            "                        template_processor=template_processor,",
            "                    )",
            "                )",
            "",
            "            time_filter_column = self.get_time_filter(",
            "                time_col=dttm_col,",
            "                start_dttm=from_dttm,",
            "                end_dttm=to_dttm,",
            "                template_processor=template_processor,",
            "            )",
            "            time_filters.append(time_filter_column)",
            "",
            "        # Always remove duplicates by column name, as sometimes `metrics_exprs`",
            "        # can have the same name as a groupby column (e.g. when users use",
            "        # raw columns as custom SQL adhoc metric).",
            "        select_exprs = remove_duplicates(",
            "            select_exprs + metrics_exprs, key=lambda x: x.name",
            "        )",
            "",
            "        # Expected output columns",
            "        labels_expected = [c.key for c in select_exprs]",
            "",
            "        # Order by columns are \"hidden\" columns, some databases require them",
            "        # always be present in SELECT if an aggregation function is used",
            "        if not db_engine_spec.allows_hidden_orderby_agg:",
            "            select_exprs = remove_duplicates(select_exprs + orderby_exprs)",
            "",
            "        qry = sa.select(select_exprs)",
            "",
            "        tbl, cte = self.get_from_clause(template_processor)",
            "",
            "        if groupby_all_columns:",
            "            qry = qry.group_by(*groupby_all_columns.values())",
            "",
            "        where_clause_and = []",
            "        having_clause_and = []",
            "",
            "        for flt in filter:  # type: ignore",
            "            if not all(flt.get(s) for s in [\"col\", \"op\"]):",
            "                continue",
            "            flt_col = flt[\"col\"]",
            "            val = flt.get(\"val\")",
            "            flt_grain = flt.get(\"grain\")",
            "            op = flt[\"op\"].upper()",
            "            col_obj: Optional[\"TableColumn\"] = None",
            "            sqla_col: Optional[Column] = None",
            "            if flt_col == utils.DTTM_ALIAS and is_timeseries and dttm_col:",
            "                col_obj = dttm_col",
            "            elif is_adhoc_column(flt_col):",
            "                try:",
            "                    sqla_col = self.adhoc_column_to_sqla(flt_col, force_type_check=True)",
            "                    applied_adhoc_filters_columns.append(flt_col)",
            "                except ColumnNotFoundException:",
            "                    rejected_adhoc_filters_columns.append(flt_col)",
            "                    continue",
            "            else:",
            "                col_obj = columns_by_name.get(cast(str, flt_col))",
            "            filter_grain = flt.get(\"grain\")",
            "",
            "            if get_column_name(flt_col) in removed_filters:",
            "                # Skip generating SQLA filter when the jinja template handles it.",
            "                continue",
            "",
            "            if col_obj or sqla_col is not None:",
            "                if sqla_col is not None:",
            "                    pass",
            "                elif col_obj and filter_grain:",
            "                    sqla_col = col_obj.get_timestamp_expression(",
            "                        time_grain=filter_grain, template_processor=template_processor",
            "                    )",
            "                elif col_obj:",
            "                    sqla_col = self.convert_tbl_column_to_sqla_col(",
            "                        tbl_column=col_obj, template_processor=template_processor",
            "                    )",
            "                col_type = col_obj.type if col_obj else None",
            "                col_spec = db_engine_spec.get_column_spec(native_type=col_type)",
            "                is_list_target = op in (",
            "                    utils.FilterOperator.IN.value,",
            "                    utils.FilterOperator.NOT_IN.value,",
            "                )",
            "",
            "                col_advanced_data_type = col_obj.advanced_data_type if col_obj else \"\"",
            "",
            "                if col_spec and not col_advanced_data_type:",
            "                    target_generic_type = col_spec.generic_type",
            "                else:",
            "                    target_generic_type = GenericDataType.STRING",
            "                eq = self.filter_values_handler(",
            "                    values=val,",
            "                    operator=op,",
            "                    target_generic_type=target_generic_type,",
            "                    target_native_type=col_type,",
            "                    is_list_target=is_list_target,",
            "                    db_engine_spec=db_engine_spec,",
            "                )",
            "                if (",
            "                    col_advanced_data_type != \"\"",
            "                    and feature_flag_manager.is_feature_enabled(",
            "                        \"ENABLE_ADVANCED_DATA_TYPES\"",
            "                    )",
            "                    and col_advanced_data_type in ADVANCED_DATA_TYPES",
            "                ):",
            "                    values = eq if is_list_target else [eq]  # type: ignore",
            "                    bus_resp: AdvancedDataTypeResponse = ADVANCED_DATA_TYPES[",
            "                        col_advanced_data_type",
            "                    ].translate_type(",
            "                        {",
            "                            \"type\": col_advanced_data_type,",
            "                            \"values\": values,",
            "                        }",
            "                    )",
            "                    if bus_resp[\"error_message\"]:",
            "                        raise AdvancedDataTypeResponseError(",
            "                            _(bus_resp[\"error_message\"])",
            "                        )",
            "",
            "                    where_clause_and.append(",
            "                        ADVANCED_DATA_TYPES[col_advanced_data_type].translate_filter(",
            "                            sqla_col, op, bus_resp[\"values\"]",
            "                        )",
            "                    )",
            "                elif is_list_target:",
            "                    assert isinstance(eq, (tuple, list))",
            "                    if len(eq) == 0:",
            "                        raise QueryObjectValidationError(",
            "                            _(\"Filter value list cannot be empty\")",
            "                        )",
            "                    if len(eq) > len(",
            "                        eq_without_none := [x for x in eq if x is not None]",
            "                    ):",
            "                        is_null_cond = sqla_col.is_(None)",
            "                        if eq:",
            "                            cond = or_(is_null_cond, sqla_col.in_(eq_without_none))",
            "                        else:",
            "                            cond = is_null_cond",
            "                    else:",
            "                        cond = sqla_col.in_(eq)",
            "                    if op == utils.FilterOperator.NOT_IN.value:",
            "                        cond = ~cond",
            "                    where_clause_and.append(cond)",
            "                elif op == utils.FilterOperator.IS_NULL.value:",
            "                    where_clause_and.append(sqla_col.is_(None))",
            "                elif op == utils.FilterOperator.IS_NOT_NULL.value:",
            "                    where_clause_and.append(sqla_col.isnot(None))",
            "                elif op == utils.FilterOperator.IS_TRUE.value:",
            "                    where_clause_and.append(sqla_col.is_(True))",
            "                elif op == utils.FilterOperator.IS_FALSE.value:",
            "                    where_clause_and.append(sqla_col.is_(False))",
            "                else:",
            "                    if (",
            "                        op",
            "                        not in {",
            "                            utils.FilterOperator.EQUALS.value,",
            "                            utils.FilterOperator.NOT_EQUALS.value,",
            "                        }",
            "                        and eq is None",
            "                    ):",
            "                        raise QueryObjectValidationError(",
            "                            _(",
            "                                \"Must specify a value for filters \"",
            "                                \"with comparison operators\"",
            "                            )",
            "                        )",
            "                    if op == utils.FilterOperator.EQUALS.value:",
            "                        where_clause_and.append(sqla_col == eq)",
            "                    elif op == utils.FilterOperator.NOT_EQUALS.value:",
            "                        where_clause_and.append(sqla_col != eq)",
            "                    elif op == utils.FilterOperator.GREATER_THAN.value:",
            "                        where_clause_and.append(sqla_col > eq)",
            "                    elif op == utils.FilterOperator.LESS_THAN.value:",
            "                        where_clause_and.append(sqla_col < eq)",
            "                    elif op == utils.FilterOperator.GREATER_THAN_OR_EQUALS.value:",
            "                        where_clause_and.append(sqla_col >= eq)",
            "                    elif op == utils.FilterOperator.LESS_THAN_OR_EQUALS.value:",
            "                        where_clause_and.append(sqla_col <= eq)",
            "                    elif op in {",
            "                        utils.FilterOperator.ILIKE.value,",
            "                        utils.FilterOperator.LIKE.value,",
            "                    }:",
            "                        if target_generic_type != GenericDataType.STRING:",
            "                            sqla_col = sa.cast(sqla_col, sa.String)",
            "",
            "                        if op == utils.FilterOperator.LIKE.value:",
            "                            where_clause_and.append(sqla_col.like(eq))",
            "                        else:",
            "                            where_clause_and.append(sqla_col.ilike(eq))",
            "                    elif (",
            "                        op == utils.FilterOperator.TEMPORAL_RANGE.value",
            "                        and isinstance(eq, str)",
            "                        and col_obj is not None",
            "                    ):",
            "                        _since, _until = get_since_until_from_time_range(",
            "                            time_range=eq,",
            "                            time_shift=time_shift,",
            "                            extras=extras,",
            "                        )",
            "                        where_clause_and.append(",
            "                            self.get_time_filter(",
            "                                time_col=col_obj,",
            "                                start_dttm=_since,",
            "                                end_dttm=_until,",
            "                                time_grain=flt_grain,",
            "                                label=sqla_col.key,",
            "                                template_processor=template_processor,",
            "                            )",
            "                        )",
            "                    else:",
            "                        raise QueryObjectValidationError(",
            "                            _(\"Invalid filter operation type: %(op)s\", op=op)",
            "                        )",
            "        where_clause_and += self.get_sqla_row_level_filters(template_processor)",
            "        if extras:",
            "            where = extras.get(\"where\")",
            "            if where:",
            "                try:",
            "                    where = template_processor.process_template(f\"({where})\")",
            "                except TemplateError as ex:",
            "                    raise QueryObjectValidationError(",
            "                        _(",
            "                            \"Error in jinja expression in WHERE clause: %(msg)s\",",
            "                            msg=ex.message,",
            "                        )",
            "                    ) from ex",
            "                where = self._process_sql_expression(",
            "                    expression=where,",
            "                    database_id=self.database_id,",
            "                    schema=self.schema,",
            "                    template_processor=template_processor,",
            "                )",
            "                where_clause_and += [self.text(where)]",
            "            having = extras.get(\"having\")",
            "            if having:",
            "                try:",
            "                    having = template_processor.process_template(f\"({having})\")",
            "                except TemplateError as ex:",
            "                    raise QueryObjectValidationError(",
            "                        _(",
            "                            \"Error in jinja expression in HAVING clause: %(msg)s\",",
            "                            msg=ex.message,",
            "                        )",
            "                    ) from ex",
            "                having = self._process_sql_expression(",
            "                    expression=having,",
            "                    database_id=self.database_id,",
            "                    schema=self.schema,",
            "                    template_processor=template_processor,",
            "                )",
            "                having_clause_and += [self.text(having)]",
            "",
            "        if apply_fetch_values_predicate and self.fetch_values_predicate:",
            "            qry = qry.where(",
            "                self.get_fetch_values_predicate(template_processor=template_processor)",
            "            )",
            "        if granularity:",
            "            qry = qry.where(and_(*(time_filters + where_clause_and)))",
            "        else:",
            "            qry = qry.where(and_(*where_clause_and))",
            "        qry = qry.having(and_(*having_clause_and))",
            "",
            "        self.make_orderby_compatible(select_exprs, orderby_exprs)",
            "",
            "        for col, (orig_col, ascending) in zip(orderby_exprs, orderby):",
            "            if not db_engine_spec.allows_alias_in_orderby and isinstance(col, Label):",
            "                # if engine does not allow using SELECT alias in ORDER BY",
            "                # revert to the underlying column",
            "                col = col.element",
            "",
            "            if (",
            "                db_engine_spec.get_allows_alias_in_select(self.database)",
            "                and db_engine_spec.allows_hidden_cc_in_orderby",
            "                and col.name in [select_col.name for select_col in select_exprs]",
            "            ):",
            "                with self.database.get_sqla_engine_with_context() as engine:",
            "                    quote = engine.dialect.identifier_preparer.quote",
            "                    col = literal_column(quote(col.name))",
            "            direction = sa.asc if ascending else sa.desc",
            "            qry = qry.order_by(direction(col))",
            "",
            "        if row_limit:",
            "            qry = qry.limit(row_limit)",
            "        if row_offset:",
            "            qry = qry.offset(row_offset)",
            "",
            "        if series_limit and groupby_series_columns:",
            "            if db_engine_spec.allows_joins and db_engine_spec.allows_subqueries:",
            "                # some sql dialects require for order by expressions",
            "                # to also be in the select clause -- others, e.g. vertica,",
            "                # require a unique inner alias",
            "                inner_main_metric_expr = self.make_sqla_column_compatible(",
            "                    main_metric_expr, \"mme_inner__\"",
            "                )",
            "                inner_groupby_exprs = []",
            "                inner_select_exprs = []",
            "                for gby_name, gby_obj in groupby_series_columns.items():",
            "                    inner = self.make_sqla_column_compatible(gby_obj, gby_name + \"__\")",
            "                    inner_groupby_exprs.append(inner)",
            "                    inner_select_exprs.append(inner)",
            "",
            "                inner_select_exprs += [inner_main_metric_expr]",
            "                subq = sa.select(inner_select_exprs).select_from(tbl)",
            "                inner_time_filter = []",
            "",
            "                if dttm_col and not db_engine_spec.time_groupby_inline:",
            "                    inner_time_filter = [",
            "                        self.get_time_filter(",
            "                            time_col=dttm_col,",
            "                            start_dttm=inner_from_dttm or from_dttm,",
            "                            end_dttm=inner_to_dttm or to_dttm,",
            "                            template_processor=template_processor,",
            "                        )",
            "                    ]",
            "                subq = subq.where(and_(*(where_clause_and + inner_time_filter)))",
            "                subq = subq.group_by(*inner_groupby_exprs)",
            "",
            "                ob = inner_main_metric_expr",
            "                if series_limit_metric:",
            "                    ob = self._get_series_orderby(",
            "                        series_limit_metric=series_limit_metric,",
            "                        metrics_by_name=metrics_by_name,",
            "                        columns_by_name=columns_by_name,",
            "                        template_processor=template_processor,",
            "                    )",
            "                direction = sa.desc if order_desc else sa.asc",
            "                subq = subq.order_by(direction(ob))",
            "                subq = subq.limit(series_limit)",
            "",
            "                on_clause = []",
            "                for gby_name, gby_obj in groupby_series_columns.items():",
            "                    # in this case the column name, not the alias, needs to be",
            "                    # conditionally mutated, as it refers to the column alias in",
            "                    # the inner query",
            "                    col_name = db_engine_spec.make_label_compatible(gby_name + \"__\")",
            "                    on_clause.append(gby_obj == sa.column(col_name))",
            "",
            "                tbl = tbl.join(subq.alias(SERIES_LIMIT_SUBQ_ALIAS), and_(*on_clause))",
            "            else:",
            "                if series_limit_metric:",
            "                    orderby = [",
            "                        (",
            "                            self._get_series_orderby(",
            "                                series_limit_metric=series_limit_metric,",
            "                                metrics_by_name=metrics_by_name,",
            "                                columns_by_name=columns_by_name,",
            "                                template_processor=template_processor,",
            "                            ),",
            "                            not order_desc,",
            "                        )",
            "                    ]",
            "",
            "                # run prequery to get top groups",
            "                prequery_obj = {",
            "                    \"is_timeseries\": False,",
            "                    \"row_limit\": series_limit,",
            "                    \"metrics\": metrics,",
            "                    \"granularity\": granularity,",
            "                    \"groupby\": groupby,",
            "                    \"from_dttm\": inner_from_dttm or from_dttm,",
            "                    \"to_dttm\": inner_to_dttm or to_dttm,",
            "                    \"filter\": filter,",
            "                    \"orderby\": orderby,",
            "                    \"extras\": extras,",
            "                    \"columns\": columns,",
            "                    \"order_desc\": True,",
            "                }",
            "",
            "                result = self.query(prequery_obj)",
            "                prequeries.append(result.query)",
            "                dimensions = [",
            "                    c",
            "                    for c in result.df.columns",
            "                    if c not in metrics and c in groupby_series_columns",
            "                ]",
            "                top_groups = self._get_top_groups(",
            "                    result.df, dimensions, groupby_series_columns, columns_by_name",
            "                )",
            "                qry = qry.where(top_groups)",
            "",
            "        qry = qry.select_from(tbl)",
            "",
            "        if is_rowcount:",
            "            if not db_engine_spec.allows_subqueries:",
            "                raise QueryObjectValidationError(",
            "                    _(\"Database does not support subqueries\")",
            "                )",
            "            label = \"rowcount\"",
            "            col = self.make_sqla_column_compatible(literal_column(\"COUNT(*)\"), label)",
            "            qry = sa.select([col]).select_from(qry.alias(\"rowcount_qry\"))",
            "            labels_expected = [label]",
            "",
            "        filter_columns = [flt.get(\"col\") for flt in filter] if filter else []",
            "        rejected_filter_columns = [",
            "            col",
            "            for col in filter_columns",
            "            if col",
            "            and not is_adhoc_column(col)",
            "            and col not in self.column_names",
            "            and col not in applied_template_filters",
            "        ] + rejected_adhoc_filters_columns",
            "",
            "        applied_filter_columns = [",
            "            col",
            "            for col in filter_columns",
            "            if col",
            "            and not is_adhoc_column(col)",
            "            and (col in self.column_names or col in applied_template_filters)",
            "        ] + applied_adhoc_filters_columns",
            "",
            "        return SqlaQuery(",
            "            applied_template_filters=applied_template_filters,",
            "            cte=cte,",
            "            applied_filter_columns=applied_filter_columns,",
            "            rejected_filter_columns=rejected_filter_columns,",
            "            extra_cache_keys=extra_cache_keys,",
            "            labels_expected=labels_expected,",
            "            sqla_query=qry,",
            "            prequeries=prequeries,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "904": [
                "ExploreMixin",
                "get_query_str_extended"
            ],
            "909": [
                "ExploreMixin",
                "get_query_str_extended"
            ],
            "1057": [
                "ExploreMixin",
                "get_rendered_sql"
            ],
            "1074": [
                "ExploreMixin",
                "get_rendered_sql"
            ],
            "1075": [
                "ExploreMixin",
                "get_rendered_sql"
            ],
            "1076": [
                "ExploreMixin",
                "get_rendered_sql"
            ],
            "1077": [
                "ExploreMixin",
                "get_rendered_sql"
            ]
        },
        "addLocation": []
    }
}