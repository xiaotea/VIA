{
    "superset/commands/importers/v1/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from superset.commands.importers.exceptions import IncorrectVersionError"
            },
            "1": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from superset.databases.ssh_tunnel.models import SSHTunnel"
            },
            "2": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from superset.models.core import Database"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+from superset.utils.core import check_is_safe_zip"
            },
            "4": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " METADATA_FILE_NAME = \"metadata.yaml\""
            },
            "6": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " IMPORT_VERSION = \"1.0.0\""
            },
            "7": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 208,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 209,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 210,
                "PatchRowcode": " def get_contents_from_bundle(bundle: ZipFile) -> dict[str, str]:"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+    check_is_safe_zip(bundle)"
            },
            "11": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 212,
                "PatchRowcode": "     return {"
            },
            "12": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "         remove_root(file_name): bundle.read(file_name).decode()"
            },
            "13": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "         for file_name in bundle.namelist()"
            }
        },
        "frontPatchFile": [
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import logging",
            "from pathlib import Path, PurePosixPath",
            "from typing import Any, Optional",
            "from zipfile import ZipFile",
            "",
            "import yaml",
            "from marshmallow import fields, Schema, validate",
            "from marshmallow.exceptions import ValidationError",
            "",
            "from superset import db",
            "from superset.commands.importers.exceptions import IncorrectVersionError",
            "from superset.databases.ssh_tunnel.models import SSHTunnel",
            "from superset.models.core import Database",
            "",
            "METADATA_FILE_NAME = \"metadata.yaml\"",
            "IMPORT_VERSION = \"1.0.0\"",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def remove_root(file_path: str) -> str:",
            "    \"\"\"Remove the first directory of a path\"\"\"",
            "    full_path = PurePosixPath(file_path)",
            "    relative_path = PurePosixPath(*full_path.parts[1:])",
            "    return str(relative_path)",
            "",
            "",
            "class MetadataSchema(Schema):",
            "    version = fields.String(required=True, validate=validate.Equal(IMPORT_VERSION))",
            "    type = fields.String(required=False)",
            "    timestamp = fields.DateTime()",
            "",
            "",
            "def load_yaml(file_name: str, content: str) -> dict[str, Any]:",
            "    \"\"\"Try to load a YAML file\"\"\"",
            "    try:",
            "        return yaml.safe_load(content)",
            "    except yaml.parser.ParserError as ex:",
            "        logger.exception(\"Invalid YAML in %s\", file_name)",
            "        raise ValidationError({file_name: \"Not a valid YAML file\"}) from ex",
            "",
            "",
            "def load_metadata(contents: dict[str, str]) -> dict[str, str]:",
            "    \"\"\"Apply validation and load a metadata file\"\"\"",
            "    if METADATA_FILE_NAME not in contents:",
            "        # if the contents have no METADATA_FILE_NAME this is probably",
            "        # a original export without versioning that should not be",
            "        # handled by this command",
            "        raise IncorrectVersionError(f\"Missing {METADATA_FILE_NAME}\")",
            "",
            "    metadata = load_yaml(METADATA_FILE_NAME, contents[METADATA_FILE_NAME])",
            "    try:",
            "        MetadataSchema().load(metadata)",
            "    except ValidationError as ex:",
            "        # if the version doesn't match raise an exception so that the",
            "        # dispatcher can try a different command version",
            "        if \"version\" in ex.messages:",
            "            raise IncorrectVersionError(ex.messages[\"version\"][0]) from ex",
            "",
            "        # otherwise we raise the validation error",
            "        ex.messages = {METADATA_FILE_NAME: ex.messages}",
            "        raise ex",
            "",
            "    return metadata",
            "",
            "",
            "def validate_metadata_type(",
            "    metadata: Optional[dict[str, str]],",
            "    type_: str,",
            "    exceptions: list[ValidationError],",
            ") -> None:",
            "    \"\"\"Validate that the type declared in METADATA_FILE_NAME is correct\"\"\"",
            "    if metadata and \"type\" in metadata:",
            "        type_validator = validate.Equal(type_)",
            "        try:",
            "            type_validator(metadata[\"type\"])",
            "        except ValidationError as exc:",
            "            exc.messages = {METADATA_FILE_NAME: {\"type\": exc.messages}}",
            "            exceptions.append(exc)",
            "",
            "",
            "# pylint: disable=too-many-locals,too-many-arguments",
            "def load_configs(",
            "    contents: dict[str, str],",
            "    schemas: dict[str, Schema],",
            "    passwords: dict[str, str],",
            "    exceptions: list[ValidationError],",
            "    ssh_tunnel_passwords: dict[str, str],",
            "    ssh_tunnel_private_keys: dict[str, str],",
            "    ssh_tunnel_priv_key_passwords: dict[str, str],",
            ") -> dict[str, Any]:",
            "    configs: dict[str, Any] = {}",
            "",
            "    # load existing databases so we can apply the password validation",
            "    db_passwords: dict[str, str] = {",
            "        str(uuid): password",
            "        for uuid, password in db.session.query(Database.uuid, Database.password).all()",
            "    }",
            "    # load existing ssh_tunnels so we can apply the password validation",
            "    db_ssh_tunnel_passwords: dict[str, str] = {",
            "        str(uuid): password",
            "        for uuid, password in db.session.query(SSHTunnel.uuid, SSHTunnel.password).all()",
            "    }",
            "    # load existing ssh_tunnels so we can apply the private_key validation",
            "    db_ssh_tunnel_private_keys: dict[str, str] = {",
            "        str(uuid): private_key",
            "        for uuid, private_key in db.session.query(",
            "            SSHTunnel.uuid, SSHTunnel.private_key",
            "        ).all()",
            "    }",
            "    # load existing ssh_tunnels so we can apply the private_key_password validation",
            "    db_ssh_tunnel_priv_key_passws: dict[str, str] = {",
            "        str(uuid): private_key_password",
            "        for uuid, private_key_password in db.session.query(",
            "            SSHTunnel.uuid, SSHTunnel.private_key_password",
            "        ).all()",
            "    }",
            "    for file_name, content in contents.items():",
            "        # skip directories",
            "        if not content:",
            "            continue",
            "",
            "        prefix = file_name.split(\"/\")[0]",
            "        schema = schemas.get(f\"{prefix}/\")",
            "        if schema:",
            "            try:",
            "                config = load_yaml(file_name, content)",
            "",
            "                # populate passwords from the request or from existing DBs",
            "                if file_name in passwords:",
            "                    config[\"password\"] = passwords[file_name]",
            "                elif prefix == \"databases\" and config[\"uuid\"] in db_passwords:",
            "                    config[\"password\"] = db_passwords[config[\"uuid\"]]",
            "",
            "                # populate ssh_tunnel_passwords from the request or from existing DBs",
            "                if file_name in ssh_tunnel_passwords:",
            "                    config[\"ssh_tunnel\"][\"password\"] = ssh_tunnel_passwords[file_name]",
            "                elif (",
            "                    prefix == \"databases\" and config[\"uuid\"] in db_ssh_tunnel_passwords",
            "                ):",
            "                    config[\"ssh_tunnel\"][\"password\"] = db_ssh_tunnel_passwords[",
            "                        config[\"uuid\"]",
            "                    ]",
            "",
            "                # populate ssh_tunnel_private_keys from the request or from existing DBs",
            "                if file_name in ssh_tunnel_private_keys:",
            "                    config[\"ssh_tunnel\"][\"private_key\"] = ssh_tunnel_private_keys[",
            "                        file_name",
            "                    ]",
            "                elif (",
            "                    prefix == \"databases\"",
            "                    and config[\"uuid\"] in db_ssh_tunnel_private_keys",
            "                ):",
            "                    config[\"ssh_tunnel\"][\"private_key\"] = db_ssh_tunnel_private_keys[",
            "                        config[\"uuid\"]",
            "                    ]",
            "",
            "                # populate ssh_tunnel_passwords from the request or from existing DBs",
            "                if file_name in ssh_tunnel_priv_key_passwords:",
            "                    config[\"ssh_tunnel\"][",
            "                        \"private_key_password\"",
            "                    ] = ssh_tunnel_priv_key_passwords[file_name]",
            "                elif (",
            "                    prefix == \"databases\"",
            "                    and config[\"uuid\"] in db_ssh_tunnel_priv_key_passws",
            "                ):",
            "                    config[\"ssh_tunnel\"][",
            "                        \"private_key_password\"",
            "                    ] = db_ssh_tunnel_priv_key_passws[config[\"uuid\"]]",
            "",
            "                schema.load(config)",
            "                configs[file_name] = config",
            "            except ValidationError as exc:",
            "                exc.messages = {file_name: exc.messages}",
            "                exceptions.append(exc)",
            "",
            "    return configs",
            "",
            "",
            "def is_valid_config(file_name: str) -> bool:",
            "    path = Path(file_name)",
            "",
            "    # ignore system files that might've been added to the bundle",
            "    if path.name.startswith(\".\") or path.name.startswith(\"_\"):",
            "        return False",
            "",
            "    # ensure extension is YAML",
            "    if path.suffix.lower() not in {\".yaml\", \".yml\"}:",
            "        return False",
            "",
            "    return True",
            "",
            "",
            "def get_contents_from_bundle(bundle: ZipFile) -> dict[str, str]:",
            "    return {",
            "        remove_root(file_name): bundle.read(file_name).decode()",
            "        for file_name in bundle.namelist()",
            "        if is_valid_config(file_name)",
            "    }"
        ],
        "afterPatchFile": [
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import logging",
            "from pathlib import Path, PurePosixPath",
            "from typing import Any, Optional",
            "from zipfile import ZipFile",
            "",
            "import yaml",
            "from marshmallow import fields, Schema, validate",
            "from marshmallow.exceptions import ValidationError",
            "",
            "from superset import db",
            "from superset.commands.importers.exceptions import IncorrectVersionError",
            "from superset.databases.ssh_tunnel.models import SSHTunnel",
            "from superset.models.core import Database",
            "from superset.utils.core import check_is_safe_zip",
            "",
            "METADATA_FILE_NAME = \"metadata.yaml\"",
            "IMPORT_VERSION = \"1.0.0\"",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def remove_root(file_path: str) -> str:",
            "    \"\"\"Remove the first directory of a path\"\"\"",
            "    full_path = PurePosixPath(file_path)",
            "    relative_path = PurePosixPath(*full_path.parts[1:])",
            "    return str(relative_path)",
            "",
            "",
            "class MetadataSchema(Schema):",
            "    version = fields.String(required=True, validate=validate.Equal(IMPORT_VERSION))",
            "    type = fields.String(required=False)",
            "    timestamp = fields.DateTime()",
            "",
            "",
            "def load_yaml(file_name: str, content: str) -> dict[str, Any]:",
            "    \"\"\"Try to load a YAML file\"\"\"",
            "    try:",
            "        return yaml.safe_load(content)",
            "    except yaml.parser.ParserError as ex:",
            "        logger.exception(\"Invalid YAML in %s\", file_name)",
            "        raise ValidationError({file_name: \"Not a valid YAML file\"}) from ex",
            "",
            "",
            "def load_metadata(contents: dict[str, str]) -> dict[str, str]:",
            "    \"\"\"Apply validation and load a metadata file\"\"\"",
            "    if METADATA_FILE_NAME not in contents:",
            "        # if the contents have no METADATA_FILE_NAME this is probably",
            "        # a original export without versioning that should not be",
            "        # handled by this command",
            "        raise IncorrectVersionError(f\"Missing {METADATA_FILE_NAME}\")",
            "",
            "    metadata = load_yaml(METADATA_FILE_NAME, contents[METADATA_FILE_NAME])",
            "    try:",
            "        MetadataSchema().load(metadata)",
            "    except ValidationError as ex:",
            "        # if the version doesn't match raise an exception so that the",
            "        # dispatcher can try a different command version",
            "        if \"version\" in ex.messages:",
            "            raise IncorrectVersionError(ex.messages[\"version\"][0]) from ex",
            "",
            "        # otherwise we raise the validation error",
            "        ex.messages = {METADATA_FILE_NAME: ex.messages}",
            "        raise ex",
            "",
            "    return metadata",
            "",
            "",
            "def validate_metadata_type(",
            "    metadata: Optional[dict[str, str]],",
            "    type_: str,",
            "    exceptions: list[ValidationError],",
            ") -> None:",
            "    \"\"\"Validate that the type declared in METADATA_FILE_NAME is correct\"\"\"",
            "    if metadata and \"type\" in metadata:",
            "        type_validator = validate.Equal(type_)",
            "        try:",
            "            type_validator(metadata[\"type\"])",
            "        except ValidationError as exc:",
            "            exc.messages = {METADATA_FILE_NAME: {\"type\": exc.messages}}",
            "            exceptions.append(exc)",
            "",
            "",
            "# pylint: disable=too-many-locals,too-many-arguments",
            "def load_configs(",
            "    contents: dict[str, str],",
            "    schemas: dict[str, Schema],",
            "    passwords: dict[str, str],",
            "    exceptions: list[ValidationError],",
            "    ssh_tunnel_passwords: dict[str, str],",
            "    ssh_tunnel_private_keys: dict[str, str],",
            "    ssh_tunnel_priv_key_passwords: dict[str, str],",
            ") -> dict[str, Any]:",
            "    configs: dict[str, Any] = {}",
            "",
            "    # load existing databases so we can apply the password validation",
            "    db_passwords: dict[str, str] = {",
            "        str(uuid): password",
            "        for uuid, password in db.session.query(Database.uuid, Database.password).all()",
            "    }",
            "    # load existing ssh_tunnels so we can apply the password validation",
            "    db_ssh_tunnel_passwords: dict[str, str] = {",
            "        str(uuid): password",
            "        for uuid, password in db.session.query(SSHTunnel.uuid, SSHTunnel.password).all()",
            "    }",
            "    # load existing ssh_tunnels so we can apply the private_key validation",
            "    db_ssh_tunnel_private_keys: dict[str, str] = {",
            "        str(uuid): private_key",
            "        for uuid, private_key in db.session.query(",
            "            SSHTunnel.uuid, SSHTunnel.private_key",
            "        ).all()",
            "    }",
            "    # load existing ssh_tunnels so we can apply the private_key_password validation",
            "    db_ssh_tunnel_priv_key_passws: dict[str, str] = {",
            "        str(uuid): private_key_password",
            "        for uuid, private_key_password in db.session.query(",
            "            SSHTunnel.uuid, SSHTunnel.private_key_password",
            "        ).all()",
            "    }",
            "    for file_name, content in contents.items():",
            "        # skip directories",
            "        if not content:",
            "            continue",
            "",
            "        prefix = file_name.split(\"/\")[0]",
            "        schema = schemas.get(f\"{prefix}/\")",
            "        if schema:",
            "            try:",
            "                config = load_yaml(file_name, content)",
            "",
            "                # populate passwords from the request or from existing DBs",
            "                if file_name in passwords:",
            "                    config[\"password\"] = passwords[file_name]",
            "                elif prefix == \"databases\" and config[\"uuid\"] in db_passwords:",
            "                    config[\"password\"] = db_passwords[config[\"uuid\"]]",
            "",
            "                # populate ssh_tunnel_passwords from the request or from existing DBs",
            "                if file_name in ssh_tunnel_passwords:",
            "                    config[\"ssh_tunnel\"][\"password\"] = ssh_tunnel_passwords[file_name]",
            "                elif (",
            "                    prefix == \"databases\" and config[\"uuid\"] in db_ssh_tunnel_passwords",
            "                ):",
            "                    config[\"ssh_tunnel\"][\"password\"] = db_ssh_tunnel_passwords[",
            "                        config[\"uuid\"]",
            "                    ]",
            "",
            "                # populate ssh_tunnel_private_keys from the request or from existing DBs",
            "                if file_name in ssh_tunnel_private_keys:",
            "                    config[\"ssh_tunnel\"][\"private_key\"] = ssh_tunnel_private_keys[",
            "                        file_name",
            "                    ]",
            "                elif (",
            "                    prefix == \"databases\"",
            "                    and config[\"uuid\"] in db_ssh_tunnel_private_keys",
            "                ):",
            "                    config[\"ssh_tunnel\"][\"private_key\"] = db_ssh_tunnel_private_keys[",
            "                        config[\"uuid\"]",
            "                    ]",
            "",
            "                # populate ssh_tunnel_passwords from the request or from existing DBs",
            "                if file_name in ssh_tunnel_priv_key_passwords:",
            "                    config[\"ssh_tunnel\"][",
            "                        \"private_key_password\"",
            "                    ] = ssh_tunnel_priv_key_passwords[file_name]",
            "                elif (",
            "                    prefix == \"databases\"",
            "                    and config[\"uuid\"] in db_ssh_tunnel_priv_key_passws",
            "                ):",
            "                    config[\"ssh_tunnel\"][",
            "                        \"private_key_password\"",
            "                    ] = db_ssh_tunnel_priv_key_passws[config[\"uuid\"]]",
            "",
            "                schema.load(config)",
            "                configs[file_name] = config",
            "            except ValidationError as exc:",
            "                exc.messages = {file_name: exc.messages}",
            "                exceptions.append(exc)",
            "",
            "    return configs",
            "",
            "",
            "def is_valid_config(file_name: str) -> bool:",
            "    path = Path(file_name)",
            "",
            "    # ignore system files that might've been added to the bundle",
            "    if path.name.startswith(\".\") or path.name.startswith(\"_\"):",
            "        return False",
            "",
            "    # ensure extension is YAML",
            "    if path.suffix.lower() not in {\".yaml\", \".yml\"}:",
            "        return False",
            "",
            "    return True",
            "",
            "",
            "def get_contents_from_bundle(bundle: ZipFile) -> dict[str, str]:",
            "    check_is_safe_zip(bundle)",
            "    return {",
            "        remove_root(file_name): bundle.read(file_name).decode()",
            "        for file_name in bundle.namelist()",
            "        if is_valid_config(file_name)",
            "    }"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.twisted.web.client.URI.fromBytes"
        ]
    },
    "superset/config.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1600,
                "afterPatchRowNumber": 1600,
                "PatchRowcode": "     Literal[\"examples\", \"all\"] | tuple[str, list[dict[str, Any]]]"
            },
            "1": {
                "beforePatchRowNumber": 1601,
                "afterPatchRowNumber": 1601,
                "PatchRowcode": " ) = \"all\""
            },
            "2": {
                "beforePatchRowNumber": 1602,
                "afterPatchRowNumber": 1602,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1603,
                "PatchRowcode": "+# Max allowed size for a zipped file"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1604,
                "PatchRowcode": "+ZIPPED_FILE_MAX_SIZE = 100 * 1024 * 1024  # 100MB"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1605,
                "PatchRowcode": "+# Max allowed compression ratio for a zipped file"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1606,
                "PatchRowcode": "+ZIP_FILE_MAX_COMPRESS_RATIO = 200.0"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1607,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": 1603,
                "afterPatchRowNumber": 1608,
                "PatchRowcode": " # Configuration for environment tag shown on the navbar. Setting 'text' to '' will hide the tag."
            },
            "9": {
                "beforePatchRowNumber": 1604,
                "afterPatchRowNumber": 1609,
                "PatchRowcode": " # 'color' can either be a hex color code, or a dot-indexed theme color (e.g. error.base)"
            },
            "10": {
                "beforePatchRowNumber": 1605,
                "afterPatchRowNumber": 1610,
                "PatchRowcode": " ENVIRONMENT_TAG_CONFIG = {"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"The main config file for Superset",
            "",
            "All configuration in this file can be overridden by providing a superset_config",
            "in your PYTHONPATH as there is a ``from superset_config import *``",
            "at the end of this file.",
            "\"\"\"",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import imp  # pylint: disable=deprecated-module",
            "import importlib.util",
            "import json",
            "import logging",
            "import os",
            "import re",
            "import sys",
            "from collections import OrderedDict",
            "from datetime import timedelta",
            "from email.mime.multipart import MIMEMultipart",
            "from importlib.resources import files",
            "from typing import Any, Callable, Literal, TYPE_CHECKING, TypedDict",
            "",
            "import pkg_resources",
            "from celery.schedules import crontab",
            "from flask import Blueprint",
            "from flask_appbuilder.security.manager import AUTH_DB",
            "from flask_caching.backends.base import BaseCache",
            "from pandas import Series",
            "from pandas._libs.parsers import STR_NA_VALUES  # pylint: disable=no-name-in-module",
            "from sqlalchemy.orm.query import Query",
            "",
            "from superset.advanced_data_type.plugins.internet_address import internet_address",
            "from superset.advanced_data_type.plugins.internet_port import internet_port",
            "from superset.advanced_data_type.types import AdvancedDataType",
            "from superset.constants import CHANGE_ME_SECRET_KEY",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.key_value.types import JsonKeyValueCodec",
            "from superset.stats_logger import DummyStatsLogger",
            "from superset.superset_typing import CacheConfig",
            "from superset.tasks.types import ExecutorType",
            "from superset.utils import core as utils",
            "from superset.utils.core import is_test, NO_TIME_RANGE, parse_boolean_string",
            "from superset.utils.encrypt import SQLAlchemyUtilsAdapter",
            "from superset.utils.log import DBEventLogger",
            "from superset.utils.logging_configurator import DefaultLoggingConfigurator",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from flask_appbuilder.security.sqla import models",
            "",
            "    from superset.connectors.sqla.models import SqlaTable",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.slice import Slice",
            "",
            "# Realtime stats logger, a StatsD implementation exists",
            "STATS_LOGGER = DummyStatsLogger()",
            "EVENT_LOGGER = DBEventLogger()",
            "",
            "SUPERSET_LOG_VIEW = True",
            "",
            "BASE_DIR = pkg_resources.resource_filename(\"superset\", \"\")",
            "if \"SUPERSET_HOME\" in os.environ:",
            "    DATA_DIR = os.environ[\"SUPERSET_HOME\"]",
            "else:",
            "    DATA_DIR = os.path.expanduser(\"~/.superset\")",
            "",
            "# ---------------------------------------------------------",
            "# Superset specific config",
            "# ---------------------------------------------------------",
            "VERSION_INFO_FILE = str(files(\"superset\") / \"static/version_info.json\")",
            "PACKAGE_JSON_FILE = str(files(\"superset\") / \"static/assets/package.json\")",
            "",
            "",
            "# Multiple favicons can be specified here. The \"href\" property",
            "# is mandatory, but \"sizes,\" \"type,\" and \"rel\" are optional.",
            "# For example:",
            "# {",
            "#     \"href\":path/to/image.png\",",
            "#     \"sizes\": \"16x16\",",
            "#     \"type\": \"image/png\"",
            "#     \"rel\": \"icon\"",
            "# },",
            "FAVICONS = [{\"href\": \"/static/assets/images/favicon.png\"}]",
            "",
            "",
            "def _try_json_readversion(filepath: str) -> str | None:",
            "    try:",
            "        with open(filepath) as f:",
            "            return json.load(f).get(\"version\")",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "def _try_json_readsha(filepath: str, length: int) -> str | None:",
            "    try:",
            "        with open(filepath) as f:",
            "            return json.load(f).get(\"GIT_SHA\")[:length]",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "#",
            "# If True, we will skip the call to load the logger config found in alembic.init",
            "#",
            "ALEMBIC_SKIP_LOG_CONFIG = False",
            "",
            "# Depending on the context in which this config is loaded, the",
            "# version_info.json file may or may not be available, as it is",
            "# generated on install via setup.py. In the event that we're",
            "# actually running Superset, we will have already installed,",
            "# therefore it WILL exist. When unit tests are running, however,",
            "# it WILL NOT exist, so we fall back to reading package.json",
            "VERSION_STRING = _try_json_readversion(VERSION_INFO_FILE) or _try_json_readversion(",
            "    PACKAGE_JSON_FILE",
            ")",
            "",
            "VERSION_SHA_LENGTH = 8",
            "VERSION_SHA = _try_json_readsha(VERSION_INFO_FILE, VERSION_SHA_LENGTH)",
            "",
            "# Build number is shown in the About section if available. This",
            "# can be replaced at build time to expose build information.",
            "BUILD_NUMBER = None",
            "",
            "# default viz used in chart explorer & SQL Lab explore",
            "DEFAULT_VIZ_TYPE = \"table\"",
            "",
            "# default row limit when requesting chart data",
            "ROW_LIMIT = 50000",
            "# default row limit when requesting samples from datasource in explore view",
            "SAMPLES_ROW_LIMIT = 1000",
            "# default row limit for native filters",
            "NATIVE_FILTER_DEFAULT_ROW_LIMIT = 1000",
            "# max rows retrieved by filter select auto complete",
            "FILTER_SELECT_ROW_LIMIT = 10000",
            "# default time filter in explore",
            "# values may be \"Last day\", \"Last week\", \"<ISO date> : now\", etc.",
            "DEFAULT_TIME_FILTER = NO_TIME_RANGE",
            "",
            "SUPERSET_WEBSERVER_PROTOCOL = \"http\"",
            "SUPERSET_WEBSERVER_ADDRESS = \"0.0.0.0\"",
            "SUPERSET_WEBSERVER_PORT = 8088",
            "",
            "# This is an important setting, and should be lower than your",
            "# [load balancer / proxy / envoy / kong / ...] timeout settings.",
            "# You should also make sure to configure your WSGI server",
            "# (gunicorn, nginx, apache, ...) timeout setting to be <= to this setting",
            "SUPERSET_WEBSERVER_TIMEOUT = int(timedelta(minutes=1).total_seconds())",
            "",
            "# this 2 settings are used by dashboard period force refresh feature",
            "# When user choose auto force refresh frequency",
            "# < SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT",
            "# they will see warning message in the Refresh Interval Modal.",
            "# please check PR #9886",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT = 0",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_WARNING_MESSAGE = None",
            "",
            "SUPERSET_DASHBOARD_POSITION_DATA_LIMIT = 65535",
            "CUSTOM_SECURITY_MANAGER = None",
            "SQLALCHEMY_TRACK_MODIFICATIONS = False",
            "# ---------------------------------------------------------",
            "",
            "# Your App secret key. Make sure you override it on superset_config.py",
            "# or use `SUPERSET_SECRET_KEY` environment variable.",
            "# Use a strong complex alphanumeric string and use a tool to help you generate",
            "# a sufficiently random sequence, ex: openssl rand -base64 42\"",
            "SECRET_KEY = os.environ.get(\"SUPERSET_SECRET_KEY\") or CHANGE_ME_SECRET_KEY",
            "",
            "# The SQLAlchemy connection string.",
            "SQLALCHEMY_DATABASE_URI = \"sqlite:///\" + os.path.join(DATA_DIR, \"superset.db\")",
            "# SQLALCHEMY_DATABASE_URI = 'mysql://myapp@localhost/myapp'",
            "# SQLALCHEMY_DATABASE_URI = 'postgresql://root:password@localhost/myapp'",
            "",
            "# In order to hook up a custom password store for all SQLALCHEMY connections",
            "# implement a function that takes a single argument of type 'sqla.engine.url',",
            "# returns a password and set SQLALCHEMY_CUSTOM_PASSWORD_STORE.",
            "#",
            "# e.g.:",
            "# def lookup_password(url):",
            "#     return 'secret'",
            "# SQLALCHEMY_CUSTOM_PASSWORD_STORE = lookup_password",
            "SQLALCHEMY_CUSTOM_PASSWORD_STORE = None",
            "",
            "#",
            "# The EncryptedFieldTypeAdapter is used whenever we're building SqlAlchemy models",
            "# which include sensitive fields that should be app-encrypted BEFORE sending",
            "# to the DB.",
            "#",
            "# Note: the default impl leverages SqlAlchemyUtils' EncryptedType, which defaults",
            "#  to AesEngine that uses AES-128 under the covers using the app's SECRET_KEY",
            "#  as key material. Do note that AesEngine allows for queryability over the",
            "#  encrypted fields.",
            "#",
            "#  To change the default engine you need to define your own adapter:",
            "#",
            "# e.g.:",
            "#",
            "# class AesGcmEncryptedAdapter(",
            "#     AbstractEncryptedFieldAdapter",
            "# ):",
            "#     def create(",
            "#         self,",
            "#         app_config: Optional[Dict[str, Any]],",
            "#         *args: List[Any],",
            "#         **kwargs: Optional[Dict[str, Any]],",
            "#     ) -> TypeDecorator:",
            "#         if app_config:",
            "#             return EncryptedType(",
            "#                 *args, app_config[\"SECRET_KEY\"], engine=AesGcmEngine, **kwargs",
            "#             )",
            "#         raise Exception(\"Missing app_config kwarg\")",
            "#",
            "#",
            "#  SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = AesGcmEncryptedAdapter",
            "SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = (  # pylint: disable=invalid-name",
            "    SQLAlchemyUtilsAdapter",
            ")",
            "# The limit of queries fetched for query search",
            "QUERY_SEARCH_LIMIT = 1000",
            "",
            "# Flask-WTF flag for CSRF",
            "WTF_CSRF_ENABLED = True",
            "",
            "# Add endpoints that need to be exempt from CSRF protection",
            "WTF_CSRF_EXEMPT_LIST = [",
            "    \"superset.views.core.log\",",
            "    \"superset.views.core.explore_json\",",
            "    \"superset.charts.data.api.data\",",
            "]",
            "",
            "# Whether to run the web server in debug mode or not",
            "DEBUG = os.environ.get(\"FLASK_DEBUG\")",
            "FLASK_USE_RELOAD = True",
            "",
            "# Enable profiling of Python calls. Turn this on and append ``?_instrument=1``",
            "# to the page to see the call stack.",
            "PROFILING = False",
            "",
            "# Superset allows server-side python stacktraces to be surfaced to the",
            "# user when this feature is on. This may have security implications",
            "# and it's more secure to turn it off in production settings.",
            "SHOW_STACKTRACE = False",
            "",
            "# Use all X-Forwarded headers when ENABLE_PROXY_FIX is True.",
            "# When proxying to a different port, set \"x_port\" to 0 to avoid downstream issues.",
            "ENABLE_PROXY_FIX = False",
            "PROXY_FIX_CONFIG = {\"x_for\": 1, \"x_proto\": 1, \"x_host\": 1, \"x_port\": 1, \"x_prefix\": 1}",
            "",
            "# Configuration for scheduling queries from SQL Lab.",
            "SCHEDULED_QUERIES: dict[str, Any] = {}",
            "",
            "# FAB Rate limiting: this is a security feature for preventing DDOS attacks. The",
            "# feature is on by default to make Superset secure by default, but you should",
            "# fine tune the limits to your needs. You can read more about the different",
            "# parameters here: https://flask-limiter.readthedocs.io/en/stable/configuration.html",
            "RATELIMIT_ENABLED = True",
            "RATELIMIT_APPLICATION = \"50 per second\"",
            "AUTH_RATE_LIMITED = True",
            "AUTH_RATE_LIMIT = \"5 per second\"",
            "# A storage location conforming to the scheme in storage-scheme. See the limits",
            "# library for allowed values: https://limits.readthedocs.io/en/stable/storage.html",
            "# RATELIMIT_STORAGE_URI = \"redis://host:port\"",
            "# A callable that returns the unique identity of the current request.",
            "# RATELIMIT_REQUEST_IDENTIFIER = flask.Request.endpoint",
            "",
            "# ------------------------------",
            "# GLOBALS FOR APP Builder",
            "# ------------------------------",
            "# Uncomment to setup Your App name",
            "APP_NAME = \"Superset\"",
            "",
            "# Specify the App icon",
            "APP_ICON = \"/static/assets/images/superset-logo-horiz.png\"",
            "",
            "# Specify where clicking the logo would take the user",
            "# e.g. setting it to '/' would take the user to '/superset/welcome/'",
            "LOGO_TARGET_PATH = None",
            "",
            "# Specify tooltip that should appear when hovering over the App Icon/Logo",
            "LOGO_TOOLTIP = \"\"",
            "",
            "# Specify any text that should appear to the right of the logo",
            "LOGO_RIGHT_TEXT: Callable[[], str] | str = \"\"",
            "",
            "# Enables SWAGGER UI for superset openapi spec",
            "# ex: http://localhost:8080/swagger/v1",
            "FAB_API_SWAGGER_UI = True",
            "",
            "# ----------------------------------------------------",
            "# AUTHENTICATION CONFIG",
            "# ----------------------------------------------------",
            "# The authentication type",
            "# AUTH_OID : Is for OpenID",
            "# AUTH_DB : Is for database (username/password)",
            "# AUTH_LDAP : Is for LDAP",
            "# AUTH_REMOTE_USER : Is for using REMOTE_USER from web server",
            "AUTH_TYPE = AUTH_DB",
            "",
            "# Uncomment to setup Full admin role name",
            "# AUTH_ROLE_ADMIN = 'Admin'",
            "",
            "# Uncomment to setup Public role name, no authentication needed",
            "# AUTH_ROLE_PUBLIC = 'Public'",
            "",
            "# Will allow user self registration",
            "# AUTH_USER_REGISTRATION = True",
            "",
            "# The default user self registration role",
            "# AUTH_USER_REGISTRATION_ROLE = \"Public\"",
            "",
            "# When using LDAP Auth, setup the LDAP server",
            "# AUTH_LDAP_SERVER = \"ldap://ldapserver.new\"",
            "",
            "# Uncomment to setup OpenID providers example for OpenID authentication",
            "# OPENID_PROVIDERS = [",
            "#    { 'name': 'Yahoo', 'url': 'https://open.login.yahoo.com/' },",
            "#    { 'name': 'Flickr', 'url': 'https://www.flickr.com/<username>' },",
            "",
            "# ---------------------------------------------------",
            "# Roles config",
            "# ---------------------------------------------------",
            "# Grant public role the same set of permissions as for a selected builtin role.",
            "# This is useful if one wants to enable anonymous users to view",
            "# dashboards. Explicit grant on specific datasets is still required.",
            "PUBLIC_ROLE_LIKE: str | None = None",
            "",
            "# ---------------------------------------------------",
            "# Babel config for translations",
            "# ---------------------------------------------------",
            "# Setup default language",
            "BABEL_DEFAULT_LOCALE = \"en\"",
            "# Your application default translation path",
            "BABEL_DEFAULT_FOLDER = \"superset/translations\"",
            "# The allowed translation for your app",
            "LANGUAGES = {",
            "    \"en\": {\"flag\": \"us\", \"name\": \"English\"},",
            "    \"es\": {\"flag\": \"es\", \"name\": \"Spanish\"},",
            "    \"it\": {\"flag\": \"it\", \"name\": \"Italian\"},",
            "    \"fr\": {\"flag\": \"fr\", \"name\": \"French\"},",
            "    \"zh\": {\"flag\": \"cn\", \"name\": \"Chinese\"},",
            "    \"ja\": {\"flag\": \"jp\", \"name\": \"Japanese\"},",
            "    \"de\": {\"flag\": \"de\", \"name\": \"German\"},",
            "    \"pt\": {\"flag\": \"pt\", \"name\": \"Portuguese\"},",
            "    \"pt_BR\": {\"flag\": \"br\", \"name\": \"Brazilian Portuguese\"},",
            "    \"ru\": {\"flag\": \"ru\", \"name\": \"Russian\"},",
            "    \"ko\": {\"flag\": \"kr\", \"name\": \"Korean\"},",
            "    \"sk\": {\"flag\": \"sk\", \"name\": \"Slovak\"},",
            "    \"sl\": {\"flag\": \"si\", \"name\": \"Slovenian\"},",
            "    \"nl\": {\"flag\": \"nl\", \"name\": \"Dutch\"},",
            "}",
            "# Turning off i18n by default as translation in most languages are",
            "# incomplete and not well maintained.",
            "LANGUAGES = {}",
            "",
            "",
            "# Override the default d3 locale format",
            "# Default values are equivalent to",
            "# D3_FORMAT = {",
            "#     \"decimal\": \".\",           # - decimal place string (e.g., \".\").",
            "#     \"thousands\": \",\",         # - group separator string (e.g., \",\").",
            "#     \"grouping\": [3],          # - array of group sizes (e.g., [3]), cycled as needed.",
            "#     \"currency\": [\"$\", \"\"]     # - currency prefix/suffix strings (e.g., [\"$\", \"\"])",
            "# }",
            "# https://github.com/d3/d3-format/blob/main/README.md#formatLocale",
            "class D3Format(TypedDict, total=False):",
            "    decimal: str",
            "    thousands: str",
            "    grouping: list[int]",
            "    currency: list[str]",
            "",
            "",
            "D3_FORMAT: D3Format = {}",
            "",
            "CURRENCIES = [\"USD\", \"EUR\", \"GBP\", \"INR\", \"MXN\", \"JPY\", \"CNY\"]",
            "",
            "# ---------------------------------------------------",
            "# Feature flags",
            "# ---------------------------------------------------",
            "# Feature flags that are set by default go here. Their values can be",
            "# overwritten by those specified under FEATURE_FLAGS in superset_config.py",
            "# For example, DEFAULT_FEATURE_FLAGS = { 'FOO': True, 'BAR': False } here",
            "# and FEATURE_FLAGS = { 'BAR': True, 'BAZ': True } in superset_config.py",
            "# will result in combined feature flags of { 'FOO': True, 'BAR': True, 'BAZ': True }",
            "DEFAULT_FEATURE_FLAGS: dict[str, bool] = {",
            "    # Experimental feature introducing a client (browser) cache",
            "    \"CLIENT_CACHE\": False,  # deprecated",
            "    \"DISABLE_DATASET_SOURCE_EDIT\": False,  # deprecated",
            "    # When using a recent version of Druid that supports JOINs turn this on",
            "    \"DRUID_JOINS\": False,",
            "    \"DYNAMIC_PLUGINS\": False,",
            "    # With Superset 2.0, we are updating the default so that the legacy datasource",
            "    # editor no longer shows. Currently this is set to false so that the editor",
            "    # option does show, but we will be depreciating it.",
            "    \"DISABLE_LEGACY_DATASOURCE_EDITOR\": True,",
            "    # For some security concerns, you may need to enforce CSRF protection on",
            "    # all query request to explore_json endpoint. In Superset, we use",
            "    # `flask-csrf <https://sjl.bitbucket.io/flask-csrf/>`_ add csrf protection",
            "    # for all POST requests, but this protection doesn't apply to GET method.",
            "    # When ENABLE_EXPLORE_JSON_CSRF_PROTECTION is set to true, your users cannot",
            "    # make GET request to explore_json. explore_json accepts both GET and POST request.",
            "    # See `PR 7935 <https://github.com/apache/superset/pull/7935>`_ for more details.",
            "    \"ENABLE_EXPLORE_JSON_CSRF_PROTECTION\": False,  # deprecated",
            "    \"ENABLE_TEMPLATE_PROCESSING\": False,",
            "    \"ENABLE_TEMPLATE_REMOVE_FILTERS\": True,  # deprecated",
            "    # Allow for javascript controls components",
            "    # this enables programmers to customize certain charts (like the",
            "    # geospatial ones) by inputting javascript in controls. This exposes",
            "    # an XSS security vulnerability",
            "    \"ENABLE_JAVASCRIPT_CONTROLS\": False,",
            "    \"KV_STORE\": False,",
            "    # When this feature is enabled, nested types in Presto will be",
            "    # expanded into extra columns and/or arrays. This is experimental,",
            "    # and doesn't work with all nested types.",
            "    \"PRESTO_EXPAND_DATA\": False,",
            "    # Exposes API endpoint to compute thumbnails",
            "    \"THUMBNAILS\": False,",
            "    \"DASHBOARD_CACHE\": False,  # deprecated",
            "    \"REMOVE_SLICE_LEVEL_LABEL_COLORS\": False,  # deprecated",
            "    \"SHARE_QUERIES_VIA_KV_STORE\": False,",
            "    \"TAGGING_SYSTEM\": False,",
            "    \"SQLLAB_BACKEND_PERSISTENCE\": True,",
            "    \"LISTVIEWS_DEFAULT_CARD_VIEW\": False,",
            "    # When True, this escapes HTML (rather than rendering it) in Markdown components",
            "    \"ESCAPE_MARKDOWN_HTML\": False,",
            "    \"DASHBOARD_NATIVE_FILTERS\": True,  # deprecated",
            "    \"DASHBOARD_CROSS_FILTERS\": True,",
            "    # Feature is under active development and breaking changes are expected",
            "    \"DASHBOARD_NATIVE_FILTERS_SET\": False,  # deprecated",
            "    \"DASHBOARD_FILTERS_EXPERIMENTAL\": False,  # deprecated",
            "    \"DASHBOARD_VIRTUALIZATION\": False,",
            "    \"GLOBAL_ASYNC_QUERIES\": False,",
            "    \"VERSIONED_EXPORT\": True,  # deprecated",
            "    \"EMBEDDED_SUPERSET\": False,",
            "    # Enables Alerts and reports new implementation",
            "    \"ALERT_REPORTS\": False,",
            "    \"DASHBOARD_RBAC\": False,",
            "    \"ENABLE_EXPLORE_DRAG_AND_DROP\": True,  # deprecated",
            "    \"ENABLE_ADVANCED_DATA_TYPES\": False,",
            "    # Enabling ALERTS_ATTACH_REPORTS, the system sends email and slack message",
            "    # with screenshot and link",
            "    # Disables ALERTS_ATTACH_REPORTS, the system DOES NOT generate screenshot",
            "    # for report with type 'alert' and sends email and slack message with only link;",
            "    # for report with type 'report' still send with email and slack message with",
            "    # screenshot and link",
            "    \"ALERTS_ATTACH_REPORTS\": True,",
            "    # Allow users to export full CSV of table viz type.",
            "    # This could cause the server to run out of memory or compute.",
            "    \"ALLOW_FULL_CSV_EXPORT\": False,",
            "    \"GENERIC_CHART_AXES\": True,  # deprecated",
            "    \"ALLOW_ADHOC_SUBQUERY\": False,",
            "    \"USE_ANALAGOUS_COLORS\": False,",
            "    # Apply RLS rules to SQL Lab queries. This requires parsing and manipulating the",
            "    # query, and might break queries and/or allow users to bypass RLS. Use with care!",
            "    \"RLS_IN_SQLLAB\": False,",
            "    # Enable caching per impersonation key (e.g username) in a datasource where user",
            "    # impersonation is enabled",
            "    \"CACHE_IMPERSONATION\": False,",
            "    # Enable caching per user key for Superset cache (not database cache impersonation)",
            "    \"CACHE_QUERY_BY_USER\": False,",
            "    # Enable sharing charts with embedding",
            "    \"EMBEDDABLE_CHARTS\": True,",
            "    \"DRILL_TO_DETAIL\": True,",
            "    \"DRILL_BY\": False,",
            "    \"DATAPANEL_CLOSED_BY_DEFAULT\": False,",
            "    \"HORIZONTAL_FILTER_BAR\": False,",
            "    # The feature is off by default, and currently only supported in Presto and Postgres,",
            "    # and Bigquery.",
            "    # It also needs to be enabled on a per-database basis, by adding the key/value pair",
            "    # `cost_estimate_enabled: true` to the database `extra` attribute.",
            "    \"ESTIMATE_QUERY_COST\": False,",
            "    # Allow users to enable ssh tunneling when creating a DB.",
            "    # Users must check whether the DB engine supports SSH Tunnels",
            "    # otherwise enabling this flag won't have any effect on the DB.",
            "    \"SSH_TUNNELING\": False,",
            "    \"AVOID_COLORS_COLLISION\": True,",
            "    # Set to False to only allow viewing own recent activity",
            "    # or to disallow users from viewing other users profile page",
            "    # Do not show user info or profile in the menu",
            "    \"MENU_HIDE_USER_INFO\": False,",
            "    # Allows users to add a ``superset://`` DB that can query across databases. This is",
            "    # an experimental feature with potential security and performance risks, so use with",
            "    # caution. If the feature is enabled you can also set a limit for how much data is",
            "    # returned from each database in the ``SUPERSET_META_DB_LIMIT`` configuration value",
            "    # in this file.",
            "    \"ENABLE_SUPERSET_META_DB\": False,",
            "    # Set to True to replace Selenium with Playwright to execute reports and thumbnails.",
            "    # Unlike Selenium, Playwright reports support deck.gl visualizations",
            "    # Enabling this feature flag requires installing \"playwright\" pip package",
            "    \"PLAYWRIGHT_REPORTS_AND_THUMBNAILS\": False,",
            "}",
            "",
            "# ------------------------------",
            "# SSH Tunnel",
            "# ------------------------------",
            "# Allow users to set the host used when connecting to the SSH Tunnel",
            "# as localhost and any other alias (0.0.0.0)",
            "# ----------------------------------------------------------------------",
            "#                             |",
            "# -------------+              |    +----------+",
            "#     LOCAL    |              |    |  REMOTE  | :22 SSH",
            "#     CLIENT   | <== SSH ========> |  SERVER  | :8080 web service",
            "# -------------+              |    +----------+",
            "#                             |",
            "#                          FIREWALL (only port 22 is open)",
            "",
            "# ----------------------------------------------------------------------",
            "SSH_TUNNEL_MANAGER_CLASS = \"superset.extensions.ssh.SSHManager\"",
            "SSH_TUNNEL_LOCAL_BIND_ADDRESS = \"127.0.0.1\"",
            "#: Timeout (seconds) for tunnel connection (open_channel timeout)",
            "SSH_TUNNEL_TIMEOUT_SEC = 10.0",
            "#: Timeout (seconds) for transport socket (``socket.settimeout``)",
            "SSH_TUNNEL_PACKET_TIMEOUT_SEC = 1.0",
            "",
            "",
            "# Feature flags may also be set via 'SUPERSET_FEATURE_' prefixed environment vars.",
            "DEFAULT_FEATURE_FLAGS.update(",
            "    {",
            "        k[len(\"SUPERSET_FEATURE_\") :]: parse_boolean_string(v)",
            "        for k, v in os.environ.items()",
            "        if re.search(r\"^SUPERSET_FEATURE_\\w+\", k)",
            "    }",
            ")",
            "",
            "# This is merely a default.",
            "FEATURE_FLAGS: dict[str, bool] = {}",
            "",
            "# A function that receives a dict of all feature flags",
            "# (DEFAULT_FEATURE_FLAGS merged with FEATURE_FLAGS)",
            "# can alter it, and returns a similar dict. Note the dict of feature",
            "# flags passed to the function is a deepcopy of the dict in the config,",
            "# and can therefore be mutated without side-effect",
            "#",
            "# GET_FEATURE_FLAGS_FUNC can be used to implement progressive rollouts,",
            "# role-based features, or a full on A/B testing framework.",
            "#",
            "# from flask import g, request",
            "# def GET_FEATURE_FLAGS_FUNC(feature_flags_dict: Dict[str, bool]) -> Dict[str, bool]:",
            "#     if hasattr(g, \"user\") and g.user.is_active:",
            "#         feature_flags_dict['some_feature'] = g.user and g.user.get_id() == 5",
            "#     return feature_flags_dict",
            "GET_FEATURE_FLAGS_FUNC: Callable[[dict[str, bool]], dict[str, bool]] | None = None",
            "# A function that receives a feature flag name and an optional default value.",
            "# Has a similar utility to GET_FEATURE_FLAGS_FUNC but it's useful to not force the",
            "# evaluation of all feature flags when just evaluating a single one.",
            "#",
            "# Note that the default `get_feature_flags` will evaluate each feature with this",
            "# callable when the config key is set, so don't use both GET_FEATURE_FLAGS_FUNC",
            "# and IS_FEATURE_ENABLED_FUNC in conjunction.",
            "IS_FEATURE_ENABLED_FUNC: Callable[[str, bool | None], bool] | None = None",
            "# A function that expands/overrides the frontend `bootstrap_data.common` object.",
            "# Can be used to implement custom frontend functionality,",
            "# or dynamically change certain configs.",
            "#",
            "# Values in `bootstrap_data.common` should have these characteristics:",
            "# - They are not specific to a page the user is visiting",
            "# - They do not contain secrets",
            "#",
            "# Takes as a parameter the common bootstrap payload before transformations.",
            "# Returns a dict containing data that should be added or overridden to the payload.",
            "COMMON_BOOTSTRAP_OVERRIDES_FUNC: Callable[",
            "    [dict[str, Any]], dict[str, Any]",
            "] = lambda data: {}  # default: empty dict",
            "",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES is used for adding custom categorical color schemes",
            "# example code for \"My custom warm to hot\" color scheme",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES = [",
            "#     {",
            "#         \"id\": 'myVisualizationColors',",
            "#         \"description\": '',",
            "#         \"label\": 'My Visualization Colors',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77',",
            "#          '#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_CATEGORICAL_COLOR_SCHEMES: list[dict[str, Any]] = []",
            "",
            "# THEME_OVERRIDES is used for adding custom theme to superset",
            "# example code for \"My theme\" custom scheme",
            "# THEME_OVERRIDES = {",
            "#   \"borderRadius\": 4,",
            "#   \"colors\": {",
            "#     \"primary\": {",
            "#       \"base\": 'red',",
            "#     },",
            "#     \"secondary\": {",
            "#       \"base\": 'green',",
            "#     },",
            "#     \"grayscale\": {",
            "#       \"base\": 'orange',",
            "#     }",
            "#   }",
            "# }",
            "",
            "THEME_OVERRIDES: dict[str, Any] = {}",
            "",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES is used for adding custom sequential color schemes",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES =  [",
            "#     {",
            "#         \"id\": 'warmToHot',",
            "#         \"description\": '',",
            "#         \"isDiverging\": True,",
            "#         \"label\": 'My custom warm to hot',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD',",
            "#          '#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_SEQUENTIAL_COLOR_SCHEMES: list[dict[str, Any]] = []",
            "",
            "# ---------------------------------------------------",
            "# Thumbnail config (behind feature flag)",
            "# ---------------------------------------------------",
            "# By default, thumbnails are rendered per user, and will fall back to the Selenium",
            "# user for anonymous users. Similar to Alerts & Reports, thumbnails",
            "# can be configured to always be rendered as a fixed user. See",
            "# `superset.tasks.types.ExecutorType` for a full list of executor options.",
            "# To always use a fixed user account, use the following configuration:",
            "# THUMBNAIL_EXECUTE_AS = [ExecutorType.SELENIUM]",
            "THUMBNAIL_SELENIUM_USER: str | None = \"admin\"",
            "THUMBNAIL_EXECUTE_AS = [ExecutorType.CURRENT_USER, ExecutorType.SELENIUM]",
            "",
            "# By default, thumbnail digests are calculated based on various parameters in the",
            "# chart/dashboard metadata, and in the case of user-specific thumbnails, the",
            "# username. To specify a custom digest function, use the following config parameters",
            "# to define callbacks that receive",
            "# 1. the model (dashboard or chart)",
            "# 2. the executor type (e.g. ExecutorType.SELENIUM)",
            "# 3. the executor's username (note, this is the executor as defined by",
            "# `THUMBNAIL_EXECUTE_AS`; the executor is only equal to the currently logged in",
            "# user if the executor type is equal to `ExecutorType.CURRENT_USER`)",
            "# and return the final digest string:",
            "THUMBNAIL_DASHBOARD_DIGEST_FUNC: None | (",
            "    Callable[[Dashboard, ExecutorType, str], str]",
            ") = None",
            "THUMBNAIL_CHART_DIGEST_FUNC: Callable[[Slice, ExecutorType, str], str] | None = None",
            "",
            "THUMBNAIL_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"NullCache\",",
            "    \"CACHE_NO_NULL_WARNING\": True,",
            "}",
            "",
            "# Time before selenium times out after trying to locate an element on the page and wait",
            "# for that element to load for a screenshot.",
            "SCREENSHOT_LOCATE_WAIT = int(timedelta(seconds=10).total_seconds())",
            "# Time before selenium times out after waiting for all DOM class elements named",
            "# \"loading\" are gone.",
            "SCREENSHOT_LOAD_WAIT = int(timedelta(minutes=1).total_seconds())",
            "# Selenium destroy retries",
            "SCREENSHOT_SELENIUM_RETRIES = 5",
            "# Give selenium an headstart, in seconds",
            "SCREENSHOT_SELENIUM_HEADSTART = 3",
            "# Wait for the chart animation, in seconds",
            "SCREENSHOT_SELENIUM_ANIMATION_WAIT = 5",
            "# Replace unexpected errors in screenshots with real error messages",
            "SCREENSHOT_REPLACE_UNEXPECTED_ERRORS = False",
            "# Max time to wait for error message modal to show up, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_VISIBLE = 5",
            "# Max time to wait for error message modal to close, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_INVISIBLE = 5",
            "",
            "# ---------------------------------------------------",
            "# Image and file configuration",
            "# ---------------------------------------------------",
            "# The file upload folder, when using models with files",
            "UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "UPLOAD_CHUNK_SIZE = 4096",
            "",
            "# The image upload folder, when using models with images",
            "IMG_UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "",
            "# The image upload url, when using models with images",
            "IMG_UPLOAD_URL = \"/static/uploads/\"",
            "# Setup image size default is (300, 200, True)",
            "# IMG_SIZE = (300, 200, True)",
            "",
            "# Default cache timeout, applies to all cache backends unless specifically overridden in",
            "# each cache config.",
            "CACHE_DEFAULT_TIMEOUT = int(timedelta(days=1).total_seconds())",
            "",
            "# Default cache for Superset objects",
            "CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for datasource metadata and query results",
            "DATA_CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for dashboard filter state. `CACHE_TYPE` defaults to `SupersetMetastoreCache`",
            "# that stores the values in the key-value table in the Superset metastore, as it's",
            "# required for Superset to operate correctly, but can be replaced by any",
            "# `Flask-Caching` backend.",
            "FILTER_STATE_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"SupersetMetastoreCache\",",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=90).total_seconds()),",
            "    # Should the timeout be reset when retrieving a cached value?",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "    # The following parameter only applies to `MetastoreCache`:",
            "    # How should entries be serialized/deserialized?",
            "    \"CODEC\": JsonKeyValueCodec(),",
            "}",
            "",
            "# Cache for explore form data state. `CACHE_TYPE` defaults to `SupersetMetastoreCache`",
            "# that stores the values in the key-value table in the Superset metastore, as it's",
            "# required for Superset to operate correctly, but can be replaced by any",
            "# `Flask-Caching` backend.",
            "EXPLORE_FORM_DATA_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"SupersetMetastoreCache\",",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=7).total_seconds()),",
            "    # Should the timeout be reset when retrieving a cached value?",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "    # The following parameter only applies to `MetastoreCache`:",
            "    # How should entries be serialized/deserialized?",
            "    \"CODEC\": JsonKeyValueCodec(),",
            "}",
            "",
            "# store cache keys by datasource UID (via CacheKey) for custom processing/invalidation",
            "STORE_CACHE_KEYS_IN_METADATA_DB = False",
            "",
            "# CORS Options",
            "ENABLE_CORS = False",
            "CORS_OPTIONS: dict[Any, Any] = {}",
            "",
            "# Sanitizes the HTML content used in markdowns to allow its rendering in a safe manner.",
            "# Disabling this option is not recommended for security reasons. If you wish to allow",
            "# valid safe elements that are not included in the default sanitization schema, use the",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS configuration.",
            "HTML_SANITIZATION = True",
            "",
            "# Use this configuration to extend the HTML sanitization schema.",
            "# By default we use the GitHub schema defined in",
            "# https://github.com/syntax-tree/hast-util-sanitize/blob/main/lib/schema.js",
            "# For example, the following configuration would allow the rendering of the",
            "# style attribute for div elements and the ftp protocol in hrefs:",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS = {",
            "#   \"attributes\": {",
            "#     \"div\": [\"style\"],",
            "#   },",
            "#   \"protocols\": {",
            "#     \"href\": [\"ftp\"],",
            "#   }",
            "# }",
            "# Be careful when extending the default schema to avoid XSS attacks.",
            "HTML_SANITIZATION_SCHEMA_EXTENSIONS: dict[str, Any] = {}",
            "",
            "# Chrome allows up to 6 open connections per domain at a time. When there are more",
            "# than 6 slices in dashboard, a lot of time fetch requests are queued up and wait for",
            "# next available socket. PR #5039 is trying to allow domain sharding for Superset,",
            "# and this feature will be enabled by configuration only (by default Superset",
            "# doesn't allow cross-domain request).",
            "SUPERSET_WEBSERVER_DOMAINS = None",
            "",
            "# Allowed format types for upload on Database view",
            "EXCEL_EXTENSIONS = {\"xlsx\", \"xls\"}",
            "CSV_EXTENSIONS = {\"csv\", \"tsv\", \"txt\"}",
            "COLUMNAR_EXTENSIONS = {\"parquet\", \"zip\"}",
            "ALLOWED_EXTENSIONS = {*EXCEL_EXTENSIONS, *CSV_EXTENSIONS, *COLUMNAR_EXTENSIONS}",
            "",
            "# Optional maximum file size in bytes when uploading a CSV",
            "CSV_UPLOAD_MAX_SIZE = None",
            "",
            "# CSV Options: key/value pairs that will be passed as argument to DataFrame.to_csv",
            "# method.",
            "# note: index option should not be overridden",
            "CSV_EXPORT = {\"encoding\": \"utf-8\"}",
            "",
            "# Excel Options: key/value pairs that will be passed as argument to DataFrame.to_excel",
            "# method.",
            "# note: index option should not be overridden",
            "EXCEL_EXPORT: dict[str, Any] = {}",
            "",
            "# ---------------------------------------------------",
            "# Time grain configurations",
            "# ---------------------------------------------------",
            "# List of time grains to disable in the application (see list of builtin",
            "# time grains in superset/db_engine_specs/base.py).",
            "# For example: to disable 1 second time grain:",
            "# TIME_GRAIN_DENYLIST = ['PT1S']",
            "TIME_GRAIN_DENYLIST: list[str] = []",
            "",
            "# Additional time grains to be supported using similar definitions as in",
            "# superset/db_engine_specs/base.py.",
            "# For example: To add a new 2 second time grain:",
            "# TIME_GRAIN_ADDONS = {'PT2S': '2 second'}",
            "TIME_GRAIN_ADDONS: dict[str, str] = {}",
            "",
            "# Implementation of additional time grains per engine.",
            "# The column to be truncated is denoted `{col}` in the expression.",
            "# For example: To implement 2 second time grain on clickhouse engine:",
            "# TIME_GRAIN_ADDON_EXPRESSIONS = {",
            "#     'clickhouse': {",
            "#         'PT2S': 'toDateTime(intDiv(toUInt32(toDateTime({col})), 2)*2)'",
            "#     }",
            "# }",
            "TIME_GRAIN_ADDON_EXPRESSIONS: dict[str, dict[str, str]] = {}",
            "",
            "# Map of custom time grains and artificial join column producers used",
            "# when generating the join key between results and time shifts.",
            "# See superset/common/query_context_processor.get_aggregated_join_column",
            "#",
            "# Example of a join column producer that aggregates by fiscal year",
            "# def join_producer(row: Series, column_index: int) -> str:",
            "#    return row[index].strftime(\"%F\")",
            "#",
            "# TIME_GRAIN_JOIN_COLUMN_PRODUCERS = {\"P1F\": join_producer}",
            "TIME_GRAIN_JOIN_COLUMN_PRODUCERS: dict[str, Callable[[Series, int], str]] = {}",
            "",
            "# ---------------------------------------------------",
            "# List of viz_types not allowed in your environment",
            "# For example: Disable pivot table and treemap:",
            "#  VIZ_TYPE_DENYLIST = ['pivot_table', 'treemap']",
            "# ---------------------------------------------------",
            "",
            "VIZ_TYPE_DENYLIST: list[str] = []",
            "",
            "# --------------------------------------------------",
            "# Modules, datasources and middleware to be registered",
            "# --------------------------------------------------",
            "DEFAULT_MODULE_DS_MAP = OrderedDict(",
            "    [",
            "        (\"superset.connectors.sqla.models\", [\"SqlaTable\"]),",
            "    ]",
            ")",
            "ADDITIONAL_MODULE_DS_MAP: dict[str, list[str]] = {}",
            "ADDITIONAL_MIDDLEWARE: list[Callable[..., Any]] = []",
            "",
            "# 1) https://docs.python-guide.org/writing/logging/",
            "# 2) https://docs.python.org/2/library/logging.config.html",
            "",
            "# Default configurator will consume the LOG_* settings below",
            "LOGGING_CONFIGURATOR = DefaultLoggingConfigurator()",
            "",
            "# Console Log Settings",
            "",
            "LOG_FORMAT = \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\"",
            "LOG_LEVEL = \"DEBUG\"",
            "",
            "# ---------------------------------------------------",
            "# Enable Time Rotate Log Handler",
            "# ---------------------------------------------------",
            "# LOG_LEVEL = DEBUG, INFO, WARNING, ERROR, CRITICAL",
            "",
            "ENABLE_TIME_ROTATE = False",
            "TIME_ROTATE_LOG_LEVEL = \"DEBUG\"",
            "FILENAME = os.path.join(DATA_DIR, \"superset.log\")",
            "ROLLOVER = \"midnight\"",
            "INTERVAL = 1",
            "BACKUP_COUNT = 30",
            "",
            "# Custom logger for auditing queries. This can be used to send ran queries to a",
            "# structured immutable store for auditing purposes. The function is called for",
            "# every query ran, in both SQL Lab and charts/dashboards.",
            "# def QUERY_LOGGER(",
            "#     database,",
            "#     query,",
            "#     schema=None,",
            "#     client=None,",
            "#     security_manager=None,",
            "#     log_params=None,",
            "# ):",
            "#     pass",
            "QUERY_LOGGER = None",
            "",
            "# Set this API key to enable Mapbox visualizations",
            "MAPBOX_API_KEY = os.environ.get(\"MAPBOX_API_KEY\", \"\")",
            "",
            "# Maximum number of rows returned for any analytical database query",
            "SQL_MAX_ROW = 100000",
            "",
            "# Maximum number of rows displayed in SQL Lab UI",
            "# Is set to avoid out of memory/localstorage issues in browsers. Does not affect",
            "# exported CSVs",
            "DISPLAY_MAX_ROW = 10000",
            "",
            "# Default row limit for SQL Lab queries. Is overridden by setting a new limit in",
            "# the SQL Lab UI",
            "DEFAULT_SQLLAB_LIMIT = 1000",
            "",
            "# The limit for the Superset Meta DB when the feature flag ENABLE_SUPERSET_META_DB is on",
            "SUPERSET_META_DB_LIMIT: int | None = 1000",
            "",
            "# Adds a warning message on sqllab save query and schedule query modals.",
            "SQLLAB_SAVE_WARNING_MESSAGE = None",
            "SQLLAB_SCHEDULE_WARNING_MESSAGE = None",
            "",
            "# Force refresh while auto-refresh in dashboard",
            "DASHBOARD_AUTO_REFRESH_MODE: Literal[\"fetch\", \"force\"] = \"force\"",
            "# Dashboard auto refresh intervals",
            "DASHBOARD_AUTO_REFRESH_INTERVALS = [",
            "    [0, \"Don't refresh\"],",
            "    [10, \"10 seconds\"],",
            "    [30, \"30 seconds\"],",
            "    [60, \"1 minute\"],",
            "    [300, \"5 minutes\"],",
            "    [1800, \"30 minutes\"],",
            "    [3600, \"1 hour\"],",
            "    [21600, \"6 hours\"],",
            "    [43200, \"12 hours\"],",
            "    [86400, \"24 hours\"],",
            "]",
            "",
            "# This is used as a workaround for the alerts & reports scheduler task to get the time",
            "# celery beat triggered it, see https://github.com/celery/celery/issues/6974 for details",
            "CELERY_BEAT_SCHEDULER_EXPIRES = timedelta(weeks=1)",
            "",
            "# Default celery config is to use SQLA as a broker, in a production setting",
            "# you'll want to use a proper broker as specified here:",
            "# https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/index.html",
            "",
            "",
            "class CeleryConfig:  # pylint: disable=too-few-public-methods",
            "    broker_url = \"sqla+sqlite:///celerydb.sqlite\"",
            "    imports = (\"superset.sql_lab\",)",
            "    result_backend = \"db+sqlite:///celery_results.sqlite\"",
            "    worker_prefetch_multiplier = 1",
            "    task_acks_late = False",
            "    task_annotations = {",
            "        \"sql_lab.get_sql_results\": {\"rate_limit\": \"100/s\"},",
            "        \"email_reports.send\": {",
            "            \"rate_limit\": \"1/s\",",
            "            \"time_limit\": int(timedelta(seconds=120).total_seconds()),",
            "            \"soft_time_limit\": int(timedelta(seconds=150).total_seconds()),",
            "            \"ignore_result\": True,",
            "        },",
            "    }",
            "    beat_schedule = {",
            "        \"email_reports.schedule_hourly\": {",
            "            \"task\": \"email_reports.schedule_hourly\",",
            "            \"schedule\": crontab(minute=1, hour=\"*\"),",
            "        },",
            "        \"reports.scheduler\": {",
            "            \"task\": \"reports.scheduler\",",
            "            \"schedule\": crontab(minute=\"*\", hour=\"*\"),",
            "            \"options\": {\"expires\": int(CELERY_BEAT_SCHEDULER_EXPIRES.total_seconds())},",
            "        },",
            "        \"reports.prune_log\": {",
            "            \"task\": \"reports.prune_log\",",
            "            \"schedule\": crontab(minute=0, hour=0),",
            "        },",
            "    }",
            "",
            "",
            "CELERY_CONFIG = CeleryConfig  # pylint: disable=invalid-name",
            "",
            "# Set celery config to None to disable all the above configuration",
            "# CELERY_CONFIG = None",
            "",
            "# Additional static HTTP headers to be served by your Superset server. Note",
            "# Flask-Talisman applies the relevant security HTTP headers.",
            "#",
            "# DEFAULT_HTTP_HEADERS: sets default values for HTTP headers. These may be overridden",
            "# within the app",
            "# OVERRIDE_HTTP_HEADERS: sets override values for HTTP headers. These values will",
            "# override anything set within the app",
            "DEFAULT_HTTP_HEADERS: dict[str, Any] = {}",
            "OVERRIDE_HTTP_HEADERS: dict[str, Any] = {}",
            "HTTP_HEADERS: dict[str, Any] = {}",
            "",
            "# The db id here results in selecting this one as a default in SQL Lab",
            "DEFAULT_DB_ID = None",
            "",
            "# Timeout duration for SQL Lab synchronous queries",
            "SQLLAB_TIMEOUT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Timeout duration for SQL Lab query validation",
            "SQLLAB_VALIDATION_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# SQLLAB_DEFAULT_DBID",
            "SQLLAB_DEFAULT_DBID = None",
            "",
            "# The MAX duration a query can run for before being killed by celery.",
            "SQLLAB_ASYNC_TIME_LIMIT_SEC = int(timedelta(hours=6).total_seconds())",
            "",
            "# Some databases support running EXPLAIN queries that allow users to estimate",
            "# query costs before they run. These EXPLAIN queries should have a small",
            "# timeout.",
            "SQLLAB_QUERY_COST_ESTIMATE_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# The cost returned by the databases is a relative value; in order to map the cost to",
            "# a tangible value you need to define a custom formatter that takes into consideration",
            "# your specific infrastructure. For example, you could analyze queries a posteriori by",
            "# running EXPLAIN on them, and compute a histogram of relative costs to present the",
            "# cost as a percentile, this step is optional as every db engine spec has its own",
            "# query cost formatter, but it you wanna customize it you can define it inside the config:",
            "",
            "# def postgres_query_cost_formatter(",
            "#     result: List[Dict[str, Any]]",
            "# ) -> List[Dict[str, str]]:",
            "#     # 25, 50, 75% percentiles",
            "#     percentile_costs = [100.0, 1000.0, 10000.0]",
            "#",
            "#     out = []",
            "#     for row in result:",
            "#         relative_cost = row[\"Total cost\"]",
            "#         percentile = bisect.bisect_left(percentile_costs, relative_cost) + 1",
            "#         out.append({",
            "#             \"Relative cost\": relative_cost,",
            "#             \"Percentile\": str(percentile * 25) + \"%\",",
            "#         })",
            "#",
            "#     return out",
            "#",
            "# QUERY_COST_FORMATTERS_BY_ENGINE: {\"postgresql\": postgres_query_cost_formatter}",
            "QUERY_COST_FORMATTERS_BY_ENGINE: dict[",
            "    str, Callable[[list[dict[str, Any]]], list[dict[str, Any]]]",
            "] = {}",
            "",
            "# Flag that controls if limit should be enforced on the CTA (create table as queries).",
            "SQLLAB_CTAS_NO_LIMIT = False",
            "",
            "# This allows you to define custom logic around the \"CREATE TABLE AS\" or CTAS feature",
            "# in SQL Lab that defines where the target schema should be for a given user.",
            "# Database `CTAS Schema` has a precedence over this setting.",
            "# Example below returns a username and CTA queries will write tables into the schema",
            "# name `username`",
            "# SQLLAB_CTAS_SCHEMA_NAME_FUNC = lambda database, user, schema, sql: user.username",
            "# This is move involved example where depending on the database you can leverage data",
            "# available to assign schema for the CTA query:",
            "# def compute_schema_name(database: Database, user: User, schema: str, sql: str) -> str:",
            "#     if database.name == 'mysql_payments_slave':",
            "#         return 'tmp_superset_schema'",
            "#     if database.name == 'presto_gold':",
            "#         return user.username",
            "#     if database.name == 'analytics':",
            "#         if 'analytics' in [r.name for r in user.roles]:",
            "#             return 'analytics_cta'",
            "#         else:",
            "#             return f'tmp_{schema}'",
            "# Function accepts database object, user object, schema name and sql that will be run.",
            "SQLLAB_CTAS_SCHEMA_NAME_FUNC: None | (",
            "    Callable[[Database, models.User, str, str], str]",
            ") = None",
            "",
            "# If enabled, it can be used to store the results of long-running queries",
            "# in SQL Lab by using the \"Run Async\" button/feature",
            "RESULTS_BACKEND: BaseCache | None = None",
            "",
            "# Use PyArrow and MessagePack for async query results serialization,",
            "# rather than JSON. This feature requires additional testing from the",
            "# community before it is fully adopted, so this config option is provided",
            "# in order to disable should breaking issues be discovered.",
            "RESULTS_BACKEND_USE_MSGPACK = True",
            "",
            "# The S3 bucket where you want to store your external hive tables created",
            "# from CSV files. For example, 'companyname-superset'",
            "CSV_TO_HIVE_UPLOAD_S3_BUCKET = None",
            "",
            "# The directory within the bucket specified above that will",
            "# contain all the external tables",
            "CSV_TO_HIVE_UPLOAD_DIRECTORY = \"EXTERNAL_HIVE_TABLES/\"",
            "",
            "",
            "# Function that creates upload directory dynamically based on the",
            "# database used, user and schema provided.",
            "def CSV_TO_HIVE_UPLOAD_DIRECTORY_FUNC(  # pylint: disable=invalid-name",
            "    database: Database,",
            "    user: models.User,  # pylint: disable=unused-argument",
            "    schema: str | None,",
            ") -> str:",
            "    # Note the final empty path enforces a trailing slash.",
            "    return os.path.join(",
            "        CSV_TO_HIVE_UPLOAD_DIRECTORY, str(database.id), schema or \"\", \"\"",
            "    )",
            "",
            "",
            "# The namespace within hive where the tables created from",
            "# uploading CSVs will be stored.",
            "UPLOADED_CSV_HIVE_NAMESPACE: str | None = None",
            "",
            "# Function that computes the allowed schemas for the CSV uploads.",
            "# Allowed schemas will be a union of schemas_allowed_for_file_upload",
            "# db configuration and a result of this function.",
            "",
            "# mypy doesn't catch that if case ensures list content being always str",
            "ALLOWED_USER_CSV_SCHEMA_FUNC: Callable[[Database, models.User], list[str]] = (",
            "    lambda database, user: [UPLOADED_CSV_HIVE_NAMESPACE]",
            "    if UPLOADED_CSV_HIVE_NAMESPACE",
            "    else []",
            ")",
            "",
            "# Values that should be treated as nulls for the csv uploads.",
            "CSV_DEFAULT_NA_NAMES = list(STR_NA_VALUES)",
            "",
            "# A dictionary of items that gets merged into the Jinja context for",
            "# SQL Lab. The existing context gets updated with this dictionary,",
            "# meaning values for existing keys get overwritten by the content of this",
            "# dictionary. Exposing functionality through JINJA_CONTEXT_ADDONS has security",
            "# implications as it opens a window for a user to execute untrusted code.",
            "# It's important to make sure that the objects exposed (as well as objects attached",
            "# to those objects) are harmless. We recommend only exposing simple/pure functions that",
            "# return native types.",
            "JINJA_CONTEXT_ADDONS: dict[str, Callable[..., Any]] = {}",
            "",
            "# A dictionary of macro template processors (by engine) that gets merged into global",
            "# template processors. The existing template processors get updated with this",
            "# dictionary, which means the existing keys get overwritten by the content of this",
            "# dictionary. The customized addons don't necessarily need to use Jinja templating",
            "# language. This allows you to define custom logic to process templates on a per-engine",
            "# basis. Example value = `{\"presto\": CustomPrestoTemplateProcessor}`",
            "CUSTOM_TEMPLATE_PROCESSORS: dict[str, type[BaseTemplateProcessor]] = {}",
            "",
            "# Roles that are controlled by the API / Superset and should not be changed",
            "# by humans.",
            "ROBOT_PERMISSION_ROLES = [\"Public\", \"Gamma\", \"Alpha\", \"Admin\", \"sql_lab\"]",
            "",
            "CONFIG_PATH_ENV_VAR = \"SUPERSET_CONFIG_PATH\"",
            "",
            "# If a callable is specified, it will be called at app startup while passing",
            "# a reference to the Flask app. This can be used to alter the Flask app",
            "# in whatever way.",
            "# example: FLASK_APP_MUTATOR = lambda x: x.before_request = f",
            "FLASK_APP_MUTATOR = None",
            "",
            "# smtp server configuration",
            "EMAIL_NOTIFICATIONS = False  # all the emails are sent using dryrun",
            "SMTP_HOST = \"localhost\"",
            "SMTP_STARTTLS = True",
            "SMTP_SSL = False",
            "SMTP_USER = \"superset\"",
            "SMTP_PORT = 25",
            "SMTP_PASSWORD = \"superset\"",
            "SMTP_MAIL_FROM = \"superset@superset.com\"",
            "# If True creates a default SSL context with ssl.Purpose.CLIENT_AUTH using the",
            "# default system root CA certificates.",
            "SMTP_SSL_SERVER_AUTH = False",
            "ENABLE_CHUNK_ENCODING = False",
            "",
            "# Whether to bump the logging level to ERROR on the flask_appbuilder package",
            "# Set to False if/when debugging FAB related issues like",
            "# permission management",
            "SILENCE_FAB = True",
            "",
            "FAB_ADD_SECURITY_VIEWS = True",
            "FAB_ADD_SECURITY_PERMISSION_VIEW = False",
            "FAB_ADD_SECURITY_VIEW_MENU_VIEW = False",
            "FAB_ADD_SECURITY_PERMISSION_VIEWS_VIEW = False",
            "",
            "# The link to a page containing common errors and their resolutions",
            "# It will be appended at the bottom of sql_lab errors.",
            "TROUBLESHOOTING_LINK = \"\"",
            "",
            "# CSRF token timeout, set to None for a token that never expires",
            "WTF_CSRF_TIME_LIMIT = int(timedelta(weeks=1).total_seconds())",
            "",
            "# This link should lead to a page with instructions on how to gain access to a",
            "# Datasource. It will be placed at the bottom of permissions errors.",
            "PERMISSION_INSTRUCTIONS_LINK = \"\"",
            "",
            "# Integrate external Blueprints to the app by passing them to your",
            "# configuration. These blueprints will get integrated in the app",
            "BLUEPRINTS: list[Blueprint] = []",
            "",
            "# Provide a callable that receives a tracking_url and returns another",
            "# URL. This is used to translate internal Hadoop job tracker URL",
            "# into a proxied one",
            "",
            "",
            "# Transform SQL query tracking url for Hive and Presto engines. You may also",
            "# access information about the query itself by adding a second parameter",
            "# to your transformer function, e.g.:",
            "#   TRACKING_URL_TRANSFORMER = (",
            "#       lambda url, query: url if is_fresh(query) else None",
            "#   )",
            "# pylint: disable-next=unnecessary-lambda-assignment",
            "TRACKING_URL_TRANSFORMER = lambda url: url",
            "",
            "",
            "# customize the polling time of each engine",
            "DB_POLL_INTERVAL_SECONDS: dict[str, int] = {}",
            "",
            "# Interval between consecutive polls when using Presto Engine",
            "# See here: https://github.com/dropbox/PyHive/blob/8eb0aeab8ca300f3024655419b93dad926c1a351/pyhive/presto.py#L93  # pylint: disable=line-too-long,useless-suppression",
            "PRESTO_POLL_INTERVAL = int(timedelta(seconds=1).total_seconds())",
            "",
            "# Allow list of custom authentications for each DB engine.",
            "# Example:",
            "# from your.module import AuthClass",
            "# from another.extra import auth_method",
            "#",
            "# ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {",
            "#     \"trino\": {",
            "#         \"custom_auth\": AuthClass,",
            "#         \"another_auth_method\": auth_method,",
            "#     },",
            "# }",
            "ALLOWED_EXTRA_AUTHENTICATIONS: dict[str, dict[str, Callable[..., Any]]] = {}",
            "",
            "# The id of a template dashboard that should be copied to every new user",
            "DASHBOARD_TEMPLATE_ID = None",
            "",
            "# A callable that allows altering the database connection URL and params",
            "# on the fly, at runtime. This allows for things like impersonation or",
            "# arbitrary logic. For instance you can wire different users to",
            "# use different connection parameters, or pass their email address as the",
            "# username. The function receives the connection uri object, connection",
            "# params, the username, and returns the mutated uri and params objects.",
            "# Example:",
            "#   def DB_CONNECTION_MUTATOR(uri, params, username, security_manager, source):",
            "#       user = security_manager.find_user(username=username)",
            "#       if user and user.email:",
            "#           uri.username = user.email",
            "#       return uri, params",
            "#",
            "# Note that the returned uri and params are passed directly to sqlalchemy's",
            "# as such `create_engine(url, **params)`",
            "DB_CONNECTION_MUTATOR = None",
            "",
            "",
            "# A function that intercepts the SQL to be executed and can alter it.",
            "# The use case is can be around adding some sort of comment header",
            "# with information such as the username and worker node information",
            "#",
            "#    def SQL_QUERY_MUTATOR(",
            "#        sql,",
            "#        security_manager=security_manager,",
            "#        database=database,",
            "#    ):",
            "#        dttm = datetime.now().isoformat()",
            "#        return f\"-- [SQL LAB] {user_name} {dttm}\\n{sql}\"",
            "# For backward compatibility, you can unpack any of the above arguments in your",
            "# function definition, but keep the **kwargs as the last argument to allow new args",
            "# to be added later without any errors.",
            "def SQL_QUERY_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    sql: str, **kwargs: Any",
            ") -> str:",
            "    return sql",
            "",
            "",
            "# A variable that chooses whether to apply the SQL_QUERY_MUTATOR before or after splitting the input query",
            "# It allows for using the SQL_QUERY_MUTATOR function for more than comments",
            "# Usage: If you want to apply a change to every statement to a given query, set MUTATE_AFTER_SPLIT = True",
            "# An example use case is if data has role based access controls, and you want to apply",
            "# a SET ROLE statement alongside every user query. Changing this variable maintains",
            "# functionality for both the SQL_Lab and Charts.",
            "MUTATE_AFTER_SPLIT = False",
            "",
            "",
            "# This allows for a user to add header data to any outgoing emails. For example,",
            "# if you need to include metadata in the header or you want to change the specifications",
            "# of the email title, header, or sender.",
            "def EMAIL_HEADER_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    msg: MIMEMultipart, **kwargs: Any",
            ") -> MIMEMultipart:",
            "    return msg",
            "",
            "",
            "# Define a list of usernames to be excluded from all dropdown lists of users",
            "# Owners, filters for created_by, etc.",
            "# The users can also be excluded by overriding the get_exclude_users_from_lists method",
            "# in security manager",
            "EXCLUDE_USERS_FROM_LISTS: list[str] | None = None",
            "",
            "# For database connections, this dictionary will remove engines from the available",
            "# list/dropdown if you do not want these dbs to show as available.",
            "# The available list is generated by driver installed, and some engines have multiple",
            "# drivers.",
            "# e.g., DBS_AVAILABLE_DENYLIST: Dict[str, Set[str]] = {\"databricks\": {\"pyhive\", \"pyodbc\"}}",
            "DBS_AVAILABLE_DENYLIST: dict[str, set[str]] = {}",
            "",
            "# This auth provider is used by background (offline) tasks that need to access",
            "# protected resources. Can be overridden by end users in order to support",
            "# custom auth mechanisms",
            "MACHINE_AUTH_PROVIDER_CLASS = \"superset.utils.machine_auth.MachineAuthProvider\"",
            "",
            "# ---------------------------------------------------",
            "# Alerts & Reports",
            "# ---------------------------------------------------",
            "# Used for Alerts/Reports (Feature flask ALERT_REPORTS) to set the size for the",
            "# sliding cron window size, should be synced with the celery beat config minus 1 second",
            "ALERT_REPORTS_CRON_WINDOW_SIZE = 59",
            "ALERT_REPORTS_WORKING_TIME_OUT_KILL = True",
            "# Which user to attempt to execute Alerts/Reports as. By default,",
            "# execute as the primary owner of the alert/report (giving priority to the last",
            "# modifier and then the creator if either is contained within the list of owners,",
            "# otherwise the first owner will be used).",
            "#",
            "# To first try to execute as the creator in the owners list (if present), then fall",
            "# back to the creator, then the last modifier in the owners list (if present), then the",
            "# last modifier, then an owner and finally `THUMBNAIL_SELENIUM_USER`, set as follows:",
            "# ALERT_REPORTS_EXECUTE_AS = [",
            "#     ExecutorType.CREATOR_OWNER,",
            "#     ExecutorType.CREATOR,",
            "#     ExecutorType.MODIFIER_OWNER,",
            "#     ExecutorType.MODIFIER,",
            "#     ExecutorType.OWNER,",
            "#     ExecutorType.SELENIUM,",
            "# ]",
            "ALERT_REPORTS_EXECUTE_AS: list[ExecutorType] = [ExecutorType.OWNER]",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_TIME_OUT_LAG = int(timedelta(seconds=10).total_seconds())",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG = int(timedelta(seconds=1).total_seconds())",
            "# Default values that user using when creating alert",
            "ALERT_REPORTS_DEFAULT_WORKING_TIMEOUT = 3600",
            "ALERT_REPORTS_DEFAULT_RETENTION = 90",
            "ALERT_REPORTS_DEFAULT_CRON_VALUE = \"0 * * * *\"  # every hour",
            "# If set to true no notification is sent, the worker will just log a message.",
            "# Useful for debugging",
            "ALERT_REPORTS_NOTIFICATION_DRY_RUN = False",
            "# Max tries to run queries to prevent false errors caused by transient errors",
            "# being returned to users. Set to a value >1 to enable retries.",
            "ALERT_REPORTS_QUERY_EXECUTION_MAX_TRIES = 1",
            "# Custom width for screenshots",
            "ALERT_REPORTS_MIN_CUSTOM_SCREENSHOT_WIDTH = 600",
            "ALERT_REPORTS_MAX_CUSTOM_SCREENSHOT_WIDTH = 2400",
            "",
            "# A custom prefix to use on all Alerts & Reports emails",
            "EMAIL_REPORTS_SUBJECT_PREFIX = \"[Report] \"",
            "",
            "# The text for call-to-action link in Alerts & Reports emails",
            "EMAIL_REPORTS_CTA = \"Explore in Superset\"",
            "",
            "# Slack API token for the superset reports, either string or callable",
            "SLACK_API_TOKEN: Callable[[], str] | str | None = None",
            "SLACK_PROXY = None",
            "",
            "# The webdriver to use for generating reports. Use one of the following",
            "# firefox",
            "#   Requires: geckodriver and firefox installations",
            "#   Limitations: can be buggy at times",
            "# chrome:",
            "#   Requires: headless chrome",
            "#   Limitations: unable to generate screenshots of elements",
            "WEBDRIVER_TYPE = \"firefox\"",
            "",
            "# Window size - this will impact the rendering of the data",
            "WEBDRIVER_WINDOW = {",
            "    \"dashboard\": (1600, 2000),",
            "    \"slice\": (3000, 1200),",
            "    \"pixel_density\": 1,",
            "}",
            "",
            "# An optional override to the default auth hook used to provide auth to the offline",
            "# webdriver (when using Selenium) or browser context (when using Playwright - see",
            "# PLAYWRIGHT_REPORTS_AND_THUMBNAILS feature flag)",
            "WEBDRIVER_AUTH_FUNC = None",
            "",
            "# Any config options to be passed as-is to the webdriver",
            "WEBDRIVER_CONFIGURATION: dict[Any, Any] = {\"service_log_path\": \"/dev/null\"}",
            "",
            "# Additional args to be passed as arguments to the config object",
            "# Note: If using Chrome, you'll want to add the \"--marionette\" arg.",
            "WEBDRIVER_OPTION_ARGS = [\"--headless\"]",
            "",
            "# The base URL to query for accessing the user interface",
            "WEBDRIVER_BASEURL = \"http://0.0.0.0:8080/\"",
            "# The base URL for the email report hyperlinks.",
            "WEBDRIVER_BASEURL_USER_FRIENDLY = WEBDRIVER_BASEURL",
            "# Time selenium will wait for the page to load and render for the email report.",
            "EMAIL_PAGE_RENDER_WAIT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Send user to a link where they can report bugs",
            "BUG_REPORT_URL = None",
            "BUG_REPORT_TEXT = \"Report a bug\"",
            "BUG_REPORT_ICON = None  # Recommended size: 16x16",
            "",
            "# Send user to a link where they can read more about Superset",
            "DOCUMENTATION_URL = None",
            "DOCUMENTATION_TEXT = \"Documentation\"",
            "DOCUMENTATION_ICON = None  # Recommended size: 16x16",
            "",
            "# What is the Last N days relative in the time selector to:",
            "# 'today' means it is midnight (00:00:00) in the local timezone",
            "# 'now' means it is relative to the query issue time",
            "# If both start and end time is set to now, this will make the time",
            "# filter a moving window. By only setting the end time to now,",
            "# start time will be set to midnight, while end will be relative to",
            "# the query issue time.",
            "DEFAULT_RELATIVE_START_TIME = \"today\"",
            "DEFAULT_RELATIVE_END_TIME = \"today\"",
            "",
            "# Configure which SQL validator to use for each engine",
            "SQL_VALIDATORS_BY_ENGINE = {",
            "    \"presto\": \"PrestoDBSQLValidator\",",
            "    \"postgresql\": \"PostgreSQLValidator\",",
            "}",
            "",
            "# A list of preferred databases, in order. These databases will be",
            "# displayed prominently in the \"Add Database\" dialog. You should",
            "# use the \"engine_name\" attribute of the corresponding DB engine spec",
            "# in `superset/db_engine_specs/`.",
            "PREFERRED_DATABASES: list[str] = [",
            "    \"PostgreSQL\",",
            "    \"Presto\",",
            "    \"MySQL\",",
            "    \"SQLite\",",
            "    # etc.",
            "]",
            "# When adding a new database we try to connect to it. Depending on which parameters are",
            "# incorrect this could take a couple minutes, until the SQLAlchemy driver pinging the",
            "# database times out. Instead of relying on the driver timeout we can specify a shorter",
            "# one here.",
            "TEST_DATABASE_CONNECTION_TIMEOUT = timedelta(seconds=30)",
            "",
            "# Enable/disable CSP warning",
            "CONTENT_SECURITY_POLICY_WARNING = True",
            "",
            "# Do you want Talisman enabled?",
            "TALISMAN_ENABLED = utils.cast_to_boolean(os.environ.get(\"TALISMAN_ENABLED\", True))",
            "",
            "# If you want Talisman, how do you want it configured??",
            "TALISMAN_CONFIG = {",
            "    \"content_security_policy\": {",
            "        \"default-src\": [\"'self'\"],",
            "        \"img-src\": [\"'self'\", \"blob:\", \"data:\"],",
            "        \"worker-src\": [\"'self'\", \"blob:\"],",
            "        \"connect-src\": [",
            "            \"'self'\",",
            "            \"https://api.mapbox.com\",",
            "            \"https://events.mapbox.com\",",
            "        ],",
            "        \"object-src\": \"'none'\",",
            "        \"style-src\": [",
            "            \"'self'\",",
            "            \"'unsafe-inline'\",",
            "            \"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css\",",
            "        ],",
            "        \"script-src\": [\"'self'\", \"'strict-dynamic'\"],",
            "    },",
            "    \"content_security_policy_nonce_in\": [\"script-src\"],",
            "    \"force_https\": False,",
            "}",
            "# React requires `eval` to work correctly in dev mode",
            "TALISMAN_DEV_CONFIG = {",
            "    \"content_security_policy\": {",
            "        \"default-src\": [\"'self'\"],",
            "        \"img-src\": [\"'self'\", \"blob:\", \"data:\"],",
            "        \"worker-src\": [\"'self'\", \"blob:\"],",
            "        \"connect-src\": [",
            "            \"'self'\",",
            "            \"https://api.mapbox.com\",",
            "            \"https://events.mapbox.com\",",
            "        ],",
            "        \"object-src\": \"'none'\",",
            "        \"style-src\": [",
            "            \"'self'\",",
            "            \"'unsafe-inline'\",",
            "            \"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css\",",
            "        ],",
            "        \"script-src\": [\"'self'\", \"'unsafe-inline'\", \"'unsafe-eval'\"],",
            "    },",
            "    \"content_security_policy_nonce_in\": [\"script-src\"],",
            "    \"force_https\": False,",
            "}",
            "",
            "#",
            "# Flask session cookie options",
            "#",
            "# See https://flask.palletsprojects.com/en/1.1.x/security/#set-cookie-options",
            "# for details",
            "#",
            "SESSION_COOKIE_HTTPONLY = True  # Prevent cookie from being read by frontend JS?",
            "SESSION_COOKIE_SECURE = False  # Prevent cookie from being transmitted over non-tls?",
            "SESSION_COOKIE_SAMESITE: Literal[\"None\", \"Lax\", \"Strict\"] | None = \"Lax\"",
            "",
            "# Cache static resources.",
            "SEND_FILE_MAX_AGE_DEFAULT = int(timedelta(days=365).total_seconds())",
            "",
            "# URI to database storing the example data, points to",
            "# SQLALCHEMY_DATABASE_URI by default if set to `None`",
            "SQLALCHEMY_EXAMPLES_URI = \"sqlite:///\" + os.path.join(DATA_DIR, \"examples.db\")",
            "",
            "# Optional prefix to be added to all static asset paths when rendering the UI.",
            "# This is useful for hosting assets in an external CDN, for example",
            "STATIC_ASSETS_PREFIX = \"\"",
            "",
            "# Some sqlalchemy connection strings can open Superset to security risks.",
            "# Typically these should not be allowed.",
            "PREVENT_UNSAFE_DB_CONNECTIONS = True",
            "",
            "# If true all default urls on datasets will be handled as relative URLs by the frontend",
            "PREVENT_UNSAFE_DEFAULT_URLS_ON_DATASET = True",
            "",
            "# Define a list of allowed URLs for dataset data imports (v1).",
            "# Simple example to only allow URLs that belong to certain domains:",
            "# ALLOWED_IMPORT_URL_DOMAINS = [",
            "#     r\"^https://.+\\.domain1\\.com\\/?.*\", r\"^https://.+\\.domain2\\.com\\/?.*\"",
            "# ]",
            "DATASET_IMPORT_ALLOWED_DATA_URLS = [r\".*\"]",
            "",
            "# Path used to store SSL certificates that are generated when using custom certs.",
            "# Defaults to temporary directory.",
            "# Example: SSL_CERT_PATH = \"/certs\"",
            "SSL_CERT_PATH: str | None = None",
            "",
            "# SQLA table mutator, every time we fetch the metadata for a certain table",
            "# (superset.connectors.sqla.models.SqlaTable), we call this hook",
            "# to allow mutating the object with this callback.",
            "# This can be used to set any properties of the object based on naming",
            "# conventions and such. You can find examples in the tests.",
            "",
            "# pylint: disable-next=unnecessary-lambda-assignment",
            "SQLA_TABLE_MUTATOR = lambda table: table",
            "",
            "",
            "# Global async query config options.",
            "# Requires GLOBAL_ASYNC_QUERIES feature flag to be enabled.",
            "GLOBAL_ASYNC_QUERY_MANAGER_CLASS = (",
            "    \"superset.async_events.async_query_manager.AsyncQueryManager\"",
            ")",
            "GLOBAL_ASYNC_QUERIES_REDIS_CONFIG = {",
            "    \"port\": 6379,",
            "    \"host\": \"127.0.0.1\",",
            "    \"password\": \"\",",
            "    \"db\": 0,",
            "    \"ssl\": False,",
            "}",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_PREFIX = \"async-events-\"",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT = 1000",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT_FIREHOSE = 1000000",
            "GLOBAL_ASYNC_QUERIES_REGISTER_REQUEST_HANDLERS = True",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_NAME = \"async-token\"",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SECURE = False",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SAMESITE: None | (",
            "    Literal[\"None\", \"Lax\", \"Strict\"]",
            ") = None",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_DOMAIN = None",
            "GLOBAL_ASYNC_QUERIES_JWT_SECRET = \"test-secret-change-me\"",
            "GLOBAL_ASYNC_QUERIES_TRANSPORT = \"polling\"",
            "GLOBAL_ASYNC_QUERIES_POLLING_DELAY = int(",
            "    timedelta(milliseconds=500).total_seconds() * 1000",
            ")",
            "GLOBAL_ASYNC_QUERIES_WEBSOCKET_URL = \"ws://127.0.0.1:8080/\"",
            "",
            "# Embedded config options",
            "GUEST_ROLE_NAME = \"Public\"",
            "GUEST_TOKEN_JWT_SECRET = \"test-guest-secret-change-me\"",
            "GUEST_TOKEN_JWT_ALGO = \"HS256\"",
            "GUEST_TOKEN_HEADER_NAME = \"X-GuestToken\"",
            "GUEST_TOKEN_JWT_EXP_SECONDS = 300  # 5 minutes",
            "# Guest token audience for the embedded superset, either string or callable",
            "GUEST_TOKEN_JWT_AUDIENCE: Callable[[], str] | str | None = None",
            "",
            "# A SQL dataset health check. Note if enabled it is strongly advised that the callable",
            "# be memoized to aid with performance, i.e.,",
            "#",
            "#    @cache_manager.cache.memoize(timeout=0)",
            "#    def DATASET_HEALTH_CHECK(datasource: SqlaTable) -> Optional[str]:",
            "#        if (",
            "#            datasource.sql and",
            "#            len(sql_parse.ParsedQuery(datasource.sql, strip_comments=True).tables) == 1",
            "#        ):",
            "#            return (",
            "#                \"This virtual dataset queries only one table and therefore could be \"",
            "#                \"replaced by querying the table directly.\"",
            "#            )",
            "#",
            "#        return None",
            "#",
            "# Within the FLASK_APP_MUTATOR callable, i.e., once the application and thus cache have",
            "# been initialized it is also necessary to add the following logic to blow the cache for",
            "# all datasources if the callback function changed.",
            "#",
            "#    def FLASK_APP_MUTATOR(app: Flask) -> None:",
            "#        name = \"DATASET_HEALTH_CHECK\"",
            "#        func = app.config[name]",
            "#        code = func.uncached.__code__.co_code",
            "#",
            "#        if cache_manager.cache.get(name) != code:",
            "#            cache_manager.cache.delete_memoized(func)",
            "#            cache_manager.cache.set(name, code, timeout=0)",
            "#",
            "DATASET_HEALTH_CHECK: Callable[[SqlaTable], str] | None = None",
            "",
            "# the advanced data type key should correspond to that set in the column metadata",
            "ADVANCED_DATA_TYPES: dict[str, AdvancedDataType] = {",
            "    \"internet_address\": internet_address,",
            "    \"port\": internet_port,",
            "}",
            "",
            "# By default, the Welcome page features all charts and dashboards the user has access",
            "# to. This can be changed to show only examples, or a custom view",
            "# by providing the title and a FAB filter:",
            "# WELCOME_PAGE_LAST_TAB = (",
            "#     \"Xyz\",",
            "#     [{\"col\": 'created_by', \"opr\": 'rel_o_m', \"value\": 10}],",
            "# )",
            "WELCOME_PAGE_LAST_TAB: (",
            "    Literal[\"examples\", \"all\"] | tuple[str, list[dict[str, Any]]]",
            ") = \"all\"",
            "",
            "# Configuration for environment tag shown on the navbar. Setting 'text' to '' will hide the tag.",
            "# 'color' can either be a hex color code, or a dot-indexed theme color (e.g. error.base)",
            "ENVIRONMENT_TAG_CONFIG = {",
            "    \"variable\": \"SUPERSET_ENV\",",
            "    \"values\": {",
            "        \"debug\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"flask-debug\",",
            "        },",
            "        \"development\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"Development\",",
            "        },",
            "        \"production\": {",
            "            \"color\": \"\",",
            "            \"text\": \"\",",
            "        },",
            "    },",
            "}",
            "",
            "",
            "# Extra related query filters make it possible to limit which objects are shown",
            "# in the UI. For examples, to only show \"admin\" or users starting with the letter \"b\" in",
            "# the \"Owners\" dropdowns, you could add the following in your config:",
            "# def user_filter(query: Query, *args, *kwargs):",
            "#     from superset import security_manager",
            "#",
            "#     user_model = security_manager.user_model",
            "#     filters = [",
            "#         user_model.username == \"admin\",",
            "#         user_model.username.ilike(\"b%\"),",
            "#     ]",
            "#     return query.filter(or_(*filters))",
            "#",
            "#  EXTRA_RELATED_QUERY_FILTERS = {\"user\": user_filter}",
            "#",
            "# Similarly, to restrict the roles in the \"Roles\" dropdown you can provide a custom",
            "# filter callback for the \"role\" key.",
            "class ExtraRelatedQueryFilters(TypedDict, total=False):",
            "    role: Callable[[Query], Query]",
            "    user: Callable[[Query], Query]",
            "",
            "",
            "EXTRA_RELATED_QUERY_FILTERS: ExtraRelatedQueryFilters = {}",
            "",
            "",
            "# Extra dynamic query filters make it possible to limit which objects are shown",
            "# in the UI before any other filtering is applied. Useful for example when",
            "# considering to filter using Feature Flags along with regular role filters",
            "# that get applied by default in our base_filters.",
            "# For example, to only show a database starting with the letter \"b\"",
            "# in the \"Database Connections\" list, you could add the following in your config:",
            "# def initial_database_filter(query: Query, *args, *kwargs):",
            "#     from superset.models.core import Database",
            "#",
            "#     filter = Database.database_name.startswith('b')",
            "#     return query.filter(filter)",
            "#",
            "#  EXTRA_DYNAMIC_QUERY_FILTERS = {\"database\": initial_database_filter}",
            "class ExtraDynamicQueryFilters(TypedDict, total=False):",
            "    databases: Callable[[Query], Query]",
            "",
            "",
            "EXTRA_DYNAMIC_QUERY_FILTERS: ExtraDynamicQueryFilters = {}",
            "",
            "",
            "# -------------------------------------------------------------------",
            "# *                WARNING:  STOP EDITING  HERE                    *",
            "# -------------------------------------------------------------------",
            "# Don't add config values below this line since local configs won't be",
            "# able to override them.",
            "if CONFIG_PATH_ENV_VAR in os.environ:",
            "    # Explicitly import config module that is not necessarily in pythonpath; useful",
            "    # for case where app is being executed via pex.",
            "    cfg_path = os.environ[CONFIG_PATH_ENV_VAR]",
            "    try:",
            "        module = sys.modules[__name__]",
            "        override_conf = imp.load_source(\"superset_config\", cfg_path)",
            "        for key in dir(override_conf):",
            "            if key.isupper():",
            "                setattr(module, key, getattr(override_conf, key))",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{cfg_path}]\")",
            "    except Exception:",
            "        logger.exception(",
            "            \"Failed to import config for %s=%s\", CONFIG_PATH_ENV_VAR, cfg_path",
            "        )",
            "        raise",
            "elif importlib.util.find_spec(\"superset_config\") and not is_test():",
            "    try:",
            "        # pylint: disable=import-error,wildcard-import,unused-wildcard-import",
            "        import superset_config",
            "        from superset_config import *  # type: ignore",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{superset_config.__file__}]\")",
            "    except Exception:",
            "        logger.exception(\"Found but failed to import local superset_config\")",
            "        raise"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"The main config file for Superset",
            "",
            "All configuration in this file can be overridden by providing a superset_config",
            "in your PYTHONPATH as there is a ``from superset_config import *``",
            "at the end of this file.",
            "\"\"\"",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import imp  # pylint: disable=deprecated-module",
            "import importlib.util",
            "import json",
            "import logging",
            "import os",
            "import re",
            "import sys",
            "from collections import OrderedDict",
            "from datetime import timedelta",
            "from email.mime.multipart import MIMEMultipart",
            "from importlib.resources import files",
            "from typing import Any, Callable, Literal, TYPE_CHECKING, TypedDict",
            "",
            "import pkg_resources",
            "from celery.schedules import crontab",
            "from flask import Blueprint",
            "from flask_appbuilder.security.manager import AUTH_DB",
            "from flask_caching.backends.base import BaseCache",
            "from pandas import Series",
            "from pandas._libs.parsers import STR_NA_VALUES  # pylint: disable=no-name-in-module",
            "from sqlalchemy.orm.query import Query",
            "",
            "from superset.advanced_data_type.plugins.internet_address import internet_address",
            "from superset.advanced_data_type.plugins.internet_port import internet_port",
            "from superset.advanced_data_type.types import AdvancedDataType",
            "from superset.constants import CHANGE_ME_SECRET_KEY",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.key_value.types import JsonKeyValueCodec",
            "from superset.stats_logger import DummyStatsLogger",
            "from superset.superset_typing import CacheConfig",
            "from superset.tasks.types import ExecutorType",
            "from superset.utils import core as utils",
            "from superset.utils.core import is_test, NO_TIME_RANGE, parse_boolean_string",
            "from superset.utils.encrypt import SQLAlchemyUtilsAdapter",
            "from superset.utils.log import DBEventLogger",
            "from superset.utils.logging_configurator import DefaultLoggingConfigurator",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from flask_appbuilder.security.sqla import models",
            "",
            "    from superset.connectors.sqla.models import SqlaTable",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.slice import Slice",
            "",
            "# Realtime stats logger, a StatsD implementation exists",
            "STATS_LOGGER = DummyStatsLogger()",
            "EVENT_LOGGER = DBEventLogger()",
            "",
            "SUPERSET_LOG_VIEW = True",
            "",
            "BASE_DIR = pkg_resources.resource_filename(\"superset\", \"\")",
            "if \"SUPERSET_HOME\" in os.environ:",
            "    DATA_DIR = os.environ[\"SUPERSET_HOME\"]",
            "else:",
            "    DATA_DIR = os.path.expanduser(\"~/.superset\")",
            "",
            "# ---------------------------------------------------------",
            "# Superset specific config",
            "# ---------------------------------------------------------",
            "VERSION_INFO_FILE = str(files(\"superset\") / \"static/version_info.json\")",
            "PACKAGE_JSON_FILE = str(files(\"superset\") / \"static/assets/package.json\")",
            "",
            "",
            "# Multiple favicons can be specified here. The \"href\" property",
            "# is mandatory, but \"sizes,\" \"type,\" and \"rel\" are optional.",
            "# For example:",
            "# {",
            "#     \"href\":path/to/image.png\",",
            "#     \"sizes\": \"16x16\",",
            "#     \"type\": \"image/png\"",
            "#     \"rel\": \"icon\"",
            "# },",
            "FAVICONS = [{\"href\": \"/static/assets/images/favicon.png\"}]",
            "",
            "",
            "def _try_json_readversion(filepath: str) -> str | None:",
            "    try:",
            "        with open(filepath) as f:",
            "            return json.load(f).get(\"version\")",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "def _try_json_readsha(filepath: str, length: int) -> str | None:",
            "    try:",
            "        with open(filepath) as f:",
            "            return json.load(f).get(\"GIT_SHA\")[:length]",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "#",
            "# If True, we will skip the call to load the logger config found in alembic.init",
            "#",
            "ALEMBIC_SKIP_LOG_CONFIG = False",
            "",
            "# Depending on the context in which this config is loaded, the",
            "# version_info.json file may or may not be available, as it is",
            "# generated on install via setup.py. In the event that we're",
            "# actually running Superset, we will have already installed,",
            "# therefore it WILL exist. When unit tests are running, however,",
            "# it WILL NOT exist, so we fall back to reading package.json",
            "VERSION_STRING = _try_json_readversion(VERSION_INFO_FILE) or _try_json_readversion(",
            "    PACKAGE_JSON_FILE",
            ")",
            "",
            "VERSION_SHA_LENGTH = 8",
            "VERSION_SHA = _try_json_readsha(VERSION_INFO_FILE, VERSION_SHA_LENGTH)",
            "",
            "# Build number is shown in the About section if available. This",
            "# can be replaced at build time to expose build information.",
            "BUILD_NUMBER = None",
            "",
            "# default viz used in chart explorer & SQL Lab explore",
            "DEFAULT_VIZ_TYPE = \"table\"",
            "",
            "# default row limit when requesting chart data",
            "ROW_LIMIT = 50000",
            "# default row limit when requesting samples from datasource in explore view",
            "SAMPLES_ROW_LIMIT = 1000",
            "# default row limit for native filters",
            "NATIVE_FILTER_DEFAULT_ROW_LIMIT = 1000",
            "# max rows retrieved by filter select auto complete",
            "FILTER_SELECT_ROW_LIMIT = 10000",
            "# default time filter in explore",
            "# values may be \"Last day\", \"Last week\", \"<ISO date> : now\", etc.",
            "DEFAULT_TIME_FILTER = NO_TIME_RANGE",
            "",
            "SUPERSET_WEBSERVER_PROTOCOL = \"http\"",
            "SUPERSET_WEBSERVER_ADDRESS = \"0.0.0.0\"",
            "SUPERSET_WEBSERVER_PORT = 8088",
            "",
            "# This is an important setting, and should be lower than your",
            "# [load balancer / proxy / envoy / kong / ...] timeout settings.",
            "# You should also make sure to configure your WSGI server",
            "# (gunicorn, nginx, apache, ...) timeout setting to be <= to this setting",
            "SUPERSET_WEBSERVER_TIMEOUT = int(timedelta(minutes=1).total_seconds())",
            "",
            "# this 2 settings are used by dashboard period force refresh feature",
            "# When user choose auto force refresh frequency",
            "# < SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT",
            "# they will see warning message in the Refresh Interval Modal.",
            "# please check PR #9886",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT = 0",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_WARNING_MESSAGE = None",
            "",
            "SUPERSET_DASHBOARD_POSITION_DATA_LIMIT = 65535",
            "CUSTOM_SECURITY_MANAGER = None",
            "SQLALCHEMY_TRACK_MODIFICATIONS = False",
            "# ---------------------------------------------------------",
            "",
            "# Your App secret key. Make sure you override it on superset_config.py",
            "# or use `SUPERSET_SECRET_KEY` environment variable.",
            "# Use a strong complex alphanumeric string and use a tool to help you generate",
            "# a sufficiently random sequence, ex: openssl rand -base64 42\"",
            "SECRET_KEY = os.environ.get(\"SUPERSET_SECRET_KEY\") or CHANGE_ME_SECRET_KEY",
            "",
            "# The SQLAlchemy connection string.",
            "SQLALCHEMY_DATABASE_URI = \"sqlite:///\" + os.path.join(DATA_DIR, \"superset.db\")",
            "# SQLALCHEMY_DATABASE_URI = 'mysql://myapp@localhost/myapp'",
            "# SQLALCHEMY_DATABASE_URI = 'postgresql://root:password@localhost/myapp'",
            "",
            "# In order to hook up a custom password store for all SQLALCHEMY connections",
            "# implement a function that takes a single argument of type 'sqla.engine.url',",
            "# returns a password and set SQLALCHEMY_CUSTOM_PASSWORD_STORE.",
            "#",
            "# e.g.:",
            "# def lookup_password(url):",
            "#     return 'secret'",
            "# SQLALCHEMY_CUSTOM_PASSWORD_STORE = lookup_password",
            "SQLALCHEMY_CUSTOM_PASSWORD_STORE = None",
            "",
            "#",
            "# The EncryptedFieldTypeAdapter is used whenever we're building SqlAlchemy models",
            "# which include sensitive fields that should be app-encrypted BEFORE sending",
            "# to the DB.",
            "#",
            "# Note: the default impl leverages SqlAlchemyUtils' EncryptedType, which defaults",
            "#  to AesEngine that uses AES-128 under the covers using the app's SECRET_KEY",
            "#  as key material. Do note that AesEngine allows for queryability over the",
            "#  encrypted fields.",
            "#",
            "#  To change the default engine you need to define your own adapter:",
            "#",
            "# e.g.:",
            "#",
            "# class AesGcmEncryptedAdapter(",
            "#     AbstractEncryptedFieldAdapter",
            "# ):",
            "#     def create(",
            "#         self,",
            "#         app_config: Optional[Dict[str, Any]],",
            "#         *args: List[Any],",
            "#         **kwargs: Optional[Dict[str, Any]],",
            "#     ) -> TypeDecorator:",
            "#         if app_config:",
            "#             return EncryptedType(",
            "#                 *args, app_config[\"SECRET_KEY\"], engine=AesGcmEngine, **kwargs",
            "#             )",
            "#         raise Exception(\"Missing app_config kwarg\")",
            "#",
            "#",
            "#  SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = AesGcmEncryptedAdapter",
            "SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = (  # pylint: disable=invalid-name",
            "    SQLAlchemyUtilsAdapter",
            ")",
            "# The limit of queries fetched for query search",
            "QUERY_SEARCH_LIMIT = 1000",
            "",
            "# Flask-WTF flag for CSRF",
            "WTF_CSRF_ENABLED = True",
            "",
            "# Add endpoints that need to be exempt from CSRF protection",
            "WTF_CSRF_EXEMPT_LIST = [",
            "    \"superset.views.core.log\",",
            "    \"superset.views.core.explore_json\",",
            "    \"superset.charts.data.api.data\",",
            "]",
            "",
            "# Whether to run the web server in debug mode or not",
            "DEBUG = os.environ.get(\"FLASK_DEBUG\")",
            "FLASK_USE_RELOAD = True",
            "",
            "# Enable profiling of Python calls. Turn this on and append ``?_instrument=1``",
            "# to the page to see the call stack.",
            "PROFILING = False",
            "",
            "# Superset allows server-side python stacktraces to be surfaced to the",
            "# user when this feature is on. This may have security implications",
            "# and it's more secure to turn it off in production settings.",
            "SHOW_STACKTRACE = False",
            "",
            "# Use all X-Forwarded headers when ENABLE_PROXY_FIX is True.",
            "# When proxying to a different port, set \"x_port\" to 0 to avoid downstream issues.",
            "ENABLE_PROXY_FIX = False",
            "PROXY_FIX_CONFIG = {\"x_for\": 1, \"x_proto\": 1, \"x_host\": 1, \"x_port\": 1, \"x_prefix\": 1}",
            "",
            "# Configuration for scheduling queries from SQL Lab.",
            "SCHEDULED_QUERIES: dict[str, Any] = {}",
            "",
            "# FAB Rate limiting: this is a security feature for preventing DDOS attacks. The",
            "# feature is on by default to make Superset secure by default, but you should",
            "# fine tune the limits to your needs. You can read more about the different",
            "# parameters here: https://flask-limiter.readthedocs.io/en/stable/configuration.html",
            "RATELIMIT_ENABLED = True",
            "RATELIMIT_APPLICATION = \"50 per second\"",
            "AUTH_RATE_LIMITED = True",
            "AUTH_RATE_LIMIT = \"5 per second\"",
            "# A storage location conforming to the scheme in storage-scheme. See the limits",
            "# library for allowed values: https://limits.readthedocs.io/en/stable/storage.html",
            "# RATELIMIT_STORAGE_URI = \"redis://host:port\"",
            "# A callable that returns the unique identity of the current request.",
            "# RATELIMIT_REQUEST_IDENTIFIER = flask.Request.endpoint",
            "",
            "# ------------------------------",
            "# GLOBALS FOR APP Builder",
            "# ------------------------------",
            "# Uncomment to setup Your App name",
            "APP_NAME = \"Superset\"",
            "",
            "# Specify the App icon",
            "APP_ICON = \"/static/assets/images/superset-logo-horiz.png\"",
            "",
            "# Specify where clicking the logo would take the user",
            "# e.g. setting it to '/' would take the user to '/superset/welcome/'",
            "LOGO_TARGET_PATH = None",
            "",
            "# Specify tooltip that should appear when hovering over the App Icon/Logo",
            "LOGO_TOOLTIP = \"\"",
            "",
            "# Specify any text that should appear to the right of the logo",
            "LOGO_RIGHT_TEXT: Callable[[], str] | str = \"\"",
            "",
            "# Enables SWAGGER UI for superset openapi spec",
            "# ex: http://localhost:8080/swagger/v1",
            "FAB_API_SWAGGER_UI = True",
            "",
            "# ----------------------------------------------------",
            "# AUTHENTICATION CONFIG",
            "# ----------------------------------------------------",
            "# The authentication type",
            "# AUTH_OID : Is for OpenID",
            "# AUTH_DB : Is for database (username/password)",
            "# AUTH_LDAP : Is for LDAP",
            "# AUTH_REMOTE_USER : Is for using REMOTE_USER from web server",
            "AUTH_TYPE = AUTH_DB",
            "",
            "# Uncomment to setup Full admin role name",
            "# AUTH_ROLE_ADMIN = 'Admin'",
            "",
            "# Uncomment to setup Public role name, no authentication needed",
            "# AUTH_ROLE_PUBLIC = 'Public'",
            "",
            "# Will allow user self registration",
            "# AUTH_USER_REGISTRATION = True",
            "",
            "# The default user self registration role",
            "# AUTH_USER_REGISTRATION_ROLE = \"Public\"",
            "",
            "# When using LDAP Auth, setup the LDAP server",
            "# AUTH_LDAP_SERVER = \"ldap://ldapserver.new\"",
            "",
            "# Uncomment to setup OpenID providers example for OpenID authentication",
            "# OPENID_PROVIDERS = [",
            "#    { 'name': 'Yahoo', 'url': 'https://open.login.yahoo.com/' },",
            "#    { 'name': 'Flickr', 'url': 'https://www.flickr.com/<username>' },",
            "",
            "# ---------------------------------------------------",
            "# Roles config",
            "# ---------------------------------------------------",
            "# Grant public role the same set of permissions as for a selected builtin role.",
            "# This is useful if one wants to enable anonymous users to view",
            "# dashboards. Explicit grant on specific datasets is still required.",
            "PUBLIC_ROLE_LIKE: str | None = None",
            "",
            "# ---------------------------------------------------",
            "# Babel config for translations",
            "# ---------------------------------------------------",
            "# Setup default language",
            "BABEL_DEFAULT_LOCALE = \"en\"",
            "# Your application default translation path",
            "BABEL_DEFAULT_FOLDER = \"superset/translations\"",
            "# The allowed translation for your app",
            "LANGUAGES = {",
            "    \"en\": {\"flag\": \"us\", \"name\": \"English\"},",
            "    \"es\": {\"flag\": \"es\", \"name\": \"Spanish\"},",
            "    \"it\": {\"flag\": \"it\", \"name\": \"Italian\"},",
            "    \"fr\": {\"flag\": \"fr\", \"name\": \"French\"},",
            "    \"zh\": {\"flag\": \"cn\", \"name\": \"Chinese\"},",
            "    \"ja\": {\"flag\": \"jp\", \"name\": \"Japanese\"},",
            "    \"de\": {\"flag\": \"de\", \"name\": \"German\"},",
            "    \"pt\": {\"flag\": \"pt\", \"name\": \"Portuguese\"},",
            "    \"pt_BR\": {\"flag\": \"br\", \"name\": \"Brazilian Portuguese\"},",
            "    \"ru\": {\"flag\": \"ru\", \"name\": \"Russian\"},",
            "    \"ko\": {\"flag\": \"kr\", \"name\": \"Korean\"},",
            "    \"sk\": {\"flag\": \"sk\", \"name\": \"Slovak\"},",
            "    \"sl\": {\"flag\": \"si\", \"name\": \"Slovenian\"},",
            "    \"nl\": {\"flag\": \"nl\", \"name\": \"Dutch\"},",
            "}",
            "# Turning off i18n by default as translation in most languages are",
            "# incomplete and not well maintained.",
            "LANGUAGES = {}",
            "",
            "",
            "# Override the default d3 locale format",
            "# Default values are equivalent to",
            "# D3_FORMAT = {",
            "#     \"decimal\": \".\",           # - decimal place string (e.g., \".\").",
            "#     \"thousands\": \",\",         # - group separator string (e.g., \",\").",
            "#     \"grouping\": [3],          # - array of group sizes (e.g., [3]), cycled as needed.",
            "#     \"currency\": [\"$\", \"\"]     # - currency prefix/suffix strings (e.g., [\"$\", \"\"])",
            "# }",
            "# https://github.com/d3/d3-format/blob/main/README.md#formatLocale",
            "class D3Format(TypedDict, total=False):",
            "    decimal: str",
            "    thousands: str",
            "    grouping: list[int]",
            "    currency: list[str]",
            "",
            "",
            "D3_FORMAT: D3Format = {}",
            "",
            "CURRENCIES = [\"USD\", \"EUR\", \"GBP\", \"INR\", \"MXN\", \"JPY\", \"CNY\"]",
            "",
            "# ---------------------------------------------------",
            "# Feature flags",
            "# ---------------------------------------------------",
            "# Feature flags that are set by default go here. Their values can be",
            "# overwritten by those specified under FEATURE_FLAGS in superset_config.py",
            "# For example, DEFAULT_FEATURE_FLAGS = { 'FOO': True, 'BAR': False } here",
            "# and FEATURE_FLAGS = { 'BAR': True, 'BAZ': True } in superset_config.py",
            "# will result in combined feature flags of { 'FOO': True, 'BAR': True, 'BAZ': True }",
            "DEFAULT_FEATURE_FLAGS: dict[str, bool] = {",
            "    # Experimental feature introducing a client (browser) cache",
            "    \"CLIENT_CACHE\": False,  # deprecated",
            "    \"DISABLE_DATASET_SOURCE_EDIT\": False,  # deprecated",
            "    # When using a recent version of Druid that supports JOINs turn this on",
            "    \"DRUID_JOINS\": False,",
            "    \"DYNAMIC_PLUGINS\": False,",
            "    # With Superset 2.0, we are updating the default so that the legacy datasource",
            "    # editor no longer shows. Currently this is set to false so that the editor",
            "    # option does show, but we will be depreciating it.",
            "    \"DISABLE_LEGACY_DATASOURCE_EDITOR\": True,",
            "    # For some security concerns, you may need to enforce CSRF protection on",
            "    # all query request to explore_json endpoint. In Superset, we use",
            "    # `flask-csrf <https://sjl.bitbucket.io/flask-csrf/>`_ add csrf protection",
            "    # for all POST requests, but this protection doesn't apply to GET method.",
            "    # When ENABLE_EXPLORE_JSON_CSRF_PROTECTION is set to true, your users cannot",
            "    # make GET request to explore_json. explore_json accepts both GET and POST request.",
            "    # See `PR 7935 <https://github.com/apache/superset/pull/7935>`_ for more details.",
            "    \"ENABLE_EXPLORE_JSON_CSRF_PROTECTION\": False,  # deprecated",
            "    \"ENABLE_TEMPLATE_PROCESSING\": False,",
            "    \"ENABLE_TEMPLATE_REMOVE_FILTERS\": True,  # deprecated",
            "    # Allow for javascript controls components",
            "    # this enables programmers to customize certain charts (like the",
            "    # geospatial ones) by inputting javascript in controls. This exposes",
            "    # an XSS security vulnerability",
            "    \"ENABLE_JAVASCRIPT_CONTROLS\": False,",
            "    \"KV_STORE\": False,",
            "    # When this feature is enabled, nested types in Presto will be",
            "    # expanded into extra columns and/or arrays. This is experimental,",
            "    # and doesn't work with all nested types.",
            "    \"PRESTO_EXPAND_DATA\": False,",
            "    # Exposes API endpoint to compute thumbnails",
            "    \"THUMBNAILS\": False,",
            "    \"DASHBOARD_CACHE\": False,  # deprecated",
            "    \"REMOVE_SLICE_LEVEL_LABEL_COLORS\": False,  # deprecated",
            "    \"SHARE_QUERIES_VIA_KV_STORE\": False,",
            "    \"TAGGING_SYSTEM\": False,",
            "    \"SQLLAB_BACKEND_PERSISTENCE\": True,",
            "    \"LISTVIEWS_DEFAULT_CARD_VIEW\": False,",
            "    # When True, this escapes HTML (rather than rendering it) in Markdown components",
            "    \"ESCAPE_MARKDOWN_HTML\": False,",
            "    \"DASHBOARD_NATIVE_FILTERS\": True,  # deprecated",
            "    \"DASHBOARD_CROSS_FILTERS\": True,",
            "    # Feature is under active development and breaking changes are expected",
            "    \"DASHBOARD_NATIVE_FILTERS_SET\": False,  # deprecated",
            "    \"DASHBOARD_FILTERS_EXPERIMENTAL\": False,  # deprecated",
            "    \"DASHBOARD_VIRTUALIZATION\": False,",
            "    \"GLOBAL_ASYNC_QUERIES\": False,",
            "    \"VERSIONED_EXPORT\": True,  # deprecated",
            "    \"EMBEDDED_SUPERSET\": False,",
            "    # Enables Alerts and reports new implementation",
            "    \"ALERT_REPORTS\": False,",
            "    \"DASHBOARD_RBAC\": False,",
            "    \"ENABLE_EXPLORE_DRAG_AND_DROP\": True,  # deprecated",
            "    \"ENABLE_ADVANCED_DATA_TYPES\": False,",
            "    # Enabling ALERTS_ATTACH_REPORTS, the system sends email and slack message",
            "    # with screenshot and link",
            "    # Disables ALERTS_ATTACH_REPORTS, the system DOES NOT generate screenshot",
            "    # for report with type 'alert' and sends email and slack message with only link;",
            "    # for report with type 'report' still send with email and slack message with",
            "    # screenshot and link",
            "    \"ALERTS_ATTACH_REPORTS\": True,",
            "    # Allow users to export full CSV of table viz type.",
            "    # This could cause the server to run out of memory or compute.",
            "    \"ALLOW_FULL_CSV_EXPORT\": False,",
            "    \"GENERIC_CHART_AXES\": True,  # deprecated",
            "    \"ALLOW_ADHOC_SUBQUERY\": False,",
            "    \"USE_ANALAGOUS_COLORS\": False,",
            "    # Apply RLS rules to SQL Lab queries. This requires parsing and manipulating the",
            "    # query, and might break queries and/or allow users to bypass RLS. Use with care!",
            "    \"RLS_IN_SQLLAB\": False,",
            "    # Enable caching per impersonation key (e.g username) in a datasource where user",
            "    # impersonation is enabled",
            "    \"CACHE_IMPERSONATION\": False,",
            "    # Enable caching per user key for Superset cache (not database cache impersonation)",
            "    \"CACHE_QUERY_BY_USER\": False,",
            "    # Enable sharing charts with embedding",
            "    \"EMBEDDABLE_CHARTS\": True,",
            "    \"DRILL_TO_DETAIL\": True,",
            "    \"DRILL_BY\": False,",
            "    \"DATAPANEL_CLOSED_BY_DEFAULT\": False,",
            "    \"HORIZONTAL_FILTER_BAR\": False,",
            "    # The feature is off by default, and currently only supported in Presto and Postgres,",
            "    # and Bigquery.",
            "    # It also needs to be enabled on a per-database basis, by adding the key/value pair",
            "    # `cost_estimate_enabled: true` to the database `extra` attribute.",
            "    \"ESTIMATE_QUERY_COST\": False,",
            "    # Allow users to enable ssh tunneling when creating a DB.",
            "    # Users must check whether the DB engine supports SSH Tunnels",
            "    # otherwise enabling this flag won't have any effect on the DB.",
            "    \"SSH_TUNNELING\": False,",
            "    \"AVOID_COLORS_COLLISION\": True,",
            "    # Set to False to only allow viewing own recent activity",
            "    # or to disallow users from viewing other users profile page",
            "    # Do not show user info or profile in the menu",
            "    \"MENU_HIDE_USER_INFO\": False,",
            "    # Allows users to add a ``superset://`` DB that can query across databases. This is",
            "    # an experimental feature with potential security and performance risks, so use with",
            "    # caution. If the feature is enabled you can also set a limit for how much data is",
            "    # returned from each database in the ``SUPERSET_META_DB_LIMIT`` configuration value",
            "    # in this file.",
            "    \"ENABLE_SUPERSET_META_DB\": False,",
            "    # Set to True to replace Selenium with Playwright to execute reports and thumbnails.",
            "    # Unlike Selenium, Playwright reports support deck.gl visualizations",
            "    # Enabling this feature flag requires installing \"playwright\" pip package",
            "    \"PLAYWRIGHT_REPORTS_AND_THUMBNAILS\": False,",
            "}",
            "",
            "# ------------------------------",
            "# SSH Tunnel",
            "# ------------------------------",
            "# Allow users to set the host used when connecting to the SSH Tunnel",
            "# as localhost and any other alias (0.0.0.0)",
            "# ----------------------------------------------------------------------",
            "#                             |",
            "# -------------+              |    +----------+",
            "#     LOCAL    |              |    |  REMOTE  | :22 SSH",
            "#     CLIENT   | <== SSH ========> |  SERVER  | :8080 web service",
            "# -------------+              |    +----------+",
            "#                             |",
            "#                          FIREWALL (only port 22 is open)",
            "",
            "# ----------------------------------------------------------------------",
            "SSH_TUNNEL_MANAGER_CLASS = \"superset.extensions.ssh.SSHManager\"",
            "SSH_TUNNEL_LOCAL_BIND_ADDRESS = \"127.0.0.1\"",
            "#: Timeout (seconds) for tunnel connection (open_channel timeout)",
            "SSH_TUNNEL_TIMEOUT_SEC = 10.0",
            "#: Timeout (seconds) for transport socket (``socket.settimeout``)",
            "SSH_TUNNEL_PACKET_TIMEOUT_SEC = 1.0",
            "",
            "",
            "# Feature flags may also be set via 'SUPERSET_FEATURE_' prefixed environment vars.",
            "DEFAULT_FEATURE_FLAGS.update(",
            "    {",
            "        k[len(\"SUPERSET_FEATURE_\") :]: parse_boolean_string(v)",
            "        for k, v in os.environ.items()",
            "        if re.search(r\"^SUPERSET_FEATURE_\\w+\", k)",
            "    }",
            ")",
            "",
            "# This is merely a default.",
            "FEATURE_FLAGS: dict[str, bool] = {}",
            "",
            "# A function that receives a dict of all feature flags",
            "# (DEFAULT_FEATURE_FLAGS merged with FEATURE_FLAGS)",
            "# can alter it, and returns a similar dict. Note the dict of feature",
            "# flags passed to the function is a deepcopy of the dict in the config,",
            "# and can therefore be mutated without side-effect",
            "#",
            "# GET_FEATURE_FLAGS_FUNC can be used to implement progressive rollouts,",
            "# role-based features, or a full on A/B testing framework.",
            "#",
            "# from flask import g, request",
            "# def GET_FEATURE_FLAGS_FUNC(feature_flags_dict: Dict[str, bool]) -> Dict[str, bool]:",
            "#     if hasattr(g, \"user\") and g.user.is_active:",
            "#         feature_flags_dict['some_feature'] = g.user and g.user.get_id() == 5",
            "#     return feature_flags_dict",
            "GET_FEATURE_FLAGS_FUNC: Callable[[dict[str, bool]], dict[str, bool]] | None = None",
            "# A function that receives a feature flag name and an optional default value.",
            "# Has a similar utility to GET_FEATURE_FLAGS_FUNC but it's useful to not force the",
            "# evaluation of all feature flags when just evaluating a single one.",
            "#",
            "# Note that the default `get_feature_flags` will evaluate each feature with this",
            "# callable when the config key is set, so don't use both GET_FEATURE_FLAGS_FUNC",
            "# and IS_FEATURE_ENABLED_FUNC in conjunction.",
            "IS_FEATURE_ENABLED_FUNC: Callable[[str, bool | None], bool] | None = None",
            "# A function that expands/overrides the frontend `bootstrap_data.common` object.",
            "# Can be used to implement custom frontend functionality,",
            "# or dynamically change certain configs.",
            "#",
            "# Values in `bootstrap_data.common` should have these characteristics:",
            "# - They are not specific to a page the user is visiting",
            "# - They do not contain secrets",
            "#",
            "# Takes as a parameter the common bootstrap payload before transformations.",
            "# Returns a dict containing data that should be added or overridden to the payload.",
            "COMMON_BOOTSTRAP_OVERRIDES_FUNC: Callable[",
            "    [dict[str, Any]], dict[str, Any]",
            "] = lambda data: {}  # default: empty dict",
            "",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES is used for adding custom categorical color schemes",
            "# example code for \"My custom warm to hot\" color scheme",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES = [",
            "#     {",
            "#         \"id\": 'myVisualizationColors',",
            "#         \"description\": '',",
            "#         \"label\": 'My Visualization Colors',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77',",
            "#          '#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_CATEGORICAL_COLOR_SCHEMES: list[dict[str, Any]] = []",
            "",
            "# THEME_OVERRIDES is used for adding custom theme to superset",
            "# example code for \"My theme\" custom scheme",
            "# THEME_OVERRIDES = {",
            "#   \"borderRadius\": 4,",
            "#   \"colors\": {",
            "#     \"primary\": {",
            "#       \"base\": 'red',",
            "#     },",
            "#     \"secondary\": {",
            "#       \"base\": 'green',",
            "#     },",
            "#     \"grayscale\": {",
            "#       \"base\": 'orange',",
            "#     }",
            "#   }",
            "# }",
            "",
            "THEME_OVERRIDES: dict[str, Any] = {}",
            "",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES is used for adding custom sequential color schemes",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES =  [",
            "#     {",
            "#         \"id\": 'warmToHot',",
            "#         \"description\": '',",
            "#         \"isDiverging\": True,",
            "#         \"label\": 'My custom warm to hot',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD',",
            "#          '#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_SEQUENTIAL_COLOR_SCHEMES: list[dict[str, Any]] = []",
            "",
            "# ---------------------------------------------------",
            "# Thumbnail config (behind feature flag)",
            "# ---------------------------------------------------",
            "# By default, thumbnails are rendered per user, and will fall back to the Selenium",
            "# user for anonymous users. Similar to Alerts & Reports, thumbnails",
            "# can be configured to always be rendered as a fixed user. See",
            "# `superset.tasks.types.ExecutorType` for a full list of executor options.",
            "# To always use a fixed user account, use the following configuration:",
            "# THUMBNAIL_EXECUTE_AS = [ExecutorType.SELENIUM]",
            "THUMBNAIL_SELENIUM_USER: str | None = \"admin\"",
            "THUMBNAIL_EXECUTE_AS = [ExecutorType.CURRENT_USER, ExecutorType.SELENIUM]",
            "",
            "# By default, thumbnail digests are calculated based on various parameters in the",
            "# chart/dashboard metadata, and in the case of user-specific thumbnails, the",
            "# username. To specify a custom digest function, use the following config parameters",
            "# to define callbacks that receive",
            "# 1. the model (dashboard or chart)",
            "# 2. the executor type (e.g. ExecutorType.SELENIUM)",
            "# 3. the executor's username (note, this is the executor as defined by",
            "# `THUMBNAIL_EXECUTE_AS`; the executor is only equal to the currently logged in",
            "# user if the executor type is equal to `ExecutorType.CURRENT_USER`)",
            "# and return the final digest string:",
            "THUMBNAIL_DASHBOARD_DIGEST_FUNC: None | (",
            "    Callable[[Dashboard, ExecutorType, str], str]",
            ") = None",
            "THUMBNAIL_CHART_DIGEST_FUNC: Callable[[Slice, ExecutorType, str], str] | None = None",
            "",
            "THUMBNAIL_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"NullCache\",",
            "    \"CACHE_NO_NULL_WARNING\": True,",
            "}",
            "",
            "# Time before selenium times out after trying to locate an element on the page and wait",
            "# for that element to load for a screenshot.",
            "SCREENSHOT_LOCATE_WAIT = int(timedelta(seconds=10).total_seconds())",
            "# Time before selenium times out after waiting for all DOM class elements named",
            "# \"loading\" are gone.",
            "SCREENSHOT_LOAD_WAIT = int(timedelta(minutes=1).total_seconds())",
            "# Selenium destroy retries",
            "SCREENSHOT_SELENIUM_RETRIES = 5",
            "# Give selenium an headstart, in seconds",
            "SCREENSHOT_SELENIUM_HEADSTART = 3",
            "# Wait for the chart animation, in seconds",
            "SCREENSHOT_SELENIUM_ANIMATION_WAIT = 5",
            "# Replace unexpected errors in screenshots with real error messages",
            "SCREENSHOT_REPLACE_UNEXPECTED_ERRORS = False",
            "# Max time to wait for error message modal to show up, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_VISIBLE = 5",
            "# Max time to wait for error message modal to close, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_INVISIBLE = 5",
            "",
            "# ---------------------------------------------------",
            "# Image and file configuration",
            "# ---------------------------------------------------",
            "# The file upload folder, when using models with files",
            "UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "UPLOAD_CHUNK_SIZE = 4096",
            "",
            "# The image upload folder, when using models with images",
            "IMG_UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "",
            "# The image upload url, when using models with images",
            "IMG_UPLOAD_URL = \"/static/uploads/\"",
            "# Setup image size default is (300, 200, True)",
            "# IMG_SIZE = (300, 200, True)",
            "",
            "# Default cache timeout, applies to all cache backends unless specifically overridden in",
            "# each cache config.",
            "CACHE_DEFAULT_TIMEOUT = int(timedelta(days=1).total_seconds())",
            "",
            "# Default cache for Superset objects",
            "CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for datasource metadata and query results",
            "DATA_CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for dashboard filter state. `CACHE_TYPE` defaults to `SupersetMetastoreCache`",
            "# that stores the values in the key-value table in the Superset metastore, as it's",
            "# required for Superset to operate correctly, but can be replaced by any",
            "# `Flask-Caching` backend.",
            "FILTER_STATE_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"SupersetMetastoreCache\",",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=90).total_seconds()),",
            "    # Should the timeout be reset when retrieving a cached value?",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "    # The following parameter only applies to `MetastoreCache`:",
            "    # How should entries be serialized/deserialized?",
            "    \"CODEC\": JsonKeyValueCodec(),",
            "}",
            "",
            "# Cache for explore form data state. `CACHE_TYPE` defaults to `SupersetMetastoreCache`",
            "# that stores the values in the key-value table in the Superset metastore, as it's",
            "# required for Superset to operate correctly, but can be replaced by any",
            "# `Flask-Caching` backend.",
            "EXPLORE_FORM_DATA_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"SupersetMetastoreCache\",",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=7).total_seconds()),",
            "    # Should the timeout be reset when retrieving a cached value?",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "    # The following parameter only applies to `MetastoreCache`:",
            "    # How should entries be serialized/deserialized?",
            "    \"CODEC\": JsonKeyValueCodec(),",
            "}",
            "",
            "# store cache keys by datasource UID (via CacheKey) for custom processing/invalidation",
            "STORE_CACHE_KEYS_IN_METADATA_DB = False",
            "",
            "# CORS Options",
            "ENABLE_CORS = False",
            "CORS_OPTIONS: dict[Any, Any] = {}",
            "",
            "# Sanitizes the HTML content used in markdowns to allow its rendering in a safe manner.",
            "# Disabling this option is not recommended for security reasons. If you wish to allow",
            "# valid safe elements that are not included in the default sanitization schema, use the",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS configuration.",
            "HTML_SANITIZATION = True",
            "",
            "# Use this configuration to extend the HTML sanitization schema.",
            "# By default we use the GitHub schema defined in",
            "# https://github.com/syntax-tree/hast-util-sanitize/blob/main/lib/schema.js",
            "# For example, the following configuration would allow the rendering of the",
            "# style attribute for div elements and the ftp protocol in hrefs:",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS = {",
            "#   \"attributes\": {",
            "#     \"div\": [\"style\"],",
            "#   },",
            "#   \"protocols\": {",
            "#     \"href\": [\"ftp\"],",
            "#   }",
            "# }",
            "# Be careful when extending the default schema to avoid XSS attacks.",
            "HTML_SANITIZATION_SCHEMA_EXTENSIONS: dict[str, Any] = {}",
            "",
            "# Chrome allows up to 6 open connections per domain at a time. When there are more",
            "# than 6 slices in dashboard, a lot of time fetch requests are queued up and wait for",
            "# next available socket. PR #5039 is trying to allow domain sharding for Superset,",
            "# and this feature will be enabled by configuration only (by default Superset",
            "# doesn't allow cross-domain request).",
            "SUPERSET_WEBSERVER_DOMAINS = None",
            "",
            "# Allowed format types for upload on Database view",
            "EXCEL_EXTENSIONS = {\"xlsx\", \"xls\"}",
            "CSV_EXTENSIONS = {\"csv\", \"tsv\", \"txt\"}",
            "COLUMNAR_EXTENSIONS = {\"parquet\", \"zip\"}",
            "ALLOWED_EXTENSIONS = {*EXCEL_EXTENSIONS, *CSV_EXTENSIONS, *COLUMNAR_EXTENSIONS}",
            "",
            "# Optional maximum file size in bytes when uploading a CSV",
            "CSV_UPLOAD_MAX_SIZE = None",
            "",
            "# CSV Options: key/value pairs that will be passed as argument to DataFrame.to_csv",
            "# method.",
            "# note: index option should not be overridden",
            "CSV_EXPORT = {\"encoding\": \"utf-8\"}",
            "",
            "# Excel Options: key/value pairs that will be passed as argument to DataFrame.to_excel",
            "# method.",
            "# note: index option should not be overridden",
            "EXCEL_EXPORT: dict[str, Any] = {}",
            "",
            "# ---------------------------------------------------",
            "# Time grain configurations",
            "# ---------------------------------------------------",
            "# List of time grains to disable in the application (see list of builtin",
            "# time grains in superset/db_engine_specs/base.py).",
            "# For example: to disable 1 second time grain:",
            "# TIME_GRAIN_DENYLIST = ['PT1S']",
            "TIME_GRAIN_DENYLIST: list[str] = []",
            "",
            "# Additional time grains to be supported using similar definitions as in",
            "# superset/db_engine_specs/base.py.",
            "# For example: To add a new 2 second time grain:",
            "# TIME_GRAIN_ADDONS = {'PT2S': '2 second'}",
            "TIME_GRAIN_ADDONS: dict[str, str] = {}",
            "",
            "# Implementation of additional time grains per engine.",
            "# The column to be truncated is denoted `{col}` in the expression.",
            "# For example: To implement 2 second time grain on clickhouse engine:",
            "# TIME_GRAIN_ADDON_EXPRESSIONS = {",
            "#     'clickhouse': {",
            "#         'PT2S': 'toDateTime(intDiv(toUInt32(toDateTime({col})), 2)*2)'",
            "#     }",
            "# }",
            "TIME_GRAIN_ADDON_EXPRESSIONS: dict[str, dict[str, str]] = {}",
            "",
            "# Map of custom time grains and artificial join column producers used",
            "# when generating the join key between results and time shifts.",
            "# See superset/common/query_context_processor.get_aggregated_join_column",
            "#",
            "# Example of a join column producer that aggregates by fiscal year",
            "# def join_producer(row: Series, column_index: int) -> str:",
            "#    return row[index].strftime(\"%F\")",
            "#",
            "# TIME_GRAIN_JOIN_COLUMN_PRODUCERS = {\"P1F\": join_producer}",
            "TIME_GRAIN_JOIN_COLUMN_PRODUCERS: dict[str, Callable[[Series, int], str]] = {}",
            "",
            "# ---------------------------------------------------",
            "# List of viz_types not allowed in your environment",
            "# For example: Disable pivot table and treemap:",
            "#  VIZ_TYPE_DENYLIST = ['pivot_table', 'treemap']",
            "# ---------------------------------------------------",
            "",
            "VIZ_TYPE_DENYLIST: list[str] = []",
            "",
            "# --------------------------------------------------",
            "# Modules, datasources and middleware to be registered",
            "# --------------------------------------------------",
            "DEFAULT_MODULE_DS_MAP = OrderedDict(",
            "    [",
            "        (\"superset.connectors.sqla.models\", [\"SqlaTable\"]),",
            "    ]",
            ")",
            "ADDITIONAL_MODULE_DS_MAP: dict[str, list[str]] = {}",
            "ADDITIONAL_MIDDLEWARE: list[Callable[..., Any]] = []",
            "",
            "# 1) https://docs.python-guide.org/writing/logging/",
            "# 2) https://docs.python.org/2/library/logging.config.html",
            "",
            "# Default configurator will consume the LOG_* settings below",
            "LOGGING_CONFIGURATOR = DefaultLoggingConfigurator()",
            "",
            "# Console Log Settings",
            "",
            "LOG_FORMAT = \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\"",
            "LOG_LEVEL = \"DEBUG\"",
            "",
            "# ---------------------------------------------------",
            "# Enable Time Rotate Log Handler",
            "# ---------------------------------------------------",
            "# LOG_LEVEL = DEBUG, INFO, WARNING, ERROR, CRITICAL",
            "",
            "ENABLE_TIME_ROTATE = False",
            "TIME_ROTATE_LOG_LEVEL = \"DEBUG\"",
            "FILENAME = os.path.join(DATA_DIR, \"superset.log\")",
            "ROLLOVER = \"midnight\"",
            "INTERVAL = 1",
            "BACKUP_COUNT = 30",
            "",
            "# Custom logger for auditing queries. This can be used to send ran queries to a",
            "# structured immutable store for auditing purposes. The function is called for",
            "# every query ran, in both SQL Lab and charts/dashboards.",
            "# def QUERY_LOGGER(",
            "#     database,",
            "#     query,",
            "#     schema=None,",
            "#     client=None,",
            "#     security_manager=None,",
            "#     log_params=None,",
            "# ):",
            "#     pass",
            "QUERY_LOGGER = None",
            "",
            "# Set this API key to enable Mapbox visualizations",
            "MAPBOX_API_KEY = os.environ.get(\"MAPBOX_API_KEY\", \"\")",
            "",
            "# Maximum number of rows returned for any analytical database query",
            "SQL_MAX_ROW = 100000",
            "",
            "# Maximum number of rows displayed in SQL Lab UI",
            "# Is set to avoid out of memory/localstorage issues in browsers. Does not affect",
            "# exported CSVs",
            "DISPLAY_MAX_ROW = 10000",
            "",
            "# Default row limit for SQL Lab queries. Is overridden by setting a new limit in",
            "# the SQL Lab UI",
            "DEFAULT_SQLLAB_LIMIT = 1000",
            "",
            "# The limit for the Superset Meta DB when the feature flag ENABLE_SUPERSET_META_DB is on",
            "SUPERSET_META_DB_LIMIT: int | None = 1000",
            "",
            "# Adds a warning message on sqllab save query and schedule query modals.",
            "SQLLAB_SAVE_WARNING_MESSAGE = None",
            "SQLLAB_SCHEDULE_WARNING_MESSAGE = None",
            "",
            "# Force refresh while auto-refresh in dashboard",
            "DASHBOARD_AUTO_REFRESH_MODE: Literal[\"fetch\", \"force\"] = \"force\"",
            "# Dashboard auto refresh intervals",
            "DASHBOARD_AUTO_REFRESH_INTERVALS = [",
            "    [0, \"Don't refresh\"],",
            "    [10, \"10 seconds\"],",
            "    [30, \"30 seconds\"],",
            "    [60, \"1 minute\"],",
            "    [300, \"5 minutes\"],",
            "    [1800, \"30 minutes\"],",
            "    [3600, \"1 hour\"],",
            "    [21600, \"6 hours\"],",
            "    [43200, \"12 hours\"],",
            "    [86400, \"24 hours\"],",
            "]",
            "",
            "# This is used as a workaround for the alerts & reports scheduler task to get the time",
            "# celery beat triggered it, see https://github.com/celery/celery/issues/6974 for details",
            "CELERY_BEAT_SCHEDULER_EXPIRES = timedelta(weeks=1)",
            "",
            "# Default celery config is to use SQLA as a broker, in a production setting",
            "# you'll want to use a proper broker as specified here:",
            "# https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/index.html",
            "",
            "",
            "class CeleryConfig:  # pylint: disable=too-few-public-methods",
            "    broker_url = \"sqla+sqlite:///celerydb.sqlite\"",
            "    imports = (\"superset.sql_lab\",)",
            "    result_backend = \"db+sqlite:///celery_results.sqlite\"",
            "    worker_prefetch_multiplier = 1",
            "    task_acks_late = False",
            "    task_annotations = {",
            "        \"sql_lab.get_sql_results\": {\"rate_limit\": \"100/s\"},",
            "        \"email_reports.send\": {",
            "            \"rate_limit\": \"1/s\",",
            "            \"time_limit\": int(timedelta(seconds=120).total_seconds()),",
            "            \"soft_time_limit\": int(timedelta(seconds=150).total_seconds()),",
            "            \"ignore_result\": True,",
            "        },",
            "    }",
            "    beat_schedule = {",
            "        \"email_reports.schedule_hourly\": {",
            "            \"task\": \"email_reports.schedule_hourly\",",
            "            \"schedule\": crontab(minute=1, hour=\"*\"),",
            "        },",
            "        \"reports.scheduler\": {",
            "            \"task\": \"reports.scheduler\",",
            "            \"schedule\": crontab(minute=\"*\", hour=\"*\"),",
            "            \"options\": {\"expires\": int(CELERY_BEAT_SCHEDULER_EXPIRES.total_seconds())},",
            "        },",
            "        \"reports.prune_log\": {",
            "            \"task\": \"reports.prune_log\",",
            "            \"schedule\": crontab(minute=0, hour=0),",
            "        },",
            "    }",
            "",
            "",
            "CELERY_CONFIG = CeleryConfig  # pylint: disable=invalid-name",
            "",
            "# Set celery config to None to disable all the above configuration",
            "# CELERY_CONFIG = None",
            "",
            "# Additional static HTTP headers to be served by your Superset server. Note",
            "# Flask-Talisman applies the relevant security HTTP headers.",
            "#",
            "# DEFAULT_HTTP_HEADERS: sets default values for HTTP headers. These may be overridden",
            "# within the app",
            "# OVERRIDE_HTTP_HEADERS: sets override values for HTTP headers. These values will",
            "# override anything set within the app",
            "DEFAULT_HTTP_HEADERS: dict[str, Any] = {}",
            "OVERRIDE_HTTP_HEADERS: dict[str, Any] = {}",
            "HTTP_HEADERS: dict[str, Any] = {}",
            "",
            "# The db id here results in selecting this one as a default in SQL Lab",
            "DEFAULT_DB_ID = None",
            "",
            "# Timeout duration for SQL Lab synchronous queries",
            "SQLLAB_TIMEOUT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Timeout duration for SQL Lab query validation",
            "SQLLAB_VALIDATION_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# SQLLAB_DEFAULT_DBID",
            "SQLLAB_DEFAULT_DBID = None",
            "",
            "# The MAX duration a query can run for before being killed by celery.",
            "SQLLAB_ASYNC_TIME_LIMIT_SEC = int(timedelta(hours=6).total_seconds())",
            "",
            "# Some databases support running EXPLAIN queries that allow users to estimate",
            "# query costs before they run. These EXPLAIN queries should have a small",
            "# timeout.",
            "SQLLAB_QUERY_COST_ESTIMATE_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# The cost returned by the databases is a relative value; in order to map the cost to",
            "# a tangible value you need to define a custom formatter that takes into consideration",
            "# your specific infrastructure. For example, you could analyze queries a posteriori by",
            "# running EXPLAIN on them, and compute a histogram of relative costs to present the",
            "# cost as a percentile, this step is optional as every db engine spec has its own",
            "# query cost formatter, but it you wanna customize it you can define it inside the config:",
            "",
            "# def postgres_query_cost_formatter(",
            "#     result: List[Dict[str, Any]]",
            "# ) -> List[Dict[str, str]]:",
            "#     # 25, 50, 75% percentiles",
            "#     percentile_costs = [100.0, 1000.0, 10000.0]",
            "#",
            "#     out = []",
            "#     for row in result:",
            "#         relative_cost = row[\"Total cost\"]",
            "#         percentile = bisect.bisect_left(percentile_costs, relative_cost) + 1",
            "#         out.append({",
            "#             \"Relative cost\": relative_cost,",
            "#             \"Percentile\": str(percentile * 25) + \"%\",",
            "#         })",
            "#",
            "#     return out",
            "#",
            "# QUERY_COST_FORMATTERS_BY_ENGINE: {\"postgresql\": postgres_query_cost_formatter}",
            "QUERY_COST_FORMATTERS_BY_ENGINE: dict[",
            "    str, Callable[[list[dict[str, Any]]], list[dict[str, Any]]]",
            "] = {}",
            "",
            "# Flag that controls if limit should be enforced on the CTA (create table as queries).",
            "SQLLAB_CTAS_NO_LIMIT = False",
            "",
            "# This allows you to define custom logic around the \"CREATE TABLE AS\" or CTAS feature",
            "# in SQL Lab that defines where the target schema should be for a given user.",
            "# Database `CTAS Schema` has a precedence over this setting.",
            "# Example below returns a username and CTA queries will write tables into the schema",
            "# name `username`",
            "# SQLLAB_CTAS_SCHEMA_NAME_FUNC = lambda database, user, schema, sql: user.username",
            "# This is move involved example where depending on the database you can leverage data",
            "# available to assign schema for the CTA query:",
            "# def compute_schema_name(database: Database, user: User, schema: str, sql: str) -> str:",
            "#     if database.name == 'mysql_payments_slave':",
            "#         return 'tmp_superset_schema'",
            "#     if database.name == 'presto_gold':",
            "#         return user.username",
            "#     if database.name == 'analytics':",
            "#         if 'analytics' in [r.name for r in user.roles]:",
            "#             return 'analytics_cta'",
            "#         else:",
            "#             return f'tmp_{schema}'",
            "# Function accepts database object, user object, schema name and sql that will be run.",
            "SQLLAB_CTAS_SCHEMA_NAME_FUNC: None | (",
            "    Callable[[Database, models.User, str, str], str]",
            ") = None",
            "",
            "# If enabled, it can be used to store the results of long-running queries",
            "# in SQL Lab by using the \"Run Async\" button/feature",
            "RESULTS_BACKEND: BaseCache | None = None",
            "",
            "# Use PyArrow and MessagePack for async query results serialization,",
            "# rather than JSON. This feature requires additional testing from the",
            "# community before it is fully adopted, so this config option is provided",
            "# in order to disable should breaking issues be discovered.",
            "RESULTS_BACKEND_USE_MSGPACK = True",
            "",
            "# The S3 bucket where you want to store your external hive tables created",
            "# from CSV files. For example, 'companyname-superset'",
            "CSV_TO_HIVE_UPLOAD_S3_BUCKET = None",
            "",
            "# The directory within the bucket specified above that will",
            "# contain all the external tables",
            "CSV_TO_HIVE_UPLOAD_DIRECTORY = \"EXTERNAL_HIVE_TABLES/\"",
            "",
            "",
            "# Function that creates upload directory dynamically based on the",
            "# database used, user and schema provided.",
            "def CSV_TO_HIVE_UPLOAD_DIRECTORY_FUNC(  # pylint: disable=invalid-name",
            "    database: Database,",
            "    user: models.User,  # pylint: disable=unused-argument",
            "    schema: str | None,",
            ") -> str:",
            "    # Note the final empty path enforces a trailing slash.",
            "    return os.path.join(",
            "        CSV_TO_HIVE_UPLOAD_DIRECTORY, str(database.id), schema or \"\", \"\"",
            "    )",
            "",
            "",
            "# The namespace within hive where the tables created from",
            "# uploading CSVs will be stored.",
            "UPLOADED_CSV_HIVE_NAMESPACE: str | None = None",
            "",
            "# Function that computes the allowed schemas for the CSV uploads.",
            "# Allowed schemas will be a union of schemas_allowed_for_file_upload",
            "# db configuration and a result of this function.",
            "",
            "# mypy doesn't catch that if case ensures list content being always str",
            "ALLOWED_USER_CSV_SCHEMA_FUNC: Callable[[Database, models.User], list[str]] = (",
            "    lambda database, user: [UPLOADED_CSV_HIVE_NAMESPACE]",
            "    if UPLOADED_CSV_HIVE_NAMESPACE",
            "    else []",
            ")",
            "",
            "# Values that should be treated as nulls for the csv uploads.",
            "CSV_DEFAULT_NA_NAMES = list(STR_NA_VALUES)",
            "",
            "# A dictionary of items that gets merged into the Jinja context for",
            "# SQL Lab. The existing context gets updated with this dictionary,",
            "# meaning values for existing keys get overwritten by the content of this",
            "# dictionary. Exposing functionality through JINJA_CONTEXT_ADDONS has security",
            "# implications as it opens a window for a user to execute untrusted code.",
            "# It's important to make sure that the objects exposed (as well as objects attached",
            "# to those objects) are harmless. We recommend only exposing simple/pure functions that",
            "# return native types.",
            "JINJA_CONTEXT_ADDONS: dict[str, Callable[..., Any]] = {}",
            "",
            "# A dictionary of macro template processors (by engine) that gets merged into global",
            "# template processors. The existing template processors get updated with this",
            "# dictionary, which means the existing keys get overwritten by the content of this",
            "# dictionary. The customized addons don't necessarily need to use Jinja templating",
            "# language. This allows you to define custom logic to process templates on a per-engine",
            "# basis. Example value = `{\"presto\": CustomPrestoTemplateProcessor}`",
            "CUSTOM_TEMPLATE_PROCESSORS: dict[str, type[BaseTemplateProcessor]] = {}",
            "",
            "# Roles that are controlled by the API / Superset and should not be changed",
            "# by humans.",
            "ROBOT_PERMISSION_ROLES = [\"Public\", \"Gamma\", \"Alpha\", \"Admin\", \"sql_lab\"]",
            "",
            "CONFIG_PATH_ENV_VAR = \"SUPERSET_CONFIG_PATH\"",
            "",
            "# If a callable is specified, it will be called at app startup while passing",
            "# a reference to the Flask app. This can be used to alter the Flask app",
            "# in whatever way.",
            "# example: FLASK_APP_MUTATOR = lambda x: x.before_request = f",
            "FLASK_APP_MUTATOR = None",
            "",
            "# smtp server configuration",
            "EMAIL_NOTIFICATIONS = False  # all the emails are sent using dryrun",
            "SMTP_HOST = \"localhost\"",
            "SMTP_STARTTLS = True",
            "SMTP_SSL = False",
            "SMTP_USER = \"superset\"",
            "SMTP_PORT = 25",
            "SMTP_PASSWORD = \"superset\"",
            "SMTP_MAIL_FROM = \"superset@superset.com\"",
            "# If True creates a default SSL context with ssl.Purpose.CLIENT_AUTH using the",
            "# default system root CA certificates.",
            "SMTP_SSL_SERVER_AUTH = False",
            "ENABLE_CHUNK_ENCODING = False",
            "",
            "# Whether to bump the logging level to ERROR on the flask_appbuilder package",
            "# Set to False if/when debugging FAB related issues like",
            "# permission management",
            "SILENCE_FAB = True",
            "",
            "FAB_ADD_SECURITY_VIEWS = True",
            "FAB_ADD_SECURITY_PERMISSION_VIEW = False",
            "FAB_ADD_SECURITY_VIEW_MENU_VIEW = False",
            "FAB_ADD_SECURITY_PERMISSION_VIEWS_VIEW = False",
            "",
            "# The link to a page containing common errors and their resolutions",
            "# It will be appended at the bottom of sql_lab errors.",
            "TROUBLESHOOTING_LINK = \"\"",
            "",
            "# CSRF token timeout, set to None for a token that never expires",
            "WTF_CSRF_TIME_LIMIT = int(timedelta(weeks=1).total_seconds())",
            "",
            "# This link should lead to a page with instructions on how to gain access to a",
            "# Datasource. It will be placed at the bottom of permissions errors.",
            "PERMISSION_INSTRUCTIONS_LINK = \"\"",
            "",
            "# Integrate external Blueprints to the app by passing them to your",
            "# configuration. These blueprints will get integrated in the app",
            "BLUEPRINTS: list[Blueprint] = []",
            "",
            "# Provide a callable that receives a tracking_url and returns another",
            "# URL. This is used to translate internal Hadoop job tracker URL",
            "# into a proxied one",
            "",
            "",
            "# Transform SQL query tracking url for Hive and Presto engines. You may also",
            "# access information about the query itself by adding a second parameter",
            "# to your transformer function, e.g.:",
            "#   TRACKING_URL_TRANSFORMER = (",
            "#       lambda url, query: url if is_fresh(query) else None",
            "#   )",
            "# pylint: disable-next=unnecessary-lambda-assignment",
            "TRACKING_URL_TRANSFORMER = lambda url: url",
            "",
            "",
            "# customize the polling time of each engine",
            "DB_POLL_INTERVAL_SECONDS: dict[str, int] = {}",
            "",
            "# Interval between consecutive polls when using Presto Engine",
            "# See here: https://github.com/dropbox/PyHive/blob/8eb0aeab8ca300f3024655419b93dad926c1a351/pyhive/presto.py#L93  # pylint: disable=line-too-long,useless-suppression",
            "PRESTO_POLL_INTERVAL = int(timedelta(seconds=1).total_seconds())",
            "",
            "# Allow list of custom authentications for each DB engine.",
            "# Example:",
            "# from your.module import AuthClass",
            "# from another.extra import auth_method",
            "#",
            "# ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {",
            "#     \"trino\": {",
            "#         \"custom_auth\": AuthClass,",
            "#         \"another_auth_method\": auth_method,",
            "#     },",
            "# }",
            "ALLOWED_EXTRA_AUTHENTICATIONS: dict[str, dict[str, Callable[..., Any]]] = {}",
            "",
            "# The id of a template dashboard that should be copied to every new user",
            "DASHBOARD_TEMPLATE_ID = None",
            "",
            "# A callable that allows altering the database connection URL and params",
            "# on the fly, at runtime. This allows for things like impersonation or",
            "# arbitrary logic. For instance you can wire different users to",
            "# use different connection parameters, or pass their email address as the",
            "# username. The function receives the connection uri object, connection",
            "# params, the username, and returns the mutated uri and params objects.",
            "# Example:",
            "#   def DB_CONNECTION_MUTATOR(uri, params, username, security_manager, source):",
            "#       user = security_manager.find_user(username=username)",
            "#       if user and user.email:",
            "#           uri.username = user.email",
            "#       return uri, params",
            "#",
            "# Note that the returned uri and params are passed directly to sqlalchemy's",
            "# as such `create_engine(url, **params)`",
            "DB_CONNECTION_MUTATOR = None",
            "",
            "",
            "# A function that intercepts the SQL to be executed and can alter it.",
            "# The use case is can be around adding some sort of comment header",
            "# with information such as the username and worker node information",
            "#",
            "#    def SQL_QUERY_MUTATOR(",
            "#        sql,",
            "#        security_manager=security_manager,",
            "#        database=database,",
            "#    ):",
            "#        dttm = datetime.now().isoformat()",
            "#        return f\"-- [SQL LAB] {user_name} {dttm}\\n{sql}\"",
            "# For backward compatibility, you can unpack any of the above arguments in your",
            "# function definition, but keep the **kwargs as the last argument to allow new args",
            "# to be added later without any errors.",
            "def SQL_QUERY_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    sql: str, **kwargs: Any",
            ") -> str:",
            "    return sql",
            "",
            "",
            "# A variable that chooses whether to apply the SQL_QUERY_MUTATOR before or after splitting the input query",
            "# It allows for using the SQL_QUERY_MUTATOR function for more than comments",
            "# Usage: If you want to apply a change to every statement to a given query, set MUTATE_AFTER_SPLIT = True",
            "# An example use case is if data has role based access controls, and you want to apply",
            "# a SET ROLE statement alongside every user query. Changing this variable maintains",
            "# functionality for both the SQL_Lab and Charts.",
            "MUTATE_AFTER_SPLIT = False",
            "",
            "",
            "# This allows for a user to add header data to any outgoing emails. For example,",
            "# if you need to include metadata in the header or you want to change the specifications",
            "# of the email title, header, or sender.",
            "def EMAIL_HEADER_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    msg: MIMEMultipart, **kwargs: Any",
            ") -> MIMEMultipart:",
            "    return msg",
            "",
            "",
            "# Define a list of usernames to be excluded from all dropdown lists of users",
            "# Owners, filters for created_by, etc.",
            "# The users can also be excluded by overriding the get_exclude_users_from_lists method",
            "# in security manager",
            "EXCLUDE_USERS_FROM_LISTS: list[str] | None = None",
            "",
            "# For database connections, this dictionary will remove engines from the available",
            "# list/dropdown if you do not want these dbs to show as available.",
            "# The available list is generated by driver installed, and some engines have multiple",
            "# drivers.",
            "# e.g., DBS_AVAILABLE_DENYLIST: Dict[str, Set[str]] = {\"databricks\": {\"pyhive\", \"pyodbc\"}}",
            "DBS_AVAILABLE_DENYLIST: dict[str, set[str]] = {}",
            "",
            "# This auth provider is used by background (offline) tasks that need to access",
            "# protected resources. Can be overridden by end users in order to support",
            "# custom auth mechanisms",
            "MACHINE_AUTH_PROVIDER_CLASS = \"superset.utils.machine_auth.MachineAuthProvider\"",
            "",
            "# ---------------------------------------------------",
            "# Alerts & Reports",
            "# ---------------------------------------------------",
            "# Used for Alerts/Reports (Feature flask ALERT_REPORTS) to set the size for the",
            "# sliding cron window size, should be synced with the celery beat config minus 1 second",
            "ALERT_REPORTS_CRON_WINDOW_SIZE = 59",
            "ALERT_REPORTS_WORKING_TIME_OUT_KILL = True",
            "# Which user to attempt to execute Alerts/Reports as. By default,",
            "# execute as the primary owner of the alert/report (giving priority to the last",
            "# modifier and then the creator if either is contained within the list of owners,",
            "# otherwise the first owner will be used).",
            "#",
            "# To first try to execute as the creator in the owners list (if present), then fall",
            "# back to the creator, then the last modifier in the owners list (if present), then the",
            "# last modifier, then an owner and finally `THUMBNAIL_SELENIUM_USER`, set as follows:",
            "# ALERT_REPORTS_EXECUTE_AS = [",
            "#     ExecutorType.CREATOR_OWNER,",
            "#     ExecutorType.CREATOR,",
            "#     ExecutorType.MODIFIER_OWNER,",
            "#     ExecutorType.MODIFIER,",
            "#     ExecutorType.OWNER,",
            "#     ExecutorType.SELENIUM,",
            "# ]",
            "ALERT_REPORTS_EXECUTE_AS: list[ExecutorType] = [ExecutorType.OWNER]",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_TIME_OUT_LAG = int(timedelta(seconds=10).total_seconds())",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG = int(timedelta(seconds=1).total_seconds())",
            "# Default values that user using when creating alert",
            "ALERT_REPORTS_DEFAULT_WORKING_TIMEOUT = 3600",
            "ALERT_REPORTS_DEFAULT_RETENTION = 90",
            "ALERT_REPORTS_DEFAULT_CRON_VALUE = \"0 * * * *\"  # every hour",
            "# If set to true no notification is sent, the worker will just log a message.",
            "# Useful for debugging",
            "ALERT_REPORTS_NOTIFICATION_DRY_RUN = False",
            "# Max tries to run queries to prevent false errors caused by transient errors",
            "# being returned to users. Set to a value >1 to enable retries.",
            "ALERT_REPORTS_QUERY_EXECUTION_MAX_TRIES = 1",
            "# Custom width for screenshots",
            "ALERT_REPORTS_MIN_CUSTOM_SCREENSHOT_WIDTH = 600",
            "ALERT_REPORTS_MAX_CUSTOM_SCREENSHOT_WIDTH = 2400",
            "",
            "# A custom prefix to use on all Alerts & Reports emails",
            "EMAIL_REPORTS_SUBJECT_PREFIX = \"[Report] \"",
            "",
            "# The text for call-to-action link in Alerts & Reports emails",
            "EMAIL_REPORTS_CTA = \"Explore in Superset\"",
            "",
            "# Slack API token for the superset reports, either string or callable",
            "SLACK_API_TOKEN: Callable[[], str] | str | None = None",
            "SLACK_PROXY = None",
            "",
            "# The webdriver to use for generating reports. Use one of the following",
            "# firefox",
            "#   Requires: geckodriver and firefox installations",
            "#   Limitations: can be buggy at times",
            "# chrome:",
            "#   Requires: headless chrome",
            "#   Limitations: unable to generate screenshots of elements",
            "WEBDRIVER_TYPE = \"firefox\"",
            "",
            "# Window size - this will impact the rendering of the data",
            "WEBDRIVER_WINDOW = {",
            "    \"dashboard\": (1600, 2000),",
            "    \"slice\": (3000, 1200),",
            "    \"pixel_density\": 1,",
            "}",
            "",
            "# An optional override to the default auth hook used to provide auth to the offline",
            "# webdriver (when using Selenium) or browser context (when using Playwright - see",
            "# PLAYWRIGHT_REPORTS_AND_THUMBNAILS feature flag)",
            "WEBDRIVER_AUTH_FUNC = None",
            "",
            "# Any config options to be passed as-is to the webdriver",
            "WEBDRIVER_CONFIGURATION: dict[Any, Any] = {\"service_log_path\": \"/dev/null\"}",
            "",
            "# Additional args to be passed as arguments to the config object",
            "# Note: If using Chrome, you'll want to add the \"--marionette\" arg.",
            "WEBDRIVER_OPTION_ARGS = [\"--headless\"]",
            "",
            "# The base URL to query for accessing the user interface",
            "WEBDRIVER_BASEURL = \"http://0.0.0.0:8080/\"",
            "# The base URL for the email report hyperlinks.",
            "WEBDRIVER_BASEURL_USER_FRIENDLY = WEBDRIVER_BASEURL",
            "# Time selenium will wait for the page to load and render for the email report.",
            "EMAIL_PAGE_RENDER_WAIT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Send user to a link where they can report bugs",
            "BUG_REPORT_URL = None",
            "BUG_REPORT_TEXT = \"Report a bug\"",
            "BUG_REPORT_ICON = None  # Recommended size: 16x16",
            "",
            "# Send user to a link where they can read more about Superset",
            "DOCUMENTATION_URL = None",
            "DOCUMENTATION_TEXT = \"Documentation\"",
            "DOCUMENTATION_ICON = None  # Recommended size: 16x16",
            "",
            "# What is the Last N days relative in the time selector to:",
            "# 'today' means it is midnight (00:00:00) in the local timezone",
            "# 'now' means it is relative to the query issue time",
            "# If both start and end time is set to now, this will make the time",
            "# filter a moving window. By only setting the end time to now,",
            "# start time will be set to midnight, while end will be relative to",
            "# the query issue time.",
            "DEFAULT_RELATIVE_START_TIME = \"today\"",
            "DEFAULT_RELATIVE_END_TIME = \"today\"",
            "",
            "# Configure which SQL validator to use for each engine",
            "SQL_VALIDATORS_BY_ENGINE = {",
            "    \"presto\": \"PrestoDBSQLValidator\",",
            "    \"postgresql\": \"PostgreSQLValidator\",",
            "}",
            "",
            "# A list of preferred databases, in order. These databases will be",
            "# displayed prominently in the \"Add Database\" dialog. You should",
            "# use the \"engine_name\" attribute of the corresponding DB engine spec",
            "# in `superset/db_engine_specs/`.",
            "PREFERRED_DATABASES: list[str] = [",
            "    \"PostgreSQL\",",
            "    \"Presto\",",
            "    \"MySQL\",",
            "    \"SQLite\",",
            "    # etc.",
            "]",
            "# When adding a new database we try to connect to it. Depending on which parameters are",
            "# incorrect this could take a couple minutes, until the SQLAlchemy driver pinging the",
            "# database times out. Instead of relying on the driver timeout we can specify a shorter",
            "# one here.",
            "TEST_DATABASE_CONNECTION_TIMEOUT = timedelta(seconds=30)",
            "",
            "# Enable/disable CSP warning",
            "CONTENT_SECURITY_POLICY_WARNING = True",
            "",
            "# Do you want Talisman enabled?",
            "TALISMAN_ENABLED = utils.cast_to_boolean(os.environ.get(\"TALISMAN_ENABLED\", True))",
            "",
            "# If you want Talisman, how do you want it configured??",
            "TALISMAN_CONFIG = {",
            "    \"content_security_policy\": {",
            "        \"default-src\": [\"'self'\"],",
            "        \"img-src\": [\"'self'\", \"blob:\", \"data:\"],",
            "        \"worker-src\": [\"'self'\", \"blob:\"],",
            "        \"connect-src\": [",
            "            \"'self'\",",
            "            \"https://api.mapbox.com\",",
            "            \"https://events.mapbox.com\",",
            "        ],",
            "        \"object-src\": \"'none'\",",
            "        \"style-src\": [",
            "            \"'self'\",",
            "            \"'unsafe-inline'\",",
            "            \"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css\",",
            "        ],",
            "        \"script-src\": [\"'self'\", \"'strict-dynamic'\"],",
            "    },",
            "    \"content_security_policy_nonce_in\": [\"script-src\"],",
            "    \"force_https\": False,",
            "}",
            "# React requires `eval` to work correctly in dev mode",
            "TALISMAN_DEV_CONFIG = {",
            "    \"content_security_policy\": {",
            "        \"default-src\": [\"'self'\"],",
            "        \"img-src\": [\"'self'\", \"blob:\", \"data:\"],",
            "        \"worker-src\": [\"'self'\", \"blob:\"],",
            "        \"connect-src\": [",
            "            \"'self'\",",
            "            \"https://api.mapbox.com\",",
            "            \"https://events.mapbox.com\",",
            "        ],",
            "        \"object-src\": \"'none'\",",
            "        \"style-src\": [",
            "            \"'self'\",",
            "            \"'unsafe-inline'\",",
            "            \"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css\",",
            "        ],",
            "        \"script-src\": [\"'self'\", \"'unsafe-inline'\", \"'unsafe-eval'\"],",
            "    },",
            "    \"content_security_policy_nonce_in\": [\"script-src\"],",
            "    \"force_https\": False,",
            "}",
            "",
            "#",
            "# Flask session cookie options",
            "#",
            "# See https://flask.palletsprojects.com/en/1.1.x/security/#set-cookie-options",
            "# for details",
            "#",
            "SESSION_COOKIE_HTTPONLY = True  # Prevent cookie from being read by frontend JS?",
            "SESSION_COOKIE_SECURE = False  # Prevent cookie from being transmitted over non-tls?",
            "SESSION_COOKIE_SAMESITE: Literal[\"None\", \"Lax\", \"Strict\"] | None = \"Lax\"",
            "",
            "# Cache static resources.",
            "SEND_FILE_MAX_AGE_DEFAULT = int(timedelta(days=365).total_seconds())",
            "",
            "# URI to database storing the example data, points to",
            "# SQLALCHEMY_DATABASE_URI by default if set to `None`",
            "SQLALCHEMY_EXAMPLES_URI = \"sqlite:///\" + os.path.join(DATA_DIR, \"examples.db\")",
            "",
            "# Optional prefix to be added to all static asset paths when rendering the UI.",
            "# This is useful for hosting assets in an external CDN, for example",
            "STATIC_ASSETS_PREFIX = \"\"",
            "",
            "# Some sqlalchemy connection strings can open Superset to security risks.",
            "# Typically these should not be allowed.",
            "PREVENT_UNSAFE_DB_CONNECTIONS = True",
            "",
            "# If true all default urls on datasets will be handled as relative URLs by the frontend",
            "PREVENT_UNSAFE_DEFAULT_URLS_ON_DATASET = True",
            "",
            "# Define a list of allowed URLs for dataset data imports (v1).",
            "# Simple example to only allow URLs that belong to certain domains:",
            "# ALLOWED_IMPORT_URL_DOMAINS = [",
            "#     r\"^https://.+\\.domain1\\.com\\/?.*\", r\"^https://.+\\.domain2\\.com\\/?.*\"",
            "# ]",
            "DATASET_IMPORT_ALLOWED_DATA_URLS = [r\".*\"]",
            "",
            "# Path used to store SSL certificates that are generated when using custom certs.",
            "# Defaults to temporary directory.",
            "# Example: SSL_CERT_PATH = \"/certs\"",
            "SSL_CERT_PATH: str | None = None",
            "",
            "# SQLA table mutator, every time we fetch the metadata for a certain table",
            "# (superset.connectors.sqla.models.SqlaTable), we call this hook",
            "# to allow mutating the object with this callback.",
            "# This can be used to set any properties of the object based on naming",
            "# conventions and such. You can find examples in the tests.",
            "",
            "# pylint: disable-next=unnecessary-lambda-assignment",
            "SQLA_TABLE_MUTATOR = lambda table: table",
            "",
            "",
            "# Global async query config options.",
            "# Requires GLOBAL_ASYNC_QUERIES feature flag to be enabled.",
            "GLOBAL_ASYNC_QUERY_MANAGER_CLASS = (",
            "    \"superset.async_events.async_query_manager.AsyncQueryManager\"",
            ")",
            "GLOBAL_ASYNC_QUERIES_REDIS_CONFIG = {",
            "    \"port\": 6379,",
            "    \"host\": \"127.0.0.1\",",
            "    \"password\": \"\",",
            "    \"db\": 0,",
            "    \"ssl\": False,",
            "}",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_PREFIX = \"async-events-\"",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT = 1000",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT_FIREHOSE = 1000000",
            "GLOBAL_ASYNC_QUERIES_REGISTER_REQUEST_HANDLERS = True",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_NAME = \"async-token\"",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SECURE = False",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SAMESITE: None | (",
            "    Literal[\"None\", \"Lax\", \"Strict\"]",
            ") = None",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_DOMAIN = None",
            "GLOBAL_ASYNC_QUERIES_JWT_SECRET = \"test-secret-change-me\"",
            "GLOBAL_ASYNC_QUERIES_TRANSPORT = \"polling\"",
            "GLOBAL_ASYNC_QUERIES_POLLING_DELAY = int(",
            "    timedelta(milliseconds=500).total_seconds() * 1000",
            ")",
            "GLOBAL_ASYNC_QUERIES_WEBSOCKET_URL = \"ws://127.0.0.1:8080/\"",
            "",
            "# Embedded config options",
            "GUEST_ROLE_NAME = \"Public\"",
            "GUEST_TOKEN_JWT_SECRET = \"test-guest-secret-change-me\"",
            "GUEST_TOKEN_JWT_ALGO = \"HS256\"",
            "GUEST_TOKEN_HEADER_NAME = \"X-GuestToken\"",
            "GUEST_TOKEN_JWT_EXP_SECONDS = 300  # 5 minutes",
            "# Guest token audience for the embedded superset, either string or callable",
            "GUEST_TOKEN_JWT_AUDIENCE: Callable[[], str] | str | None = None",
            "",
            "# A SQL dataset health check. Note if enabled it is strongly advised that the callable",
            "# be memoized to aid with performance, i.e.,",
            "#",
            "#    @cache_manager.cache.memoize(timeout=0)",
            "#    def DATASET_HEALTH_CHECK(datasource: SqlaTable) -> Optional[str]:",
            "#        if (",
            "#            datasource.sql and",
            "#            len(sql_parse.ParsedQuery(datasource.sql, strip_comments=True).tables) == 1",
            "#        ):",
            "#            return (",
            "#                \"This virtual dataset queries only one table and therefore could be \"",
            "#                \"replaced by querying the table directly.\"",
            "#            )",
            "#",
            "#        return None",
            "#",
            "# Within the FLASK_APP_MUTATOR callable, i.e., once the application and thus cache have",
            "# been initialized it is also necessary to add the following logic to blow the cache for",
            "# all datasources if the callback function changed.",
            "#",
            "#    def FLASK_APP_MUTATOR(app: Flask) -> None:",
            "#        name = \"DATASET_HEALTH_CHECK\"",
            "#        func = app.config[name]",
            "#        code = func.uncached.__code__.co_code",
            "#",
            "#        if cache_manager.cache.get(name) != code:",
            "#            cache_manager.cache.delete_memoized(func)",
            "#            cache_manager.cache.set(name, code, timeout=0)",
            "#",
            "DATASET_HEALTH_CHECK: Callable[[SqlaTable], str] | None = None",
            "",
            "# the advanced data type key should correspond to that set in the column metadata",
            "ADVANCED_DATA_TYPES: dict[str, AdvancedDataType] = {",
            "    \"internet_address\": internet_address,",
            "    \"port\": internet_port,",
            "}",
            "",
            "# By default, the Welcome page features all charts and dashboards the user has access",
            "# to. This can be changed to show only examples, or a custom view",
            "# by providing the title and a FAB filter:",
            "# WELCOME_PAGE_LAST_TAB = (",
            "#     \"Xyz\",",
            "#     [{\"col\": 'created_by', \"opr\": 'rel_o_m', \"value\": 10}],",
            "# )",
            "WELCOME_PAGE_LAST_TAB: (",
            "    Literal[\"examples\", \"all\"] | tuple[str, list[dict[str, Any]]]",
            ") = \"all\"",
            "",
            "# Max allowed size for a zipped file",
            "ZIPPED_FILE_MAX_SIZE = 100 * 1024 * 1024  # 100MB",
            "# Max allowed compression ratio for a zipped file",
            "ZIP_FILE_MAX_COMPRESS_RATIO = 200.0",
            "",
            "# Configuration for environment tag shown on the navbar. Setting 'text' to '' will hide the tag.",
            "# 'color' can either be a hex color code, or a dot-indexed theme color (e.g. error.base)",
            "ENVIRONMENT_TAG_CONFIG = {",
            "    \"variable\": \"SUPERSET_ENV\",",
            "    \"values\": {",
            "        \"debug\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"flask-debug\",",
            "        },",
            "        \"development\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"Development\",",
            "        },",
            "        \"production\": {",
            "            \"color\": \"\",",
            "            \"text\": \"\",",
            "        },",
            "    },",
            "}",
            "",
            "",
            "# Extra related query filters make it possible to limit which objects are shown",
            "# in the UI. For examples, to only show \"admin\" or users starting with the letter \"b\" in",
            "# the \"Owners\" dropdowns, you could add the following in your config:",
            "# def user_filter(query: Query, *args, *kwargs):",
            "#     from superset import security_manager",
            "#",
            "#     user_model = security_manager.user_model",
            "#     filters = [",
            "#         user_model.username == \"admin\",",
            "#         user_model.username.ilike(\"b%\"),",
            "#     ]",
            "#     return query.filter(or_(*filters))",
            "#",
            "#  EXTRA_RELATED_QUERY_FILTERS = {\"user\": user_filter}",
            "#",
            "# Similarly, to restrict the roles in the \"Roles\" dropdown you can provide a custom",
            "# filter callback for the \"role\" key.",
            "class ExtraRelatedQueryFilters(TypedDict, total=False):",
            "    role: Callable[[Query], Query]",
            "    user: Callable[[Query], Query]",
            "",
            "",
            "EXTRA_RELATED_QUERY_FILTERS: ExtraRelatedQueryFilters = {}",
            "",
            "",
            "# Extra dynamic query filters make it possible to limit which objects are shown",
            "# in the UI before any other filtering is applied. Useful for example when",
            "# considering to filter using Feature Flags along with regular role filters",
            "# that get applied by default in our base_filters.",
            "# For example, to only show a database starting with the letter \"b\"",
            "# in the \"Database Connections\" list, you could add the following in your config:",
            "# def initial_database_filter(query: Query, *args, *kwargs):",
            "#     from superset.models.core import Database",
            "#",
            "#     filter = Database.database_name.startswith('b')",
            "#     return query.filter(filter)",
            "#",
            "#  EXTRA_DYNAMIC_QUERY_FILTERS = {\"database\": initial_database_filter}",
            "class ExtraDynamicQueryFilters(TypedDict, total=False):",
            "    databases: Callable[[Query], Query]",
            "",
            "",
            "EXTRA_DYNAMIC_QUERY_FILTERS: ExtraDynamicQueryFilters = {}",
            "",
            "",
            "# -------------------------------------------------------------------",
            "# *                WARNING:  STOP EDITING  HERE                    *",
            "# -------------------------------------------------------------------",
            "# Don't add config values below this line since local configs won't be",
            "# able to override them.",
            "if CONFIG_PATH_ENV_VAR in os.environ:",
            "    # Explicitly import config module that is not necessarily in pythonpath; useful",
            "    # for case where app is being executed via pex.",
            "    cfg_path = os.environ[CONFIG_PATH_ENV_VAR]",
            "    try:",
            "        module = sys.modules[__name__]",
            "        override_conf = imp.load_source(\"superset_config\", cfg_path)",
            "        for key in dir(override_conf):",
            "            if key.isupper():",
            "                setattr(module, key, getattr(override_conf, key))",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{cfg_path}]\")",
            "    except Exception:",
            "        logger.exception(",
            "            \"Failed to import config for %s=%s\", CONFIG_PATH_ENV_VAR, cfg_path",
            "        )",
            "        raise",
            "elif importlib.util.find_spec(\"superset_config\") and not is_test():",
            "    try:",
            "        # pylint: disable=import-error,wildcard-import,unused-wildcard-import",
            "        import superset_config",
            "        from superset_config import *  # type: ignore",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{superset_config.__file__}]\")",
            "    except Exception:",
            "        logger.exception(\"Found but failed to import local superset_config\")",
            "        raise"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "superset/utils/core.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1917,
                "afterPatchRowNumber": 1917,
                "PatchRowcode": "     return buf"
            },
            "1": {
                "beforePatchRowNumber": 1918,
                "afterPatchRowNumber": 1918,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 1919,
                "afterPatchRowNumber": 1919,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1920,
                "PatchRowcode": "+def check_is_safe_zip(zip_file: ZipFile) -> None:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1921,
                "PatchRowcode": "+    \"\"\""
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1922,
                "PatchRowcode": "+    Checks whether a ZIP file is safe, raises SupersetException if not."
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1923,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1924,
                "PatchRowcode": "+    :param zip_file:"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1925,
                "PatchRowcode": "+    :return:"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1926,
                "PatchRowcode": "+    \"\"\""
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1927,
                "PatchRowcode": "+    uncompress_size = 0"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1928,
                "PatchRowcode": "+    compress_size = 0"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1929,
                "PatchRowcode": "+    for zip_file_element in zip_file.infolist():"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1930,
                "PatchRowcode": "+        if zip_file_element.file_size > current_app.config[\"ZIPPED_FILE_MAX_SIZE\"]:"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1931,
                "PatchRowcode": "+            raise SupersetException(\"Found file with size above allowed threshold\")"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1932,
                "PatchRowcode": "+        uncompress_size += zip_file_element.file_size"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1933,
                "PatchRowcode": "+        compress_size += zip_file_element.compress_size"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1934,
                "PatchRowcode": "+    compress_ratio = uncompress_size / compress_size"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1935,
                "PatchRowcode": "+    if compress_ratio > current_app.config[\"ZIP_FILE_MAX_COMPRESS_RATIO\"]:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1936,
                "PatchRowcode": "+        raise SupersetException(\"Zip compress ratio above allowed threshold\")"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1937,
                "PatchRowcode": "+"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1938,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": 1920,
                "afterPatchRowNumber": 1939,
                "PatchRowcode": " def remove_extra_adhoc_filters(form_data: dict[str, Any]) -> None:"
            },
            "23": {
                "beforePatchRowNumber": 1921,
                "afterPatchRowNumber": 1940,
                "PatchRowcode": "     \"\"\""
            },
            "24": {
                "beforePatchRowNumber": 1922,
                "afterPatchRowNumber": 1941,
                "PatchRowcode": "     Remove filters from slice data that originate from a filter box or native filter"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"Utility functions used across Superset\"\"\"",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import _thread",
            "import collections",
            "import decimal",
            "import errno",
            "import json",
            "import logging",
            "import os",
            "import platform",
            "import re",
            "import signal",
            "import smtplib",
            "import sqlite3",
            "import ssl",
            "import tempfile",
            "import threading",
            "import traceback",
            "import uuid",
            "import zlib",
            "from collections.abc import Iterable, Iterator, Sequence",
            "from contextlib import closing, contextmanager",
            "from dataclasses import dataclass",
            "from datetime import date, datetime, time, timedelta",
            "from email.mime.application import MIMEApplication",
            "from email.mime.image import MIMEImage",
            "from email.mime.multipart import MIMEMultipart",
            "from email.mime.text import MIMEText",
            "from email.utils import formatdate",
            "from enum import Enum, IntEnum",
            "from io import BytesIO",
            "from timeit import default_timer",
            "from types import TracebackType",
            "from typing import Any, Callable, cast, NamedTuple, TYPE_CHECKING, TypedDict, TypeVar",
            "from urllib.parse import unquote_plus",
            "from zipfile import ZipFile",
            "",
            "import markdown as md",
            "import nh3",
            "import numpy as np",
            "import pandas as pd",
            "import sqlalchemy as sa",
            "from cryptography.hazmat.backends import default_backend",
            "from cryptography.x509 import Certificate, load_pem_x509_certificate",
            "from flask import current_app, flash, g, Markup, request",
            "from flask_appbuilder import SQLA",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __",
            "from flask_babel.speaklater import LazyString",
            "from pandas.api.types import infer_dtype",
            "from pandas.core.dtypes.common import is_numeric_dtype",
            "from sqlalchemy import event, exc, inspect, select, Text",
            "from sqlalchemy.dialects.mysql import MEDIUMTEXT",
            "from sqlalchemy.engine import Connection, Engine",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.sql.type_api import Variant",
            "from sqlalchemy.types import TEXT, TypeDecorator, TypeEngine",
            "from typing_extensions import TypeGuard",
            "",
            "from superset.constants import (",
            "    EXTRA_FORM_DATA_APPEND_KEYS,",
            "    EXTRA_FORM_DATA_OVERRIDE_EXTRA_KEYS,",
            "    EXTRA_FORM_DATA_OVERRIDE_REGULAR_MAPPINGS,",
            "    NO_TIME_RANGE,",
            ")",
            "from superset.errors import ErrorLevel, SupersetErrorType",
            "from superset.exceptions import (",
            "    CertificateException,",
            "    SupersetException,",
            "    SupersetTimeoutException,",
            ")",
            "from superset.sql_parse import sanitize_clause",
            "from superset.superset_typing import (",
            "    AdhocColumn,",
            "    AdhocMetric,",
            "    AdhocMetricColumn,",
            "    Column,",
            "    FilterValues,",
            "    FlaskResponse,",
            "    FormData,",
            "    Metric,",
            ")",
            "from superset.utils.backports import StrEnum",
            "from superset.utils.database import get_example_database",
            "from superset.utils.date_parser import parse_human_timedelta",
            "from superset.utils.dates import datetime_to_epoch, EPOCH",
            "from superset.utils.hashing import md5_sha_from_dict, md5_sha_from_str",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.base.models import BaseColumn, BaseDatasource",
            "    from superset.models.sql_lab import Query",
            "",
            "logging.getLogger(\"MARKDOWN\").setLevel(logging.INFO)",
            "logger = logging.getLogger(__name__)",
            "",
            "DTTM_ALIAS = \"__timestamp\"",
            "",
            "TIME_COMPARISON = \"__\"",
            "",
            "JS_MAX_INTEGER = 9007199254740991  # Largest int Java Script can handle 2^53-1",
            "",
            "InputType = TypeVar(\"InputType\")  # pylint: disable=invalid-name",
            "",
            "ADHOC_FILTERS_REGEX = re.compile(\"^adhoc_filters\")",
            "",
            "",
            "class LenientEnum(Enum):",
            "    \"\"\"Enums with a `get` method that convert a enum value to `Enum` if it is a",
            "    valid value.\"\"\"",
            "",
            "    @classmethod",
            "    def get(cls, value: Any) -> Any:",
            "        try:",
            "            return super().__new__(cls, value)",
            "        except ValueError:",
            "            return None",
            "",
            "",
            "class AdhocMetricExpressionType(StrEnum):",
            "    SIMPLE = \"SIMPLE\"",
            "    SQL = \"SQL\"",
            "",
            "",
            "class AnnotationType(StrEnum):",
            "    FORMULA = \"FORMULA\"",
            "    INTERVAL = \"INTERVAL\"",
            "    EVENT = \"EVENT\"",
            "    TIME_SERIES = \"TIME_SERIES\"",
            "",
            "",
            "class GenericDataType(IntEnum):",
            "    \"\"\"",
            "    Generic database column type that fits both frontend and backend.",
            "    \"\"\"",
            "",
            "    NUMERIC = 0",
            "    STRING = 1",
            "    TEMPORAL = 2",
            "    BOOLEAN = 3",
            "    # ARRAY = 4     # Mapping all the complex data types to STRING for now",
            "    # JSON = 5      # and leaving these as a reminder.",
            "    # MAP = 6",
            "    # ROW = 7",
            "",
            "",
            "class DatasourceType(StrEnum):",
            "    SLTABLE = \"sl_table\"",
            "    TABLE = \"table\"",
            "    DATASET = \"dataset\"",
            "    QUERY = \"query\"",
            "    SAVEDQUERY = \"saved_query\"",
            "    VIEW = \"view\"",
            "",
            "",
            "class LoggerLevel(StrEnum):",
            "    INFO = \"info\"",
            "    WARNING = \"warning\"",
            "    EXCEPTION = \"exception\"",
            "",
            "",
            "class HeaderDataType(TypedDict):",
            "    notification_format: str",
            "    owners: list[int]",
            "    notification_type: str",
            "    notification_source: str | None",
            "    chart_id: int | None",
            "    dashboard_id: int | None",
            "",
            "",
            "class DatasourceDict(TypedDict):",
            "    type: str  # todo(hugh): update this to be DatasourceType",
            "    id: int",
            "",
            "",
            "class AdhocFilterClause(TypedDict, total=False):",
            "    clause: str",
            "    expressionType: str",
            "    filterOptionName: str | None",
            "    comparator: FilterValues | None",
            "    operator: str",
            "    subject: str",
            "    isExtra: bool | None",
            "    sqlExpression: str | None",
            "",
            "",
            "class QueryObjectFilterClause(TypedDict, total=False):",
            "    col: Column",
            "    op: str  # pylint: disable=invalid-name",
            "    val: FilterValues | None",
            "    grain: str | None",
            "    isExtra: bool | None",
            "",
            "",
            "class ExtraFiltersTimeColumnType(StrEnum):",
            "    TIME_COL = \"__time_col\"",
            "    TIME_GRAIN = \"__time_grain\"",
            "    TIME_ORIGIN = \"__time_origin\"",
            "    TIME_RANGE = \"__time_range\"",
            "",
            "",
            "class ExtraFiltersReasonType(StrEnum):",
            "    NO_TEMPORAL_COLUMN = \"no_temporal_column\"",
            "    COL_NOT_IN_DATASOURCE = \"not_in_datasource\"",
            "",
            "",
            "class FilterOperator(StrEnum):",
            "    \"\"\"",
            "    Operators used filter controls",
            "    \"\"\"",
            "",
            "    EQUALS = \"==\"",
            "    NOT_EQUALS = \"!=\"",
            "    GREATER_THAN = \">\"",
            "    LESS_THAN = \"<\"",
            "    GREATER_THAN_OR_EQUALS = \">=\"",
            "    LESS_THAN_OR_EQUALS = \"<=\"",
            "    LIKE = \"LIKE\"",
            "    ILIKE = \"ILIKE\"",
            "    IS_NULL = \"IS NULL\"",
            "    IS_NOT_NULL = \"IS NOT NULL\"",
            "    IN = \"IN\"",
            "    NOT_IN = \"NOT IN\"",
            "    IS_TRUE = \"IS TRUE\"",
            "    IS_FALSE = \"IS FALSE\"",
            "    TEMPORAL_RANGE = \"TEMPORAL_RANGE\"",
            "",
            "",
            "class FilterStringOperators(StrEnum):",
            "    EQUALS = (\"EQUALS\",)",
            "    NOT_EQUALS = (\"NOT_EQUALS\",)",
            "    LESS_THAN = (\"LESS_THAN\",)",
            "    GREATER_THAN = (\"GREATER_THAN\",)",
            "    LESS_THAN_OR_EQUAL = (\"LESS_THAN_OR_EQUAL\",)",
            "    GREATER_THAN_OR_EQUAL = (\"GREATER_THAN_OR_EQUAL\",)",
            "    IN = (\"IN\",)",
            "    NOT_IN = (\"NOT_IN\",)",
            "    ILIKE = (\"ILIKE\",)",
            "    LIKE = (\"LIKE\",)",
            "    IS_NOT_NULL = (\"IS_NOT_NULL\",)",
            "    IS_NULL = (\"IS_NULL\",)",
            "    LATEST_PARTITION = (\"LATEST_PARTITION\",)",
            "    IS_TRUE = (\"IS_TRUE\",)",
            "    IS_FALSE = (\"IS_FALSE\",)",
            "",
            "",
            "class PostProcessingBoxplotWhiskerType(StrEnum):",
            "    \"\"\"",
            "    Calculate cell contribution to row/column total",
            "    \"\"\"",
            "",
            "    TUKEY = \"tukey\"",
            "    MINMAX = \"min/max\"",
            "    PERCENTILE = \"percentile\"",
            "",
            "",
            "class PostProcessingContributionOrientation(StrEnum):",
            "    \"\"\"",
            "    Calculate cell contribution to row/column total",
            "    \"\"\"",
            "",
            "    ROW = \"row\"",
            "    COLUMN = \"column\"",
            "",
            "",
            "class QueryMode(str, LenientEnum):",
            "    \"\"\"",
            "    Whether the query runs on aggregate or returns raw records",
            "    \"\"\"",
            "",
            "    RAW = \"raw\"",
            "    AGGREGATE = \"aggregate\"",
            "",
            "",
            "class QuerySource(Enum):",
            "    \"\"\"",
            "    The source of a SQL query.",
            "    \"\"\"",
            "",
            "    CHART = 0",
            "    DASHBOARD = 1",
            "    SQL_LAB = 2",
            "",
            "",
            "class QueryStatus(StrEnum):",
            "    \"\"\"Enum-type class for query statuses\"\"\"",
            "",
            "    STOPPED: str = \"stopped\"",
            "    FAILED: str = \"failed\"",
            "    PENDING: str = \"pending\"",
            "    RUNNING: str = \"running\"",
            "    SCHEDULED: str = \"scheduled\"",
            "    SUCCESS: str = \"success\"",
            "    FETCHING: str = \"fetching\"",
            "    TIMED_OUT: str = \"timed_out\"",
            "",
            "",
            "class DashboardStatus(StrEnum):",
            "    \"\"\"Dashboard status used for frontend filters\"\"\"",
            "",
            "    PUBLISHED = \"published\"",
            "    DRAFT = \"draft\"",
            "",
            "",
            "class ReservedUrlParameters(StrEnum):",
            "    \"\"\"",
            "    Reserved URL parameters that are used internally by Superset. These will not be",
            "    passed to chart queries, as they control the behavior of the UI.",
            "    \"\"\"",
            "",
            "    STANDALONE = \"standalone\"",
            "    EDIT_MODE = \"edit\"",
            "",
            "    @staticmethod",
            "    def is_standalone_mode() -> bool | None:",
            "        standalone_param = request.args.get(ReservedUrlParameters.STANDALONE.value)",
            "        standalone: bool | None = bool(",
            "            standalone_param and standalone_param != \"false\" and standalone_param != \"0\"",
            "        )",
            "        return standalone",
            "",
            "",
            "class RowLevelSecurityFilterType(StrEnum):",
            "    REGULAR = \"Regular\"",
            "    BASE = \"Base\"",
            "",
            "",
            "class ColumnTypeSource(Enum):",
            "    GET_TABLE = 1",
            "    CURSOR_DESCRIPTION = 2",
            "",
            "",
            "class ColumnSpec(NamedTuple):",
            "    sqla_type: TypeEngine | str",
            "    generic_type: GenericDataType",
            "    is_dttm: bool",
            "    python_date_format: str | None = None",
            "",
            "",
            "def flasher(msg: str, severity: str = \"message\") -> None:",
            "    \"\"\"Flask's flash if available, logging call if not\"\"\"",
            "    try:",
            "        flash(msg, severity)",
            "    except RuntimeError:",
            "        if severity == \"danger\":",
            "            logger.error(msg, exc_info=True)",
            "        else:",
            "            logger.info(msg)",
            "",
            "",
            "def parse_js_uri_path_item(",
            "    item: str | None, unquote: bool = True, eval_undefined: bool = False",
            ") -> str | None:",
            "    \"\"\"Parse an uri path item made with js.",
            "",
            "    :param item: an uri path component",
            "    :param unquote: Perform unquoting of string using urllib.parse.unquote_plus()",
            "    :param eval_undefined: When set to True and item is either 'null' or 'undefined',",
            "    assume item is undefined and return None.",
            "    :return: Either None, the original item or unquoted item",
            "    \"\"\"",
            "    item = None if eval_undefined and item in (\"null\", \"undefined\") else item",
            "    return unquote_plus(item) if unquote and item else item",
            "",
            "",
            "def cast_to_num(value: float | int | str | None) -> float | int | None:",
            "    \"\"\"Casts a value to an int/float",
            "",
            "    >>> cast_to_num('1 ')",
            "    1.0",
            "    >>> cast_to_num(' 2')",
            "    2.0",
            "    >>> cast_to_num('5')",
            "    5",
            "    >>> cast_to_num('5.2')",
            "    5.2",
            "    >>> cast_to_num(10)",
            "    10",
            "    >>> cast_to_num(10.1)",
            "    10.1",
            "    >>> cast_to_num(None) is None",
            "    True",
            "    >>> cast_to_num('this is not a string') is None",
            "    True",
            "",
            "    :param value: value to be converted to numeric representation",
            "    :returns: value cast to `int` if value is all digits, `float` if `value` is",
            "              decimal value and `None`` if it can't be converted",
            "    \"\"\"",
            "    if value is None:",
            "        return None",
            "    if isinstance(value, (int, float)):",
            "        return value",
            "    if value.isdigit():",
            "        return int(value)",
            "    try:",
            "        return float(value)",
            "    except ValueError:",
            "        return None",
            "",
            "",
            "def cast_to_boolean(value: Any) -> bool | None:",
            "    \"\"\"Casts a value to an int/float",
            "",
            "    >>> cast_to_boolean(1)",
            "    True",
            "    >>> cast_to_boolean(0)",
            "    False",
            "    >>> cast_to_boolean(0.5)",
            "    True",
            "    >>> cast_to_boolean('true')",
            "    True",
            "    >>> cast_to_boolean('false')",
            "    False",
            "    >>> cast_to_boolean('False')",
            "    False",
            "    >>> cast_to_boolean(None)",
            "",
            "    :param value: value to be converted to boolean representation",
            "    :returns: value cast to `bool`. when value is 'true' or value that are not 0",
            "              converted into True. Return `None` if value is `None`",
            "    \"\"\"",
            "    if value is None:",
            "        return None",
            "    if isinstance(value, bool):",
            "        return value",
            "    if isinstance(value, (int, float)):",
            "        return value != 0",
            "    if isinstance(value, str):",
            "        return value.strip().lower() == \"true\"",
            "    return False",
            "",
            "",
            "def list_minus(l: list[Any], minus: list[Any]) -> list[Any]:",
            "    \"\"\"Returns l without what is in minus",
            "",
            "    >>> list_minus([1, 2, 3], [2])",
            "    [1, 3]",
            "    \"\"\"",
            "    return [o for o in l if o not in minus]",
            "",
            "",
            "class DashboardEncoder(json.JSONEncoder):",
            "    def __init__(self, *args: Any, **kwargs: Any) -> None:",
            "        super().__init__(*args, **kwargs)",
            "        self.sort_keys = True",
            "",
            "    def default(self, o: Any) -> dict[Any, Any] | str:",
            "        if isinstance(o, uuid.UUID):",
            "            return str(o)",
            "        try:",
            "            vals = {k: v for k, v in o.__dict__.items() if k != \"_sa_instance_state\"}",
            "            return {f\"__{o.__class__.__name__}__\": vals}",
            "        except Exception:  # pylint: disable=broad-except",
            "            if isinstance(o, datetime):",
            "                return {\"__datetime__\": o.replace(microsecond=0).isoformat()}",
            "            return json.JSONEncoder(sort_keys=True).default(o)",
            "",
            "",
            "class JSONEncodedDict(TypeDecorator):  # pylint: disable=abstract-method",
            "    \"\"\"Represents an immutable structure as a json-encoded string.\"\"\"",
            "",
            "    impl = TEXT",
            "",
            "    def process_bind_param(",
            "        self, value: dict[Any, Any] | None, dialect: str",
            "    ) -> str | None:",
            "        return json.dumps(value) if value is not None else None",
            "",
            "    def process_result_value(",
            "        self, value: str | None, dialect: str",
            "    ) -> dict[Any, Any] | None:",
            "        return json.loads(value) if value is not None else None",
            "",
            "",
            "def format_timedelta(time_delta: timedelta) -> str:",
            "    \"\"\"",
            "    Ensures negative time deltas are easily interpreted by humans",
            "",
            "    >>> td = timedelta(0) - timedelta(days=1, hours=5,minutes=6)",
            "    >>> str(td)",
            "    '-2 days, 18:54:00'",
            "    >>> format_timedelta(td)",
            "    '-1 day, 5:06:00'",
            "    \"\"\"",
            "    if time_delta < timedelta(0):",
            "        return \"-\" + str(abs(time_delta))",
            "",
            "    # Change this to format positive time deltas the way you want",
            "    return str(time_delta)",
            "",
            "",
            "def base_json_conv(obj: Any) -> Any:",
            "    \"\"\"",
            "    Tries to convert additional types to JSON compatible forms.",
            "",
            "    :param obj: The serializable object",
            "    :returns: The JSON compatible form",
            "    :raises TypeError: If the object cannot be serialized",
            "    :see: https://docs.python.org/3/library/json.html#encoders-and-decoders",
            "    \"\"\"",
            "",
            "    if isinstance(obj, memoryview):",
            "        obj = obj.tobytes()",
            "    if isinstance(obj, np.int64):",
            "        return int(obj)",
            "    if isinstance(obj, np.bool_):",
            "        return bool(obj)",
            "    if isinstance(obj, np.ndarray):",
            "        return obj.tolist()",
            "    if isinstance(obj, set):",
            "        return list(obj)",
            "    if isinstance(obj, decimal.Decimal):",
            "        return float(obj)",
            "    if isinstance(obj, (uuid.UUID, time, LazyString)):",
            "        return str(obj)",
            "    if isinstance(obj, timedelta):",
            "        return format_timedelta(obj)",
            "    if isinstance(obj, bytes):",
            "        try:",
            "            return obj.decode(\"utf-8\")",
            "        except Exception:  # pylint: disable=broad-except",
            "            return \"[bytes]\"",
            "",
            "    raise TypeError(f\"Unserializable object {obj} of type {type(obj)}\")",
            "",
            "",
            "def json_iso_dttm_ser(obj: Any, pessimistic: bool = False) -> Any:",
            "    \"\"\"",
            "    A JSON serializer that deals with dates by serializing them to ISO 8601.",
            "",
            "        >>> json.dumps({'dttm': datetime(1970, 1, 1)}, default=json_iso_dttm_ser)",
            "        '{\"dttm\": \"1970-01-01T00:00:00\"}'",
            "",
            "    :param obj: The serializable object",
            "    :param pessimistic: Whether to be pessimistic regarding serialization",
            "    :returns: The JSON compatible form",
            "    :raises TypeError: If the non-pessimistic object cannot be serialized",
            "    \"\"\"",
            "",
            "    if isinstance(obj, (datetime, date, pd.Timestamp)):",
            "        return obj.isoformat()",
            "",
            "    try:",
            "        return base_json_conv(obj)",
            "    except TypeError as ex:",
            "        if pessimistic:",
            "            return f\"Unserializable [{type(obj)}]\"",
            "",
            "        raise ex",
            "",
            "",
            "def pessimistic_json_iso_dttm_ser(obj: Any) -> Any:",
            "    \"\"\"Proxy to call json_iso_dttm_ser in a pessimistic way",
            "",
            "    If one of object is not serializable to json, it will still succeed\"\"\"",
            "    return json_iso_dttm_ser(obj, pessimistic=True)",
            "",
            "",
            "def json_int_dttm_ser(obj: Any) -> Any:",
            "    \"\"\"",
            "    A JSON serializer that deals with dates by serializing them to EPOCH.",
            "",
            "        >>> json.dumps({'dttm': datetime(1970, 1, 1)}, default=json_int_dttm_ser)",
            "        '{\"dttm\": 0.0}'",
            "",
            "    :param obj: The serializable object",
            "    :returns: The JSON compatible form",
            "    :raises TypeError: If the object cannot be serialized",
            "    \"\"\"",
            "",
            "    if isinstance(obj, (datetime, pd.Timestamp)):",
            "        return datetime_to_epoch(obj)",
            "",
            "    if isinstance(obj, date):",
            "        return (obj - EPOCH.date()).total_seconds() * 1000",
            "",
            "    return base_json_conv(obj)",
            "",
            "",
            "def json_dumps_w_dates(payload: dict[Any, Any], sort_keys: bool = False) -> str:",
            "    \"\"\"Dumps payload to JSON with Datetime objects properly converted\"\"\"",
            "    return json.dumps(payload, default=json_int_dttm_ser, sort_keys=sort_keys)",
            "",
            "",
            "def error_msg_from_exception(ex: Exception) -> str:",
            "    \"\"\"Translate exception into error message",
            "",
            "    Database have different ways to handle exception. This function attempts",
            "    to make sense of the exception object and construct a human readable",
            "    sentence.",
            "",
            "    TODO(bkyryliuk): parse the Presto error message from the connection",
            "                     created via create_engine.",
            "    engine = create_engine('presto://localhost:3506/silver') -",
            "      gives an e.message as the str(dict)",
            "    presto.connect('localhost', port=3506, catalog='silver') - as a dict.",
            "    The latter version is parsed correctly by this function.",
            "    \"\"\"",
            "    msg = \"\"",
            "    if hasattr(ex, \"message\"):",
            "        if isinstance(ex.message, dict):",
            "            msg = ex.message.get(\"message\")  # type: ignore",
            "        elif ex.message:",
            "            msg = ex.message",
            "    return msg or str(ex)",
            "",
            "",
            "def markdown(raw: str, markup_wrap: bool | None = False) -> str:",
            "    safe_markdown_tags = {",
            "        \"h1\",",
            "        \"h2\",",
            "        \"h3\",",
            "        \"h4\",",
            "        \"h5\",",
            "        \"h6\",",
            "        \"b\",",
            "        \"i\",",
            "        \"strong\",",
            "        \"em\",",
            "        \"tt\",",
            "        \"p\",",
            "        \"br\",",
            "        \"span\",",
            "        \"div\",",
            "        \"blockquote\",",
            "        \"code\",",
            "        \"hr\",",
            "        \"ul\",",
            "        \"ol\",",
            "        \"li\",",
            "        \"dd\",",
            "        \"dt\",",
            "        \"img\",",
            "        \"a\",",
            "    }",
            "    safe_markdown_attrs = {",
            "        \"img\": {\"src\", \"alt\", \"title\"},",
            "        \"a\": {\"href\", \"alt\", \"title\"},",
            "    }",
            "    safe = md.markdown(",
            "        raw or \"\",",
            "        extensions=[",
            "            \"markdown.extensions.tables\",",
            "            \"markdown.extensions.fenced_code\",",
            "            \"markdown.extensions.codehilite\",",
            "        ],",
            "    )",
            "    # pylint: disable=no-member",
            "    safe = nh3.clean(safe, tags=safe_markdown_tags, attributes=safe_markdown_attrs)",
            "    if markup_wrap:",
            "        safe = Markup(safe)",
            "    return safe",
            "",
            "",
            "def readfile(file_path: str) -> str | None:",
            "    with open(file_path) as f:",
            "        content = f.read()",
            "    return content",
            "",
            "",
            "def generic_find_constraint_name(",
            "    table: str, columns: set[str], referenced: str, database: SQLA",
            ") -> str | None:",
            "    \"\"\"Utility to find a constraint name in alembic migrations\"\"\"",
            "    tbl = sa.Table(",
            "        table, database.metadata, autoload=True, autoload_with=database.engine",
            "    )",
            "",
            "    for fk in tbl.foreign_key_constraints:",
            "        if fk.referred_table.name == referenced and set(fk.column_keys) == columns:",
            "            return fk.name",
            "",
            "    return None",
            "",
            "",
            "def generic_find_fk_constraint_name(",
            "    table: str, columns: set[str], referenced: str, insp: Inspector",
            ") -> str | None:",
            "    \"\"\"Utility to find a foreign-key constraint name in alembic migrations\"\"\"",
            "    for fk in insp.get_foreign_keys(table):",
            "        if (",
            "            fk[\"referred_table\"] == referenced",
            "            and set(fk[\"referred_columns\"]) == columns",
            "        ):",
            "            return fk[\"name\"]",
            "",
            "    return None",
            "",
            "",
            "def generic_find_fk_constraint_names(  # pylint: disable=invalid-name",
            "    table: str, columns: set[str], referenced: str, insp: Inspector",
            ") -> set[str]:",
            "    \"\"\"Utility to find foreign-key constraint names in alembic migrations\"\"\"",
            "    names = set()",
            "",
            "    for fk in insp.get_foreign_keys(table):",
            "        if (",
            "            fk[\"referred_table\"] == referenced",
            "            and set(fk[\"referred_columns\"]) == columns",
            "        ):",
            "            names.add(fk[\"name\"])",
            "",
            "    return names",
            "",
            "",
            "def generic_find_uq_constraint_name(",
            "    table: str, columns: set[str], insp: Inspector",
            ") -> str | None:",
            "    \"\"\"Utility to find a unique constraint name in alembic migrations\"\"\"",
            "",
            "    for uq in insp.get_unique_constraints(table):",
            "        if columns == set(uq[\"column_names\"]):",
            "            return uq[\"name\"]",
            "",
            "    return None",
            "",
            "",
            "def get_datasource_full_name(",
            "    database_name: str, datasource_name: str, schema: str | None = None",
            ") -> str:",
            "    if not schema:",
            "        return f\"[{database_name}].[{datasource_name}]\"",
            "    return f\"[{database_name}].[{schema}].[{datasource_name}]\"",
            "",
            "",
            "def validate_json(obj: bytes | bytearray | str) -> None:",
            "    if obj:",
            "        try:",
            "            json.loads(obj)",
            "        except Exception as ex:",
            "            logger.error(\"JSON is not valid %s\", str(ex), exc_info=True)",
            "            raise SupersetException(\"JSON is not valid\") from ex",
            "",
            "",
            "class SigalrmTimeout:",
            "    \"\"\"",
            "    To be used in a ``with`` block and timeout its content.",
            "    \"\"\"",
            "",
            "    def __init__(self, seconds: int = 1, error_message: str = \"Timeout\") -> None:",
            "        self.seconds = seconds",
            "        self.error_message = error_message",
            "",
            "    def handle_timeout(  # pylint: disable=unused-argument",
            "        self, signum: int, frame: Any",
            "    ) -> None:",
            "        logger.error(\"Process timed out\", exc_info=True)",
            "        raise SupersetTimeoutException(",
            "            error_type=SupersetErrorType.BACKEND_TIMEOUT_ERROR,",
            "            message=self.error_message,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"timeout\": self.seconds},",
            "        )",
            "",
            "    def __enter__(self) -> None:",
            "        try:",
            "            if threading.current_thread() == threading.main_thread():",
            "                signal.signal(signal.SIGALRM, self.handle_timeout)",
            "                signal.alarm(self.seconds)",
            "        except ValueError as ex:",
            "            logger.warning(\"timeout can't be used in the current context\")",
            "            logger.exception(ex)",
            "",
            "    def __exit__(  # pylint: disable=redefined-outer-name,redefined-builtin",
            "        self, type: Any, value: Any, traceback: TracebackType",
            "    ) -> None:",
            "        try:",
            "            signal.alarm(0)",
            "        except ValueError as ex:",
            "            logger.warning(\"timeout can't be used in the current context\")",
            "            logger.exception(ex)",
            "",
            "",
            "class TimerTimeout:",
            "    def __init__(self, seconds: int = 1, error_message: str = \"Timeout\") -> None:",
            "        self.seconds = seconds",
            "        self.error_message = error_message",
            "        self.timer = threading.Timer(seconds, _thread.interrupt_main)",
            "",
            "    def __enter__(self) -> None:",
            "        self.timer.start()",
            "",
            "    def __exit__(  # pylint: disable=redefined-outer-name,redefined-builtin",
            "        self, type: Any, value: Any, traceback: TracebackType",
            "    ) -> None:",
            "        self.timer.cancel()",
            "        if type is KeyboardInterrupt:  # raised by _thread.interrupt_main",
            "            raise SupersetTimeoutException(",
            "                error_type=SupersetErrorType.BACKEND_TIMEOUT_ERROR,",
            "                message=self.error_message,",
            "                level=ErrorLevel.ERROR,",
            "                extra={\"timeout\": self.seconds},",
            "            )",
            "",
            "",
            "# Windows has no support for SIGALRM, so we use the timer based timeout",
            "timeout: type[TimerTimeout] | type[SigalrmTimeout] = (",
            "    TimerTimeout if platform.system() == \"Windows\" else SigalrmTimeout",
            ")",
            "",
            "",
            "def pessimistic_connection_handling(some_engine: Engine) -> None:",
            "    @event.listens_for(some_engine, \"engine_connect\")",
            "    def ping_connection(connection: Connection, branch: bool) -> None:",
            "        if branch:",
            "            # 'branch' refers to a sub-connection of a connection,",
            "            # we don't want to bother pinging on these.",
            "            return",
            "",
            "        # turn off 'close with result'.  This flag is only used with",
            "        # 'connectionless' execution, otherwise will be False in any case",
            "        save_should_close_with_result = connection.should_close_with_result",
            "        connection.should_close_with_result = False",
            "",
            "        try:",
            "            # run a SELECT 1.   use a core select() so that",
            "            # the SELECT of a scalar value without a table is",
            "            # appropriately formatted for the backend",
            "            connection.scalar(select([1]))",
            "        except exc.DBAPIError as err:",
            "            # catch SQLAlchemy's DBAPIError, which is a wrapper",
            "            # for the DBAPI's exception.  It includes a .connection_invalidated",
            "            # attribute which specifies if this connection is a 'disconnect'",
            "            # condition, which is based on inspection of the original exception",
            "            # by the dialect in use.",
            "            if err.connection_invalidated:",
            "                # run the same SELECT again - the connection will re-validate",
            "                # itself and establish a new connection.  The disconnect detection",
            "                # here also causes the whole connection pool to be invalidated",
            "                # so that all stale connections are discarded.",
            "                connection.scalar(select([1]))",
            "            else:",
            "                raise",
            "        finally:",
            "            # restore 'close with result'",
            "            connection.should_close_with_result = save_should_close_with_result",
            "",
            "    if some_engine.dialect.name == \"sqlite\":",
            "",
            "        @event.listens_for(some_engine, \"connect\")",
            "        def set_sqlite_pragma(  # pylint: disable=unused-argument",
            "            connection: sqlite3.Connection,",
            "            *args: Any,",
            "        ) -> None:",
            "            r\"\"\"",
            "            Enable foreign key support for SQLite.",
            "",
            "            :param connection: The SQLite connection",
            "            :param \\*args: Additional positional arguments",
            "            :see: https://docs.sqlalchemy.org/en/latest/dialects/sqlite.html",
            "            \"\"\"",
            "",
            "            with closing(connection.cursor()) as cursor:",
            "                cursor.execute(\"PRAGMA foreign_keys=ON\")",
            "",
            "",
            "def send_email_smtp(  # pylint: disable=invalid-name,too-many-arguments,too-many-locals",
            "    to: str,",
            "    subject: str,",
            "    html_content: str,",
            "    config: dict[str, Any],",
            "    files: list[str] | None = None,",
            "    data: dict[str, str] | None = None,",
            "    images: dict[str, bytes] | None = None,",
            "    dryrun: bool = False,",
            "    cc: str | None = None,",
            "    bcc: str | None = None,",
            "    mime_subtype: str = \"mixed\",",
            "    header_data: HeaderDataType | None = None,",
            ") -> None:",
            "    \"\"\"",
            "    Send an email with html content, eg:",
            "    send_email_smtp(",
            "        'test@example.com', 'foo', '<b>Foo</b> bar',['/dev/null'], dryrun=True)",
            "    \"\"\"",
            "    smtp_mail_from = config[\"SMTP_MAIL_FROM\"]",
            "    smtp_mail_to = get_email_address_list(to)",
            "",
            "    msg = MIMEMultipart(mime_subtype)",
            "    msg[\"Subject\"] = subject",
            "    msg[\"From\"] = smtp_mail_from",
            "    msg[\"To\"] = \", \".join(smtp_mail_to)",
            "",
            "    msg.preamble = \"This is a multi-part message in MIME format.\"",
            "",
            "    recipients = smtp_mail_to",
            "    if cc:",
            "        smtp_mail_cc = get_email_address_list(cc)",
            "        msg[\"CC\"] = \", \".join(smtp_mail_cc)",
            "        recipients = recipients + smtp_mail_cc",
            "",
            "    if bcc:",
            "        # don't add bcc in header",
            "        smtp_mail_bcc = get_email_address_list(bcc)",
            "        recipients = recipients + smtp_mail_bcc",
            "",
            "    msg[\"Date\"] = formatdate(localtime=True)",
            "    mime_text = MIMEText(html_content, \"html\")",
            "    msg.attach(mime_text)",
            "",
            "    # Attach files by reading them from disk",
            "    for fname in files or []:",
            "        basename = os.path.basename(fname)",
            "        with open(fname, \"rb\") as f:",
            "            msg.attach(",
            "                MIMEApplication(",
            "                    f.read(),",
            "                    Content_Disposition=f\"attachment; filename='{basename}'\",",
            "                    Name=basename,",
            "                )",
            "            )",
            "",
            "    # Attach any files passed directly",
            "    for name, body in (data or {}).items():",
            "        msg.attach(",
            "            MIMEApplication(",
            "                body, Content_Disposition=f\"attachment; filename='{name}'\", Name=name",
            "            )",
            "        )",
            "",
            "    # Attach any inline images, which may be required for display in",
            "    # HTML content (inline)",
            "    for msgid, imgdata in (images or {}).items():",
            "        formatted_time = formatdate(localtime=True)",
            "        file_name = f\"{subject} {formatted_time}\"",
            "        image = MIMEImage(imgdata, name=file_name)",
            "        image.add_header(\"Content-ID\", f\"<{msgid}>\")",
            "        image.add_header(\"Content-Disposition\", \"inline\")",
            "        msg.attach(image)",
            "    msg_mutator = config[\"EMAIL_HEADER_MUTATOR\"]",
            "    # the base notification returns the message without any editing.",
            "    new_msg = msg_mutator(msg, **(header_data or {}))",
            "    send_mime_email(smtp_mail_from, recipients, new_msg, config, dryrun=dryrun)",
            "",
            "",
            "def send_mime_email(",
            "    e_from: str,",
            "    e_to: list[str],",
            "    mime_msg: MIMEMultipart,",
            "    config: dict[str, Any],",
            "    dryrun: bool = False,",
            ") -> None:",
            "    smtp_host = config[\"SMTP_HOST\"]",
            "    smtp_port = config[\"SMTP_PORT\"]",
            "    smtp_user = config[\"SMTP_USER\"]",
            "    smtp_password = config[\"SMTP_PASSWORD\"]",
            "    smtp_starttls = config[\"SMTP_STARTTLS\"]",
            "    smtp_ssl = config[\"SMTP_SSL\"]",
            "    smtp_ssl_server_auth = config[\"SMTP_SSL_SERVER_AUTH\"]",
            "",
            "    if dryrun:",
            "        logger.info(\"Dryrun enabled, email notification content is below:\")",
            "        logger.info(mime_msg.as_string())",
            "        return",
            "",
            "    # Default ssl context is SERVER_AUTH using the default system",
            "    # root CA certificates",
            "    ssl_context = ssl.create_default_context() if smtp_ssl_server_auth else None",
            "    smtp = (",
            "        smtplib.SMTP_SSL(smtp_host, smtp_port, context=ssl_context)",
            "        if smtp_ssl",
            "        else smtplib.SMTP(smtp_host, smtp_port)",
            "    )",
            "    if smtp_starttls:",
            "        smtp.starttls(context=ssl_context)",
            "    if smtp_user and smtp_password:",
            "        smtp.login(smtp_user, smtp_password)",
            "    logger.debug(\"Sent an email to %s\", str(e_to))",
            "    smtp.sendmail(e_from, e_to, mime_msg.as_string())",
            "    smtp.quit()",
            "",
            "",
            "def get_email_address_list(address_string: str) -> list[str]:",
            "    address_string_list: list[str] = []",
            "    if isinstance(address_string, str):",
            "        address_string_list = re.split(r\",|\\s|;\", address_string)",
            "    return [x.strip() for x in address_string_list if x.strip()]",
            "",
            "",
            "def get_email_address_str(address_string: str) -> str:",
            "    address_list = get_email_address_list(address_string)",
            "    address_list_str = \", \".join(address_list)",
            "",
            "    return address_list_str",
            "",
            "",
            "def choicify(values: Iterable[Any]) -> list[tuple[Any, Any]]:",
            "    \"\"\"Takes an iterable and makes an iterable of tuples with it\"\"\"",
            "    return [(v, v) for v in values]",
            "",
            "",
            "def zlib_compress(data: bytes | str) -> bytes:",
            "    \"\"\"",
            "    Compress things in a py2/3 safe fashion",
            "    >>> json_str = '{\"test\": 1}'",
            "    >>> blob = zlib_compress(json_str)",
            "    \"\"\"",
            "    if isinstance(data, str):",
            "        return zlib.compress(bytes(data, \"utf-8\"))",
            "    return zlib.compress(data)",
            "",
            "",
            "def zlib_decompress(blob: bytes, decode: bool | None = True) -> bytes | str:",
            "    \"\"\"",
            "    Decompress things to a string in a py2/3 safe fashion",
            "    >>> json_str = '{\"test\": 1}'",
            "    >>> blob = zlib_compress(json_str)",
            "    >>> got_str = zlib_decompress(blob)",
            "    >>> got_str == json_str",
            "    True",
            "    \"\"\"",
            "    if isinstance(blob, bytes):",
            "        decompressed = zlib.decompress(blob)",
            "    else:",
            "        decompressed = zlib.decompress(bytes(blob, \"utf-8\"))",
            "    return decompressed.decode(\"utf-8\") if decode else decompressed",
            "",
            "",
            "def simple_filter_to_adhoc(",
            "    filter_clause: QueryObjectFilterClause,",
            "    clause: str = \"where\",",
            ") -> AdhocFilterClause:",
            "    result: AdhocFilterClause = {",
            "        \"clause\": clause.upper(),",
            "        \"expressionType\": \"SIMPLE\",",
            "        \"comparator\": filter_clause.get(\"val\"),",
            "        \"operator\": filter_clause[\"op\"],",
            "        \"subject\": cast(str, filter_clause[\"col\"]),",
            "    }",
            "    if filter_clause.get(\"isExtra\"):",
            "        result[\"isExtra\"] = True",
            "    result[\"filterOptionName\"] = md5_sha_from_dict(cast(dict[Any, Any], result))",
            "",
            "    return result",
            "",
            "",
            "def form_data_to_adhoc(form_data: dict[str, Any], clause: str) -> AdhocFilterClause:",
            "    if clause not in (\"where\", \"having\"):",
            "        raise ValueError(__(\"Unsupported clause type: %(clause)s\", clause=clause))",
            "    result: AdhocFilterClause = {",
            "        \"clause\": clause.upper(),",
            "        \"expressionType\": \"SQL\",",
            "        \"sqlExpression\": form_data.get(clause),",
            "    }",
            "    result[\"filterOptionName\"] = md5_sha_from_dict(cast(dict[Any, Any], result))",
            "",
            "    return result",
            "",
            "",
            "def merge_extra_form_data(form_data: dict[str, Any]) -> None:",
            "    \"\"\"",
            "    Merge extra form data (appends and overrides) into the main payload",
            "    and add applied time extras to the payload.",
            "    \"\"\"",
            "    filter_keys = [\"filters\", \"adhoc_filters\"]",
            "    extra_form_data = form_data.pop(\"extra_form_data\", {})",
            "    append_filters: list[QueryObjectFilterClause] = extra_form_data.get(\"filters\", None)",
            "",
            "    # merge append extras",
            "    for key in [key for key in EXTRA_FORM_DATA_APPEND_KEYS if key not in filter_keys]:",
            "        extra_value = getattr(extra_form_data, key, {})",
            "        form_value = getattr(form_data, key, {})",
            "        form_value.update(extra_value)",
            "        if form_value:",
            "            form_data[\"key\"] = extra_value",
            "",
            "    # map regular extras that apply to form data properties",
            "    for src_key, target_key in EXTRA_FORM_DATA_OVERRIDE_REGULAR_MAPPINGS.items():",
            "        value = extra_form_data.get(src_key)",
            "        if value is not None:",
            "            form_data[target_key] = value",
            "",
            "    # map extras that apply to form data extra properties",
            "    extras = form_data.get(\"extras\", {})",
            "    for key in EXTRA_FORM_DATA_OVERRIDE_EXTRA_KEYS:",
            "        value = extra_form_data.get(key)",
            "        if value is not None:",
            "            extras[key] = value",
            "    if extras:",
            "        form_data[\"extras\"] = extras",
            "",
            "    adhoc_filters: list[AdhocFilterClause] = form_data.get(\"adhoc_filters\", [])",
            "    form_data[\"adhoc_filters\"] = adhoc_filters",
            "    append_adhoc_filters: list[AdhocFilterClause] = extra_form_data.get(",
            "        \"adhoc_filters\", []",
            "    )",
            "    adhoc_filters.extend(",
            "        {\"isExtra\": True, **fltr} for fltr in append_adhoc_filters  # type: ignore",
            "    )",
            "    if append_filters:",
            "        for key, value in form_data.items():",
            "            if re.match(\"adhoc_filter.*\", key):",
            "                value.extend(",
            "                    simple_filter_to_adhoc({\"isExtra\": True, **fltr})  # type: ignore",
            "                    for fltr in append_filters",
            "                    if fltr",
            "                )",
            "    if form_data.get(\"time_range\") and not form_data.get(\"granularity_sqla\"):",
            "        for adhoc_filter in form_data.get(\"adhoc_filters\", []):",
            "            if adhoc_filter.get(\"operator\") == \"TEMPORAL_RANGE\":",
            "                adhoc_filter[\"comparator\"] = form_data[\"time_range\"]",
            "",
            "",
            "def merge_extra_filters(form_data: dict[str, Any]) -> None:",
            "    # extra_filters are temporary/contextual filters (using the legacy constructs)",
            "    # that are external to the slice definition. We use those for dynamic",
            "    # interactive filters like the ones emitted by the \"Filter Box\" visualization.",
            "    # Note extra_filters only support simple filters.",
            "    form_data.setdefault(\"applied_time_extras\", {})",
            "    adhoc_filters = form_data.get(\"adhoc_filters\", [])",
            "    form_data[\"adhoc_filters\"] = adhoc_filters",
            "    merge_extra_form_data(form_data)",
            "    if \"extra_filters\" in form_data:",
            "        # __form and __to are special extra_filters that target time",
            "        # boundaries. The rest of extra_filters are simple",
            "        # [column_name in list_of_values]. `__` prefix is there to avoid",
            "        # potential conflicts with column that would be named `from` or `to`",
            "        date_options = {",
            "            \"__time_range\": \"time_range\",",
            "            \"__time_col\": \"granularity_sqla\",",
            "            \"__time_grain\": \"time_grain_sqla\",",
            "        }",
            "",
            "        # Grab list of existing filters 'keyed' on the column and operator",
            "",
            "        def get_filter_key(f: dict[str, Any]) -> str:",
            "            if \"expressionType\" in f:",
            "                return f\"{f['subject']}__{f['operator']}\"",
            "",
            "            return f\"{f['col']}__{f['op']}\"",
            "",
            "        existing_filters = {}",
            "        for existing in adhoc_filters:",
            "            if (",
            "                existing[\"expressionType\"] == \"SIMPLE\"",
            "                and existing.get(\"comparator\") is not None",
            "                and existing.get(\"subject\") is not None",
            "            ):",
            "                existing_filters[get_filter_key(existing)] = existing[\"comparator\"]",
            "",
            "        for filtr in form_data[  # pylint: disable=too-many-nested-blocks",
            "            \"extra_filters\"",
            "        ]:",
            "            filtr[\"isExtra\"] = True",
            "            # Pull out time filters/options and merge into form data",
            "            filter_column = filtr[\"col\"]",
            "            if time_extra := date_options.get(filter_column):",
            "                time_extra_value = filtr.get(\"val\")",
            "                if time_extra_value and time_extra_value != NO_TIME_RANGE:",
            "                    form_data[time_extra] = time_extra_value",
            "                    form_data[\"applied_time_extras\"][filter_column] = time_extra_value",
            "            elif filtr[\"val\"]:",
            "                # Merge column filters",
            "                if (filter_key := get_filter_key(filtr)) in existing_filters:",
            "                    # Check if the filter already exists",
            "                    if isinstance(filtr[\"val\"], list):",
            "                        if isinstance(existing_filters[filter_key], list):",
            "                            # Add filters for unequal lists",
            "                            # order doesn't matter",
            "                            if set(existing_filters[filter_key]) != set(filtr[\"val\"]):",
            "                                adhoc_filters.append(simple_filter_to_adhoc(filtr))",
            "                        else:",
            "                            adhoc_filters.append(simple_filter_to_adhoc(filtr))",
            "                    else:",
            "                        # Do not add filter if same value already exists",
            "                        if filtr[\"val\"] != existing_filters[filter_key]:",
            "                            adhoc_filters.append(simple_filter_to_adhoc(filtr))",
            "                else:",
            "                    # Filter not found, add it",
            "                    adhoc_filters.append(simple_filter_to_adhoc(filtr))",
            "        # Remove extra filters from the form data since no longer needed",
            "        del form_data[\"extra_filters\"]",
            "",
            "",
            "def merge_request_params(form_data: dict[str, Any], params: dict[str, Any]) -> None:",
            "    \"\"\"",
            "    Merge request parameters to the key `url_params` in form_data. Only updates",
            "    or appends parameters to `form_data` that are defined in `params; preexisting",
            "    parameters not defined in params are left unchanged.",
            "",
            "    :param form_data: object to be updated",
            "    :param params: request parameters received via query string",
            "    \"\"\"",
            "    url_params = form_data.get(\"url_params\", {})",
            "    for key, value in params.items():",
            "        if key in (\"form_data\", \"r\"):",
            "            continue",
            "        url_params[key] = value",
            "    form_data[\"url_params\"] = url_params",
            "",
            "",
            "def user_label(user: User) -> str | None:",
            "    \"\"\"Given a user ORM FAB object, returns a label\"\"\"",
            "    if user:",
            "        if user.first_name and user.last_name:",
            "            return user.first_name + \" \" + user.last_name",
            "",
            "        return user.username",
            "",
            "    return None",
            "",
            "",
            "def get_example_default_schema() -> str | None:",
            "    \"\"\"",
            "    Return the default schema of the examples database, if any.",
            "    \"\"\"",
            "    database = get_example_database()",
            "    with database.get_sqla_engine_with_context() as engine:",
            "        return inspect(engine).default_schema_name",
            "",
            "",
            "def backend() -> str:",
            "    return get_example_database().backend",
            "",
            "",
            "def is_adhoc_metric(metric: Metric) -> TypeGuard[AdhocMetric]:",
            "    return isinstance(metric, dict) and \"expressionType\" in metric",
            "",
            "",
            "def is_adhoc_column(column: Column) -> TypeGuard[AdhocColumn]:",
            "    return isinstance(column, dict) and ({\"label\", \"sqlExpression\"}).issubset(",
            "        column.keys()",
            "    )",
            "",
            "",
            "def get_base_axis_labels(columns: list[Column] | None) -> tuple[str, ...]:",
            "    axis_cols = [",
            "        col",
            "        for col in columns or []",
            "        if is_adhoc_column(col) and col.get(\"columnType\") == \"BASE_AXIS\"",
            "    ]",
            "    return tuple(get_column_name(col) for col in axis_cols)",
            "",
            "",
            "def get_xaxis_label(columns: list[Column] | None) -> str | None:",
            "    labels = get_base_axis_labels(columns)",
            "    return labels[0] if labels else None",
            "",
            "",
            "def get_column_name(column: Column, verbose_map: dict[str, Any] | None = None) -> str:",
            "    \"\"\"",
            "    Extract label from column",
            "",
            "    :param column: object to extract label from",
            "    :param verbose_map: verbose_map from dataset for optional mapping from",
            "                        raw name to verbose name",
            "    :return: String representation of column",
            "    :raises ValueError: if metric object is invalid",
            "    \"\"\"",
            "    if isinstance(column, dict):",
            "        if label := column.get(\"label\"):",
            "            return label",
            "        if expr := column.get(\"sqlExpression\"):",
            "            return expr",
            "",
            "    if isinstance(column, str):",
            "        verbose_map = verbose_map or {}",
            "        return verbose_map.get(column, column)",
            "",
            "    raise ValueError(\"Missing label\")",
            "",
            "",
            "def get_metric_name(metric: Metric, verbose_map: dict[str, Any] | None = None) -> str:",
            "    \"\"\"",
            "    Extract label from metric",
            "",
            "    :param metric: object to extract label from",
            "    :param verbose_map: verbose_map from dataset for optional mapping from",
            "                        raw name to verbose name",
            "    :return: String representation of metric",
            "    :raises ValueError: if metric object is invalid",
            "    \"\"\"",
            "    if is_adhoc_metric(metric):",
            "        if label := metric.get(\"label\"):",
            "            return label",
            "        if (expression_type := metric.get(\"expressionType\")) == \"SQL\":",
            "            if sql_expression := metric.get(\"sqlExpression\"):",
            "                return sql_expression",
            "        if expression_type == \"SIMPLE\":",
            "            column: AdhocMetricColumn = metric.get(\"column\") or {}",
            "            column_name = column.get(\"column_name\")",
            "            aggregate = metric.get(\"aggregate\")",
            "            if column and aggregate:",
            "                return f\"{aggregate}({column_name})\"",
            "            if column_name:",
            "                return column_name",
            "",
            "    if isinstance(metric, str):",
            "        verbose_map = verbose_map or {}",
            "        return verbose_map.get(metric, metric)",
            "",
            "    raise ValueError(__(\"Invalid metric object: %(metric)s\", metric=str(metric)))",
            "",
            "",
            "def get_column_names(",
            "    columns: Sequence[Column] | None,",
            "    verbose_map: dict[str, Any] | None = None,",
            ") -> list[str]:",
            "    return [",
            "        column",
            "        for column in [get_column_name(column, verbose_map) for column in columns or []]",
            "        if column",
            "    ]",
            "",
            "",
            "def get_metric_names(",
            "    metrics: Sequence[Metric] | None,",
            "    verbose_map: dict[str, Any] | None = None,",
            ") -> list[str]:",
            "    return [",
            "        metric",
            "        for metric in [get_metric_name(metric, verbose_map) for metric in metrics or []]",
            "        if metric",
            "    ]",
            "",
            "",
            "def get_first_metric_name(",
            "    metrics: Sequence[Metric] | None,",
            "    verbose_map: dict[str, Any] | None = None,",
            ") -> str | None:",
            "    metric_labels = get_metric_names(metrics, verbose_map)",
            "    return metric_labels[0] if metric_labels else None",
            "",
            "",
            "def ensure_path_exists(path: str) -> None:",
            "    try:",
            "        os.makedirs(path)",
            "    except OSError as ex:",
            "        if not (os.path.isdir(path) and ex.errno == errno.EEXIST):",
            "            raise",
            "",
            "",
            "def convert_legacy_filters_into_adhoc(  # pylint: disable=invalid-name",
            "    form_data: FormData,",
            ") -> None:",
            "    if not form_data.get(\"adhoc_filters\"):",
            "        adhoc_filters: list[AdhocFilterClause] = []",
            "        form_data[\"adhoc_filters\"] = adhoc_filters",
            "",
            "        for clause in (\"having\", \"where\"):",
            "            if clause in form_data and form_data[clause] != \"\":",
            "                adhoc_filters.append(form_data_to_adhoc(form_data, clause))",
            "",
            "        if \"filters\" in form_data:",
            "            adhoc_filters.extend(",
            "                simple_filter_to_adhoc(fltr, \"where\")",
            "                for fltr in form_data[\"filters\"]",
            "                if fltr is not None",
            "            )",
            "",
            "    for key in (\"filters\", \"having\", \"where\"):",
            "        if key in form_data:",
            "            del form_data[key]",
            "",
            "",
            "def split_adhoc_filters_into_base_filters(  # pylint: disable=invalid-name",
            "    form_data: FormData,",
            ") -> None:",
            "    \"\"\"",
            "    Mutates form data to restructure the adhoc filters in the form of the three base",
            "    filters, `where`, `having`, and `filters` which represent free form where sql,",
            "    free form having sql, and structured where clauses.",
            "    \"\"\"",
            "    adhoc_filters = form_data.get(\"adhoc_filters\")",
            "    if isinstance(adhoc_filters, list):",
            "        simple_where_filters = []",
            "        sql_where_filters = []",
            "        sql_having_filters = []",
            "        for adhoc_filter in adhoc_filters:",
            "            expression_type = adhoc_filter.get(\"expressionType\")",
            "            clause = adhoc_filter.get(\"clause\")",
            "            if expression_type == \"SIMPLE\":",
            "                if clause == \"WHERE\":",
            "                    simple_where_filters.append(",
            "                        {",
            "                            \"col\": adhoc_filter.get(\"subject\"),",
            "                            \"op\": adhoc_filter.get(\"operator\"),",
            "                            \"val\": adhoc_filter.get(\"comparator\"),",
            "                        }",
            "                    )",
            "            elif expression_type == \"SQL\":",
            "                sql_expression = adhoc_filter.get(\"sqlExpression\")",
            "                sql_expression = sanitize_clause(sql_expression)",
            "                if clause == \"WHERE\":",
            "                    sql_where_filters.append(sql_expression)",
            "                elif clause == \"HAVING\":",
            "                    sql_having_filters.append(sql_expression)",
            "        form_data[\"where\"] = \" AND \".join([f\"({sql})\" for sql in sql_where_filters])",
            "        form_data[\"having\"] = \" AND \".join([f\"({sql})\" for sql in sql_having_filters])",
            "        form_data[\"filters\"] = simple_where_filters",
            "",
            "",
            "def get_username() -> str | None:",
            "    \"\"\"",
            "    Get username (if defined) associated with the current user.",
            "",
            "    :returns: The username",
            "    \"\"\"",
            "",
            "    try:",
            "        return g.user.username",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "def get_user_id() -> int | None:",
            "    \"\"\"",
            "    Get the user identifier (if defined) associated with the current user.",
            "",
            "    Though the Flask-AppBuilder `User` and Flask-Login  `AnonymousUserMixin` and",
            "    `UserMixin` models provide a convenience `get_id` method, for generality, the",
            "    identifier is encoded as a `str` whereas in Superset all identifiers are encoded as",
            "    an `int`.",
            "",
            "    returns: The user identifier",
            "    \"\"\"",
            "",
            "    try:",
            "        return g.user.id",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "@contextmanager",
            "def override_user(user: User | None, force: bool = True) -> Iterator[Any]:",
            "    \"\"\"",
            "    Temporarily override the current user per `flask.g` with the specified user.",
            "",
            "    Sometimes, often in the context of async Celery tasks, it is useful to switch the",
            "    current user (which may be undefined) to different one, execute some SQLAlchemy",
            "    tasks et al. and then revert back to the original one.",
            "",
            "    :param user: The override user",
            "    :param force: Whether to override the current user if set",
            "    \"\"\"",
            "",
            "    if hasattr(g, \"user\"):",
            "        if force or g.user is None:",
            "            current = g.user",
            "            g.user = user",
            "            yield",
            "            g.user = current",
            "        else:",
            "            yield",
            "    else:",
            "        g.user = user",
            "        yield",
            "        delattr(g, \"user\")",
            "",
            "",
            "def parse_ssl_cert(certificate: str) -> Certificate:",
            "    \"\"\"",
            "    Parses the contents of a certificate and returns a valid certificate object",
            "    if valid.",
            "",
            "    :param certificate: Contents of certificate file",
            "    :return: Valid certificate instance",
            "    :raises CertificateException: If certificate is not valid/unparseable",
            "    \"\"\"",
            "    try:",
            "        return load_pem_x509_certificate(certificate.encode(\"utf-8\"), default_backend())",
            "    except ValueError as ex:",
            "        raise CertificateException(\"Invalid certificate\") from ex",
            "",
            "",
            "def create_ssl_cert_file(certificate: str) -> str:",
            "    \"\"\"",
            "    This creates a certificate file that can be used to validate HTTPS",
            "    sessions. A certificate is only written to disk once; on subsequent calls,",
            "    only the path of the existing certificate is returned.",
            "",
            "    :param certificate: The contents of the certificate",
            "    :return: The path to the certificate file",
            "    :raises CertificateException: If certificate is not valid/unparseable",
            "    \"\"\"",
            "    filename = f\"{md5_sha_from_str(certificate)}.crt\"",
            "    cert_dir = current_app.config[\"SSL_CERT_PATH\"]",
            "    path = cert_dir if cert_dir else tempfile.gettempdir()",
            "    path = os.path.join(path, filename)",
            "    if not os.path.exists(path):",
            "        # Validate certificate prior to persisting to temporary directory",
            "        parse_ssl_cert(certificate)",
            "        with open(path, \"w\") as cert_file:",
            "            cert_file.write(certificate)",
            "    return path",
            "",
            "",
            "def time_function(",
            "    func: Callable[..., FlaskResponse], *args: Any, **kwargs: Any",
            ") -> tuple[float, Any]:",
            "    \"\"\"",
            "    Measures the amount of time a function takes to execute in ms",
            "",
            "    :param func: The function execution time to measure",
            "    :param args: args to be passed to the function",
            "    :param kwargs: kwargs to be passed to the function",
            "    :return: A tuple with the duration and response from the function",
            "    \"\"\"",
            "    start = default_timer()",
            "    response = func(*args, **kwargs)",
            "    stop = default_timer()",
            "    return (stop - start) * 1000.0, response",
            "",
            "",
            "def MediumText() -> Variant:  # pylint:disable=invalid-name",
            "    return Text().with_variant(MEDIUMTEXT(), \"mysql\")",
            "",
            "",
            "def shortid() -> str:",
            "    return f\"{uuid.uuid4()}\"[-12:]",
            "",
            "",
            "class DatasourceName(NamedTuple):",
            "    table: str",
            "    schema: str",
            "",
            "",
            "def get_stacktrace() -> str | None:",
            "    if current_app.config[\"SHOW_STACKTRACE\"]:",
            "        return traceback.format_exc()",
            "    return None",
            "",
            "",
            "def split(",
            "    string: str, delimiter: str = \" \", quote: str = '\"', escaped_quote: str = r\"\\\"\"",
            ") -> Iterator[str]:",
            "    \"\"\"",
            "    A split function that is aware of quotes and parentheses.",
            "",
            "    :param string: string to split",
            "    :param delimiter: string defining where to split, usually a comma or space",
            "    :param quote: string, either a single or a double quote",
            "    :param escaped_quote: string representing an escaped quote",
            "    :return: list of strings",
            "    \"\"\"",
            "    parens = 0",
            "    quotes = False",
            "    i = 0",
            "    for j, character in enumerate(string):",
            "        complete = parens == 0 and not quotes",
            "        if complete and character == delimiter:",
            "            yield string[i:j]",
            "            i = j + len(delimiter)",
            "        elif character == \"(\":",
            "            parens += 1",
            "        elif character == \")\":",
            "            parens -= 1",
            "        elif character == quote:",
            "            if quotes and string[j - len(escaped_quote) + 1 : j + 1] != escaped_quote:",
            "                quotes = False",
            "            elif not quotes:",
            "                quotes = True",
            "    yield string[i:]",
            "",
            "",
            "T = TypeVar(\"T\")",
            "",
            "",
            "def as_list(x: T | list[T]) -> list[T]:",
            "    \"\"\"",
            "    Wrap an object in a list if it's not a list.",
            "",
            "    :param x: The object",
            "    :returns: A list wrapping the object if it's not already a list",
            "    \"\"\"",
            "    return x if isinstance(x, list) else [x]",
            "",
            "",
            "def get_form_data_token(form_data: dict[str, Any]) -> str:",
            "    \"\"\"",
            "    Return the token contained within form data or generate a new one.",
            "",
            "    :param form_data: chart form data",
            "    :return: original token if predefined, otherwise new uuid4 based token",
            "    \"\"\"",
            "    return form_data.get(\"token\") or \"token_\" + uuid.uuid4().hex[:8]",
            "",
            "",
            "def get_column_name_from_column(column: Column) -> str | None:",
            "    \"\"\"",
            "    Extract the physical column that a column is referencing. If the column is",
            "    an adhoc column, always returns `None`.",
            "",
            "    :param column: Physical and ad-hoc column",
            "    :return: column name if physical column, otherwise None",
            "    \"\"\"",
            "    if is_adhoc_column(column):",
            "        return None",
            "    return column  # type: ignore",
            "",
            "",
            "def get_column_names_from_columns(columns: list[Column]) -> list[str]:",
            "    \"\"\"",
            "    Extract the physical columns that a list of columns are referencing. Ignore",
            "    adhoc columns",
            "",
            "    :param columns: Physical and adhoc columns",
            "    :return: column names of all physical columns",
            "    \"\"\"",
            "    return [col for col in map(get_column_name_from_column, columns) if col]",
            "",
            "",
            "def get_column_name_from_metric(metric: Metric) -> str | None:",
            "    \"\"\"",
            "    Extract the column that a metric is referencing. If the metric isn't",
            "    a simple metric, always returns `None`.",
            "",
            "    :param metric: Ad-hoc metric",
            "    :return: column name if simple metric, otherwise None",
            "    \"\"\"",
            "    if is_adhoc_metric(metric):",
            "        metric = cast(AdhocMetric, metric)",
            "        if metric[\"expressionType\"] == AdhocMetricExpressionType.SIMPLE:",
            "            return cast(dict[str, Any], metric[\"column\"])[\"column_name\"]",
            "    return None",
            "",
            "",
            "def get_column_names_from_metrics(metrics: list[Metric]) -> list[str]:",
            "    \"\"\"",
            "    Extract the columns that a list of metrics are referencing. Excludes all",
            "    SQL metrics.",
            "",
            "    :param metrics: Ad-hoc metric",
            "    :return: column name if simple metric, otherwise None",
            "    \"\"\"",
            "    return [col for col in map(get_column_name_from_metric, metrics) if col]",
            "",
            "",
            "def extract_dataframe_dtypes(",
            "    df: pd.DataFrame,",
            "    datasource: BaseDatasource | Query | None = None,",
            ") -> list[GenericDataType]:",
            "    \"\"\"Serialize pandas/numpy dtypes to generic types\"\"\"",
            "",
            "    # omitting string types as those will be the default type",
            "    inferred_type_map: dict[str, GenericDataType] = {",
            "        \"floating\": GenericDataType.NUMERIC,",
            "        \"integer\": GenericDataType.NUMERIC,",
            "        \"mixed-integer-float\": GenericDataType.NUMERIC,",
            "        \"decimal\": GenericDataType.NUMERIC,",
            "        \"boolean\": GenericDataType.BOOLEAN,",
            "        \"datetime64\": GenericDataType.TEMPORAL,",
            "        \"datetime\": GenericDataType.TEMPORAL,",
            "        \"date\": GenericDataType.TEMPORAL,",
            "    }",
            "",
            "    columns_by_name: dict[str, Any] = {}",
            "    if datasource:",
            "        for column in datasource.columns:",
            "            if isinstance(column, dict):",
            "                columns_by_name[column.get(\"column_name\")] = column",
            "            else:",
            "                columns_by_name[column.column_name] = column",
            "",
            "    generic_types: list[GenericDataType] = []",
            "    for column in df.columns:",
            "        column_object = columns_by_name.get(column)",
            "        series = df[column]",
            "        inferred_type = infer_dtype(series)",
            "        if isinstance(column_object, dict):",
            "            generic_type = (",
            "                GenericDataType.TEMPORAL",
            "                if column_object and column_object.get(\"is_dttm\")",
            "                else inferred_type_map.get(inferred_type, GenericDataType.STRING)",
            "            )",
            "        else:",
            "            generic_type = (",
            "                GenericDataType.TEMPORAL",
            "                if column_object and column_object.is_dttm",
            "                else inferred_type_map.get(inferred_type, GenericDataType.STRING)",
            "            )",
            "        generic_types.append(generic_type)",
            "",
            "    return generic_types",
            "",
            "",
            "def extract_column_dtype(col: BaseColumn) -> GenericDataType:",
            "    if col.is_temporal:",
            "        return GenericDataType.TEMPORAL",
            "    if col.is_numeric:",
            "        return GenericDataType.NUMERIC",
            "    # TODO: add check for boolean data type when proper support is added",
            "    return GenericDataType.STRING",
            "",
            "",
            "def indexed(items: list[Any], key: str | Callable[[Any], Any]) -> dict[Any, list[Any]]:",
            "    \"\"\"Build an index for a list of objects\"\"\"",
            "    idx: dict[Any, Any] = {}",
            "    for item in items:",
            "        key_ = getattr(item, key) if isinstance(key, str) else key(item)",
            "        idx.setdefault(key_, []).append(item)",
            "    return idx",
            "",
            "",
            "def is_test() -> bool:",
            "    return parse_boolean_string(os.environ.get(\"SUPERSET_TESTENV\", \"false\"))",
            "",
            "",
            "def get_time_filter_status(",
            "    datasource: BaseDatasource,",
            "    applied_time_extras: dict[str, str],",
            ") -> tuple[list[dict[str, str]], list[dict[str, str]]]:",
            "    temporal_columns: set[Any] = {",
            "        col.column_name for col in datasource.columns if col.is_dttm",
            "    }",
            "    applied: list[dict[str, str]] = []",
            "    rejected: list[dict[str, str]] = []",
            "    if time_column := applied_time_extras.get(ExtraFiltersTimeColumnType.TIME_COL):",
            "        if time_column in temporal_columns:",
            "            applied.append({\"column\": ExtraFiltersTimeColumnType.TIME_COL})",
            "        else:",
            "            rejected.append(",
            "                {",
            "                    \"reason\": ExtraFiltersReasonType.COL_NOT_IN_DATASOURCE,",
            "                    \"column\": ExtraFiltersTimeColumnType.TIME_COL,",
            "                }",
            "            )",
            "",
            "    if ExtraFiltersTimeColumnType.TIME_GRAIN in applied_time_extras:",
            "        # are there any temporal columns to assign the time grain to?",
            "        if temporal_columns:",
            "            applied.append({\"column\": ExtraFiltersTimeColumnType.TIME_GRAIN})",
            "        else:",
            "            rejected.append(",
            "                {",
            "                    \"reason\": ExtraFiltersReasonType.NO_TEMPORAL_COLUMN,",
            "                    \"column\": ExtraFiltersTimeColumnType.TIME_GRAIN,",
            "                }",
            "            )",
            "",
            "    if applied_time_extras.get(ExtraFiltersTimeColumnType.TIME_RANGE):",
            "        # are there any temporal columns to assign the time range to?",
            "        if temporal_columns:",
            "            applied.append({\"column\": ExtraFiltersTimeColumnType.TIME_RANGE})",
            "        else:",
            "            rejected.append(",
            "                {",
            "                    \"reason\": ExtraFiltersReasonType.NO_TEMPORAL_COLUMN,",
            "                    \"column\": ExtraFiltersTimeColumnType.TIME_RANGE,",
            "                }",
            "            )",
            "",
            "    return applied, rejected",
            "",
            "",
            "def format_list(items: Sequence[str], sep: str = \", \", quote: str = '\"') -> str:",
            "    quote_escaped = \"\\\\\" + quote",
            "    return sep.join(f\"{quote}{x.replace(quote, quote_escaped)}{quote}\" for x in items)",
            "",
            "",
            "def find_duplicates(items: Iterable[InputType]) -> list[InputType]:",
            "    \"\"\"Find duplicate items in an iterable.\"\"\"",
            "    return [item for item, count in collections.Counter(items).items() if count > 1]",
            "",
            "",
            "def remove_duplicates(",
            "    items: Iterable[InputType], key: Callable[[InputType], Any] | None = None",
            ") -> list[InputType]:",
            "    \"\"\"Remove duplicate items in an iterable.\"\"\"",
            "    if not key:",
            "        return list(dict.fromkeys(items).keys())",
            "    seen = set()",
            "    result = []",
            "    for item in items:",
            "        item_key = key(item)",
            "        if item_key not in seen:",
            "            seen.add(item_key)",
            "            result.append(item)",
            "    return result",
            "",
            "",
            "@dataclass",
            "class DateColumn:",
            "    col_label: str",
            "    timestamp_format: str | None = None",
            "    offset: int | None = None",
            "    time_shift: str | None = None",
            "",
            "    def __hash__(self) -> int:",
            "        return hash(self.col_label)",
            "",
            "    def __eq__(self, other: object) -> bool:",
            "        return isinstance(other, DateColumn) and hash(self) == hash(other)",
            "",
            "    @classmethod",
            "    def get_legacy_time_column(",
            "        cls,",
            "        timestamp_format: str | None,",
            "        offset: int | None,",
            "        time_shift: str | None,",
            "    ) -> DateColumn:",
            "        return cls(",
            "            timestamp_format=timestamp_format,",
            "            offset=offset,",
            "            time_shift=time_shift,",
            "            col_label=DTTM_ALIAS,",
            "        )",
            "",
            "",
            "def normalize_dttm_col(",
            "    df: pd.DataFrame,",
            "    dttm_cols: tuple[DateColumn, ...] = tuple(),",
            ") -> None:",
            "    for _col in dttm_cols:",
            "        if _col.col_label not in df.columns:",
            "            continue",
            "",
            "        if _col.timestamp_format in (\"epoch_s\", \"epoch_ms\"):",
            "            dttm_series = df[_col.col_label]",
            "            if is_numeric_dtype(dttm_series):",
            "                # Column is formatted as a numeric value",
            "                unit = _col.timestamp_format.replace(\"epoch_\", \"\")",
            "                df[_col.col_label] = pd.to_datetime(",
            "                    dttm_series,",
            "                    utc=False,",
            "                    unit=unit,",
            "                    origin=\"unix\",",
            "                    errors=\"raise\",",
            "                    exact=False,",
            "                )",
            "            else:",
            "                # Column has already been formatted as a timestamp.",
            "                df[_col.col_label] = dttm_series.apply(pd.Timestamp)",
            "        else:",
            "            df[_col.col_label] = pd.to_datetime(",
            "                df[_col.col_label],",
            "                utc=False,",
            "                format=_col.timestamp_format,",
            "                errors=\"raise\",",
            "                exact=False,",
            "            )",
            "        if _col.offset:",
            "            df[_col.col_label] += timedelta(hours=_col.offset)",
            "        if _col.time_shift is not None:",
            "            df[_col.col_label] += parse_human_timedelta(_col.time_shift)",
            "",
            "",
            "def parse_boolean_string(bool_str: str | None) -> bool:",
            "    \"\"\"",
            "    Convert a string representation of a true/false value into a boolean",
            "",
            "    >>> parse_boolean_string(None)",
            "    False",
            "    >>> parse_boolean_string('false')",
            "    False",
            "    >>> parse_boolean_string('true')",
            "    True",
            "    >>> parse_boolean_string('False')",
            "    False",
            "    >>> parse_boolean_string('True')",
            "    True",
            "    >>> parse_boolean_string('foo')",
            "    False",
            "    >>> parse_boolean_string('0')",
            "    False",
            "    >>> parse_boolean_string('1')",
            "    True",
            "",
            "    :param bool_str: string representation of a value that is assumed to be boolean",
            "    :return: parsed boolean value",
            "    \"\"\"",
            "    if bool_str is None:",
            "        return False",
            "    return bool_str.lower() in (\"y\", \"Y\", \"yes\", \"True\", \"t\", \"true\", \"On\", \"on\", \"1\")",
            "",
            "",
            "def apply_max_row_limit(",
            "    limit: int,",
            "    max_limit: int | None = None,",
            ") -> int:",
            "    \"\"\"",
            "    Override row limit if max global limit is defined",
            "",
            "    :param limit: requested row limit",
            "    :param max_limit: Maximum allowed row limit",
            "    :return: Capped row limit",
            "",
            "    >>> apply_max_row_limit(100000, 10)",
            "    10",
            "    >>> apply_max_row_limit(10, 100000)",
            "    10",
            "    >>> apply_max_row_limit(0, 10000)",
            "    10000",
            "    \"\"\"",
            "    if max_limit is None:",
            "        max_limit = current_app.config[\"SQL_MAX_ROW\"]",
            "    if limit != 0:",
            "        return min(max_limit, limit)",
            "    return max_limit",
            "",
            "",
            "def create_zip(files: dict[str, Any]) -> BytesIO:",
            "    buf = BytesIO()",
            "    with ZipFile(buf, \"w\") as bundle:",
            "        for filename, contents in files.items():",
            "            with bundle.open(filename, \"w\") as fp:",
            "                fp.write(contents)",
            "    buf.seek(0)",
            "    return buf",
            "",
            "",
            "def remove_extra_adhoc_filters(form_data: dict[str, Any]) -> None:",
            "    \"\"\"",
            "    Remove filters from slice data that originate from a filter box or native filter",
            "    \"\"\"",
            "    adhoc_filters = {",
            "        key: value for key, value in form_data.items() if ADHOC_FILTERS_REGEX.match(key)",
            "    }",
            "    for key, value in adhoc_filters.items():",
            "        form_data[key] = [",
            "            filter_ for filter_ in value or [] if not filter_.get(\"isExtra\")",
            "        ]"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"Utility functions used across Superset\"\"\"",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import _thread",
            "import collections",
            "import decimal",
            "import errno",
            "import json",
            "import logging",
            "import os",
            "import platform",
            "import re",
            "import signal",
            "import smtplib",
            "import sqlite3",
            "import ssl",
            "import tempfile",
            "import threading",
            "import traceback",
            "import uuid",
            "import zlib",
            "from collections.abc import Iterable, Iterator, Sequence",
            "from contextlib import closing, contextmanager",
            "from dataclasses import dataclass",
            "from datetime import date, datetime, time, timedelta",
            "from email.mime.application import MIMEApplication",
            "from email.mime.image import MIMEImage",
            "from email.mime.multipart import MIMEMultipart",
            "from email.mime.text import MIMEText",
            "from email.utils import formatdate",
            "from enum import Enum, IntEnum",
            "from io import BytesIO",
            "from timeit import default_timer",
            "from types import TracebackType",
            "from typing import Any, Callable, cast, NamedTuple, TYPE_CHECKING, TypedDict, TypeVar",
            "from urllib.parse import unquote_plus",
            "from zipfile import ZipFile",
            "",
            "import markdown as md",
            "import nh3",
            "import numpy as np",
            "import pandas as pd",
            "import sqlalchemy as sa",
            "from cryptography.hazmat.backends import default_backend",
            "from cryptography.x509 import Certificate, load_pem_x509_certificate",
            "from flask import current_app, flash, g, Markup, request",
            "from flask_appbuilder import SQLA",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __",
            "from flask_babel.speaklater import LazyString",
            "from pandas.api.types import infer_dtype",
            "from pandas.core.dtypes.common import is_numeric_dtype",
            "from sqlalchemy import event, exc, inspect, select, Text",
            "from sqlalchemy.dialects.mysql import MEDIUMTEXT",
            "from sqlalchemy.engine import Connection, Engine",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.sql.type_api import Variant",
            "from sqlalchemy.types import TEXT, TypeDecorator, TypeEngine",
            "from typing_extensions import TypeGuard",
            "",
            "from superset.constants import (",
            "    EXTRA_FORM_DATA_APPEND_KEYS,",
            "    EXTRA_FORM_DATA_OVERRIDE_EXTRA_KEYS,",
            "    EXTRA_FORM_DATA_OVERRIDE_REGULAR_MAPPINGS,",
            "    NO_TIME_RANGE,",
            ")",
            "from superset.errors import ErrorLevel, SupersetErrorType",
            "from superset.exceptions import (",
            "    CertificateException,",
            "    SupersetException,",
            "    SupersetTimeoutException,",
            ")",
            "from superset.sql_parse import sanitize_clause",
            "from superset.superset_typing import (",
            "    AdhocColumn,",
            "    AdhocMetric,",
            "    AdhocMetricColumn,",
            "    Column,",
            "    FilterValues,",
            "    FlaskResponse,",
            "    FormData,",
            "    Metric,",
            ")",
            "from superset.utils.backports import StrEnum",
            "from superset.utils.database import get_example_database",
            "from superset.utils.date_parser import parse_human_timedelta",
            "from superset.utils.dates import datetime_to_epoch, EPOCH",
            "from superset.utils.hashing import md5_sha_from_dict, md5_sha_from_str",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.base.models import BaseColumn, BaseDatasource",
            "    from superset.models.sql_lab import Query",
            "",
            "logging.getLogger(\"MARKDOWN\").setLevel(logging.INFO)",
            "logger = logging.getLogger(__name__)",
            "",
            "DTTM_ALIAS = \"__timestamp\"",
            "",
            "TIME_COMPARISON = \"__\"",
            "",
            "JS_MAX_INTEGER = 9007199254740991  # Largest int Java Script can handle 2^53-1",
            "",
            "InputType = TypeVar(\"InputType\")  # pylint: disable=invalid-name",
            "",
            "ADHOC_FILTERS_REGEX = re.compile(\"^adhoc_filters\")",
            "",
            "",
            "class LenientEnum(Enum):",
            "    \"\"\"Enums with a `get` method that convert a enum value to `Enum` if it is a",
            "    valid value.\"\"\"",
            "",
            "    @classmethod",
            "    def get(cls, value: Any) -> Any:",
            "        try:",
            "            return super().__new__(cls, value)",
            "        except ValueError:",
            "            return None",
            "",
            "",
            "class AdhocMetricExpressionType(StrEnum):",
            "    SIMPLE = \"SIMPLE\"",
            "    SQL = \"SQL\"",
            "",
            "",
            "class AnnotationType(StrEnum):",
            "    FORMULA = \"FORMULA\"",
            "    INTERVAL = \"INTERVAL\"",
            "    EVENT = \"EVENT\"",
            "    TIME_SERIES = \"TIME_SERIES\"",
            "",
            "",
            "class GenericDataType(IntEnum):",
            "    \"\"\"",
            "    Generic database column type that fits both frontend and backend.",
            "    \"\"\"",
            "",
            "    NUMERIC = 0",
            "    STRING = 1",
            "    TEMPORAL = 2",
            "    BOOLEAN = 3",
            "    # ARRAY = 4     # Mapping all the complex data types to STRING for now",
            "    # JSON = 5      # and leaving these as a reminder.",
            "    # MAP = 6",
            "    # ROW = 7",
            "",
            "",
            "class DatasourceType(StrEnum):",
            "    SLTABLE = \"sl_table\"",
            "    TABLE = \"table\"",
            "    DATASET = \"dataset\"",
            "    QUERY = \"query\"",
            "    SAVEDQUERY = \"saved_query\"",
            "    VIEW = \"view\"",
            "",
            "",
            "class LoggerLevel(StrEnum):",
            "    INFO = \"info\"",
            "    WARNING = \"warning\"",
            "    EXCEPTION = \"exception\"",
            "",
            "",
            "class HeaderDataType(TypedDict):",
            "    notification_format: str",
            "    owners: list[int]",
            "    notification_type: str",
            "    notification_source: str | None",
            "    chart_id: int | None",
            "    dashboard_id: int | None",
            "",
            "",
            "class DatasourceDict(TypedDict):",
            "    type: str  # todo(hugh): update this to be DatasourceType",
            "    id: int",
            "",
            "",
            "class AdhocFilterClause(TypedDict, total=False):",
            "    clause: str",
            "    expressionType: str",
            "    filterOptionName: str | None",
            "    comparator: FilterValues | None",
            "    operator: str",
            "    subject: str",
            "    isExtra: bool | None",
            "    sqlExpression: str | None",
            "",
            "",
            "class QueryObjectFilterClause(TypedDict, total=False):",
            "    col: Column",
            "    op: str  # pylint: disable=invalid-name",
            "    val: FilterValues | None",
            "    grain: str | None",
            "    isExtra: bool | None",
            "",
            "",
            "class ExtraFiltersTimeColumnType(StrEnum):",
            "    TIME_COL = \"__time_col\"",
            "    TIME_GRAIN = \"__time_grain\"",
            "    TIME_ORIGIN = \"__time_origin\"",
            "    TIME_RANGE = \"__time_range\"",
            "",
            "",
            "class ExtraFiltersReasonType(StrEnum):",
            "    NO_TEMPORAL_COLUMN = \"no_temporal_column\"",
            "    COL_NOT_IN_DATASOURCE = \"not_in_datasource\"",
            "",
            "",
            "class FilterOperator(StrEnum):",
            "    \"\"\"",
            "    Operators used filter controls",
            "    \"\"\"",
            "",
            "    EQUALS = \"==\"",
            "    NOT_EQUALS = \"!=\"",
            "    GREATER_THAN = \">\"",
            "    LESS_THAN = \"<\"",
            "    GREATER_THAN_OR_EQUALS = \">=\"",
            "    LESS_THAN_OR_EQUALS = \"<=\"",
            "    LIKE = \"LIKE\"",
            "    ILIKE = \"ILIKE\"",
            "    IS_NULL = \"IS NULL\"",
            "    IS_NOT_NULL = \"IS NOT NULL\"",
            "    IN = \"IN\"",
            "    NOT_IN = \"NOT IN\"",
            "    IS_TRUE = \"IS TRUE\"",
            "    IS_FALSE = \"IS FALSE\"",
            "    TEMPORAL_RANGE = \"TEMPORAL_RANGE\"",
            "",
            "",
            "class FilterStringOperators(StrEnum):",
            "    EQUALS = (\"EQUALS\",)",
            "    NOT_EQUALS = (\"NOT_EQUALS\",)",
            "    LESS_THAN = (\"LESS_THAN\",)",
            "    GREATER_THAN = (\"GREATER_THAN\",)",
            "    LESS_THAN_OR_EQUAL = (\"LESS_THAN_OR_EQUAL\",)",
            "    GREATER_THAN_OR_EQUAL = (\"GREATER_THAN_OR_EQUAL\",)",
            "    IN = (\"IN\",)",
            "    NOT_IN = (\"NOT_IN\",)",
            "    ILIKE = (\"ILIKE\",)",
            "    LIKE = (\"LIKE\",)",
            "    IS_NOT_NULL = (\"IS_NOT_NULL\",)",
            "    IS_NULL = (\"IS_NULL\",)",
            "    LATEST_PARTITION = (\"LATEST_PARTITION\",)",
            "    IS_TRUE = (\"IS_TRUE\",)",
            "    IS_FALSE = (\"IS_FALSE\",)",
            "",
            "",
            "class PostProcessingBoxplotWhiskerType(StrEnum):",
            "    \"\"\"",
            "    Calculate cell contribution to row/column total",
            "    \"\"\"",
            "",
            "    TUKEY = \"tukey\"",
            "    MINMAX = \"min/max\"",
            "    PERCENTILE = \"percentile\"",
            "",
            "",
            "class PostProcessingContributionOrientation(StrEnum):",
            "    \"\"\"",
            "    Calculate cell contribution to row/column total",
            "    \"\"\"",
            "",
            "    ROW = \"row\"",
            "    COLUMN = \"column\"",
            "",
            "",
            "class QueryMode(str, LenientEnum):",
            "    \"\"\"",
            "    Whether the query runs on aggregate or returns raw records",
            "    \"\"\"",
            "",
            "    RAW = \"raw\"",
            "    AGGREGATE = \"aggregate\"",
            "",
            "",
            "class QuerySource(Enum):",
            "    \"\"\"",
            "    The source of a SQL query.",
            "    \"\"\"",
            "",
            "    CHART = 0",
            "    DASHBOARD = 1",
            "    SQL_LAB = 2",
            "",
            "",
            "class QueryStatus(StrEnum):",
            "    \"\"\"Enum-type class for query statuses\"\"\"",
            "",
            "    STOPPED: str = \"stopped\"",
            "    FAILED: str = \"failed\"",
            "    PENDING: str = \"pending\"",
            "    RUNNING: str = \"running\"",
            "    SCHEDULED: str = \"scheduled\"",
            "    SUCCESS: str = \"success\"",
            "    FETCHING: str = \"fetching\"",
            "    TIMED_OUT: str = \"timed_out\"",
            "",
            "",
            "class DashboardStatus(StrEnum):",
            "    \"\"\"Dashboard status used for frontend filters\"\"\"",
            "",
            "    PUBLISHED = \"published\"",
            "    DRAFT = \"draft\"",
            "",
            "",
            "class ReservedUrlParameters(StrEnum):",
            "    \"\"\"",
            "    Reserved URL parameters that are used internally by Superset. These will not be",
            "    passed to chart queries, as they control the behavior of the UI.",
            "    \"\"\"",
            "",
            "    STANDALONE = \"standalone\"",
            "    EDIT_MODE = \"edit\"",
            "",
            "    @staticmethod",
            "    def is_standalone_mode() -> bool | None:",
            "        standalone_param = request.args.get(ReservedUrlParameters.STANDALONE.value)",
            "        standalone: bool | None = bool(",
            "            standalone_param and standalone_param != \"false\" and standalone_param != \"0\"",
            "        )",
            "        return standalone",
            "",
            "",
            "class RowLevelSecurityFilterType(StrEnum):",
            "    REGULAR = \"Regular\"",
            "    BASE = \"Base\"",
            "",
            "",
            "class ColumnTypeSource(Enum):",
            "    GET_TABLE = 1",
            "    CURSOR_DESCRIPTION = 2",
            "",
            "",
            "class ColumnSpec(NamedTuple):",
            "    sqla_type: TypeEngine | str",
            "    generic_type: GenericDataType",
            "    is_dttm: bool",
            "    python_date_format: str | None = None",
            "",
            "",
            "def flasher(msg: str, severity: str = \"message\") -> None:",
            "    \"\"\"Flask's flash if available, logging call if not\"\"\"",
            "    try:",
            "        flash(msg, severity)",
            "    except RuntimeError:",
            "        if severity == \"danger\":",
            "            logger.error(msg, exc_info=True)",
            "        else:",
            "            logger.info(msg)",
            "",
            "",
            "def parse_js_uri_path_item(",
            "    item: str | None, unquote: bool = True, eval_undefined: bool = False",
            ") -> str | None:",
            "    \"\"\"Parse an uri path item made with js.",
            "",
            "    :param item: an uri path component",
            "    :param unquote: Perform unquoting of string using urllib.parse.unquote_plus()",
            "    :param eval_undefined: When set to True and item is either 'null' or 'undefined',",
            "    assume item is undefined and return None.",
            "    :return: Either None, the original item or unquoted item",
            "    \"\"\"",
            "    item = None if eval_undefined and item in (\"null\", \"undefined\") else item",
            "    return unquote_plus(item) if unquote and item else item",
            "",
            "",
            "def cast_to_num(value: float | int | str | None) -> float | int | None:",
            "    \"\"\"Casts a value to an int/float",
            "",
            "    >>> cast_to_num('1 ')",
            "    1.0",
            "    >>> cast_to_num(' 2')",
            "    2.0",
            "    >>> cast_to_num('5')",
            "    5",
            "    >>> cast_to_num('5.2')",
            "    5.2",
            "    >>> cast_to_num(10)",
            "    10",
            "    >>> cast_to_num(10.1)",
            "    10.1",
            "    >>> cast_to_num(None) is None",
            "    True",
            "    >>> cast_to_num('this is not a string') is None",
            "    True",
            "",
            "    :param value: value to be converted to numeric representation",
            "    :returns: value cast to `int` if value is all digits, `float` if `value` is",
            "              decimal value and `None`` if it can't be converted",
            "    \"\"\"",
            "    if value is None:",
            "        return None",
            "    if isinstance(value, (int, float)):",
            "        return value",
            "    if value.isdigit():",
            "        return int(value)",
            "    try:",
            "        return float(value)",
            "    except ValueError:",
            "        return None",
            "",
            "",
            "def cast_to_boolean(value: Any) -> bool | None:",
            "    \"\"\"Casts a value to an int/float",
            "",
            "    >>> cast_to_boolean(1)",
            "    True",
            "    >>> cast_to_boolean(0)",
            "    False",
            "    >>> cast_to_boolean(0.5)",
            "    True",
            "    >>> cast_to_boolean('true')",
            "    True",
            "    >>> cast_to_boolean('false')",
            "    False",
            "    >>> cast_to_boolean('False')",
            "    False",
            "    >>> cast_to_boolean(None)",
            "",
            "    :param value: value to be converted to boolean representation",
            "    :returns: value cast to `bool`. when value is 'true' or value that are not 0",
            "              converted into True. Return `None` if value is `None`",
            "    \"\"\"",
            "    if value is None:",
            "        return None",
            "    if isinstance(value, bool):",
            "        return value",
            "    if isinstance(value, (int, float)):",
            "        return value != 0",
            "    if isinstance(value, str):",
            "        return value.strip().lower() == \"true\"",
            "    return False",
            "",
            "",
            "def list_minus(l: list[Any], minus: list[Any]) -> list[Any]:",
            "    \"\"\"Returns l without what is in minus",
            "",
            "    >>> list_minus([1, 2, 3], [2])",
            "    [1, 3]",
            "    \"\"\"",
            "    return [o for o in l if o not in minus]",
            "",
            "",
            "class DashboardEncoder(json.JSONEncoder):",
            "    def __init__(self, *args: Any, **kwargs: Any) -> None:",
            "        super().__init__(*args, **kwargs)",
            "        self.sort_keys = True",
            "",
            "    def default(self, o: Any) -> dict[Any, Any] | str:",
            "        if isinstance(o, uuid.UUID):",
            "            return str(o)",
            "        try:",
            "            vals = {k: v for k, v in o.__dict__.items() if k != \"_sa_instance_state\"}",
            "            return {f\"__{o.__class__.__name__}__\": vals}",
            "        except Exception:  # pylint: disable=broad-except",
            "            if isinstance(o, datetime):",
            "                return {\"__datetime__\": o.replace(microsecond=0).isoformat()}",
            "            return json.JSONEncoder(sort_keys=True).default(o)",
            "",
            "",
            "class JSONEncodedDict(TypeDecorator):  # pylint: disable=abstract-method",
            "    \"\"\"Represents an immutable structure as a json-encoded string.\"\"\"",
            "",
            "    impl = TEXT",
            "",
            "    def process_bind_param(",
            "        self, value: dict[Any, Any] | None, dialect: str",
            "    ) -> str | None:",
            "        return json.dumps(value) if value is not None else None",
            "",
            "    def process_result_value(",
            "        self, value: str | None, dialect: str",
            "    ) -> dict[Any, Any] | None:",
            "        return json.loads(value) if value is not None else None",
            "",
            "",
            "def format_timedelta(time_delta: timedelta) -> str:",
            "    \"\"\"",
            "    Ensures negative time deltas are easily interpreted by humans",
            "",
            "    >>> td = timedelta(0) - timedelta(days=1, hours=5,minutes=6)",
            "    >>> str(td)",
            "    '-2 days, 18:54:00'",
            "    >>> format_timedelta(td)",
            "    '-1 day, 5:06:00'",
            "    \"\"\"",
            "    if time_delta < timedelta(0):",
            "        return \"-\" + str(abs(time_delta))",
            "",
            "    # Change this to format positive time deltas the way you want",
            "    return str(time_delta)",
            "",
            "",
            "def base_json_conv(obj: Any) -> Any:",
            "    \"\"\"",
            "    Tries to convert additional types to JSON compatible forms.",
            "",
            "    :param obj: The serializable object",
            "    :returns: The JSON compatible form",
            "    :raises TypeError: If the object cannot be serialized",
            "    :see: https://docs.python.org/3/library/json.html#encoders-and-decoders",
            "    \"\"\"",
            "",
            "    if isinstance(obj, memoryview):",
            "        obj = obj.tobytes()",
            "    if isinstance(obj, np.int64):",
            "        return int(obj)",
            "    if isinstance(obj, np.bool_):",
            "        return bool(obj)",
            "    if isinstance(obj, np.ndarray):",
            "        return obj.tolist()",
            "    if isinstance(obj, set):",
            "        return list(obj)",
            "    if isinstance(obj, decimal.Decimal):",
            "        return float(obj)",
            "    if isinstance(obj, (uuid.UUID, time, LazyString)):",
            "        return str(obj)",
            "    if isinstance(obj, timedelta):",
            "        return format_timedelta(obj)",
            "    if isinstance(obj, bytes):",
            "        try:",
            "            return obj.decode(\"utf-8\")",
            "        except Exception:  # pylint: disable=broad-except",
            "            return \"[bytes]\"",
            "",
            "    raise TypeError(f\"Unserializable object {obj} of type {type(obj)}\")",
            "",
            "",
            "def json_iso_dttm_ser(obj: Any, pessimistic: bool = False) -> Any:",
            "    \"\"\"",
            "    A JSON serializer that deals with dates by serializing them to ISO 8601.",
            "",
            "        >>> json.dumps({'dttm': datetime(1970, 1, 1)}, default=json_iso_dttm_ser)",
            "        '{\"dttm\": \"1970-01-01T00:00:00\"}'",
            "",
            "    :param obj: The serializable object",
            "    :param pessimistic: Whether to be pessimistic regarding serialization",
            "    :returns: The JSON compatible form",
            "    :raises TypeError: If the non-pessimistic object cannot be serialized",
            "    \"\"\"",
            "",
            "    if isinstance(obj, (datetime, date, pd.Timestamp)):",
            "        return obj.isoformat()",
            "",
            "    try:",
            "        return base_json_conv(obj)",
            "    except TypeError as ex:",
            "        if pessimistic:",
            "            return f\"Unserializable [{type(obj)}]\"",
            "",
            "        raise ex",
            "",
            "",
            "def pessimistic_json_iso_dttm_ser(obj: Any) -> Any:",
            "    \"\"\"Proxy to call json_iso_dttm_ser in a pessimistic way",
            "",
            "    If one of object is not serializable to json, it will still succeed\"\"\"",
            "    return json_iso_dttm_ser(obj, pessimistic=True)",
            "",
            "",
            "def json_int_dttm_ser(obj: Any) -> Any:",
            "    \"\"\"",
            "    A JSON serializer that deals with dates by serializing them to EPOCH.",
            "",
            "        >>> json.dumps({'dttm': datetime(1970, 1, 1)}, default=json_int_dttm_ser)",
            "        '{\"dttm\": 0.0}'",
            "",
            "    :param obj: The serializable object",
            "    :returns: The JSON compatible form",
            "    :raises TypeError: If the object cannot be serialized",
            "    \"\"\"",
            "",
            "    if isinstance(obj, (datetime, pd.Timestamp)):",
            "        return datetime_to_epoch(obj)",
            "",
            "    if isinstance(obj, date):",
            "        return (obj - EPOCH.date()).total_seconds() * 1000",
            "",
            "    return base_json_conv(obj)",
            "",
            "",
            "def json_dumps_w_dates(payload: dict[Any, Any], sort_keys: bool = False) -> str:",
            "    \"\"\"Dumps payload to JSON with Datetime objects properly converted\"\"\"",
            "    return json.dumps(payload, default=json_int_dttm_ser, sort_keys=sort_keys)",
            "",
            "",
            "def error_msg_from_exception(ex: Exception) -> str:",
            "    \"\"\"Translate exception into error message",
            "",
            "    Database have different ways to handle exception. This function attempts",
            "    to make sense of the exception object and construct a human readable",
            "    sentence.",
            "",
            "    TODO(bkyryliuk): parse the Presto error message from the connection",
            "                     created via create_engine.",
            "    engine = create_engine('presto://localhost:3506/silver') -",
            "      gives an e.message as the str(dict)",
            "    presto.connect('localhost', port=3506, catalog='silver') - as a dict.",
            "    The latter version is parsed correctly by this function.",
            "    \"\"\"",
            "    msg = \"\"",
            "    if hasattr(ex, \"message\"):",
            "        if isinstance(ex.message, dict):",
            "            msg = ex.message.get(\"message\")  # type: ignore",
            "        elif ex.message:",
            "            msg = ex.message",
            "    return msg or str(ex)",
            "",
            "",
            "def markdown(raw: str, markup_wrap: bool | None = False) -> str:",
            "    safe_markdown_tags = {",
            "        \"h1\",",
            "        \"h2\",",
            "        \"h3\",",
            "        \"h4\",",
            "        \"h5\",",
            "        \"h6\",",
            "        \"b\",",
            "        \"i\",",
            "        \"strong\",",
            "        \"em\",",
            "        \"tt\",",
            "        \"p\",",
            "        \"br\",",
            "        \"span\",",
            "        \"div\",",
            "        \"blockquote\",",
            "        \"code\",",
            "        \"hr\",",
            "        \"ul\",",
            "        \"ol\",",
            "        \"li\",",
            "        \"dd\",",
            "        \"dt\",",
            "        \"img\",",
            "        \"a\",",
            "    }",
            "    safe_markdown_attrs = {",
            "        \"img\": {\"src\", \"alt\", \"title\"},",
            "        \"a\": {\"href\", \"alt\", \"title\"},",
            "    }",
            "    safe = md.markdown(",
            "        raw or \"\",",
            "        extensions=[",
            "            \"markdown.extensions.tables\",",
            "            \"markdown.extensions.fenced_code\",",
            "            \"markdown.extensions.codehilite\",",
            "        ],",
            "    )",
            "    # pylint: disable=no-member",
            "    safe = nh3.clean(safe, tags=safe_markdown_tags, attributes=safe_markdown_attrs)",
            "    if markup_wrap:",
            "        safe = Markup(safe)",
            "    return safe",
            "",
            "",
            "def readfile(file_path: str) -> str | None:",
            "    with open(file_path) as f:",
            "        content = f.read()",
            "    return content",
            "",
            "",
            "def generic_find_constraint_name(",
            "    table: str, columns: set[str], referenced: str, database: SQLA",
            ") -> str | None:",
            "    \"\"\"Utility to find a constraint name in alembic migrations\"\"\"",
            "    tbl = sa.Table(",
            "        table, database.metadata, autoload=True, autoload_with=database.engine",
            "    )",
            "",
            "    for fk in tbl.foreign_key_constraints:",
            "        if fk.referred_table.name == referenced and set(fk.column_keys) == columns:",
            "            return fk.name",
            "",
            "    return None",
            "",
            "",
            "def generic_find_fk_constraint_name(",
            "    table: str, columns: set[str], referenced: str, insp: Inspector",
            ") -> str | None:",
            "    \"\"\"Utility to find a foreign-key constraint name in alembic migrations\"\"\"",
            "    for fk in insp.get_foreign_keys(table):",
            "        if (",
            "            fk[\"referred_table\"] == referenced",
            "            and set(fk[\"referred_columns\"]) == columns",
            "        ):",
            "            return fk[\"name\"]",
            "",
            "    return None",
            "",
            "",
            "def generic_find_fk_constraint_names(  # pylint: disable=invalid-name",
            "    table: str, columns: set[str], referenced: str, insp: Inspector",
            ") -> set[str]:",
            "    \"\"\"Utility to find foreign-key constraint names in alembic migrations\"\"\"",
            "    names = set()",
            "",
            "    for fk in insp.get_foreign_keys(table):",
            "        if (",
            "            fk[\"referred_table\"] == referenced",
            "            and set(fk[\"referred_columns\"]) == columns",
            "        ):",
            "            names.add(fk[\"name\"])",
            "",
            "    return names",
            "",
            "",
            "def generic_find_uq_constraint_name(",
            "    table: str, columns: set[str], insp: Inspector",
            ") -> str | None:",
            "    \"\"\"Utility to find a unique constraint name in alembic migrations\"\"\"",
            "",
            "    for uq in insp.get_unique_constraints(table):",
            "        if columns == set(uq[\"column_names\"]):",
            "            return uq[\"name\"]",
            "",
            "    return None",
            "",
            "",
            "def get_datasource_full_name(",
            "    database_name: str, datasource_name: str, schema: str | None = None",
            ") -> str:",
            "    if not schema:",
            "        return f\"[{database_name}].[{datasource_name}]\"",
            "    return f\"[{database_name}].[{schema}].[{datasource_name}]\"",
            "",
            "",
            "def validate_json(obj: bytes | bytearray | str) -> None:",
            "    if obj:",
            "        try:",
            "            json.loads(obj)",
            "        except Exception as ex:",
            "            logger.error(\"JSON is not valid %s\", str(ex), exc_info=True)",
            "            raise SupersetException(\"JSON is not valid\") from ex",
            "",
            "",
            "class SigalrmTimeout:",
            "    \"\"\"",
            "    To be used in a ``with`` block and timeout its content.",
            "    \"\"\"",
            "",
            "    def __init__(self, seconds: int = 1, error_message: str = \"Timeout\") -> None:",
            "        self.seconds = seconds",
            "        self.error_message = error_message",
            "",
            "    def handle_timeout(  # pylint: disable=unused-argument",
            "        self, signum: int, frame: Any",
            "    ) -> None:",
            "        logger.error(\"Process timed out\", exc_info=True)",
            "        raise SupersetTimeoutException(",
            "            error_type=SupersetErrorType.BACKEND_TIMEOUT_ERROR,",
            "            message=self.error_message,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"timeout\": self.seconds},",
            "        )",
            "",
            "    def __enter__(self) -> None:",
            "        try:",
            "            if threading.current_thread() == threading.main_thread():",
            "                signal.signal(signal.SIGALRM, self.handle_timeout)",
            "                signal.alarm(self.seconds)",
            "        except ValueError as ex:",
            "            logger.warning(\"timeout can't be used in the current context\")",
            "            logger.exception(ex)",
            "",
            "    def __exit__(  # pylint: disable=redefined-outer-name,redefined-builtin",
            "        self, type: Any, value: Any, traceback: TracebackType",
            "    ) -> None:",
            "        try:",
            "            signal.alarm(0)",
            "        except ValueError as ex:",
            "            logger.warning(\"timeout can't be used in the current context\")",
            "            logger.exception(ex)",
            "",
            "",
            "class TimerTimeout:",
            "    def __init__(self, seconds: int = 1, error_message: str = \"Timeout\") -> None:",
            "        self.seconds = seconds",
            "        self.error_message = error_message",
            "        self.timer = threading.Timer(seconds, _thread.interrupt_main)",
            "",
            "    def __enter__(self) -> None:",
            "        self.timer.start()",
            "",
            "    def __exit__(  # pylint: disable=redefined-outer-name,redefined-builtin",
            "        self, type: Any, value: Any, traceback: TracebackType",
            "    ) -> None:",
            "        self.timer.cancel()",
            "        if type is KeyboardInterrupt:  # raised by _thread.interrupt_main",
            "            raise SupersetTimeoutException(",
            "                error_type=SupersetErrorType.BACKEND_TIMEOUT_ERROR,",
            "                message=self.error_message,",
            "                level=ErrorLevel.ERROR,",
            "                extra={\"timeout\": self.seconds},",
            "            )",
            "",
            "",
            "# Windows has no support for SIGALRM, so we use the timer based timeout",
            "timeout: type[TimerTimeout] | type[SigalrmTimeout] = (",
            "    TimerTimeout if platform.system() == \"Windows\" else SigalrmTimeout",
            ")",
            "",
            "",
            "def pessimistic_connection_handling(some_engine: Engine) -> None:",
            "    @event.listens_for(some_engine, \"engine_connect\")",
            "    def ping_connection(connection: Connection, branch: bool) -> None:",
            "        if branch:",
            "            # 'branch' refers to a sub-connection of a connection,",
            "            # we don't want to bother pinging on these.",
            "            return",
            "",
            "        # turn off 'close with result'.  This flag is only used with",
            "        # 'connectionless' execution, otherwise will be False in any case",
            "        save_should_close_with_result = connection.should_close_with_result",
            "        connection.should_close_with_result = False",
            "",
            "        try:",
            "            # run a SELECT 1.   use a core select() so that",
            "            # the SELECT of a scalar value without a table is",
            "            # appropriately formatted for the backend",
            "            connection.scalar(select([1]))",
            "        except exc.DBAPIError as err:",
            "            # catch SQLAlchemy's DBAPIError, which is a wrapper",
            "            # for the DBAPI's exception.  It includes a .connection_invalidated",
            "            # attribute which specifies if this connection is a 'disconnect'",
            "            # condition, which is based on inspection of the original exception",
            "            # by the dialect in use.",
            "            if err.connection_invalidated:",
            "                # run the same SELECT again - the connection will re-validate",
            "                # itself and establish a new connection.  The disconnect detection",
            "                # here also causes the whole connection pool to be invalidated",
            "                # so that all stale connections are discarded.",
            "                connection.scalar(select([1]))",
            "            else:",
            "                raise",
            "        finally:",
            "            # restore 'close with result'",
            "            connection.should_close_with_result = save_should_close_with_result",
            "",
            "    if some_engine.dialect.name == \"sqlite\":",
            "",
            "        @event.listens_for(some_engine, \"connect\")",
            "        def set_sqlite_pragma(  # pylint: disable=unused-argument",
            "            connection: sqlite3.Connection,",
            "            *args: Any,",
            "        ) -> None:",
            "            r\"\"\"",
            "            Enable foreign key support for SQLite.",
            "",
            "            :param connection: The SQLite connection",
            "            :param \\*args: Additional positional arguments",
            "            :see: https://docs.sqlalchemy.org/en/latest/dialects/sqlite.html",
            "            \"\"\"",
            "",
            "            with closing(connection.cursor()) as cursor:",
            "                cursor.execute(\"PRAGMA foreign_keys=ON\")",
            "",
            "",
            "def send_email_smtp(  # pylint: disable=invalid-name,too-many-arguments,too-many-locals",
            "    to: str,",
            "    subject: str,",
            "    html_content: str,",
            "    config: dict[str, Any],",
            "    files: list[str] | None = None,",
            "    data: dict[str, str] | None = None,",
            "    images: dict[str, bytes] | None = None,",
            "    dryrun: bool = False,",
            "    cc: str | None = None,",
            "    bcc: str | None = None,",
            "    mime_subtype: str = \"mixed\",",
            "    header_data: HeaderDataType | None = None,",
            ") -> None:",
            "    \"\"\"",
            "    Send an email with html content, eg:",
            "    send_email_smtp(",
            "        'test@example.com', 'foo', '<b>Foo</b> bar',['/dev/null'], dryrun=True)",
            "    \"\"\"",
            "    smtp_mail_from = config[\"SMTP_MAIL_FROM\"]",
            "    smtp_mail_to = get_email_address_list(to)",
            "",
            "    msg = MIMEMultipart(mime_subtype)",
            "    msg[\"Subject\"] = subject",
            "    msg[\"From\"] = smtp_mail_from",
            "    msg[\"To\"] = \", \".join(smtp_mail_to)",
            "",
            "    msg.preamble = \"This is a multi-part message in MIME format.\"",
            "",
            "    recipients = smtp_mail_to",
            "    if cc:",
            "        smtp_mail_cc = get_email_address_list(cc)",
            "        msg[\"CC\"] = \", \".join(smtp_mail_cc)",
            "        recipients = recipients + smtp_mail_cc",
            "",
            "    if bcc:",
            "        # don't add bcc in header",
            "        smtp_mail_bcc = get_email_address_list(bcc)",
            "        recipients = recipients + smtp_mail_bcc",
            "",
            "    msg[\"Date\"] = formatdate(localtime=True)",
            "    mime_text = MIMEText(html_content, \"html\")",
            "    msg.attach(mime_text)",
            "",
            "    # Attach files by reading them from disk",
            "    for fname in files or []:",
            "        basename = os.path.basename(fname)",
            "        with open(fname, \"rb\") as f:",
            "            msg.attach(",
            "                MIMEApplication(",
            "                    f.read(),",
            "                    Content_Disposition=f\"attachment; filename='{basename}'\",",
            "                    Name=basename,",
            "                )",
            "            )",
            "",
            "    # Attach any files passed directly",
            "    for name, body in (data or {}).items():",
            "        msg.attach(",
            "            MIMEApplication(",
            "                body, Content_Disposition=f\"attachment; filename='{name}'\", Name=name",
            "            )",
            "        )",
            "",
            "    # Attach any inline images, which may be required for display in",
            "    # HTML content (inline)",
            "    for msgid, imgdata in (images or {}).items():",
            "        formatted_time = formatdate(localtime=True)",
            "        file_name = f\"{subject} {formatted_time}\"",
            "        image = MIMEImage(imgdata, name=file_name)",
            "        image.add_header(\"Content-ID\", f\"<{msgid}>\")",
            "        image.add_header(\"Content-Disposition\", \"inline\")",
            "        msg.attach(image)",
            "    msg_mutator = config[\"EMAIL_HEADER_MUTATOR\"]",
            "    # the base notification returns the message without any editing.",
            "    new_msg = msg_mutator(msg, **(header_data or {}))",
            "    send_mime_email(smtp_mail_from, recipients, new_msg, config, dryrun=dryrun)",
            "",
            "",
            "def send_mime_email(",
            "    e_from: str,",
            "    e_to: list[str],",
            "    mime_msg: MIMEMultipart,",
            "    config: dict[str, Any],",
            "    dryrun: bool = False,",
            ") -> None:",
            "    smtp_host = config[\"SMTP_HOST\"]",
            "    smtp_port = config[\"SMTP_PORT\"]",
            "    smtp_user = config[\"SMTP_USER\"]",
            "    smtp_password = config[\"SMTP_PASSWORD\"]",
            "    smtp_starttls = config[\"SMTP_STARTTLS\"]",
            "    smtp_ssl = config[\"SMTP_SSL\"]",
            "    smtp_ssl_server_auth = config[\"SMTP_SSL_SERVER_AUTH\"]",
            "",
            "    if dryrun:",
            "        logger.info(\"Dryrun enabled, email notification content is below:\")",
            "        logger.info(mime_msg.as_string())",
            "        return",
            "",
            "    # Default ssl context is SERVER_AUTH using the default system",
            "    # root CA certificates",
            "    ssl_context = ssl.create_default_context() if smtp_ssl_server_auth else None",
            "    smtp = (",
            "        smtplib.SMTP_SSL(smtp_host, smtp_port, context=ssl_context)",
            "        if smtp_ssl",
            "        else smtplib.SMTP(smtp_host, smtp_port)",
            "    )",
            "    if smtp_starttls:",
            "        smtp.starttls(context=ssl_context)",
            "    if smtp_user and smtp_password:",
            "        smtp.login(smtp_user, smtp_password)",
            "    logger.debug(\"Sent an email to %s\", str(e_to))",
            "    smtp.sendmail(e_from, e_to, mime_msg.as_string())",
            "    smtp.quit()",
            "",
            "",
            "def get_email_address_list(address_string: str) -> list[str]:",
            "    address_string_list: list[str] = []",
            "    if isinstance(address_string, str):",
            "        address_string_list = re.split(r\",|\\s|;\", address_string)",
            "    return [x.strip() for x in address_string_list if x.strip()]",
            "",
            "",
            "def get_email_address_str(address_string: str) -> str:",
            "    address_list = get_email_address_list(address_string)",
            "    address_list_str = \", \".join(address_list)",
            "",
            "    return address_list_str",
            "",
            "",
            "def choicify(values: Iterable[Any]) -> list[tuple[Any, Any]]:",
            "    \"\"\"Takes an iterable and makes an iterable of tuples with it\"\"\"",
            "    return [(v, v) for v in values]",
            "",
            "",
            "def zlib_compress(data: bytes | str) -> bytes:",
            "    \"\"\"",
            "    Compress things in a py2/3 safe fashion",
            "    >>> json_str = '{\"test\": 1}'",
            "    >>> blob = zlib_compress(json_str)",
            "    \"\"\"",
            "    if isinstance(data, str):",
            "        return zlib.compress(bytes(data, \"utf-8\"))",
            "    return zlib.compress(data)",
            "",
            "",
            "def zlib_decompress(blob: bytes, decode: bool | None = True) -> bytes | str:",
            "    \"\"\"",
            "    Decompress things to a string in a py2/3 safe fashion",
            "    >>> json_str = '{\"test\": 1}'",
            "    >>> blob = zlib_compress(json_str)",
            "    >>> got_str = zlib_decompress(blob)",
            "    >>> got_str == json_str",
            "    True",
            "    \"\"\"",
            "    if isinstance(blob, bytes):",
            "        decompressed = zlib.decompress(blob)",
            "    else:",
            "        decompressed = zlib.decompress(bytes(blob, \"utf-8\"))",
            "    return decompressed.decode(\"utf-8\") if decode else decompressed",
            "",
            "",
            "def simple_filter_to_adhoc(",
            "    filter_clause: QueryObjectFilterClause,",
            "    clause: str = \"where\",",
            ") -> AdhocFilterClause:",
            "    result: AdhocFilterClause = {",
            "        \"clause\": clause.upper(),",
            "        \"expressionType\": \"SIMPLE\",",
            "        \"comparator\": filter_clause.get(\"val\"),",
            "        \"operator\": filter_clause[\"op\"],",
            "        \"subject\": cast(str, filter_clause[\"col\"]),",
            "    }",
            "    if filter_clause.get(\"isExtra\"):",
            "        result[\"isExtra\"] = True",
            "    result[\"filterOptionName\"] = md5_sha_from_dict(cast(dict[Any, Any], result))",
            "",
            "    return result",
            "",
            "",
            "def form_data_to_adhoc(form_data: dict[str, Any], clause: str) -> AdhocFilterClause:",
            "    if clause not in (\"where\", \"having\"):",
            "        raise ValueError(__(\"Unsupported clause type: %(clause)s\", clause=clause))",
            "    result: AdhocFilterClause = {",
            "        \"clause\": clause.upper(),",
            "        \"expressionType\": \"SQL\",",
            "        \"sqlExpression\": form_data.get(clause),",
            "    }",
            "    result[\"filterOptionName\"] = md5_sha_from_dict(cast(dict[Any, Any], result))",
            "",
            "    return result",
            "",
            "",
            "def merge_extra_form_data(form_data: dict[str, Any]) -> None:",
            "    \"\"\"",
            "    Merge extra form data (appends and overrides) into the main payload",
            "    and add applied time extras to the payload.",
            "    \"\"\"",
            "    filter_keys = [\"filters\", \"adhoc_filters\"]",
            "    extra_form_data = form_data.pop(\"extra_form_data\", {})",
            "    append_filters: list[QueryObjectFilterClause] = extra_form_data.get(\"filters\", None)",
            "",
            "    # merge append extras",
            "    for key in [key for key in EXTRA_FORM_DATA_APPEND_KEYS if key not in filter_keys]:",
            "        extra_value = getattr(extra_form_data, key, {})",
            "        form_value = getattr(form_data, key, {})",
            "        form_value.update(extra_value)",
            "        if form_value:",
            "            form_data[\"key\"] = extra_value",
            "",
            "    # map regular extras that apply to form data properties",
            "    for src_key, target_key in EXTRA_FORM_DATA_OVERRIDE_REGULAR_MAPPINGS.items():",
            "        value = extra_form_data.get(src_key)",
            "        if value is not None:",
            "            form_data[target_key] = value",
            "",
            "    # map extras that apply to form data extra properties",
            "    extras = form_data.get(\"extras\", {})",
            "    for key in EXTRA_FORM_DATA_OVERRIDE_EXTRA_KEYS:",
            "        value = extra_form_data.get(key)",
            "        if value is not None:",
            "            extras[key] = value",
            "    if extras:",
            "        form_data[\"extras\"] = extras",
            "",
            "    adhoc_filters: list[AdhocFilterClause] = form_data.get(\"adhoc_filters\", [])",
            "    form_data[\"adhoc_filters\"] = adhoc_filters",
            "    append_adhoc_filters: list[AdhocFilterClause] = extra_form_data.get(",
            "        \"adhoc_filters\", []",
            "    )",
            "    adhoc_filters.extend(",
            "        {\"isExtra\": True, **fltr} for fltr in append_adhoc_filters  # type: ignore",
            "    )",
            "    if append_filters:",
            "        for key, value in form_data.items():",
            "            if re.match(\"adhoc_filter.*\", key):",
            "                value.extend(",
            "                    simple_filter_to_adhoc({\"isExtra\": True, **fltr})  # type: ignore",
            "                    for fltr in append_filters",
            "                    if fltr",
            "                )",
            "    if form_data.get(\"time_range\") and not form_data.get(\"granularity_sqla\"):",
            "        for adhoc_filter in form_data.get(\"adhoc_filters\", []):",
            "            if adhoc_filter.get(\"operator\") == \"TEMPORAL_RANGE\":",
            "                adhoc_filter[\"comparator\"] = form_data[\"time_range\"]",
            "",
            "",
            "def merge_extra_filters(form_data: dict[str, Any]) -> None:",
            "    # extra_filters are temporary/contextual filters (using the legacy constructs)",
            "    # that are external to the slice definition. We use those for dynamic",
            "    # interactive filters like the ones emitted by the \"Filter Box\" visualization.",
            "    # Note extra_filters only support simple filters.",
            "    form_data.setdefault(\"applied_time_extras\", {})",
            "    adhoc_filters = form_data.get(\"adhoc_filters\", [])",
            "    form_data[\"adhoc_filters\"] = adhoc_filters",
            "    merge_extra_form_data(form_data)",
            "    if \"extra_filters\" in form_data:",
            "        # __form and __to are special extra_filters that target time",
            "        # boundaries. The rest of extra_filters are simple",
            "        # [column_name in list_of_values]. `__` prefix is there to avoid",
            "        # potential conflicts with column that would be named `from` or `to`",
            "        date_options = {",
            "            \"__time_range\": \"time_range\",",
            "            \"__time_col\": \"granularity_sqla\",",
            "            \"__time_grain\": \"time_grain_sqla\",",
            "        }",
            "",
            "        # Grab list of existing filters 'keyed' on the column and operator",
            "",
            "        def get_filter_key(f: dict[str, Any]) -> str:",
            "            if \"expressionType\" in f:",
            "                return f\"{f['subject']}__{f['operator']}\"",
            "",
            "            return f\"{f['col']}__{f['op']}\"",
            "",
            "        existing_filters = {}",
            "        for existing in adhoc_filters:",
            "            if (",
            "                existing[\"expressionType\"] == \"SIMPLE\"",
            "                and existing.get(\"comparator\") is not None",
            "                and existing.get(\"subject\") is not None",
            "            ):",
            "                existing_filters[get_filter_key(existing)] = existing[\"comparator\"]",
            "",
            "        for filtr in form_data[  # pylint: disable=too-many-nested-blocks",
            "            \"extra_filters\"",
            "        ]:",
            "            filtr[\"isExtra\"] = True",
            "            # Pull out time filters/options and merge into form data",
            "            filter_column = filtr[\"col\"]",
            "            if time_extra := date_options.get(filter_column):",
            "                time_extra_value = filtr.get(\"val\")",
            "                if time_extra_value and time_extra_value != NO_TIME_RANGE:",
            "                    form_data[time_extra] = time_extra_value",
            "                    form_data[\"applied_time_extras\"][filter_column] = time_extra_value",
            "            elif filtr[\"val\"]:",
            "                # Merge column filters",
            "                if (filter_key := get_filter_key(filtr)) in existing_filters:",
            "                    # Check if the filter already exists",
            "                    if isinstance(filtr[\"val\"], list):",
            "                        if isinstance(existing_filters[filter_key], list):",
            "                            # Add filters for unequal lists",
            "                            # order doesn't matter",
            "                            if set(existing_filters[filter_key]) != set(filtr[\"val\"]):",
            "                                adhoc_filters.append(simple_filter_to_adhoc(filtr))",
            "                        else:",
            "                            adhoc_filters.append(simple_filter_to_adhoc(filtr))",
            "                    else:",
            "                        # Do not add filter if same value already exists",
            "                        if filtr[\"val\"] != existing_filters[filter_key]:",
            "                            adhoc_filters.append(simple_filter_to_adhoc(filtr))",
            "                else:",
            "                    # Filter not found, add it",
            "                    adhoc_filters.append(simple_filter_to_adhoc(filtr))",
            "        # Remove extra filters from the form data since no longer needed",
            "        del form_data[\"extra_filters\"]",
            "",
            "",
            "def merge_request_params(form_data: dict[str, Any], params: dict[str, Any]) -> None:",
            "    \"\"\"",
            "    Merge request parameters to the key `url_params` in form_data. Only updates",
            "    or appends parameters to `form_data` that are defined in `params; preexisting",
            "    parameters not defined in params are left unchanged.",
            "",
            "    :param form_data: object to be updated",
            "    :param params: request parameters received via query string",
            "    \"\"\"",
            "    url_params = form_data.get(\"url_params\", {})",
            "    for key, value in params.items():",
            "        if key in (\"form_data\", \"r\"):",
            "            continue",
            "        url_params[key] = value",
            "    form_data[\"url_params\"] = url_params",
            "",
            "",
            "def user_label(user: User) -> str | None:",
            "    \"\"\"Given a user ORM FAB object, returns a label\"\"\"",
            "    if user:",
            "        if user.first_name and user.last_name:",
            "            return user.first_name + \" \" + user.last_name",
            "",
            "        return user.username",
            "",
            "    return None",
            "",
            "",
            "def get_example_default_schema() -> str | None:",
            "    \"\"\"",
            "    Return the default schema of the examples database, if any.",
            "    \"\"\"",
            "    database = get_example_database()",
            "    with database.get_sqla_engine_with_context() as engine:",
            "        return inspect(engine).default_schema_name",
            "",
            "",
            "def backend() -> str:",
            "    return get_example_database().backend",
            "",
            "",
            "def is_adhoc_metric(metric: Metric) -> TypeGuard[AdhocMetric]:",
            "    return isinstance(metric, dict) and \"expressionType\" in metric",
            "",
            "",
            "def is_adhoc_column(column: Column) -> TypeGuard[AdhocColumn]:",
            "    return isinstance(column, dict) and ({\"label\", \"sqlExpression\"}).issubset(",
            "        column.keys()",
            "    )",
            "",
            "",
            "def get_base_axis_labels(columns: list[Column] | None) -> tuple[str, ...]:",
            "    axis_cols = [",
            "        col",
            "        for col in columns or []",
            "        if is_adhoc_column(col) and col.get(\"columnType\") == \"BASE_AXIS\"",
            "    ]",
            "    return tuple(get_column_name(col) for col in axis_cols)",
            "",
            "",
            "def get_xaxis_label(columns: list[Column] | None) -> str | None:",
            "    labels = get_base_axis_labels(columns)",
            "    return labels[0] if labels else None",
            "",
            "",
            "def get_column_name(column: Column, verbose_map: dict[str, Any] | None = None) -> str:",
            "    \"\"\"",
            "    Extract label from column",
            "",
            "    :param column: object to extract label from",
            "    :param verbose_map: verbose_map from dataset for optional mapping from",
            "                        raw name to verbose name",
            "    :return: String representation of column",
            "    :raises ValueError: if metric object is invalid",
            "    \"\"\"",
            "    if isinstance(column, dict):",
            "        if label := column.get(\"label\"):",
            "            return label",
            "        if expr := column.get(\"sqlExpression\"):",
            "            return expr",
            "",
            "    if isinstance(column, str):",
            "        verbose_map = verbose_map or {}",
            "        return verbose_map.get(column, column)",
            "",
            "    raise ValueError(\"Missing label\")",
            "",
            "",
            "def get_metric_name(metric: Metric, verbose_map: dict[str, Any] | None = None) -> str:",
            "    \"\"\"",
            "    Extract label from metric",
            "",
            "    :param metric: object to extract label from",
            "    :param verbose_map: verbose_map from dataset for optional mapping from",
            "                        raw name to verbose name",
            "    :return: String representation of metric",
            "    :raises ValueError: if metric object is invalid",
            "    \"\"\"",
            "    if is_adhoc_metric(metric):",
            "        if label := metric.get(\"label\"):",
            "            return label",
            "        if (expression_type := metric.get(\"expressionType\")) == \"SQL\":",
            "            if sql_expression := metric.get(\"sqlExpression\"):",
            "                return sql_expression",
            "        if expression_type == \"SIMPLE\":",
            "            column: AdhocMetricColumn = metric.get(\"column\") or {}",
            "            column_name = column.get(\"column_name\")",
            "            aggregate = metric.get(\"aggregate\")",
            "            if column and aggregate:",
            "                return f\"{aggregate}({column_name})\"",
            "            if column_name:",
            "                return column_name",
            "",
            "    if isinstance(metric, str):",
            "        verbose_map = verbose_map or {}",
            "        return verbose_map.get(metric, metric)",
            "",
            "    raise ValueError(__(\"Invalid metric object: %(metric)s\", metric=str(metric)))",
            "",
            "",
            "def get_column_names(",
            "    columns: Sequence[Column] | None,",
            "    verbose_map: dict[str, Any] | None = None,",
            ") -> list[str]:",
            "    return [",
            "        column",
            "        for column in [get_column_name(column, verbose_map) for column in columns or []]",
            "        if column",
            "    ]",
            "",
            "",
            "def get_metric_names(",
            "    metrics: Sequence[Metric] | None,",
            "    verbose_map: dict[str, Any] | None = None,",
            ") -> list[str]:",
            "    return [",
            "        metric",
            "        for metric in [get_metric_name(metric, verbose_map) for metric in metrics or []]",
            "        if metric",
            "    ]",
            "",
            "",
            "def get_first_metric_name(",
            "    metrics: Sequence[Metric] | None,",
            "    verbose_map: dict[str, Any] | None = None,",
            ") -> str | None:",
            "    metric_labels = get_metric_names(metrics, verbose_map)",
            "    return metric_labels[0] if metric_labels else None",
            "",
            "",
            "def ensure_path_exists(path: str) -> None:",
            "    try:",
            "        os.makedirs(path)",
            "    except OSError as ex:",
            "        if not (os.path.isdir(path) and ex.errno == errno.EEXIST):",
            "            raise",
            "",
            "",
            "def convert_legacy_filters_into_adhoc(  # pylint: disable=invalid-name",
            "    form_data: FormData,",
            ") -> None:",
            "    if not form_data.get(\"adhoc_filters\"):",
            "        adhoc_filters: list[AdhocFilterClause] = []",
            "        form_data[\"adhoc_filters\"] = adhoc_filters",
            "",
            "        for clause in (\"having\", \"where\"):",
            "            if clause in form_data and form_data[clause] != \"\":",
            "                adhoc_filters.append(form_data_to_adhoc(form_data, clause))",
            "",
            "        if \"filters\" in form_data:",
            "            adhoc_filters.extend(",
            "                simple_filter_to_adhoc(fltr, \"where\")",
            "                for fltr in form_data[\"filters\"]",
            "                if fltr is not None",
            "            )",
            "",
            "    for key in (\"filters\", \"having\", \"where\"):",
            "        if key in form_data:",
            "            del form_data[key]",
            "",
            "",
            "def split_adhoc_filters_into_base_filters(  # pylint: disable=invalid-name",
            "    form_data: FormData,",
            ") -> None:",
            "    \"\"\"",
            "    Mutates form data to restructure the adhoc filters in the form of the three base",
            "    filters, `where`, `having`, and `filters` which represent free form where sql,",
            "    free form having sql, and structured where clauses.",
            "    \"\"\"",
            "    adhoc_filters = form_data.get(\"adhoc_filters\")",
            "    if isinstance(adhoc_filters, list):",
            "        simple_where_filters = []",
            "        sql_where_filters = []",
            "        sql_having_filters = []",
            "        for adhoc_filter in adhoc_filters:",
            "            expression_type = adhoc_filter.get(\"expressionType\")",
            "            clause = adhoc_filter.get(\"clause\")",
            "            if expression_type == \"SIMPLE\":",
            "                if clause == \"WHERE\":",
            "                    simple_where_filters.append(",
            "                        {",
            "                            \"col\": adhoc_filter.get(\"subject\"),",
            "                            \"op\": adhoc_filter.get(\"operator\"),",
            "                            \"val\": adhoc_filter.get(\"comparator\"),",
            "                        }",
            "                    )",
            "            elif expression_type == \"SQL\":",
            "                sql_expression = adhoc_filter.get(\"sqlExpression\")",
            "                sql_expression = sanitize_clause(sql_expression)",
            "                if clause == \"WHERE\":",
            "                    sql_where_filters.append(sql_expression)",
            "                elif clause == \"HAVING\":",
            "                    sql_having_filters.append(sql_expression)",
            "        form_data[\"where\"] = \" AND \".join([f\"({sql})\" for sql in sql_where_filters])",
            "        form_data[\"having\"] = \" AND \".join([f\"({sql})\" for sql in sql_having_filters])",
            "        form_data[\"filters\"] = simple_where_filters",
            "",
            "",
            "def get_username() -> str | None:",
            "    \"\"\"",
            "    Get username (if defined) associated with the current user.",
            "",
            "    :returns: The username",
            "    \"\"\"",
            "",
            "    try:",
            "        return g.user.username",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "def get_user_id() -> int | None:",
            "    \"\"\"",
            "    Get the user identifier (if defined) associated with the current user.",
            "",
            "    Though the Flask-AppBuilder `User` and Flask-Login  `AnonymousUserMixin` and",
            "    `UserMixin` models provide a convenience `get_id` method, for generality, the",
            "    identifier is encoded as a `str` whereas in Superset all identifiers are encoded as",
            "    an `int`.",
            "",
            "    returns: The user identifier",
            "    \"\"\"",
            "",
            "    try:",
            "        return g.user.id",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "@contextmanager",
            "def override_user(user: User | None, force: bool = True) -> Iterator[Any]:",
            "    \"\"\"",
            "    Temporarily override the current user per `flask.g` with the specified user.",
            "",
            "    Sometimes, often in the context of async Celery tasks, it is useful to switch the",
            "    current user (which may be undefined) to different one, execute some SQLAlchemy",
            "    tasks et al. and then revert back to the original one.",
            "",
            "    :param user: The override user",
            "    :param force: Whether to override the current user if set",
            "    \"\"\"",
            "",
            "    if hasattr(g, \"user\"):",
            "        if force or g.user is None:",
            "            current = g.user",
            "            g.user = user",
            "            yield",
            "            g.user = current",
            "        else:",
            "            yield",
            "    else:",
            "        g.user = user",
            "        yield",
            "        delattr(g, \"user\")",
            "",
            "",
            "def parse_ssl_cert(certificate: str) -> Certificate:",
            "    \"\"\"",
            "    Parses the contents of a certificate and returns a valid certificate object",
            "    if valid.",
            "",
            "    :param certificate: Contents of certificate file",
            "    :return: Valid certificate instance",
            "    :raises CertificateException: If certificate is not valid/unparseable",
            "    \"\"\"",
            "    try:",
            "        return load_pem_x509_certificate(certificate.encode(\"utf-8\"), default_backend())",
            "    except ValueError as ex:",
            "        raise CertificateException(\"Invalid certificate\") from ex",
            "",
            "",
            "def create_ssl_cert_file(certificate: str) -> str:",
            "    \"\"\"",
            "    This creates a certificate file that can be used to validate HTTPS",
            "    sessions. A certificate is only written to disk once; on subsequent calls,",
            "    only the path of the existing certificate is returned.",
            "",
            "    :param certificate: The contents of the certificate",
            "    :return: The path to the certificate file",
            "    :raises CertificateException: If certificate is not valid/unparseable",
            "    \"\"\"",
            "    filename = f\"{md5_sha_from_str(certificate)}.crt\"",
            "    cert_dir = current_app.config[\"SSL_CERT_PATH\"]",
            "    path = cert_dir if cert_dir else tempfile.gettempdir()",
            "    path = os.path.join(path, filename)",
            "    if not os.path.exists(path):",
            "        # Validate certificate prior to persisting to temporary directory",
            "        parse_ssl_cert(certificate)",
            "        with open(path, \"w\") as cert_file:",
            "            cert_file.write(certificate)",
            "    return path",
            "",
            "",
            "def time_function(",
            "    func: Callable[..., FlaskResponse], *args: Any, **kwargs: Any",
            ") -> tuple[float, Any]:",
            "    \"\"\"",
            "    Measures the amount of time a function takes to execute in ms",
            "",
            "    :param func: The function execution time to measure",
            "    :param args: args to be passed to the function",
            "    :param kwargs: kwargs to be passed to the function",
            "    :return: A tuple with the duration and response from the function",
            "    \"\"\"",
            "    start = default_timer()",
            "    response = func(*args, **kwargs)",
            "    stop = default_timer()",
            "    return (stop - start) * 1000.0, response",
            "",
            "",
            "def MediumText() -> Variant:  # pylint:disable=invalid-name",
            "    return Text().with_variant(MEDIUMTEXT(), \"mysql\")",
            "",
            "",
            "def shortid() -> str:",
            "    return f\"{uuid.uuid4()}\"[-12:]",
            "",
            "",
            "class DatasourceName(NamedTuple):",
            "    table: str",
            "    schema: str",
            "",
            "",
            "def get_stacktrace() -> str | None:",
            "    if current_app.config[\"SHOW_STACKTRACE\"]:",
            "        return traceback.format_exc()",
            "    return None",
            "",
            "",
            "def split(",
            "    string: str, delimiter: str = \" \", quote: str = '\"', escaped_quote: str = r\"\\\"\"",
            ") -> Iterator[str]:",
            "    \"\"\"",
            "    A split function that is aware of quotes and parentheses.",
            "",
            "    :param string: string to split",
            "    :param delimiter: string defining where to split, usually a comma or space",
            "    :param quote: string, either a single or a double quote",
            "    :param escaped_quote: string representing an escaped quote",
            "    :return: list of strings",
            "    \"\"\"",
            "    parens = 0",
            "    quotes = False",
            "    i = 0",
            "    for j, character in enumerate(string):",
            "        complete = parens == 0 and not quotes",
            "        if complete and character == delimiter:",
            "            yield string[i:j]",
            "            i = j + len(delimiter)",
            "        elif character == \"(\":",
            "            parens += 1",
            "        elif character == \")\":",
            "            parens -= 1",
            "        elif character == quote:",
            "            if quotes and string[j - len(escaped_quote) + 1 : j + 1] != escaped_quote:",
            "                quotes = False",
            "            elif not quotes:",
            "                quotes = True",
            "    yield string[i:]",
            "",
            "",
            "T = TypeVar(\"T\")",
            "",
            "",
            "def as_list(x: T | list[T]) -> list[T]:",
            "    \"\"\"",
            "    Wrap an object in a list if it's not a list.",
            "",
            "    :param x: The object",
            "    :returns: A list wrapping the object if it's not already a list",
            "    \"\"\"",
            "    return x if isinstance(x, list) else [x]",
            "",
            "",
            "def get_form_data_token(form_data: dict[str, Any]) -> str:",
            "    \"\"\"",
            "    Return the token contained within form data or generate a new one.",
            "",
            "    :param form_data: chart form data",
            "    :return: original token if predefined, otherwise new uuid4 based token",
            "    \"\"\"",
            "    return form_data.get(\"token\") or \"token_\" + uuid.uuid4().hex[:8]",
            "",
            "",
            "def get_column_name_from_column(column: Column) -> str | None:",
            "    \"\"\"",
            "    Extract the physical column that a column is referencing. If the column is",
            "    an adhoc column, always returns `None`.",
            "",
            "    :param column: Physical and ad-hoc column",
            "    :return: column name if physical column, otherwise None",
            "    \"\"\"",
            "    if is_adhoc_column(column):",
            "        return None",
            "    return column  # type: ignore",
            "",
            "",
            "def get_column_names_from_columns(columns: list[Column]) -> list[str]:",
            "    \"\"\"",
            "    Extract the physical columns that a list of columns are referencing. Ignore",
            "    adhoc columns",
            "",
            "    :param columns: Physical and adhoc columns",
            "    :return: column names of all physical columns",
            "    \"\"\"",
            "    return [col for col in map(get_column_name_from_column, columns) if col]",
            "",
            "",
            "def get_column_name_from_metric(metric: Metric) -> str | None:",
            "    \"\"\"",
            "    Extract the column that a metric is referencing. If the metric isn't",
            "    a simple metric, always returns `None`.",
            "",
            "    :param metric: Ad-hoc metric",
            "    :return: column name if simple metric, otherwise None",
            "    \"\"\"",
            "    if is_adhoc_metric(metric):",
            "        metric = cast(AdhocMetric, metric)",
            "        if metric[\"expressionType\"] == AdhocMetricExpressionType.SIMPLE:",
            "            return cast(dict[str, Any], metric[\"column\"])[\"column_name\"]",
            "    return None",
            "",
            "",
            "def get_column_names_from_metrics(metrics: list[Metric]) -> list[str]:",
            "    \"\"\"",
            "    Extract the columns that a list of metrics are referencing. Excludes all",
            "    SQL metrics.",
            "",
            "    :param metrics: Ad-hoc metric",
            "    :return: column name if simple metric, otherwise None",
            "    \"\"\"",
            "    return [col for col in map(get_column_name_from_metric, metrics) if col]",
            "",
            "",
            "def extract_dataframe_dtypes(",
            "    df: pd.DataFrame,",
            "    datasource: BaseDatasource | Query | None = None,",
            ") -> list[GenericDataType]:",
            "    \"\"\"Serialize pandas/numpy dtypes to generic types\"\"\"",
            "",
            "    # omitting string types as those will be the default type",
            "    inferred_type_map: dict[str, GenericDataType] = {",
            "        \"floating\": GenericDataType.NUMERIC,",
            "        \"integer\": GenericDataType.NUMERIC,",
            "        \"mixed-integer-float\": GenericDataType.NUMERIC,",
            "        \"decimal\": GenericDataType.NUMERIC,",
            "        \"boolean\": GenericDataType.BOOLEAN,",
            "        \"datetime64\": GenericDataType.TEMPORAL,",
            "        \"datetime\": GenericDataType.TEMPORAL,",
            "        \"date\": GenericDataType.TEMPORAL,",
            "    }",
            "",
            "    columns_by_name: dict[str, Any] = {}",
            "    if datasource:",
            "        for column in datasource.columns:",
            "            if isinstance(column, dict):",
            "                columns_by_name[column.get(\"column_name\")] = column",
            "            else:",
            "                columns_by_name[column.column_name] = column",
            "",
            "    generic_types: list[GenericDataType] = []",
            "    for column in df.columns:",
            "        column_object = columns_by_name.get(column)",
            "        series = df[column]",
            "        inferred_type = infer_dtype(series)",
            "        if isinstance(column_object, dict):",
            "            generic_type = (",
            "                GenericDataType.TEMPORAL",
            "                if column_object and column_object.get(\"is_dttm\")",
            "                else inferred_type_map.get(inferred_type, GenericDataType.STRING)",
            "            )",
            "        else:",
            "            generic_type = (",
            "                GenericDataType.TEMPORAL",
            "                if column_object and column_object.is_dttm",
            "                else inferred_type_map.get(inferred_type, GenericDataType.STRING)",
            "            )",
            "        generic_types.append(generic_type)",
            "",
            "    return generic_types",
            "",
            "",
            "def extract_column_dtype(col: BaseColumn) -> GenericDataType:",
            "    if col.is_temporal:",
            "        return GenericDataType.TEMPORAL",
            "    if col.is_numeric:",
            "        return GenericDataType.NUMERIC",
            "    # TODO: add check for boolean data type when proper support is added",
            "    return GenericDataType.STRING",
            "",
            "",
            "def indexed(items: list[Any], key: str | Callable[[Any], Any]) -> dict[Any, list[Any]]:",
            "    \"\"\"Build an index for a list of objects\"\"\"",
            "    idx: dict[Any, Any] = {}",
            "    for item in items:",
            "        key_ = getattr(item, key) if isinstance(key, str) else key(item)",
            "        idx.setdefault(key_, []).append(item)",
            "    return idx",
            "",
            "",
            "def is_test() -> bool:",
            "    return parse_boolean_string(os.environ.get(\"SUPERSET_TESTENV\", \"false\"))",
            "",
            "",
            "def get_time_filter_status(",
            "    datasource: BaseDatasource,",
            "    applied_time_extras: dict[str, str],",
            ") -> tuple[list[dict[str, str]], list[dict[str, str]]]:",
            "    temporal_columns: set[Any] = {",
            "        col.column_name for col in datasource.columns if col.is_dttm",
            "    }",
            "    applied: list[dict[str, str]] = []",
            "    rejected: list[dict[str, str]] = []",
            "    if time_column := applied_time_extras.get(ExtraFiltersTimeColumnType.TIME_COL):",
            "        if time_column in temporal_columns:",
            "            applied.append({\"column\": ExtraFiltersTimeColumnType.TIME_COL})",
            "        else:",
            "            rejected.append(",
            "                {",
            "                    \"reason\": ExtraFiltersReasonType.COL_NOT_IN_DATASOURCE,",
            "                    \"column\": ExtraFiltersTimeColumnType.TIME_COL,",
            "                }",
            "            )",
            "",
            "    if ExtraFiltersTimeColumnType.TIME_GRAIN in applied_time_extras:",
            "        # are there any temporal columns to assign the time grain to?",
            "        if temporal_columns:",
            "            applied.append({\"column\": ExtraFiltersTimeColumnType.TIME_GRAIN})",
            "        else:",
            "            rejected.append(",
            "                {",
            "                    \"reason\": ExtraFiltersReasonType.NO_TEMPORAL_COLUMN,",
            "                    \"column\": ExtraFiltersTimeColumnType.TIME_GRAIN,",
            "                }",
            "            )",
            "",
            "    if applied_time_extras.get(ExtraFiltersTimeColumnType.TIME_RANGE):",
            "        # are there any temporal columns to assign the time range to?",
            "        if temporal_columns:",
            "            applied.append({\"column\": ExtraFiltersTimeColumnType.TIME_RANGE})",
            "        else:",
            "            rejected.append(",
            "                {",
            "                    \"reason\": ExtraFiltersReasonType.NO_TEMPORAL_COLUMN,",
            "                    \"column\": ExtraFiltersTimeColumnType.TIME_RANGE,",
            "                }",
            "            )",
            "",
            "    return applied, rejected",
            "",
            "",
            "def format_list(items: Sequence[str], sep: str = \", \", quote: str = '\"') -> str:",
            "    quote_escaped = \"\\\\\" + quote",
            "    return sep.join(f\"{quote}{x.replace(quote, quote_escaped)}{quote}\" for x in items)",
            "",
            "",
            "def find_duplicates(items: Iterable[InputType]) -> list[InputType]:",
            "    \"\"\"Find duplicate items in an iterable.\"\"\"",
            "    return [item for item, count in collections.Counter(items).items() if count > 1]",
            "",
            "",
            "def remove_duplicates(",
            "    items: Iterable[InputType], key: Callable[[InputType], Any] | None = None",
            ") -> list[InputType]:",
            "    \"\"\"Remove duplicate items in an iterable.\"\"\"",
            "    if not key:",
            "        return list(dict.fromkeys(items).keys())",
            "    seen = set()",
            "    result = []",
            "    for item in items:",
            "        item_key = key(item)",
            "        if item_key not in seen:",
            "            seen.add(item_key)",
            "            result.append(item)",
            "    return result",
            "",
            "",
            "@dataclass",
            "class DateColumn:",
            "    col_label: str",
            "    timestamp_format: str | None = None",
            "    offset: int | None = None",
            "    time_shift: str | None = None",
            "",
            "    def __hash__(self) -> int:",
            "        return hash(self.col_label)",
            "",
            "    def __eq__(self, other: object) -> bool:",
            "        return isinstance(other, DateColumn) and hash(self) == hash(other)",
            "",
            "    @classmethod",
            "    def get_legacy_time_column(",
            "        cls,",
            "        timestamp_format: str | None,",
            "        offset: int | None,",
            "        time_shift: str | None,",
            "    ) -> DateColumn:",
            "        return cls(",
            "            timestamp_format=timestamp_format,",
            "            offset=offset,",
            "            time_shift=time_shift,",
            "            col_label=DTTM_ALIAS,",
            "        )",
            "",
            "",
            "def normalize_dttm_col(",
            "    df: pd.DataFrame,",
            "    dttm_cols: tuple[DateColumn, ...] = tuple(),",
            ") -> None:",
            "    for _col in dttm_cols:",
            "        if _col.col_label not in df.columns:",
            "            continue",
            "",
            "        if _col.timestamp_format in (\"epoch_s\", \"epoch_ms\"):",
            "            dttm_series = df[_col.col_label]",
            "            if is_numeric_dtype(dttm_series):",
            "                # Column is formatted as a numeric value",
            "                unit = _col.timestamp_format.replace(\"epoch_\", \"\")",
            "                df[_col.col_label] = pd.to_datetime(",
            "                    dttm_series,",
            "                    utc=False,",
            "                    unit=unit,",
            "                    origin=\"unix\",",
            "                    errors=\"raise\",",
            "                    exact=False,",
            "                )",
            "            else:",
            "                # Column has already been formatted as a timestamp.",
            "                df[_col.col_label] = dttm_series.apply(pd.Timestamp)",
            "        else:",
            "            df[_col.col_label] = pd.to_datetime(",
            "                df[_col.col_label],",
            "                utc=False,",
            "                format=_col.timestamp_format,",
            "                errors=\"raise\",",
            "                exact=False,",
            "            )",
            "        if _col.offset:",
            "            df[_col.col_label] += timedelta(hours=_col.offset)",
            "        if _col.time_shift is not None:",
            "            df[_col.col_label] += parse_human_timedelta(_col.time_shift)",
            "",
            "",
            "def parse_boolean_string(bool_str: str | None) -> bool:",
            "    \"\"\"",
            "    Convert a string representation of a true/false value into a boolean",
            "",
            "    >>> parse_boolean_string(None)",
            "    False",
            "    >>> parse_boolean_string('false')",
            "    False",
            "    >>> parse_boolean_string('true')",
            "    True",
            "    >>> parse_boolean_string('False')",
            "    False",
            "    >>> parse_boolean_string('True')",
            "    True",
            "    >>> parse_boolean_string('foo')",
            "    False",
            "    >>> parse_boolean_string('0')",
            "    False",
            "    >>> parse_boolean_string('1')",
            "    True",
            "",
            "    :param bool_str: string representation of a value that is assumed to be boolean",
            "    :return: parsed boolean value",
            "    \"\"\"",
            "    if bool_str is None:",
            "        return False",
            "    return bool_str.lower() in (\"y\", \"Y\", \"yes\", \"True\", \"t\", \"true\", \"On\", \"on\", \"1\")",
            "",
            "",
            "def apply_max_row_limit(",
            "    limit: int,",
            "    max_limit: int | None = None,",
            ") -> int:",
            "    \"\"\"",
            "    Override row limit if max global limit is defined",
            "",
            "    :param limit: requested row limit",
            "    :param max_limit: Maximum allowed row limit",
            "    :return: Capped row limit",
            "",
            "    >>> apply_max_row_limit(100000, 10)",
            "    10",
            "    >>> apply_max_row_limit(10, 100000)",
            "    10",
            "    >>> apply_max_row_limit(0, 10000)",
            "    10000",
            "    \"\"\"",
            "    if max_limit is None:",
            "        max_limit = current_app.config[\"SQL_MAX_ROW\"]",
            "    if limit != 0:",
            "        return min(max_limit, limit)",
            "    return max_limit",
            "",
            "",
            "def create_zip(files: dict[str, Any]) -> BytesIO:",
            "    buf = BytesIO()",
            "    with ZipFile(buf, \"w\") as bundle:",
            "        for filename, contents in files.items():",
            "            with bundle.open(filename, \"w\") as fp:",
            "                fp.write(contents)",
            "    buf.seek(0)",
            "    return buf",
            "",
            "",
            "def check_is_safe_zip(zip_file: ZipFile) -> None:",
            "    \"\"\"",
            "    Checks whether a ZIP file is safe, raises SupersetException if not.",
            "",
            "    :param zip_file:",
            "    :return:",
            "    \"\"\"",
            "    uncompress_size = 0",
            "    compress_size = 0",
            "    for zip_file_element in zip_file.infolist():",
            "        if zip_file_element.file_size > current_app.config[\"ZIPPED_FILE_MAX_SIZE\"]:",
            "            raise SupersetException(\"Found file with size above allowed threshold\")",
            "        uncompress_size += zip_file_element.file_size",
            "        compress_size += zip_file_element.compress_size",
            "    compress_ratio = uncompress_size / compress_size",
            "    if compress_ratio > current_app.config[\"ZIP_FILE_MAX_COMPRESS_RATIO\"]:",
            "        raise SupersetException(\"Zip compress ratio above allowed threshold\")",
            "",
            "",
            "def remove_extra_adhoc_filters(form_data: dict[str, Any]) -> None:",
            "    \"\"\"",
            "    Remove filters from slice data that originate from a filter box or native filter",
            "    \"\"\"",
            "    adhoc_filters = {",
            "        key: value for key, value in form_data.items() if ADHOC_FILTERS_REGEX.match(key)",
            "    }",
            "    for key, value in adhoc_filters.items():",
            "        form_data[key] = [",
            "            filter_ for filter_ in value or [] if not filter_.get(\"isExtra\")",
            "        ]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.twisted.web.client.URI.fromBytes"
        ]
    }
}