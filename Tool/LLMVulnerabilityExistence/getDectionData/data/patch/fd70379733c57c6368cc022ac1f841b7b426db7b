{
    "src/_bentoml_impl/client/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " from .base import AbstractClient"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from .http import AsyncHTTPClient"
            },
            "2": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+from .http import AsyncHTTPClient as _AsyncHTTPClient"
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from .http import HTTPClient"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from .http import SyncHTTPClient"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 4,
                "PatchRowcode": "+from .http import SyncHTTPClient as _SyncHTTPClient"
            },
            "6": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " from .proxy import RemoteProxy"
            },
            "7": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " __all__ = ["
            },
            "9": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": "     \"AbstractClient\","
            },
            "10": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": "     \"RemoteProxy\","
            },
            "11": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " ]"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 14,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 15,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+class SyncHTTPClient(_SyncHTTPClient):"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 17,
                "PatchRowcode": "+    \"\"\"A synchronous client for BentoML service."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 18,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+    Args:"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+        url (str): URL of the BentoML service."
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+        token (str, optional): Authentication token. Defaults to None."
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+        timeout (float, optional): Timeout for the client. Defaults to 30."
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+    Example::"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+        with SyncHTTPClient(\"http://localhost:3000\") as client:"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+            resp = client.call(\"classify\", input_series=[[1,2,3,4]])"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+            assert resp == [0]"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+            # Or using named method directly"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+            resp = client.classify(input_series=[[1,2,3,4]])"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+            assert resp == [0]"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+    \"\"\""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+    def __init__("
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+        self, url: str, *, token: str | None = None, timeout: float = 30"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+    ) -> None:"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+        super().__init__(url, token=token, timeout=timeout)"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+class AsyncHTTPClient(_AsyncHTTPClient):"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+    \"\"\"An asynchronous client for BentoML service."
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+    Args:"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+        url (str): URL of the BentoML service."
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+        token (str, optional): Authentication token. Defaults to None."
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        timeout (float, optional): Timeout for the client. Defaults to 30."
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+    Example::"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+        async with AsyncHTTPClient(\"http://localhost:3000\") as client:"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+            resp = await client.call(\"classify\", input_series=[[1,2,3,4]])"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+            assert resp == [0]"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+            # Or using named method directly"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+            resp = await client.classify(input_series=[[1,2,3,4]])"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+            assert resp == [0]"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+            # Streaming"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+            resp = client.stream(prompt=\"hello\")"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+            async for data in resp:"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+                print(data)"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+    \"\"\""
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+    def __init__("
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+        self, url: str, *, token: str | None = None, timeout: float = 30"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+    ) -> None:"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+        super().__init__(url, token=token, timeout=timeout)"
            }
        },
        "frontPatchFile": [
            "from .base import AbstractClient",
            "from .http import AsyncHTTPClient",
            "from .http import HTTPClient",
            "from .http import SyncHTTPClient",
            "from .proxy import RemoteProxy",
            "",
            "__all__ = [",
            "    \"AsyncHTTPClient\",",
            "    \"SyncHTTPClient\",",
            "    \"HTTPClient\",",
            "    \"AbstractClient\",",
            "    \"RemoteProxy\",",
            "]"
        ],
        "afterPatchFile": [
            "from .base import AbstractClient",
            "from .http import AsyncHTTPClient as _AsyncHTTPClient",
            "from .http import HTTPClient",
            "from .http import SyncHTTPClient as _SyncHTTPClient",
            "from .proxy import RemoteProxy",
            "",
            "__all__ = [",
            "    \"AsyncHTTPClient\",",
            "    \"SyncHTTPClient\",",
            "    \"HTTPClient\",",
            "    \"AbstractClient\",",
            "    \"RemoteProxy\",",
            "]",
            "",
            "",
            "class SyncHTTPClient(_SyncHTTPClient):",
            "    \"\"\"A synchronous client for BentoML service.",
            "",
            "    Args:",
            "        url (str): URL of the BentoML service.",
            "        token (str, optional): Authentication token. Defaults to None.",
            "        timeout (float, optional): Timeout for the client. Defaults to 30.",
            "",
            "    Example::",
            "",
            "        with SyncHTTPClient(\"http://localhost:3000\") as client:",
            "            resp = client.call(\"classify\", input_series=[[1,2,3,4]])",
            "            assert resp == [0]",
            "            # Or using named method directly",
            "            resp = client.classify(input_series=[[1,2,3,4]])",
            "            assert resp == [0]",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self, url: str, *, token: str | None = None, timeout: float = 30",
            "    ) -> None:",
            "        super().__init__(url, token=token, timeout=timeout)",
            "",
            "",
            "class AsyncHTTPClient(_AsyncHTTPClient):",
            "    \"\"\"An asynchronous client for BentoML service.",
            "",
            "    Args:",
            "        url (str): URL of the BentoML service.",
            "        token (str, optional): Authentication token. Defaults to None.",
            "        timeout (float, optional): Timeout for the client. Defaults to 30.",
            "",
            "    Example::",
            "",
            "        async with AsyncHTTPClient(\"http://localhost:3000\") as client:",
            "            resp = await client.call(\"classify\", input_series=[[1,2,3,4]])",
            "            assert resp == [0]",
            "            # Or using named method directly",
            "            resp = await client.classify(input_series=[[1,2,3,4]])",
            "            assert resp == [0]",
            "",
            "            # Streaming",
            "            resp = client.stream(prompt=\"hello\")",
            "            async for data in resp:",
            "                print(data)",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self, url: str, *, token: str | None = None, timeout: float = 30",
            "    ) -> None:",
            "        super().__init__(url, token=token, timeout=timeout)"
        ],
        "action": [
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "2": [],
            "4": []
        },
        "addLocation": []
    },
    "src/_bentoml_impl/client/http.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 340,
                "afterPatchRowNumber": 340,
                "PatchRowcode": " class SyncHTTPClient(HTTPClient[httpx.Client]):"
            },
            "1": {
                "beforePatchRowNumber": 341,
                "afterPatchRowNumber": 341,
                "PatchRowcode": "     \"\"\"A synchronous client for BentoML service."
            },
            "2": {
                "beforePatchRowNumber": 342,
                "afterPatchRowNumber": 342,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 343,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Example:"
            },
            "4": {
                "beforePatchRowNumber": 344,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "5": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        with SyncHTTPClient(\"http://localhost:3000\") as client:"
            },
            "6": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            resp = client.call(\"classify\", input_series=[[1,2,3,4]])"
            },
            "7": {
                "beforePatchRowNumber": 347,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            assert resp == [0]"
            },
            "8": {
                "beforePatchRowNumber": 348,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # Or using named method directly"
            },
            "9": {
                "beforePatchRowNumber": 349,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            resp = client.classify(input_series=[[1,2,3,4]])"
            },
            "10": {
                "beforePatchRowNumber": 350,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            assert resp == [0]"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+    .. note:: Inner usage ONLY"
            },
            "12": {
                "beforePatchRowNumber": 351,
                "afterPatchRowNumber": 344,
                "PatchRowcode": "     \"\"\""
            },
            "13": {
                "beforePatchRowNumber": 352,
                "afterPatchRowNumber": 345,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 353,
                "afterPatchRowNumber": 346,
                "PatchRowcode": "     client_cls = httpx.Client"
            },
            "15": {
                "beforePatchRowNumber": 449,
                "afterPatchRowNumber": 442,
                "PatchRowcode": " class AsyncHTTPClient(HTTPClient[httpx.AsyncClient]):"
            },
            "16": {
                "beforePatchRowNumber": 450,
                "afterPatchRowNumber": 443,
                "PatchRowcode": "     \"\"\"An asynchronous client for BentoML service."
            },
            "17": {
                "beforePatchRowNumber": 451,
                "afterPatchRowNumber": 444,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 452,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Example:"
            },
            "19": {
                "beforePatchRowNumber": 453,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "20": {
                "beforePatchRowNumber": 454,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        async with AsyncHTTPClient(\"http://localhost:3000\") as client:"
            },
            "21": {
                "beforePatchRowNumber": 455,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            resp = await client.call(\"classify\", input_series=[[1,2,3,4]])"
            },
            "22": {
                "beforePatchRowNumber": 456,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            assert resp == [0]"
            },
            "23": {
                "beforePatchRowNumber": 457,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # Or using named method directly"
            },
            "24": {
                "beforePatchRowNumber": 458,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            resp = await client.classify(input_series=[[1,2,3,4]])"
            },
            "25": {
                "beforePatchRowNumber": 459,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            assert resp == [0]"
            },
            "26": {
                "beforePatchRowNumber": 460,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "27": {
                "beforePatchRowNumber": 461,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    .. note::"
            },
            "28": {
                "beforePatchRowNumber": 462,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "29": {
                "beforePatchRowNumber": 463,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        If the endpoint returns an async generator, it should be awaited before iterating."
            },
            "30": {
                "beforePatchRowNumber": 464,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "31": {
                "beforePatchRowNumber": 465,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Example:"
            },
            "32": {
                "beforePatchRowNumber": 466,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "33": {
                "beforePatchRowNumber": 467,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            resp = await client.stream(prompt=\"hello\")"
            },
            "34": {
                "beforePatchRowNumber": 468,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            async for data in resp:"
            },
            "35": {
                "beforePatchRowNumber": 469,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                print(data)"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 445,
                "PatchRowcode": "+    .. note:: Inner usage ONLY"
            },
            "37": {
                "beforePatchRowNumber": 470,
                "afterPatchRowNumber": 446,
                "PatchRowcode": "     \"\"\""
            },
            "38": {
                "beforePatchRowNumber": 471,
                "afterPatchRowNumber": 447,
                "PatchRowcode": " "
            },
            "39": {
                "beforePatchRowNumber": 472,
                "afterPatchRowNumber": 448,
                "PatchRowcode": "     client_cls = httpx.AsyncClient"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import inspect",
            "import io",
            "import json",
            "import logging",
            "import mimetypes",
            "import pathlib",
            "import tempfile",
            "import typing as t",
            "from abc import abstractmethod",
            "from functools import cached_property",
            "from http import HTTPStatus",
            "from urllib.parse import urljoin",
            "from urllib.parse import urlparse",
            "",
            "import attr",
            "import httpx",
            "from pydantic import RootModel",
            "",
            "from _bentoml_sdk import IODescriptor",
            "from _bentoml_sdk.typing_utils import is_image_type",
            "from bentoml import __version__",
            "from bentoml._internal.utils.uri import uri_to_path",
            "from bentoml.exceptions import BentoMLException",
            "",
            "from .base import AbstractClient",
            "from .base import ClientEndpoint",
            "",
            "if t.TYPE_CHECKING:",
            "    from httpx._client import BaseClient",
            "    from httpx._types import RequestFiles",
            "",
            "    from _bentoml_sdk import Service",
            "",
            "    from ..serde import Serde",
            "",
            "    T = t.TypeVar(\"T\", bound=\"HTTPClient[t.Any]\")",
            "",
            "C = t.TypeVar(\"C\", bound=\"BaseClient\")",
            "logger = logging.getLogger(\"bentoml.io\")",
            "MAX_RETRIES = 3",
            "",
            "",
            "def is_http_url(url: str) -> bool:",
            "    return urlparse(url).scheme in {\"http\", \"https\"}",
            "",
            "",
            "@attr.define",
            "class HTTPClient(AbstractClient, t.Generic[C]):",
            "    client_cls: t.ClassVar[type[BaseClient]]",
            "",
            "    url: str",
            "    endpoints: dict[str, ClientEndpoint] = attr.field(factory=dict)",
            "    media_type: str = \"application/json\"",
            "    timeout: float = 30",
            "    token: str | None = None",
            "    const_headers: dict[str, str] = attr.field(factory=dict)",
            "",
            "    _opened_files: list[io.BufferedReader] = attr.field(init=False, factory=list)",
            "    _temp_dir: tempfile.TemporaryDirectory[str] = attr.field(init=False)",
            "",
            "    @cached_property",
            "    def client(self) -> C:",
            "        parsed = urlparse(self.url)",
            "        transport = None",
            "        url = self.url",
            "        if parsed.scheme == \"file\":",
            "            uds = uri_to_path(self.url)",
            "            if self.client_cls is httpx.Client:",
            "                transport = httpx.HTTPTransport(uds=uds)",
            "            else:",
            "                transport = httpx.AsyncHTTPTransport(uds=uds)",
            "            url = \"http://127.0.0.1:3000\"",
            "        elif parsed.scheme == \"tcp\":",
            "            url = f\"http://{parsed.netloc}\"",
            "        return t.cast(",
            "            \"C\",",
            "            self.client_cls(",
            "                base_url=url,",
            "                transport=transport,  # type: ignore",
            "                headers=self.default_headers,",
            "                timeout=self.timeout,",
            "                follow_redirects=True,",
            "            ),",
            "        )",
            "",
            "    @_temp_dir.default  # type: ignore",
            "    def default_temp_dir(self) -> tempfile.TemporaryDirectory[str]:",
            "        return tempfile.TemporaryDirectory(prefix=\"bentoml-client-\")",
            "",
            "    def __init__(",
            "        self,",
            "        url: str,",
            "        *,",
            "        media_type: str = \"application/json\",",
            "        service: Service[t.Any] | None = None,",
            "        token: str | None = None,",
            "        timeout: float = 30,",
            "    ) -> None:",
            "        \"\"\"Create a client instance from a URL.",
            "",
            "        Args:",
            "            url: The URL of the BentoML service.",
            "            media_type: The media type to use for serialization. Defaults to",
            "                \"application/json\".",
            "",
            "        .. note::",
            "",
            "            The client created with this method can only return primitive types without a model.",
            "        \"\"\"",
            "        routes: dict[str, ClientEndpoint] = {}",
            "        if service is None:",
            "            schema_url = urljoin(url, \"/schema.json\")",
            "",
            "            headers = {\"User-Agent\": f\"BentoML HTTP Client/{__version__}\"}",
            "            if token:",
            "                headers[\"Authorization\"] = f\"Bearer {token}\"",
            "",
            "            with httpx.Client(headers=headers) as client:",
            "                resp = client.get(schema_url)",
            "",
            "                if resp.is_error:",
            "                    raise RuntimeError(f\"Failed to fetch schema from {schema_url}\")",
            "                for route in resp.json()[\"routes\"]:",
            "                    routes[route[\"name\"]] = ClientEndpoint(",
            "                        name=route[\"name\"],",
            "                        route=route[\"route\"],",
            "                        input=route[\"input\"],",
            "                        output=route[\"output\"],",
            "                        doc=route.get(\"doc\"),",
            "                        stream_output=route[\"output\"].get(\"is_stream\", False),",
            "                    )",
            "            const_headers = {}",
            "        else:",
            "            for name, method in service.apis.items():",
            "                routes[name] = ClientEndpoint(",
            "                    name=name,",
            "                    route=method.route,",
            "                    input=method.input_spec.model_json_schema(),",
            "                    output=method.output_spec.model_json_schema(),",
            "                    doc=method.doc,",
            "                    input_spec=method.input_spec,",
            "                    output_spec=method.output_spec,",
            "                    stream_output=method.is_stream,",
            "                )",
            "",
            "            from bentoml._internal.context import component_context",
            "",
            "            const_headers = {",
            "                \"Bento-Name\": component_context.bento_name,",
            "                \"Bento-Version\": component_context.bento_version,",
            "                \"Runner-Name\": service.name,",
            "                \"Yatai-Bento-Deployment-Name\": component_context.yatai_bento_deployment_name,",
            "                \"Yatai-Bento-Deployment-Namespace\": component_context.yatai_bento_deployment_namespace,",
            "            }",
            "        self.__attrs_init__(  # type: ignore",
            "            url=url,",
            "            endpoints=routes,",
            "            media_type=media_type,",
            "            token=token,",
            "            const_headers=const_headers,",
            "            timeout=timeout,",
            "        )",
            "        super().__init__()",
            "",
            "    @cached_property",
            "    def serde(self) -> Serde:",
            "        from ..serde import ALL_SERDE",
            "",
            "        return ALL_SERDE[self.media_type]()",
            "",
            "    @cached_property",
            "    def default_headers(self) -> dict[str, str]:",
            "        headers = self.const_headers.copy()",
            "        headers[\"User-Agent\"] = f\"BentoML HTTP Client/{__version__}\"",
            "        if self.token:",
            "            headers[\"Authorization\"] = f\"Bearer {self.token}\"",
            "",
            "        return headers",
            "",
            "    def _build_request(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        args: t.Sequence[t.Any],",
            "        kwargs: dict[str, t.Any],",
            "        headers: t.Mapping[str, str],",
            "    ) -> httpx.Request:",
            "        headers = httpx.Headers({\"Content-Type\": self.media_type, **headers})",
            "        if endpoint.input_spec is not None:",
            "            model = endpoint.input_spec.from_inputs(*args, **kwargs)",
            "            if model.multipart_fields and self.media_type == \"application/json\":",
            "                return self._build_multipart(endpoint, model, headers)",
            "            else:",
            "                return self.client.build_request(",
            "                    \"POST\",",
            "                    endpoint.route,",
            "                    headers=headers,",
            "                    content=self.serde.serialize_model(model),",
            "                )",
            "",
            "        for name, value in zip(endpoint.input[\"properties\"], args):",
            "            if name in kwargs:",
            "                raise TypeError(f\"Duplicate argument {name}\")",
            "            kwargs[name] = value",
            "",
            "        params = set(endpoint.input[\"properties\"].keys())",
            "        non_exist_args = set(kwargs.keys()) - set(params)",
            "        if non_exist_args:",
            "            raise TypeError(",
            "                f\"Arguments not found in endpoint {endpoint.name}: {non_exist_args}\"",
            "            )",
            "        required = set(endpoint.input.get(\"required\", []))",
            "        missing_args = set(required) - set(kwargs.keys())",
            "        if missing_args:",
            "            raise TypeError(",
            "                f\"Missing required arguments in endpoint {endpoint.name}: {missing_args}\"",
            "            )",
            "        has_file = any(",
            "            schema.get(\"type\") == \"file\"",
            "            or schema.get(\"type\") == \"array\"",
            "            and schema[\"items\"].get(\"type\") == \"file\"",
            "            for schema in endpoint.input[\"properties\"].values()",
            "        )",
            "        if has_file and self.media_type == \"application/json\":",
            "            return self._build_multipart(endpoint, kwargs, headers)",
            "        return self.client.build_request(",
            "            \"POST\",",
            "            endpoint.route,",
            "            content=self.serde.serialize(kwargs, endpoint.input),",
            "            headers=headers,",
            "        )",
            "",
            "    def _build_multipart(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        model: IODescriptor | dict[str, t.Any],",
            "        headers: httpx.Headers,",
            "    ) -> httpx.Request:",
            "        def is_file_field(k: str) -> bool:",
            "            if isinstance(model, IODescriptor):",
            "                return k in model.multipart_fields",
            "            if (f := endpoint.input[\"properties\"].get(k, {})).get(\"type\") == \"file\":",
            "                return True",
            "            if f.get(\"type\") == \"array\" and f[\"items\"].get(\"type\") == \"file\":",
            "                return True",
            "            return False",
            "",
            "        if isinstance(model, dict):",
            "            fields = model",
            "        else:",
            "            fields = {k: getattr(model, k) for k in model.model_fields}",
            "        data: dict[str, t.Any] = {}",
            "        files: RequestFiles = []",
            "",
            "        for name, value in fields.items():",
            "            if not is_file_field(name):",
            "                data[name] = json.dumps(value)",
            "                continue",
            "            if not isinstance(value, (list, tuple)):",
            "                value = [value]",
            "",
            "            for v in value:",
            "                if isinstance(v, str) and not is_http_url(v):",
            "                    v = pathlib.Path(v)",
            "                if is_image_type(type(v)):",
            "                    files.append(",
            "                        (",
            "                            name,",
            "                            (",
            "                                None,",
            "                                getattr(v, \"_fp\", v.fp),",
            "                                f\"image/{v.format.lower()}\",",
            "                            ),",
            "                        )",
            "                    )",
            "                elif isinstance(v, pathlib.PurePath):",
            "                    file = open(v, \"rb\")",
            "                    files.append((name, (v.name, file, mimetypes.guess_type(v)[0])))",
            "                    self._opened_files.append(file)",
            "                elif isinstance(v, str):",
            "                    data.setdefault(name, []).append(v)",
            "                else:",
            "                    assert isinstance(v, t.BinaryIO)",
            "                    filename = (",
            "                        pathlib.Path(fn).name",
            "                        if (fn := getattr(v, \"name\", None))",
            "                        else None",
            "                    )",
            "                    content_type = (",
            "                        mimetypes.guess_type(filename)[0] if filename else None",
            "                    )",
            "                    files.append((name, (filename, v, content_type)))",
            "        headers.pop(\"content-type\", None)",
            "        return self.client.build_request(",
            "            \"POST\", endpoint.route, data=data, files=files, headers=headers",
            "        )",
            "",
            "    def _deserialize_output(self, data: bytes, endpoint: ClientEndpoint) -> t.Any:",
            "        if endpoint.output_spec is not None:",
            "            model = self.serde.deserialize_model(data, endpoint.output_spec)",
            "            if isinstance(model, RootModel):",
            "                return model.root  # type: ignore",
            "            return model",
            "        elif (ot := endpoint.output.get(\"type\")) == \"string\":",
            "            return data.decode(\"utf-8\")",
            "        elif ot == \"bytes\":",
            "            return data",
            "        else:",
            "            return self.serde.deserialize(data, endpoint.output)",
            "",
            "    def call(self, __name: str, /, *args: t.Any, **kwargs: t.Any) -> t.Any:",
            "        try:",
            "            endpoint = self.endpoints[__name]",
            "        except KeyError:",
            "            raise BentoMLException(f\"Endpoint {__name} not found\") from None",
            "        if endpoint.stream_output:",
            "            return self._get_stream(endpoint, args, kwargs)",
            "        else:",
            "            return self._call(endpoint, args, kwargs)",
            "",
            "    @abstractmethod",
            "    def _call(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        args: t.Sequence[t.Any],",
            "        kwargs: dict[str, t.Any],",
            "        *,",
            "        headers: t.Mapping[str, str] | None = None,",
            "    ) -> t.Any:",
            "        ...",
            "",
            "    @abstractmethod",
            "    def _get_stream(",
            "        self, endpoint: ClientEndpoint, args: t.Any, kwargs: t.Any",
            "    ) -> t.Any:",
            "        ...",
            "",
            "",
            "class SyncHTTPClient(HTTPClient[httpx.Client]):",
            "    \"\"\"A synchronous client for BentoML service.",
            "",
            "    Example:",
            "",
            "        with SyncHTTPClient(\"http://localhost:3000\") as client:",
            "            resp = client.call(\"classify\", input_series=[[1,2,3,4]])",
            "            assert resp == [0]",
            "            # Or using named method directly",
            "            resp = client.classify(input_series=[[1,2,3,4]])",
            "            assert resp == [0]",
            "    \"\"\"",
            "",
            "    client_cls = httpx.Client",
            "",
            "    def __enter__(self: T) -> T:",
            "        return self",
            "",
            "    def __exit__(self, exc_type: t.Any, exc: t.Any, tb: t.Any) -> None:",
            "        return self.close()",
            "",
            "    def is_ready(self, timeout: int | None = None) -> bool:",
            "        try:",
            "            resp = self.client.get(",
            "                \"/readyz\", timeout=timeout or httpx.USE_CLIENT_DEFAULT",
            "            )",
            "            return resp.status_code == 200",
            "        except httpx.TimeoutException:",
            "            logger.warn(\"Timed out waiting for runner to be ready\")",
            "            return False",
            "",
            "    def close(self) -> None:",
            "        if \"client\" in vars(self):",
            "            self.client.close()",
            "",
            "    def _get_stream(",
            "        self, endpoint: ClientEndpoint, args: t.Any, kwargs: t.Any",
            "    ) -> t.Generator[t.Any, None, None]:",
            "        resp = self._call(endpoint, args, kwargs)",
            "        for data in resp:",
            "            yield data",
            "",
            "    def _call(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        args: t.Sequence[t.Any],",
            "        kwargs: dict[str, t.Any],",
            "        *,",
            "        headers: t.Mapping[str, str] | None = None,",
            "    ) -> t.Any:",
            "        try:",
            "            req = self._build_request(endpoint, args, kwargs, headers or {})",
            "            resp = self.client.send(req, stream=endpoint.stream_output)",
            "            if resp.is_error:",
            "                resp.read()",
            "                raise BentoMLException(",
            "                    f\"Error making request: {resp.status_code}: {resp.text}\",",
            "                    error_code=HTTPStatus(resp.status_code),",
            "                )",
            "            if endpoint.stream_output:",
            "                return self._parse_stream_response(endpoint, resp)",
            "            elif (",
            "                endpoint.output.get(\"type\") == \"file\"",
            "                and self.media_type == \"application/json\"",
            "            ):",
            "                return self._parse_file_response(endpoint, resp)",
            "            else:",
            "                return self._parse_response(endpoint, resp)",
            "        finally:",
            "            for f in self._opened_files:",
            "                f.close()",
            "            self._opened_files.clear()",
            "",
            "    def _parse_response(self, endpoint: ClientEndpoint, resp: httpx.Response) -> t.Any:",
            "        data = resp.read()",
            "        return self._deserialize_output(data, endpoint)",
            "",
            "    def _parse_stream_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> t.Generator[t.Any, None, None]:",
            "        try:",
            "            for data in resp.iter_bytes():",
            "                yield self._deserialize_output(data, endpoint)",
            "        finally:",
            "            resp.close()",
            "",
            "    def _parse_file_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> pathlib.Path:",
            "        from multipart.multipart import parse_options_header",
            "",
            "        content_disposition = resp.headers.get(\"content-disposition\")",
            "        filename: str | None = None",
            "        if content_disposition:",
            "            _, options = parse_options_header(content_disposition)",
            "            if b\"filename\" in options:",
            "                filename = str(",
            "                    options[b\"filename\"],",
            "                    resp.charset_encoding or \"utf-8\",",
            "                    errors=\"ignore\",",
            "                )",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            \"wb\", suffix=filename, dir=self._temp_dir.name, delete=False",
            "        ) as f:",
            "            f.write(resp.read())",
            "        return pathlib.Path(f.name)",
            "",
            "",
            "class AsyncHTTPClient(HTTPClient[httpx.AsyncClient]):",
            "    \"\"\"An asynchronous client for BentoML service.",
            "",
            "    Example:",
            "",
            "        async with AsyncHTTPClient(\"http://localhost:3000\") as client:",
            "            resp = await client.call(\"classify\", input_series=[[1,2,3,4]])",
            "            assert resp == [0]",
            "            # Or using named method directly",
            "            resp = await client.classify(input_series=[[1,2,3,4]])",
            "            assert resp == [0]",
            "",
            "    .. note::",
            "",
            "        If the endpoint returns an async generator, it should be awaited before iterating.",
            "",
            "        Example:",
            "",
            "            resp = await client.stream(prompt=\"hello\")",
            "            async for data in resp:",
            "                print(data)",
            "    \"\"\"",
            "",
            "    client_cls = httpx.AsyncClient",
            "",
            "    async def is_ready(self, timeout: int | None = None) -> bool:",
            "        try:",
            "            resp = await self.client.get(",
            "                \"/readyz\", timeout=timeout or httpx.USE_CLIENT_DEFAULT",
            "            )",
            "            return resp.status_code == 200",
            "        except httpx.TimeoutException:",
            "            logger.warn(\"Timed out waiting for runner to be ready\")",
            "            return False",
            "",
            "    async def _get_stream(",
            "        self, endpoint: ClientEndpoint, args: t.Any, kwargs: t.Any",
            "    ) -> t.AsyncGenerator[t.Any, None]:",
            "        resp = await self._call(endpoint, args, kwargs)",
            "        assert inspect.isasyncgen(resp)",
            "        async for data in resp:",
            "            yield data",
            "",
            "    async def __aenter__(self: T) -> T:",
            "        return self",
            "",
            "    async def __aexit__(self, *args: t.Any) -> None:",
            "        return await self.close()",
            "",
            "    async def _call(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        args: t.Sequence[t.Any],",
            "        kwargs: dict[str, t.Any],",
            "        *,",
            "        headers: t.Mapping[str, str] | None = None,",
            "    ) -> t.Any:",
            "        try:",
            "            req = self._build_request(endpoint, args, kwargs, headers or {})",
            "            resp = await self.client.send(req, stream=endpoint.stream_output)",
            "            if resp.is_error:",
            "                await resp.aread()",
            "                raise BentoMLException(",
            "                    f\"Error making request: {resp.status_code}: {resp.text}\",",
            "                    error_code=HTTPStatus(resp.status_code),",
            "                )",
            "            if endpoint.stream_output:",
            "                return self._parse_stream_response(endpoint, resp)",
            "            elif (",
            "                endpoint.output.get(\"type\") == \"file\"",
            "                and self.media_type == \"application/json\"",
            "            ):",
            "                return await self._parse_file_response(endpoint, resp)",
            "            else:",
            "                return await self._parse_response(endpoint, resp)",
            "        finally:",
            "            for f in self._opened_files:",
            "                f.close()",
            "            self._opened_files.clear()",
            "",
            "    async def _parse_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> t.Any:",
            "        data = await resp.aread()",
            "        return self._deserialize_output(data, endpoint)",
            "",
            "    async def _parse_stream_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> t.AsyncGenerator[t.Any, None]:",
            "        try:",
            "            async for data in resp.aiter_bytes():",
            "                yield self._deserialize_output(data, endpoint)",
            "        finally:",
            "            await resp.aclose()",
            "",
            "    async def _parse_file_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> pathlib.Path:",
            "        from multipart.multipart import parse_options_header",
            "",
            "        content_disposition = resp.headers.get(\"content-disposition\")",
            "        filename: str | None = None",
            "        if content_disposition:",
            "            _, options = parse_options_header(content_disposition)",
            "            if b\"filename\" in options:",
            "                filename = str(",
            "                    options[b\"filename\"],",
            "                    resp.charset_encoding or \"utf-8\",",
            "                    errors=\"ignore\",",
            "                )",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            \"wb\", suffix=filename, dir=self._temp_dir.name, delete=False",
            "        ) as f:",
            "            f.write(await resp.aread())",
            "        return pathlib.Path(f.name)",
            "",
            "    async def close(self) -> None:",
            "        if \"client\" in vars(self):",
            "            await self.client.aclose()"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import inspect",
            "import io",
            "import json",
            "import logging",
            "import mimetypes",
            "import pathlib",
            "import tempfile",
            "import typing as t",
            "from abc import abstractmethod",
            "from functools import cached_property",
            "from http import HTTPStatus",
            "from urllib.parse import urljoin",
            "from urllib.parse import urlparse",
            "",
            "import attr",
            "import httpx",
            "from pydantic import RootModel",
            "",
            "from _bentoml_sdk import IODescriptor",
            "from _bentoml_sdk.typing_utils import is_image_type",
            "from bentoml import __version__",
            "from bentoml._internal.utils.uri import uri_to_path",
            "from bentoml.exceptions import BentoMLException",
            "",
            "from .base import AbstractClient",
            "from .base import ClientEndpoint",
            "",
            "if t.TYPE_CHECKING:",
            "    from httpx._client import BaseClient",
            "    from httpx._types import RequestFiles",
            "",
            "    from _bentoml_sdk import Service",
            "",
            "    from ..serde import Serde",
            "",
            "    T = t.TypeVar(\"T\", bound=\"HTTPClient[t.Any]\")",
            "",
            "C = t.TypeVar(\"C\", bound=\"BaseClient\")",
            "logger = logging.getLogger(\"bentoml.io\")",
            "MAX_RETRIES = 3",
            "",
            "",
            "def is_http_url(url: str) -> bool:",
            "    return urlparse(url).scheme in {\"http\", \"https\"}",
            "",
            "",
            "@attr.define",
            "class HTTPClient(AbstractClient, t.Generic[C]):",
            "    client_cls: t.ClassVar[type[BaseClient]]",
            "",
            "    url: str",
            "    endpoints: dict[str, ClientEndpoint] = attr.field(factory=dict)",
            "    media_type: str = \"application/json\"",
            "    timeout: float = 30",
            "    token: str | None = None",
            "    const_headers: dict[str, str] = attr.field(factory=dict)",
            "",
            "    _opened_files: list[io.BufferedReader] = attr.field(init=False, factory=list)",
            "    _temp_dir: tempfile.TemporaryDirectory[str] = attr.field(init=False)",
            "",
            "    @cached_property",
            "    def client(self) -> C:",
            "        parsed = urlparse(self.url)",
            "        transport = None",
            "        url = self.url",
            "        if parsed.scheme == \"file\":",
            "            uds = uri_to_path(self.url)",
            "            if self.client_cls is httpx.Client:",
            "                transport = httpx.HTTPTransport(uds=uds)",
            "            else:",
            "                transport = httpx.AsyncHTTPTransport(uds=uds)",
            "            url = \"http://127.0.0.1:3000\"",
            "        elif parsed.scheme == \"tcp\":",
            "            url = f\"http://{parsed.netloc}\"",
            "        return t.cast(",
            "            \"C\",",
            "            self.client_cls(",
            "                base_url=url,",
            "                transport=transport,  # type: ignore",
            "                headers=self.default_headers,",
            "                timeout=self.timeout,",
            "                follow_redirects=True,",
            "            ),",
            "        )",
            "",
            "    @_temp_dir.default  # type: ignore",
            "    def default_temp_dir(self) -> tempfile.TemporaryDirectory[str]:",
            "        return tempfile.TemporaryDirectory(prefix=\"bentoml-client-\")",
            "",
            "    def __init__(",
            "        self,",
            "        url: str,",
            "        *,",
            "        media_type: str = \"application/json\",",
            "        service: Service[t.Any] | None = None,",
            "        token: str | None = None,",
            "        timeout: float = 30,",
            "    ) -> None:",
            "        \"\"\"Create a client instance from a URL.",
            "",
            "        Args:",
            "            url: The URL of the BentoML service.",
            "            media_type: The media type to use for serialization. Defaults to",
            "                \"application/json\".",
            "",
            "        .. note::",
            "",
            "            The client created with this method can only return primitive types without a model.",
            "        \"\"\"",
            "        routes: dict[str, ClientEndpoint] = {}",
            "        if service is None:",
            "            schema_url = urljoin(url, \"/schema.json\")",
            "",
            "            headers = {\"User-Agent\": f\"BentoML HTTP Client/{__version__}\"}",
            "            if token:",
            "                headers[\"Authorization\"] = f\"Bearer {token}\"",
            "",
            "            with httpx.Client(headers=headers) as client:",
            "                resp = client.get(schema_url)",
            "",
            "                if resp.is_error:",
            "                    raise RuntimeError(f\"Failed to fetch schema from {schema_url}\")",
            "                for route in resp.json()[\"routes\"]:",
            "                    routes[route[\"name\"]] = ClientEndpoint(",
            "                        name=route[\"name\"],",
            "                        route=route[\"route\"],",
            "                        input=route[\"input\"],",
            "                        output=route[\"output\"],",
            "                        doc=route.get(\"doc\"),",
            "                        stream_output=route[\"output\"].get(\"is_stream\", False),",
            "                    )",
            "            const_headers = {}",
            "        else:",
            "            for name, method in service.apis.items():",
            "                routes[name] = ClientEndpoint(",
            "                    name=name,",
            "                    route=method.route,",
            "                    input=method.input_spec.model_json_schema(),",
            "                    output=method.output_spec.model_json_schema(),",
            "                    doc=method.doc,",
            "                    input_spec=method.input_spec,",
            "                    output_spec=method.output_spec,",
            "                    stream_output=method.is_stream,",
            "                )",
            "",
            "            from bentoml._internal.context import component_context",
            "",
            "            const_headers = {",
            "                \"Bento-Name\": component_context.bento_name,",
            "                \"Bento-Version\": component_context.bento_version,",
            "                \"Runner-Name\": service.name,",
            "                \"Yatai-Bento-Deployment-Name\": component_context.yatai_bento_deployment_name,",
            "                \"Yatai-Bento-Deployment-Namespace\": component_context.yatai_bento_deployment_namespace,",
            "            }",
            "        self.__attrs_init__(  # type: ignore",
            "            url=url,",
            "            endpoints=routes,",
            "            media_type=media_type,",
            "            token=token,",
            "            const_headers=const_headers,",
            "            timeout=timeout,",
            "        )",
            "        super().__init__()",
            "",
            "    @cached_property",
            "    def serde(self) -> Serde:",
            "        from ..serde import ALL_SERDE",
            "",
            "        return ALL_SERDE[self.media_type]()",
            "",
            "    @cached_property",
            "    def default_headers(self) -> dict[str, str]:",
            "        headers = self.const_headers.copy()",
            "        headers[\"User-Agent\"] = f\"BentoML HTTP Client/{__version__}\"",
            "        if self.token:",
            "            headers[\"Authorization\"] = f\"Bearer {self.token}\"",
            "",
            "        return headers",
            "",
            "    def _build_request(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        args: t.Sequence[t.Any],",
            "        kwargs: dict[str, t.Any],",
            "        headers: t.Mapping[str, str],",
            "    ) -> httpx.Request:",
            "        headers = httpx.Headers({\"Content-Type\": self.media_type, **headers})",
            "        if endpoint.input_spec is not None:",
            "            model = endpoint.input_spec.from_inputs(*args, **kwargs)",
            "            if model.multipart_fields and self.media_type == \"application/json\":",
            "                return self._build_multipart(endpoint, model, headers)",
            "            else:",
            "                return self.client.build_request(",
            "                    \"POST\",",
            "                    endpoint.route,",
            "                    headers=headers,",
            "                    content=self.serde.serialize_model(model),",
            "                )",
            "",
            "        for name, value in zip(endpoint.input[\"properties\"], args):",
            "            if name in kwargs:",
            "                raise TypeError(f\"Duplicate argument {name}\")",
            "            kwargs[name] = value",
            "",
            "        params = set(endpoint.input[\"properties\"].keys())",
            "        non_exist_args = set(kwargs.keys()) - set(params)",
            "        if non_exist_args:",
            "            raise TypeError(",
            "                f\"Arguments not found in endpoint {endpoint.name}: {non_exist_args}\"",
            "            )",
            "        required = set(endpoint.input.get(\"required\", []))",
            "        missing_args = set(required) - set(kwargs.keys())",
            "        if missing_args:",
            "            raise TypeError(",
            "                f\"Missing required arguments in endpoint {endpoint.name}: {missing_args}\"",
            "            )",
            "        has_file = any(",
            "            schema.get(\"type\") == \"file\"",
            "            or schema.get(\"type\") == \"array\"",
            "            and schema[\"items\"].get(\"type\") == \"file\"",
            "            for schema in endpoint.input[\"properties\"].values()",
            "        )",
            "        if has_file and self.media_type == \"application/json\":",
            "            return self._build_multipart(endpoint, kwargs, headers)",
            "        return self.client.build_request(",
            "            \"POST\",",
            "            endpoint.route,",
            "            content=self.serde.serialize(kwargs, endpoint.input),",
            "            headers=headers,",
            "        )",
            "",
            "    def _build_multipart(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        model: IODescriptor | dict[str, t.Any],",
            "        headers: httpx.Headers,",
            "    ) -> httpx.Request:",
            "        def is_file_field(k: str) -> bool:",
            "            if isinstance(model, IODescriptor):",
            "                return k in model.multipart_fields",
            "            if (f := endpoint.input[\"properties\"].get(k, {})).get(\"type\") == \"file\":",
            "                return True",
            "            if f.get(\"type\") == \"array\" and f[\"items\"].get(\"type\") == \"file\":",
            "                return True",
            "            return False",
            "",
            "        if isinstance(model, dict):",
            "            fields = model",
            "        else:",
            "            fields = {k: getattr(model, k) for k in model.model_fields}",
            "        data: dict[str, t.Any] = {}",
            "        files: RequestFiles = []",
            "",
            "        for name, value in fields.items():",
            "            if not is_file_field(name):",
            "                data[name] = json.dumps(value)",
            "                continue",
            "            if not isinstance(value, (list, tuple)):",
            "                value = [value]",
            "",
            "            for v in value:",
            "                if isinstance(v, str) and not is_http_url(v):",
            "                    v = pathlib.Path(v)",
            "                if is_image_type(type(v)):",
            "                    files.append(",
            "                        (",
            "                            name,",
            "                            (",
            "                                None,",
            "                                getattr(v, \"_fp\", v.fp),",
            "                                f\"image/{v.format.lower()}\",",
            "                            ),",
            "                        )",
            "                    )",
            "                elif isinstance(v, pathlib.PurePath):",
            "                    file = open(v, \"rb\")",
            "                    files.append((name, (v.name, file, mimetypes.guess_type(v)[0])))",
            "                    self._opened_files.append(file)",
            "                elif isinstance(v, str):",
            "                    data.setdefault(name, []).append(v)",
            "                else:",
            "                    assert isinstance(v, t.BinaryIO)",
            "                    filename = (",
            "                        pathlib.Path(fn).name",
            "                        if (fn := getattr(v, \"name\", None))",
            "                        else None",
            "                    )",
            "                    content_type = (",
            "                        mimetypes.guess_type(filename)[0] if filename else None",
            "                    )",
            "                    files.append((name, (filename, v, content_type)))",
            "        headers.pop(\"content-type\", None)",
            "        return self.client.build_request(",
            "            \"POST\", endpoint.route, data=data, files=files, headers=headers",
            "        )",
            "",
            "    def _deserialize_output(self, data: bytes, endpoint: ClientEndpoint) -> t.Any:",
            "        if endpoint.output_spec is not None:",
            "            model = self.serde.deserialize_model(data, endpoint.output_spec)",
            "            if isinstance(model, RootModel):",
            "                return model.root  # type: ignore",
            "            return model",
            "        elif (ot := endpoint.output.get(\"type\")) == \"string\":",
            "            return data.decode(\"utf-8\")",
            "        elif ot == \"bytes\":",
            "            return data",
            "        else:",
            "            return self.serde.deserialize(data, endpoint.output)",
            "",
            "    def call(self, __name: str, /, *args: t.Any, **kwargs: t.Any) -> t.Any:",
            "        try:",
            "            endpoint = self.endpoints[__name]",
            "        except KeyError:",
            "            raise BentoMLException(f\"Endpoint {__name} not found\") from None",
            "        if endpoint.stream_output:",
            "            return self._get_stream(endpoint, args, kwargs)",
            "        else:",
            "            return self._call(endpoint, args, kwargs)",
            "",
            "    @abstractmethod",
            "    def _call(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        args: t.Sequence[t.Any],",
            "        kwargs: dict[str, t.Any],",
            "        *,",
            "        headers: t.Mapping[str, str] | None = None,",
            "    ) -> t.Any:",
            "        ...",
            "",
            "    @abstractmethod",
            "    def _get_stream(",
            "        self, endpoint: ClientEndpoint, args: t.Any, kwargs: t.Any",
            "    ) -> t.Any:",
            "        ...",
            "",
            "",
            "class SyncHTTPClient(HTTPClient[httpx.Client]):",
            "    \"\"\"A synchronous client for BentoML service.",
            "",
            "    .. note:: Inner usage ONLY",
            "    \"\"\"",
            "",
            "    client_cls = httpx.Client",
            "",
            "    def __enter__(self: T) -> T:",
            "        return self",
            "",
            "    def __exit__(self, exc_type: t.Any, exc: t.Any, tb: t.Any) -> None:",
            "        return self.close()",
            "",
            "    def is_ready(self, timeout: int | None = None) -> bool:",
            "        try:",
            "            resp = self.client.get(",
            "                \"/readyz\", timeout=timeout or httpx.USE_CLIENT_DEFAULT",
            "            )",
            "            return resp.status_code == 200",
            "        except httpx.TimeoutException:",
            "            logger.warn(\"Timed out waiting for runner to be ready\")",
            "            return False",
            "",
            "    def close(self) -> None:",
            "        if \"client\" in vars(self):",
            "            self.client.close()",
            "",
            "    def _get_stream(",
            "        self, endpoint: ClientEndpoint, args: t.Any, kwargs: t.Any",
            "    ) -> t.Generator[t.Any, None, None]:",
            "        resp = self._call(endpoint, args, kwargs)",
            "        for data in resp:",
            "            yield data",
            "",
            "    def _call(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        args: t.Sequence[t.Any],",
            "        kwargs: dict[str, t.Any],",
            "        *,",
            "        headers: t.Mapping[str, str] | None = None,",
            "    ) -> t.Any:",
            "        try:",
            "            req = self._build_request(endpoint, args, kwargs, headers or {})",
            "            resp = self.client.send(req, stream=endpoint.stream_output)",
            "            if resp.is_error:",
            "                resp.read()",
            "                raise BentoMLException(",
            "                    f\"Error making request: {resp.status_code}: {resp.text}\",",
            "                    error_code=HTTPStatus(resp.status_code),",
            "                )",
            "            if endpoint.stream_output:",
            "                return self._parse_stream_response(endpoint, resp)",
            "            elif (",
            "                endpoint.output.get(\"type\") == \"file\"",
            "                and self.media_type == \"application/json\"",
            "            ):",
            "                return self._parse_file_response(endpoint, resp)",
            "            else:",
            "                return self._parse_response(endpoint, resp)",
            "        finally:",
            "            for f in self._opened_files:",
            "                f.close()",
            "            self._opened_files.clear()",
            "",
            "    def _parse_response(self, endpoint: ClientEndpoint, resp: httpx.Response) -> t.Any:",
            "        data = resp.read()",
            "        return self._deserialize_output(data, endpoint)",
            "",
            "    def _parse_stream_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> t.Generator[t.Any, None, None]:",
            "        try:",
            "            for data in resp.iter_bytes():",
            "                yield self._deserialize_output(data, endpoint)",
            "        finally:",
            "            resp.close()",
            "",
            "    def _parse_file_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> pathlib.Path:",
            "        from multipart.multipart import parse_options_header",
            "",
            "        content_disposition = resp.headers.get(\"content-disposition\")",
            "        filename: str | None = None",
            "        if content_disposition:",
            "            _, options = parse_options_header(content_disposition)",
            "            if b\"filename\" in options:",
            "                filename = str(",
            "                    options[b\"filename\"],",
            "                    resp.charset_encoding or \"utf-8\",",
            "                    errors=\"ignore\",",
            "                )",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            \"wb\", suffix=filename, dir=self._temp_dir.name, delete=False",
            "        ) as f:",
            "            f.write(resp.read())",
            "        return pathlib.Path(f.name)",
            "",
            "",
            "class AsyncHTTPClient(HTTPClient[httpx.AsyncClient]):",
            "    \"\"\"An asynchronous client for BentoML service.",
            "",
            "    .. note:: Inner usage ONLY",
            "    \"\"\"",
            "",
            "    client_cls = httpx.AsyncClient",
            "",
            "    async def is_ready(self, timeout: int | None = None) -> bool:",
            "        try:",
            "            resp = await self.client.get(",
            "                \"/readyz\", timeout=timeout or httpx.USE_CLIENT_DEFAULT",
            "            )",
            "            return resp.status_code == 200",
            "        except httpx.TimeoutException:",
            "            logger.warn(\"Timed out waiting for runner to be ready\")",
            "            return False",
            "",
            "    async def _get_stream(",
            "        self, endpoint: ClientEndpoint, args: t.Any, kwargs: t.Any",
            "    ) -> t.AsyncGenerator[t.Any, None]:",
            "        resp = await self._call(endpoint, args, kwargs)",
            "        assert inspect.isasyncgen(resp)",
            "        async for data in resp:",
            "            yield data",
            "",
            "    async def __aenter__(self: T) -> T:",
            "        return self",
            "",
            "    async def __aexit__(self, *args: t.Any) -> None:",
            "        return await self.close()",
            "",
            "    async def _call(",
            "        self,",
            "        endpoint: ClientEndpoint,",
            "        args: t.Sequence[t.Any],",
            "        kwargs: dict[str, t.Any],",
            "        *,",
            "        headers: t.Mapping[str, str] | None = None,",
            "    ) -> t.Any:",
            "        try:",
            "            req = self._build_request(endpoint, args, kwargs, headers or {})",
            "            resp = await self.client.send(req, stream=endpoint.stream_output)",
            "            if resp.is_error:",
            "                await resp.aread()",
            "                raise BentoMLException(",
            "                    f\"Error making request: {resp.status_code}: {resp.text}\",",
            "                    error_code=HTTPStatus(resp.status_code),",
            "                )",
            "            if endpoint.stream_output:",
            "                return self._parse_stream_response(endpoint, resp)",
            "            elif (",
            "                endpoint.output.get(\"type\") == \"file\"",
            "                and self.media_type == \"application/json\"",
            "            ):",
            "                return await self._parse_file_response(endpoint, resp)",
            "            else:",
            "                return await self._parse_response(endpoint, resp)",
            "        finally:",
            "            for f in self._opened_files:",
            "                f.close()",
            "            self._opened_files.clear()",
            "",
            "    async def _parse_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> t.Any:",
            "        data = await resp.aread()",
            "        return self._deserialize_output(data, endpoint)",
            "",
            "    async def _parse_stream_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> t.AsyncGenerator[t.Any, None]:",
            "        try:",
            "            async for data in resp.aiter_bytes():",
            "                yield self._deserialize_output(data, endpoint)",
            "        finally:",
            "            await resp.aclose()",
            "",
            "    async def _parse_file_response(",
            "        self, endpoint: ClientEndpoint, resp: httpx.Response",
            "    ) -> pathlib.Path:",
            "        from multipart.multipart import parse_options_header",
            "",
            "        content_disposition = resp.headers.get(\"content-disposition\")",
            "        filename: str | None = None",
            "        if content_disposition:",
            "            _, options = parse_options_header(content_disposition)",
            "            if b\"filename\" in options:",
            "                filename = str(",
            "                    options[b\"filename\"],",
            "                    resp.charset_encoding or \"utf-8\",",
            "                    errors=\"ignore\",",
            "                )",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            \"wb\", suffix=filename, dir=self._temp_dir.name, delete=False",
            "        ) as f:",
            "            f.write(await resp.aread())",
            "        return pathlib.Path(f.name)",
            "",
            "    async def close(self) -> None:",
            "        if \"client\" in vars(self):",
            "            await self.client.aclose()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "343": [
                "SyncHTTPClient"
            ],
            "344": [
                "SyncHTTPClient"
            ],
            "345": [
                "SyncHTTPClient"
            ],
            "346": [
                "SyncHTTPClient"
            ],
            "347": [
                "SyncHTTPClient"
            ],
            "348": [
                "SyncHTTPClient"
            ],
            "349": [
                "SyncHTTPClient"
            ],
            "350": [
                "SyncHTTPClient"
            ],
            "452": [
                "AsyncHTTPClient"
            ],
            "453": [
                "AsyncHTTPClient"
            ],
            "454": [
                "AsyncHTTPClient"
            ],
            "455": [
                "AsyncHTTPClient"
            ],
            "456": [
                "AsyncHTTPClient"
            ],
            "457": [
                "AsyncHTTPClient"
            ],
            "458": [
                "AsyncHTTPClient"
            ],
            "459": [
                "AsyncHTTPClient"
            ],
            "460": [
                "AsyncHTTPClient"
            ],
            "461": [
                "AsyncHTTPClient"
            ],
            "462": [
                "AsyncHTTPClient"
            ],
            "463": [
                "AsyncHTTPClient"
            ],
            "464": [
                "AsyncHTTPClient"
            ],
            "465": [
                "AsyncHTTPClient"
            ],
            "466": [
                "AsyncHTTPClient"
            ],
            "467": [
                "AsyncHTTPClient"
            ],
            "468": [
                "AsyncHTTPClient"
            ],
            "469": [
                "AsyncHTTPClient"
            ]
        },
        "addLocation": []
    },
    "src/_bentoml_impl/loader.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "     from _bentoml_sdk import Service"
            },
            "1": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 107,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "     if bento_path is None:"
            },
            "3": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        bento_path = pathlib.Path(\".\").absolute()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+        bento_path = pathlib.Path(\".\")"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+    bento_path = bento_path.absolute()"
            },
            "6": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 111,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "     # patch python path if needed"
            },
            "8": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if bento_path != pathlib.Path(\".\"):"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+    if bento_path != pathlib.Path(\".\").absolute():"
            },
            "10": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "         # a project"
            },
            "11": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        extra_python_path = str(bento_path.absolute())"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+        extra_python_path = str(bento_path)"
            },
            "13": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "         sys.path.insert(0, extra_python_path)"
            },
            "14": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "     else:"
            },
            "15": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "         # a project under current directory"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import importlib",
            "import pathlib",
            "import sys",
            "import typing as t",
            "",
            "import yaml",
            "",
            "if t.TYPE_CHECKING:",
            "    from _bentoml_sdk import Service",
            "",
            "BENTO_YAML_FILENAME = \"bento.yaml\"",
            "",
            "",
            "def normalize_identifier(",
            "    service_identifier: str,",
            "    working_dir: str | None = None,",
            ") -> tuple[str, pathlib.Path]:",
            "    \"\"\"",
            "    Normalize a service identifier to a package:Service format, and return the bento",
            "    path.",
            "",
            "    valid identifiers:",
            "    - package:Service                        # bentoml serve projects or normalized",
            "    - package:Service.dependency             # bentocloud dependencies services",
            "    - ~/bentoml/bentos/my_bento/version      # bentocloud entry service",
            "    - ~/bentoml/bentos/my_bento/version/bento.yaml",
            "    - bento1/bentofile.yaml                  # bentoml serve from multi-target projects",
            "    - my_service:a7ab819                     # bentoml serve built bentos",
            "    - package.py:Service",
            "    - package.py:Service.dependency",
            "    - .",
            "    \"\"\"",
            "    if working_dir is not None:",
            "        path = pathlib.Path(working_dir).joinpath(service_identifier)",
            "    else:",
            "        path = pathlib.Path(service_identifier)",
            "    if path.exists():",
            "        if path.is_file() and path.name == BENTO_YAML_FILENAME:",
            "            # this is a bento.yaml file",
            "            yaml_path = path",
            "            bento_path = path.parent.joinpath(\"src\")",
            "        elif path.is_dir() and path.joinpath(BENTO_YAML_FILENAME).is_file():",
            "            # this is a bento directory",
            "            yaml_path = path.joinpath(BENTO_YAML_FILENAME)",
            "            bento_path = path.joinpath(\"src\")",
            "        elif path.is_file() and path.name == \"bentofile.yaml\":",
            "            # this is a bentofile.yaml file",
            "            yaml_path = path",
            "            bento_path = (",
            "                pathlib.Path(working_dir) if working_dir is not None else path.parent",
            "            )",
            "        elif path.is_dir() and path.joinpath(\"bentofile.yaml\").is_file():",
            "            # this is a bento project directory",
            "            yaml_path = path.joinpath(\"bentofile.yaml\")",
            "            bento_path = (",
            "                pathlib.Path(working_dir) if working_dir is not None else path.parent",
            "            )",
            "        else:",
            "            raise ValueError(f\"found a path but not a bento: {service_identifier}\")",
            "",
            "        with open(yaml_path, \"r\") as f:",
            "            bento_yaml = yaml.safe_load(f)",
            "        assert \"service\" in bento_yaml, \"service field is required in bento.yaml\"",
            "        return normalize_package(bento_yaml[\"service\"]), bento_path",
            "",
            "    elif \":\" in service_identifier:",
            "        # a python import str or a built bento in store",
            "",
            "        # TODO(jiang): bento store configs are sdk configs, should be moved to sdk in the future",
            "        from bentoml._internal.configuration.containers import BentoMLContainer",
            "        from bentoml.exceptions import NotFound",
            "",
            "        bento_store = BentoMLContainer.bento_store.get()",
            "",
            "        try:",
            "            bento = bento_store.get(service_identifier)",
            "        except NotFound:",
            "            # a python import str",
            "            return normalize_package(service_identifier), pathlib.Path(",
            "                working_dir if working_dir is not None else \".\"",
            "            )",
            "        else:",
            "            # a built bento in bento store",
            "",
            "            yaml_path = pathlib.Path(bento.path).joinpath(BENTO_YAML_FILENAME)",
            "            with open(yaml_path, \"r\") as f:",
            "                bento_yaml = yaml.safe_load(f)",
            "            assert \"service\" in bento_yaml, \"service field is required in bento.yaml\"",
            "            return normalize_package(bento_yaml[\"service\"]), yaml_path.parent.joinpath(",
            "                \"src\"",
            "            )",
            "    else:",
            "        raise ValueError(f\"invalid service: {service_identifier}\")",
            "",
            "",
            "def import_service(",
            "    service_identifier: str,",
            "    bento_path: pathlib.Path | None = None,",
            ") -> Service[t.Any]:",
            "    \"\"\"",
            "    import a service from a service identifier, which should be normalized by",
            "    `normalize_identifier` function.",
            "    \"\"\"",
            "    from _bentoml_sdk import Service",
            "",
            "    if bento_path is None:",
            "        bento_path = pathlib.Path(\".\").absolute()",
            "",
            "    # patch python path if needed",
            "    if bento_path != pathlib.Path(\".\"):",
            "        # a project",
            "        extra_python_path = str(bento_path.absolute())",
            "        sys.path.insert(0, extra_python_path)",
            "    else:",
            "        # a project under current directory",
            "        extra_python_path = None",
            "",
            "    # patch model store if needed",
            "    if (",
            "        bento_path.parent.joinpath(BENTO_YAML_FILENAME).exists()",
            "        and bento_path.parent.joinpath(\"models\").exists()",
            "    ):",
            "        from bentoml._internal.configuration.containers import BentoMLContainer",
            "        from bentoml._internal.models import ModelStore",
            "",
            "        original_model_store = BentoMLContainer.model_store.get()",
            "",
            "        BentoMLContainer.model_store.set(",
            "            ModelStore((bento_path.parent.joinpath(\"models\").absolute()))",
            "        )",
            "    else:",
            "        original_model_store = None",
            "",
            "    try:",
            "        module_name, _, attrs_str = service_identifier.partition(\":\")",
            "",
            "        assert (",
            "            module_name and attrs_str",
            "        ), f'Invalid import target \"{service_identifier}\", must format as \"<module>:<attribute>\"'",
            "",
            "        module = importlib.import_module(module_name)",
            "        root_service_name, _, depend_path = attrs_str.partition(\".\")",
            "        root_service = getattr(module, root_service_name)",
            "",
            "        assert isinstance(",
            "            root_service, Service",
            "        ), f'import target \"{module_name}:{attrs_str}\" is not a bentoml.Service instance'",
            "",
            "        if not depend_path:",
            "            return root_service  # type: ignore",
            "        else:",
            "            return root_service.find_dependent(depend_path)",
            "",
            "    except (ImportError, AttributeError, KeyError, AssertionError) as e:",
            "        sys_path = sys.path.copy()",
            "        if extra_python_path is not None:",
            "            sys.path.remove(extra_python_path)",
            "",
            "        if original_model_store is not None:",
            "            from bentoml._internal.configuration.containers import BentoMLContainer",
            "",
            "            BentoMLContainer.model_store.set(original_model_store)",
            "        from bentoml.exceptions import ImportServiceError",
            "",
            "        raise ImportServiceError(",
            "            f'Failed to import service \"{service_identifier}\": {e}, sys.path: {sys_path}, cwd: {pathlib.Path.cwd()}'",
            "        ) from None",
            "",
            "",
            "def normalize_package(service_identifier: str) -> str:",
            "    \"\"\"",
            "    service.py:Service -> service:Service",
            "    \"\"\"",
            "    package, _, service_path = service_identifier.partition(\":\")",
            "    if package.endswith(\".py\"):",
            "        package = package[:-3]",
            "    return \":\".join([package, service_path])"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import importlib",
            "import pathlib",
            "import sys",
            "import typing as t",
            "",
            "import yaml",
            "",
            "if t.TYPE_CHECKING:",
            "    from _bentoml_sdk import Service",
            "",
            "BENTO_YAML_FILENAME = \"bento.yaml\"",
            "",
            "",
            "def normalize_identifier(",
            "    service_identifier: str,",
            "    working_dir: str | None = None,",
            ") -> tuple[str, pathlib.Path]:",
            "    \"\"\"",
            "    Normalize a service identifier to a package:Service format, and return the bento",
            "    path.",
            "",
            "    valid identifiers:",
            "    - package:Service                        # bentoml serve projects or normalized",
            "    - package:Service.dependency             # bentocloud dependencies services",
            "    - ~/bentoml/bentos/my_bento/version      # bentocloud entry service",
            "    - ~/bentoml/bentos/my_bento/version/bento.yaml",
            "    - bento1/bentofile.yaml                  # bentoml serve from multi-target projects",
            "    - my_service:a7ab819                     # bentoml serve built bentos",
            "    - package.py:Service",
            "    - package.py:Service.dependency",
            "    - .",
            "    \"\"\"",
            "    if working_dir is not None:",
            "        path = pathlib.Path(working_dir).joinpath(service_identifier)",
            "    else:",
            "        path = pathlib.Path(service_identifier)",
            "    if path.exists():",
            "        if path.is_file() and path.name == BENTO_YAML_FILENAME:",
            "            # this is a bento.yaml file",
            "            yaml_path = path",
            "            bento_path = path.parent.joinpath(\"src\")",
            "        elif path.is_dir() and path.joinpath(BENTO_YAML_FILENAME).is_file():",
            "            # this is a bento directory",
            "            yaml_path = path.joinpath(BENTO_YAML_FILENAME)",
            "            bento_path = path.joinpath(\"src\")",
            "        elif path.is_file() and path.name == \"bentofile.yaml\":",
            "            # this is a bentofile.yaml file",
            "            yaml_path = path",
            "            bento_path = (",
            "                pathlib.Path(working_dir) if working_dir is not None else path.parent",
            "            )",
            "        elif path.is_dir() and path.joinpath(\"bentofile.yaml\").is_file():",
            "            # this is a bento project directory",
            "            yaml_path = path.joinpath(\"bentofile.yaml\")",
            "            bento_path = (",
            "                pathlib.Path(working_dir) if working_dir is not None else path.parent",
            "            )",
            "        else:",
            "            raise ValueError(f\"found a path but not a bento: {service_identifier}\")",
            "",
            "        with open(yaml_path, \"r\") as f:",
            "            bento_yaml = yaml.safe_load(f)",
            "        assert \"service\" in bento_yaml, \"service field is required in bento.yaml\"",
            "        return normalize_package(bento_yaml[\"service\"]), bento_path",
            "",
            "    elif \":\" in service_identifier:",
            "        # a python import str or a built bento in store",
            "",
            "        # TODO(jiang): bento store configs are sdk configs, should be moved to sdk in the future",
            "        from bentoml._internal.configuration.containers import BentoMLContainer",
            "        from bentoml.exceptions import NotFound",
            "",
            "        bento_store = BentoMLContainer.bento_store.get()",
            "",
            "        try:",
            "            bento = bento_store.get(service_identifier)",
            "        except NotFound:",
            "            # a python import str",
            "            return normalize_package(service_identifier), pathlib.Path(",
            "                working_dir if working_dir is not None else \".\"",
            "            )",
            "        else:",
            "            # a built bento in bento store",
            "",
            "            yaml_path = pathlib.Path(bento.path).joinpath(BENTO_YAML_FILENAME)",
            "            with open(yaml_path, \"r\") as f:",
            "                bento_yaml = yaml.safe_load(f)",
            "            assert \"service\" in bento_yaml, \"service field is required in bento.yaml\"",
            "            return normalize_package(bento_yaml[\"service\"]), yaml_path.parent.joinpath(",
            "                \"src\"",
            "            )",
            "    else:",
            "        raise ValueError(f\"invalid service: {service_identifier}\")",
            "",
            "",
            "def import_service(",
            "    service_identifier: str,",
            "    bento_path: pathlib.Path | None = None,",
            ") -> Service[t.Any]:",
            "    \"\"\"",
            "    import a service from a service identifier, which should be normalized by",
            "    `normalize_identifier` function.",
            "    \"\"\"",
            "    from _bentoml_sdk import Service",
            "",
            "    if bento_path is None:",
            "        bento_path = pathlib.Path(\".\")",
            "    bento_path = bento_path.absolute()",
            "",
            "    # patch python path if needed",
            "    if bento_path != pathlib.Path(\".\").absolute():",
            "        # a project",
            "        extra_python_path = str(bento_path)",
            "        sys.path.insert(0, extra_python_path)",
            "    else:",
            "        # a project under current directory",
            "        extra_python_path = None",
            "",
            "    # patch model store if needed",
            "    if (",
            "        bento_path.parent.joinpath(BENTO_YAML_FILENAME).exists()",
            "        and bento_path.parent.joinpath(\"models\").exists()",
            "    ):",
            "        from bentoml._internal.configuration.containers import BentoMLContainer",
            "        from bentoml._internal.models import ModelStore",
            "",
            "        original_model_store = BentoMLContainer.model_store.get()",
            "",
            "        BentoMLContainer.model_store.set(",
            "            ModelStore((bento_path.parent.joinpath(\"models\").absolute()))",
            "        )",
            "    else:",
            "        original_model_store = None",
            "",
            "    try:",
            "        module_name, _, attrs_str = service_identifier.partition(\":\")",
            "",
            "        assert (",
            "            module_name and attrs_str",
            "        ), f'Invalid import target \"{service_identifier}\", must format as \"<module>:<attribute>\"'",
            "",
            "        module = importlib.import_module(module_name)",
            "        root_service_name, _, depend_path = attrs_str.partition(\".\")",
            "        root_service = getattr(module, root_service_name)",
            "",
            "        assert isinstance(",
            "            root_service, Service",
            "        ), f'import target \"{module_name}:{attrs_str}\" is not a bentoml.Service instance'",
            "",
            "        if not depend_path:",
            "            return root_service  # type: ignore",
            "        else:",
            "            return root_service.find_dependent(depend_path)",
            "",
            "    except (ImportError, AttributeError, KeyError, AssertionError) as e:",
            "        sys_path = sys.path.copy()",
            "        if extra_python_path is not None:",
            "            sys.path.remove(extra_python_path)",
            "",
            "        if original_model_store is not None:",
            "            from bentoml._internal.configuration.containers import BentoMLContainer",
            "",
            "            BentoMLContainer.model_store.set(original_model_store)",
            "        from bentoml.exceptions import ImportServiceError",
            "",
            "        raise ImportServiceError(",
            "            f'Failed to import service \"{service_identifier}\": {e}, sys.path: {sys_path}, cwd: {pathlib.Path.cwd()}'",
            "        ) from None",
            "",
            "",
            "def normalize_package(service_identifier: str) -> str:",
            "    \"\"\"",
            "    service.py:Service -> service:Service",
            "    \"\"\"",
            "    package, _, service_path = service_identifier.partition(\":\")",
            "    if package.endswith(\".py\"):",
            "        package = package[:-3]",
            "    return \":\".join([package, service_path])"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "109": [
                "import_service"
            ],
            "112": [
                "import_service"
            ],
            "114": [
                "import_service"
            ]
        },
        "addLocation": []
    },
    "src/_bentoml_impl/server/app.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import inspect"
            },
            "1": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " import sys"
            },
            "2": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " import typing as t"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 8,
                "PatchRowcode": "+from http import HTTPStatus"
            },
            "4": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " from pathlib import Path"
            },
            "5": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import anyio"
            },
            "7": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "     def __init__("
            },
            "8": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "         self,"
            },
            "9": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "         service: Service[t.Any],"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+        is_main: bool = False,"
            },
            "11": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "         enable_metrics: bool = Provide["
            },
            "12": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "             BentoMLContainer.api_server_config.metrics.enabled"
            },
            "13": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "         ],"
            },
            "14": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 76,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "         self.service = service"
            },
            "16": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "         self.enable_metrics = enable_metrics"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+        self.is_main = is_main"
            },
            "18": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "         timeout = traffic.get(\"timeout\")"
            },
            "19": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "         max_concurrency = traffic.get(\"max_concurrency\")"
            },
            "20": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 82,
                "PatchRowcode": "         self.enable_access_control = enable_access_control"
            },
            "21": {
                "beforePatchRowNumber": 175,
                "afterPatchRowNumber": 178,
                "PatchRowcode": "         else:"
            },
            "22": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 179,
                "PatchRowcode": "             return JSONResponse(\"\", status_code=status)"
            },
            "23": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": 180,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def __call__(self, is_main: bool = False) -> Starlette:"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+    def __call__(self) -> Starlette:"
            },
            "26": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "         app = super().__call__()"
            },
            "27": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 183,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 184,
                "PatchRowcode": "         app.add_exception_handler("
            },
            "29": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "         app.add_exception_handler(BentoMLException, self.handle_bentoml_exception)"
            },
            "30": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "         app.add_exception_handler(Exception, self.handle_uncaught_exception)"
            },
            "31": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 189,
                "PatchRowcode": "         app.add_route(\"/schema.json\", self.schema_view, name=\"schema\")"
            },
            "32": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if is_main:"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+        if self.is_main:"
            },
            "34": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "             if BentoMLContainer.new_index:"
            },
            "35": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 192,
                "PatchRowcode": "                 assets = Path(__file__).parent / \"assets\""
            },
            "36": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 193,
                "PatchRowcode": "                 app.mount(\"/assets\", StaticFiles(directory=assets), name=\"assets\")"
            },
            "37": {
                "beforePatchRowNumber": 407,
                "afterPatchRowNumber": 410,
                "PatchRowcode": " "
            },
            "38": {
                "beforePatchRowNumber": 408,
                "afterPatchRowNumber": 411,
                "PatchRowcode": "         media_type = request.headers.get(\"Content-Type\", \"application/json\")"
            },
            "39": {
                "beforePatchRowNumber": 409,
                "afterPatchRowNumber": 412,
                "PatchRowcode": "         media_type = media_type.split(\";\")[0].strip()"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 413,
                "PatchRowcode": "+        if self.is_main and media_type == \"application/vnd.bentoml+pickle\":"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 414,
                "PatchRowcode": "+            # Disallow pickle media type for main service for security reasons"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 415,
                "PatchRowcode": "+            raise BentoMLException("
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 416,
                "PatchRowcode": "+                \"Pickle media type is not allowed for main service\","
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 417,
                "PatchRowcode": "+                error_code=HTTPStatus.UNSUPPORTED_MEDIA_TYPE,"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 418,
                "PatchRowcode": "+            )"
            },
            "46": {
                "beforePatchRowNumber": 410,
                "afterPatchRowNumber": 419,
                "PatchRowcode": " "
            },
            "47": {
                "beforePatchRowNumber": 411,
                "afterPatchRowNumber": 420,
                "PatchRowcode": "         method = self.service.apis[name]"
            },
            "48": {
                "beforePatchRowNumber": 412,
                "afterPatchRowNumber": 421,
                "PatchRowcode": "         func = getattr(self._service_instance, name)"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import functools",
            "import inspect",
            "import sys",
            "import typing as t",
            "from pathlib import Path",
            "",
            "import anyio",
            "import anyio.to_thread",
            "import pydantic",
            "from simple_di import Provide",
            "from simple_di import inject",
            "from starlette.middleware import Middleware",
            "from starlette.staticfiles import StaticFiles",
            "",
            "from _bentoml_sdk import Service",
            "from bentoml._internal.container import BentoMLContainer",
            "from bentoml._internal.marshal.dispatcher import CorkDispatcher",
            "from bentoml._internal.server.base_app import BaseAppFactory",
            "from bentoml._internal.server.http_app import log_exception",
            "from bentoml._internal.utils.metrics import exponential_buckets",
            "from bentoml.exceptions import BentoMLException",
            "from bentoml.exceptions import ServiceUnavailable",
            "",
            "if t.TYPE_CHECKING:",
            "    from opentelemetry.sdk.trace import Span",
            "    from starlette.applications import Starlette",
            "    from starlette.requests import Request",
            "    from starlette.responses import Response",
            "    from starlette.routing import BaseRoute",
            "",
            "    from bentoml._internal import external_typing as ext",
            "    from bentoml._internal.context import ServiceContext",
            "    from bentoml._internal.types import LifecycleHook",
            "",
            "R = t.TypeVar(\"R\")",
            "",
            "",
            "class ContextMiddleware:",
            "    def __init__(self, app: ext.ASGIApp, context: ServiceContext) -> None:",
            "        self.app = app",
            "        self.context = context",
            "",
            "    async def __call__(",
            "        self, scope: ext.ASGIScope, receive: ext.ASGIReceive, send: ext.ASGISend",
            "    ) -> None:",
            "        from starlette.requests import Request",
            "",
            "        if scope[\"type\"] not in (\"http\", \"websocket\"):",
            "            return await self.app(scope, receive, send)",
            "",
            "        req = Request(scope, receive, send)",
            "        with self.context.in_request(req):",
            "            await self.app(scope, receive, send)",
            "",
            "",
            "class ServiceAppFactory(BaseAppFactory):",
            "    @inject",
            "    def __init__(",
            "        self,",
            "        service: Service[t.Any],",
            "        enable_metrics: bool = Provide[",
            "            BentoMLContainer.api_server_config.metrics.enabled",
            "        ],",
            "        traffic: dict[str, t.Any] = Provide[BentoMLContainer.api_server_config.traffic],",
            "        enable_access_control: bool = Provide[BentoMLContainer.http.cors.enabled],",
            "        access_control_options: dict[str, list[str] | str | int] = Provide[",
            "            BentoMLContainer.access_control_options",
            "        ],",
            "    ) -> None:",
            "        from bentoml._internal.runner.container import AutoContainer",
            "",
            "        self.service = service",
            "        self.enable_metrics = enable_metrics",
            "        timeout = traffic.get(\"timeout\")",
            "        max_concurrency = traffic.get(\"max_concurrency\")",
            "        self.enable_access_control = enable_access_control",
            "        self.access_control_options = access_control_options",
            "        super().__init__(timeout=timeout, max_concurrency=max_concurrency)",
            "",
            "        self.dispatchers: dict[str, CorkDispatcher[t.Any, t.Any]] = {}",
            "        self._service_instance: t.Any | None = None",
            "        self._limiter: anyio.CapacityLimiter | None = None",
            "",
            "        def fallback() -> t.NoReturn:",
            "            raise ServiceUnavailable(\"process is overloaded\")",
            "",
            "        for name, method in service.apis.items():",
            "            if not method.batchable:",
            "                continue",
            "            self.dispatchers[name] = CorkDispatcher(",
            "                max_latency_in_ms=method.max_latency_ms,",
            "                max_batch_size=method.max_batch_size,",
            "                fallback=fallback,",
            "                get_batch_size=functools.partial(",
            "                    AutoContainer.get_batch_size, batch_dim=method.batch_dim[0]",
            "                ),",
            "            )",
            "",
            "        metrics_client = BentoMLContainer.metrics_client.get()",
            "        max_max_batch_size = max(",
            "            (",
            "                method.max_batch_size",
            "                for method in service.apis.values()",
            "                if method.batchable",
            "            ),",
            "            default=100,",
            "        )",
            "",
            "        self.adaptive_batch_size_hist = metrics_client.Histogram(",
            "            namespace=\"bentoml_service\",",
            "            name=\"adaptive_batch_size\",",
            "            documentation=\"Service adaptive batch size\",",
            "            labelnames=[",
            "                \"runner_name\",",
            "                \"worker_index\",",
            "                \"method_name\",",
            "                \"service_version\",",
            "                \"service_name\",",
            "            ],",
            "            buckets=exponential_buckets(1, 2, max_max_batch_size),",
            "        )",
            "",
            "    async def index_page(self, _: Request) -> Response:",
            "        from starlette.responses import FileResponse",
            "",
            "        if BentoMLContainer.new_index:",
            "            filename = \"main-ui.html\"",
            "        else:",
            "            filename = \"main-openapi.html\"",
            "        return FileResponse(Path(__file__).parent / filename)",
            "",
            "    async def openapi_spec_view(self, req: Request) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        try:",
            "            return JSONResponse(self.service.openapi_spec.asdict())",
            "        except Exception:",
            "            log_exception(req, sys.exc_info())",
            "            raise",
            "",
            "    async def handle_uncaught_exception(self, req: Request, exc: Exception) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        log_exception(req, sys.exc_info())",
            "        return JSONResponse(",
            "            {\"error\": \"An unexpected error has occurred, please check the server log.\"},",
            "            status_code=500,",
            "        )",
            "",
            "    async def handle_validation_error(self, req: Request, exc: Exception) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        assert isinstance(exc, pydantic.ValidationError)",
            "",
            "        data = {",
            "            \"error\": f\"{exc.error_count()} validation error for {exc.title}\",",
            "            \"detail\": exc.errors(),",
            "        }",
            "        return JSONResponse(data, status_code=400)",
            "",
            "    async def handle_bentoml_exception(self, req: Request, exc: Exception) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        log_exception(req, sys.exc_info())",
            "        assert isinstance(exc, BentoMLException)",
            "        status = exc.error_code.value",
            "        if 400 <= status < 500 and status not in (401, 403):",
            "            return JSONResponse(",
            "                {\"error\": f\"BentoService error handling API request: {exc}\"},",
            "                status_code=status,",
            "            )",
            "        else:",
            "            return JSONResponse(\"\", status_code=status)",
            "",
            "    def __call__(self, is_main: bool = False) -> Starlette:",
            "        app = super().__call__()",
            "",
            "        app.add_exception_handler(",
            "            pydantic.ValidationError, self.handle_validation_error",
            "        )",
            "        app.add_exception_handler(BentoMLException, self.handle_bentoml_exception)",
            "        app.add_exception_handler(Exception, self.handle_uncaught_exception)",
            "        app.add_route(\"/schema.json\", self.schema_view, name=\"schema\")",
            "        if is_main:",
            "            if BentoMLContainer.new_index:",
            "                assets = Path(__file__).parent / \"assets\"",
            "                app.mount(\"/assets\", StaticFiles(directory=assets), name=\"assets\")",
            "            else:",
            "                from bentoml._internal import server",
            "",
            "                assets = Path(server.__file__).parent / \"static_content\"",
            "                app.mount(",
            "                    \"/static_content\",",
            "                    StaticFiles(directory=assets),",
            "                    name=\"static_content\",",
            "                )",
            "                app.add_route(\"/docs.json\", self.openapi_spec_view, name=\"openapi-spec\")",
            "            app.add_route(\"/\", self.index_page, name=\"index\")",
            "        for mount_app, path, name in self.service.mount_apps:",
            "            app.mount(app=mount_app, path=path, name=name)",
            "        return app",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        return self.service.name",
            "",
            "    @property",
            "    def middlewares(self) -> list[Middleware]:",
            "        from opentelemetry.instrumentation.asgi import OpenTelemetryMiddleware",
            "",
            "        from bentoml._internal.container import BentoMLContainer",
            "",
            "        middlewares = super().middlewares + [",
            "            Middleware(ContextMiddleware, context=self.service.context)",
            "        ]",
            "",
            "        for middleware_cls, options in self.service.middlewares:",
            "            middlewares.append(Middleware(middleware_cls, **options))",
            "",
            "        if self.enable_access_control:",
            "            assert (",
            "                self.access_control_options.get(\"allow_origins\") is not None",
            "            ), \"To enable cors, access_control_allow_origin must be set\"",
            "",
            "            from starlette.middleware.cors import CORSMiddleware",
            "",
            "            middlewares.append(",
            "                Middleware(CORSMiddleware, **self.access_control_options)",
            "            )",
            "",
            "        def client_request_hook(span: Span | None, _scope: dict[str, t.Any]) -> None:",
            "            from bentoml._internal.context import trace_context",
            "",
            "            if span is not None:",
            "                trace_context.request_id = span.context.span_id",
            "",
            "        middlewares.append(",
            "            Middleware(",
            "                OpenTelemetryMiddleware,",
            "                excluded_urls=BentoMLContainer.tracing_excluded_urls.get(),",
            "                default_span_details=None,",
            "                server_request_hook=None,",
            "                client_request_hook=client_request_hook,",
            "                tracer_provider=BentoMLContainer.tracer_provider.get(),",
            "            )",
            "        )",
            "",
            "        if self.enable_metrics:",
            "            from bentoml._internal.server.http.instruments import (",
            "                RunnerTrafficMetricsMiddleware,",
            "            )",
            "",
            "            middlewares.append(",
            "                Middleware(RunnerTrafficMetricsMiddleware, namespace=\"bentoml_service\")",
            "            )",
            "",
            "        access_log_config = BentoMLContainer.api_server_config.logging.access",
            "        if access_log_config.enabled.get():",
            "            from bentoml._internal.server.http.access import AccessLogMiddleware",
            "",
            "            middlewares.append(",
            "                Middleware(",
            "                    AccessLogMiddleware,",
            "                    has_request_content_length=access_log_config.request_content_length.get(),",
            "                    has_request_content_type=access_log_config.request_content_type.get(),",
            "                    has_response_content_length=access_log_config.response_content_length.get(),",
            "                    has_response_content_type=access_log_config.response_content_type.get(),",
            "                    skip_paths=access_log_config.skip_paths.get(),",
            "                )",
            "            )",
            "",
            "        return middlewares",
            "",
            "    def create_instance(self) -> None:",
            "        self._service_instance = self.service()",
            "",
            "    async def destroy_instance(self) -> None:",
            "        from _bentoml_sdk.service.dependency import cleanup",
            "",
            "        await cleanup()",
            "        self._service_instance = None",
            "",
            "    async def readyz(self, _: Request) -> Response:",
            "        from starlette.exceptions import HTTPException",
            "        from starlette.responses import PlainTextResponse",
            "",
            "        from ..client import RemoteProxy",
            "",
            "        if BentoMLContainer.api_server_config.runner_probe.enabled.get():",
            "            dependency_statuses: list[t.Coroutine[None, None, bool]] = []",
            "            for dependency in self.service.dependencies.values():",
            "                real = dependency.get()",
            "                if isinstance(real, RemoteProxy):",
            "                    dependency_statuses.append(real.is_ready())",
            "            runners_ready = all(await asyncio.gather(*dependency_statuses))",
            "",
            "            if not runners_ready:",
            "                raise HTTPException(status_code=503, detail=\"Runners are not ready.\")",
            "",
            "        return PlainTextResponse(\"\\n\", status_code=200)",
            "",
            "    @property",
            "    def on_startup(self) -> list[LifecycleHook]:",
            "        return [*super().on_startup, self.create_instance, *self.service.startup_hooks]",
            "",
            "    @property",
            "    def on_shutdown(self) -> list[LifecycleHook]:",
            "        return [",
            "            *super().on_shutdown,",
            "            self.destroy_instance,",
            "            *self.service.shutdown_hooks,",
            "        ]",
            "",
            "    async def schema_view(self, request: Request) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        schema = self.service.schema()",
            "        return JSONResponse(schema)",
            "",
            "    @property",
            "    def routes(self) -> list[BaseRoute]:",
            "        from starlette.routing import Route",
            "",
            "        routes = super().routes",
            "",
            "        for name, method in self.service.apis.items():",
            "            api_endpoint = functools.partial(self.api_endpoint, name)",
            "            route_path = method.route",
            "            if not route_path.startswith(\"/\"):",
            "                route_path = \"/\" + route_path",
            "            routes.append(Route(route_path, api_endpoint, methods=[\"POST\"], name=name))",
            "        return routes",
            "",
            "    async def _to_thread(",
            "        self,",
            "        func: t.Callable[..., R],",
            "        *args: t.Any,",
            "        **kwargs: t.Any,",
            "    ) -> R:",
            "        if self._limiter is None:",
            "            threads = self.service.config.get(\"threads\", 1)",
            "            self._limiter = anyio.CapacityLimiter(threads)",
            "        func = functools.partial(func, *args, **kwargs)",
            "        output = await anyio.to_thread.run_sync(func, limiter=self._limiter)",
            "        return output",
            "",
            "    async def batch_infer(",
            "        self, name: str, input_args: tuple[t.Any, ...], input_kwargs: dict[str, t.Any]",
            "    ) -> t.Any:",
            "        method = self.service.apis[name]",
            "        func = getattr(self._service_instance, name)",
            "",
            "        async def inner_infer(",
            "            batches: t.Sequence[t.Any], **kwargs: t.Any",
            "        ) -> t.Sequence[t.Any]:",
            "            from bentoml._internal.context import component_context",
            "            from bentoml._internal.runner.container import AutoContainer",
            "            from bentoml._internal.utils import is_async_callable",
            "",
            "            self.adaptive_batch_size_hist.labels(  # type: ignore",
            "                runner_name=self.service.name,",
            "                worker_index=component_context.component_index,",
            "                method_name=name,",
            "                service_version=component_context.bento_version,",
            "                service_name=component_context.bento_name,",
            "            ).observe(len(batches))",
            "",
            "            if len(batches) == 0:",
            "                return []",
            "",
            "            batch, indices = AutoContainer.batches_to_batch(",
            "                batches, method.batch_dim[0]",
            "            )",
            "            if is_async_callable(func):",
            "                result = await func(batch, **kwargs)",
            "            else:",
            "                result = await self._to_thread(func, batch, **kwargs)",
            "            return AutoContainer.batch_to_batches(result, indices, method.batch_dim[1])",
            "",
            "        arg_names = [k for k in input_kwargs if k not in (\"ctx\", \"context\")]",
            "        if input_args:",
            "            if len(input_args) > 1 or len(arg_names) > 0:",
            "                raise TypeError(\"Batch inference function only accept one argument\")",
            "            value = input_args[0]",
            "        else:",
            "            if len(arg_names) != 1:",
            "                raise TypeError(\"Batch inference function only accept one argument\")",
            "            value = input_kwargs.pop(arg_names[0])",
            "        return await self.dispatchers[name](",
            "            functools.partial(inner_infer, **input_kwargs)",
            "        )(value)",
            "",
            "    async def api_endpoint(self, name: str, request: Request) -> Response:",
            "        from starlette.background import BackgroundTask",
            "",
            "        from _bentoml_sdk.io_models import ARGS",
            "        from _bentoml_sdk.io_models import KWARGS",
            "        from bentoml._internal.container import BentoMLContainer",
            "        from bentoml._internal.context import trace_context",
            "        from bentoml._internal.utils import get_original_func",
            "        from bentoml._internal.utils.http import set_cookies",
            "",
            "        from ..serde import ALL_SERDE",
            "",
            "        media_type = request.headers.get(\"Content-Type\", \"application/json\")",
            "        media_type = media_type.split(\";\")[0].strip()",
            "",
            "        method = self.service.apis[name]",
            "        func = getattr(self._service_instance, name)",
            "        ctx = self.service.context",
            "        serde = ALL_SERDE[media_type]()",
            "        input_data = await method.input_spec.from_http_request(request, serde)",
            "        input_args: tuple[t.Any, ...] = ()",
            "        input_params = {k: getattr(input_data, k) for k in input_data.model_fields}",
            "        if method.ctx_param is not None:",
            "            input_params[method.ctx_param] = ctx",
            "        if ARGS in input_params:",
            "            input_args = tuple(input_params.pop(ARGS))",
            "        if KWARGS in input_params:",
            "            input_params.update(input_params.pop(KWARGS))",
            "",
            "        original_func = get_original_func(func)",
            "",
            "        if method.batchable:",
            "            output = await self.batch_infer(name, input_args, input_params)",
            "        elif inspect.iscoroutinefunction(original_func):",
            "            output = await func(*input_args, **input_params)",
            "        elif inspect.isasyncgenfunction(original_func):",
            "            output = func(*input_args, **input_params)",
            "        elif inspect.isgeneratorfunction(original_func):",
            "",
            "            async def inner() -> t.AsyncGenerator[t.Any, None]:",
            "                gen = func(*input_args, **input_params)",
            "                while True:",
            "                    try:",
            "                        yield await self._to_thread(next, gen)",
            "                    except StopIteration:",
            "                        break",
            "                    except RuntimeError as e:",
            "                        if \"StopIteration\" in str(e):",
            "                            break",
            "                        raise",
            "",
            "            output = inner()",
            "        else:",
            "            output = await self._to_thread(func, *input_args, **input_params)",
            "",
            "        response = await method.output_spec.to_http_response(output, serde)",
            "        response.headers.update({\"Server\": f\"BentoML Service/{self.service.name}\"})",
            "",
            "        if method.ctx_param is not None:",
            "            response.status_code = ctx.response.status_code",
            "            response.headers.update(ctx.response.metadata)",
            "            set_cookies(response, ctx.response.cookies)",
            "        if trace_context.request_id is not None:",
            "            response.headers[\"X-BentoML-Request-ID\"] = str(trace_context.request_id)",
            "        if (",
            "            BentoMLContainer.http.response.trace_id.get()",
            "            and trace_context.trace_id is not None",
            "        ):",
            "            response.headers[\"X-BentoML-Trace-ID\"] = str(trace_context.trace_id)",
            "        # clean the request resources after the response is consumed.",
            "        response.background = BackgroundTask(request.close)",
            "        return response"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import functools",
            "import inspect",
            "import sys",
            "import typing as t",
            "from http import HTTPStatus",
            "from pathlib import Path",
            "",
            "import anyio",
            "import anyio.to_thread",
            "import pydantic",
            "from simple_di import Provide",
            "from simple_di import inject",
            "from starlette.middleware import Middleware",
            "from starlette.staticfiles import StaticFiles",
            "",
            "from _bentoml_sdk import Service",
            "from bentoml._internal.container import BentoMLContainer",
            "from bentoml._internal.marshal.dispatcher import CorkDispatcher",
            "from bentoml._internal.server.base_app import BaseAppFactory",
            "from bentoml._internal.server.http_app import log_exception",
            "from bentoml._internal.utils.metrics import exponential_buckets",
            "from bentoml.exceptions import BentoMLException",
            "from bentoml.exceptions import ServiceUnavailable",
            "",
            "if t.TYPE_CHECKING:",
            "    from opentelemetry.sdk.trace import Span",
            "    from starlette.applications import Starlette",
            "    from starlette.requests import Request",
            "    from starlette.responses import Response",
            "    from starlette.routing import BaseRoute",
            "",
            "    from bentoml._internal import external_typing as ext",
            "    from bentoml._internal.context import ServiceContext",
            "    from bentoml._internal.types import LifecycleHook",
            "",
            "R = t.TypeVar(\"R\")",
            "",
            "",
            "class ContextMiddleware:",
            "    def __init__(self, app: ext.ASGIApp, context: ServiceContext) -> None:",
            "        self.app = app",
            "        self.context = context",
            "",
            "    async def __call__(",
            "        self, scope: ext.ASGIScope, receive: ext.ASGIReceive, send: ext.ASGISend",
            "    ) -> None:",
            "        from starlette.requests import Request",
            "",
            "        if scope[\"type\"] not in (\"http\", \"websocket\"):",
            "            return await self.app(scope, receive, send)",
            "",
            "        req = Request(scope, receive, send)",
            "        with self.context.in_request(req):",
            "            await self.app(scope, receive, send)",
            "",
            "",
            "class ServiceAppFactory(BaseAppFactory):",
            "    @inject",
            "    def __init__(",
            "        self,",
            "        service: Service[t.Any],",
            "        is_main: bool = False,",
            "        enable_metrics: bool = Provide[",
            "            BentoMLContainer.api_server_config.metrics.enabled",
            "        ],",
            "        traffic: dict[str, t.Any] = Provide[BentoMLContainer.api_server_config.traffic],",
            "        enable_access_control: bool = Provide[BentoMLContainer.http.cors.enabled],",
            "        access_control_options: dict[str, list[str] | str | int] = Provide[",
            "            BentoMLContainer.access_control_options",
            "        ],",
            "    ) -> None:",
            "        from bentoml._internal.runner.container import AutoContainer",
            "",
            "        self.service = service",
            "        self.enable_metrics = enable_metrics",
            "        self.is_main = is_main",
            "        timeout = traffic.get(\"timeout\")",
            "        max_concurrency = traffic.get(\"max_concurrency\")",
            "        self.enable_access_control = enable_access_control",
            "        self.access_control_options = access_control_options",
            "        super().__init__(timeout=timeout, max_concurrency=max_concurrency)",
            "",
            "        self.dispatchers: dict[str, CorkDispatcher[t.Any, t.Any]] = {}",
            "        self._service_instance: t.Any | None = None",
            "        self._limiter: anyio.CapacityLimiter | None = None",
            "",
            "        def fallback() -> t.NoReturn:",
            "            raise ServiceUnavailable(\"process is overloaded\")",
            "",
            "        for name, method in service.apis.items():",
            "            if not method.batchable:",
            "                continue",
            "            self.dispatchers[name] = CorkDispatcher(",
            "                max_latency_in_ms=method.max_latency_ms,",
            "                max_batch_size=method.max_batch_size,",
            "                fallback=fallback,",
            "                get_batch_size=functools.partial(",
            "                    AutoContainer.get_batch_size, batch_dim=method.batch_dim[0]",
            "                ),",
            "            )",
            "",
            "        metrics_client = BentoMLContainer.metrics_client.get()",
            "        max_max_batch_size = max(",
            "            (",
            "                method.max_batch_size",
            "                for method in service.apis.values()",
            "                if method.batchable",
            "            ),",
            "            default=100,",
            "        )",
            "",
            "        self.adaptive_batch_size_hist = metrics_client.Histogram(",
            "            namespace=\"bentoml_service\",",
            "            name=\"adaptive_batch_size\",",
            "            documentation=\"Service adaptive batch size\",",
            "            labelnames=[",
            "                \"runner_name\",",
            "                \"worker_index\",",
            "                \"method_name\",",
            "                \"service_version\",",
            "                \"service_name\",",
            "            ],",
            "            buckets=exponential_buckets(1, 2, max_max_batch_size),",
            "        )",
            "",
            "    async def index_page(self, _: Request) -> Response:",
            "        from starlette.responses import FileResponse",
            "",
            "        if BentoMLContainer.new_index:",
            "            filename = \"main-ui.html\"",
            "        else:",
            "            filename = \"main-openapi.html\"",
            "        return FileResponse(Path(__file__).parent / filename)",
            "",
            "    async def openapi_spec_view(self, req: Request) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        try:",
            "            return JSONResponse(self.service.openapi_spec.asdict())",
            "        except Exception:",
            "            log_exception(req, sys.exc_info())",
            "            raise",
            "",
            "    async def handle_uncaught_exception(self, req: Request, exc: Exception) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        log_exception(req, sys.exc_info())",
            "        return JSONResponse(",
            "            {\"error\": \"An unexpected error has occurred, please check the server log.\"},",
            "            status_code=500,",
            "        )",
            "",
            "    async def handle_validation_error(self, req: Request, exc: Exception) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        assert isinstance(exc, pydantic.ValidationError)",
            "",
            "        data = {",
            "            \"error\": f\"{exc.error_count()} validation error for {exc.title}\",",
            "            \"detail\": exc.errors(),",
            "        }",
            "        return JSONResponse(data, status_code=400)",
            "",
            "    async def handle_bentoml_exception(self, req: Request, exc: Exception) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        log_exception(req, sys.exc_info())",
            "        assert isinstance(exc, BentoMLException)",
            "        status = exc.error_code.value",
            "        if 400 <= status < 500 and status not in (401, 403):",
            "            return JSONResponse(",
            "                {\"error\": f\"BentoService error handling API request: {exc}\"},",
            "                status_code=status,",
            "            )",
            "        else:",
            "            return JSONResponse(\"\", status_code=status)",
            "",
            "    def __call__(self) -> Starlette:",
            "        app = super().__call__()",
            "",
            "        app.add_exception_handler(",
            "            pydantic.ValidationError, self.handle_validation_error",
            "        )",
            "        app.add_exception_handler(BentoMLException, self.handle_bentoml_exception)",
            "        app.add_exception_handler(Exception, self.handle_uncaught_exception)",
            "        app.add_route(\"/schema.json\", self.schema_view, name=\"schema\")",
            "        if self.is_main:",
            "            if BentoMLContainer.new_index:",
            "                assets = Path(__file__).parent / \"assets\"",
            "                app.mount(\"/assets\", StaticFiles(directory=assets), name=\"assets\")",
            "            else:",
            "                from bentoml._internal import server",
            "",
            "                assets = Path(server.__file__).parent / \"static_content\"",
            "                app.mount(",
            "                    \"/static_content\",",
            "                    StaticFiles(directory=assets),",
            "                    name=\"static_content\",",
            "                )",
            "                app.add_route(\"/docs.json\", self.openapi_spec_view, name=\"openapi-spec\")",
            "            app.add_route(\"/\", self.index_page, name=\"index\")",
            "        for mount_app, path, name in self.service.mount_apps:",
            "            app.mount(app=mount_app, path=path, name=name)",
            "        return app",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        return self.service.name",
            "",
            "    @property",
            "    def middlewares(self) -> list[Middleware]:",
            "        from opentelemetry.instrumentation.asgi import OpenTelemetryMiddleware",
            "",
            "        from bentoml._internal.container import BentoMLContainer",
            "",
            "        middlewares = super().middlewares + [",
            "            Middleware(ContextMiddleware, context=self.service.context)",
            "        ]",
            "",
            "        for middleware_cls, options in self.service.middlewares:",
            "            middlewares.append(Middleware(middleware_cls, **options))",
            "",
            "        if self.enable_access_control:",
            "            assert (",
            "                self.access_control_options.get(\"allow_origins\") is not None",
            "            ), \"To enable cors, access_control_allow_origin must be set\"",
            "",
            "            from starlette.middleware.cors import CORSMiddleware",
            "",
            "            middlewares.append(",
            "                Middleware(CORSMiddleware, **self.access_control_options)",
            "            )",
            "",
            "        def client_request_hook(span: Span | None, _scope: dict[str, t.Any]) -> None:",
            "            from bentoml._internal.context import trace_context",
            "",
            "            if span is not None:",
            "                trace_context.request_id = span.context.span_id",
            "",
            "        middlewares.append(",
            "            Middleware(",
            "                OpenTelemetryMiddleware,",
            "                excluded_urls=BentoMLContainer.tracing_excluded_urls.get(),",
            "                default_span_details=None,",
            "                server_request_hook=None,",
            "                client_request_hook=client_request_hook,",
            "                tracer_provider=BentoMLContainer.tracer_provider.get(),",
            "            )",
            "        )",
            "",
            "        if self.enable_metrics:",
            "            from bentoml._internal.server.http.instruments import (",
            "                RunnerTrafficMetricsMiddleware,",
            "            )",
            "",
            "            middlewares.append(",
            "                Middleware(RunnerTrafficMetricsMiddleware, namespace=\"bentoml_service\")",
            "            )",
            "",
            "        access_log_config = BentoMLContainer.api_server_config.logging.access",
            "        if access_log_config.enabled.get():",
            "            from bentoml._internal.server.http.access import AccessLogMiddleware",
            "",
            "            middlewares.append(",
            "                Middleware(",
            "                    AccessLogMiddleware,",
            "                    has_request_content_length=access_log_config.request_content_length.get(),",
            "                    has_request_content_type=access_log_config.request_content_type.get(),",
            "                    has_response_content_length=access_log_config.response_content_length.get(),",
            "                    has_response_content_type=access_log_config.response_content_type.get(),",
            "                    skip_paths=access_log_config.skip_paths.get(),",
            "                )",
            "            )",
            "",
            "        return middlewares",
            "",
            "    def create_instance(self) -> None:",
            "        self._service_instance = self.service()",
            "",
            "    async def destroy_instance(self) -> None:",
            "        from _bentoml_sdk.service.dependency import cleanup",
            "",
            "        await cleanup()",
            "        self._service_instance = None",
            "",
            "    async def readyz(self, _: Request) -> Response:",
            "        from starlette.exceptions import HTTPException",
            "        from starlette.responses import PlainTextResponse",
            "",
            "        from ..client import RemoteProxy",
            "",
            "        if BentoMLContainer.api_server_config.runner_probe.enabled.get():",
            "            dependency_statuses: list[t.Coroutine[None, None, bool]] = []",
            "            for dependency in self.service.dependencies.values():",
            "                real = dependency.get()",
            "                if isinstance(real, RemoteProxy):",
            "                    dependency_statuses.append(real.is_ready())",
            "            runners_ready = all(await asyncio.gather(*dependency_statuses))",
            "",
            "            if not runners_ready:",
            "                raise HTTPException(status_code=503, detail=\"Runners are not ready.\")",
            "",
            "        return PlainTextResponse(\"\\n\", status_code=200)",
            "",
            "    @property",
            "    def on_startup(self) -> list[LifecycleHook]:",
            "        return [*super().on_startup, self.create_instance, *self.service.startup_hooks]",
            "",
            "    @property",
            "    def on_shutdown(self) -> list[LifecycleHook]:",
            "        return [",
            "            *super().on_shutdown,",
            "            self.destroy_instance,",
            "            *self.service.shutdown_hooks,",
            "        ]",
            "",
            "    async def schema_view(self, request: Request) -> Response:",
            "        from starlette.responses import JSONResponse",
            "",
            "        schema = self.service.schema()",
            "        return JSONResponse(schema)",
            "",
            "    @property",
            "    def routes(self) -> list[BaseRoute]:",
            "        from starlette.routing import Route",
            "",
            "        routes = super().routes",
            "",
            "        for name, method in self.service.apis.items():",
            "            api_endpoint = functools.partial(self.api_endpoint, name)",
            "            route_path = method.route",
            "            if not route_path.startswith(\"/\"):",
            "                route_path = \"/\" + route_path",
            "            routes.append(Route(route_path, api_endpoint, methods=[\"POST\"], name=name))",
            "        return routes",
            "",
            "    async def _to_thread(",
            "        self,",
            "        func: t.Callable[..., R],",
            "        *args: t.Any,",
            "        **kwargs: t.Any,",
            "    ) -> R:",
            "        if self._limiter is None:",
            "            threads = self.service.config.get(\"threads\", 1)",
            "            self._limiter = anyio.CapacityLimiter(threads)",
            "        func = functools.partial(func, *args, **kwargs)",
            "        output = await anyio.to_thread.run_sync(func, limiter=self._limiter)",
            "        return output",
            "",
            "    async def batch_infer(",
            "        self, name: str, input_args: tuple[t.Any, ...], input_kwargs: dict[str, t.Any]",
            "    ) -> t.Any:",
            "        method = self.service.apis[name]",
            "        func = getattr(self._service_instance, name)",
            "",
            "        async def inner_infer(",
            "            batches: t.Sequence[t.Any], **kwargs: t.Any",
            "        ) -> t.Sequence[t.Any]:",
            "            from bentoml._internal.context import component_context",
            "            from bentoml._internal.runner.container import AutoContainer",
            "            from bentoml._internal.utils import is_async_callable",
            "",
            "            self.adaptive_batch_size_hist.labels(  # type: ignore",
            "                runner_name=self.service.name,",
            "                worker_index=component_context.component_index,",
            "                method_name=name,",
            "                service_version=component_context.bento_version,",
            "                service_name=component_context.bento_name,",
            "            ).observe(len(batches))",
            "",
            "            if len(batches) == 0:",
            "                return []",
            "",
            "            batch, indices = AutoContainer.batches_to_batch(",
            "                batches, method.batch_dim[0]",
            "            )",
            "            if is_async_callable(func):",
            "                result = await func(batch, **kwargs)",
            "            else:",
            "                result = await self._to_thread(func, batch, **kwargs)",
            "            return AutoContainer.batch_to_batches(result, indices, method.batch_dim[1])",
            "",
            "        arg_names = [k for k in input_kwargs if k not in (\"ctx\", \"context\")]",
            "        if input_args:",
            "            if len(input_args) > 1 or len(arg_names) > 0:",
            "                raise TypeError(\"Batch inference function only accept one argument\")",
            "            value = input_args[0]",
            "        else:",
            "            if len(arg_names) != 1:",
            "                raise TypeError(\"Batch inference function only accept one argument\")",
            "            value = input_kwargs.pop(arg_names[0])",
            "        return await self.dispatchers[name](",
            "            functools.partial(inner_infer, **input_kwargs)",
            "        )(value)",
            "",
            "    async def api_endpoint(self, name: str, request: Request) -> Response:",
            "        from starlette.background import BackgroundTask",
            "",
            "        from _bentoml_sdk.io_models import ARGS",
            "        from _bentoml_sdk.io_models import KWARGS",
            "        from bentoml._internal.container import BentoMLContainer",
            "        from bentoml._internal.context import trace_context",
            "        from bentoml._internal.utils import get_original_func",
            "        from bentoml._internal.utils.http import set_cookies",
            "",
            "        from ..serde import ALL_SERDE",
            "",
            "        media_type = request.headers.get(\"Content-Type\", \"application/json\")",
            "        media_type = media_type.split(\";\")[0].strip()",
            "        if self.is_main and media_type == \"application/vnd.bentoml+pickle\":",
            "            # Disallow pickle media type for main service for security reasons",
            "            raise BentoMLException(",
            "                \"Pickle media type is not allowed for main service\",",
            "                error_code=HTTPStatus.UNSUPPORTED_MEDIA_TYPE,",
            "            )",
            "",
            "        method = self.service.apis[name]",
            "        func = getattr(self._service_instance, name)",
            "        ctx = self.service.context",
            "        serde = ALL_SERDE[media_type]()",
            "        input_data = await method.input_spec.from_http_request(request, serde)",
            "        input_args: tuple[t.Any, ...] = ()",
            "        input_params = {k: getattr(input_data, k) for k in input_data.model_fields}",
            "        if method.ctx_param is not None:",
            "            input_params[method.ctx_param] = ctx",
            "        if ARGS in input_params:",
            "            input_args = tuple(input_params.pop(ARGS))",
            "        if KWARGS in input_params:",
            "            input_params.update(input_params.pop(KWARGS))",
            "",
            "        original_func = get_original_func(func)",
            "",
            "        if method.batchable:",
            "            output = await self.batch_infer(name, input_args, input_params)",
            "        elif inspect.iscoroutinefunction(original_func):",
            "            output = await func(*input_args, **input_params)",
            "        elif inspect.isasyncgenfunction(original_func):",
            "            output = func(*input_args, **input_params)",
            "        elif inspect.isgeneratorfunction(original_func):",
            "",
            "            async def inner() -> t.AsyncGenerator[t.Any, None]:",
            "                gen = func(*input_args, **input_params)",
            "                while True:",
            "                    try:",
            "                        yield await self._to_thread(next, gen)",
            "                    except StopIteration:",
            "                        break",
            "                    except RuntimeError as e:",
            "                        if \"StopIteration\" in str(e):",
            "                            break",
            "                        raise",
            "",
            "            output = inner()",
            "        else:",
            "            output = await self._to_thread(func, *input_args, **input_params)",
            "",
            "        response = await method.output_spec.to_http_response(output, serde)",
            "        response.headers.update({\"Server\": f\"BentoML Service/{self.service.name}\"})",
            "",
            "        if method.ctx_param is not None:",
            "            response.status_code = ctx.response.status_code",
            "            response.headers.update(ctx.response.metadata)",
            "            set_cookies(response, ctx.response.cookies)",
            "        if trace_context.request_id is not None:",
            "            response.headers[\"X-BentoML-Request-ID\"] = str(trace_context.request_id)",
            "        if (",
            "            BentoMLContainer.http.response.trace_id.get()",
            "            and trace_context.trace_id is not None",
            "        ):",
            "            response.headers[\"X-BentoML-Trace-ID\"] = str(trace_context.trace_id)",
            "        # clean the request resources after the response is consumed.",
            "        response.background = BackgroundTask(request.close)",
            "        return response"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "178": [
                "ServiceAppFactory",
                "__call__"
            ],
            "187": [
                "ServiceAppFactory",
                "__call__"
            ]
        },
        "addLocation": [
            "src._bentoml_impl.server.app.ServiceAppFactory",
            "lib.ansible.modules.system.user.main"
        ]
    },
    "src/_bentoml_impl/server/serving.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 260,
                "afterPatchRowNumber": 260,
                "PatchRowcode": "             \"$(CIRCUS.WID)\","
            },
            "1": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 261,
                "PatchRowcode": "             \"--prometheus-dir\","
            },
            "2": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 262,
                "PatchRowcode": "             prometheus_dir,"
            },
            "3": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            \"--main\","
            },
            "4": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 263,
                "PatchRowcode": "             *ssl_args,"
            },
            "5": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 264,
                "PatchRowcode": "             *timeout_args,"
            },
            "6": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 265,
                "PatchRowcode": "         ]"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextlib",
            "import ipaddress",
            "import json",
            "import logging",
            "import os",
            "import pathlib",
            "import platform",
            "import socket",
            "import tempfile",
            "import typing as t",
            "",
            "from simple_di import Provide",
            "from simple_di import inject",
            "",
            "from _bentoml_sdk import Service",
            "from bentoml._internal.container import BentoMLContainer",
            "from bentoml.exceptions import BentoMLConfigException",
            "",
            "AnyService = Service[t.Any]",
            "",
            "if t.TYPE_CHECKING:",
            "    from circus.sockets import CircusSocket",
            "    from circus.watcher import Watcher",
            "",
            "    from .allocator import ResourceAllocator",
            "",
            "POSIX = os.name == \"posix\"",
            "WINDOWS = os.name == \"nt\"",
            "IS_WSL = \"microsoft-standard\" in platform.release()",
            "API_SERVER_NAME = \"_bento_api_server\"",
            "",
            "MAX_AF_UNIX_PATH_LENGTH = 103",
            "logger = logging.getLogger(\"bentoml.serve\")",
            "",
            "if POSIX and not IS_WSL:",
            "",
            "    def _get_server_socket(",
            "        service: AnyService,",
            "        uds_path: str,",
            "        port_stack: contextlib.ExitStack,",
            "        backlog: int,",
            "    ) -> tuple[str, CircusSocket]:",
            "        from circus.sockets import CircusSocket",
            "",
            "        from bentoml._internal.utils.uri import path_to_uri",
            "",
            "        socket_path = os.path.join(uds_path, f\"{id(service)}.sock\")",
            "        assert len(socket_path) < MAX_AF_UNIX_PATH_LENGTH",
            "        return path_to_uri(socket_path), CircusSocket(",
            "            name=service.name, path=socket_path, backlog=backlog",
            "        )",
            "",
            "elif WINDOWS or IS_WSL:",
            "",
            "    def _get_server_socket(",
            "        service: AnyService,",
            "        uds_path: str,",
            "        port_stack: contextlib.ExitStack,",
            "        backlog: int,",
            "    ) -> tuple[str, CircusSocket]:",
            "        from circus.sockets import CircusSocket",
            "",
            "        from bentoml._internal.utils import reserve_free_port",
            "",
            "        runner_port = port_stack.enter_context(reserve_free_port())",
            "        runner_host = \"127.0.0.1\"",
            "",
            "        return f\"tcp://{runner_host}:{runner_port}\", CircusSocket(",
            "            name=service.name,",
            "            host=runner_host,",
            "            port=runner_port,",
            "            backlog=backlog,",
            "        )",
            "",
            "else:",
            "",
            "    def _get_server_socket(",
            "        service: AnyService,",
            "        uds_path: str | None,",
            "        port_stack: contextlib.ExitStack,",
            "        backlog: int,",
            "    ) -> tuple[str, CircusSocket]:",
            "        from bentoml.exceptions import BentoMLException",
            "",
            "        raise BentoMLException(\"Unsupported platform\")",
            "",
            "",
            "_SERVICE_WORKER_SCRIPT = \"_bentoml_impl.worker.service\"",
            "",
            "",
            "def create_dependency_watcher(",
            "    bento_identifier: str,",
            "    svc: AnyService,",
            "    uds_path: str,",
            "    port_stack: contextlib.ExitStack,",
            "    backlog: int,",
            "    dependency_map: dict[str, str],",
            "    scheduler: ResourceAllocator,",
            "    working_dir: str | None = None,",
            ") -> tuple[Watcher, CircusSocket, str]:",
            "    from bentoml.serve import create_watcher",
            "",
            "    num_workers, worker_envs = scheduler.get_worker_env(svc)",
            "    uri, socket = _get_server_socket(svc, uds_path, port_stack, backlog)",
            "    args = [",
            "        \"-m\",",
            "        _SERVICE_WORKER_SCRIPT,",
            "        bento_identifier,",
            "        \"--service-name\",",
            "        svc.name,",
            "        \"--fd\",",
            "        f\"$(circus.sockets.{svc.name})\",",
            "        \"--worker-id\",",
            "        \"$(CIRCUS.WID)\",",
            "    ]",
            "",
            "    if worker_envs:",
            "        args.extend([\"--worker-env\", json.dumps(worker_envs)])",
            "",
            "    watcher = create_watcher(",
            "        name=f\"service_{svc.name}\",",
            "        args=args,",
            "        numprocesses=num_workers,",
            "        working_dir=working_dir,",
            "    )",
            "    return watcher, socket, uri",
            "",
            "",
            "@inject",
            "def serve_http(",
            "    bento_identifier: str | AnyService,",
            "    working_dir: str | None = None,",
            "    host: str = Provide[BentoMLContainer.http.host],",
            "    port: int = Provide[BentoMLContainer.http.port],",
            "    backlog: int = Provide[BentoMLContainer.api_server_config.backlog],",
            "    timeout: int | None = None,",
            "    ssl_certfile: str | None = Provide[BentoMLContainer.ssl.certfile],",
            "    ssl_keyfile: str | None = Provide[BentoMLContainer.ssl.keyfile],",
            "    ssl_keyfile_password: str | None = Provide[BentoMLContainer.ssl.keyfile_password],",
            "    ssl_version: int | None = Provide[BentoMLContainer.ssl.version],",
            "    ssl_cert_reqs: int | None = Provide[BentoMLContainer.ssl.cert_reqs],",
            "    ssl_ca_certs: str | None = Provide[BentoMLContainer.ssl.ca_certs],",
            "    ssl_ciphers: str | None = Provide[BentoMLContainer.ssl.ciphers],",
            "    bentoml_home: str = Provide[BentoMLContainer.bentoml_home],",
            "    development_mode: bool = False,",
            "    reload: bool = False,",
            "    dependency_map: dict[str, str] | None = None,",
            "    service_name: str = \"\",",
            ") -> None:",
            "    from circus.sockets import CircusSocket",
            "",
            "    from bentoml._internal.log import SERVER_LOGGING_CONFIG",
            "    from bentoml._internal.utils import reserve_free_port",
            "    from bentoml._internal.utils.analytics.usage_stats import track_serve",
            "    from bentoml._internal.utils.circus import create_standalone_arbiter",
            "    from bentoml.serve import construct_ssl_args",
            "    from bentoml.serve import create_watcher",
            "    from bentoml.serve import ensure_prometheus_dir",
            "    from bentoml.serve import make_reload_plugin",
            "",
            "    from ..loader import import_service",
            "    from ..loader import normalize_identifier",
            "    from .allocator import ResourceAllocator",
            "",
            "    prometheus_dir = ensure_prometheus_dir()",
            "    if isinstance(bento_identifier, Service):",
            "        svc = bento_identifier",
            "        bento_identifier = svc.import_string",
            "        assert (",
            "            working_dir is None",
            "        ), \"working_dir should not be set when passing a service in process\"",
            "        # use cwd",
            "        bento_path = pathlib.Path(\".\")",
            "    else:",
            "        bento_identifier, bento_path = normalize_identifier(",
            "            bento_identifier, working_dir",
            "        )",
            "",
            "        svc = import_service(bento_identifier, bento_path)",
            "",
            "    watchers: list[Watcher] = []",
            "    sockets: list[CircusSocket] = []",
            "    allocator = ResourceAllocator()",
            "    if dependency_map is None:",
            "        dependency_map = {}",
            "    if service_name:",
            "        svc = svc.find_dependent(service_name)",
            "    num_workers, worker_envs = allocator.get_worker_env(svc)",
            "    with tempfile.TemporaryDirectory(prefix=\"bentoml-uds-\") as uds_path:",
            "        if not service_name and not development_mode:",
            "            with contextlib.ExitStack() as port_stack:",
            "                for name, dep_svc in svc.all_services().items():",
            "                    if name == svc.name:",
            "                        continue",
            "                    if name in dependency_map:",
            "                        continue",
            "                    new_watcher, new_socket, uri = create_dependency_watcher(",
            "                        bento_identifier,",
            "                        dep_svc,",
            "                        uds_path,",
            "                        port_stack,",
            "                        backlog,",
            "                        dependency_map,",
            "                        allocator,",
            "                        str(bento_path.absolute()),",
            "                    )",
            "                    watchers.append(new_watcher)",
            "                    sockets.append(new_socket)",
            "                    dependency_map[name] = uri",
            "                # reserve one more to avoid conflicts",
            "                port_stack.enter_context(reserve_free_port())",
            "",
            "        try:",
            "            ipaddr = ipaddress.ip_address(host)",
            "            if ipaddr.version == 4:",
            "                family = socket.AF_INET",
            "            elif ipaddr.version == 6:",
            "                family = socket.AF_INET6",
            "            else:",
            "                raise BentoMLConfigException(",
            "                    f\"Unsupported host IP address version: {ipaddr.version}\"",
            "                )",
            "        except ValueError as e:",
            "            raise BentoMLConfigException(f\"Invalid host IP address: {host}\") from e",
            "",
            "        sockets.append(",
            "            CircusSocket(",
            "                name=API_SERVER_NAME,",
            "                host=host,",
            "                port=port,",
            "                family=family,",
            "                backlog=backlog,",
            "            )",
            "        )",
            "",
            "        ssl_args = construct_ssl_args(",
            "            ssl_certfile=ssl_certfile,",
            "            ssl_keyfile=ssl_keyfile,",
            "            ssl_keyfile_password=ssl_keyfile_password,",
            "            ssl_version=ssl_version,",
            "            ssl_cert_reqs=ssl_cert_reqs,",
            "            ssl_ca_certs=ssl_ca_certs,",
            "            ssl_ciphers=ssl_ciphers,",
            "        )",
            "        timeout_args = [\"--timeout\", str(timeout)] if timeout else []",
            "",
            "        server_args = [",
            "            \"-m\",",
            "            _SERVICE_WORKER_SCRIPT,",
            "            bento_identifier,",
            "            \"--fd\",",
            "            f\"$(circus.sockets.{API_SERVER_NAME})\",",
            "            \"--service-name\",",
            "            svc.name,",
            "            \"--backlog\",",
            "            str(backlog),",
            "            \"--worker-id\",",
            "            \"$(CIRCUS.WID)\",",
            "            \"--prometheus-dir\",",
            "            prometheus_dir,",
            "            \"--main\",",
            "            *ssl_args,",
            "            *timeout_args,",
            "        ]",
            "        if worker_envs:",
            "            server_args.extend([\"--worker-env\", json.dumps(worker_envs)])",
            "        if development_mode:",
            "            server_args.append(\"--development-mode\")",
            "",
            "        scheme = \"https\" if BentoMLContainer.ssl.enabled.get() else \"http\"",
            "        watchers.append(",
            "            create_watcher(",
            "                name=\"service\",",
            "                args=server_args,",
            "                working_dir=str(bento_path.absolute()),",
            "                numprocesses=num_workers,",
            "                close_child_stdin=not development_mode,",
            "            )",
            "        )",
            "",
            "        log_host = \"localhost\" if host in [\"0.0.0.0\", \"::\"] else host",
            "",
            "        # inject runner map now",
            "        inject_env = {\"BENTOML_RUNNER_MAP\": json.dumps(dependency_map)}",
            "        for watcher in watchers:",
            "            if watcher.env is None:",
            "                watcher.env = inject_env",
            "            else:",
            "                watcher.env.update(inject_env)",
            "",
            "        arbiter_kwargs: dict[str, t.Any] = {\"watchers\": watchers, \"sockets\": sockets}",
            "",
            "        if reload:",
            "            reload_plugin = make_reload_plugin(str(bento_path.absolute()), bentoml_home)",
            "            arbiter_kwargs[\"plugins\"] = [reload_plugin]",
            "",
            "        if development_mode:",
            "            arbiter_kwargs[\"debug\"] = True",
            "            arbiter_kwargs[\"loggerconfig\"] = SERVER_LOGGING_CONFIG",
            "            arbiter_kwargs[\"loglevel\"] = \"WARNING\"",
            "",
            "        arbiter = create_standalone_arbiter(**arbiter_kwargs)",
            "        with track_serve(svc, production=not development_mode):",
            "            arbiter.start(",
            "                cb=lambda _: logger.info(  # type: ignore",
            "                    'Starting production %s BentoServer from \"%s\" listening on %s://%s:%d (Press CTRL+C to quit)',",
            "                    scheme.upper(),",
            "                    bento_identifier,",
            "                    scheme,",
            "                    log_host,",
            "                    port,",
            "                ),",
            "            )"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextlib",
            "import ipaddress",
            "import json",
            "import logging",
            "import os",
            "import pathlib",
            "import platform",
            "import socket",
            "import tempfile",
            "import typing as t",
            "",
            "from simple_di import Provide",
            "from simple_di import inject",
            "",
            "from _bentoml_sdk import Service",
            "from bentoml._internal.container import BentoMLContainer",
            "from bentoml.exceptions import BentoMLConfigException",
            "",
            "AnyService = Service[t.Any]",
            "",
            "if t.TYPE_CHECKING:",
            "    from circus.sockets import CircusSocket",
            "    from circus.watcher import Watcher",
            "",
            "    from .allocator import ResourceAllocator",
            "",
            "POSIX = os.name == \"posix\"",
            "WINDOWS = os.name == \"nt\"",
            "IS_WSL = \"microsoft-standard\" in platform.release()",
            "API_SERVER_NAME = \"_bento_api_server\"",
            "",
            "MAX_AF_UNIX_PATH_LENGTH = 103",
            "logger = logging.getLogger(\"bentoml.serve\")",
            "",
            "if POSIX and not IS_WSL:",
            "",
            "    def _get_server_socket(",
            "        service: AnyService,",
            "        uds_path: str,",
            "        port_stack: contextlib.ExitStack,",
            "        backlog: int,",
            "    ) -> tuple[str, CircusSocket]:",
            "        from circus.sockets import CircusSocket",
            "",
            "        from bentoml._internal.utils.uri import path_to_uri",
            "",
            "        socket_path = os.path.join(uds_path, f\"{id(service)}.sock\")",
            "        assert len(socket_path) < MAX_AF_UNIX_PATH_LENGTH",
            "        return path_to_uri(socket_path), CircusSocket(",
            "            name=service.name, path=socket_path, backlog=backlog",
            "        )",
            "",
            "elif WINDOWS or IS_WSL:",
            "",
            "    def _get_server_socket(",
            "        service: AnyService,",
            "        uds_path: str,",
            "        port_stack: contextlib.ExitStack,",
            "        backlog: int,",
            "    ) -> tuple[str, CircusSocket]:",
            "        from circus.sockets import CircusSocket",
            "",
            "        from bentoml._internal.utils import reserve_free_port",
            "",
            "        runner_port = port_stack.enter_context(reserve_free_port())",
            "        runner_host = \"127.0.0.1\"",
            "",
            "        return f\"tcp://{runner_host}:{runner_port}\", CircusSocket(",
            "            name=service.name,",
            "            host=runner_host,",
            "            port=runner_port,",
            "            backlog=backlog,",
            "        )",
            "",
            "else:",
            "",
            "    def _get_server_socket(",
            "        service: AnyService,",
            "        uds_path: str | None,",
            "        port_stack: contextlib.ExitStack,",
            "        backlog: int,",
            "    ) -> tuple[str, CircusSocket]:",
            "        from bentoml.exceptions import BentoMLException",
            "",
            "        raise BentoMLException(\"Unsupported platform\")",
            "",
            "",
            "_SERVICE_WORKER_SCRIPT = \"_bentoml_impl.worker.service\"",
            "",
            "",
            "def create_dependency_watcher(",
            "    bento_identifier: str,",
            "    svc: AnyService,",
            "    uds_path: str,",
            "    port_stack: contextlib.ExitStack,",
            "    backlog: int,",
            "    dependency_map: dict[str, str],",
            "    scheduler: ResourceAllocator,",
            "    working_dir: str | None = None,",
            ") -> tuple[Watcher, CircusSocket, str]:",
            "    from bentoml.serve import create_watcher",
            "",
            "    num_workers, worker_envs = scheduler.get_worker_env(svc)",
            "    uri, socket = _get_server_socket(svc, uds_path, port_stack, backlog)",
            "    args = [",
            "        \"-m\",",
            "        _SERVICE_WORKER_SCRIPT,",
            "        bento_identifier,",
            "        \"--service-name\",",
            "        svc.name,",
            "        \"--fd\",",
            "        f\"$(circus.sockets.{svc.name})\",",
            "        \"--worker-id\",",
            "        \"$(CIRCUS.WID)\",",
            "    ]",
            "",
            "    if worker_envs:",
            "        args.extend([\"--worker-env\", json.dumps(worker_envs)])",
            "",
            "    watcher = create_watcher(",
            "        name=f\"service_{svc.name}\",",
            "        args=args,",
            "        numprocesses=num_workers,",
            "        working_dir=working_dir,",
            "    )",
            "    return watcher, socket, uri",
            "",
            "",
            "@inject",
            "def serve_http(",
            "    bento_identifier: str | AnyService,",
            "    working_dir: str | None = None,",
            "    host: str = Provide[BentoMLContainer.http.host],",
            "    port: int = Provide[BentoMLContainer.http.port],",
            "    backlog: int = Provide[BentoMLContainer.api_server_config.backlog],",
            "    timeout: int | None = None,",
            "    ssl_certfile: str | None = Provide[BentoMLContainer.ssl.certfile],",
            "    ssl_keyfile: str | None = Provide[BentoMLContainer.ssl.keyfile],",
            "    ssl_keyfile_password: str | None = Provide[BentoMLContainer.ssl.keyfile_password],",
            "    ssl_version: int | None = Provide[BentoMLContainer.ssl.version],",
            "    ssl_cert_reqs: int | None = Provide[BentoMLContainer.ssl.cert_reqs],",
            "    ssl_ca_certs: str | None = Provide[BentoMLContainer.ssl.ca_certs],",
            "    ssl_ciphers: str | None = Provide[BentoMLContainer.ssl.ciphers],",
            "    bentoml_home: str = Provide[BentoMLContainer.bentoml_home],",
            "    development_mode: bool = False,",
            "    reload: bool = False,",
            "    dependency_map: dict[str, str] | None = None,",
            "    service_name: str = \"\",",
            ") -> None:",
            "    from circus.sockets import CircusSocket",
            "",
            "    from bentoml._internal.log import SERVER_LOGGING_CONFIG",
            "    from bentoml._internal.utils import reserve_free_port",
            "    from bentoml._internal.utils.analytics.usage_stats import track_serve",
            "    from bentoml._internal.utils.circus import create_standalone_arbiter",
            "    from bentoml.serve import construct_ssl_args",
            "    from bentoml.serve import create_watcher",
            "    from bentoml.serve import ensure_prometheus_dir",
            "    from bentoml.serve import make_reload_plugin",
            "",
            "    from ..loader import import_service",
            "    from ..loader import normalize_identifier",
            "    from .allocator import ResourceAllocator",
            "",
            "    prometheus_dir = ensure_prometheus_dir()",
            "    if isinstance(bento_identifier, Service):",
            "        svc = bento_identifier",
            "        bento_identifier = svc.import_string",
            "        assert (",
            "            working_dir is None",
            "        ), \"working_dir should not be set when passing a service in process\"",
            "        # use cwd",
            "        bento_path = pathlib.Path(\".\")",
            "    else:",
            "        bento_identifier, bento_path = normalize_identifier(",
            "            bento_identifier, working_dir",
            "        )",
            "",
            "        svc = import_service(bento_identifier, bento_path)",
            "",
            "    watchers: list[Watcher] = []",
            "    sockets: list[CircusSocket] = []",
            "    allocator = ResourceAllocator()",
            "    if dependency_map is None:",
            "        dependency_map = {}",
            "    if service_name:",
            "        svc = svc.find_dependent(service_name)",
            "    num_workers, worker_envs = allocator.get_worker_env(svc)",
            "    with tempfile.TemporaryDirectory(prefix=\"bentoml-uds-\") as uds_path:",
            "        if not service_name and not development_mode:",
            "            with contextlib.ExitStack() as port_stack:",
            "                for name, dep_svc in svc.all_services().items():",
            "                    if name == svc.name:",
            "                        continue",
            "                    if name in dependency_map:",
            "                        continue",
            "                    new_watcher, new_socket, uri = create_dependency_watcher(",
            "                        bento_identifier,",
            "                        dep_svc,",
            "                        uds_path,",
            "                        port_stack,",
            "                        backlog,",
            "                        dependency_map,",
            "                        allocator,",
            "                        str(bento_path.absolute()),",
            "                    )",
            "                    watchers.append(new_watcher)",
            "                    sockets.append(new_socket)",
            "                    dependency_map[name] = uri",
            "                # reserve one more to avoid conflicts",
            "                port_stack.enter_context(reserve_free_port())",
            "",
            "        try:",
            "            ipaddr = ipaddress.ip_address(host)",
            "            if ipaddr.version == 4:",
            "                family = socket.AF_INET",
            "            elif ipaddr.version == 6:",
            "                family = socket.AF_INET6",
            "            else:",
            "                raise BentoMLConfigException(",
            "                    f\"Unsupported host IP address version: {ipaddr.version}\"",
            "                )",
            "        except ValueError as e:",
            "            raise BentoMLConfigException(f\"Invalid host IP address: {host}\") from e",
            "",
            "        sockets.append(",
            "            CircusSocket(",
            "                name=API_SERVER_NAME,",
            "                host=host,",
            "                port=port,",
            "                family=family,",
            "                backlog=backlog,",
            "            )",
            "        )",
            "",
            "        ssl_args = construct_ssl_args(",
            "            ssl_certfile=ssl_certfile,",
            "            ssl_keyfile=ssl_keyfile,",
            "            ssl_keyfile_password=ssl_keyfile_password,",
            "            ssl_version=ssl_version,",
            "            ssl_cert_reqs=ssl_cert_reqs,",
            "            ssl_ca_certs=ssl_ca_certs,",
            "            ssl_ciphers=ssl_ciphers,",
            "        )",
            "        timeout_args = [\"--timeout\", str(timeout)] if timeout else []",
            "",
            "        server_args = [",
            "            \"-m\",",
            "            _SERVICE_WORKER_SCRIPT,",
            "            bento_identifier,",
            "            \"--fd\",",
            "            f\"$(circus.sockets.{API_SERVER_NAME})\",",
            "            \"--service-name\",",
            "            svc.name,",
            "            \"--backlog\",",
            "            str(backlog),",
            "            \"--worker-id\",",
            "            \"$(CIRCUS.WID)\",",
            "            \"--prometheus-dir\",",
            "            prometheus_dir,",
            "            *ssl_args,",
            "            *timeout_args,",
            "        ]",
            "        if worker_envs:",
            "            server_args.extend([\"--worker-env\", json.dumps(worker_envs)])",
            "        if development_mode:",
            "            server_args.append(\"--development-mode\")",
            "",
            "        scheme = \"https\" if BentoMLContainer.ssl.enabled.get() else \"http\"",
            "        watchers.append(",
            "            create_watcher(",
            "                name=\"service\",",
            "                args=server_args,",
            "                working_dir=str(bento_path.absolute()),",
            "                numprocesses=num_workers,",
            "                close_child_stdin=not development_mode,",
            "            )",
            "        )",
            "",
            "        log_host = \"localhost\" if host in [\"0.0.0.0\", \"::\"] else host",
            "",
            "        # inject runner map now",
            "        inject_env = {\"BENTOML_RUNNER_MAP\": json.dumps(dependency_map)}",
            "        for watcher in watchers:",
            "            if watcher.env is None:",
            "                watcher.env = inject_env",
            "            else:",
            "                watcher.env.update(inject_env)",
            "",
            "        arbiter_kwargs: dict[str, t.Any] = {\"watchers\": watchers, \"sockets\": sockets}",
            "",
            "        if reload:",
            "            reload_plugin = make_reload_plugin(str(bento_path.absolute()), bentoml_home)",
            "            arbiter_kwargs[\"plugins\"] = [reload_plugin]",
            "",
            "        if development_mode:",
            "            arbiter_kwargs[\"debug\"] = True",
            "            arbiter_kwargs[\"loggerconfig\"] = SERVER_LOGGING_CONFIG",
            "            arbiter_kwargs[\"loglevel\"] = \"WARNING\"",
            "",
            "        arbiter = create_standalone_arbiter(**arbiter_kwargs)",
            "        with track_serve(svc, production=not development_mode):",
            "            arbiter.start(",
            "                cb=lambda _: logger.info(  # type: ignore",
            "                    'Starting production %s BentoServer from \"%s\" listening on %s://%s:%d (Press CTRL+C to quit)',",
            "                    scheme.upper(),",
            "                    bento_identifier,",
            "                    scheme,",
            "                    log_host,",
            "                    port,",
            "                ),",
            "            )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "263": [
                "serve_http"
            ]
        },
        "addLocation": []
    },
    "src/_bentoml_impl/worker/service.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "     type=click.INT,"
            },
            "1": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "     help=\"Specify the timeout for API server\","
            },
            "2": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 97,
                "PatchRowcode": " )"
            },
            "3": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-@click.option(\"--main\", \"is_main\", type=click.BOOL, default=False, is_flag=True)"
            },
            "4": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 98,
                "PatchRowcode": " def main("
            },
            "5": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "     bento_identifier: str,"
            },
            "6": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "     service_name: str,"
            },
            "7": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "     ssl_ciphers: str | None,"
            },
            "8": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "     development_mode: bool,"
            },
            "9": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "     timeout: int,"
            },
            "10": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    is_main: bool = False,"
            },
            "11": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 116,
                "PatchRowcode": " ):"
            },
            "12": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "     \"\"\""
            },
            "13": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "     Start a HTTP server worker for given service."
            },
            "14": {
                "beforePatchRowNumber": 165,
                "afterPatchRowNumber": 163,
                "PatchRowcode": "         BentoMLContainer.prometheus_multiproc_dir.set(prometheus_dir)"
            },
            "15": {
                "beforePatchRowNumber": 166,
                "afterPatchRowNumber": 164,
                "PatchRowcode": "     component_context.component_name = service.name"
            },
            "16": {
                "beforePatchRowNumber": 167,
                "afterPatchRowNumber": 165,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    app_factory = ServiceAppFactory(service)"
            },
            "18": {
                "beforePatchRowNumber": 169,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    asgi_app = app_factory(is_main=is_main)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 166,
                "PatchRowcode": "+    asgi_app = ServiceAppFactory("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+        service, is_main=component_context.component_type == \"entry_service\""
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+    )()"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 170,
                "PatchRowcode": "     uvicorn_extra_options: dict[str, t.Any] = {}"
            },
            "24": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": 171,
                "PatchRowcode": "     if ssl_version is not None:"
            },
            "25": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 172,
                "PatchRowcode": "         uvicorn_extra_options[\"ssl_version\"] = ssl_version"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import json",
            "import os",
            "import typing as t",
            "",
            "import click",
            "",
            "",
            "@click.command()",
            "@click.argument(\"bento_identifier\", type=click.STRING, required=False, default=\".\")",
            "@click.option(\"--service-name\", type=click.STRING, required=False, default=\"\")",
            "@click.option(",
            "    \"--fd\",",
            "    type=click.INT,",
            "    required=True,",
            "    help=\"File descriptor of the socket to listen on\",",
            ")",
            "@click.option(",
            "    \"--runner-map\",",
            "    type=click.STRING,",
            "    envvar=\"BENTOML_RUNNER_MAP\",",
            "    help=\"JSON string of runners map, default sets to envars `BENTOML_RUNNER_MAP`\",",
            ")",
            "@click.option(",
            "    \"--backlog\", type=click.INT, default=2048, help=\"Backlog size for the socket\"",
            ")",
            "@click.option(",
            "    \"--prometheus-dir\",",
            "    type=click.Path(exists=True),",
            "    help=\"Required by prometheus to pass the metrics in multi-process mode\",",
            ")",
            "@click.option(",
            "    \"--worker-env\", type=click.STRING, default=None, help=\"Environment variables\"",
            ")",
            "@click.option(",
            "    \"--worker-id\",",
            "    required=False,",
            "    type=click.INT,",
            "    default=None,",
            "    help=\"If set, start the server as a bare worker with the given worker ID. Otherwise start a standalone server with a supervisor process.\",",
            ")",
            "@click.option(",
            "    \"--ssl-certfile\",",
            "    type=str,",
            "    default=None,",
            "    help=\"SSL certificate file\",",
            ")",
            "@click.option(",
            "    \"--ssl-keyfile\",",
            "    type=str,",
            "    default=None,",
            "    help=\"SSL key file\",",
            ")",
            "@click.option(",
            "    \"--ssl-keyfile-password\",",
            "    type=str,",
            "    default=None,",
            "    help=\"SSL keyfile password\",",
            ")",
            "@click.option(",
            "    \"--ssl-version\",",
            "    type=int,",
            "    default=None,",
            "    help=\"SSL version to use (see stdlib 'ssl' module)\",",
            ")",
            "@click.option(",
            "    \"--ssl-cert-reqs\",",
            "    type=int,",
            "    default=None,",
            "    help=\"Whether client certificate is required (see stdlib 'ssl' module)\",",
            ")",
            "@click.option(",
            "    \"--ssl-ca-certs\",",
            "    type=str,",
            "    default=None,",
            "    help=\"CA certificates file\",",
            ")",
            "@click.option(",
            "    \"--ssl-ciphers\",",
            "    type=str,",
            "    default=None,",
            "    help=\"Ciphers to use (see stdlib 'ssl' module)\",",
            ")",
            "@click.option(",
            "    \"--development-mode\",",
            "    type=click.BOOL,",
            "    help=\"Run the API server in development mode\",",
            "    is_flag=True,",
            "    default=False,",
            "    show_default=True,",
            ")",
            "@click.option(",
            "    \"--timeout\",",
            "    type=click.INT,",
            "    help=\"Specify the timeout for API server\",",
            ")",
            "@click.option(\"--main\", \"is_main\", type=click.BOOL, default=False, is_flag=True)",
            "def main(",
            "    bento_identifier: str,",
            "    service_name: str,",
            "    fd: int,",
            "    runner_map: str | None,",
            "    backlog: int,",
            "    worker_env: str | None,",
            "    worker_id: int | None,",
            "    prometheus_dir: str | None,",
            "    ssl_certfile: str | None,",
            "    ssl_keyfile: str | None,",
            "    ssl_keyfile_password: str | None,",
            "    ssl_version: int | None,",
            "    ssl_cert_reqs: int | None,",
            "    ssl_ca_certs: str | None,",
            "    ssl_ciphers: str | None,",
            "    development_mode: bool,",
            "    timeout: int,",
            "    is_main: bool = False,",
            "):",
            "    \"\"\"",
            "    Start a HTTP server worker for given service.",
            "    \"\"\"",
            "    import psutil",
            "    import uvicorn",
            "",
            "    if worker_env:",
            "        env_list: list[dict[str, t.Any]] = json.loads(worker_env)",
            "        if worker_id is not None:",
            "            # worker id from circus starts from 1",
            "            worker_key = worker_id - 1",
            "            if worker_key >= len(env_list):",
            "                raise IndexError(",
            "                    f\"Worker ID {worker_id} is out of range, \"",
            "                    f\"the maximum worker ID is {len(env_list)}\"",
            "                )",
            "            os.environ.update(env_list[worker_key])",
            "",
            "    from _bentoml_impl.loader import import_service",
            "    from bentoml._internal.container import BentoMLContainer",
            "    from bentoml._internal.context import component_context",
            "    from bentoml._internal.log import configure_server_logging",
            "",
            "    from ..server.app import ServiceAppFactory",
            "",
            "    if runner_map:",
            "        BentoMLContainer.remote_runner_mapping.set(",
            "            t.cast(t.Dict[str, str], json.loads(runner_map))",
            "        )",
            "",
            "    service = import_service(bento_identifier)",
            "    service.inject_config()",
            "",
            "    if service_name and service_name != service.name:",
            "        service = service.find_dependent(service_name)",
            "        component_context.component_type = \"service\"",
            "    else:",
            "        component_context.component_type = \"entry_service\"",
            "",
            "    if worker_id is not None:",
            "        component_context.component_index = worker_id",
            "",
            "    configure_server_logging()",
            "    BentoMLContainer.development_mode.set(development_mode)",
            "",
            "    if prometheus_dir is not None:",
            "        BentoMLContainer.prometheus_multiproc_dir.set(prometheus_dir)",
            "    component_context.component_name = service.name",
            "",
            "    app_factory = ServiceAppFactory(service)",
            "    asgi_app = app_factory(is_main=is_main)",
            "    uvicorn_extra_options: dict[str, t.Any] = {}",
            "    if ssl_version is not None:",
            "        uvicorn_extra_options[\"ssl_version\"] = ssl_version",
            "    if ssl_cert_reqs is not None:",
            "        uvicorn_extra_options[\"ssl_cert_reqs\"] = ssl_cert_reqs",
            "    if ssl_ciphers is not None:",
            "        uvicorn_extra_options[\"ssl_ciphers\"] = ssl_ciphers",
            "",
            "    if psutil.WINDOWS:",
            "        # 1. uvloop is not supported on Windows",
            "        # 2. the default policy for Python > 3.8 on Windows is ProactorEventLoop, which doesn't",
            "        #    support listen on a existing socket file descriptors",
            "        # See https://docs.python.org/3.8/library/asyncio-platforms.html#windows",
            "        uvicorn_extra_options[\"loop\"] = \"asyncio\"",
            "        import asyncio",
            "",
            "        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())  # type: ignore",
            "",
            "    uvicorn.run(",
            "        app=asgi_app,",
            "        fd=fd,",
            "        backlog=backlog,",
            "        log_config=None,",
            "        workers=1,",
            "        ssl_certfile=ssl_certfile,",
            "        ssl_keyfile=ssl_keyfile,",
            "        ssl_keyfile_password=ssl_keyfile_password,",
            "        ssl_ca_certs=ssl_ca_certs,",
            "        **uvicorn_extra_options,",
            "    )",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()  # pylint: disable=no-value-for-parameter"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import json",
            "import os",
            "import typing as t",
            "",
            "import click",
            "",
            "",
            "@click.command()",
            "@click.argument(\"bento_identifier\", type=click.STRING, required=False, default=\".\")",
            "@click.option(\"--service-name\", type=click.STRING, required=False, default=\"\")",
            "@click.option(",
            "    \"--fd\",",
            "    type=click.INT,",
            "    required=True,",
            "    help=\"File descriptor of the socket to listen on\",",
            ")",
            "@click.option(",
            "    \"--runner-map\",",
            "    type=click.STRING,",
            "    envvar=\"BENTOML_RUNNER_MAP\",",
            "    help=\"JSON string of runners map, default sets to envars `BENTOML_RUNNER_MAP`\",",
            ")",
            "@click.option(",
            "    \"--backlog\", type=click.INT, default=2048, help=\"Backlog size for the socket\"",
            ")",
            "@click.option(",
            "    \"--prometheus-dir\",",
            "    type=click.Path(exists=True),",
            "    help=\"Required by prometheus to pass the metrics in multi-process mode\",",
            ")",
            "@click.option(",
            "    \"--worker-env\", type=click.STRING, default=None, help=\"Environment variables\"",
            ")",
            "@click.option(",
            "    \"--worker-id\",",
            "    required=False,",
            "    type=click.INT,",
            "    default=None,",
            "    help=\"If set, start the server as a bare worker with the given worker ID. Otherwise start a standalone server with a supervisor process.\",",
            ")",
            "@click.option(",
            "    \"--ssl-certfile\",",
            "    type=str,",
            "    default=None,",
            "    help=\"SSL certificate file\",",
            ")",
            "@click.option(",
            "    \"--ssl-keyfile\",",
            "    type=str,",
            "    default=None,",
            "    help=\"SSL key file\",",
            ")",
            "@click.option(",
            "    \"--ssl-keyfile-password\",",
            "    type=str,",
            "    default=None,",
            "    help=\"SSL keyfile password\",",
            ")",
            "@click.option(",
            "    \"--ssl-version\",",
            "    type=int,",
            "    default=None,",
            "    help=\"SSL version to use (see stdlib 'ssl' module)\",",
            ")",
            "@click.option(",
            "    \"--ssl-cert-reqs\",",
            "    type=int,",
            "    default=None,",
            "    help=\"Whether client certificate is required (see stdlib 'ssl' module)\",",
            ")",
            "@click.option(",
            "    \"--ssl-ca-certs\",",
            "    type=str,",
            "    default=None,",
            "    help=\"CA certificates file\",",
            ")",
            "@click.option(",
            "    \"--ssl-ciphers\",",
            "    type=str,",
            "    default=None,",
            "    help=\"Ciphers to use (see stdlib 'ssl' module)\",",
            ")",
            "@click.option(",
            "    \"--development-mode\",",
            "    type=click.BOOL,",
            "    help=\"Run the API server in development mode\",",
            "    is_flag=True,",
            "    default=False,",
            "    show_default=True,",
            ")",
            "@click.option(",
            "    \"--timeout\",",
            "    type=click.INT,",
            "    help=\"Specify the timeout for API server\",",
            ")",
            "def main(",
            "    bento_identifier: str,",
            "    service_name: str,",
            "    fd: int,",
            "    runner_map: str | None,",
            "    backlog: int,",
            "    worker_env: str | None,",
            "    worker_id: int | None,",
            "    prometheus_dir: str | None,",
            "    ssl_certfile: str | None,",
            "    ssl_keyfile: str | None,",
            "    ssl_keyfile_password: str | None,",
            "    ssl_version: int | None,",
            "    ssl_cert_reqs: int | None,",
            "    ssl_ca_certs: str | None,",
            "    ssl_ciphers: str | None,",
            "    development_mode: bool,",
            "    timeout: int,",
            "):",
            "    \"\"\"",
            "    Start a HTTP server worker for given service.",
            "    \"\"\"",
            "    import psutil",
            "    import uvicorn",
            "",
            "    if worker_env:",
            "        env_list: list[dict[str, t.Any]] = json.loads(worker_env)",
            "        if worker_id is not None:",
            "            # worker id from circus starts from 1",
            "            worker_key = worker_id - 1",
            "            if worker_key >= len(env_list):",
            "                raise IndexError(",
            "                    f\"Worker ID {worker_id} is out of range, \"",
            "                    f\"the maximum worker ID is {len(env_list)}\"",
            "                )",
            "            os.environ.update(env_list[worker_key])",
            "",
            "    from _bentoml_impl.loader import import_service",
            "    from bentoml._internal.container import BentoMLContainer",
            "    from bentoml._internal.context import component_context",
            "    from bentoml._internal.log import configure_server_logging",
            "",
            "    from ..server.app import ServiceAppFactory",
            "",
            "    if runner_map:",
            "        BentoMLContainer.remote_runner_mapping.set(",
            "            t.cast(t.Dict[str, str], json.loads(runner_map))",
            "        )",
            "",
            "    service = import_service(bento_identifier)",
            "    service.inject_config()",
            "",
            "    if service_name and service_name != service.name:",
            "        service = service.find_dependent(service_name)",
            "        component_context.component_type = \"service\"",
            "    else:",
            "        component_context.component_type = \"entry_service\"",
            "",
            "    if worker_id is not None:",
            "        component_context.component_index = worker_id",
            "",
            "    configure_server_logging()",
            "    BentoMLContainer.development_mode.set(development_mode)",
            "",
            "    if prometheus_dir is not None:",
            "        BentoMLContainer.prometheus_multiproc_dir.set(prometheus_dir)",
            "    component_context.component_name = service.name",
            "",
            "    asgi_app = ServiceAppFactory(",
            "        service, is_main=component_context.component_type == \"entry_service\"",
            "    )()",
            "",
            "    uvicorn_extra_options: dict[str, t.Any] = {}",
            "    if ssl_version is not None:",
            "        uvicorn_extra_options[\"ssl_version\"] = ssl_version",
            "    if ssl_cert_reqs is not None:",
            "        uvicorn_extra_options[\"ssl_cert_reqs\"] = ssl_cert_reqs",
            "    if ssl_ciphers is not None:",
            "        uvicorn_extra_options[\"ssl_ciphers\"] = ssl_ciphers",
            "",
            "    if psutil.WINDOWS:",
            "        # 1. uvloop is not supported on Windows",
            "        # 2. the default policy for Python > 3.8 on Windows is ProactorEventLoop, which doesn't",
            "        #    support listen on a existing socket file descriptors",
            "        # See https://docs.python.org/3.8/library/asyncio-platforms.html#windows",
            "        uvicorn_extra_options[\"loop\"] = \"asyncio\"",
            "        import asyncio",
            "",
            "        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())  # type: ignore",
            "",
            "    uvicorn.run(",
            "        app=asgi_app,",
            "        fd=fd,",
            "        backlog=backlog,",
            "        log_config=None,",
            "        workers=1,",
            "        ssl_certfile=ssl_certfile,",
            "        ssl_keyfile=ssl_keyfile,",
            "        ssl_keyfile_password=ssl_keyfile_password,",
            "        ssl_ca_certs=ssl_ca_certs,",
            "        **uvicorn_extra_options,",
            "    )",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()  # pylint: disable=no-value-for-parameter"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "98": [],
            "117": [
                "main"
            ],
            "168": [
                "main"
            ],
            "169": [
                "main"
            ]
        },
        "addLocation": []
    }
}