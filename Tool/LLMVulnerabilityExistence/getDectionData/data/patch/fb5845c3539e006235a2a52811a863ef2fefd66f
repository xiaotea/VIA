{
    "gradio/data_classes.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from fastapi import Request"
            },
            "1": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from gradio_client.documentation import document"
            },
            "2": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from gradio_client.utils import traverse"
            },
            "3": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from pydantic import BaseModel, RootModel, ValidationError"
            },
            "4": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from typing_extensions import NotRequired"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+from pydantic import ("
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+    BaseModel,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+    GetCoreSchemaHandler,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+    GetJsonSchemaHandler,"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+    RootModel,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+    ValidationError,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+from pydantic.json_schema import JsonSchemaValue"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+from pydantic_core import core_schema"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+from typing_extensions import Annotated, NotRequired"
            },
            "15": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " try:"
            },
            "17": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "     from pydantic import JsonValue"
            },
            "18": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "     session_hash: Optional[str] = None"
            },
            "19": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 55,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 56,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-class PredictBody(BaseModel):"
            },
            "22": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    model_config = {\"arbitrary_types_allowed\": True}"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+class _StarletteRequestPydanticAnnotation:"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+    @classmethod"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+    def __get_pydantic_core_schema__("
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+        cls,"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+        _source_type: Any,"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+        _handler: GetCoreSchemaHandler,"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+    ) -> core_schema.CoreSchema:"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+        def validate_request(value: Any) -> Request:"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+            if isinstance(value, Request):"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+                return value"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+            raise ValueError(\"Input must be a Starlette Request object\")"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+        return core_schema.no_info_plain_validator_function(validate_request)"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+    @classmethod"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+    def __get_pydantic_json_schema__("
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+        cls, _core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+    ) -> JsonSchemaValue:"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+        return {\"type\": \"object\", \"title\": \"StarletteRequest\"}"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+PydanticStarletteRequest = Annotated[Request, _StarletteRequestPydanticAnnotation]"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+"
            },
            "46": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " "
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+class PredictBody(BaseModel):"
            },
            "48": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 82,
                "PatchRowcode": "     session_hash: Optional[str] = None"
            },
            "49": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "     event_id: Optional[str] = None"
            },
            "50": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "     data: List[Any]"
            },
            "51": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "     batched: Optional[bool] = ("
            },
            "52": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "         False  # Whether the data is a batch of samples (i.e. called from the queue if batch=True) or a single sample (i.e. called from the UI)"
            },
            "53": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "     )"
            },
            "54": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    request: Optional[Request] = ("
            },
            "55": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        None  # dictionary of request headers, query parameters, url, etc. (used to to pass in request for queuing)"
            },
            "56": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    )"
            },
            "57": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 92,
                "PatchRowcode": " "
            },
            "58": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "     @classmethod"
            },
            "59": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "     def __get_pydantic_json_schema__(cls, core_schema, handler):"
            },
            "60": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "                 \"trigger_id\": {\"type\": \"integer\"},"
            },
            "61": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "                 \"simple_format\": {\"type\": \"boolean\"},"
            },
            "62": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "                 \"batched\": {\"type\": \"boolean\"},"
            },
            "63": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"request\": {\"type\": \"object\"},"
            },
            "64": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "             },"
            },
            "65": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "             \"required\": [\"data\"],"
            },
            "66": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 109,
                "PatchRowcode": "         }"
            },
            "67": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 110,
                "PatchRowcode": " "
            },
            "68": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 111,
                "PatchRowcode": " "
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+class PredictBodyInternal(PredictBody):"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+    \"Separate class to avoid exposing PydanticStarletteRequest in the API validation\""
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+    request: Optional[PydanticStarletteRequest] = ("
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        None  # dictionary of request headers, query parameters, url, etc. (used to to pass in request for queuing)"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+    )"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+"
            },
            "77": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 120,
                "PatchRowcode": " class ResetBody(BaseModel):"
            },
            "78": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "     event_id: str"
            },
            "79": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 122,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\"Pydantic data models and other dataclasses. This is the only file that uses Optional[]",
            "typing syntax instead of | None syntax to work with pydantic\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import pathlib",
            "import secrets",
            "import shutil",
            "from abc import ABC, abstractmethod",
            "from enum import Enum, auto",
            "from typing import (",
            "    Any,",
            "    Iterator,",
            "    List,",
            "    Literal,",
            "    NewType,",
            "    Optional,",
            "    Tuple,",
            "    TypedDict,",
            "    Union,",
            ")",
            "",
            "from fastapi import Request",
            "from gradio_client.documentation import document",
            "from gradio_client.utils import traverse",
            "from pydantic import BaseModel, RootModel, ValidationError",
            "from typing_extensions import NotRequired",
            "",
            "try:",
            "    from pydantic import JsonValue",
            "except ImportError:",
            "    JsonValue = Any",
            "",
            "DeveloperPath = NewType(\"DeveloperPath\", str)",
            "UserProvidedPath = NewType(\"UserProvidedPath\", str)",
            "",
            "",
            "class CancelBody(BaseModel):",
            "    session_hash: str",
            "    fn_index: int",
            "    event_id: str",
            "",
            "",
            "class SimplePredictBody(BaseModel):",
            "    data: List[Any]",
            "    session_hash: Optional[str] = None",
            "",
            "",
            "class PredictBody(BaseModel):",
            "    model_config = {\"arbitrary_types_allowed\": True}",
            "",
            "    session_hash: Optional[str] = None",
            "    event_id: Optional[str] = None",
            "    data: List[Any]",
            "    event_data: Optional[Any] = None",
            "    fn_index: Optional[int] = None",
            "    trigger_id: Optional[int] = None",
            "    simple_format: bool = False",
            "    batched: Optional[bool] = (",
            "        False  # Whether the data is a batch of samples (i.e. called from the queue if batch=True) or a single sample (i.e. called from the UI)",
            "    )",
            "    request: Optional[Request] = (",
            "        None  # dictionary of request headers, query parameters, url, etc. (used to to pass in request for queuing)",
            "    )",
            "",
            "    @classmethod",
            "    def __get_pydantic_json_schema__(cls, core_schema, handler):",
            "        return {",
            "            \"title\": \"PredictBody\",",
            "            \"type\": \"object\",",
            "            \"properties\": {",
            "                \"session_hash\": {\"type\": \"string\"},",
            "                \"event_id\": {\"type\": \"string\"},",
            "                \"data\": {\"type\": \"array\", \"items\": {\"type\": \"object\"}},",
            "                \"event_data\": {\"type\": \"object\"},",
            "                \"fn_index\": {\"type\": \"integer\"},",
            "                \"trigger_id\": {\"type\": \"integer\"},",
            "                \"simple_format\": {\"type\": \"boolean\"},",
            "                \"batched\": {\"type\": \"boolean\"},",
            "                \"request\": {\"type\": \"object\"},",
            "            },",
            "            \"required\": [\"data\"],",
            "        }",
            "",
            "",
            "class ResetBody(BaseModel):",
            "    event_id: str",
            "",
            "",
            "class ComponentServerJSONBody(BaseModel):",
            "    session_hash: str",
            "    component_id: int",
            "    fn_name: str",
            "    data: Any",
            "",
            "",
            "class DataWithFiles(BaseModel):",
            "    data: Any",
            "    files: List[Tuple[str, bytes]]",
            "",
            "",
            "class ComponentServerBlobBody(BaseModel):",
            "    session_hash: str",
            "    component_id: int",
            "    fn_name: str",
            "    data: DataWithFiles",
            "",
            "",
            "class InterfaceTypes(Enum):",
            "    STANDARD = auto()",
            "    INPUT_ONLY = auto()",
            "    OUTPUT_ONLY = auto()",
            "    UNIFIED = auto()",
            "",
            "",
            "class GradioBaseModel(ABC):",
            "    def copy_to_dir(self, dir: str | pathlib.Path) -> GradioDataModel:",
            "        if not isinstance(self, (BaseModel, RootModel)):",
            "            raise TypeError(\"must be used in a Pydantic model\")",
            "        dir = pathlib.Path(dir)",
            "",
            "        # TODO: Making sure path is unique should be done in caller",
            "        def unique_copy(obj: dict):",
            "            data = FileData(**obj)",
            "            return data._copy_to_dir(",
            "                str(pathlib.Path(dir / secrets.token_hex(10)))",
            "            ).model_dump()",
            "",
            "        return self.__class__.from_json(",
            "            x=traverse(",
            "                self.model_dump(),",
            "                unique_copy,",
            "                FileData.is_file_data,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    @abstractmethod",
            "    def from_json(cls, x) -> GradioDataModel:",
            "        pass",
            "",
            "",
            "class JsonData(RootModel):",
            "    \"\"\"JSON data returned from a component that should not be modified further.\"\"\"",
            "",
            "    root: JsonValue",
            "",
            "",
            "class GradioModel(GradioBaseModel, BaseModel):",
            "    @classmethod",
            "    def from_json(cls, x) -> GradioModel:",
            "        return cls(**x)",
            "",
            "",
            "class GradioRootModel(GradioBaseModel, RootModel):",
            "    @classmethod",
            "    def from_json(cls, x) -> GradioRootModel:",
            "        return cls(root=x)",
            "",
            "",
            "GradioDataModel = Union[GradioModel, GradioRootModel]",
            "",
            "",
            "class FileDataDict(TypedDict):",
            "    path: str  # server filepath",
            "    url: Optional[str]  # normalised server url",
            "    size: Optional[int]  # size in bytes",
            "    orig_name: Optional[str]  # original filename",
            "    mime_type: Optional[str]",
            "    is_stream: bool",
            "    meta: dict",
            "",
            "",
            "@document()",
            "class FileData(GradioModel):",
            "    \"\"\"",
            "    The FileData class is a subclass of the GradioModel class that represents a file object within a Gradio interface. It is used to store file data and metadata when a file is uploaded.",
            "",
            "    Attributes:",
            "        path: The server file path where the file is stored.",
            "        url: The normalized server URL pointing to the file.",
            "        size: The size of the file in bytes.",
            "        orig_name: The original filename before upload.",
            "        mime_type: The MIME type of the file.",
            "        is_stream: Indicates whether the file is a stream.",
            "        meta: Additional metadata used internally (should not be changed).",
            "    \"\"\"",
            "",
            "    path: str  # server filepath",
            "    url: Optional[str] = None  # normalised server url",
            "    size: Optional[int] = None  # size in bytes",
            "    orig_name: Optional[str] = None  # original filename",
            "    mime_type: Optional[str] = None",
            "    is_stream: bool = False",
            "    meta: dict = {\"_type\": \"gradio.FileData\"}",
            "",
            "    @property",
            "    def is_none(self) -> bool:",
            "        \"\"\"",
            "        Checks if the FileData object is empty, i.e., all attributes are None.",
            "",
            "        Returns:",
            "            bool: True if all attributes (except 'is_stream' and 'meta') are None, False otherwise.",
            "        \"\"\"",
            "        return all(",
            "            f is None",
            "            for f in [",
            "                self.path,",
            "                self.url,",
            "                self.size,",
            "                self.orig_name,",
            "                self.mime_type,",
            "            ]",
            "        )",
            "",
            "    @classmethod",
            "    def from_path(cls, path: str) -> FileData:",
            "        \"\"\"",
            "        Creates a FileData object from a given file path.",
            "",
            "        Args:",
            "            path: The file path.",
            "",
            "        Returns:",
            "            FileData: An instance of FileData representing the file at the specified path.",
            "        \"\"\"",
            "        return cls(path=path)",
            "",
            "    def _copy_to_dir(self, dir: str) -> FileData:",
            "        \"\"\"",
            "        Copies the file to a specified directory and returns a new FileData object representing the copied file.",
            "",
            "        Args:",
            "            dir: The destination directory.",
            "",
            "        Returns:",
            "            FileData: A new FileData object representing the copied file.",
            "",
            "        Raises:",
            "            ValueError: If the source file path is not set.",
            "        \"\"\"",
            "        pathlib.Path(dir).mkdir(exist_ok=True)",
            "        new_obj = dict(self)",
            "",
            "        if not self.path:",
            "            raise ValueError(\"Source file path is not set\")",
            "        new_name = shutil.copy(self.path, dir)",
            "        new_obj[\"path\"] = new_name",
            "        return self.__class__(**new_obj)",
            "",
            "    @classmethod",
            "    def is_file_data(cls, obj: Any) -> bool:",
            "        \"\"\"",
            "        Checks if an object is a valid FileData instance.",
            "",
            "        Args:",
            "            obj: The object to check.",
            "",
            "        Returns:",
            "            bool: True if the object is a valid FileData instance, False otherwise.",
            "        \"\"\"",
            "        if isinstance(obj, dict):",
            "            try:",
            "                return not FileData(**obj).is_none",
            "            except (TypeError, ValidationError):",
            "                return False",
            "        return False",
            "",
            "",
            "class ListFiles(GradioRootModel):",
            "    root: List[FileData]",
            "",
            "    def __getitem__(self, index):",
            "        return self.root[index]",
            "",
            "    def __iter__(self) -> Iterator[FileData]:  # type: ignore[override]",
            "        return iter(self.root)",
            "",
            "",
            "class _StaticFiles:",
            "    \"\"\"",
            "    Class to hold all static files for an app",
            "    \"\"\"",
            "",
            "    all_paths = []",
            "",
            "    def __init__(self, paths: list[str | pathlib.Path]) -> None:",
            "        self.paths = paths",
            "        self.all_paths = [pathlib.Path(p).resolve() for p in paths]",
            "",
            "    @classmethod",
            "    def clear(cls):",
            "        cls.all_paths = []",
            "",
            "",
            "class BodyCSS(TypedDict):",
            "    body_background_fill: str",
            "    body_text_color: str",
            "    body_background_fill_dark: str",
            "    body_text_color_dark: str",
            "",
            "",
            "class Layout(TypedDict):",
            "    id: int",
            "    children: list[int | Layout]",
            "",
            "",
            "class BlocksConfigDict(TypedDict):",
            "    version: str",
            "    mode: str",
            "    app_id: int",
            "    dev_mode: bool",
            "    analytics_enabled: bool",
            "    components: list[dict[str, Any]]",
            "    css: str | None",
            "    connect_heartbeat: bool",
            "    js: str | None",
            "    head: str | None",
            "    title: str",
            "    space_id: str | None",
            "    enable_queue: bool",
            "    show_error: bool",
            "    show_api: bool",
            "    is_colab: bool",
            "    max_file_size: int | None",
            "    stylesheets: list[str]",
            "    theme: str | None",
            "    protocol: Literal[\"ws\", \"sse\", \"sse_v1\", \"sse_v2\", \"sse_v2.1\", \"sse_v3\"]",
            "    body_css: BodyCSS",
            "    fill_height: bool",
            "    fill_width: bool",
            "    theme_hash: str",
            "    layout: NotRequired[Layout]",
            "    dependencies: NotRequired[list[dict[str, Any]]]",
            "    root: NotRequired[str | None]",
            "    username: NotRequired[str | None]"
        ],
        "afterPatchFile": [
            "\"\"\"Pydantic data models and other dataclasses. This is the only file that uses Optional[]",
            "typing syntax instead of | None syntax to work with pydantic\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import pathlib",
            "import secrets",
            "import shutil",
            "from abc import ABC, abstractmethod",
            "from enum import Enum, auto",
            "from typing import (",
            "    Any,",
            "    Iterator,",
            "    List,",
            "    Literal,",
            "    NewType,",
            "    Optional,",
            "    Tuple,",
            "    TypedDict,",
            "    Union,",
            ")",
            "",
            "from fastapi import Request",
            "from gradio_client.documentation import document",
            "from gradio_client.utils import traverse",
            "from pydantic import (",
            "    BaseModel,",
            "    GetCoreSchemaHandler,",
            "    GetJsonSchemaHandler,",
            "    RootModel,",
            "    ValidationError,",
            ")",
            "from pydantic.json_schema import JsonSchemaValue",
            "from pydantic_core import core_schema",
            "from typing_extensions import Annotated, NotRequired",
            "",
            "try:",
            "    from pydantic import JsonValue",
            "except ImportError:",
            "    JsonValue = Any",
            "",
            "DeveloperPath = NewType(\"DeveloperPath\", str)",
            "UserProvidedPath = NewType(\"UserProvidedPath\", str)",
            "",
            "",
            "class CancelBody(BaseModel):",
            "    session_hash: str",
            "    fn_index: int",
            "    event_id: str",
            "",
            "",
            "class SimplePredictBody(BaseModel):",
            "    data: List[Any]",
            "    session_hash: Optional[str] = None",
            "",
            "",
            "class _StarletteRequestPydanticAnnotation:",
            "    @classmethod",
            "    def __get_pydantic_core_schema__(",
            "        cls,",
            "        _source_type: Any,",
            "        _handler: GetCoreSchemaHandler,",
            "    ) -> core_schema.CoreSchema:",
            "        def validate_request(value: Any) -> Request:",
            "            if isinstance(value, Request):",
            "                return value",
            "            raise ValueError(\"Input must be a Starlette Request object\")",
            "",
            "        return core_schema.no_info_plain_validator_function(validate_request)",
            "",
            "    @classmethod",
            "    def __get_pydantic_json_schema__(",
            "        cls, _core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler",
            "    ) -> JsonSchemaValue:",
            "        return {\"type\": \"object\", \"title\": \"StarletteRequest\"}",
            "",
            "",
            "PydanticStarletteRequest = Annotated[Request, _StarletteRequestPydanticAnnotation]",
            "",
            "",
            "class PredictBody(BaseModel):",
            "    session_hash: Optional[str] = None",
            "    event_id: Optional[str] = None",
            "    data: List[Any]",
            "    event_data: Optional[Any] = None",
            "    fn_index: Optional[int] = None",
            "    trigger_id: Optional[int] = None",
            "    simple_format: bool = False",
            "    batched: Optional[bool] = (",
            "        False  # Whether the data is a batch of samples (i.e. called from the queue if batch=True) or a single sample (i.e. called from the UI)",
            "    )",
            "",
            "    @classmethod",
            "    def __get_pydantic_json_schema__(cls, core_schema, handler):",
            "        return {",
            "            \"title\": \"PredictBody\",",
            "            \"type\": \"object\",",
            "            \"properties\": {",
            "                \"session_hash\": {\"type\": \"string\"},",
            "                \"event_id\": {\"type\": \"string\"},",
            "                \"data\": {\"type\": \"array\", \"items\": {\"type\": \"object\"}},",
            "                \"event_data\": {\"type\": \"object\"},",
            "                \"fn_index\": {\"type\": \"integer\"},",
            "                \"trigger_id\": {\"type\": \"integer\"},",
            "                \"simple_format\": {\"type\": \"boolean\"},",
            "                \"batched\": {\"type\": \"boolean\"},",
            "            },",
            "            \"required\": [\"data\"],",
            "        }",
            "",
            "",
            "class PredictBodyInternal(PredictBody):",
            "    \"Separate class to avoid exposing PydanticStarletteRequest in the API validation\"",
            "",
            "    request: Optional[PydanticStarletteRequest] = (",
            "        None  # dictionary of request headers, query parameters, url, etc. (used to to pass in request for queuing)",
            "    )",
            "",
            "",
            "class ResetBody(BaseModel):",
            "    event_id: str",
            "",
            "",
            "class ComponentServerJSONBody(BaseModel):",
            "    session_hash: str",
            "    component_id: int",
            "    fn_name: str",
            "    data: Any",
            "",
            "",
            "class DataWithFiles(BaseModel):",
            "    data: Any",
            "    files: List[Tuple[str, bytes]]",
            "",
            "",
            "class ComponentServerBlobBody(BaseModel):",
            "    session_hash: str",
            "    component_id: int",
            "    fn_name: str",
            "    data: DataWithFiles",
            "",
            "",
            "class InterfaceTypes(Enum):",
            "    STANDARD = auto()",
            "    INPUT_ONLY = auto()",
            "    OUTPUT_ONLY = auto()",
            "    UNIFIED = auto()",
            "",
            "",
            "class GradioBaseModel(ABC):",
            "    def copy_to_dir(self, dir: str | pathlib.Path) -> GradioDataModel:",
            "        if not isinstance(self, (BaseModel, RootModel)):",
            "            raise TypeError(\"must be used in a Pydantic model\")",
            "        dir = pathlib.Path(dir)",
            "",
            "        # TODO: Making sure path is unique should be done in caller",
            "        def unique_copy(obj: dict):",
            "            data = FileData(**obj)",
            "            return data._copy_to_dir(",
            "                str(pathlib.Path(dir / secrets.token_hex(10)))",
            "            ).model_dump()",
            "",
            "        return self.__class__.from_json(",
            "            x=traverse(",
            "                self.model_dump(),",
            "                unique_copy,",
            "                FileData.is_file_data,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    @abstractmethod",
            "    def from_json(cls, x) -> GradioDataModel:",
            "        pass",
            "",
            "",
            "class JsonData(RootModel):",
            "    \"\"\"JSON data returned from a component that should not be modified further.\"\"\"",
            "",
            "    root: JsonValue",
            "",
            "",
            "class GradioModel(GradioBaseModel, BaseModel):",
            "    @classmethod",
            "    def from_json(cls, x) -> GradioModel:",
            "        return cls(**x)",
            "",
            "",
            "class GradioRootModel(GradioBaseModel, RootModel):",
            "    @classmethod",
            "    def from_json(cls, x) -> GradioRootModel:",
            "        return cls(root=x)",
            "",
            "",
            "GradioDataModel = Union[GradioModel, GradioRootModel]",
            "",
            "",
            "class FileDataDict(TypedDict):",
            "    path: str  # server filepath",
            "    url: Optional[str]  # normalised server url",
            "    size: Optional[int]  # size in bytes",
            "    orig_name: Optional[str]  # original filename",
            "    mime_type: Optional[str]",
            "    is_stream: bool",
            "    meta: dict",
            "",
            "",
            "@document()",
            "class FileData(GradioModel):",
            "    \"\"\"",
            "    The FileData class is a subclass of the GradioModel class that represents a file object within a Gradio interface. It is used to store file data and metadata when a file is uploaded.",
            "",
            "    Attributes:",
            "        path: The server file path where the file is stored.",
            "        url: The normalized server URL pointing to the file.",
            "        size: The size of the file in bytes.",
            "        orig_name: The original filename before upload.",
            "        mime_type: The MIME type of the file.",
            "        is_stream: Indicates whether the file is a stream.",
            "        meta: Additional metadata used internally (should not be changed).",
            "    \"\"\"",
            "",
            "    path: str  # server filepath",
            "    url: Optional[str] = None  # normalised server url",
            "    size: Optional[int] = None  # size in bytes",
            "    orig_name: Optional[str] = None  # original filename",
            "    mime_type: Optional[str] = None",
            "    is_stream: bool = False",
            "    meta: dict = {\"_type\": \"gradio.FileData\"}",
            "",
            "    @property",
            "    def is_none(self) -> bool:",
            "        \"\"\"",
            "        Checks if the FileData object is empty, i.e., all attributes are None.",
            "",
            "        Returns:",
            "            bool: True if all attributes (except 'is_stream' and 'meta') are None, False otherwise.",
            "        \"\"\"",
            "        return all(",
            "            f is None",
            "            for f in [",
            "                self.path,",
            "                self.url,",
            "                self.size,",
            "                self.orig_name,",
            "                self.mime_type,",
            "            ]",
            "        )",
            "",
            "    @classmethod",
            "    def from_path(cls, path: str) -> FileData:",
            "        \"\"\"",
            "        Creates a FileData object from a given file path.",
            "",
            "        Args:",
            "            path: The file path.",
            "",
            "        Returns:",
            "            FileData: An instance of FileData representing the file at the specified path.",
            "        \"\"\"",
            "        return cls(path=path)",
            "",
            "    def _copy_to_dir(self, dir: str) -> FileData:",
            "        \"\"\"",
            "        Copies the file to a specified directory and returns a new FileData object representing the copied file.",
            "",
            "        Args:",
            "            dir: The destination directory.",
            "",
            "        Returns:",
            "            FileData: A new FileData object representing the copied file.",
            "",
            "        Raises:",
            "            ValueError: If the source file path is not set.",
            "        \"\"\"",
            "        pathlib.Path(dir).mkdir(exist_ok=True)",
            "        new_obj = dict(self)",
            "",
            "        if not self.path:",
            "            raise ValueError(\"Source file path is not set\")",
            "        new_name = shutil.copy(self.path, dir)",
            "        new_obj[\"path\"] = new_name",
            "        return self.__class__(**new_obj)",
            "",
            "    @classmethod",
            "    def is_file_data(cls, obj: Any) -> bool:",
            "        \"\"\"",
            "        Checks if an object is a valid FileData instance.",
            "",
            "        Args:",
            "            obj: The object to check.",
            "",
            "        Returns:",
            "            bool: True if the object is a valid FileData instance, False otherwise.",
            "        \"\"\"",
            "        if isinstance(obj, dict):",
            "            try:",
            "                return not FileData(**obj).is_none",
            "            except (TypeError, ValidationError):",
            "                return False",
            "        return False",
            "",
            "",
            "class ListFiles(GradioRootModel):",
            "    root: List[FileData]",
            "",
            "    def __getitem__(self, index):",
            "        return self.root[index]",
            "",
            "    def __iter__(self) -> Iterator[FileData]:  # type: ignore[override]",
            "        return iter(self.root)",
            "",
            "",
            "class _StaticFiles:",
            "    \"\"\"",
            "    Class to hold all static files for an app",
            "    \"\"\"",
            "",
            "    all_paths = []",
            "",
            "    def __init__(self, paths: list[str | pathlib.Path]) -> None:",
            "        self.paths = paths",
            "        self.all_paths = [pathlib.Path(p).resolve() for p in paths]",
            "",
            "    @classmethod",
            "    def clear(cls):",
            "        cls.all_paths = []",
            "",
            "",
            "class BodyCSS(TypedDict):",
            "    body_background_fill: str",
            "    body_text_color: str",
            "    body_background_fill_dark: str",
            "    body_text_color_dark: str",
            "",
            "",
            "class Layout(TypedDict):",
            "    id: int",
            "    children: list[int | Layout]",
            "",
            "",
            "class BlocksConfigDict(TypedDict):",
            "    version: str",
            "    mode: str",
            "    app_id: int",
            "    dev_mode: bool",
            "    analytics_enabled: bool",
            "    components: list[dict[str, Any]]",
            "    css: str | None",
            "    connect_heartbeat: bool",
            "    js: str | None",
            "    head: str | None",
            "    title: str",
            "    space_id: str | None",
            "    enable_queue: bool",
            "    show_error: bool",
            "    show_api: bool",
            "    is_colab: bool",
            "    max_file_size: int | None",
            "    stylesheets: list[str]",
            "    theme: str | None",
            "    protocol: Literal[\"ws\", \"sse\", \"sse_v1\", \"sse_v2\", \"sse_v2.1\", \"sse_v3\"]",
            "    body_css: BodyCSS",
            "    fill_height: bool",
            "    fill_width: bool",
            "    theme_hash: str",
            "    layout: NotRequired[Layout]",
            "    dependencies: NotRequired[list[dict[str, Any]]]",
            "    root: NotRequired[str | None]",
            "    username: NotRequired[str | None]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "26": [],
            "27": [],
            "49": [
                "PredictBody"
            ],
            "50": [
                "PredictBody"
            ],
            "62": [
                "PredictBody"
            ],
            "63": [
                "PredictBody"
            ],
            "64": [
                "PredictBody"
            ],
            "80": [
                "PredictBody",
                "__get_pydantic_json_schema__"
            ]
        },
        "addLocation": []
    },
    "gradio/queueing.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from gradio import route_utils, routes"
            },
            "2": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from gradio.data_classes import ("
            },
            "3": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    PredictBody,"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+    PredictBodyInternal,"
            },
            "5": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " )"
            },
            "6": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from gradio.helpers import TrackedIterable"
            },
            "7": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from gradio.server_messages import ("
            },
            "8": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "         self.request = request"
            },
            "9": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "         self.username = username"
            },
            "10": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "         self.concurrency_id = fn.concurrency_id"
            },
            "11": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.data: PredictBody | None = None"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+        self.data: PredictBodyInternal | None = None"
            },
            "13": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "         self.progress: ProgressMessage | None = None"
            },
            "14": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "         self.progress_pending: bool = False"
            },
            "15": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "         self.alive = True"
            },
            "16": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 192,
                "PatchRowcode": "         return total_len"
            },
            "17": {
                "beforePatchRowNumber": 193,
                "afterPatchRowNumber": 193,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 194,
                "afterPatchRowNumber": 194,
                "PatchRowcode": "     async def push("
            },
            "19": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self, body: PredictBody, request: fastapi.Request, username: str | None"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+        self, body: PredictBodyInternal, request: fastapi.Request, username: str | None"
            },
            "21": {
                "beforePatchRowNumber": 196,
                "afterPatchRowNumber": 196,
                "PatchRowcode": "     ) -> tuple[bool, str]:"
            },
            "22": {
                "beforePatchRowNumber": 197,
                "afterPatchRowNumber": 197,
                "PatchRowcode": "         if body.fn_index is None:"
            },
            "23": {
                "beforePatchRowNumber": 198,
                "afterPatchRowNumber": 198,
                "PatchRowcode": "             return False, \"No function index provided.\""
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import copy",
            "import os",
            "import random",
            "import time",
            "import traceback",
            "import uuid",
            "from collections import defaultdict",
            "from queue import Queue as ThreadQueue",
            "from typing import TYPE_CHECKING",
            "",
            "import fastapi",
            "from typing_extensions import Literal",
            "",
            "from gradio import route_utils, routes",
            "from gradio.data_classes import (",
            "    PredictBody,",
            ")",
            "from gradio.helpers import TrackedIterable",
            "from gradio.server_messages import (",
            "    EstimationMessage,",
            "    EventMessage,",
            "    LogMessage,",
            "    ProcessCompletedMessage,",
            "    ProcessGeneratingMessage,",
            "    ProcessStartsMessage,",
            "    ProgressMessage,",
            "    ProgressUnit,",
            ")",
            "from gradio.utils import (",
            "    LRUCache,",
            "    error_payload,",
            "    run_coro_in_background,",
            "    safe_get_lock,",
            "    set_task_name,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import BlockFunction, Blocks",
            "",
            "",
            "class Event:",
            "    def __init__(",
            "        self,",
            "        session_hash: str | None,",
            "        fn: BlockFunction,",
            "        request: fastapi.Request,",
            "        username: str | None,",
            "    ):",
            "        self._id = uuid.uuid4().hex",
            "        self.session_hash: str = session_hash or self._id",
            "        self.fn = fn",
            "        self.request = request",
            "        self.username = username",
            "        self.concurrency_id = fn.concurrency_id",
            "        self.data: PredictBody | None = None",
            "        self.progress: ProgressMessage | None = None",
            "        self.progress_pending: bool = False",
            "        self.alive = True",
            "",
            "",
            "class EventQueue:",
            "    def __init__(self, concurrency_id: str, concurrency_limit: int | None):",
            "        self.queue: list[Event] = []",
            "        self.concurrency_id = concurrency_id",
            "        self.concurrency_limit = concurrency_limit",
            "        self.current_concurrency = 0",
            "        self.start_times_per_fn: defaultdict[BlockFunction, set[float]] = defaultdict(",
            "            set",
            "        )",
            "",
            "",
            "class ProcessTime:",
            "    def __init__(self):",
            "        self.process_time = 0",
            "        self.count = 0",
            "        self.avg_time = 0",
            "",
            "    def add(self, time: float):",
            "        self.process_time += time",
            "        self.count += 1",
            "        self.avg_time = self.process_time / self.count",
            "",
            "",
            "class Queue:",
            "    def __init__(",
            "        self,",
            "        live_updates: bool,",
            "        concurrency_count: int,",
            "        update_intervals: float,",
            "        max_size: int | None,",
            "        blocks: Blocks,",
            "        default_concurrency_limit: int | None | Literal[\"not_set\"] = \"not_set\",",
            "    ):",
            "        self.pending_messages_per_session: LRUCache[str, ThreadQueue[EventMessage]] = (",
            "            LRUCache(2000)",
            "        )",
            "        self.pending_event_ids_session: dict[str, set[str]] = {}",
            "        self.pending_message_lock = safe_get_lock()",
            "        self.event_queue_per_concurrency_id: dict[str, EventQueue] = {}",
            "        self.stopped = False",
            "        self.max_thread_count = concurrency_count",
            "        self.update_intervals = update_intervals",
            "        self.active_jobs: list[None | list[Event]] = []",
            "        self.delete_lock = safe_get_lock()",
            "        self.server_app = None",
            "        self.process_time_per_fn: defaultdict[BlockFunction, ProcessTime] = defaultdict(",
            "            ProcessTime",
            "        )",
            "        self.live_updates = live_updates",
            "        self.sleep_when_free = 0.05",
            "        self.progress_update_sleep_when_free = 0.1",
            "        self.max_size = max_size",
            "        self.blocks = blocks",
            "        self._asyncio_tasks: list[asyncio.Task] = []",
            "        self.default_concurrency_limit = self._resolve_concurrency_limit(",
            "            default_concurrency_limit",
            "        )",
            "        self.event_analytics: dict[str, dict[str, float | str | None]] = {}",
            "",
            "    def start(self):",
            "        self.active_jobs = [None] * self.max_thread_count",
            "",
            "        run_coro_in_background(self.start_processing)",
            "        run_coro_in_background(self.start_progress_updates)",
            "        if not self.live_updates:",
            "            run_coro_in_background(self.notify_clients)",
            "",
            "    def create_event_queue_for_fn(self, block_fn: BlockFunction):",
            "        concurrency_id = block_fn.concurrency_id",
            "        concurrency_limit: int | None",
            "        if block_fn.concurrency_limit == \"default\":",
            "            concurrency_limit = self.default_concurrency_limit",
            "        else:",
            "            concurrency_limit = block_fn.concurrency_limit",
            "        if concurrency_id not in self.event_queue_per_concurrency_id:",
            "            self.event_queue_per_concurrency_id[concurrency_id] = EventQueue(",
            "                concurrency_id, concurrency_limit",
            "            )",
            "        elif (",
            "            concurrency_limit is not None",
            "        ):  # Update concurrency limit if it is lower than existing limit",
            "            existing_event_queue = self.event_queue_per_concurrency_id[concurrency_id]",
            "            if (",
            "                existing_event_queue.concurrency_limit is None",
            "                or concurrency_limit < existing_event_queue.concurrency_limit",
            "            ):",
            "                existing_event_queue.concurrency_limit = concurrency_limit",
            "",
            "    def close(self):",
            "        self.stopped = True",
            "",
            "    def send_message(",
            "        self,",
            "        event: Event,",
            "        event_message: EventMessage,",
            "    ):",
            "        if not event.alive:",
            "            return",
            "        event_message.event_id = event._id",
            "        messages = self.pending_messages_per_session[event.session_hash]",
            "        messages.put_nowait(event_message)",
            "",
            "    def _resolve_concurrency_limit(",
            "        self, default_concurrency_limit: int | None | Literal[\"not_set\"]",
            "    ) -> int | None:",
            "        \"\"\"",
            "        Handles the logic of resolving the default_concurrency_limit as this can be specified via a combination",
            "        of the `default_concurrency_limit` parameter of the `Blocks.queue()` or the `GRADIO_DEFAULT_CONCURRENCY_LIMIT`",
            "        environment variable. The parameter in `Blocks.queue()` takes precedence over the environment variable.",
            "        Parameters:",
            "            default_concurrency_limit: The default concurrency limit, as specified by a user in `Blocks.queu()`.",
            "        \"\"\"",
            "        if default_concurrency_limit != \"not_set\":",
            "            return default_concurrency_limit",
            "        if default_concurrency_limit_env := os.environ.get(",
            "            \"GRADIO_DEFAULT_CONCURRENCY_LIMIT\"",
            "        ):",
            "            if default_concurrency_limit_env.lower() == \"none\":",
            "                return None",
            "            else:",
            "                return int(default_concurrency_limit_env)",
            "        else:",
            "            return 1",
            "",
            "    def __len__(self):",
            "        total_len = 0",
            "        for event_queue in self.event_queue_per_concurrency_id.values():",
            "            total_len += len(event_queue.queue)",
            "        return total_len",
            "",
            "    async def push(",
            "        self, body: PredictBody, request: fastapi.Request, username: str | None",
            "    ) -> tuple[bool, str]:",
            "        if body.fn_index is None:",
            "            return False, \"No function index provided.\"",
            "        if self.max_size is not None and len(self) >= self.max_size:",
            "            return (",
            "                False,",
            "                f\"Queue is full. Max size is {self.max_size} and size is {len(self)}.\",",
            "            )",
            "",
            "        if body.session_hash:",
            "            session_state = self.blocks.state_holder[body.session_hash]",
            "            fn = session_state.blocks_config.fns[body.fn_index]",
            "        else:",
            "            fn = self.blocks.fns[body.fn_index]",
            "",
            "        fn = route_utils.get_fn(self.blocks, None, body)",
            "        self.create_event_queue_for_fn(fn)",
            "        event = Event(",
            "            body.session_hash,",
            "            fn,",
            "            request,",
            "            username,",
            "        )",
            "        event.data = body",
            "        if body.session_hash is None:",
            "            body.session_hash = event.session_hash",
            "        async with self.pending_message_lock:",
            "            if body.session_hash not in self.pending_messages_per_session:",
            "                self.pending_messages_per_session[body.session_hash] = ThreadQueue()",
            "            if body.session_hash not in self.pending_event_ids_session:",
            "                self.pending_event_ids_session[body.session_hash] = set()",
            "        self.pending_event_ids_session[body.session_hash].add(event._id)",
            "        try:",
            "            event_queue = self.event_queue_per_concurrency_id[event.concurrency_id]",
            "        except KeyError as e:",
            "            raise KeyError(",
            "                \"Event not found in queue. If you are deploying this Gradio app with multiple replicas, please enable stickiness to ensure that all requests from the same user are routed to the same instance.\"",
            "            ) from e",
            "        event_queue.queue.append(event)",
            "        self.event_analytics[event._id] = {",
            "            \"time\": time.time(),",
            "            \"status\": \"queued\",",
            "            \"process_time\": None,",
            "            \"function\": fn.api_name,",
            "            \"session_hash\": body.session_hash,",
            "        }",
            "",
            "        self.broadcast_estimations(event.concurrency_id, len(event_queue.queue) - 1)",
            "",
            "        return True, event._id",
            "",
            "    def _cancel_asyncio_tasks(self):",
            "        for task in self._asyncio_tasks:",
            "            task.cancel()",
            "        self._asyncio_tasks = []",
            "",
            "    def set_server_app(self, app: routes.App):",
            "        self.server_app = app",
            "",
            "    def get_active_worker_count(self) -> int:",
            "        count = 0",
            "        for worker in self.active_jobs:",
            "            if worker is not None:",
            "                count += 1",
            "        return count",
            "",
            "    def get_events(self) -> tuple[list[Event], bool, str] | None:",
            "        concurrency_ids = list(self.event_queue_per_concurrency_id.keys())",
            "        random.shuffle(concurrency_ids)",
            "        for concurrency_id in concurrency_ids:",
            "            event_queue = self.event_queue_per_concurrency_id[concurrency_id]",
            "            if len(event_queue.queue) and (",
            "                event_queue.concurrency_limit is None",
            "                or event_queue.current_concurrency < event_queue.concurrency_limit",
            "            ):",
            "                first_event = event_queue.queue[0]",
            "                block_fn = first_event.fn",
            "                events = [first_event]",
            "                batch = block_fn.batch",
            "                if batch:",
            "                    events += [",
            "                        event",
            "                        for event in event_queue.queue[1:]",
            "                        if event.fn == first_event.fn",
            "                    ][: block_fn.max_batch_size - 1]",
            "",
            "                for event in events:",
            "                    event_queue.queue.remove(event)",
            "",
            "                return events, batch, concurrency_id",
            "",
            "    async def start_processing(self) -> None:",
            "        try:",
            "            while not self.stopped:",
            "                if len(self) == 0:",
            "                    await asyncio.sleep(self.sleep_when_free)",
            "                    continue",
            "",
            "                if None not in self.active_jobs:",
            "                    await asyncio.sleep(self.sleep_when_free)",
            "                    continue",
            "",
            "                # Using mutex to avoid editing a list in use",
            "                async with self.delete_lock:",
            "                    event_batch = self.get_events()",
            "",
            "                if event_batch:",
            "                    events, batch, concurrency_id = event_batch",
            "                    self.active_jobs[self.active_jobs.index(None)] = events",
            "                    event_queue = self.event_queue_per_concurrency_id[concurrency_id]",
            "                    event_queue.current_concurrency += 1",
            "                    start_time = time.time()",
            "                    event_queue.start_times_per_fn[events[0].fn].add(start_time)",
            "                    for event in events:",
            "                        self.event_analytics[event._id][\"status\"] = \"processing\"",
            "                    process_event_task = run_coro_in_background(",
            "                        self.process_events, events, batch, start_time",
            "                    )",
            "                    set_task_name(",
            "                        process_event_task,",
            "                        events[0].session_hash,",
            "                        events[0].fn._id,",
            "                        events[0]._id,",
            "                        batch,",
            "                    )",
            "",
            "                    self._asyncio_tasks.append(process_event_task)",
            "                    if self.live_updates:",
            "                        self.broadcast_estimations(concurrency_id)",
            "                else:",
            "                    await asyncio.sleep(self.sleep_when_free)",
            "        finally:",
            "            self.stopped = True",
            "            self._cancel_asyncio_tasks()",
            "",
            "    async def start_progress_updates(self) -> None:",
            "        \"\"\"",
            "        Because progress updates can be very frequent, we do not necessarily want to send a message per update.",
            "        Rather, we check for progress updates at regular intervals, and send a message if there is a pending update.",
            "        Consecutive progress updates between sends will overwrite each other so only the most recent update will be sent.",
            "        \"\"\"",
            "        while not self.stopped:",
            "            events = [evt for job in self.active_jobs if job is not None for evt in job]",
            "",
            "            if len(events) == 0:",
            "                await asyncio.sleep(self.progress_update_sleep_when_free)",
            "                continue",
            "",
            "            for event in events:",
            "                if event.progress_pending and event.progress:",
            "                    event.progress_pending = False",
            "                    self.send_message(event, event.progress)",
            "",
            "            await asyncio.sleep(self.progress_update_sleep_when_free)",
            "",
            "    def set_progress(",
            "        self,",
            "        event_id: str,",
            "        iterables: list[TrackedIterable] | None,",
            "    ):",
            "        if iterables is None:",
            "            return",
            "        for job in self.active_jobs:",
            "            if job is None:",
            "                continue",
            "            for evt in job:",
            "                if evt._id == event_id:",
            "                    progress_data: list[ProgressUnit] = []",
            "                    for iterable in iterables:",
            "                        progress_unit = ProgressUnit(",
            "                            index=iterable.index,",
            "                            length=iterable.length,",
            "                            unit=iterable.unit,",
            "                            progress=iterable.progress,",
            "                            desc=iterable.desc,",
            "                        )",
            "                        progress_data.append(progress_unit)",
            "                    evt.progress = ProgressMessage(progress_data=progress_data)",
            "                    evt.progress_pending = True",
            "",
            "    def log_message(",
            "        self,",
            "        event_id: str,",
            "        log: str,",
            "        level: Literal[\"info\", \"warning\"],",
            "        duration: float | None = 10,",
            "        visible: bool = True,",
            "    ):",
            "        events = [evt for job in self.active_jobs if job is not None for evt in job]",
            "        for event in events:",
            "            if event._id == event_id:",
            "                log_message = LogMessage(",
            "                    log=log,",
            "                    level=level,",
            "                    duration=duration,",
            "                    visible=visible,",
            "                )",
            "                self.send_message(event, log_message)",
            "",
            "    async def clean_events(",
            "        self, *, session_hash: str | None = None, event_id: str | None = None",
            "    ) -> None:",
            "        for job_set in self.active_jobs:",
            "            if job_set:",
            "                for job in job_set:",
            "                    if job.session_hash == session_hash or job._id == event_id:",
            "                        job.alive = False",
            "",
            "        async with self.delete_lock:",
            "            events_to_remove: list[Event] = []",
            "            for event_queue in self.event_queue_per_concurrency_id.values():",
            "                for event in event_queue.queue:",
            "                    if event.session_hash == session_hash or event._id == event_id:",
            "                        events_to_remove.append(event)",
            "",
            "            for event in events_to_remove:",
            "                self.event_queue_per_concurrency_id[event.concurrency_id].queue.remove(",
            "                    event",
            "                )",
            "",
            "    async def notify_clients(self) -> None:",
            "        \"\"\"",
            "        Notify clients about events statuses in the queue periodically.",
            "        \"\"\"",
            "        while not self.stopped:",
            "            await asyncio.sleep(self.update_intervals)",
            "            if len(self) > 0:",
            "                for concurrency_id in self.event_queue_per_concurrency_id:",
            "                    self.broadcast_estimations(concurrency_id)",
            "",
            "    def broadcast_estimations(",
            "        self, concurrency_id: str, after: int | None = None",
            "    ) -> None:",
            "        wait_so_far = 0",
            "        event_queue = self.event_queue_per_concurrency_id[concurrency_id]",
            "        time_till_available_worker: int | None = 0",
            "",
            "        if event_queue.current_concurrency == event_queue.concurrency_limit:",
            "            expected_end_times = []",
            "            for fn, start_times in event_queue.start_times_per_fn.items():",
            "                if fn not in self.process_time_per_fn:",
            "                    time_till_available_worker = None",
            "                    break",
            "                process_time = self.process_time_per_fn[fn].avg_time",
            "                expected_end_times += [",
            "                    start_time + process_time for start_time in start_times",
            "                ]",
            "            if time_till_available_worker is not None and len(expected_end_times) > 0:",
            "                time_of_first_completion = min(expected_end_times)",
            "                time_till_available_worker = max(",
            "                    time_of_first_completion - time.time(), 0",
            "                )",
            "",
            "        for rank, event in enumerate(event_queue.queue):",
            "            process_time_for_fn = (",
            "                self.process_time_per_fn[event.fn].avg_time",
            "                if event.fn in self.process_time_per_fn",
            "                else None",
            "            )",
            "            rank_eta = (",
            "                process_time_for_fn + wait_so_far + time_till_available_worker",
            "                if process_time_for_fn is not None",
            "                and wait_so_far is not None",
            "                and time_till_available_worker is not None",
            "                else None",
            "            )",
            "",
            "            if after is None or rank >= after:",
            "                self.send_message(",
            "                    event,",
            "                    EstimationMessage(",
            "                        rank=rank, rank_eta=rank_eta, queue_size=len(event_queue.queue)",
            "                    ),",
            "                )",
            "            if event_queue.concurrency_limit is None:",
            "                wait_so_far = 0",
            "            elif wait_so_far is not None and process_time_for_fn is not None:",
            "                wait_so_far += process_time_for_fn / event_queue.concurrency_limit",
            "            else:",
            "                wait_so_far = None",
            "",
            "    def get_status(self) -> EstimationMessage:",
            "        return EstimationMessage(",
            "            queue_size=len(self),",
            "        )",
            "",
            "    async def process_events(",
            "        self, events: list[Event], batch: bool, begin_time: float",
            "    ) -> None:",
            "        awake_events: list[Event] = []",
            "        fn = events[0].fn",
            "        success = False",
            "        try:",
            "            for event in events:",
            "                if event.alive:",
            "                    self.send_message(",
            "                        event,",
            "                        ProcessStartsMessage(",
            "                            eta=self.process_time_per_fn[fn].avg_time",
            "                            if fn in self.process_time_per_fn",
            "                            else None",
            "                        ),",
            "                    )",
            "                    awake_events.append(event)",
            "            if not awake_events:",
            "                return",
            "",
            "            events = awake_events",
            "            body = events[0].data",
            "            if body is None:",
            "                raise ValueError(\"No event data\")",
            "            username = events[0].username",
            "            body.event_id = events[0]._id if not batch else None",
            "            try:",
            "                body.request = events[0].request",
            "            except ValueError:",
            "                pass",
            "",
            "            if batch:",
            "                body.data = list(",
            "                    zip(*[event.data.data for event in events if event.data])",
            "                )",
            "                body.request = events[0].request",
            "                body.batched = True",
            "",
            "            app = self.server_app",
            "            if app is None:",
            "                raise Exception(\"Server app has not been set.\")",
            "",
            "            gr_request = route_utils.compile_gr_request(",
            "                body=body,",
            "                fn=fn,",
            "                username=username,",
            "                request=None,",
            "            )",
            "            assert body.request is not None  # noqa: S101",
            "            root_path = route_utils.get_root_url(",
            "                request=body.request, route_path=\"/queue/join\", root_path=app.root_path",
            "            )",
            "            try:",
            "                response = await route_utils.call_process_api(",
            "                    app=app,",
            "                    body=body,",
            "                    gr_request=gr_request,",
            "                    fn=fn,",
            "                    root_path=root_path,",
            "                )",
            "                err = None",
            "            except Exception as e:",
            "                traceback.print_exc()",
            "                response = None",
            "                err = e",
            "                for event in awake_events:",
            "                    content = error_payload(err, app.get_blocks().show_error)",
            "                    self.send_message(",
            "                        event,",
            "                        ProcessCompletedMessage(",
            "                            output=content,",
            "                            success=False,",
            "                        ),",
            "                    )",
            "            if response and response.get(\"is_generating\", False):",
            "                old_response = response",
            "                old_err = err",
            "                while response and response.get(\"is_generating\", False):",
            "                    old_response = response",
            "                    old_err = err",
            "                    for event in awake_events:",
            "                        self.send_message(",
            "                            event,",
            "                            ProcessGeneratingMessage(",
            "                                output=old_response,",
            "                                success=old_response is not None,",
            "                            ),",
            "                        )",
            "                    awake_events = [event for event in awake_events if event.alive]",
            "                    if not awake_events:",
            "                        return",
            "                    try:",
            "                        response = await route_utils.call_process_api(",
            "                            app=app,",
            "                            body=body,",
            "                            gr_request=gr_request,",
            "                            fn=fn,",
            "                            root_path=root_path,",
            "                        )",
            "                    except Exception as e:",
            "                        traceback.print_exc()",
            "                        response = None",
            "                        err = e",
            "",
            "                if response:",
            "                    success = True",
            "                    output = response",
            "                else:",
            "                    success = False",
            "                    error = err or old_err",
            "                    output = error_payload(error, app.get_blocks().show_error)",
            "                for event in awake_events:",
            "                    self.send_message(",
            "                        event, ProcessCompletedMessage(output=output, success=success)",
            "                    )",
            "",
            "            elif response:",
            "                output = copy.deepcopy(response)",
            "                for e, event in enumerate(awake_events):",
            "                    if batch and \"data\" in output:",
            "                        output[\"data\"] = list(zip(*response.get(\"data\")))[e]",
            "                    success = response is not None",
            "                    self.send_message(",
            "                        event,",
            "                        ProcessCompletedMessage(",
            "                            output=output,",
            "                            success=success,",
            "                        ),",
            "                    )",
            "            end_time = time.time()",
            "            if response is not None:",
            "                duration = end_time - begin_time",
            "                self.process_time_per_fn[events[0].fn].add(duration)",
            "                for event in events:",
            "                    self.event_analytics[event._id][\"process_time\"] = duration",
            "        except Exception as e:",
            "            traceback.print_exc()",
            "        finally:",
            "            event_queue = self.event_queue_per_concurrency_id[events[0].concurrency_id]",
            "            event_queue.current_concurrency -= 1",
            "            start_times = event_queue.start_times_per_fn[fn]",
            "            if begin_time in start_times:",
            "                start_times.remove(begin_time)",
            "            try:",
            "                self.active_jobs[self.active_jobs.index(events)] = None",
            "            except ValueError:",
            "                # `events` can be absent from `self.active_jobs`",
            "                # when this coroutine is called from the `join_queue` endpoint handler in `routes.py`",
            "                # without putting the `events` into `self.active_jobs`.",
            "                # https://github.com/gradio-app/gradio/blob/f09aea34d6bd18c1e2fef80c86ab2476a6d1dd83/gradio/routes.py#L594-L596",
            "                pass",
            "            for event in events:",
            "                # Always reset the state of the iterator",
            "                # If the job finished successfully, this has no effect",
            "                # If the job is cancelled, this will enable future runs",
            "                # to start \"from scratch\"",
            "                await self.reset_iterators(event._id)",
            "",
            "                if event in awake_events:",
            "                    self.event_analytics[event._id][\"status\"] = (",
            "                        \"success\" if success else \"failed\"",
            "                    )",
            "                else:",
            "                    self.event_analytics[event._id][\"status\"] = \"cancelled\"",
            "",
            "    async def reset_iterators(self, event_id: str):",
            "        # Do the same thing as the /reset route",
            "        app = self.server_app",
            "        if app is None:",
            "            raise Exception(\"Server app has not been set.\")",
            "        if event_id not in app.iterators:",
            "            # Failure, but don't raise an error",
            "            return",
            "        async with app.lock:",
            "            del app.iterators[event_id]",
            "            app.iterators_to_reset.add(event_id)",
            "        return"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import copy",
            "import os",
            "import random",
            "import time",
            "import traceback",
            "import uuid",
            "from collections import defaultdict",
            "from queue import Queue as ThreadQueue",
            "from typing import TYPE_CHECKING",
            "",
            "import fastapi",
            "from typing_extensions import Literal",
            "",
            "from gradio import route_utils, routes",
            "from gradio.data_classes import (",
            "    PredictBodyInternal,",
            ")",
            "from gradio.helpers import TrackedIterable",
            "from gradio.server_messages import (",
            "    EstimationMessage,",
            "    EventMessage,",
            "    LogMessage,",
            "    ProcessCompletedMessage,",
            "    ProcessGeneratingMessage,",
            "    ProcessStartsMessage,",
            "    ProgressMessage,",
            "    ProgressUnit,",
            ")",
            "from gradio.utils import (",
            "    LRUCache,",
            "    error_payload,",
            "    run_coro_in_background,",
            "    safe_get_lock,",
            "    set_task_name,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import BlockFunction, Blocks",
            "",
            "",
            "class Event:",
            "    def __init__(",
            "        self,",
            "        session_hash: str | None,",
            "        fn: BlockFunction,",
            "        request: fastapi.Request,",
            "        username: str | None,",
            "    ):",
            "        self._id = uuid.uuid4().hex",
            "        self.session_hash: str = session_hash or self._id",
            "        self.fn = fn",
            "        self.request = request",
            "        self.username = username",
            "        self.concurrency_id = fn.concurrency_id",
            "        self.data: PredictBodyInternal | None = None",
            "        self.progress: ProgressMessage | None = None",
            "        self.progress_pending: bool = False",
            "        self.alive = True",
            "",
            "",
            "class EventQueue:",
            "    def __init__(self, concurrency_id: str, concurrency_limit: int | None):",
            "        self.queue: list[Event] = []",
            "        self.concurrency_id = concurrency_id",
            "        self.concurrency_limit = concurrency_limit",
            "        self.current_concurrency = 0",
            "        self.start_times_per_fn: defaultdict[BlockFunction, set[float]] = defaultdict(",
            "            set",
            "        )",
            "",
            "",
            "class ProcessTime:",
            "    def __init__(self):",
            "        self.process_time = 0",
            "        self.count = 0",
            "        self.avg_time = 0",
            "",
            "    def add(self, time: float):",
            "        self.process_time += time",
            "        self.count += 1",
            "        self.avg_time = self.process_time / self.count",
            "",
            "",
            "class Queue:",
            "    def __init__(",
            "        self,",
            "        live_updates: bool,",
            "        concurrency_count: int,",
            "        update_intervals: float,",
            "        max_size: int | None,",
            "        blocks: Blocks,",
            "        default_concurrency_limit: int | None | Literal[\"not_set\"] = \"not_set\",",
            "    ):",
            "        self.pending_messages_per_session: LRUCache[str, ThreadQueue[EventMessage]] = (",
            "            LRUCache(2000)",
            "        )",
            "        self.pending_event_ids_session: dict[str, set[str]] = {}",
            "        self.pending_message_lock = safe_get_lock()",
            "        self.event_queue_per_concurrency_id: dict[str, EventQueue] = {}",
            "        self.stopped = False",
            "        self.max_thread_count = concurrency_count",
            "        self.update_intervals = update_intervals",
            "        self.active_jobs: list[None | list[Event]] = []",
            "        self.delete_lock = safe_get_lock()",
            "        self.server_app = None",
            "        self.process_time_per_fn: defaultdict[BlockFunction, ProcessTime] = defaultdict(",
            "            ProcessTime",
            "        )",
            "        self.live_updates = live_updates",
            "        self.sleep_when_free = 0.05",
            "        self.progress_update_sleep_when_free = 0.1",
            "        self.max_size = max_size",
            "        self.blocks = blocks",
            "        self._asyncio_tasks: list[asyncio.Task] = []",
            "        self.default_concurrency_limit = self._resolve_concurrency_limit(",
            "            default_concurrency_limit",
            "        )",
            "        self.event_analytics: dict[str, dict[str, float | str | None]] = {}",
            "",
            "    def start(self):",
            "        self.active_jobs = [None] * self.max_thread_count",
            "",
            "        run_coro_in_background(self.start_processing)",
            "        run_coro_in_background(self.start_progress_updates)",
            "        if not self.live_updates:",
            "            run_coro_in_background(self.notify_clients)",
            "",
            "    def create_event_queue_for_fn(self, block_fn: BlockFunction):",
            "        concurrency_id = block_fn.concurrency_id",
            "        concurrency_limit: int | None",
            "        if block_fn.concurrency_limit == \"default\":",
            "            concurrency_limit = self.default_concurrency_limit",
            "        else:",
            "            concurrency_limit = block_fn.concurrency_limit",
            "        if concurrency_id not in self.event_queue_per_concurrency_id:",
            "            self.event_queue_per_concurrency_id[concurrency_id] = EventQueue(",
            "                concurrency_id, concurrency_limit",
            "            )",
            "        elif (",
            "            concurrency_limit is not None",
            "        ):  # Update concurrency limit if it is lower than existing limit",
            "            existing_event_queue = self.event_queue_per_concurrency_id[concurrency_id]",
            "            if (",
            "                existing_event_queue.concurrency_limit is None",
            "                or concurrency_limit < existing_event_queue.concurrency_limit",
            "            ):",
            "                existing_event_queue.concurrency_limit = concurrency_limit",
            "",
            "    def close(self):",
            "        self.stopped = True",
            "",
            "    def send_message(",
            "        self,",
            "        event: Event,",
            "        event_message: EventMessage,",
            "    ):",
            "        if not event.alive:",
            "            return",
            "        event_message.event_id = event._id",
            "        messages = self.pending_messages_per_session[event.session_hash]",
            "        messages.put_nowait(event_message)",
            "",
            "    def _resolve_concurrency_limit(",
            "        self, default_concurrency_limit: int | None | Literal[\"not_set\"]",
            "    ) -> int | None:",
            "        \"\"\"",
            "        Handles the logic of resolving the default_concurrency_limit as this can be specified via a combination",
            "        of the `default_concurrency_limit` parameter of the `Blocks.queue()` or the `GRADIO_DEFAULT_CONCURRENCY_LIMIT`",
            "        environment variable. The parameter in `Blocks.queue()` takes precedence over the environment variable.",
            "        Parameters:",
            "            default_concurrency_limit: The default concurrency limit, as specified by a user in `Blocks.queu()`.",
            "        \"\"\"",
            "        if default_concurrency_limit != \"not_set\":",
            "            return default_concurrency_limit",
            "        if default_concurrency_limit_env := os.environ.get(",
            "            \"GRADIO_DEFAULT_CONCURRENCY_LIMIT\"",
            "        ):",
            "            if default_concurrency_limit_env.lower() == \"none\":",
            "                return None",
            "            else:",
            "                return int(default_concurrency_limit_env)",
            "        else:",
            "            return 1",
            "",
            "    def __len__(self):",
            "        total_len = 0",
            "        for event_queue in self.event_queue_per_concurrency_id.values():",
            "            total_len += len(event_queue.queue)",
            "        return total_len",
            "",
            "    async def push(",
            "        self, body: PredictBodyInternal, request: fastapi.Request, username: str | None",
            "    ) -> tuple[bool, str]:",
            "        if body.fn_index is None:",
            "            return False, \"No function index provided.\"",
            "        if self.max_size is not None and len(self) >= self.max_size:",
            "            return (",
            "                False,",
            "                f\"Queue is full. Max size is {self.max_size} and size is {len(self)}.\",",
            "            )",
            "",
            "        if body.session_hash:",
            "            session_state = self.blocks.state_holder[body.session_hash]",
            "            fn = session_state.blocks_config.fns[body.fn_index]",
            "        else:",
            "            fn = self.blocks.fns[body.fn_index]",
            "",
            "        fn = route_utils.get_fn(self.blocks, None, body)",
            "        self.create_event_queue_for_fn(fn)",
            "        event = Event(",
            "            body.session_hash,",
            "            fn,",
            "            request,",
            "            username,",
            "        )",
            "        event.data = body",
            "        if body.session_hash is None:",
            "            body.session_hash = event.session_hash",
            "        async with self.pending_message_lock:",
            "            if body.session_hash not in self.pending_messages_per_session:",
            "                self.pending_messages_per_session[body.session_hash] = ThreadQueue()",
            "            if body.session_hash not in self.pending_event_ids_session:",
            "                self.pending_event_ids_session[body.session_hash] = set()",
            "        self.pending_event_ids_session[body.session_hash].add(event._id)",
            "        try:",
            "            event_queue = self.event_queue_per_concurrency_id[event.concurrency_id]",
            "        except KeyError as e:",
            "            raise KeyError(",
            "                \"Event not found in queue. If you are deploying this Gradio app with multiple replicas, please enable stickiness to ensure that all requests from the same user are routed to the same instance.\"",
            "            ) from e",
            "        event_queue.queue.append(event)",
            "        self.event_analytics[event._id] = {",
            "            \"time\": time.time(),",
            "            \"status\": \"queued\",",
            "            \"process_time\": None,",
            "            \"function\": fn.api_name,",
            "            \"session_hash\": body.session_hash,",
            "        }",
            "",
            "        self.broadcast_estimations(event.concurrency_id, len(event_queue.queue) - 1)",
            "",
            "        return True, event._id",
            "",
            "    def _cancel_asyncio_tasks(self):",
            "        for task in self._asyncio_tasks:",
            "            task.cancel()",
            "        self._asyncio_tasks = []",
            "",
            "    def set_server_app(self, app: routes.App):",
            "        self.server_app = app",
            "",
            "    def get_active_worker_count(self) -> int:",
            "        count = 0",
            "        for worker in self.active_jobs:",
            "            if worker is not None:",
            "                count += 1",
            "        return count",
            "",
            "    def get_events(self) -> tuple[list[Event], bool, str] | None:",
            "        concurrency_ids = list(self.event_queue_per_concurrency_id.keys())",
            "        random.shuffle(concurrency_ids)",
            "        for concurrency_id in concurrency_ids:",
            "            event_queue = self.event_queue_per_concurrency_id[concurrency_id]",
            "            if len(event_queue.queue) and (",
            "                event_queue.concurrency_limit is None",
            "                or event_queue.current_concurrency < event_queue.concurrency_limit",
            "            ):",
            "                first_event = event_queue.queue[0]",
            "                block_fn = first_event.fn",
            "                events = [first_event]",
            "                batch = block_fn.batch",
            "                if batch:",
            "                    events += [",
            "                        event",
            "                        for event in event_queue.queue[1:]",
            "                        if event.fn == first_event.fn",
            "                    ][: block_fn.max_batch_size - 1]",
            "",
            "                for event in events:",
            "                    event_queue.queue.remove(event)",
            "",
            "                return events, batch, concurrency_id",
            "",
            "    async def start_processing(self) -> None:",
            "        try:",
            "            while not self.stopped:",
            "                if len(self) == 0:",
            "                    await asyncio.sleep(self.sleep_when_free)",
            "                    continue",
            "",
            "                if None not in self.active_jobs:",
            "                    await asyncio.sleep(self.sleep_when_free)",
            "                    continue",
            "",
            "                # Using mutex to avoid editing a list in use",
            "                async with self.delete_lock:",
            "                    event_batch = self.get_events()",
            "",
            "                if event_batch:",
            "                    events, batch, concurrency_id = event_batch",
            "                    self.active_jobs[self.active_jobs.index(None)] = events",
            "                    event_queue = self.event_queue_per_concurrency_id[concurrency_id]",
            "                    event_queue.current_concurrency += 1",
            "                    start_time = time.time()",
            "                    event_queue.start_times_per_fn[events[0].fn].add(start_time)",
            "                    for event in events:",
            "                        self.event_analytics[event._id][\"status\"] = \"processing\"",
            "                    process_event_task = run_coro_in_background(",
            "                        self.process_events, events, batch, start_time",
            "                    )",
            "                    set_task_name(",
            "                        process_event_task,",
            "                        events[0].session_hash,",
            "                        events[0].fn._id,",
            "                        events[0]._id,",
            "                        batch,",
            "                    )",
            "",
            "                    self._asyncio_tasks.append(process_event_task)",
            "                    if self.live_updates:",
            "                        self.broadcast_estimations(concurrency_id)",
            "                else:",
            "                    await asyncio.sleep(self.sleep_when_free)",
            "        finally:",
            "            self.stopped = True",
            "            self._cancel_asyncio_tasks()",
            "",
            "    async def start_progress_updates(self) -> None:",
            "        \"\"\"",
            "        Because progress updates can be very frequent, we do not necessarily want to send a message per update.",
            "        Rather, we check for progress updates at regular intervals, and send a message if there is a pending update.",
            "        Consecutive progress updates between sends will overwrite each other so only the most recent update will be sent.",
            "        \"\"\"",
            "        while not self.stopped:",
            "            events = [evt for job in self.active_jobs if job is not None for evt in job]",
            "",
            "            if len(events) == 0:",
            "                await asyncio.sleep(self.progress_update_sleep_when_free)",
            "                continue",
            "",
            "            for event in events:",
            "                if event.progress_pending and event.progress:",
            "                    event.progress_pending = False",
            "                    self.send_message(event, event.progress)",
            "",
            "            await asyncio.sleep(self.progress_update_sleep_when_free)",
            "",
            "    def set_progress(",
            "        self,",
            "        event_id: str,",
            "        iterables: list[TrackedIterable] | None,",
            "    ):",
            "        if iterables is None:",
            "            return",
            "        for job in self.active_jobs:",
            "            if job is None:",
            "                continue",
            "            for evt in job:",
            "                if evt._id == event_id:",
            "                    progress_data: list[ProgressUnit] = []",
            "                    for iterable in iterables:",
            "                        progress_unit = ProgressUnit(",
            "                            index=iterable.index,",
            "                            length=iterable.length,",
            "                            unit=iterable.unit,",
            "                            progress=iterable.progress,",
            "                            desc=iterable.desc,",
            "                        )",
            "                        progress_data.append(progress_unit)",
            "                    evt.progress = ProgressMessage(progress_data=progress_data)",
            "                    evt.progress_pending = True",
            "",
            "    def log_message(",
            "        self,",
            "        event_id: str,",
            "        log: str,",
            "        level: Literal[\"info\", \"warning\"],",
            "        duration: float | None = 10,",
            "        visible: bool = True,",
            "    ):",
            "        events = [evt for job in self.active_jobs if job is not None for evt in job]",
            "        for event in events:",
            "            if event._id == event_id:",
            "                log_message = LogMessage(",
            "                    log=log,",
            "                    level=level,",
            "                    duration=duration,",
            "                    visible=visible,",
            "                )",
            "                self.send_message(event, log_message)",
            "",
            "    async def clean_events(",
            "        self, *, session_hash: str | None = None, event_id: str | None = None",
            "    ) -> None:",
            "        for job_set in self.active_jobs:",
            "            if job_set:",
            "                for job in job_set:",
            "                    if job.session_hash == session_hash or job._id == event_id:",
            "                        job.alive = False",
            "",
            "        async with self.delete_lock:",
            "            events_to_remove: list[Event] = []",
            "            for event_queue in self.event_queue_per_concurrency_id.values():",
            "                for event in event_queue.queue:",
            "                    if event.session_hash == session_hash or event._id == event_id:",
            "                        events_to_remove.append(event)",
            "",
            "            for event in events_to_remove:",
            "                self.event_queue_per_concurrency_id[event.concurrency_id].queue.remove(",
            "                    event",
            "                )",
            "",
            "    async def notify_clients(self) -> None:",
            "        \"\"\"",
            "        Notify clients about events statuses in the queue periodically.",
            "        \"\"\"",
            "        while not self.stopped:",
            "            await asyncio.sleep(self.update_intervals)",
            "            if len(self) > 0:",
            "                for concurrency_id in self.event_queue_per_concurrency_id:",
            "                    self.broadcast_estimations(concurrency_id)",
            "",
            "    def broadcast_estimations(",
            "        self, concurrency_id: str, after: int | None = None",
            "    ) -> None:",
            "        wait_so_far = 0",
            "        event_queue = self.event_queue_per_concurrency_id[concurrency_id]",
            "        time_till_available_worker: int | None = 0",
            "",
            "        if event_queue.current_concurrency == event_queue.concurrency_limit:",
            "            expected_end_times = []",
            "            for fn, start_times in event_queue.start_times_per_fn.items():",
            "                if fn not in self.process_time_per_fn:",
            "                    time_till_available_worker = None",
            "                    break",
            "                process_time = self.process_time_per_fn[fn].avg_time",
            "                expected_end_times += [",
            "                    start_time + process_time for start_time in start_times",
            "                ]",
            "            if time_till_available_worker is not None and len(expected_end_times) > 0:",
            "                time_of_first_completion = min(expected_end_times)",
            "                time_till_available_worker = max(",
            "                    time_of_first_completion - time.time(), 0",
            "                )",
            "",
            "        for rank, event in enumerate(event_queue.queue):",
            "            process_time_for_fn = (",
            "                self.process_time_per_fn[event.fn].avg_time",
            "                if event.fn in self.process_time_per_fn",
            "                else None",
            "            )",
            "            rank_eta = (",
            "                process_time_for_fn + wait_so_far + time_till_available_worker",
            "                if process_time_for_fn is not None",
            "                and wait_so_far is not None",
            "                and time_till_available_worker is not None",
            "                else None",
            "            )",
            "",
            "            if after is None or rank >= after:",
            "                self.send_message(",
            "                    event,",
            "                    EstimationMessage(",
            "                        rank=rank, rank_eta=rank_eta, queue_size=len(event_queue.queue)",
            "                    ),",
            "                )",
            "            if event_queue.concurrency_limit is None:",
            "                wait_so_far = 0",
            "            elif wait_so_far is not None and process_time_for_fn is not None:",
            "                wait_so_far += process_time_for_fn / event_queue.concurrency_limit",
            "            else:",
            "                wait_so_far = None",
            "",
            "    def get_status(self) -> EstimationMessage:",
            "        return EstimationMessage(",
            "            queue_size=len(self),",
            "        )",
            "",
            "    async def process_events(",
            "        self, events: list[Event], batch: bool, begin_time: float",
            "    ) -> None:",
            "        awake_events: list[Event] = []",
            "        fn = events[0].fn",
            "        success = False",
            "        try:",
            "            for event in events:",
            "                if event.alive:",
            "                    self.send_message(",
            "                        event,",
            "                        ProcessStartsMessage(",
            "                            eta=self.process_time_per_fn[fn].avg_time",
            "                            if fn in self.process_time_per_fn",
            "                            else None",
            "                        ),",
            "                    )",
            "                    awake_events.append(event)",
            "            if not awake_events:",
            "                return",
            "",
            "            events = awake_events",
            "            body = events[0].data",
            "            if body is None:",
            "                raise ValueError(\"No event data\")",
            "            username = events[0].username",
            "            body.event_id = events[0]._id if not batch else None",
            "            try:",
            "                body.request = events[0].request",
            "            except ValueError:",
            "                pass",
            "",
            "            if batch:",
            "                body.data = list(",
            "                    zip(*[event.data.data for event in events if event.data])",
            "                )",
            "                body.request = events[0].request",
            "                body.batched = True",
            "",
            "            app = self.server_app",
            "            if app is None:",
            "                raise Exception(\"Server app has not been set.\")",
            "",
            "            gr_request = route_utils.compile_gr_request(",
            "                body=body,",
            "                fn=fn,",
            "                username=username,",
            "                request=None,",
            "            )",
            "            assert body.request is not None  # noqa: S101",
            "            root_path = route_utils.get_root_url(",
            "                request=body.request, route_path=\"/queue/join\", root_path=app.root_path",
            "            )",
            "            try:",
            "                response = await route_utils.call_process_api(",
            "                    app=app,",
            "                    body=body,",
            "                    gr_request=gr_request,",
            "                    fn=fn,",
            "                    root_path=root_path,",
            "                )",
            "                err = None",
            "            except Exception as e:",
            "                traceback.print_exc()",
            "                response = None",
            "                err = e",
            "                for event in awake_events:",
            "                    content = error_payload(err, app.get_blocks().show_error)",
            "                    self.send_message(",
            "                        event,",
            "                        ProcessCompletedMessage(",
            "                            output=content,",
            "                            success=False,",
            "                        ),",
            "                    )",
            "            if response and response.get(\"is_generating\", False):",
            "                old_response = response",
            "                old_err = err",
            "                while response and response.get(\"is_generating\", False):",
            "                    old_response = response",
            "                    old_err = err",
            "                    for event in awake_events:",
            "                        self.send_message(",
            "                            event,",
            "                            ProcessGeneratingMessage(",
            "                                output=old_response,",
            "                                success=old_response is not None,",
            "                            ),",
            "                        )",
            "                    awake_events = [event for event in awake_events if event.alive]",
            "                    if not awake_events:",
            "                        return",
            "                    try:",
            "                        response = await route_utils.call_process_api(",
            "                            app=app,",
            "                            body=body,",
            "                            gr_request=gr_request,",
            "                            fn=fn,",
            "                            root_path=root_path,",
            "                        )",
            "                    except Exception as e:",
            "                        traceback.print_exc()",
            "                        response = None",
            "                        err = e",
            "",
            "                if response:",
            "                    success = True",
            "                    output = response",
            "                else:",
            "                    success = False",
            "                    error = err or old_err",
            "                    output = error_payload(error, app.get_blocks().show_error)",
            "                for event in awake_events:",
            "                    self.send_message(",
            "                        event, ProcessCompletedMessage(output=output, success=success)",
            "                    )",
            "",
            "            elif response:",
            "                output = copy.deepcopy(response)",
            "                for e, event in enumerate(awake_events):",
            "                    if batch and \"data\" in output:",
            "                        output[\"data\"] = list(zip(*response.get(\"data\")))[e]",
            "                    success = response is not None",
            "                    self.send_message(",
            "                        event,",
            "                        ProcessCompletedMessage(",
            "                            output=output,",
            "                            success=success,",
            "                        ),",
            "                    )",
            "            end_time = time.time()",
            "            if response is not None:",
            "                duration = end_time - begin_time",
            "                self.process_time_per_fn[events[0].fn].add(duration)",
            "                for event in events:",
            "                    self.event_analytics[event._id][\"process_time\"] = duration",
            "        except Exception as e:",
            "            traceback.print_exc()",
            "        finally:",
            "            event_queue = self.event_queue_per_concurrency_id[events[0].concurrency_id]",
            "            event_queue.current_concurrency -= 1",
            "            start_times = event_queue.start_times_per_fn[fn]",
            "            if begin_time in start_times:",
            "                start_times.remove(begin_time)",
            "            try:",
            "                self.active_jobs[self.active_jobs.index(events)] = None",
            "            except ValueError:",
            "                # `events` can be absent from `self.active_jobs`",
            "                # when this coroutine is called from the `join_queue` endpoint handler in `routes.py`",
            "                # without putting the `events` into `self.active_jobs`.",
            "                # https://github.com/gradio-app/gradio/blob/f09aea34d6bd18c1e2fef80c86ab2476a6d1dd83/gradio/routes.py#L594-L596",
            "                pass",
            "            for event in events:",
            "                # Always reset the state of the iterator",
            "                # If the job finished successfully, this has no effect",
            "                # If the job is cancelled, this will enable future runs",
            "                # to start \"from scratch\"",
            "                await self.reset_iterators(event._id)",
            "",
            "                if event in awake_events:",
            "                    self.event_analytics[event._id][\"status\"] = (",
            "                        \"success\" if success else \"failed\"",
            "                    )",
            "                else:",
            "                    self.event_analytics[event._id][\"status\"] = \"cancelled\"",
            "",
            "    async def reset_iterators(self, event_id: str):",
            "        # Do the same thing as the /reset route",
            "        app = self.server_app",
            "        if app is None:",
            "            raise Exception(\"Server app has not been set.\")",
            "        if event_id not in app.iterators:",
            "            # Failure, but don't raise an error",
            "            return",
            "        async with app.lock:",
            "            del app.iterators[event_id]",
            "            app.iterators_to_reset.add(event_id)",
            "        return"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "19": [],
            "58": [
                "Event",
                "__init__"
            ],
            "195": [
                "Queue"
            ]
        },
        "addLocation": []
    },
    "gradio/route_utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": " from gradio.data_classes import ("
            },
            "1": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "     BlocksConfigDict,"
            },
            "2": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "     PredictBody,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+    PredictBodyInternal,"
            },
            "4": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 51,
                "PatchRowcode": " )"
            },
            "5": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 52,
                "PatchRowcode": " from gradio.exceptions import Error"
            },
            "6": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " from gradio.helpers import EventData"
            },
            "7": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": 236,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": 237,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": 238,
                "PatchRowcode": " def compile_gr_request("
            },
            "10": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    body: PredictBody,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 239,
                "PatchRowcode": "+    body: PredictBodyInternal,"
            },
            "12": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": 240,
                "PatchRowcode": "     fn: BlockFunction,"
            },
            "13": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "     username: Optional[str],"
            },
            "14": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "     request: Optional[fastapi.Request],"
            },
            "15": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 262,
                "PatchRowcode": "     return gr_request"
            },
            "16": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 263,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 264,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def restore_session_state(app: App, body: PredictBody):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 265,
                "PatchRowcode": "+def restore_session_state(app: App, body: PredictBodyInternal):"
            },
            "20": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 266,
                "PatchRowcode": "     event_id = body.event_id"
            },
            "21": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 267,
                "PatchRowcode": "     session_hash = getattr(body, \"session_hash\", None)"
            },
            "22": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "     if session_hash is not None:"
            },
            "23": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": 288,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": 289,
                "PatchRowcode": " def prepare_event_data("
            },
            "25": {
                "beforePatchRowNumber": 289,
                "afterPatchRowNumber": 290,
                "PatchRowcode": "     blocks: Blocks,"
            },
            "26": {
                "beforePatchRowNumber": 290,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    body: PredictBody,"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 291,
                "PatchRowcode": "+    body: PredictBodyInternal,"
            },
            "28": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": 292,
                "PatchRowcode": " ) -> EventData:"
            },
            "29": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 293,
                "PatchRowcode": "     target = body.trigger_id"
            },
            "30": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 294,
                "PatchRowcode": "     event_data = EventData("
            },
            "31": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": 300,
                "PatchRowcode": " "
            },
            "32": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": 301,
                "PatchRowcode": " async def call_process_api("
            },
            "33": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": 302,
                "PatchRowcode": "     app: App,"
            },
            "34": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    body: PredictBody,"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+    body: PredictBodyInternal,"
            },
            "36": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": 304,
                "PatchRowcode": "     gr_request: Union[Request, list[Request]],"
            },
            "37": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 305,
                "PatchRowcode": "     fn: BlockFunction,"
            },
            "38": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": 306,
                "PatchRowcode": "     root_path: str,"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import functools",
            "import hashlib",
            "import hmac",
            "import json",
            "import os",
            "import pickle",
            "import re",
            "import shutil",
            "import sys",
            "import threading",
            "from collections import deque",
            "from contextlib import AsyncExitStack, asynccontextmanager",
            "from dataclasses import dataclass as python_dataclass",
            "from datetime import datetime",
            "from pathlib import Path",
            "from tempfile import NamedTemporaryFile, _TemporaryFileWrapper",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    AsyncContextManager,",
            "    AsyncGenerator,",
            "    BinaryIO,",
            "    Callable,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "from urllib.parse import urlparse",
            "",
            "import anyio",
            "import fastapi",
            "import gradio_client.utils as client_utils",
            "import httpx",
            "import multipart",
            "from gradio_client.documentation import document",
            "from multipart.multipart import parse_options_header",
            "from starlette.datastructures import FormData, Headers, MutableHeaders, UploadFile",
            "from starlette.formparsers import MultiPartException, MultipartPart",
            "from starlette.responses import PlainTextResponse, Response",
            "from starlette.types import ASGIApp, Message, Receive, Scope, Send",
            "",
            "from gradio import processing_utils, utils",
            "from gradio.data_classes import (",
            "    BlocksConfigDict,",
            "    PredictBody,",
            ")",
            "from gradio.exceptions import Error",
            "from gradio.helpers import EventData",
            "from gradio.state_holder import SessionState",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import BlockFunction, Blocks",
            "    from gradio.routes import App",
            "",
            "",
            "config_lock = threading.Lock()",
            "",
            "",
            "class Obj:",
            "    \"\"\"",
            "    Using a class to convert dictionaries into objects. Used by the `Request` class.",
            "    Credit: https://www.geeksforgeeks.org/convert-nested-python-dictionary-to-object/",
            "    \"\"\"",
            "",
            "    def __init__(self, dict_):",
            "        self.__dict__.update(dict_)",
            "        for key, value in dict_.items():",
            "            if isinstance(value, (dict, list)):",
            "                value = Obj(value)",
            "            setattr(self, key, value)",
            "",
            "    def __getitem__(self, item):",
            "        return self.__dict__[item]",
            "",
            "    def __setitem__(self, item, value):",
            "        self.__dict__[item] = value",
            "",
            "    def __iter__(self):",
            "        for key, value in self.__dict__.items():",
            "            if isinstance(value, Obj):",
            "                yield (key, dict(value))",
            "            else:",
            "                yield (key, value)",
            "",
            "    def __contains__(self, item) -> bool:",
            "        if item in self.__dict__:",
            "            return True",
            "        for value in self.__dict__.values():",
            "            if isinstance(value, Obj) and item in value:",
            "                return True",
            "        return False",
            "",
            "    def get(self, item, default=None):",
            "        if item in self:",
            "            return self.__dict__[item]",
            "        return default",
            "",
            "    def keys(self):",
            "        return self.__dict__.keys()",
            "",
            "    def values(self):",
            "        return self.__dict__.values()",
            "",
            "    def items(self):",
            "        return self.__dict__.items()",
            "",
            "    def __str__(self) -> str:",
            "        return str(self.__dict__)",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.__dict__)",
            "",
            "    def pop(self, item, default=None):",
            "        if item in self:",
            "            return self.__dict__.pop(item)",
            "        return default",
            "",
            "",
            "@document()",
            "class Request:",
            "    \"\"\"",
            "    A Gradio request object that can be used to access the request headers, cookies,",
            "    query parameters and other information about the request from within the prediction",
            "    function. The class is a thin wrapper around the fastapi.Request class. Attributes",
            "    of this class include: `headers`, `client`, `query_params`, `session_hash`, and `path_params`. If",
            "    auth is enabled, the `username` attribute can be used to get the logged in user. In some environments,",
            "    the dict-like attributes (e.g. `requests.headers`, `requests.query_params`) of this class are automatically",
            "    converted to to dictionaries, so we recommend converting them to dictionaries before accessing",
            "    attributes for consistent behavior in different environments.",
            "    Example:",
            "        import gradio as gr",
            "        def echo(text, request: gr.Request):",
            "            if request:",
            "                print(\"Request headers dictionary:\", dict(request.headers))",
            "                print(\"Query parameters:\", dict(request.query_params))",
            "                print(\"IP address:\", request.client.host)",
            "                print(\"Gradio session hash:\", request.session_hash)",
            "            return text",
            "        io = gr.Interface(echo, \"textbox\", \"textbox\").launch()",
            "    Demos: request_ip_headers",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        request: fastapi.Request | None = None,",
            "        username: str | None = None,",
            "        session_hash: str | None = None,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Can be instantiated with either a fastapi.Request or by manually passing in",
            "        attributes (needed for queueing).",
            "        Parameters:",
            "            request: A fastapi.Request",
            "            username: The username of the logged in user (if auth is enabled)",
            "            session_hash: The session hash of the current session. It is unique for each page load.",
            "        \"\"\"",
            "        self.request = request",
            "        self.username = username",
            "        self.session_hash: str | None = session_hash",
            "        self.kwargs: dict[str, Any] = kwargs",
            "",
            "    def dict_to_obj(self, d):",
            "        if isinstance(d, dict):",
            "            return json.loads(json.dumps(d), object_hook=Obj)",
            "        else:",
            "            return d",
            "",
            "    def __getattr__(self, name: str):",
            "        if self.request:",
            "            return self.dict_to_obj(getattr(self.request, name))",
            "        else:",
            "            try:",
            "                obj = self.kwargs[name]",
            "            except KeyError as ke:",
            "                raise AttributeError(",
            "                    f\"'Request' object has no attribute '{name}'\"",
            "                ) from ke",
            "            return self.dict_to_obj(obj)",
            "",
            "    def __getstate__(self) -> dict[str, Any]:",
            "        self.kwargs.update(",
            "            {",
            "                \"headers\": dict(getattr(self, \"headers\", {})),",
            "                \"query_params\": dict(getattr(self, \"query_params\", {})),",
            "                \"cookies\": dict(getattr(self, \"cookies\", {})),",
            "                \"path_params\": dict(getattr(self, \"path_params\", {})),",
            "                \"client\": {",
            "                    \"host\": getattr(self, \"client\", {}) and self.client.host,",
            "                    \"port\": getattr(self, \"client\", {}) and self.client.port,",
            "                },",
            "                \"url\": getattr(self, \"url\", \"\"),",
            "            }",
            "        )",
            "        if request_state := hasattr(self, \"state\"):",
            "            try:",
            "                pickle.dumps(request_state)",
            "                self.kwargs[\"request_state\"] = request_state",
            "            except pickle.PicklingError:",
            "                pass",
            "        self.request = None",
            "        return self.__dict__",
            "",
            "    def __setstate__(self, state: dict[str, Any]):",
            "        if request_state := state.pop(\"request_state\", None):",
            "            self.state = request_state",
            "        self.__dict__ = state",
            "",
            "",
            "class FnIndexInferError(Exception):",
            "    pass",
            "",
            "",
            "def get_fn(blocks: Blocks, api_name: str | None, body: PredictBody) -> BlockFunction:",
            "    if body.session_hash:",
            "        session_state = blocks.state_holder[body.session_hash]",
            "        fns = session_state.blocks_config.fns",
            "    else:",
            "        fns = blocks.fns",
            "",
            "    if body.fn_index is None:",
            "        if api_name is not None:",
            "            for fn in fns.values():",
            "                if fn.api_name == api_name:",
            "                    return fn",
            "        raise FnIndexInferError(",
            "            f\"Could not infer function index for API name: {api_name}\"",
            "        )",
            "    else:",
            "        return fns[body.fn_index]",
            "",
            "",
            "def compile_gr_request(",
            "    body: PredictBody,",
            "    fn: BlockFunction,",
            "    username: Optional[str],",
            "    request: Optional[fastapi.Request],",
            "):",
            "    # If this fn_index cancels jobs, then the only input we need is the",
            "    # current session hash",
            "    if fn.cancels:",
            "        body.data = [body.session_hash]",
            "    if body.request:",
            "        if body.batched:",
            "            gr_request = [Request(username=username, request=request)]",
            "        else:",
            "            gr_request = Request(",
            "                username=username, request=body.request, session_hash=body.session_hash",
            "            )",
            "    else:",
            "        if request is None:",
            "            raise ValueError(\"request must be provided if body.request is None\")",
            "        gr_request = Request(",
            "            username=username, request=request, session_hash=body.session_hash",
            "        )",
            "",
            "    return gr_request",
            "",
            "",
            "def restore_session_state(app: App, body: PredictBody):",
            "    event_id = body.event_id",
            "    session_hash = getattr(body, \"session_hash\", None)",
            "    if session_hash is not None:",
            "        session_state = app.state_holder[session_hash]",
            "        # The should_reset set keeps track of the fn_indices",
            "        # that have been cancelled. When a job is cancelled,",
            "        # the /reset route will mark the jobs as having been reset.",
            "        # That way if the cancel job finishes BEFORE the job being cancelled",
            "        # the job being cancelled will not overwrite the state of the iterator.",
            "        if event_id is None:",
            "            iterator = None",
            "        elif event_id in app.iterators_to_reset:",
            "            iterator = None",
            "            app.iterators_to_reset.remove(event_id)",
            "        else:",
            "            iterator = app.iterators.get(event_id)",
            "    else:",
            "        session_state = SessionState(app.get_blocks())",
            "        iterator = None",
            "",
            "    return session_state, iterator",
            "",
            "",
            "def prepare_event_data(",
            "    blocks: Blocks,",
            "    body: PredictBody,",
            ") -> EventData:",
            "    target = body.trigger_id",
            "    event_data = EventData(",
            "        blocks.blocks.get(target) if target else None,",
            "        body.event_data,",
            "    )",
            "    return event_data",
            "",
            "",
            "async def call_process_api(",
            "    app: App,",
            "    body: PredictBody,",
            "    gr_request: Union[Request, list[Request]],",
            "    fn: BlockFunction,",
            "    root_path: str,",
            "):",
            "    session_state, iterator = restore_session_state(app=app, body=body)",
            "",
            "    event_data = prepare_event_data(app.get_blocks(), body)",
            "    event_id = body.event_id",
            "",
            "    session_hash = getattr(body, \"session_hash\", None)",
            "    inputs = body.data",
            "",
            "    batch_in_single_out = not body.batched and fn.batch",
            "    if batch_in_single_out:",
            "        inputs = [inputs]",
            "",
            "    try:",
            "        with utils.MatplotlibBackendMananger():",
            "            output = await app.get_blocks().process_api(",
            "                block_fn=fn,",
            "                inputs=inputs,",
            "                request=gr_request,",
            "                state=session_state,",
            "                iterator=iterator,",
            "                session_hash=session_hash,",
            "                event_id=event_id,",
            "                event_data=event_data,",
            "                in_event_listener=True,",
            "                simple_format=body.simple_format,",
            "                root_path=root_path,",
            "            )",
            "        iterator = output.pop(\"iterator\", None)",
            "        if event_id is not None:",
            "            app.iterators[event_id] = iterator  # type: ignore",
            "        if isinstance(output, Error):",
            "            raise output",
            "    except BaseException:",
            "        iterator = app.iterators.get(event_id) if event_id is not None else None",
            "        if iterator is not None:  # close off any streams that are still open",
            "            run_id = id(iterator)",
            "            pending_streams: dict[int, list] = (",
            "                app.get_blocks().pending_streams[session_hash].get(run_id, {})",
            "            )",
            "            for stream in pending_streams.values():",
            "                stream.append(None)",
            "        raise",
            "",
            "    if batch_in_single_out:",
            "        output[\"data\"] = output[\"data\"][0]",
            "",
            "    return output",
            "",
            "",
            "def get_root_url(",
            "    request: fastapi.Request, route_path: str, root_path: str | None",
            ") -> str:",
            "    \"\"\"",
            "    Gets the root url of the Gradio app (i.e. the public url of the app) without a trailing slash.",
            "",
            "    This is how the root_url is resolved:",
            "    1. If a user provides a `root_path` manually that is a full URL, it is returned directly.",
            "    2. If the request has an x-forwarded-host header (e.g. because it is behind a proxy), the root url is",
            "    constructed from the x-forwarded-host header. In this case, `route_path` is not used to construct the root url.",
            "    3. Otherwise, the root url is constructed from the request url. The query parameters and `route_path` are stripped off.",
            "    And if a relative `root_path` is provided, and it is not already the subpath of the URL, it is appended to the root url.",
            "",
            "    In cases (2) and (3), We also check to see if the x-forwarded-proto header is present, and if so, convert the root url to https.",
            "    And if there are multiple hosts in the x-forwarded-host or multiple protocols in the x-forwarded-proto, the first one is used.",
            "    \"\"\"",
            "",
            "    def get_first_header_value(header_name: str):",
            "        header_value = request.headers.get(header_name)",
            "        if header_value:",
            "            return header_value.split(\",\")[0].strip()",
            "        return None",
            "",
            "    if root_path and client_utils.is_http_url_like(root_path):",
            "        return root_path.rstrip(\"/\")",
            "",
            "    x_forwarded_host = get_first_header_value(\"x-forwarded-host\")",
            "    root_url = f\"http://{x_forwarded_host}\" if x_forwarded_host else str(request.url)",
            "    root_url = httpx.URL(root_url)",
            "    root_url = root_url.copy_with(query=None)",
            "    root_url = str(root_url).rstrip(\"/\")",
            "    if get_first_header_value(\"x-forwarded-proto\") == \"https\":",
            "        root_url = root_url.replace(\"http://\", \"https://\")",
            "",
            "    route_path = route_path.rstrip(\"/\")",
            "    if len(route_path) > 0 and not x_forwarded_host:",
            "        root_url = root_url[: -len(route_path)]",
            "    root_url = root_url.rstrip(\"/\")",
            "",
            "    root_url = httpx.URL(root_url)",
            "    if root_path and root_url.path != root_path:",
            "        root_url = root_url.copy_with(path=root_path)",
            "",
            "    return str(root_url).rstrip(\"/\")",
            "",
            "",
            "def _user_safe_decode(src: bytes, codec: str) -> str:",
            "    try:",
            "        return src.decode(codec)",
            "    except (UnicodeDecodeError, LookupError):",
            "        return src.decode(\"latin-1\")",
            "",
            "",
            "class GradioUploadFile(UploadFile):",
            "    \"\"\"UploadFile with a sha attribute.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        file: BinaryIO,",
            "        *,",
            "        size: int | None = None,",
            "        filename: str | None = None,",
            "        headers: Headers | None = None,",
            "    ) -> None:",
            "        super().__init__(file, size=size, filename=filename, headers=headers)",
            "        self.sha = hashlib.sha256()",
            "",
            "",
            "@python_dataclass(frozen=True)",
            "class FileUploadProgressUnit:",
            "    filename: str",
            "    chunk_size: int",
            "",
            "",
            "@python_dataclass",
            "class FileUploadProgressTracker:",
            "    deque: deque[FileUploadProgressUnit]",
            "    is_done: bool",
            "",
            "",
            "class FileUploadProgressNotTrackedError(Exception):",
            "    pass",
            "",
            "",
            "class FileUploadProgressNotQueuedError(Exception):",
            "    pass",
            "",
            "",
            "class FileUploadProgress:",
            "    def __init__(self) -> None:",
            "        self._statuses: dict[str, FileUploadProgressTracker] = {}",
            "",
            "    def track(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            self._statuses[upload_id] = FileUploadProgressTracker(deque(), False)",
            "",
            "    def append(self, upload_id: str, filename: str, message_bytes: bytes):",
            "        if upload_id not in self._statuses:",
            "            self.track(upload_id)",
            "        queue = self._statuses[upload_id].deque",
            "",
            "        if len(queue) == 0:",
            "            queue.append(FileUploadProgressUnit(filename, len(message_bytes)))",
            "        else:",
            "            last_unit = queue.popleft()",
            "            if last_unit.filename != filename:",
            "                queue.append(FileUploadProgressUnit(filename, len(message_bytes)))",
            "            else:",
            "                queue.append(",
            "                    FileUploadProgressUnit(",
            "                        filename,",
            "                        last_unit.chunk_size + len(message_bytes),",
            "                    )",
            "                )",
            "",
            "    def set_done(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            self.track(upload_id)",
            "        self._statuses[upload_id].is_done = True",
            "",
            "    def is_done(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            raise FileUploadProgressNotTrackedError()",
            "        return self._statuses[upload_id].is_done",
            "",
            "    def stop_tracking(self, upload_id: str):",
            "        if upload_id in self._statuses:",
            "            del self._statuses[upload_id]",
            "",
            "    def pop(self, upload_id: str) -> FileUploadProgressUnit:",
            "        if upload_id not in self._statuses:",
            "            raise FileUploadProgressNotTrackedError()",
            "        try:",
            "            return self._statuses[upload_id].deque.pop()",
            "        except IndexError as e:",
            "            raise FileUploadProgressNotQueuedError() from e",
            "",
            "",
            "class GradioMultiPartParser:",
            "    \"\"\"Vendored from starlette.MultipartParser.",
            "",
            "    Thanks starlette!",
            "",
            "    Made the following modifications",
            "        - Use GradioUploadFile instead of UploadFile",
            "        - Use NamedTemporaryFile instead of SpooledTemporaryFile",
            "        - Compute hash of data as the request is streamed",
            "",
            "    \"\"\"",
            "",
            "    max_file_size = 1024 * 1024",
            "",
            "    def __init__(",
            "        self,",
            "        headers: Headers,",
            "        stream: AsyncGenerator[bytes, None],",
            "        *,",
            "        max_files: Union[int, float] = 1000,",
            "        max_fields: Union[int, float] = 1000,",
            "        upload_id: str | None = None,",
            "        upload_progress: FileUploadProgress | None = None,",
            "        max_file_size: int | float,",
            "    ) -> None:",
            "        self.headers = headers",
            "        self.stream = stream",
            "        self.max_files = max_files",
            "        self.max_fields = max_fields",
            "        self.items: List[Tuple[str, Union[str, UploadFile]]] = []",
            "        self.upload_id = upload_id",
            "        self.upload_progress = upload_progress",
            "        self._current_files = 0",
            "        self._current_fields = 0",
            "        self.max_file_size = max_file_size",
            "        self._current_partial_header_name: bytes = b\"\"",
            "        self._current_partial_header_value: bytes = b\"\"",
            "        self._current_part = MultipartPart()",
            "        self._charset = \"\"",
            "        self._file_parts_to_write: List[Tuple[MultipartPart, bytes]] = []",
            "        self._file_parts_to_finish: List[MultipartPart] = []",
            "        self._files_to_close_on_error: List[_TemporaryFileWrapper] = []",
            "",
            "    def on_part_begin(self) -> None:",
            "        self._current_part = MultipartPart()",
            "",
            "    def on_part_data(self, data: bytes, start: int, end: int) -> None:",
            "        message_bytes = data[start:end]",
            "        if self.upload_progress is not None:",
            "            self.upload_progress.append(",
            "                self.upload_id,  # type: ignore",
            "                self._current_part.file.filename,  # type: ignore",
            "                message_bytes,",
            "            )",
            "        if self._current_part.file is None:",
            "            self._current_part.data += message_bytes",
            "        else:",
            "            self._file_parts_to_write.append((self._current_part, message_bytes))",
            "",
            "    def on_part_end(self) -> None:",
            "        if self._current_part.file is None:",
            "            self.items.append(",
            "                (",
            "                    self._current_part.field_name,",
            "                    _user_safe_decode(self._current_part.data, str(self._charset)),",
            "                )",
            "            )",
            "        else:",
            "            self._file_parts_to_finish.append(self._current_part)",
            "            # The file can be added to the items right now even though it's not",
            "            # finished yet, because it will be finished in the `parse()` method, before",
            "            # self.items is used in the return value.",
            "            self.items.append((self._current_part.field_name, self._current_part.file))",
            "",
            "    def on_header_field(self, data: bytes, start: int, end: int) -> None:",
            "        self._current_partial_header_name += data[start:end]",
            "",
            "    def on_header_value(self, data: bytes, start: int, end: int) -> None:",
            "        self._current_partial_header_value += data[start:end]",
            "",
            "    def on_header_end(self) -> None:",
            "        field = self._current_partial_header_name.lower()",
            "        if field == b\"content-disposition\":",
            "            self._current_part.content_disposition = self._current_partial_header_value",
            "        self._current_part.item_headers.append(",
            "            (field, self._current_partial_header_value)",
            "        )",
            "        self._current_partial_header_name = b\"\"",
            "        self._current_partial_header_value = b\"\"",
            "",
            "    def on_headers_finished(self) -> None:",
            "        _, options = parse_options_header(self._current_part.content_disposition or b\"\")",
            "        try:",
            "            self._current_part.field_name = _user_safe_decode(",
            "                options[b\"name\"], str(self._charset)",
            "            )",
            "        except KeyError as e:",
            "            raise MultiPartException(",
            "                'The Content-Disposition header field \"name\" must be ' \"provided.\"",
            "            ) from e",
            "        if b\"filename\" in options:",
            "            self._current_files += 1",
            "            if self._current_files > self.max_files:",
            "                raise MultiPartException(",
            "                    f\"Too many files. Maximum number of files is {self.max_files}.\"",
            "                )",
            "            filename = _user_safe_decode(options[b\"filename\"], str(self._charset))",
            "            tempfile = NamedTemporaryFile(delete=False)",
            "            self._files_to_close_on_error.append(tempfile)",
            "            self._current_part.file = GradioUploadFile(",
            "                file=tempfile,  # type: ignore[arg-type]",
            "                size=0,",
            "                filename=filename,",
            "                headers=Headers(raw=self._current_part.item_headers),",
            "            )",
            "        else:",
            "            self._current_fields += 1",
            "            if self._current_fields > self.max_fields:",
            "                raise MultiPartException(",
            "                    f\"Too many fields. Maximum number of fields is {self.max_fields}.\"",
            "                )",
            "            self._current_part.file = None",
            "",
            "    def on_end(self) -> None:",
            "        pass",
            "",
            "    async def parse(self) -> FormData:",
            "        # Parse the Content-Type header to get the multipart boundary.",
            "        _, params = parse_options_header(self.headers[\"Content-Type\"])",
            "        charset = params.get(b\"charset\", \"utf-8\")",
            "        if isinstance(charset, bytes):",
            "            charset = charset.decode(\"latin-1\")",
            "        self._charset = charset",
            "        try:",
            "            boundary = params[b\"boundary\"]",
            "        except KeyError as e:",
            "            raise MultiPartException(\"Missing boundary in multipart.\") from e",
            "",
            "        # Callbacks dictionary.",
            "        callbacks: multipart.multipart.MultipartCallbacks = {",
            "            \"on_part_begin\": self.on_part_begin,",
            "            \"on_part_data\": self.on_part_data,",
            "            \"on_part_end\": self.on_part_end,",
            "            \"on_header_field\": self.on_header_field,",
            "            \"on_header_value\": self.on_header_value,",
            "            \"on_header_end\": self.on_header_end,",
            "            \"on_headers_finished\": self.on_headers_finished,",
            "            \"on_end\": self.on_end,",
            "        }",
            "",
            "        # Create the parser.",
            "        parser = multipart.MultipartParser(boundary, callbacks)",
            "        try:",
            "            # Feed the parser with data from the request.",
            "            async for chunk in self.stream:",
            "                parser.write(chunk)",
            "                # Write file data, it needs to use await with the UploadFile methods",
            "                # that call the corresponding file methods *in a threadpool*,",
            "                # otherwise, if they were called directly in the callback methods above",
            "                # (regular, non-async functions), that would block the event loop in",
            "                # the main thread.",
            "                for part, data in self._file_parts_to_write:",
            "                    assert part.file  # for type checkers  # noqa: S101",
            "                    await part.file.write(data)",
            "                    part.file.sha.update(data)  # type: ignore",
            "                    if os.stat(part.file.file.name).st_size > self.max_file_size:",
            "                        if self.upload_progress is not None:",
            "                            self.upload_progress.set_done(self.upload_id)  # type: ignore",
            "                        raise MultiPartException(",
            "                            f\"File size exceeded maximum allowed size of {self.max_file_size} bytes.\"",
            "                        )",
            "                for part in self._file_parts_to_finish:",
            "                    assert part.file  # for type checkers  # noqa: S101",
            "                    await part.file.seek(0)",
            "                self._file_parts_to_write.clear()",
            "                self._file_parts_to_finish.clear()",
            "        except MultiPartException as exc:",
            "            # Close all the files if there was an error.",
            "            for file in self._files_to_close_on_error:",
            "                file.close()",
            "                Path(file.name).unlink()",
            "            raise exc",
            "",
            "        parser.finalize()",
            "        if self.upload_progress is not None:",
            "            self.upload_progress.set_done(self.upload_id)  # type: ignore",
            "        return FormData(self.items)",
            "",
            "",
            "def move_uploaded_files_to_cache(files: list[str], destinations: list[str]) -> None:",
            "    for file, dest in zip(files, destinations):",
            "        shutil.move(file, dest)",
            "",
            "",
            "def update_root_in_config(config: BlocksConfigDict, root: str) -> BlocksConfigDict:",
            "    \"\"\"",
            "    Updates the root \"key\" in the config dictionary to the new root url. If the",
            "    root url has changed, all of the urls in the config that correspond to component",
            "    file urls are updated to use the new root url.",
            "    \"\"\"",
            "    with config_lock:",
            "        previous_root = config.get(\"root\")",
            "        if previous_root is None or previous_root != root:",
            "            config[\"root\"] = root",
            "            config = processing_utils.add_root_url(config, root, previous_root)  # type: ignore",
            "    return config",
            "",
            "",
            "def compare_passwords_securely(input_password: str, correct_password: str) -> bool:",
            "    return hmac.compare_digest(input_password.encode(), correct_password.encode())",
            "",
            "",
            "def starts_with_protocol(string: str) -> bool:",
            "    \"\"\"This regex matches strings that start with a scheme (one or more characters not including colon, slash, or space)",
            "    followed by ://, or start with just //, \\\\/, /\\\\, or \\\\ as they are interpreted as SMB paths on Windows.",
            "    \"\"\"",
            "    pattern = r\"^(?:[a-zA-Z][a-zA-Z0-9+\\-.]*://|//|\\\\\\\\|\\\\/|/\\\\)\"",
            "    return re.match(pattern, string) is not None",
            "",
            "",
            "def get_hostname(url: str) -> str:",
            "    \"\"\"",
            "    Returns the hostname of a given url, or an empty string if the url cannot be parsed.",
            "    Examples:",
            "        get_hostname(\"https://www.gradio.app\") -> \"www.gradio.app\"",
            "        get_hostname(\"localhost:7860\") -> \"localhost\"",
            "        get_hostname(\"127.0.0.1\") -> \"127.0.0.1\"",
            "    \"\"\"",
            "    if not url:",
            "        return \"\"",
            "    if \"://\" not in url:",
            "        url = \"http://\" + url",
            "    try:",
            "        return urlparse(url).hostname or \"\"",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "class CustomCORSMiddleware:",
            "    # This is a modified version of the Starlette CORSMiddleware that restricts the allowed origins when the host is localhost.",
            "    # Adapted from: https://github.com/encode/starlette/blob/89fae174a1ea10f59ae248fe030d9b7e83d0b8a0/starlette/middleware/cors.py",
            "",
            "    def __init__(",
            "        self,",
            "        app: ASGIApp,",
            "    ) -> None:",
            "        self.app = app",
            "        self.all_methods = (\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\", \"PATCH\", \"POST\", \"PUT\")",
            "        self.preflight_headers = {",
            "            \"Access-Control-Allow-Methods\": \", \".join(self.all_methods),",
            "            \"Access-Control-Max-Age\": str(600),",
            "            \"Access-Control-Allow-Credentials\": \"true\",",
            "        }",
            "        self.simple_headers = {\"Access-Control-Allow-Credentials\": \"true\"}",
            "        # Any of these hosts suggests that the Gradio app is running locally.",
            "        # Note: \"null\" is a special case that happens if a Gradio app is running",
            "        # as an embedded web component in a local static webpage.",
            "        self.localhost_aliases = [\"localhost\", \"127.0.0.1\", \"0.0.0.0\", \"null\"]",
            "",
            "    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:",
            "        if scope[\"type\"] != \"http\":",
            "            await self.app(scope, receive, send)",
            "            return",
            "        headers = Headers(scope=scope)",
            "        origin = headers.get(\"origin\")",
            "        if origin is None:",
            "            await self.app(scope, receive, send)",
            "            return",
            "        if scope[\"method\"] == \"OPTIONS\" and \"access-control-request-method\" in headers:",
            "            response = self.preflight_response(request_headers=headers)",
            "            await response(scope, receive, send)",
            "            return",
            "        await self.simple_response(scope, receive, send, request_headers=headers)",
            "",
            "    def preflight_response(self, request_headers: Headers) -> Response:",
            "        headers = dict(self.preflight_headers)",
            "        origin = request_headers[\"Origin\"]",
            "        if self.is_valid_origin(request_headers):",
            "            headers[\"Access-Control-Allow-Origin\"] = origin",
            "        requested_headers = request_headers.get(\"access-control-request-headers\")",
            "        if requested_headers is not None:",
            "            headers[\"Access-Control-Allow-Headers\"] = requested_headers",
            "        return PlainTextResponse(\"OK\", status_code=200, headers=headers)",
            "",
            "    async def simple_response(",
            "        self, scope: Scope, receive: Receive, send: Send, request_headers: Headers",
            "    ) -> None:",
            "        send = functools.partial(self._send, send=send, request_headers=request_headers)",
            "        await self.app(scope, receive, send)",
            "",
            "    async def _send(",
            "        self, message: Message, send: Send, request_headers: Headers",
            "    ) -> None:",
            "        if message[\"type\"] != \"http.response.start\":",
            "            await send(message)",
            "            return",
            "        message.setdefault(\"headers\", [])",
            "        headers = MutableHeaders(scope=message)",
            "        headers.update(self.simple_headers)",
            "        origin = request_headers[\"Origin\"]",
            "        if self.is_valid_origin(request_headers):",
            "            self.allow_explicit_origin(headers, origin)",
            "        await send(message)",
            "",
            "    def is_valid_origin(self, request_headers: Headers) -> bool:",
            "        origin = request_headers[\"Origin\"]",
            "        host = request_headers[\"Host\"]",
            "        host_name = get_hostname(host)",
            "        origin_name = get_hostname(origin)",
            "        return (",
            "            host_name not in self.localhost_aliases",
            "            or origin_name in self.localhost_aliases",
            "        )",
            "",
            "    @staticmethod",
            "    def allow_explicit_origin(headers: MutableHeaders, origin: str) -> None:",
            "        headers[\"Access-Control-Allow-Origin\"] = origin",
            "        headers.add_vary_header(\"Origin\")",
            "",
            "",
            "def delete_files_created_by_app(blocks: Blocks, age: int | None) -> None:",
            "    \"\"\"Delete files that are older than age. If age is None, delete all files.\"\"\"",
            "    dont_delete = set()",
            "    for component in blocks.blocks.values():",
            "        dont_delete.update(getattr(component, \"keep_in_cache\", set()))",
            "    for temp_set in blocks.temp_file_sets:",
            "        # We use a copy of the set to avoid modifying the set while iterating over it",
            "        # otherwise we would get an exception: Set changed size during iteration",
            "        to_remove = set()",
            "        for file in temp_set:",
            "            if file in dont_delete:",
            "                continue",
            "            try:",
            "                file_path = Path(file)",
            "                modified_time = datetime.fromtimestamp(file_path.lstat().st_ctime)",
            "                if age is None or (datetime.now() - modified_time).seconds > age:",
            "                    os.remove(file)",
            "                    to_remove.add(file)",
            "            except FileNotFoundError:",
            "                continue",
            "        temp_set -= to_remove",
            "",
            "",
            "async def delete_files_on_schedule(app: App, frequency: int, age: int) -> None:",
            "    \"\"\"Startup task to delete files created by the app based on time since last modification.\"\"\"",
            "    while True:",
            "        await asyncio.sleep(frequency)",
            "        await anyio.to_thread.run_sync(",
            "            delete_files_created_by_app, app.get_blocks(), age",
            "        )",
            "",
            "",
            "@asynccontextmanager",
            "async def _lifespan_handler(",
            "    app: App, frequency: int = 1, age: int = 1",
            ") -> AsyncGenerator:",
            "    \"\"\"A context manager that triggers the startup and shutdown events of the app.\"\"\"",
            "    asyncio.create_task(delete_files_on_schedule(app, frequency, age))",
            "    yield",
            "    delete_files_created_by_app(app.get_blocks(), age=None)",
            "",
            "",
            "async def _delete_state(app: App):",
            "    \"\"\"Delete all expired state every second.\"\"\"",
            "    while True:",
            "        app.state_holder.delete_all_expired_state()",
            "        await asyncio.sleep(1)",
            "",
            "",
            "@asynccontextmanager",
            "async def _delete_state_handler(app: App):",
            "    \"\"\"When the server launches, regularly delete expired state.\"\"\"",
            "    # The stop event needs to get the current event loop for python 3.8",
            "    # but the loop parameter is deprecated for 3.8+",
            "    if sys.version_info < (3, 10):",
            "        loop = asyncio.get_running_loop()",
            "        app.stop_event = asyncio.Event(loop=loop)",
            "    asyncio.create_task(_delete_state(app))",
            "    yield",
            "",
            "",
            "def create_lifespan_handler(",
            "    user_lifespan: Callable[[App], AsyncContextManager] | None,",
            "    frequency: int | None = 1,",
            "    age: int | None = 1,",
            ") -> Callable[[App], AsyncContextManager]:",
            "    \"\"\"Return a context manager that applies _lifespan_handler and user_lifespan if it exists.\"\"\"",
            "",
            "    @asynccontextmanager",
            "    async def _handler(app: App):",
            "        async with AsyncExitStack() as stack:",
            "            await stack.enter_async_context(_delete_state_handler(app))",
            "            if frequency and age:",
            "                await stack.enter_async_context(_lifespan_handler(app, frequency, age))",
            "            if user_lifespan is not None:",
            "                await stack.enter_async_context(user_lifespan(app))",
            "            yield",
            "",
            "    return _handler"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import functools",
            "import hashlib",
            "import hmac",
            "import json",
            "import os",
            "import pickle",
            "import re",
            "import shutil",
            "import sys",
            "import threading",
            "from collections import deque",
            "from contextlib import AsyncExitStack, asynccontextmanager",
            "from dataclasses import dataclass as python_dataclass",
            "from datetime import datetime",
            "from pathlib import Path",
            "from tempfile import NamedTemporaryFile, _TemporaryFileWrapper",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    AsyncContextManager,",
            "    AsyncGenerator,",
            "    BinaryIO,",
            "    Callable,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "from urllib.parse import urlparse",
            "",
            "import anyio",
            "import fastapi",
            "import gradio_client.utils as client_utils",
            "import httpx",
            "import multipart",
            "from gradio_client.documentation import document",
            "from multipart.multipart import parse_options_header",
            "from starlette.datastructures import FormData, Headers, MutableHeaders, UploadFile",
            "from starlette.formparsers import MultiPartException, MultipartPart",
            "from starlette.responses import PlainTextResponse, Response",
            "from starlette.types import ASGIApp, Message, Receive, Scope, Send",
            "",
            "from gradio import processing_utils, utils",
            "from gradio.data_classes import (",
            "    BlocksConfigDict,",
            "    PredictBody,",
            "    PredictBodyInternal,",
            ")",
            "from gradio.exceptions import Error",
            "from gradio.helpers import EventData",
            "from gradio.state_holder import SessionState",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import BlockFunction, Blocks",
            "    from gradio.routes import App",
            "",
            "",
            "config_lock = threading.Lock()",
            "",
            "",
            "class Obj:",
            "    \"\"\"",
            "    Using a class to convert dictionaries into objects. Used by the `Request` class.",
            "    Credit: https://www.geeksforgeeks.org/convert-nested-python-dictionary-to-object/",
            "    \"\"\"",
            "",
            "    def __init__(self, dict_):",
            "        self.__dict__.update(dict_)",
            "        for key, value in dict_.items():",
            "            if isinstance(value, (dict, list)):",
            "                value = Obj(value)",
            "            setattr(self, key, value)",
            "",
            "    def __getitem__(self, item):",
            "        return self.__dict__[item]",
            "",
            "    def __setitem__(self, item, value):",
            "        self.__dict__[item] = value",
            "",
            "    def __iter__(self):",
            "        for key, value in self.__dict__.items():",
            "            if isinstance(value, Obj):",
            "                yield (key, dict(value))",
            "            else:",
            "                yield (key, value)",
            "",
            "    def __contains__(self, item) -> bool:",
            "        if item in self.__dict__:",
            "            return True",
            "        for value in self.__dict__.values():",
            "            if isinstance(value, Obj) and item in value:",
            "                return True",
            "        return False",
            "",
            "    def get(self, item, default=None):",
            "        if item in self:",
            "            return self.__dict__[item]",
            "        return default",
            "",
            "    def keys(self):",
            "        return self.__dict__.keys()",
            "",
            "    def values(self):",
            "        return self.__dict__.values()",
            "",
            "    def items(self):",
            "        return self.__dict__.items()",
            "",
            "    def __str__(self) -> str:",
            "        return str(self.__dict__)",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.__dict__)",
            "",
            "    def pop(self, item, default=None):",
            "        if item in self:",
            "            return self.__dict__.pop(item)",
            "        return default",
            "",
            "",
            "@document()",
            "class Request:",
            "    \"\"\"",
            "    A Gradio request object that can be used to access the request headers, cookies,",
            "    query parameters and other information about the request from within the prediction",
            "    function. The class is a thin wrapper around the fastapi.Request class. Attributes",
            "    of this class include: `headers`, `client`, `query_params`, `session_hash`, and `path_params`. If",
            "    auth is enabled, the `username` attribute can be used to get the logged in user. In some environments,",
            "    the dict-like attributes (e.g. `requests.headers`, `requests.query_params`) of this class are automatically",
            "    converted to to dictionaries, so we recommend converting them to dictionaries before accessing",
            "    attributes for consistent behavior in different environments.",
            "    Example:",
            "        import gradio as gr",
            "        def echo(text, request: gr.Request):",
            "            if request:",
            "                print(\"Request headers dictionary:\", dict(request.headers))",
            "                print(\"Query parameters:\", dict(request.query_params))",
            "                print(\"IP address:\", request.client.host)",
            "                print(\"Gradio session hash:\", request.session_hash)",
            "            return text",
            "        io = gr.Interface(echo, \"textbox\", \"textbox\").launch()",
            "    Demos: request_ip_headers",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        request: fastapi.Request | None = None,",
            "        username: str | None = None,",
            "        session_hash: str | None = None,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Can be instantiated with either a fastapi.Request or by manually passing in",
            "        attributes (needed for queueing).",
            "        Parameters:",
            "            request: A fastapi.Request",
            "            username: The username of the logged in user (if auth is enabled)",
            "            session_hash: The session hash of the current session. It is unique for each page load.",
            "        \"\"\"",
            "        self.request = request",
            "        self.username = username",
            "        self.session_hash: str | None = session_hash",
            "        self.kwargs: dict[str, Any] = kwargs",
            "",
            "    def dict_to_obj(self, d):",
            "        if isinstance(d, dict):",
            "            return json.loads(json.dumps(d), object_hook=Obj)",
            "        else:",
            "            return d",
            "",
            "    def __getattr__(self, name: str):",
            "        if self.request:",
            "            return self.dict_to_obj(getattr(self.request, name))",
            "        else:",
            "            try:",
            "                obj = self.kwargs[name]",
            "            except KeyError as ke:",
            "                raise AttributeError(",
            "                    f\"'Request' object has no attribute '{name}'\"",
            "                ) from ke",
            "            return self.dict_to_obj(obj)",
            "",
            "    def __getstate__(self) -> dict[str, Any]:",
            "        self.kwargs.update(",
            "            {",
            "                \"headers\": dict(getattr(self, \"headers\", {})),",
            "                \"query_params\": dict(getattr(self, \"query_params\", {})),",
            "                \"cookies\": dict(getattr(self, \"cookies\", {})),",
            "                \"path_params\": dict(getattr(self, \"path_params\", {})),",
            "                \"client\": {",
            "                    \"host\": getattr(self, \"client\", {}) and self.client.host,",
            "                    \"port\": getattr(self, \"client\", {}) and self.client.port,",
            "                },",
            "                \"url\": getattr(self, \"url\", \"\"),",
            "            }",
            "        )",
            "        if request_state := hasattr(self, \"state\"):",
            "            try:",
            "                pickle.dumps(request_state)",
            "                self.kwargs[\"request_state\"] = request_state",
            "            except pickle.PicklingError:",
            "                pass",
            "        self.request = None",
            "        return self.__dict__",
            "",
            "    def __setstate__(self, state: dict[str, Any]):",
            "        if request_state := state.pop(\"request_state\", None):",
            "            self.state = request_state",
            "        self.__dict__ = state",
            "",
            "",
            "class FnIndexInferError(Exception):",
            "    pass",
            "",
            "",
            "def get_fn(blocks: Blocks, api_name: str | None, body: PredictBody) -> BlockFunction:",
            "    if body.session_hash:",
            "        session_state = blocks.state_holder[body.session_hash]",
            "        fns = session_state.blocks_config.fns",
            "    else:",
            "        fns = blocks.fns",
            "",
            "    if body.fn_index is None:",
            "        if api_name is not None:",
            "            for fn in fns.values():",
            "                if fn.api_name == api_name:",
            "                    return fn",
            "        raise FnIndexInferError(",
            "            f\"Could not infer function index for API name: {api_name}\"",
            "        )",
            "    else:",
            "        return fns[body.fn_index]",
            "",
            "",
            "def compile_gr_request(",
            "    body: PredictBodyInternal,",
            "    fn: BlockFunction,",
            "    username: Optional[str],",
            "    request: Optional[fastapi.Request],",
            "):",
            "    # If this fn_index cancels jobs, then the only input we need is the",
            "    # current session hash",
            "    if fn.cancels:",
            "        body.data = [body.session_hash]",
            "    if body.request:",
            "        if body.batched:",
            "            gr_request = [Request(username=username, request=request)]",
            "        else:",
            "            gr_request = Request(",
            "                username=username, request=body.request, session_hash=body.session_hash",
            "            )",
            "    else:",
            "        if request is None:",
            "            raise ValueError(\"request must be provided if body.request is None\")",
            "        gr_request = Request(",
            "            username=username, request=request, session_hash=body.session_hash",
            "        )",
            "",
            "    return gr_request",
            "",
            "",
            "def restore_session_state(app: App, body: PredictBodyInternal):",
            "    event_id = body.event_id",
            "    session_hash = getattr(body, \"session_hash\", None)",
            "    if session_hash is not None:",
            "        session_state = app.state_holder[session_hash]",
            "        # The should_reset set keeps track of the fn_indices",
            "        # that have been cancelled. When a job is cancelled,",
            "        # the /reset route will mark the jobs as having been reset.",
            "        # That way if the cancel job finishes BEFORE the job being cancelled",
            "        # the job being cancelled will not overwrite the state of the iterator.",
            "        if event_id is None:",
            "            iterator = None",
            "        elif event_id in app.iterators_to_reset:",
            "            iterator = None",
            "            app.iterators_to_reset.remove(event_id)",
            "        else:",
            "            iterator = app.iterators.get(event_id)",
            "    else:",
            "        session_state = SessionState(app.get_blocks())",
            "        iterator = None",
            "",
            "    return session_state, iterator",
            "",
            "",
            "def prepare_event_data(",
            "    blocks: Blocks,",
            "    body: PredictBodyInternal,",
            ") -> EventData:",
            "    target = body.trigger_id",
            "    event_data = EventData(",
            "        blocks.blocks.get(target) if target else None,",
            "        body.event_data,",
            "    )",
            "    return event_data",
            "",
            "",
            "async def call_process_api(",
            "    app: App,",
            "    body: PredictBodyInternal,",
            "    gr_request: Union[Request, list[Request]],",
            "    fn: BlockFunction,",
            "    root_path: str,",
            "):",
            "    session_state, iterator = restore_session_state(app=app, body=body)",
            "",
            "    event_data = prepare_event_data(app.get_blocks(), body)",
            "    event_id = body.event_id",
            "",
            "    session_hash = getattr(body, \"session_hash\", None)",
            "    inputs = body.data",
            "",
            "    batch_in_single_out = not body.batched and fn.batch",
            "    if batch_in_single_out:",
            "        inputs = [inputs]",
            "",
            "    try:",
            "        with utils.MatplotlibBackendMananger():",
            "            output = await app.get_blocks().process_api(",
            "                block_fn=fn,",
            "                inputs=inputs,",
            "                request=gr_request,",
            "                state=session_state,",
            "                iterator=iterator,",
            "                session_hash=session_hash,",
            "                event_id=event_id,",
            "                event_data=event_data,",
            "                in_event_listener=True,",
            "                simple_format=body.simple_format,",
            "                root_path=root_path,",
            "            )",
            "        iterator = output.pop(\"iterator\", None)",
            "        if event_id is not None:",
            "            app.iterators[event_id] = iterator  # type: ignore",
            "        if isinstance(output, Error):",
            "            raise output",
            "    except BaseException:",
            "        iterator = app.iterators.get(event_id) if event_id is not None else None",
            "        if iterator is not None:  # close off any streams that are still open",
            "            run_id = id(iterator)",
            "            pending_streams: dict[int, list] = (",
            "                app.get_blocks().pending_streams[session_hash].get(run_id, {})",
            "            )",
            "            for stream in pending_streams.values():",
            "                stream.append(None)",
            "        raise",
            "",
            "    if batch_in_single_out:",
            "        output[\"data\"] = output[\"data\"][0]",
            "",
            "    return output",
            "",
            "",
            "def get_root_url(",
            "    request: fastapi.Request, route_path: str, root_path: str | None",
            ") -> str:",
            "    \"\"\"",
            "    Gets the root url of the Gradio app (i.e. the public url of the app) without a trailing slash.",
            "",
            "    This is how the root_url is resolved:",
            "    1. If a user provides a `root_path` manually that is a full URL, it is returned directly.",
            "    2. If the request has an x-forwarded-host header (e.g. because it is behind a proxy), the root url is",
            "    constructed from the x-forwarded-host header. In this case, `route_path` is not used to construct the root url.",
            "    3. Otherwise, the root url is constructed from the request url. The query parameters and `route_path` are stripped off.",
            "    And if a relative `root_path` is provided, and it is not already the subpath of the URL, it is appended to the root url.",
            "",
            "    In cases (2) and (3), We also check to see if the x-forwarded-proto header is present, and if so, convert the root url to https.",
            "    And if there are multiple hosts in the x-forwarded-host or multiple protocols in the x-forwarded-proto, the first one is used.",
            "    \"\"\"",
            "",
            "    def get_first_header_value(header_name: str):",
            "        header_value = request.headers.get(header_name)",
            "        if header_value:",
            "            return header_value.split(\",\")[0].strip()",
            "        return None",
            "",
            "    if root_path and client_utils.is_http_url_like(root_path):",
            "        return root_path.rstrip(\"/\")",
            "",
            "    x_forwarded_host = get_first_header_value(\"x-forwarded-host\")",
            "    root_url = f\"http://{x_forwarded_host}\" if x_forwarded_host else str(request.url)",
            "    root_url = httpx.URL(root_url)",
            "    root_url = root_url.copy_with(query=None)",
            "    root_url = str(root_url).rstrip(\"/\")",
            "    if get_first_header_value(\"x-forwarded-proto\") == \"https\":",
            "        root_url = root_url.replace(\"http://\", \"https://\")",
            "",
            "    route_path = route_path.rstrip(\"/\")",
            "    if len(route_path) > 0 and not x_forwarded_host:",
            "        root_url = root_url[: -len(route_path)]",
            "    root_url = root_url.rstrip(\"/\")",
            "",
            "    root_url = httpx.URL(root_url)",
            "    if root_path and root_url.path != root_path:",
            "        root_url = root_url.copy_with(path=root_path)",
            "",
            "    return str(root_url).rstrip(\"/\")",
            "",
            "",
            "def _user_safe_decode(src: bytes, codec: str) -> str:",
            "    try:",
            "        return src.decode(codec)",
            "    except (UnicodeDecodeError, LookupError):",
            "        return src.decode(\"latin-1\")",
            "",
            "",
            "class GradioUploadFile(UploadFile):",
            "    \"\"\"UploadFile with a sha attribute.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        file: BinaryIO,",
            "        *,",
            "        size: int | None = None,",
            "        filename: str | None = None,",
            "        headers: Headers | None = None,",
            "    ) -> None:",
            "        super().__init__(file, size=size, filename=filename, headers=headers)",
            "        self.sha = hashlib.sha256()",
            "",
            "",
            "@python_dataclass(frozen=True)",
            "class FileUploadProgressUnit:",
            "    filename: str",
            "    chunk_size: int",
            "",
            "",
            "@python_dataclass",
            "class FileUploadProgressTracker:",
            "    deque: deque[FileUploadProgressUnit]",
            "    is_done: bool",
            "",
            "",
            "class FileUploadProgressNotTrackedError(Exception):",
            "    pass",
            "",
            "",
            "class FileUploadProgressNotQueuedError(Exception):",
            "    pass",
            "",
            "",
            "class FileUploadProgress:",
            "    def __init__(self) -> None:",
            "        self._statuses: dict[str, FileUploadProgressTracker] = {}",
            "",
            "    def track(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            self._statuses[upload_id] = FileUploadProgressTracker(deque(), False)",
            "",
            "    def append(self, upload_id: str, filename: str, message_bytes: bytes):",
            "        if upload_id not in self._statuses:",
            "            self.track(upload_id)",
            "        queue = self._statuses[upload_id].deque",
            "",
            "        if len(queue) == 0:",
            "            queue.append(FileUploadProgressUnit(filename, len(message_bytes)))",
            "        else:",
            "            last_unit = queue.popleft()",
            "            if last_unit.filename != filename:",
            "                queue.append(FileUploadProgressUnit(filename, len(message_bytes)))",
            "            else:",
            "                queue.append(",
            "                    FileUploadProgressUnit(",
            "                        filename,",
            "                        last_unit.chunk_size + len(message_bytes),",
            "                    )",
            "                )",
            "",
            "    def set_done(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            self.track(upload_id)",
            "        self._statuses[upload_id].is_done = True",
            "",
            "    def is_done(self, upload_id: str):",
            "        if upload_id not in self._statuses:",
            "            raise FileUploadProgressNotTrackedError()",
            "        return self._statuses[upload_id].is_done",
            "",
            "    def stop_tracking(self, upload_id: str):",
            "        if upload_id in self._statuses:",
            "            del self._statuses[upload_id]",
            "",
            "    def pop(self, upload_id: str) -> FileUploadProgressUnit:",
            "        if upload_id not in self._statuses:",
            "            raise FileUploadProgressNotTrackedError()",
            "        try:",
            "            return self._statuses[upload_id].deque.pop()",
            "        except IndexError as e:",
            "            raise FileUploadProgressNotQueuedError() from e",
            "",
            "",
            "class GradioMultiPartParser:",
            "    \"\"\"Vendored from starlette.MultipartParser.",
            "",
            "    Thanks starlette!",
            "",
            "    Made the following modifications",
            "        - Use GradioUploadFile instead of UploadFile",
            "        - Use NamedTemporaryFile instead of SpooledTemporaryFile",
            "        - Compute hash of data as the request is streamed",
            "",
            "    \"\"\"",
            "",
            "    max_file_size = 1024 * 1024",
            "",
            "    def __init__(",
            "        self,",
            "        headers: Headers,",
            "        stream: AsyncGenerator[bytes, None],",
            "        *,",
            "        max_files: Union[int, float] = 1000,",
            "        max_fields: Union[int, float] = 1000,",
            "        upload_id: str | None = None,",
            "        upload_progress: FileUploadProgress | None = None,",
            "        max_file_size: int | float,",
            "    ) -> None:",
            "        self.headers = headers",
            "        self.stream = stream",
            "        self.max_files = max_files",
            "        self.max_fields = max_fields",
            "        self.items: List[Tuple[str, Union[str, UploadFile]]] = []",
            "        self.upload_id = upload_id",
            "        self.upload_progress = upload_progress",
            "        self._current_files = 0",
            "        self._current_fields = 0",
            "        self.max_file_size = max_file_size",
            "        self._current_partial_header_name: bytes = b\"\"",
            "        self._current_partial_header_value: bytes = b\"\"",
            "        self._current_part = MultipartPart()",
            "        self._charset = \"\"",
            "        self._file_parts_to_write: List[Tuple[MultipartPart, bytes]] = []",
            "        self._file_parts_to_finish: List[MultipartPart] = []",
            "        self._files_to_close_on_error: List[_TemporaryFileWrapper] = []",
            "",
            "    def on_part_begin(self) -> None:",
            "        self._current_part = MultipartPart()",
            "",
            "    def on_part_data(self, data: bytes, start: int, end: int) -> None:",
            "        message_bytes = data[start:end]",
            "        if self.upload_progress is not None:",
            "            self.upload_progress.append(",
            "                self.upload_id,  # type: ignore",
            "                self._current_part.file.filename,  # type: ignore",
            "                message_bytes,",
            "            )",
            "        if self._current_part.file is None:",
            "            self._current_part.data += message_bytes",
            "        else:",
            "            self._file_parts_to_write.append((self._current_part, message_bytes))",
            "",
            "    def on_part_end(self) -> None:",
            "        if self._current_part.file is None:",
            "            self.items.append(",
            "                (",
            "                    self._current_part.field_name,",
            "                    _user_safe_decode(self._current_part.data, str(self._charset)),",
            "                )",
            "            )",
            "        else:",
            "            self._file_parts_to_finish.append(self._current_part)",
            "            # The file can be added to the items right now even though it's not",
            "            # finished yet, because it will be finished in the `parse()` method, before",
            "            # self.items is used in the return value.",
            "            self.items.append((self._current_part.field_name, self._current_part.file))",
            "",
            "    def on_header_field(self, data: bytes, start: int, end: int) -> None:",
            "        self._current_partial_header_name += data[start:end]",
            "",
            "    def on_header_value(self, data: bytes, start: int, end: int) -> None:",
            "        self._current_partial_header_value += data[start:end]",
            "",
            "    def on_header_end(self) -> None:",
            "        field = self._current_partial_header_name.lower()",
            "        if field == b\"content-disposition\":",
            "            self._current_part.content_disposition = self._current_partial_header_value",
            "        self._current_part.item_headers.append(",
            "            (field, self._current_partial_header_value)",
            "        )",
            "        self._current_partial_header_name = b\"\"",
            "        self._current_partial_header_value = b\"\"",
            "",
            "    def on_headers_finished(self) -> None:",
            "        _, options = parse_options_header(self._current_part.content_disposition or b\"\")",
            "        try:",
            "            self._current_part.field_name = _user_safe_decode(",
            "                options[b\"name\"], str(self._charset)",
            "            )",
            "        except KeyError as e:",
            "            raise MultiPartException(",
            "                'The Content-Disposition header field \"name\" must be ' \"provided.\"",
            "            ) from e",
            "        if b\"filename\" in options:",
            "            self._current_files += 1",
            "            if self._current_files > self.max_files:",
            "                raise MultiPartException(",
            "                    f\"Too many files. Maximum number of files is {self.max_files}.\"",
            "                )",
            "            filename = _user_safe_decode(options[b\"filename\"], str(self._charset))",
            "            tempfile = NamedTemporaryFile(delete=False)",
            "            self._files_to_close_on_error.append(tempfile)",
            "            self._current_part.file = GradioUploadFile(",
            "                file=tempfile,  # type: ignore[arg-type]",
            "                size=0,",
            "                filename=filename,",
            "                headers=Headers(raw=self._current_part.item_headers),",
            "            )",
            "        else:",
            "            self._current_fields += 1",
            "            if self._current_fields > self.max_fields:",
            "                raise MultiPartException(",
            "                    f\"Too many fields. Maximum number of fields is {self.max_fields}.\"",
            "                )",
            "            self._current_part.file = None",
            "",
            "    def on_end(self) -> None:",
            "        pass",
            "",
            "    async def parse(self) -> FormData:",
            "        # Parse the Content-Type header to get the multipart boundary.",
            "        _, params = parse_options_header(self.headers[\"Content-Type\"])",
            "        charset = params.get(b\"charset\", \"utf-8\")",
            "        if isinstance(charset, bytes):",
            "            charset = charset.decode(\"latin-1\")",
            "        self._charset = charset",
            "        try:",
            "            boundary = params[b\"boundary\"]",
            "        except KeyError as e:",
            "            raise MultiPartException(\"Missing boundary in multipart.\") from e",
            "",
            "        # Callbacks dictionary.",
            "        callbacks: multipart.multipart.MultipartCallbacks = {",
            "            \"on_part_begin\": self.on_part_begin,",
            "            \"on_part_data\": self.on_part_data,",
            "            \"on_part_end\": self.on_part_end,",
            "            \"on_header_field\": self.on_header_field,",
            "            \"on_header_value\": self.on_header_value,",
            "            \"on_header_end\": self.on_header_end,",
            "            \"on_headers_finished\": self.on_headers_finished,",
            "            \"on_end\": self.on_end,",
            "        }",
            "",
            "        # Create the parser.",
            "        parser = multipart.MultipartParser(boundary, callbacks)",
            "        try:",
            "            # Feed the parser with data from the request.",
            "            async for chunk in self.stream:",
            "                parser.write(chunk)",
            "                # Write file data, it needs to use await with the UploadFile methods",
            "                # that call the corresponding file methods *in a threadpool*,",
            "                # otherwise, if they were called directly in the callback methods above",
            "                # (regular, non-async functions), that would block the event loop in",
            "                # the main thread.",
            "                for part, data in self._file_parts_to_write:",
            "                    assert part.file  # for type checkers  # noqa: S101",
            "                    await part.file.write(data)",
            "                    part.file.sha.update(data)  # type: ignore",
            "                    if os.stat(part.file.file.name).st_size > self.max_file_size:",
            "                        if self.upload_progress is not None:",
            "                            self.upload_progress.set_done(self.upload_id)  # type: ignore",
            "                        raise MultiPartException(",
            "                            f\"File size exceeded maximum allowed size of {self.max_file_size} bytes.\"",
            "                        )",
            "                for part in self._file_parts_to_finish:",
            "                    assert part.file  # for type checkers  # noqa: S101",
            "                    await part.file.seek(0)",
            "                self._file_parts_to_write.clear()",
            "                self._file_parts_to_finish.clear()",
            "        except MultiPartException as exc:",
            "            # Close all the files if there was an error.",
            "            for file in self._files_to_close_on_error:",
            "                file.close()",
            "                Path(file.name).unlink()",
            "            raise exc",
            "",
            "        parser.finalize()",
            "        if self.upload_progress is not None:",
            "            self.upload_progress.set_done(self.upload_id)  # type: ignore",
            "        return FormData(self.items)",
            "",
            "",
            "def move_uploaded_files_to_cache(files: list[str], destinations: list[str]) -> None:",
            "    for file, dest in zip(files, destinations):",
            "        shutil.move(file, dest)",
            "",
            "",
            "def update_root_in_config(config: BlocksConfigDict, root: str) -> BlocksConfigDict:",
            "    \"\"\"",
            "    Updates the root \"key\" in the config dictionary to the new root url. If the",
            "    root url has changed, all of the urls in the config that correspond to component",
            "    file urls are updated to use the new root url.",
            "    \"\"\"",
            "    with config_lock:",
            "        previous_root = config.get(\"root\")",
            "        if previous_root is None or previous_root != root:",
            "            config[\"root\"] = root",
            "            config = processing_utils.add_root_url(config, root, previous_root)  # type: ignore",
            "    return config",
            "",
            "",
            "def compare_passwords_securely(input_password: str, correct_password: str) -> bool:",
            "    return hmac.compare_digest(input_password.encode(), correct_password.encode())",
            "",
            "",
            "def starts_with_protocol(string: str) -> bool:",
            "    \"\"\"This regex matches strings that start with a scheme (one or more characters not including colon, slash, or space)",
            "    followed by ://, or start with just //, \\\\/, /\\\\, or \\\\ as they are interpreted as SMB paths on Windows.",
            "    \"\"\"",
            "    pattern = r\"^(?:[a-zA-Z][a-zA-Z0-9+\\-.]*://|//|\\\\\\\\|\\\\/|/\\\\)\"",
            "    return re.match(pattern, string) is not None",
            "",
            "",
            "def get_hostname(url: str) -> str:",
            "    \"\"\"",
            "    Returns the hostname of a given url, or an empty string if the url cannot be parsed.",
            "    Examples:",
            "        get_hostname(\"https://www.gradio.app\") -> \"www.gradio.app\"",
            "        get_hostname(\"localhost:7860\") -> \"localhost\"",
            "        get_hostname(\"127.0.0.1\") -> \"127.0.0.1\"",
            "    \"\"\"",
            "    if not url:",
            "        return \"\"",
            "    if \"://\" not in url:",
            "        url = \"http://\" + url",
            "    try:",
            "        return urlparse(url).hostname or \"\"",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "class CustomCORSMiddleware:",
            "    # This is a modified version of the Starlette CORSMiddleware that restricts the allowed origins when the host is localhost.",
            "    # Adapted from: https://github.com/encode/starlette/blob/89fae174a1ea10f59ae248fe030d9b7e83d0b8a0/starlette/middleware/cors.py",
            "",
            "    def __init__(",
            "        self,",
            "        app: ASGIApp,",
            "    ) -> None:",
            "        self.app = app",
            "        self.all_methods = (\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\", \"PATCH\", \"POST\", \"PUT\")",
            "        self.preflight_headers = {",
            "            \"Access-Control-Allow-Methods\": \", \".join(self.all_methods),",
            "            \"Access-Control-Max-Age\": str(600),",
            "            \"Access-Control-Allow-Credentials\": \"true\",",
            "        }",
            "        self.simple_headers = {\"Access-Control-Allow-Credentials\": \"true\"}",
            "        # Any of these hosts suggests that the Gradio app is running locally.",
            "        # Note: \"null\" is a special case that happens if a Gradio app is running",
            "        # as an embedded web component in a local static webpage.",
            "        self.localhost_aliases = [\"localhost\", \"127.0.0.1\", \"0.0.0.0\", \"null\"]",
            "",
            "    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:",
            "        if scope[\"type\"] != \"http\":",
            "            await self.app(scope, receive, send)",
            "            return",
            "        headers = Headers(scope=scope)",
            "        origin = headers.get(\"origin\")",
            "        if origin is None:",
            "            await self.app(scope, receive, send)",
            "            return",
            "        if scope[\"method\"] == \"OPTIONS\" and \"access-control-request-method\" in headers:",
            "            response = self.preflight_response(request_headers=headers)",
            "            await response(scope, receive, send)",
            "            return",
            "        await self.simple_response(scope, receive, send, request_headers=headers)",
            "",
            "    def preflight_response(self, request_headers: Headers) -> Response:",
            "        headers = dict(self.preflight_headers)",
            "        origin = request_headers[\"Origin\"]",
            "        if self.is_valid_origin(request_headers):",
            "            headers[\"Access-Control-Allow-Origin\"] = origin",
            "        requested_headers = request_headers.get(\"access-control-request-headers\")",
            "        if requested_headers is not None:",
            "            headers[\"Access-Control-Allow-Headers\"] = requested_headers",
            "        return PlainTextResponse(\"OK\", status_code=200, headers=headers)",
            "",
            "    async def simple_response(",
            "        self, scope: Scope, receive: Receive, send: Send, request_headers: Headers",
            "    ) -> None:",
            "        send = functools.partial(self._send, send=send, request_headers=request_headers)",
            "        await self.app(scope, receive, send)",
            "",
            "    async def _send(",
            "        self, message: Message, send: Send, request_headers: Headers",
            "    ) -> None:",
            "        if message[\"type\"] != \"http.response.start\":",
            "            await send(message)",
            "            return",
            "        message.setdefault(\"headers\", [])",
            "        headers = MutableHeaders(scope=message)",
            "        headers.update(self.simple_headers)",
            "        origin = request_headers[\"Origin\"]",
            "        if self.is_valid_origin(request_headers):",
            "            self.allow_explicit_origin(headers, origin)",
            "        await send(message)",
            "",
            "    def is_valid_origin(self, request_headers: Headers) -> bool:",
            "        origin = request_headers[\"Origin\"]",
            "        host = request_headers[\"Host\"]",
            "        host_name = get_hostname(host)",
            "        origin_name = get_hostname(origin)",
            "        return (",
            "            host_name not in self.localhost_aliases",
            "            or origin_name in self.localhost_aliases",
            "        )",
            "",
            "    @staticmethod",
            "    def allow_explicit_origin(headers: MutableHeaders, origin: str) -> None:",
            "        headers[\"Access-Control-Allow-Origin\"] = origin",
            "        headers.add_vary_header(\"Origin\")",
            "",
            "",
            "def delete_files_created_by_app(blocks: Blocks, age: int | None) -> None:",
            "    \"\"\"Delete files that are older than age. If age is None, delete all files.\"\"\"",
            "    dont_delete = set()",
            "    for component in blocks.blocks.values():",
            "        dont_delete.update(getattr(component, \"keep_in_cache\", set()))",
            "    for temp_set in blocks.temp_file_sets:",
            "        # We use a copy of the set to avoid modifying the set while iterating over it",
            "        # otherwise we would get an exception: Set changed size during iteration",
            "        to_remove = set()",
            "        for file in temp_set:",
            "            if file in dont_delete:",
            "                continue",
            "            try:",
            "                file_path = Path(file)",
            "                modified_time = datetime.fromtimestamp(file_path.lstat().st_ctime)",
            "                if age is None or (datetime.now() - modified_time).seconds > age:",
            "                    os.remove(file)",
            "                    to_remove.add(file)",
            "            except FileNotFoundError:",
            "                continue",
            "        temp_set -= to_remove",
            "",
            "",
            "async def delete_files_on_schedule(app: App, frequency: int, age: int) -> None:",
            "    \"\"\"Startup task to delete files created by the app based on time since last modification.\"\"\"",
            "    while True:",
            "        await asyncio.sleep(frequency)",
            "        await anyio.to_thread.run_sync(",
            "            delete_files_created_by_app, app.get_blocks(), age",
            "        )",
            "",
            "",
            "@asynccontextmanager",
            "async def _lifespan_handler(",
            "    app: App, frequency: int = 1, age: int = 1",
            ") -> AsyncGenerator:",
            "    \"\"\"A context manager that triggers the startup and shutdown events of the app.\"\"\"",
            "    asyncio.create_task(delete_files_on_schedule(app, frequency, age))",
            "    yield",
            "    delete_files_created_by_app(app.get_blocks(), age=None)",
            "",
            "",
            "async def _delete_state(app: App):",
            "    \"\"\"Delete all expired state every second.\"\"\"",
            "    while True:",
            "        app.state_holder.delete_all_expired_state()",
            "        await asyncio.sleep(1)",
            "",
            "",
            "@asynccontextmanager",
            "async def _delete_state_handler(app: App):",
            "    \"\"\"When the server launches, regularly delete expired state.\"\"\"",
            "    # The stop event needs to get the current event loop for python 3.8",
            "    # but the loop parameter is deprecated for 3.8+",
            "    if sys.version_info < (3, 10):",
            "        loop = asyncio.get_running_loop()",
            "        app.stop_event = asyncio.Event(loop=loop)",
            "    asyncio.create_task(_delete_state(app))",
            "    yield",
            "",
            "",
            "def create_lifespan_handler(",
            "    user_lifespan: Callable[[App], AsyncContextManager] | None,",
            "    frequency: int | None = 1,",
            "    age: int | None = 1,",
            ") -> Callable[[App], AsyncContextManager]:",
            "    \"\"\"Return a context manager that applies _lifespan_handler and user_lifespan if it exists.\"\"\"",
            "",
            "    @asynccontextmanager",
            "    async def _handler(app: App):",
            "        async with AsyncExitStack() as stack:",
            "            await stack.enter_async_context(_delete_state_handler(app))",
            "            if frequency and age:",
            "                await stack.enter_async_context(_lifespan_handler(app, frequency, age))",
            "            if user_lifespan is not None:",
            "                await stack.enter_async_context(user_lifespan(app))",
            "            yield",
            "",
            "    return _handler"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "238": [
                "compile_gr_request"
            ],
            "264": [
                "restore_session_state"
            ],
            "290": [
                "prepare_event_data"
            ],
            "302": []
        },
        "addLocation": []
    },
    "gradio/routes.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "     DataWithFiles,"
            },
            "1": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "     DeveloperPath,"
            },
            "2": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "     PredictBody,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+    PredictBodyInternal,"
            },
            "4": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "     ResetBody,"
            },
            "5": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "     SimplePredictBody,"
            },
            "6": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "     UserProvidedPath,"
            },
            "7": {
                "beforePatchRowNumber": 729,
                "afterPatchRowNumber": 730,
                "PatchRowcode": "                             route_path=f\"/hearbeat/{session_hash}\","
            },
            "8": {
                "beforePatchRowNumber": 730,
                "afterPatchRowNumber": 731,
                "PatchRowcode": "                             root_path=app.root_path,"
            },
            "9": {
                "beforePatchRowNumber": 731,
                "afterPatchRowNumber": 732,
                "PatchRowcode": "                         )"
            },
            "10": {
                "beforePatchRowNumber": 732,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        body = PredictBody("
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 733,
                "PatchRowcode": "+                        body = PredictBodyInternal("
            },
            "12": {
                "beforePatchRowNumber": 733,
                "afterPatchRowNumber": 734,
                "PatchRowcode": "                             session_hash=session_hash, data=[], request=request"
            },
            "13": {
                "beforePatchRowNumber": 734,
                "afterPatchRowNumber": 735,
                "PatchRowcode": "                         )"
            },
            "14": {
                "beforePatchRowNumber": 735,
                "afterPatchRowNumber": 736,
                "PatchRowcode": "                         unload_fn_indices = ["
            },
            "15": {
                "beforePatchRowNumber": 766,
                "afterPatchRowNumber": 767,
                "PatchRowcode": "             request: fastapi.Request,"
            },
            "16": {
                "beforePatchRowNumber": 767,
                "afterPatchRowNumber": 768,
                "PatchRowcode": "             username: str = Depends(get_current_user),"
            },
            "17": {
                "beforePatchRowNumber": 768,
                "afterPatchRowNumber": 769,
                "PatchRowcode": "         ):"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 770,
                "PatchRowcode": "+            body = PredictBodyInternal(**body.model_dump(), request=request)"
            },
            "19": {
                "beforePatchRowNumber": 769,
                "afterPatchRowNumber": 771,
                "PatchRowcode": "             fn = route_utils.get_fn("
            },
            "20": {
                "beforePatchRowNumber": 770,
                "afterPatchRowNumber": 772,
                "PatchRowcode": "                 blocks=app.get_blocks(), api_name=api_name, body=body"
            },
            "21": {
                "beforePatchRowNumber": 771,
                "afterPatchRowNumber": 773,
                "PatchRowcode": "             )"
            },
            "22": {
                "beforePatchRowNumber": 775,
                "afterPatchRowNumber": 777,
                "PatchRowcode": "                     detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\","
            },
            "23": {
                "beforePatchRowNumber": 776,
                "afterPatchRowNumber": 778,
                "PatchRowcode": "                     status_code=status.HTTP_404_NOT_FOUND,"
            },
            "24": {
                "beforePatchRowNumber": 777,
                "afterPatchRowNumber": 779,
                "PatchRowcode": "                 )"
            },
            "25": {
                "beforePatchRowNumber": 778,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "26": {
                "beforePatchRowNumber": 779,
                "afterPatchRowNumber": 780,
                "PatchRowcode": "             gr_request = route_utils.compile_gr_request("
            },
            "27": {
                "beforePatchRowNumber": 780,
                "afterPatchRowNumber": 781,
                "PatchRowcode": "                 body,"
            },
            "28": {
                "beforePatchRowNumber": 781,
                "afterPatchRowNumber": 782,
                "PatchRowcode": "                 fn=fn,"
            },
            "29": {
                "beforePatchRowNumber": 810,
                "afterPatchRowNumber": 811,
                "PatchRowcode": "             request: fastapi.Request,"
            },
            "30": {
                "beforePatchRowNumber": 811,
                "afterPatchRowNumber": 812,
                "PatchRowcode": "             username: str = Depends(get_current_user),"
            },
            "31": {
                "beforePatchRowNumber": 812,
                "afterPatchRowNumber": 813,
                "PatchRowcode": "         ):"
            },
            "32": {
                "beforePatchRowNumber": 813,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            full_body = PredictBody("
            },
            "33": {
                "beforePatchRowNumber": 814,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                **body.model_dump(), request=request, simple_format=True"
            },
            "34": {
                "beforePatchRowNumber": 815,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 814,
                "PatchRowcode": "+            full_body = PredictBody(**body.model_dump(), simple_format=True)"
            },
            "36": {
                "beforePatchRowNumber": 816,
                "afterPatchRowNumber": 815,
                "PatchRowcode": "             fn = route_utils.get_fn("
            },
            "37": {
                "beforePatchRowNumber": 817,
                "afterPatchRowNumber": 816,
                "PatchRowcode": "                 blocks=app.get_blocks(), api_name=api_name, body=full_body"
            },
            "38": {
                "beforePatchRowNumber": 818,
                "afterPatchRowNumber": 817,
                "PatchRowcode": "             )"
            },
            "39": {
                "beforePatchRowNumber": 847,
                "afterPatchRowNumber": 846,
                "PatchRowcode": "                     status_code=status.HTTP_503_SERVICE_UNAVAILABLE,"
            },
            "40": {
                "beforePatchRowNumber": 848,
                "afterPatchRowNumber": 847,
                "PatchRowcode": "                     detail=\"Queue is stopped.\","
            },
            "41": {
                "beforePatchRowNumber": 849,
                "afterPatchRowNumber": 848,
                "PatchRowcode": "                 )"
            },
            "42": {
                "beforePatchRowNumber": 850,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 849,
                "PatchRowcode": "+            body = PredictBodyInternal(**body.model_dump(), request=request)"
            },
            "44": {
                "beforePatchRowNumber": 851,
                "afterPatchRowNumber": 850,
                "PatchRowcode": "             success, event_id = await blocks._queue.push("
            },
            "45": {
                "beforePatchRowNumber": 852,
                "afterPatchRowNumber": 851,
                "PatchRowcode": "                 body=body, request=request, username=username"
            },
            "46": {
                "beforePatchRowNumber": 853,
                "afterPatchRowNumber": 852,
                "PatchRowcode": "             )"
            }
        },
        "frontPatchFile": [
            "\"\"\"Implements a FastAPI server to run the gradio interface. Note that some types in this",
            "module use the Optional/Union notation so that they work correctly with pydantic.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import contextlib",
            "import math",
            "import sys",
            "import warnings",
            "",
            "if sys.version_info >= (3, 9):",
            "    from importlib.resources import files",
            "else:",
            "    from importlib_resources import files",
            "import hashlib",
            "import inspect",
            "import json",
            "import mimetypes",
            "import os",
            "import secrets",
            "import time",
            "import traceback",
            "from pathlib import Path",
            "from queue import Empty as EmptyQueue",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    AsyncIterator,",
            "    Callable,",
            "    Dict,",
            "    List,",
            "    Literal,",
            "    Optional,",
            "    Type,",
            "    Union,",
            "    cast,",
            ")",
            "",
            "import fastapi",
            "import httpx",
            "import markupsafe",
            "import orjson",
            "from fastapi import (",
            "    BackgroundTasks,",
            "    Depends,",
            "    FastAPI,",
            "    HTTPException,",
            "    status,",
            ")",
            "from fastapi.responses import (",
            "    FileResponse,",
            "    HTMLResponse,",
            "    JSONResponse,",
            "    PlainTextResponse,",
            ")",
            "from fastapi.security import OAuth2PasswordRequestForm",
            "from fastapi.templating import Jinja2Templates",
            "from gradio_client import utils as client_utils",
            "from gradio_client.documentation import document",
            "from gradio_client.utils import ServerMessage",
            "from jinja2.exceptions import TemplateNotFound",
            "from multipart.multipart import parse_options_header",
            "from starlette.background import BackgroundTask",
            "from starlette.datastructures import UploadFile as StarletteUploadFile",
            "from starlette.responses import RedirectResponse, StreamingResponse",
            "",
            "import gradio",
            "from gradio import ranged_response, route_utils, utils, wasm_utils",
            "from gradio.context import Context",
            "from gradio.data_classes import (",
            "    CancelBody,",
            "    ComponentServerBlobBody,",
            "    ComponentServerJSONBody,",
            "    DataWithFiles,",
            "    DeveloperPath,",
            "    PredictBody,",
            "    ResetBody,",
            "    SimplePredictBody,",
            "    UserProvidedPath,",
            ")",
            "from gradio.exceptions import InvalidPathError",
            "from gradio.oauth import attach_oauth",
            "from gradio.route_utils import (  # noqa: F401",
            "    CustomCORSMiddleware,",
            "    FileUploadProgress,",
            "    FileUploadProgressNotQueuedError,",
            "    FileUploadProgressNotTrackedError,",
            "    GradioMultiPartParser,",
            "    GradioUploadFile,",
            "    MultiPartException,",
            "    Request,",
            "    compare_passwords_securely,",
            "    create_lifespan_handler,",
            "    move_uploaded_files_to_cache,",
            ")",
            "from gradio.server_messages import (",
            "    CloseStreamMessage,",
            "    EstimationMessage,",
            "    EventMessage,",
            "    HeartbeatMessage,",
            "    ProcessCompletedMessage,",
            "    ProcessGeneratingMessage,",
            "    UnexpectedErrorMessage,",
            ")",
            "from gradio.state_holder import StateHolder",
            "from gradio.utils import cancel_tasks, get_package_version, get_upload_folder",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import Block",
            "",
            "",
            "mimetypes.init()",
            "",
            "STATIC_TEMPLATE_LIB = cast(",
            "    DeveloperPath,",
            "    files(\"gradio\").joinpath(\"templates\").as_posix(),  # type: ignore",
            ")",
            "STATIC_PATH_LIB = cast(",
            "    DeveloperPath,",
            "    files(\"gradio\").joinpath(\"templates\", \"frontend\", \"static\").as_posix(),  # type: ignore",
            ")",
            "BUILD_PATH_LIB = cast(",
            "    DeveloperPath,",
            "    files(\"gradio\").joinpath(\"templates\", \"frontend\", \"assets\").as_posix(),  # type: ignore",
            ")",
            "VERSION = get_package_version()",
            "XSS_VULNERABLE_EXTENSIONS = [",
            "    \".html\",",
            "    \".htm\",",
            "    \".js\",",
            "    \".php\",",
            "    \".asp\",",
            "    \".aspx\",",
            "    \".jsp\",",
            "    \".xml\",",
            "    \".svg\",",
            "]",
            "",
            "",
            "class ORJSONResponse(JSONResponse):",
            "    media_type = \"application/json\"",
            "",
            "    @staticmethod",
            "    def _render(content: Any) -> bytes:",
            "        return orjson.dumps(",
            "            content,",
            "            option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_PASSTHROUGH_DATETIME,",
            "            default=str,",
            "        )",
            "",
            "    def render(self, content: Any) -> bytes:",
            "        return ORJSONResponse._render(content)",
            "",
            "    @staticmethod",
            "    def _render_str(content: Any) -> str:",
            "        return ORJSONResponse._render(content).decode(\"utf-8\")",
            "",
            "",
            "def toorjson(value):",
            "    return markupsafe.Markup(",
            "        ORJSONResponse._render_str(value)",
            "        .replace(\"<\", \"\\\\u003c\")",
            "        .replace(\">\", \"\\\\u003e\")",
            "        .replace(\"&\", \"\\\\u0026\")",
            "        .replace(\"'\", \"\\\\u0027\")",
            "    )",
            "",
            "",
            "templates = Jinja2Templates(directory=STATIC_TEMPLATE_LIB)",
            "templates.env.filters[\"toorjson\"] = toorjson",
            "",
            "client = httpx.AsyncClient()",
            "",
            "file_upload_statuses = FileUploadProgress()",
            "",
            "",
            "class App(FastAPI):",
            "    \"\"\"",
            "    FastAPI App Wrapper",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        auth_dependency: Callable[[fastapi.Request], str | None] | None = None,",
            "        **kwargs,",
            "    ):",
            "        self.tokens = {}",
            "        self.auth = None",
            "        self.analytics_key = secrets.token_urlsafe(16)",
            "        self.monitoring_enabled = False",
            "        self.blocks: gradio.Blocks | None = None",
            "        self.state_holder = StateHolder()",
            "        self.iterators: dict[str, AsyncIterator] = {}",
            "        self.iterators_to_reset: set[str] = set()",
            "        self.lock = utils.safe_get_lock()",
            "        self.stop_event = utils.safe_get_stop_event()",
            "        self.cookie_id = secrets.token_urlsafe(32)",
            "        self.queue_token = secrets.token_urlsafe(32)",
            "        self.startup_events_triggered = False",
            "        self.uploaded_file_dir = get_upload_folder()",
            "        self.change_count: int = 0",
            "        self.change_type: Literal[\"reload\", \"error\"] | None = None",
            "        self.reload_error_message: str | None = None",
            "        self._asyncio_tasks: list[asyncio.Task] = []",
            "        self.auth_dependency = auth_dependency",
            "        self.api_info = None",
            "        self.all_app_info = None",
            "        # Allow user to manually set `docs_url` and `redoc_url`",
            "        # when instantiating an App; when they're not set, disable docs and redoc.",
            "        kwargs.setdefault(\"docs_url\", None)",
            "        kwargs.setdefault(\"redoc_url\", None)",
            "        self.custom_component_hashes: dict[str, str] = {}",
            "        super().__init__(**kwargs)",
            "",
            "    def configure_app(self, blocks: gradio.Blocks) -> None:",
            "        auth = blocks.auth",
            "        if auth is not None:",
            "            if not callable(auth):",
            "                self.auth = {account[0]: account[1] for account in auth}",
            "            else:",
            "                self.auth = auth",
            "        else:",
            "            self.auth = None",
            "",
            "        self.blocks = blocks",
            "        self.cwd = os.getcwd()",
            "        self.favicon_path = blocks.favicon_path",
            "        self.tokens = {}",
            "        self.root_path = blocks.root_path",
            "        self.state_holder.set_blocks(blocks)",
            "",
            "    def get_blocks(self) -> gradio.Blocks:",
            "        if self.blocks is None:",
            "            raise ValueError(\"No Blocks has been configured for this app.\")",
            "        return self.blocks",
            "",
            "    def build_proxy_request(self, url_path):",
            "        url = httpx.URL(url_path)",
            "        assert self.blocks  # noqa: S101",
            "        # Don't proxy a URL unless it's a URL specifically loaded by the user using",
            "        # gr.load() to prevent SSRF or harvesting of HF tokens by malicious Spaces.",
            "        is_safe_url = any(",
            "            url.host == httpx.URL(root).host for root in self.blocks.proxy_urls",
            "        )",
            "        if not is_safe_url:",
            "            raise PermissionError(\"This URL cannot be proxied.\")",
            "        is_hf_url = url.host.endswith(\".hf.space\")",
            "        headers = {}",
            "        if Context.hf_token is not None and is_hf_url:",
            "            headers[\"Authorization\"] = f\"Bearer {Context.hf_token}\"",
            "        rp_req = client.build_request(\"GET\", url, headers=headers)",
            "        return rp_req",
            "",
            "    def _cancel_asyncio_tasks(self):",
            "        for task in self._asyncio_tasks:",
            "            task.cancel()",
            "        self._asyncio_tasks = []",
            "",
            "    @staticmethod",
            "    def create_app(",
            "        blocks: gradio.Blocks,",
            "        app_kwargs: Dict[str, Any] | None = None,",
            "        auth_dependency: Callable[[fastapi.Request], str | None] | None = None,",
            "    ) -> App:",
            "        app_kwargs = app_kwargs or {}",
            "        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)",
            "        delete_cache = blocks.delete_cache or (None, None)",
            "        app_kwargs[\"lifespan\"] = create_lifespan_handler(",
            "            app_kwargs.get(\"lifespan\", None), *delete_cache",
            "        )",
            "        app = App(auth_dependency=auth_dependency, **app_kwargs)",
            "        app.configure_app(blocks)",
            "",
            "        if not wasm_utils.IS_WASM:",
            "            app.add_middleware(CustomCORSMiddleware)",
            "",
            "        @app.get(\"/user\")",
            "        @app.get(\"/user/\")",
            "        def get_current_user(request: fastapi.Request) -> Optional[str]:",
            "            if app.auth_dependency is not None:",
            "                return app.auth_dependency(request)",
            "            token = request.cookies.get(",
            "                f\"access-token-{app.cookie_id}\"",
            "            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")",
            "            return app.tokens.get(token)",
            "",
            "        @app.get(\"/login_check\")",
            "        @app.get(\"/login_check/\")",
            "        def login_check(user: str = Depends(get_current_user)):",
            "            if (app.auth is None and app.auth_dependency is None) or user is not None:",
            "                return",
            "            raise HTTPException(",
            "                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"",
            "            )",
            "",
            "        @app.get(\"/token\")",
            "        @app.get(\"/token/\")",
            "        def get_token(request: fastapi.Request) -> dict:",
            "            token = request.cookies.get(f\"access-token-{app.cookie_id}\")",
            "            return {\"token\": token, \"user\": app.tokens.get(token)}",
            "",
            "        @app.get(\"/app_id\")",
            "        @app.get(\"/app_id/\")",
            "        def app_id(request: fastapi.Request) -> dict:  # noqa: ARG001",
            "            return {\"app_id\": app.get_blocks().app_id}",
            "",
            "        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])",
            "        async def notify_changes(",
            "            request: fastapi.Request,",
            "        ):",
            "            async def reload_checker(request: fastapi.Request):",
            "                heartbeat_rate = 15",
            "                check_rate = 0.05",
            "                last_heartbeat = time.perf_counter()",
            "                current_count = app.change_count",
            "",
            "                while True:",
            "                    if await request.is_disconnected():",
            "                        return",
            "",
            "                    if app.change_count != current_count:",
            "                        current_count = app.change_count",
            "                        msg = (",
            "                            json.dumps(f\"{app.reload_error_message}\")",
            "                            if app.change_type == \"error\"",
            "                            else \"{}\"",
            "                        )",
            "                        yield f\"\"\"event: {app.change_type}\\ndata: {msg}\\n\\n\"\"\"",
            "",
            "                    await asyncio.sleep(check_rate)",
            "                    if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                        yield \"\"\"event: heartbeat\\ndata: {}\\n\\n\"\"\"",
            "                        last_heartbeat = time.time()",
            "",
            "            return StreamingResponse(",
            "                reload_checker(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/login\")",
            "        @app.post(\"/login/\")",
            "        def login(form_data: OAuth2PasswordRequestForm = Depends()):",
            "            username, password = form_data.username.strip(), form_data.password",
            "            if app.auth is None:",
            "                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)",
            "            if (",
            "                not callable(app.auth)",
            "                and username in app.auth",
            "                and compare_passwords_securely(password, app.auth[username])  # type: ignore",
            "            ) or (callable(app.auth) and app.auth.__call__(username, password)):  # type: ignore",
            "                token = secrets.token_urlsafe(16)",
            "                app.tokens[token] = username",
            "                response = JSONResponse(content={\"success\": True})",
            "                response.set_cookie(",
            "                    key=f\"access-token-{app.cookie_id}\",",
            "                    value=token,",
            "                    httponly=True,",
            "                    samesite=\"none\",",
            "                    secure=True,",
            "                )",
            "                response.set_cookie(",
            "                    key=f\"access-token-unsecure-{app.cookie_id}\",",
            "                    value=token,",
            "                    httponly=True,",
            "                )",
            "                return response",
            "            else:",
            "                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")",
            "",
            "        ###############",
            "        # OAuth Routes",
            "        ###############",
            "",
            "        # Define OAuth routes if the app expects it (i.e. a LoginButton is defined).",
            "        # It allows users to \"Sign in with HuggingFace\". Otherwise, add the default",
            "        # logout route.",
            "        if app.blocks is not None and app.blocks.expects_oauth:",
            "            attach_oauth(app)",
            "        else:",
            "",
            "            @app.get(\"/logout\")",
            "            def logout(user: str = Depends(get_current_user)):",
            "                response = RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)",
            "                response.delete_cookie(key=f\"access-token-{app.cookie_id}\", path=\"/\")",
            "                response.delete_cookie(",
            "                    key=f\"access-token-unsecure-{app.cookie_id}\", path=\"/\"",
            "                )",
            "                # A user may have multiple tokens, so we need to delete all of them.",
            "                for token in list(app.tokens.keys()):",
            "                    if app.tokens[token] == user:",
            "                        del app.tokens[token]",
            "                return response",
            "",
            "        ###############",
            "        # Main Routes",
            "        ###############",
            "",
            "        @app.head(\"/\", response_class=HTMLResponse)",
            "        @app.get(\"/\", response_class=HTMLResponse)",
            "        def main(request: fastapi.Request, user: str = Depends(get_current_user)):",
            "            mimetypes.add_type(\"application/javascript\", \".js\")",
            "            blocks = app.get_blocks()",
            "            root = route_utils.get_root_url(",
            "                request=request, route_path=\"/\", root_path=app.root_path",
            "            )",
            "            if (app.auth is None and app.auth_dependency is None) or user is not None:",
            "                config = blocks.config",
            "                config = route_utils.update_root_in_config(config, root)",
            "                config[\"username\"] = user",
            "            elif app.auth_dependency:",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"",
            "                )",
            "            else:",
            "                config = {",
            "                    \"auth_required\": True,",
            "                    \"auth_message\": blocks.auth_message,",
            "                    \"space_id\": blocks.space_id,",
            "                    \"root\": root,",
            "                }",
            "",
            "            try:",
            "                template = (",
            "                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"",
            "                )",
            "                gradio_api_info = api_info(False)",
            "                return templates.TemplateResponse(",
            "                    template,",
            "                    {",
            "                        \"request\": request,",
            "                        \"config\": config,",
            "                        \"gradio_api_info\": gradio_api_info,",
            "                    },",
            "                )",
            "            except TemplateNotFound as err:",
            "                if blocks.share:",
            "                    raise ValueError(",
            "                        \"Did you install Gradio from source files? Share mode only \"",
            "                        \"works when Gradio is installed through the pip package.\"",
            "                    ) from err",
            "                else:",
            "                    raise ValueError(",
            "                        \"Did you install Gradio from source files? You need to build \"",
            "                        \"the frontend by running /scripts/build_frontend.sh\"",
            "                    ) from err",
            "",
            "        @app.get(\"/info/\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/info\", dependencies=[Depends(login_check)])",
            "        def api_info(all_endpoints: bool = False):",
            "            if all_endpoints:",
            "                if not app.all_app_info:",
            "                    app.all_app_info = app.get_blocks().get_api_info(all_endpoints=True)",
            "                return app.all_app_info",
            "            if not app.api_info:",
            "                app.api_info = app.get_blocks().get_api_info()",
            "            return app.api_info",
            "",
            "        @app.get(\"/config/\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/config\", dependencies=[Depends(login_check)])",
            "        def get_config(request: fastapi.Request):",
            "            config = app.get_blocks().config",
            "            root = route_utils.get_root_url(",
            "                request=request, route_path=\"/config\", root_path=app.root_path",
            "            )",
            "            config = route_utils.update_root_in_config(config, root)",
            "            config[\"username\"] = get_current_user(request)",
            "            return ORJSONResponse(content=config)",
            "",
            "        @app.get(\"/static/{path:path}\")",
            "        def static_resource(path: str):",
            "            static_file = routes_safe_join(STATIC_PATH_LIB, UserProvidedPath(path))",
            "            return FileResponse(static_file)",
            "",
            "        @app.get(\"/custom_component/{id}/{type}/{file_name}\")",
            "        def custom_component_path(",
            "            id: str, type: str, file_name: str, req: fastapi.Request",
            "        ):",
            "            config = app.get_blocks().config",
            "            components = config[\"components\"]",
            "            location = next(",
            "                (item for item in components if item[\"component_class_id\"] == id), None",
            "            )",
            "            if location is None:",
            "                raise HTTPException(status_code=404, detail=\"Component not found.\")",
            "",
            "            component_instance = app.get_blocks().get_component(location[\"id\"])",
            "",
            "            module_name = component_instance.__class__.__module__",
            "            module_path = sys.modules[module_name].__file__",
            "",
            "            if module_path is None or component_instance is None:",
            "                raise HTTPException(status_code=404, detail=\"Component not found.\")",
            "",
            "            requested_path = utils.safe_join(",
            "                component_instance.__class__.TEMPLATE_DIR,",
            "                UserProvidedPath(f\"{type}/{file_name}\"),",
            "            )",
            "",
            "            path = routes_safe_join(",
            "                DeveloperPath(str(Path(module_path).parent)),",
            "                UserProvidedPath(requested_path),",
            "            )",
            "",
            "            key = f\"{id}-{type}-{file_name}\"",
            "",
            "            if key not in app.custom_component_hashes:",
            "                app.custom_component_hashes[key] = hashlib.md5(",
            "                    Path(path).read_text(encoding=\"utf-8\").encode()",
            "                ).hexdigest()",
            "",
            "            version = app.custom_component_hashes.get(key)",
            "            headers = {\"Cache-Control\": \"max-age=0, must-revalidate\"}",
            "            if version:",
            "                headers[\"ETag\"] = version",
            "",
            "            if version and req.headers.get(\"if-none-match\") == version:",
            "                return PlainTextResponse(status_code=304, headers=headers)",
            "",
            "            return FileResponse(path, headers=headers)",
            "",
            "        @app.get(\"/assets/{path:path}\")",
            "        def build_resource(path: str):",
            "            build_file = routes_safe_join(BUILD_PATH_LIB, UserProvidedPath(path))",
            "            return FileResponse(build_file)",
            "",
            "        @app.get(\"/favicon.ico\")",
            "        async def favicon():",
            "            blocks = app.get_blocks()",
            "            if blocks.favicon_path is None:",
            "                return static_resource(\"img/logo.svg\")",
            "            else:",
            "                return FileResponse(blocks.favicon_path)",
            "",
            "        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])",
            "        async def reverse_proxy(url_path: str):",
            "            # Adapted from: https://github.com/tiangolo/fastapi/issues/1788",
            "            try:",
            "                rp_req = app.build_proxy_request(url_path)",
            "            except PermissionError as err:",
            "                raise HTTPException(status_code=400, detail=str(err)) from err",
            "            rp_resp = await client.send(rp_req, stream=True)",
            "            file_extension = os.path.splitext(url_path)[1].lower()",
            "            if file_extension in XSS_VULNERABLE_EXTENSIONS:",
            "                rp_resp.headers.update({\"Content-Disposition\": \"attachment\"})",
            "                rp_resp.headers.update({\"Content-Type\": \"application/octet-stream\"})",
            "            return StreamingResponse(",
            "                rp_resp.aiter_raw(),",
            "                status_code=rp_resp.status_code,",
            "                headers=rp_resp.headers,  # type: ignore",
            "                background=BackgroundTask(rp_resp.aclose),",
            "            )",
            "",
            "        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])",
            "        async def file(path_or_url: str, request: fastapi.Request):",
            "            blocks = app.get_blocks()",
            "            if client_utils.is_http_url_like(path_or_url):",
            "                return RedirectResponse(",
            "                    url=path_or_url, status_code=status.HTTP_302_FOUND",
            "                )",
            "",
            "            if route_utils.starts_with_protocol(path_or_url):",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            abs_path = utils.abspath(path_or_url)",
            "",
            "            in_blocklist = any(",
            "                utils.is_in_or_equal(abs_path, blocked_path)",
            "                for blocked_path in blocks.blocked_paths",
            "            )",
            "",
            "            is_dir = abs_path.is_dir()",
            "",
            "            if is_dir or in_blocklist:",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            created_by_app = False",
            "            for temp_file_set in blocks.temp_file_sets:",
            "                if abs_path in temp_file_set:",
            "                    created_by_app = True",
            "                    break",
            "            in_allowlist = any(",
            "                utils.is_in_or_equal(abs_path, allowed_path)",
            "                for allowed_path in blocks.allowed_paths",
            "            )",
            "            is_static_file = utils.is_static_file(abs_path)",
            "            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)",
            "            is_cached_example = utils.is_in_or_equal(",
            "                abs_path, utils.abspath(utils.get_cache_folder())",
            "            )",
            "",
            "            if not (",
            "                created_by_app",
            "                or in_allowlist",
            "                or was_uploaded",
            "                or is_cached_example",
            "                or is_static_file",
            "            ):",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            if not abs_path.exists():",
            "                raise HTTPException(404, f\"File not found: {path_or_url}.\")",
            "",
            "            mime_type, _ = mimetypes.guess_type(abs_path)",
            "            file_extension = os.path.splitext(abs_path)[1].lower()",
            "",
            "            if file_extension in XSS_VULNERABLE_EXTENSIONS:",
            "                media_type = \"application/octet-stream\"",
            "                content_disposition_type = \"attachment\"",
            "            else:",
            "                media_type = mime_type or \"application/octet-stream\"",
            "                content_disposition_type = \"inline\"",
            "",
            "            range_val = request.headers.get(\"Range\", \"\").strip()",
            "            if range_val.startswith(\"bytes=\") and \"-\" in range_val:",
            "                range_val = range_val[6:]",
            "                start, end = range_val.split(\"-\")",
            "                if start.isnumeric() and end.isnumeric():",
            "                    start = int(start)",
            "                    end = int(end)",
            "                    headers = dict(request.headers)",
            "                    headers[\"Content-Disposition\"] = content_disposition_type",
            "                    headers[\"Content-Type\"] = media_type",
            "                    response = ranged_response.RangedFileResponse(",
            "                        abs_path,",
            "                        ranged_response.OpenRange(start, end),",
            "                        headers,",
            "                        stat_result=os.stat(abs_path),",
            "                    )",
            "                    return response",
            "",
            "            return FileResponse(",
            "                abs_path,",
            "                headers={\"Accept-Ranges\": \"bytes\"},",
            "                content_disposition_type=content_disposition_type,",
            "                media_type=media_type,",
            "                filename=abs_path.name,",
            "            )",
            "",
            "        @app.get(",
            "            \"/stream/{session_hash}/{run}/{component_id}\",",
            "            dependencies=[Depends(login_check)],",
            "        )",
            "        async def stream(",
            "            session_hash: str,",
            "            run: int,",
            "            component_id: int,",
            "            request: fastapi.Request,  # noqa: ARG001",
            "        ):",
            "            stream: list = (",
            "                app.get_blocks()",
            "                .pending_streams[session_hash]",
            "                .get(run, {})",
            "                .get(component_id, None)",
            "            )",
            "            if stream is None:",
            "                raise HTTPException(404, \"Stream not found.\")",
            "",
            "            def stream_wrapper():",
            "                check_stream_rate = 0.01",
            "                max_wait_time = 120  # maximum wait between yields - assume generator thread has crashed otherwise.",
            "                wait_time = 0",
            "                while True:",
            "                    if len(stream) == 0:",
            "                        if wait_time > max_wait_time:",
            "                            return",
            "                        wait_time += check_stream_rate",
            "                        time.sleep(check_stream_rate)",
            "                        continue",
            "                    wait_time = 0",
            "                    next_stream = stream.pop(0)",
            "                    if next_stream is None:",
            "                        return",
            "                    yield next_stream",
            "",
            "            return StreamingResponse(stream_wrapper())",
            "",
            "        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])",
            "        async def file_deprecated(path: str, request: fastapi.Request):",
            "            return await file(path, request)",
            "",
            "        @app.post(\"/reset/\")",
            "        @app.post(\"/reset\")",
            "        async def reset_iterator(body: ResetBody):  # noqa: ARG001",
            "            # No-op, all the cancelling/reset logic handled by /cancel",
            "            return {\"success\": True}",
            "",
            "        @app.get(\"/heartbeat/{session_hash}\")",
            "        def heartbeat(",
            "            session_hash: str,",
            "            request: fastapi.Request,",
            "            background_tasks: BackgroundTasks,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            \"\"\"Clients make a persistent connection to this endpoint to keep the session alive.",
            "            When the client disconnects, the session state is deleted.",
            "            \"\"\"",
            "            heartbeat_rate = 0.25 if os.getenv(\"GRADIO_IS_E2E_TEST\", None) else 15",
            "",
            "            async def wait():",
            "                await asyncio.sleep(heartbeat_rate)",
            "                return \"wait\"",
            "",
            "            async def stop_stream():",
            "                await app.stop_event.wait()",
            "                return \"stop\"",
            "",
            "            async def iterator():",
            "                while True:",
            "                    try:",
            "                        yield \"data: ALIVE\\n\\n\"",
            "                        # We need to close the heartbeat connections as soon as the server stops",
            "                        # otherwise the server can take forever to close",
            "                        wait_task = asyncio.create_task(wait())",
            "                        stop_stream_task = asyncio.create_task(stop_stream())",
            "                        done, _ = await asyncio.wait(",
            "                            [wait_task, stop_stream_task],",
            "                            return_when=asyncio.FIRST_COMPLETED,",
            "                        )",
            "                        done = [d.result() for d in done]",
            "                        if \"stop\" in done:",
            "                            raise asyncio.CancelledError()",
            "                    except asyncio.CancelledError:",
            "                        req = Request(request, username, session_hash=session_hash)",
            "                        root_path = route_utils.get_root_url(",
            "                            request=request,",
            "                            route_path=f\"/hearbeat/{session_hash}\",",
            "                            root_path=app.root_path,",
            "                        )",
            "                        body = PredictBody(",
            "                            session_hash=session_hash, data=[], request=request",
            "                        )",
            "                        unload_fn_indices = [",
            "                            i",
            "                            for i, dep in app.get_blocks().fns.items()",
            "                            if any(t for t in dep.targets if t[1] == \"unload\")",
            "                        ]",
            "                        for fn_index in unload_fn_indices:",
            "                            # The task runnning this loop has been cancelled",
            "                            # so we add tasks in the background",
            "                            background_tasks.add_task(",
            "                                route_utils.call_process_api,",
            "                                app=app,",
            "                                body=body,",
            "                                gr_request=req,",
            "                                fn=app.get_blocks().fns[fn_index],",
            "                                root_path=root_path,",
            "                            )",
            "                        # This will mark the state to be deleted in an hour",
            "                        if session_hash in app.state_holder.session_data:",
            "                            app.state_holder.session_data[session_hash].is_closed = True",
            "                        return",
            "",
            "            return StreamingResponse(iterator(), media_type=\"text/event-stream\")",
            "",
            "        # had to use '/run' endpoint for Colab compatibility, '/api' supported for backwards compatibility",
            "        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])",
            "        async def predict(",
            "            api_name: str,",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            fn = route_utils.get_fn(",
            "                blocks=app.get_blocks(), api_name=api_name, body=body",
            "            )",
            "",
            "            if not app.get_blocks().api_open and fn.queue:",
            "                raise HTTPException(",
            "                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",",
            "                    status_code=status.HTTP_404_NOT_FOUND,",
            "                )",
            "",
            "            gr_request = route_utils.compile_gr_request(",
            "                body,",
            "                fn=fn,",
            "                username=username,",
            "                request=request,",
            "            )",
            "            root_path = route_utils.get_root_url(",
            "                request=request, route_path=f\"/api/{api_name}\", root_path=app.root_path",
            "            )",
            "            try:",
            "                output = await route_utils.call_process_api(",
            "                    app=app,",
            "                    body=body,",
            "                    gr_request=gr_request,",
            "                    fn=fn,",
            "                    root_path=root_path,",
            "                )",
            "            except BaseException as error:",
            "                content = utils.error_payload(error, app.get_blocks().show_error)",
            "                traceback.print_exc()",
            "                return JSONResponse(",
            "                    content=content,",
            "                    status_code=500,",
            "                )",
            "            return output",
            "",
            "        @app.post(\"/call/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/call/{api_name}/\", dependencies=[Depends(login_check)])",
            "        async def simple_predict_post(",
            "            api_name: str,",
            "            body: SimplePredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            full_body = PredictBody(",
            "                **body.model_dump(), request=request, simple_format=True",
            "            )",
            "            fn = route_utils.get_fn(",
            "                blocks=app.get_blocks(), api_name=api_name, body=full_body",
            "            )",
            "            full_body.fn_index = fn._id",
            "            return await queue_join_helper(full_body, request, username)",
            "",
            "        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])",
            "        async def queue_join(",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            if body.session_hash is None:",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_400_BAD_REQUEST,",
            "                    detail=\"Session hash not found.\",",
            "                )",
            "            return await queue_join_helper(body, request, username)",
            "",
            "        async def queue_join_helper(",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str,",
            "        ):",
            "            blocks = app.get_blocks()",
            "",
            "            if blocks._queue.server_app is None:",
            "                blocks._queue.set_server_app(app)",
            "",
            "            if blocks._queue.stopped:",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,",
            "                    detail=\"Queue is stopped.\",",
            "                )",
            "",
            "            success, event_id = await blocks._queue.push(",
            "                body=body, request=request, username=username",
            "            )",
            "            if not success:",
            "                status_code = (",
            "                    status.HTTP_503_SERVICE_UNAVAILABLE",
            "                    if \"Queue is full.\" in event_id",
            "                    else status.HTTP_400_BAD_REQUEST",
            "                )",
            "                raise HTTPException(status_code=status_code, detail=event_id)",
            "            return {\"event_id\": event_id}",
            "",
            "        @app.post(\"/cancel\")",
            "        async def cancel_event(body: CancelBody):",
            "            await cancel_tasks({f\"{body.session_hash}_{body.fn_index}\"})",
            "            blocks = app.get_blocks()",
            "            # Need to complete the job so that the client disconnects",
            "            session_open = (",
            "                body.session_hash in blocks._queue.pending_messages_per_session",
            "            )",
            "            event_running = (",
            "                body.event_id",
            "                in blocks._queue.pending_event_ids_session.get(body.session_hash, {})",
            "            )",
            "            if session_open and event_running:",
            "                message = ProcessCompletedMessage(",
            "                    output={}, success=True, event_id=body.event_id",
            "                )",
            "                blocks._queue.pending_messages_per_session[",
            "                    body.session_hash",
            "                ].put_nowait(message)",
            "            if body.event_id in app.iterators:",
            "                async with app.lock:",
            "                    del app.iterators[body.event_id]",
            "                    app.iterators_to_reset.add(body.event_id)",
            "            return {\"success\": True}",
            "",
            "        @app.get(\"/call/{api_name}/{event_id}\", dependencies=[Depends(login_check)])",
            "        async def simple_predict_get(",
            "            request: fastapi.Request,",
            "            event_id: str,",
            "        ):",
            "            def process_msg(message: EventMessage) -> str | None:",
            "                msg = message.model_dump()",
            "                if isinstance(message, ProcessCompletedMessage):",
            "                    event = \"complete\" if message.success else \"error\"",
            "                    data = msg[\"output\"].get(\"data\")",
            "                elif isinstance(message, ProcessGeneratingMessage):",
            "                    event = \"generating\" if message.success else \"error\"",
            "                    data = msg[\"output\"].get(\"data\")",
            "                elif isinstance(message, HeartbeatMessage):",
            "                    event = \"heartbeat\"",
            "                    data = None",
            "                elif isinstance(message, UnexpectedErrorMessage):",
            "                    event = \"error\"",
            "                    data = message.message",
            "                else:",
            "                    return None",
            "                return f\"event: {event}\\ndata: {json.dumps(data)}\\n\\n\"",
            "",
            "            return await queue_data_helper(request, event_id, process_msg)",
            "",
            "        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])",
            "        async def queue_data(",
            "            request: fastapi.Request,",
            "            session_hash: str,",
            "        ):",
            "            def process_msg(message: EventMessage) -> str:",
            "                return f\"data: {orjson.dumps(message.model_dump(), default=str).decode('utf-8')}\\n\\n\"",
            "",
            "            return await queue_data_helper(request, session_hash, process_msg)",
            "",
            "        async def queue_data_helper(",
            "            request: fastapi.Request,",
            "            session_hash: str,",
            "            process_msg: Callable[[EventMessage], str | None],",
            "        ):",
            "            blocks = app.get_blocks()",
            "",
            "            async def sse_stream(request: fastapi.Request):",
            "                try:",
            "                    last_heartbeat = time.perf_counter()",
            "                    while True:",
            "                        if await request.is_disconnected():",
            "                            await blocks._queue.clean_events(session_hash=session_hash)",
            "                            return",
            "",
            "                        if (",
            "                            session_hash",
            "                            not in blocks._queue.pending_messages_per_session",
            "                        ):",
            "                            raise HTTPException(",
            "                                status_code=status.HTTP_404_NOT_FOUND,",
            "                                detail=\"Session not found.\",",
            "                            )",
            "",
            "                        heartbeat_rate = 15",
            "                        check_rate = 0.05",
            "                        message = None",
            "                        try:",
            "                            messages = blocks._queue.pending_messages_per_session[",
            "                                session_hash",
            "                            ]",
            "                            message = messages.get_nowait()",
            "                        except EmptyQueue:",
            "                            await asyncio.sleep(check_rate)",
            "                            if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                                # Fix this",
            "                                message = HeartbeatMessage()",
            "                                # Need to reset last_heartbeat with perf_counter",
            "                                # otherwise only a single hearbeat msg will be sent",
            "                                # and then the stream will retry leading to infinite queue \ud83d\ude2c",
            "                                last_heartbeat = time.perf_counter()",
            "",
            "                        if blocks._queue.stopped:",
            "                            message = UnexpectedErrorMessage(",
            "                                message=\"Server stopped unexpectedly.\",",
            "                                success=False,",
            "                            )",
            "                        if message:",
            "                            response = process_msg(message)",
            "                            if response is not None:",
            "                                yield response",
            "                            if (",
            "                                isinstance(message, ProcessCompletedMessage)",
            "                                and message.event_id",
            "                            ):",
            "                                blocks._queue.pending_event_ids_session[",
            "                                    session_hash",
            "                                ].remove(message.event_id)",
            "                                if message.msg == ServerMessage.server_stopped or (",
            "                                    message.msg == ServerMessage.process_completed",
            "                                    and (",
            "                                        len(",
            "                                            blocks._queue.pending_event_ids_session[",
            "                                                session_hash",
            "                                            ]",
            "                                        )",
            "                                        == 0",
            "                                    )",
            "                                ):",
            "                                    message = CloseStreamMessage()",
            "                                    response = process_msg(message)",
            "                                    if response is not None:",
            "                                        yield response",
            "                                    return",
            "                except BaseException as e:",
            "                    message = UnexpectedErrorMessage(",
            "                        message=str(e),",
            "                    )",
            "                    response = process_msg(message)",
            "                    if isinstance(e, asyncio.CancelledError):",
            "                        del blocks._queue.pending_messages_per_session[session_hash]",
            "                        await blocks._queue.clean_events(session_hash=session_hash)",
            "                    if response is not None:",
            "                        yield response",
            "                    raise e",
            "",
            "            return StreamingResponse(",
            "                sse_stream(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        async def get_item_or_file(",
            "            request: fastapi.Request,",
            "        ) -> Union[ComponentServerJSONBody, ComponentServerBlobBody]:",
            "            content_type = request.headers.get(\"Content-Type\")",
            "",
            "            if isinstance(content_type, str) and content_type.startswith(",
            "                \"multipart/form-data\"",
            "            ):",
            "                files = []",
            "                data = {}",
            "                async with request.form() as form:",
            "                    for key, value in form.items():",
            "                        if (",
            "                            isinstance(value, list)",
            "                            and len(value) > 1",
            "                            and isinstance(value[0], StarletteUploadFile)",
            "                        ):",
            "                            for i, v in enumerate(value):",
            "                                if isinstance(v, StarletteUploadFile):",
            "                                    filename = v.filename",
            "                                    contents = await v.read()",
            "                                    files.append((filename, contents))",
            "                                else:",
            "                                    data[f\"{key}-{i}\"] = v",
            "                        elif isinstance(value, StarletteUploadFile):",
            "                            filename = value.filename",
            "                            contents = await value.read()",
            "                            files.append((filename, contents))",
            "                        else:",
            "                            data[key] = value",
            "",
            "                return ComponentServerBlobBody(",
            "                    data=DataWithFiles(data=data, files=files),",
            "                    component_id=data[\"component_id\"],",
            "                    session_hash=data[\"session_hash\"],",
            "                    fn_name=data[\"fn_name\"],",
            "                )",
            "            else:",
            "                try:",
            "                    data = await request.json()",
            "                    return ComponentServerJSONBody(",
            "                        data=data[\"data\"],",
            "                        component_id=data[\"component_id\"],",
            "                        session_hash=data[\"session_hash\"],",
            "                        fn_name=data[\"fn_name\"],",
            "                    )",
            "",
            "                except Exception:",
            "                    raise HTTPException(",
            "                        status_code=status.HTTP_400_BAD_REQUEST,",
            "                        detail=\"Invalid JSON body.\",",
            "                    ) from None",
            "",
            "        @app.post(",
            "            \"/component_server\",",
            "            dependencies=[Depends(login_check)],",
            "        )",
            "        @app.post(",
            "            \"/component_server/\",",
            "            dependencies=[Depends(login_check)],",
            "        )",
            "        async def component_server(",
            "            request: fastapi.Request,",
            "        ):",
            "            body = await get_item_or_file(request)",
            "            state = app.state_holder[body.session_hash]",
            "            component_id = body.component_id",
            "            block: Block",
            "            if component_id in state:",
            "                block = state[component_id]",
            "            else:",
            "                block = app.get_blocks().blocks[component_id]",
            "            fn = getattr(block, body.fn_name, None)",
            "            if fn is None or not getattr(fn, \"_is_server_fn\", False):",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_404_NOT_FOUND,",
            "                    detail=\"Function not found.\",",
            "                )",
            "            return fn(body.data)",
            "",
            "        @app.get(",
            "            \"/queue/status\",",
            "            dependencies=[Depends(login_check)],",
            "            response_model=EstimationMessage,",
            "        )",
            "        async def get_queue_status():",
            "            return app.get_blocks()._queue.get_status()",
            "",
            "        @app.get(\"/upload_progress\")",
            "        def get_upload_progress(upload_id: str, request: fastapi.Request):",
            "            async def sse_stream(request: fastapi.Request):",
            "                last_heartbeat = time.perf_counter()",
            "                is_done = False",
            "                while True:",
            "                    if await request.is_disconnected():",
            "                        file_upload_statuses.stop_tracking(upload_id)",
            "                        return",
            "                    if is_done:",
            "                        file_upload_statuses.stop_tracking(upload_id)",
            "                        return",
            "",
            "                    heartbeat_rate = 15",
            "                    check_rate = 0.05",
            "                    try:",
            "                        if file_upload_statuses.is_done(upload_id):",
            "                            message = {\"msg\": \"done\"}",
            "                            is_done = True",
            "                        else:",
            "                            update = file_upload_statuses.pop(upload_id)",
            "                            message = {",
            "                                \"msg\": \"update\",",
            "                                \"orig_name\": update.filename,",
            "                                \"chunk_size\": update.chunk_size,",
            "                            }",
            "                        yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                    except FileUploadProgressNotTrackedError:",
            "                        return",
            "                    except FileUploadProgressNotQueuedError:",
            "                        await asyncio.sleep(check_rate)",
            "                        if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                            message = {\"msg\": \"heartbeat\"}",
            "                            yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                            last_heartbeat = time.perf_counter()",
            "",
            "            return StreamingResponse(",
            "                sse_stream(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/upload\", dependencies=[Depends(login_check)])",
            "        async def upload_file(",
            "            request: fastapi.Request,",
            "            bg_tasks: BackgroundTasks,",
            "            upload_id: Optional[str] = None,",
            "        ):",
            "            content_type_header = request.headers.get(\"Content-Type\")",
            "            content_type: bytes",
            "            content_type, _ = parse_options_header(content_type_header or \"\")",
            "            if content_type != b\"multipart/form-data\":",
            "                raise HTTPException(status_code=400, detail=\"Invalid content type.\")",
            "",
            "            try:",
            "                if upload_id:",
            "                    file_upload_statuses.track(upload_id)",
            "                max_file_size = app.get_blocks().max_file_size",
            "                max_file_size = max_file_size if max_file_size is not None else math.inf",
            "                multipart_parser = GradioMultiPartParser(",
            "                    request.headers,",
            "                    request.stream(),",
            "                    max_files=1000,",
            "                    max_fields=1000,",
            "                    max_file_size=max_file_size,",
            "                    upload_id=upload_id if upload_id else None,",
            "                    upload_progress=file_upload_statuses if upload_id else None,",
            "                )",
            "                form = await multipart_parser.parse()",
            "            except MultiPartException as exc:",
            "                code = 413 if \"maximum allowed size\" in exc.message else 400",
            "                return PlainTextResponse(exc.message, status_code=code)",
            "",
            "            output_files = []",
            "            files_to_copy = []",
            "            locations: list[str] = []",
            "",
            "            for temp_file in form.getlist(\"files\"):",
            "                if not isinstance(temp_file, GradioUploadFile):",
            "                    raise TypeError(\"File is not an instance of GradioUploadFile\")",
            "                if temp_file.filename:",
            "                    file_name = Path(temp_file.filename).name",
            "                    name = client_utils.strip_invalid_filename_characters(file_name)",
            "                else:",
            "                    name = f\"tmp{secrets.token_hex(5)}\"",
            "                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()",
            "                directory.mkdir(exist_ok=True, parents=True)",
            "                try:",
            "                    dest = utils.safe_join(",
            "                        DeveloperPath(str(directory)), UserProvidedPath(name)",
            "                    )",
            "                except InvalidPathError as err:",
            "                    raise HTTPException(",
            "                        status_code=400, detail=f\"Invalid file name: {name}\"",
            "                    ) from err",
            "                temp_file.file.close()",
            "                # we need to move the temp file to the cache directory",
            "                # but that's possibly blocking and we're in an async function",
            "                # so we try to rename (this is what shutil.move tries first)",
            "                # which should be super fast.",
            "                # if that fails, we move in the background.",
            "                try:",
            "                    os.rename(temp_file.file.name, dest)",
            "                except OSError:",
            "                    files_to_copy.append(temp_file.file.name)",
            "                    locations.append(dest)",
            "                output_files.append(dest)",
            "                blocks.upload_file_set.add(dest)",
            "            if files_to_copy:",
            "                bg_tasks.add_task(",
            "                    move_uploaded_files_to_cache, files_to_copy, locations",
            "                )",
            "            return output_files",
            "",
            "        @app.on_event(\"startup\")",
            "        @app.get(\"/startup-events\")",
            "        async def startup_events():",
            "            if not app.startup_events_triggered:",
            "                app.get_blocks().startup_events()",
            "                app.startup_events_triggered = True",
            "                return True",
            "            return False",
            "",
            "        @app.get(\"/theme.css\", response_class=PlainTextResponse)",
            "        def theme_css():",
            "            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")",
            "",
            "        @app.get(\"/robots.txt\", response_class=PlainTextResponse)",
            "        def robots_txt():",
            "            if app.get_blocks().share:",
            "                return \"User-agent: *\\nDisallow: /\"",
            "            else:",
            "                return \"User-agent: *\\nDisallow: \"",
            "",
            "        @app.get(\"/monitoring\", dependencies=[Depends(login_check)])",
            "        async def analytics_login(request: fastapi.Request):",
            "            if not blocks.enable_monitoring:",
            "                raise HTTPException(",
            "                    status_code=403, detail=\"Monitoring is not enabled.\"",
            "                )",
            "            root_url = route_utils.get_root_url(",
            "                request=request, route_path=\"/monitoring\", root_path=app.root_path",
            "            )",
            "            monitoring_url = f\"{root_url}/monitoring/{app.analytics_key}\"",
            "            print(f\"* Monitoring URL: {monitoring_url} *\")",
            "            return HTMLResponse(\"See console for monitoring URL.\")",
            "",
            "        @app.get(\"/monitoring/{key}\")",
            "        async def analytics_dashboard(key: str):",
            "            if not blocks.enable_monitoring:",
            "                raise HTTPException(",
            "                    status_code=403, detail=\"Monitoring is not enabled.\"",
            "                )",
            "            if compare_passwords_securely(key, app.analytics_key):",
            "                analytics_url = f\"/monitoring/{app.analytics_key}/dashboard\"",
            "                if not app.monitoring_enabled:",
            "                    from gradio.monitoring_dashboard import data",
            "                    from gradio.monitoring_dashboard import demo as dashboard",
            "",
            "                    mount_gradio_app(app, dashboard, path=analytics_url)",
            "                    dashboard._queue.start()",
            "                    analytics = app.get_blocks()._queue.event_analytics",
            "                    data[\"data\"] = analytics",
            "                    app.monitoring_enabled = True",
            "                return RedirectResponse(",
            "                    url=analytics_url, status_code=status.HTTP_302_FOUND",
            "                )",
            "            else:",
            "                raise HTTPException(status_code=403, detail=\"Invalid key.\")",
            "",
            "        return app",
            "",
            "",
            "########",
            "# Helper functions",
            "########",
            "",
            "",
            "def routes_safe_join(directory: DeveloperPath, path: UserProvidedPath) -> str:",
            "    \"\"\"Safely join the user path to the directory while performing some additional http-related checks,",
            "    e.g. ensuring that the full path exists on the local file system and is not a directory\"\"\"",
            "    if path == \"\":",
            "        raise fastapi.HTTPException(400)",
            "    if route_utils.starts_with_protocol(path):",
            "        raise fastapi.HTTPException(403)",
            "    try:",
            "        fullpath = Path(utils.safe_join(directory, path))",
            "    except InvalidPathError as e:",
            "        raise fastapi.HTTPException(403) from e",
            "    if fullpath.is_dir():",
            "        raise fastapi.HTTPException(403)",
            "    if not fullpath.exists():",
            "        raise fastapi.HTTPException(404)",
            "    return str(fullpath)",
            "",
            "",
            "def get_types(cls_set: List[Type]):",
            "    docset = []",
            "    types = []",
            "    for cls in cls_set:",
            "        doc = inspect.getdoc(cls) or \"\"",
            "        doc_lines = doc.split(\"\\n\")",
            "        for line in doc_lines:",
            "            if \"value (\" in line:",
            "                types.append(line.split(\"value (\")[1].split(\")\")[0])",
            "        docset.append(doc_lines[1].split(\":\")[-1])",
            "    return docset, types",
            "",
            "",
            "@document()",
            "def mount_gradio_app(",
            "    app: fastapi.FastAPI,",
            "    blocks: gradio.Blocks,",
            "    path: str,",
            "    app_kwargs: dict[str, Any] | None = None,",
            "    *,",
            "    auth: Callable | tuple[str, str] | list[tuple[str, str]] | None = None,",
            "    auth_message: str | None = None,",
            "    auth_dependency: Callable[[fastapi.Request], str | None] | None = None,",
            "    root_path: str | None = None,",
            "    allowed_paths: list[str] | None = None,",
            "    blocked_paths: list[str] | None = None,",
            "    favicon_path: str | None = None,",
            "    show_error: bool = True,",
            "    max_file_size: str | int | None = None,",
            ") -> fastapi.FastAPI:",
            "    \"\"\"Mount a gradio.Blocks to an existing FastAPI application.",
            "",
            "    Parameters:",
            "        app: The parent FastAPI application.",
            "        blocks: The blocks object we want to mount to the parent app.",
            "        path: The path at which the gradio application will be mounted.",
            "        app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`",
            "        auth: If provided, username and password (or list of username-password tuples) required to access the gradio app. Can also provide function that takes username and password and returns True if valid login.",
            "        auth_message: If provided, HTML message provided on login page for this gradio app.",
            "        auth_dependency: A function that takes a FastAPI request and returns a string user ID or None. If the function returns None for a specific request, that user is not authorized to access the gradio app (they will see a 401 Unauthorized response). To be used with external authentication systems like OAuth. Cannot be used with `auth`.",
            "        root_path: The subpath corresponding to the public deployment of this FastAPI application. For example, if the application is served at \"https://example.com/myapp\", the `root_path` should be set to \"/myapp\". A full URL beginning with http:// or https:// can be provided, which will be used in its entirety. Normally, this does not need to provided (even if you are using a custom `path`). However, if you are serving the FastAPI app behind a proxy, the proxy may not provide the full path to the Gradio app in the request headers. In which case, you can provide the root path here.",
            "        allowed_paths: List of complete filepaths or parent directories that this gradio app is allowed to serve. Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",
            "        blocked_paths: List of complete filepaths or parent directories that this gradio app is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",
            "        favicon_path: If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for this gradio app's page.",
            "        show_error: If True, any errors in the gradio app will be displayed in an alert modal and printed in the browser console log. Otherwise, errors will only be visible in the terminal session running the Gradio app.",
            "        max_file_size: The maximum file size in bytes that can be uploaded. Can be a string of the form \"<value><unit>\", where value is any positive integer and unit is one of \"b\", \"kb\", \"mb\", \"gb\", \"tb\". If None, no limit is set.",
            "    Example:",
            "        from fastapi import FastAPI",
            "        import gradio as gr",
            "        app = FastAPI()",
            "        @app.get(\"/\")",
            "        def read_main():",
            "            return {\"message\": \"This is your main app\"}",
            "        io = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")",
            "        app = gr.mount_gradio_app(app, io, path=\"/gradio\")",
            "        # Then run `uvicorn run:app` from the terminal and navigate to http://localhost:8000/gradio.",
            "    \"\"\"",
            "    if favicon_path is not None and path != \"/\":",
            "        warnings.warn(",
            "            \"The 'favicon_path' parameter is set but will be ignored because 'path' is not '/'. \"",
            "            \"Please add the favicon directly to your FastAPI app.\"",
            "        )",
            "",
            "    blocks.dev_mode = False",
            "    blocks.max_file_size = utils._parse_file_size(max_file_size)",
            "    blocks.config = blocks.get_config_file()",
            "    blocks.validate_queue_settings()",
            "    if auth is not None and auth_dependency is not None:",
            "        raise ValueError(",
            "            \"You cannot provide both `auth` and `auth_dependency` in mount_gradio_app(). Please choose one.\"",
            "        )",
            "    if (",
            "        auth",
            "        and not callable(auth)",
            "        and not isinstance(auth[0], tuple)",
            "        and not isinstance(auth[0], list)",
            "    ):",
            "        blocks.auth = [auth]",
            "    else:",
            "        blocks.auth = auth",
            "    blocks.auth_message = auth_message",
            "    blocks.favicon_path = favicon_path",
            "    blocks.allowed_paths = allowed_paths or []",
            "    blocks.blocked_paths = blocked_paths or []",
            "    blocks.show_error = show_error",
            "",
            "    if not isinstance(blocks.allowed_paths, list):",
            "        raise ValueError(\"`allowed_paths` must be a list of directories.\")",
            "    if not isinstance(blocks.blocked_paths, list):",
            "        raise ValueError(\"`blocked_paths` must be a list of directories.\")",
            "",
            "    if root_path is not None:",
            "        blocks.root_path = root_path",
            "",
            "    gradio_app = App.create_app(",
            "        blocks, app_kwargs=app_kwargs, auth_dependency=auth_dependency",
            "    )",
            "    old_lifespan = app.router.lifespan_context",
            "",
            "    @contextlib.asynccontextmanager",
            "    async def new_lifespan(app: FastAPI):",
            "        async with old_lifespan(",
            "            app",
            "        ):  # Instert the startup events inside the FastAPI context manager",
            "            async with gradio_app.router.lifespan_context(gradio_app):",
            "                gradio_app.get_blocks().startup_events()",
            "                yield",
            "",
            "    app.router.lifespan_context = new_lifespan",
            "",
            "    app.mount(path, gradio_app)",
            "    return app"
        ],
        "afterPatchFile": [
            "\"\"\"Implements a FastAPI server to run the gradio interface. Note that some types in this",
            "module use the Optional/Union notation so that they work correctly with pydantic.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import contextlib",
            "import math",
            "import sys",
            "import warnings",
            "",
            "if sys.version_info >= (3, 9):",
            "    from importlib.resources import files",
            "else:",
            "    from importlib_resources import files",
            "import hashlib",
            "import inspect",
            "import json",
            "import mimetypes",
            "import os",
            "import secrets",
            "import time",
            "import traceback",
            "from pathlib import Path",
            "from queue import Empty as EmptyQueue",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    AsyncIterator,",
            "    Callable,",
            "    Dict,",
            "    List,",
            "    Literal,",
            "    Optional,",
            "    Type,",
            "    Union,",
            "    cast,",
            ")",
            "",
            "import fastapi",
            "import httpx",
            "import markupsafe",
            "import orjson",
            "from fastapi import (",
            "    BackgroundTasks,",
            "    Depends,",
            "    FastAPI,",
            "    HTTPException,",
            "    status,",
            ")",
            "from fastapi.responses import (",
            "    FileResponse,",
            "    HTMLResponse,",
            "    JSONResponse,",
            "    PlainTextResponse,",
            ")",
            "from fastapi.security import OAuth2PasswordRequestForm",
            "from fastapi.templating import Jinja2Templates",
            "from gradio_client import utils as client_utils",
            "from gradio_client.documentation import document",
            "from gradio_client.utils import ServerMessage",
            "from jinja2.exceptions import TemplateNotFound",
            "from multipart.multipart import parse_options_header",
            "from starlette.background import BackgroundTask",
            "from starlette.datastructures import UploadFile as StarletteUploadFile",
            "from starlette.responses import RedirectResponse, StreamingResponse",
            "",
            "import gradio",
            "from gradio import ranged_response, route_utils, utils, wasm_utils",
            "from gradio.context import Context",
            "from gradio.data_classes import (",
            "    CancelBody,",
            "    ComponentServerBlobBody,",
            "    ComponentServerJSONBody,",
            "    DataWithFiles,",
            "    DeveloperPath,",
            "    PredictBody,",
            "    PredictBodyInternal,",
            "    ResetBody,",
            "    SimplePredictBody,",
            "    UserProvidedPath,",
            ")",
            "from gradio.exceptions import InvalidPathError",
            "from gradio.oauth import attach_oauth",
            "from gradio.route_utils import (  # noqa: F401",
            "    CustomCORSMiddleware,",
            "    FileUploadProgress,",
            "    FileUploadProgressNotQueuedError,",
            "    FileUploadProgressNotTrackedError,",
            "    GradioMultiPartParser,",
            "    GradioUploadFile,",
            "    MultiPartException,",
            "    Request,",
            "    compare_passwords_securely,",
            "    create_lifespan_handler,",
            "    move_uploaded_files_to_cache,",
            ")",
            "from gradio.server_messages import (",
            "    CloseStreamMessage,",
            "    EstimationMessage,",
            "    EventMessage,",
            "    HeartbeatMessage,",
            "    ProcessCompletedMessage,",
            "    ProcessGeneratingMessage,",
            "    UnexpectedErrorMessage,",
            ")",
            "from gradio.state_holder import StateHolder",
            "from gradio.utils import cancel_tasks, get_package_version, get_upload_folder",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import Block",
            "",
            "",
            "mimetypes.init()",
            "",
            "STATIC_TEMPLATE_LIB = cast(",
            "    DeveloperPath,",
            "    files(\"gradio\").joinpath(\"templates\").as_posix(),  # type: ignore",
            ")",
            "STATIC_PATH_LIB = cast(",
            "    DeveloperPath,",
            "    files(\"gradio\").joinpath(\"templates\", \"frontend\", \"static\").as_posix(),  # type: ignore",
            ")",
            "BUILD_PATH_LIB = cast(",
            "    DeveloperPath,",
            "    files(\"gradio\").joinpath(\"templates\", \"frontend\", \"assets\").as_posix(),  # type: ignore",
            ")",
            "VERSION = get_package_version()",
            "XSS_VULNERABLE_EXTENSIONS = [",
            "    \".html\",",
            "    \".htm\",",
            "    \".js\",",
            "    \".php\",",
            "    \".asp\",",
            "    \".aspx\",",
            "    \".jsp\",",
            "    \".xml\",",
            "    \".svg\",",
            "]",
            "",
            "",
            "class ORJSONResponse(JSONResponse):",
            "    media_type = \"application/json\"",
            "",
            "    @staticmethod",
            "    def _render(content: Any) -> bytes:",
            "        return orjson.dumps(",
            "            content,",
            "            option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_PASSTHROUGH_DATETIME,",
            "            default=str,",
            "        )",
            "",
            "    def render(self, content: Any) -> bytes:",
            "        return ORJSONResponse._render(content)",
            "",
            "    @staticmethod",
            "    def _render_str(content: Any) -> str:",
            "        return ORJSONResponse._render(content).decode(\"utf-8\")",
            "",
            "",
            "def toorjson(value):",
            "    return markupsafe.Markup(",
            "        ORJSONResponse._render_str(value)",
            "        .replace(\"<\", \"\\\\u003c\")",
            "        .replace(\">\", \"\\\\u003e\")",
            "        .replace(\"&\", \"\\\\u0026\")",
            "        .replace(\"'\", \"\\\\u0027\")",
            "    )",
            "",
            "",
            "templates = Jinja2Templates(directory=STATIC_TEMPLATE_LIB)",
            "templates.env.filters[\"toorjson\"] = toorjson",
            "",
            "client = httpx.AsyncClient()",
            "",
            "file_upload_statuses = FileUploadProgress()",
            "",
            "",
            "class App(FastAPI):",
            "    \"\"\"",
            "    FastAPI App Wrapper",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        auth_dependency: Callable[[fastapi.Request], str | None] | None = None,",
            "        **kwargs,",
            "    ):",
            "        self.tokens = {}",
            "        self.auth = None",
            "        self.analytics_key = secrets.token_urlsafe(16)",
            "        self.monitoring_enabled = False",
            "        self.blocks: gradio.Blocks | None = None",
            "        self.state_holder = StateHolder()",
            "        self.iterators: dict[str, AsyncIterator] = {}",
            "        self.iterators_to_reset: set[str] = set()",
            "        self.lock = utils.safe_get_lock()",
            "        self.stop_event = utils.safe_get_stop_event()",
            "        self.cookie_id = secrets.token_urlsafe(32)",
            "        self.queue_token = secrets.token_urlsafe(32)",
            "        self.startup_events_triggered = False",
            "        self.uploaded_file_dir = get_upload_folder()",
            "        self.change_count: int = 0",
            "        self.change_type: Literal[\"reload\", \"error\"] | None = None",
            "        self.reload_error_message: str | None = None",
            "        self._asyncio_tasks: list[asyncio.Task] = []",
            "        self.auth_dependency = auth_dependency",
            "        self.api_info = None",
            "        self.all_app_info = None",
            "        # Allow user to manually set `docs_url` and `redoc_url`",
            "        # when instantiating an App; when they're not set, disable docs and redoc.",
            "        kwargs.setdefault(\"docs_url\", None)",
            "        kwargs.setdefault(\"redoc_url\", None)",
            "        self.custom_component_hashes: dict[str, str] = {}",
            "        super().__init__(**kwargs)",
            "",
            "    def configure_app(self, blocks: gradio.Blocks) -> None:",
            "        auth = blocks.auth",
            "        if auth is not None:",
            "            if not callable(auth):",
            "                self.auth = {account[0]: account[1] for account in auth}",
            "            else:",
            "                self.auth = auth",
            "        else:",
            "            self.auth = None",
            "",
            "        self.blocks = blocks",
            "        self.cwd = os.getcwd()",
            "        self.favicon_path = blocks.favicon_path",
            "        self.tokens = {}",
            "        self.root_path = blocks.root_path",
            "        self.state_holder.set_blocks(blocks)",
            "",
            "    def get_blocks(self) -> gradio.Blocks:",
            "        if self.blocks is None:",
            "            raise ValueError(\"No Blocks has been configured for this app.\")",
            "        return self.blocks",
            "",
            "    def build_proxy_request(self, url_path):",
            "        url = httpx.URL(url_path)",
            "        assert self.blocks  # noqa: S101",
            "        # Don't proxy a URL unless it's a URL specifically loaded by the user using",
            "        # gr.load() to prevent SSRF or harvesting of HF tokens by malicious Spaces.",
            "        is_safe_url = any(",
            "            url.host == httpx.URL(root).host for root in self.blocks.proxy_urls",
            "        )",
            "        if not is_safe_url:",
            "            raise PermissionError(\"This URL cannot be proxied.\")",
            "        is_hf_url = url.host.endswith(\".hf.space\")",
            "        headers = {}",
            "        if Context.hf_token is not None and is_hf_url:",
            "            headers[\"Authorization\"] = f\"Bearer {Context.hf_token}\"",
            "        rp_req = client.build_request(\"GET\", url, headers=headers)",
            "        return rp_req",
            "",
            "    def _cancel_asyncio_tasks(self):",
            "        for task in self._asyncio_tasks:",
            "            task.cancel()",
            "        self._asyncio_tasks = []",
            "",
            "    @staticmethod",
            "    def create_app(",
            "        blocks: gradio.Blocks,",
            "        app_kwargs: Dict[str, Any] | None = None,",
            "        auth_dependency: Callable[[fastapi.Request], str | None] | None = None,",
            "    ) -> App:",
            "        app_kwargs = app_kwargs or {}",
            "        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)",
            "        delete_cache = blocks.delete_cache or (None, None)",
            "        app_kwargs[\"lifespan\"] = create_lifespan_handler(",
            "            app_kwargs.get(\"lifespan\", None), *delete_cache",
            "        )",
            "        app = App(auth_dependency=auth_dependency, **app_kwargs)",
            "        app.configure_app(blocks)",
            "",
            "        if not wasm_utils.IS_WASM:",
            "            app.add_middleware(CustomCORSMiddleware)",
            "",
            "        @app.get(\"/user\")",
            "        @app.get(\"/user/\")",
            "        def get_current_user(request: fastapi.Request) -> Optional[str]:",
            "            if app.auth_dependency is not None:",
            "                return app.auth_dependency(request)",
            "            token = request.cookies.get(",
            "                f\"access-token-{app.cookie_id}\"",
            "            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")",
            "            return app.tokens.get(token)",
            "",
            "        @app.get(\"/login_check\")",
            "        @app.get(\"/login_check/\")",
            "        def login_check(user: str = Depends(get_current_user)):",
            "            if (app.auth is None and app.auth_dependency is None) or user is not None:",
            "                return",
            "            raise HTTPException(",
            "                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"",
            "            )",
            "",
            "        @app.get(\"/token\")",
            "        @app.get(\"/token/\")",
            "        def get_token(request: fastapi.Request) -> dict:",
            "            token = request.cookies.get(f\"access-token-{app.cookie_id}\")",
            "            return {\"token\": token, \"user\": app.tokens.get(token)}",
            "",
            "        @app.get(\"/app_id\")",
            "        @app.get(\"/app_id/\")",
            "        def app_id(request: fastapi.Request) -> dict:  # noqa: ARG001",
            "            return {\"app_id\": app.get_blocks().app_id}",
            "",
            "        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])",
            "        async def notify_changes(",
            "            request: fastapi.Request,",
            "        ):",
            "            async def reload_checker(request: fastapi.Request):",
            "                heartbeat_rate = 15",
            "                check_rate = 0.05",
            "                last_heartbeat = time.perf_counter()",
            "                current_count = app.change_count",
            "",
            "                while True:",
            "                    if await request.is_disconnected():",
            "                        return",
            "",
            "                    if app.change_count != current_count:",
            "                        current_count = app.change_count",
            "                        msg = (",
            "                            json.dumps(f\"{app.reload_error_message}\")",
            "                            if app.change_type == \"error\"",
            "                            else \"{}\"",
            "                        )",
            "                        yield f\"\"\"event: {app.change_type}\\ndata: {msg}\\n\\n\"\"\"",
            "",
            "                    await asyncio.sleep(check_rate)",
            "                    if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                        yield \"\"\"event: heartbeat\\ndata: {}\\n\\n\"\"\"",
            "                        last_heartbeat = time.time()",
            "",
            "            return StreamingResponse(",
            "                reload_checker(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/login\")",
            "        @app.post(\"/login/\")",
            "        def login(form_data: OAuth2PasswordRequestForm = Depends()):",
            "            username, password = form_data.username.strip(), form_data.password",
            "            if app.auth is None:",
            "                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)",
            "            if (",
            "                not callable(app.auth)",
            "                and username in app.auth",
            "                and compare_passwords_securely(password, app.auth[username])  # type: ignore",
            "            ) or (callable(app.auth) and app.auth.__call__(username, password)):  # type: ignore",
            "                token = secrets.token_urlsafe(16)",
            "                app.tokens[token] = username",
            "                response = JSONResponse(content={\"success\": True})",
            "                response.set_cookie(",
            "                    key=f\"access-token-{app.cookie_id}\",",
            "                    value=token,",
            "                    httponly=True,",
            "                    samesite=\"none\",",
            "                    secure=True,",
            "                )",
            "                response.set_cookie(",
            "                    key=f\"access-token-unsecure-{app.cookie_id}\",",
            "                    value=token,",
            "                    httponly=True,",
            "                )",
            "                return response",
            "            else:",
            "                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")",
            "",
            "        ###############",
            "        # OAuth Routes",
            "        ###############",
            "",
            "        # Define OAuth routes if the app expects it (i.e. a LoginButton is defined).",
            "        # It allows users to \"Sign in with HuggingFace\". Otherwise, add the default",
            "        # logout route.",
            "        if app.blocks is not None and app.blocks.expects_oauth:",
            "            attach_oauth(app)",
            "        else:",
            "",
            "            @app.get(\"/logout\")",
            "            def logout(user: str = Depends(get_current_user)):",
            "                response = RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)",
            "                response.delete_cookie(key=f\"access-token-{app.cookie_id}\", path=\"/\")",
            "                response.delete_cookie(",
            "                    key=f\"access-token-unsecure-{app.cookie_id}\", path=\"/\"",
            "                )",
            "                # A user may have multiple tokens, so we need to delete all of them.",
            "                for token in list(app.tokens.keys()):",
            "                    if app.tokens[token] == user:",
            "                        del app.tokens[token]",
            "                return response",
            "",
            "        ###############",
            "        # Main Routes",
            "        ###############",
            "",
            "        @app.head(\"/\", response_class=HTMLResponse)",
            "        @app.get(\"/\", response_class=HTMLResponse)",
            "        def main(request: fastapi.Request, user: str = Depends(get_current_user)):",
            "            mimetypes.add_type(\"application/javascript\", \".js\")",
            "            blocks = app.get_blocks()",
            "            root = route_utils.get_root_url(",
            "                request=request, route_path=\"/\", root_path=app.root_path",
            "            )",
            "            if (app.auth is None and app.auth_dependency is None) or user is not None:",
            "                config = blocks.config",
            "                config = route_utils.update_root_in_config(config, root)",
            "                config[\"username\"] = user",
            "            elif app.auth_dependency:",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"",
            "                )",
            "            else:",
            "                config = {",
            "                    \"auth_required\": True,",
            "                    \"auth_message\": blocks.auth_message,",
            "                    \"space_id\": blocks.space_id,",
            "                    \"root\": root,",
            "                }",
            "",
            "            try:",
            "                template = (",
            "                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"",
            "                )",
            "                gradio_api_info = api_info(False)",
            "                return templates.TemplateResponse(",
            "                    template,",
            "                    {",
            "                        \"request\": request,",
            "                        \"config\": config,",
            "                        \"gradio_api_info\": gradio_api_info,",
            "                    },",
            "                )",
            "            except TemplateNotFound as err:",
            "                if blocks.share:",
            "                    raise ValueError(",
            "                        \"Did you install Gradio from source files? Share mode only \"",
            "                        \"works when Gradio is installed through the pip package.\"",
            "                    ) from err",
            "                else:",
            "                    raise ValueError(",
            "                        \"Did you install Gradio from source files? You need to build \"",
            "                        \"the frontend by running /scripts/build_frontend.sh\"",
            "                    ) from err",
            "",
            "        @app.get(\"/info/\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/info\", dependencies=[Depends(login_check)])",
            "        def api_info(all_endpoints: bool = False):",
            "            if all_endpoints:",
            "                if not app.all_app_info:",
            "                    app.all_app_info = app.get_blocks().get_api_info(all_endpoints=True)",
            "                return app.all_app_info",
            "            if not app.api_info:",
            "                app.api_info = app.get_blocks().get_api_info()",
            "            return app.api_info",
            "",
            "        @app.get(\"/config/\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/config\", dependencies=[Depends(login_check)])",
            "        def get_config(request: fastapi.Request):",
            "            config = app.get_blocks().config",
            "            root = route_utils.get_root_url(",
            "                request=request, route_path=\"/config\", root_path=app.root_path",
            "            )",
            "            config = route_utils.update_root_in_config(config, root)",
            "            config[\"username\"] = get_current_user(request)",
            "            return ORJSONResponse(content=config)",
            "",
            "        @app.get(\"/static/{path:path}\")",
            "        def static_resource(path: str):",
            "            static_file = routes_safe_join(STATIC_PATH_LIB, UserProvidedPath(path))",
            "            return FileResponse(static_file)",
            "",
            "        @app.get(\"/custom_component/{id}/{type}/{file_name}\")",
            "        def custom_component_path(",
            "            id: str, type: str, file_name: str, req: fastapi.Request",
            "        ):",
            "            config = app.get_blocks().config",
            "            components = config[\"components\"]",
            "            location = next(",
            "                (item for item in components if item[\"component_class_id\"] == id), None",
            "            )",
            "            if location is None:",
            "                raise HTTPException(status_code=404, detail=\"Component not found.\")",
            "",
            "            component_instance = app.get_blocks().get_component(location[\"id\"])",
            "",
            "            module_name = component_instance.__class__.__module__",
            "            module_path = sys.modules[module_name].__file__",
            "",
            "            if module_path is None or component_instance is None:",
            "                raise HTTPException(status_code=404, detail=\"Component not found.\")",
            "",
            "            requested_path = utils.safe_join(",
            "                component_instance.__class__.TEMPLATE_DIR,",
            "                UserProvidedPath(f\"{type}/{file_name}\"),",
            "            )",
            "",
            "            path = routes_safe_join(",
            "                DeveloperPath(str(Path(module_path).parent)),",
            "                UserProvidedPath(requested_path),",
            "            )",
            "",
            "            key = f\"{id}-{type}-{file_name}\"",
            "",
            "            if key not in app.custom_component_hashes:",
            "                app.custom_component_hashes[key] = hashlib.md5(",
            "                    Path(path).read_text(encoding=\"utf-8\").encode()",
            "                ).hexdigest()",
            "",
            "            version = app.custom_component_hashes.get(key)",
            "            headers = {\"Cache-Control\": \"max-age=0, must-revalidate\"}",
            "            if version:",
            "                headers[\"ETag\"] = version",
            "",
            "            if version and req.headers.get(\"if-none-match\") == version:",
            "                return PlainTextResponse(status_code=304, headers=headers)",
            "",
            "            return FileResponse(path, headers=headers)",
            "",
            "        @app.get(\"/assets/{path:path}\")",
            "        def build_resource(path: str):",
            "            build_file = routes_safe_join(BUILD_PATH_LIB, UserProvidedPath(path))",
            "            return FileResponse(build_file)",
            "",
            "        @app.get(\"/favicon.ico\")",
            "        async def favicon():",
            "            blocks = app.get_blocks()",
            "            if blocks.favicon_path is None:",
            "                return static_resource(\"img/logo.svg\")",
            "            else:",
            "                return FileResponse(blocks.favicon_path)",
            "",
            "        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])",
            "        async def reverse_proxy(url_path: str):",
            "            # Adapted from: https://github.com/tiangolo/fastapi/issues/1788",
            "            try:",
            "                rp_req = app.build_proxy_request(url_path)",
            "            except PermissionError as err:",
            "                raise HTTPException(status_code=400, detail=str(err)) from err",
            "            rp_resp = await client.send(rp_req, stream=True)",
            "            file_extension = os.path.splitext(url_path)[1].lower()",
            "            if file_extension in XSS_VULNERABLE_EXTENSIONS:",
            "                rp_resp.headers.update({\"Content-Disposition\": \"attachment\"})",
            "                rp_resp.headers.update({\"Content-Type\": \"application/octet-stream\"})",
            "            return StreamingResponse(",
            "                rp_resp.aiter_raw(),",
            "                status_code=rp_resp.status_code,",
            "                headers=rp_resp.headers,  # type: ignore",
            "                background=BackgroundTask(rp_resp.aclose),",
            "            )",
            "",
            "        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])",
            "        async def file(path_or_url: str, request: fastapi.Request):",
            "            blocks = app.get_blocks()",
            "            if client_utils.is_http_url_like(path_or_url):",
            "                return RedirectResponse(",
            "                    url=path_or_url, status_code=status.HTTP_302_FOUND",
            "                )",
            "",
            "            if route_utils.starts_with_protocol(path_or_url):",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            abs_path = utils.abspath(path_or_url)",
            "",
            "            in_blocklist = any(",
            "                utils.is_in_or_equal(abs_path, blocked_path)",
            "                for blocked_path in blocks.blocked_paths",
            "            )",
            "",
            "            is_dir = abs_path.is_dir()",
            "",
            "            if is_dir or in_blocklist:",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            created_by_app = False",
            "            for temp_file_set in blocks.temp_file_sets:",
            "                if abs_path in temp_file_set:",
            "                    created_by_app = True",
            "                    break",
            "            in_allowlist = any(",
            "                utils.is_in_or_equal(abs_path, allowed_path)",
            "                for allowed_path in blocks.allowed_paths",
            "            )",
            "            is_static_file = utils.is_static_file(abs_path)",
            "            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)",
            "            is_cached_example = utils.is_in_or_equal(",
            "                abs_path, utils.abspath(utils.get_cache_folder())",
            "            )",
            "",
            "            if not (",
            "                created_by_app",
            "                or in_allowlist",
            "                or was_uploaded",
            "                or is_cached_example",
            "                or is_static_file",
            "            ):",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            if not abs_path.exists():",
            "                raise HTTPException(404, f\"File not found: {path_or_url}.\")",
            "",
            "            mime_type, _ = mimetypes.guess_type(abs_path)",
            "            file_extension = os.path.splitext(abs_path)[1].lower()",
            "",
            "            if file_extension in XSS_VULNERABLE_EXTENSIONS:",
            "                media_type = \"application/octet-stream\"",
            "                content_disposition_type = \"attachment\"",
            "            else:",
            "                media_type = mime_type or \"application/octet-stream\"",
            "                content_disposition_type = \"inline\"",
            "",
            "            range_val = request.headers.get(\"Range\", \"\").strip()",
            "            if range_val.startswith(\"bytes=\") and \"-\" in range_val:",
            "                range_val = range_val[6:]",
            "                start, end = range_val.split(\"-\")",
            "                if start.isnumeric() and end.isnumeric():",
            "                    start = int(start)",
            "                    end = int(end)",
            "                    headers = dict(request.headers)",
            "                    headers[\"Content-Disposition\"] = content_disposition_type",
            "                    headers[\"Content-Type\"] = media_type",
            "                    response = ranged_response.RangedFileResponse(",
            "                        abs_path,",
            "                        ranged_response.OpenRange(start, end),",
            "                        headers,",
            "                        stat_result=os.stat(abs_path),",
            "                    )",
            "                    return response",
            "",
            "            return FileResponse(",
            "                abs_path,",
            "                headers={\"Accept-Ranges\": \"bytes\"},",
            "                content_disposition_type=content_disposition_type,",
            "                media_type=media_type,",
            "                filename=abs_path.name,",
            "            )",
            "",
            "        @app.get(",
            "            \"/stream/{session_hash}/{run}/{component_id}\",",
            "            dependencies=[Depends(login_check)],",
            "        )",
            "        async def stream(",
            "            session_hash: str,",
            "            run: int,",
            "            component_id: int,",
            "            request: fastapi.Request,  # noqa: ARG001",
            "        ):",
            "            stream: list = (",
            "                app.get_blocks()",
            "                .pending_streams[session_hash]",
            "                .get(run, {})",
            "                .get(component_id, None)",
            "            )",
            "            if stream is None:",
            "                raise HTTPException(404, \"Stream not found.\")",
            "",
            "            def stream_wrapper():",
            "                check_stream_rate = 0.01",
            "                max_wait_time = 120  # maximum wait between yields - assume generator thread has crashed otherwise.",
            "                wait_time = 0",
            "                while True:",
            "                    if len(stream) == 0:",
            "                        if wait_time > max_wait_time:",
            "                            return",
            "                        wait_time += check_stream_rate",
            "                        time.sleep(check_stream_rate)",
            "                        continue",
            "                    wait_time = 0",
            "                    next_stream = stream.pop(0)",
            "                    if next_stream is None:",
            "                        return",
            "                    yield next_stream",
            "",
            "            return StreamingResponse(stream_wrapper())",
            "",
            "        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])",
            "        async def file_deprecated(path: str, request: fastapi.Request):",
            "            return await file(path, request)",
            "",
            "        @app.post(\"/reset/\")",
            "        @app.post(\"/reset\")",
            "        async def reset_iterator(body: ResetBody):  # noqa: ARG001",
            "            # No-op, all the cancelling/reset logic handled by /cancel",
            "            return {\"success\": True}",
            "",
            "        @app.get(\"/heartbeat/{session_hash}\")",
            "        def heartbeat(",
            "            session_hash: str,",
            "            request: fastapi.Request,",
            "            background_tasks: BackgroundTasks,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            \"\"\"Clients make a persistent connection to this endpoint to keep the session alive.",
            "            When the client disconnects, the session state is deleted.",
            "            \"\"\"",
            "            heartbeat_rate = 0.25 if os.getenv(\"GRADIO_IS_E2E_TEST\", None) else 15",
            "",
            "            async def wait():",
            "                await asyncio.sleep(heartbeat_rate)",
            "                return \"wait\"",
            "",
            "            async def stop_stream():",
            "                await app.stop_event.wait()",
            "                return \"stop\"",
            "",
            "            async def iterator():",
            "                while True:",
            "                    try:",
            "                        yield \"data: ALIVE\\n\\n\"",
            "                        # We need to close the heartbeat connections as soon as the server stops",
            "                        # otherwise the server can take forever to close",
            "                        wait_task = asyncio.create_task(wait())",
            "                        stop_stream_task = asyncio.create_task(stop_stream())",
            "                        done, _ = await asyncio.wait(",
            "                            [wait_task, stop_stream_task],",
            "                            return_when=asyncio.FIRST_COMPLETED,",
            "                        )",
            "                        done = [d.result() for d in done]",
            "                        if \"stop\" in done:",
            "                            raise asyncio.CancelledError()",
            "                    except asyncio.CancelledError:",
            "                        req = Request(request, username, session_hash=session_hash)",
            "                        root_path = route_utils.get_root_url(",
            "                            request=request,",
            "                            route_path=f\"/hearbeat/{session_hash}\",",
            "                            root_path=app.root_path,",
            "                        )",
            "                        body = PredictBodyInternal(",
            "                            session_hash=session_hash, data=[], request=request",
            "                        )",
            "                        unload_fn_indices = [",
            "                            i",
            "                            for i, dep in app.get_blocks().fns.items()",
            "                            if any(t for t in dep.targets if t[1] == \"unload\")",
            "                        ]",
            "                        for fn_index in unload_fn_indices:",
            "                            # The task runnning this loop has been cancelled",
            "                            # so we add tasks in the background",
            "                            background_tasks.add_task(",
            "                                route_utils.call_process_api,",
            "                                app=app,",
            "                                body=body,",
            "                                gr_request=req,",
            "                                fn=app.get_blocks().fns[fn_index],",
            "                                root_path=root_path,",
            "                            )",
            "                        # This will mark the state to be deleted in an hour",
            "                        if session_hash in app.state_holder.session_data:",
            "                            app.state_holder.session_data[session_hash].is_closed = True",
            "                        return",
            "",
            "            return StreamingResponse(iterator(), media_type=\"text/event-stream\")",
            "",
            "        # had to use '/run' endpoint for Colab compatibility, '/api' supported for backwards compatibility",
            "        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])",
            "        async def predict(",
            "            api_name: str,",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            body = PredictBodyInternal(**body.model_dump(), request=request)",
            "            fn = route_utils.get_fn(",
            "                blocks=app.get_blocks(), api_name=api_name, body=body",
            "            )",
            "",
            "            if not app.get_blocks().api_open and fn.queue:",
            "                raise HTTPException(",
            "                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",",
            "                    status_code=status.HTTP_404_NOT_FOUND,",
            "                )",
            "            gr_request = route_utils.compile_gr_request(",
            "                body,",
            "                fn=fn,",
            "                username=username,",
            "                request=request,",
            "            )",
            "            root_path = route_utils.get_root_url(",
            "                request=request, route_path=f\"/api/{api_name}\", root_path=app.root_path",
            "            )",
            "            try:",
            "                output = await route_utils.call_process_api(",
            "                    app=app,",
            "                    body=body,",
            "                    gr_request=gr_request,",
            "                    fn=fn,",
            "                    root_path=root_path,",
            "                )",
            "            except BaseException as error:",
            "                content = utils.error_payload(error, app.get_blocks().show_error)",
            "                traceback.print_exc()",
            "                return JSONResponse(",
            "                    content=content,",
            "                    status_code=500,",
            "                )",
            "            return output",
            "",
            "        @app.post(\"/call/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/call/{api_name}/\", dependencies=[Depends(login_check)])",
            "        async def simple_predict_post(",
            "            api_name: str,",
            "            body: SimplePredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            full_body = PredictBody(**body.model_dump(), simple_format=True)",
            "            fn = route_utils.get_fn(",
            "                blocks=app.get_blocks(), api_name=api_name, body=full_body",
            "            )",
            "            full_body.fn_index = fn._id",
            "            return await queue_join_helper(full_body, request, username)",
            "",
            "        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])",
            "        async def queue_join(",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            if body.session_hash is None:",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_400_BAD_REQUEST,",
            "                    detail=\"Session hash not found.\",",
            "                )",
            "            return await queue_join_helper(body, request, username)",
            "",
            "        async def queue_join_helper(",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str,",
            "        ):",
            "            blocks = app.get_blocks()",
            "",
            "            if blocks._queue.server_app is None:",
            "                blocks._queue.set_server_app(app)",
            "",
            "            if blocks._queue.stopped:",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,",
            "                    detail=\"Queue is stopped.\",",
            "                )",
            "            body = PredictBodyInternal(**body.model_dump(), request=request)",
            "            success, event_id = await blocks._queue.push(",
            "                body=body, request=request, username=username",
            "            )",
            "            if not success:",
            "                status_code = (",
            "                    status.HTTP_503_SERVICE_UNAVAILABLE",
            "                    if \"Queue is full.\" in event_id",
            "                    else status.HTTP_400_BAD_REQUEST",
            "                )",
            "                raise HTTPException(status_code=status_code, detail=event_id)",
            "            return {\"event_id\": event_id}",
            "",
            "        @app.post(\"/cancel\")",
            "        async def cancel_event(body: CancelBody):",
            "            await cancel_tasks({f\"{body.session_hash}_{body.fn_index}\"})",
            "            blocks = app.get_blocks()",
            "            # Need to complete the job so that the client disconnects",
            "            session_open = (",
            "                body.session_hash in blocks._queue.pending_messages_per_session",
            "            )",
            "            event_running = (",
            "                body.event_id",
            "                in blocks._queue.pending_event_ids_session.get(body.session_hash, {})",
            "            )",
            "            if session_open and event_running:",
            "                message = ProcessCompletedMessage(",
            "                    output={}, success=True, event_id=body.event_id",
            "                )",
            "                blocks._queue.pending_messages_per_session[",
            "                    body.session_hash",
            "                ].put_nowait(message)",
            "            if body.event_id in app.iterators:",
            "                async with app.lock:",
            "                    del app.iterators[body.event_id]",
            "                    app.iterators_to_reset.add(body.event_id)",
            "            return {\"success\": True}",
            "",
            "        @app.get(\"/call/{api_name}/{event_id}\", dependencies=[Depends(login_check)])",
            "        async def simple_predict_get(",
            "            request: fastapi.Request,",
            "            event_id: str,",
            "        ):",
            "            def process_msg(message: EventMessage) -> str | None:",
            "                msg = message.model_dump()",
            "                if isinstance(message, ProcessCompletedMessage):",
            "                    event = \"complete\" if message.success else \"error\"",
            "                    data = msg[\"output\"].get(\"data\")",
            "                elif isinstance(message, ProcessGeneratingMessage):",
            "                    event = \"generating\" if message.success else \"error\"",
            "                    data = msg[\"output\"].get(\"data\")",
            "                elif isinstance(message, HeartbeatMessage):",
            "                    event = \"heartbeat\"",
            "                    data = None",
            "                elif isinstance(message, UnexpectedErrorMessage):",
            "                    event = \"error\"",
            "                    data = message.message",
            "                else:",
            "                    return None",
            "                return f\"event: {event}\\ndata: {json.dumps(data)}\\n\\n\"",
            "",
            "            return await queue_data_helper(request, event_id, process_msg)",
            "",
            "        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])",
            "        async def queue_data(",
            "            request: fastapi.Request,",
            "            session_hash: str,",
            "        ):",
            "            def process_msg(message: EventMessage) -> str:",
            "                return f\"data: {orjson.dumps(message.model_dump(), default=str).decode('utf-8')}\\n\\n\"",
            "",
            "            return await queue_data_helper(request, session_hash, process_msg)",
            "",
            "        async def queue_data_helper(",
            "            request: fastapi.Request,",
            "            session_hash: str,",
            "            process_msg: Callable[[EventMessage], str | None],",
            "        ):",
            "            blocks = app.get_blocks()",
            "",
            "            async def sse_stream(request: fastapi.Request):",
            "                try:",
            "                    last_heartbeat = time.perf_counter()",
            "                    while True:",
            "                        if await request.is_disconnected():",
            "                            await blocks._queue.clean_events(session_hash=session_hash)",
            "                            return",
            "",
            "                        if (",
            "                            session_hash",
            "                            not in blocks._queue.pending_messages_per_session",
            "                        ):",
            "                            raise HTTPException(",
            "                                status_code=status.HTTP_404_NOT_FOUND,",
            "                                detail=\"Session not found.\",",
            "                            )",
            "",
            "                        heartbeat_rate = 15",
            "                        check_rate = 0.05",
            "                        message = None",
            "                        try:",
            "                            messages = blocks._queue.pending_messages_per_session[",
            "                                session_hash",
            "                            ]",
            "                            message = messages.get_nowait()",
            "                        except EmptyQueue:",
            "                            await asyncio.sleep(check_rate)",
            "                            if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                                # Fix this",
            "                                message = HeartbeatMessage()",
            "                                # Need to reset last_heartbeat with perf_counter",
            "                                # otherwise only a single hearbeat msg will be sent",
            "                                # and then the stream will retry leading to infinite queue \ud83d\ude2c",
            "                                last_heartbeat = time.perf_counter()",
            "",
            "                        if blocks._queue.stopped:",
            "                            message = UnexpectedErrorMessage(",
            "                                message=\"Server stopped unexpectedly.\",",
            "                                success=False,",
            "                            )",
            "                        if message:",
            "                            response = process_msg(message)",
            "                            if response is not None:",
            "                                yield response",
            "                            if (",
            "                                isinstance(message, ProcessCompletedMessage)",
            "                                and message.event_id",
            "                            ):",
            "                                blocks._queue.pending_event_ids_session[",
            "                                    session_hash",
            "                                ].remove(message.event_id)",
            "                                if message.msg == ServerMessage.server_stopped or (",
            "                                    message.msg == ServerMessage.process_completed",
            "                                    and (",
            "                                        len(",
            "                                            blocks._queue.pending_event_ids_session[",
            "                                                session_hash",
            "                                            ]",
            "                                        )",
            "                                        == 0",
            "                                    )",
            "                                ):",
            "                                    message = CloseStreamMessage()",
            "                                    response = process_msg(message)",
            "                                    if response is not None:",
            "                                        yield response",
            "                                    return",
            "                except BaseException as e:",
            "                    message = UnexpectedErrorMessage(",
            "                        message=str(e),",
            "                    )",
            "                    response = process_msg(message)",
            "                    if isinstance(e, asyncio.CancelledError):",
            "                        del blocks._queue.pending_messages_per_session[session_hash]",
            "                        await blocks._queue.clean_events(session_hash=session_hash)",
            "                    if response is not None:",
            "                        yield response",
            "                    raise e",
            "",
            "            return StreamingResponse(",
            "                sse_stream(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        async def get_item_or_file(",
            "            request: fastapi.Request,",
            "        ) -> Union[ComponentServerJSONBody, ComponentServerBlobBody]:",
            "            content_type = request.headers.get(\"Content-Type\")",
            "",
            "            if isinstance(content_type, str) and content_type.startswith(",
            "                \"multipart/form-data\"",
            "            ):",
            "                files = []",
            "                data = {}",
            "                async with request.form() as form:",
            "                    for key, value in form.items():",
            "                        if (",
            "                            isinstance(value, list)",
            "                            and len(value) > 1",
            "                            and isinstance(value[0], StarletteUploadFile)",
            "                        ):",
            "                            for i, v in enumerate(value):",
            "                                if isinstance(v, StarletteUploadFile):",
            "                                    filename = v.filename",
            "                                    contents = await v.read()",
            "                                    files.append((filename, contents))",
            "                                else:",
            "                                    data[f\"{key}-{i}\"] = v",
            "                        elif isinstance(value, StarletteUploadFile):",
            "                            filename = value.filename",
            "                            contents = await value.read()",
            "                            files.append((filename, contents))",
            "                        else:",
            "                            data[key] = value",
            "",
            "                return ComponentServerBlobBody(",
            "                    data=DataWithFiles(data=data, files=files),",
            "                    component_id=data[\"component_id\"],",
            "                    session_hash=data[\"session_hash\"],",
            "                    fn_name=data[\"fn_name\"],",
            "                )",
            "            else:",
            "                try:",
            "                    data = await request.json()",
            "                    return ComponentServerJSONBody(",
            "                        data=data[\"data\"],",
            "                        component_id=data[\"component_id\"],",
            "                        session_hash=data[\"session_hash\"],",
            "                        fn_name=data[\"fn_name\"],",
            "                    )",
            "",
            "                except Exception:",
            "                    raise HTTPException(",
            "                        status_code=status.HTTP_400_BAD_REQUEST,",
            "                        detail=\"Invalid JSON body.\",",
            "                    ) from None",
            "",
            "        @app.post(",
            "            \"/component_server\",",
            "            dependencies=[Depends(login_check)],",
            "        )",
            "        @app.post(",
            "            \"/component_server/\",",
            "            dependencies=[Depends(login_check)],",
            "        )",
            "        async def component_server(",
            "            request: fastapi.Request,",
            "        ):",
            "            body = await get_item_or_file(request)",
            "            state = app.state_holder[body.session_hash]",
            "            component_id = body.component_id",
            "            block: Block",
            "            if component_id in state:",
            "                block = state[component_id]",
            "            else:",
            "                block = app.get_blocks().blocks[component_id]",
            "            fn = getattr(block, body.fn_name, None)",
            "            if fn is None or not getattr(fn, \"_is_server_fn\", False):",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_404_NOT_FOUND,",
            "                    detail=\"Function not found.\",",
            "                )",
            "            return fn(body.data)",
            "",
            "        @app.get(",
            "            \"/queue/status\",",
            "            dependencies=[Depends(login_check)],",
            "            response_model=EstimationMessage,",
            "        )",
            "        async def get_queue_status():",
            "            return app.get_blocks()._queue.get_status()",
            "",
            "        @app.get(\"/upload_progress\")",
            "        def get_upload_progress(upload_id: str, request: fastapi.Request):",
            "            async def sse_stream(request: fastapi.Request):",
            "                last_heartbeat = time.perf_counter()",
            "                is_done = False",
            "                while True:",
            "                    if await request.is_disconnected():",
            "                        file_upload_statuses.stop_tracking(upload_id)",
            "                        return",
            "                    if is_done:",
            "                        file_upload_statuses.stop_tracking(upload_id)",
            "                        return",
            "",
            "                    heartbeat_rate = 15",
            "                    check_rate = 0.05",
            "                    try:",
            "                        if file_upload_statuses.is_done(upload_id):",
            "                            message = {\"msg\": \"done\"}",
            "                            is_done = True",
            "                        else:",
            "                            update = file_upload_statuses.pop(upload_id)",
            "                            message = {",
            "                                \"msg\": \"update\",",
            "                                \"orig_name\": update.filename,",
            "                                \"chunk_size\": update.chunk_size,",
            "                            }",
            "                        yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                    except FileUploadProgressNotTrackedError:",
            "                        return",
            "                    except FileUploadProgressNotQueuedError:",
            "                        await asyncio.sleep(check_rate)",
            "                        if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                            message = {\"msg\": \"heartbeat\"}",
            "                            yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                            last_heartbeat = time.perf_counter()",
            "",
            "            return StreamingResponse(",
            "                sse_stream(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/upload\", dependencies=[Depends(login_check)])",
            "        async def upload_file(",
            "            request: fastapi.Request,",
            "            bg_tasks: BackgroundTasks,",
            "            upload_id: Optional[str] = None,",
            "        ):",
            "            content_type_header = request.headers.get(\"Content-Type\")",
            "            content_type: bytes",
            "            content_type, _ = parse_options_header(content_type_header or \"\")",
            "            if content_type != b\"multipart/form-data\":",
            "                raise HTTPException(status_code=400, detail=\"Invalid content type.\")",
            "",
            "            try:",
            "                if upload_id:",
            "                    file_upload_statuses.track(upload_id)",
            "                max_file_size = app.get_blocks().max_file_size",
            "                max_file_size = max_file_size if max_file_size is not None else math.inf",
            "                multipart_parser = GradioMultiPartParser(",
            "                    request.headers,",
            "                    request.stream(),",
            "                    max_files=1000,",
            "                    max_fields=1000,",
            "                    max_file_size=max_file_size,",
            "                    upload_id=upload_id if upload_id else None,",
            "                    upload_progress=file_upload_statuses if upload_id else None,",
            "                )",
            "                form = await multipart_parser.parse()",
            "            except MultiPartException as exc:",
            "                code = 413 if \"maximum allowed size\" in exc.message else 400",
            "                return PlainTextResponse(exc.message, status_code=code)",
            "",
            "            output_files = []",
            "            files_to_copy = []",
            "            locations: list[str] = []",
            "",
            "            for temp_file in form.getlist(\"files\"):",
            "                if not isinstance(temp_file, GradioUploadFile):",
            "                    raise TypeError(\"File is not an instance of GradioUploadFile\")",
            "                if temp_file.filename:",
            "                    file_name = Path(temp_file.filename).name",
            "                    name = client_utils.strip_invalid_filename_characters(file_name)",
            "                else:",
            "                    name = f\"tmp{secrets.token_hex(5)}\"",
            "                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()",
            "                directory.mkdir(exist_ok=True, parents=True)",
            "                try:",
            "                    dest = utils.safe_join(",
            "                        DeveloperPath(str(directory)), UserProvidedPath(name)",
            "                    )",
            "                except InvalidPathError as err:",
            "                    raise HTTPException(",
            "                        status_code=400, detail=f\"Invalid file name: {name}\"",
            "                    ) from err",
            "                temp_file.file.close()",
            "                # we need to move the temp file to the cache directory",
            "                # but that's possibly blocking and we're in an async function",
            "                # so we try to rename (this is what shutil.move tries first)",
            "                # which should be super fast.",
            "                # if that fails, we move in the background.",
            "                try:",
            "                    os.rename(temp_file.file.name, dest)",
            "                except OSError:",
            "                    files_to_copy.append(temp_file.file.name)",
            "                    locations.append(dest)",
            "                output_files.append(dest)",
            "                blocks.upload_file_set.add(dest)",
            "            if files_to_copy:",
            "                bg_tasks.add_task(",
            "                    move_uploaded_files_to_cache, files_to_copy, locations",
            "                )",
            "            return output_files",
            "",
            "        @app.on_event(\"startup\")",
            "        @app.get(\"/startup-events\")",
            "        async def startup_events():",
            "            if not app.startup_events_triggered:",
            "                app.get_blocks().startup_events()",
            "                app.startup_events_triggered = True",
            "                return True",
            "            return False",
            "",
            "        @app.get(\"/theme.css\", response_class=PlainTextResponse)",
            "        def theme_css():",
            "            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")",
            "",
            "        @app.get(\"/robots.txt\", response_class=PlainTextResponse)",
            "        def robots_txt():",
            "            if app.get_blocks().share:",
            "                return \"User-agent: *\\nDisallow: /\"",
            "            else:",
            "                return \"User-agent: *\\nDisallow: \"",
            "",
            "        @app.get(\"/monitoring\", dependencies=[Depends(login_check)])",
            "        async def analytics_login(request: fastapi.Request):",
            "            if not blocks.enable_monitoring:",
            "                raise HTTPException(",
            "                    status_code=403, detail=\"Monitoring is not enabled.\"",
            "                )",
            "            root_url = route_utils.get_root_url(",
            "                request=request, route_path=\"/monitoring\", root_path=app.root_path",
            "            )",
            "            monitoring_url = f\"{root_url}/monitoring/{app.analytics_key}\"",
            "            print(f\"* Monitoring URL: {monitoring_url} *\")",
            "            return HTMLResponse(\"See console for monitoring URL.\")",
            "",
            "        @app.get(\"/monitoring/{key}\")",
            "        async def analytics_dashboard(key: str):",
            "            if not blocks.enable_monitoring:",
            "                raise HTTPException(",
            "                    status_code=403, detail=\"Monitoring is not enabled.\"",
            "                )",
            "            if compare_passwords_securely(key, app.analytics_key):",
            "                analytics_url = f\"/monitoring/{app.analytics_key}/dashboard\"",
            "                if not app.monitoring_enabled:",
            "                    from gradio.monitoring_dashboard import data",
            "                    from gradio.monitoring_dashboard import demo as dashboard",
            "",
            "                    mount_gradio_app(app, dashboard, path=analytics_url)",
            "                    dashboard._queue.start()",
            "                    analytics = app.get_blocks()._queue.event_analytics",
            "                    data[\"data\"] = analytics",
            "                    app.monitoring_enabled = True",
            "                return RedirectResponse(",
            "                    url=analytics_url, status_code=status.HTTP_302_FOUND",
            "                )",
            "            else:",
            "                raise HTTPException(status_code=403, detail=\"Invalid key.\")",
            "",
            "        return app",
            "",
            "",
            "########",
            "# Helper functions",
            "########",
            "",
            "",
            "def routes_safe_join(directory: DeveloperPath, path: UserProvidedPath) -> str:",
            "    \"\"\"Safely join the user path to the directory while performing some additional http-related checks,",
            "    e.g. ensuring that the full path exists on the local file system and is not a directory\"\"\"",
            "    if path == \"\":",
            "        raise fastapi.HTTPException(400)",
            "    if route_utils.starts_with_protocol(path):",
            "        raise fastapi.HTTPException(403)",
            "    try:",
            "        fullpath = Path(utils.safe_join(directory, path))",
            "    except InvalidPathError as e:",
            "        raise fastapi.HTTPException(403) from e",
            "    if fullpath.is_dir():",
            "        raise fastapi.HTTPException(403)",
            "    if not fullpath.exists():",
            "        raise fastapi.HTTPException(404)",
            "    return str(fullpath)",
            "",
            "",
            "def get_types(cls_set: List[Type]):",
            "    docset = []",
            "    types = []",
            "    for cls in cls_set:",
            "        doc = inspect.getdoc(cls) or \"\"",
            "        doc_lines = doc.split(\"\\n\")",
            "        for line in doc_lines:",
            "            if \"value (\" in line:",
            "                types.append(line.split(\"value (\")[1].split(\")\")[0])",
            "        docset.append(doc_lines[1].split(\":\")[-1])",
            "    return docset, types",
            "",
            "",
            "@document()",
            "def mount_gradio_app(",
            "    app: fastapi.FastAPI,",
            "    blocks: gradio.Blocks,",
            "    path: str,",
            "    app_kwargs: dict[str, Any] | None = None,",
            "    *,",
            "    auth: Callable | tuple[str, str] | list[tuple[str, str]] | None = None,",
            "    auth_message: str | None = None,",
            "    auth_dependency: Callable[[fastapi.Request], str | None] | None = None,",
            "    root_path: str | None = None,",
            "    allowed_paths: list[str] | None = None,",
            "    blocked_paths: list[str] | None = None,",
            "    favicon_path: str | None = None,",
            "    show_error: bool = True,",
            "    max_file_size: str | int | None = None,",
            ") -> fastapi.FastAPI:",
            "    \"\"\"Mount a gradio.Blocks to an existing FastAPI application.",
            "",
            "    Parameters:",
            "        app: The parent FastAPI application.",
            "        blocks: The blocks object we want to mount to the parent app.",
            "        path: The path at which the gradio application will be mounted.",
            "        app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`",
            "        auth: If provided, username and password (or list of username-password tuples) required to access the gradio app. Can also provide function that takes username and password and returns True if valid login.",
            "        auth_message: If provided, HTML message provided on login page for this gradio app.",
            "        auth_dependency: A function that takes a FastAPI request and returns a string user ID or None. If the function returns None for a specific request, that user is not authorized to access the gradio app (they will see a 401 Unauthorized response). To be used with external authentication systems like OAuth. Cannot be used with `auth`.",
            "        root_path: The subpath corresponding to the public deployment of this FastAPI application. For example, if the application is served at \"https://example.com/myapp\", the `root_path` should be set to \"/myapp\". A full URL beginning with http:// or https:// can be provided, which will be used in its entirety. Normally, this does not need to provided (even if you are using a custom `path`). However, if you are serving the FastAPI app behind a proxy, the proxy may not provide the full path to the Gradio app in the request headers. In which case, you can provide the root path here.",
            "        allowed_paths: List of complete filepaths or parent directories that this gradio app is allowed to serve. Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",
            "        blocked_paths: List of complete filepaths or parent directories that this gradio app is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",
            "        favicon_path: If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for this gradio app's page.",
            "        show_error: If True, any errors in the gradio app will be displayed in an alert modal and printed in the browser console log. Otherwise, errors will only be visible in the terminal session running the Gradio app.",
            "        max_file_size: The maximum file size in bytes that can be uploaded. Can be a string of the form \"<value><unit>\", where value is any positive integer and unit is one of \"b\", \"kb\", \"mb\", \"gb\", \"tb\". If None, no limit is set.",
            "    Example:",
            "        from fastapi import FastAPI",
            "        import gradio as gr",
            "        app = FastAPI()",
            "        @app.get(\"/\")",
            "        def read_main():",
            "            return {\"message\": \"This is your main app\"}",
            "        io = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")",
            "        app = gr.mount_gradio_app(app, io, path=\"/gradio\")",
            "        # Then run `uvicorn run:app` from the terminal and navigate to http://localhost:8000/gradio.",
            "    \"\"\"",
            "    if favicon_path is not None and path != \"/\":",
            "        warnings.warn(",
            "            \"The 'favicon_path' parameter is set but will be ignored because 'path' is not '/'. \"",
            "            \"Please add the favicon directly to your FastAPI app.\"",
            "        )",
            "",
            "    blocks.dev_mode = False",
            "    blocks.max_file_size = utils._parse_file_size(max_file_size)",
            "    blocks.config = blocks.get_config_file()",
            "    blocks.validate_queue_settings()",
            "    if auth is not None and auth_dependency is not None:",
            "        raise ValueError(",
            "            \"You cannot provide both `auth` and `auth_dependency` in mount_gradio_app(). Please choose one.\"",
            "        )",
            "    if (",
            "        auth",
            "        and not callable(auth)",
            "        and not isinstance(auth[0], tuple)",
            "        and not isinstance(auth[0], list)",
            "    ):",
            "        blocks.auth = [auth]",
            "    else:",
            "        blocks.auth = auth",
            "    blocks.auth_message = auth_message",
            "    blocks.favicon_path = favicon_path",
            "    blocks.allowed_paths = allowed_paths or []",
            "    blocks.blocked_paths = blocked_paths or []",
            "    blocks.show_error = show_error",
            "",
            "    if not isinstance(blocks.allowed_paths, list):",
            "        raise ValueError(\"`allowed_paths` must be a list of directories.\")",
            "    if not isinstance(blocks.blocked_paths, list):",
            "        raise ValueError(\"`blocked_paths` must be a list of directories.\")",
            "",
            "    if root_path is not None:",
            "        blocks.root_path = root_path",
            "",
            "    gradio_app = App.create_app(",
            "        blocks, app_kwargs=app_kwargs, auth_dependency=auth_dependency",
            "    )",
            "    old_lifespan = app.router.lifespan_context",
            "",
            "    @contextlib.asynccontextmanager",
            "    async def new_lifespan(app: FastAPI):",
            "        async with old_lifespan(",
            "            app",
            "        ):  # Instert the startup events inside the FastAPI context manager",
            "            async with gradio_app.router.lifespan_context(gradio_app):",
            "                gradio_app.get_blocks().startup_events()",
            "                yield",
            "",
            "    app.router.lifespan_context = new_lifespan",
            "",
            "    app.mount(path, gradio_app)",
            "    return app"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "732": [
                "App",
                "create_app",
                "heartbeat"
            ],
            "778": [
                "App",
                "create_app"
            ],
            "813": [
                "App",
                "create_app"
            ],
            "814": [
                "App",
                "create_app"
            ],
            "815": [
                "App",
                "create_app"
            ],
            "850": [
                "App",
                "create_app"
            ]
        },
        "addLocation": []
    }
}