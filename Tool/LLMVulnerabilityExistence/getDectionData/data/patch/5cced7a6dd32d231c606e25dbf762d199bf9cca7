{
    "nova/tests/virt/xenapi/test_xenapi.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 3298,
                "afterPatchRowNumber": 3298,
                "PatchRowcode": "         # ensure method is present"
            },
            "1": {
                "beforePatchRowNumber": 3299,
                "afterPatchRowNumber": 3299,
                "PatchRowcode": "         stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)"
            },
            "2": {
                "beforePatchRowNumber": 3300,
                "afterPatchRowNumber": 3300,
                "PatchRowcode": "         self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)"
            },
            "3": {
                "beforePatchRowNumber": 3301,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.conn.post_live_migration_at_destination(None, None, None, None)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3301,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3302,
                "PatchRowcode": "+        fake_instance = \"instance\""
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3303,
                "PatchRowcode": "+        fake_network_info = \"network_info\""
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3304,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3305,
                "PatchRowcode": "+        def fake_fw(instance, network_info):"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3306,
                "PatchRowcode": "+            self.assertEquals(instance, fake_instance)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3307,
                "PatchRowcode": "+            self.assertEquals(network_info, fake_network_info)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3308,
                "PatchRowcode": "+            fake_fw.called += 1"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3309,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3310,
                "PatchRowcode": "+        fake_fw.called = 0"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3311,
                "PatchRowcode": "+        _vmops = self.conn._vmops"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3312,
                "PatchRowcode": "+        self.stubs.Set(_vmops.firewall_driver,"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3313,
                "PatchRowcode": "+                       'setup_basic_filtering', fake_fw)"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3314,
                "PatchRowcode": "+        self.stubs.Set(_vmops.firewall_driver,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3315,
                "PatchRowcode": "+                       'prepare_instance_filter', fake_fw)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3316,
                "PatchRowcode": "+        self.stubs.Set(_vmops.firewall_driver,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3317,
                "PatchRowcode": "+                       'apply_instance_filter', fake_fw)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3318,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3319,
                "PatchRowcode": "+        self.conn.post_live_migration_at_destination(None, fake_instance,"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3320,
                "PatchRowcode": "+                                                     fake_network_info, None)"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3321,
                "PatchRowcode": "+        self.assertEqual(fake_fw.called, 3)"
            },
            "25": {
                "beforePatchRowNumber": 3302,
                "afterPatchRowNumber": 3322,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 3303,
                "afterPatchRowNumber": 3323,
                "PatchRowcode": "     def test_check_can_live_migrate_destination_with_block_migration(self):"
            },
            "27": {
                "beforePatchRowNumber": 3304,
                "afterPatchRowNumber": 3324,
                "PatchRowcode": "         stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)"
            }
        },
        "frontPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "#    Copyright (c) 2010 Citrix Systems, Inc.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"Test suite for XenAPI.\"\"\"",
            "",
            "import ast",
            "import base64",
            "import contextlib",
            "import copy",
            "import functools",
            "import mox",
            "import os",
            "import re",
            "",
            "import mox",
            "from oslo.config import cfg",
            "",
            "from nova.compute import api as compute_api",
            "from nova.compute import flavors",
            "from nova.compute import power_state",
            "from nova.compute import task_states",
            "from nova.compute import vm_states",
            "from nova import context",
            "from nova import crypto",
            "from nova import db",
            "from nova import exception",
            "from nova.openstack.common.gettextutils import _",
            "from nova.openstack.common import importutils",
            "from nova.openstack.common import jsonutils",
            "from nova.openstack.common import log as logging",
            "from nova import test",
            "from nova.tests.db import fakes as db_fakes",
            "from nova.tests import fake_instance",
            "from nova.tests import fake_network",
            "from nova.tests import fake_processutils",
            "import nova.tests.image.fake as fake_image",
            "from nova.tests import matchers",
            "from nova.tests.virt.xenapi import stubs",
            "from nova.virt import fake",
            "from nova.virt.xenapi import agent",
            "from nova.virt.xenapi import driver as xenapi_conn",
            "from nova.virt.xenapi import fake as xenapi_fake",
            "from nova.virt.xenapi import host",
            "from nova.virt.xenapi.image import glance",
            "from nova.virt.xenapi import pool",
            "from nova.virt.xenapi import pool_states",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import vmops",
            "from nova.virt.xenapi import volume_utils",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "CONF = cfg.CONF",
            "CONF.import_opt('compute_manager', 'nova.service')",
            "CONF.import_opt('network_manager', 'nova.service')",
            "CONF.import_opt('compute_driver', 'nova.virt.driver')",
            "CONF.import_opt('host', 'nova.netconf')",
            "CONF.import_opt('default_availability_zone', 'nova.availability_zones')",
            "",
            "IMAGE_MACHINE = '1'",
            "IMAGE_KERNEL = '2'",
            "IMAGE_RAMDISK = '3'",
            "IMAGE_RAW = '4'",
            "IMAGE_VHD = '5'",
            "IMAGE_ISO = '6'",
            "IMAGE_IPXE_ISO = '7'",
            "",
            "IMAGE_FIXTURES = {",
            "    IMAGE_MACHINE: {",
            "        'image_meta': {'name': 'fakemachine', 'size': 0,",
            "                       'disk_format': 'ami',",
            "                       'container_format': 'ami'},",
            "    },",
            "    IMAGE_KERNEL: {",
            "        'image_meta': {'name': 'fakekernel', 'size': 0,",
            "                       'disk_format': 'aki',",
            "                       'container_format': 'aki'},",
            "    },",
            "    IMAGE_RAMDISK: {",
            "        'image_meta': {'name': 'fakeramdisk', 'size': 0,",
            "                       'disk_format': 'ari',",
            "                       'container_format': 'ari'},",
            "    },",
            "    IMAGE_RAW: {",
            "        'image_meta': {'name': 'fakeraw', 'size': 0,",
            "                       'disk_format': 'raw',",
            "                       'container_format': 'bare'},",
            "    },",
            "    IMAGE_VHD: {",
            "        'image_meta': {'name': 'fakevhd', 'size': 0,",
            "                       'disk_format': 'vhd',",
            "                       'container_format': 'ovf'},",
            "    },",
            "    IMAGE_ISO: {",
            "        'image_meta': {'name': 'fakeiso', 'size': 0,",
            "                       'disk_format': 'iso',",
            "                       'container_format': 'bare'},",
            "    },",
            "    IMAGE_IPXE_ISO: {",
            "        'image_meta': {'name': 'fake_ipxe_iso', 'size': 0,",
            "                       'disk_format': 'iso',",
            "                       'container_format': 'bare',",
            "                       'properties': {'ipxe_boot': 'true'}},",
            "    },",
            "}",
            "",
            "",
            "def set_image_fixtures():",
            "    image_service = fake_image.FakeImageService()",
            "    image_service.images.clear()",
            "    for image_id, image_meta in IMAGE_FIXTURES.items():",
            "        image_meta = image_meta['image_meta']",
            "        image_meta['id'] = image_id",
            "        image_service.create(None, image_meta)",
            "",
            "",
            "def get_fake_device_info():",
            "    # FIXME: 'sr_uuid', 'introduce_sr_keys', sr_type and vdi_uuid",
            "    # can be removed from the dict when LP bug #1087308 is fixed",
            "    fake_vdi_ref = xenapi_fake.create_vdi('fake-vdi', None)",
            "    fake_vdi_uuid = xenapi_fake.get_record('VDI', fake_vdi_ref)['uuid']",
            "    fake = {'block_device_mapping':",
            "              [{'connection_info': {'driver_volume_type': 'iscsi',",
            "                                    'data': {'sr_uuid': 'falseSR',",
            "                                             'introduce_sr_keys': ['sr_type'],",
            "                                             'sr_type': 'iscsi',",
            "                                             'vdi_uuid': fake_vdi_uuid,",
            "                                             'target_discovered': False,",
            "                                             'target_iqn': 'foo_iqn:foo_volid',",
            "                                             'target_portal': 'localhost:3260',",
            "                                             'volume_id': 'foo_volid',",
            "                                             'target_lun': 1,",
            "                                             'auth_password': 'my-p@55w0rd',",
            "                                             'auth_username': 'johndoe',",
            "                                             'auth_method': u'CHAP'}, },",
            "                'mount_device': 'vda',",
            "                'delete_on_termination': False}, ],",
            "            'root_device_name': '/dev/sda',",
            "            'ephemerals': [],",
            "            'swap': None, }",
            "    return fake",
            "",
            "",
            "def stub_vm_utils_with_vdi_attached_here(function):",
            "    \"\"\"",
            "    vm_utils.with_vdi_attached_here needs to be stubbed out because it",
            "    calls down to the filesystem to attach a vdi. This provides a",
            "    decorator to handle that.",
            "    \"\"\"",
            "    @functools.wraps(function)",
            "    def decorated_function(self, *args, **kwargs):",
            "        @contextlib.contextmanager",
            "        def fake_vdi_attached_here(*args, **kwargs):",
            "            fake_dev = 'fakedev'",
            "            yield fake_dev",
            "",
            "        def fake_image_download(*args, **kwargs):",
            "            pass",
            "",
            "        orig_vdi_attached_here = vm_utils.vdi_attached_here",
            "        orig_image_download = fake_image._FakeImageService.download",
            "        try:",
            "            vm_utils.vdi_attached_here = fake_vdi_attached_here",
            "            fake_image._FakeImageService.download = fake_image_download",
            "            return function(self, *args, **kwargs)",
            "        finally:",
            "            fake_image._FakeImageService.download = orig_image_download",
            "            vm_utils.vdi_attached_here = orig_vdi_attached_here",
            "",
            "    return decorated_function",
            "",
            "",
            "def create_instance_with_system_metadata(context, instance_values):",
            "    instance_type = db.flavor_get(context,",
            "                                         instance_values['instance_type_id'])",
            "    sys_meta = flavors.save_flavor_info({},",
            "                                                      instance_type)",
            "    instance_values['system_metadata'] = sys_meta",
            "    return db.instance_create(context, instance_values)",
            "",
            "",
            "class XenAPIVolumeTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for Volume operations.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIVolumeTestCase, self).setUp()",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "        self.flags(disable_process_locking=True,",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver',",
            "                   xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass')",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        self.instance_values = {'id': 1,",
            "                  'project_id': self.user_id,",
            "                  'user_id': 'fake',",
            "                  'image_ref': 1,",
            "                  'kernel_id': 2,",
            "                  'ramdisk_id': 3,",
            "                  'root_gb': 20,",
            "                  'instance_type_id': '3',  # m1.large",
            "                  'os_type': 'linux',",
            "                  'architecture': 'x86-64'}",
            "",
            "    def _create_volume(self, size=0):",
            "        \"\"\"Create a volume object.\"\"\"",
            "        vol = {}",
            "        vol['size'] = size",
            "        vol['user_id'] = 'fake'",
            "        vol['project_id'] = 'fake'",
            "        vol['host'] = 'localhost'",
            "        vol['availability_zone'] = CONF.default_availability_zone",
            "        vol['status'] = \"creating\"",
            "        vol['attach_status'] = \"detached\"",
            "        return db.volume_create(self.context, vol)",
            "",
            "    @staticmethod",
            "    def _make_connection_data():",
            "        return {",
            "            'volume_id': 1,",
            "            'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',",
            "            'target_portal': '127.0.0.1:3260,fake',",
            "            'target_lun': None,",
            "            'auth_method': 'CHAP',",
            "            'auth_username': 'username',",
            "            'auth_password': 'password',",
            "        }",
            "",
            "    @classmethod",
            "    def _make_connection_info(cls):",
            "        return {",
            "            'driver_volume_type': 'iscsi',",
            "            'data': cls._make_connection_data()",
            "        }",
            "",
            "    def test_mountpoint_to_number(self):",
            "        cases = {",
            "            'sda': 0,",
            "            'sdp': 15,",
            "            'hda': 0,",
            "            'hdp': 15,",
            "            'vda': 0,",
            "            'xvda': 0,",
            "            '0': 0,",
            "            '10': 10,",
            "            'vdq': -1,",
            "            'sdq': -1,",
            "            'hdq': -1,",
            "            'xvdq': -1,",
            "        }",
            "",
            "        for (input, expected) in cases.iteritems():",
            "            actual = volume_utils.mountpoint_to_number(input)",
            "            self.assertEqual(actual, expected,",
            "                    '%s yielded %s, not %s' % (input, actual, expected))",
            "",
            "    def test_parse_volume_info_parsing_auth_details(self):",
            "        result = volume_utils.parse_volume_info(",
            "            self._make_connection_data())",
            "",
            "        self.assertEquals('username', result['chapuser'])",
            "        self.assertEquals('password', result['chappassword'])",
            "",
            "    def test_get_device_number_raise_exception_on_wrong_mountpoint(self):",
            "        self.assertRaises(",
            "            volume_utils.StorageError,",
            "            volume_utils.get_device_number,",
            "            'dev/sd')",
            "",
            "    def test_attach_volume(self):",
            "        # This shows how to test Ops classes' methods.",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVolumeTests)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        vm = xenapi_fake.create_vm(instance['name'], 'Running')",
            "        result = conn.attach_volume(None, self._make_connection_info(),",
            "                                    instance, '/dev/sdc')",
            "",
            "        # check that the VM has a VBD attached to it",
            "        # Get XenAPI record for VBD",
            "        vbds = xenapi_fake.get_all('VBD')",
            "        vbd = xenapi_fake.get_record('VBD', vbds[0])",
            "        vm_ref = vbd['VM']",
            "        self.assertEqual(vm_ref, vm)",
            "",
            "    def test_attach_volume_raise_exception(self):",
            "        # This shows how to test when exceptions are raised.",
            "        stubs.stubout_session(self.stubs,",
            "                              stubs.FakeSessionForVolumeFailedTests)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        self.assertRaises(exception.VolumeDriverNotFound,",
            "                          conn.attach_volume,",
            "                          None, {'driver_volume_type': 'nonexist'},",
            "                          instance, '/dev/sdc')",
            "",
            "",
            "class XenAPIVMTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for VM operations.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIVMTestCase, self).setUp()",
            "        self.useFixture(test.SampleNetworks())",
            "        self.network = importutils.import_object(CONF.network_manager)",
            "        self.flags(disable_process_locking=True,",
            "                   instance_name_template='%d',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver',",
            "                   xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',)",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        xenapi_fake.create_network('fake', 'fake_br1')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        stubs.stubout_get_this_vm_uuid(self.stubs)",
            "        stubs.stub_out_vm_methods(self.stubs)",
            "        fake_processutils.stub_out_processutils_execute(self.stubs)",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.conn._session.is_local_connection = False",
            "",
            "        self.stubs.Set(fake.FakeVirtAPI, 'instance_update',",
            "                       lambda *args, **kwargs: ('fake-oldref', 'fake-newref'))",
            "        fake_image.stub_out_image_service(self.stubs)",
            "        set_image_fixtures()",
            "        stubs.stubout_image_service_download(self.stubs)",
            "        stubs.stubout_stream_disk(self.stubs)",
            "",
            "        def fake_inject_instance_metadata(self, instance, vm):",
            "            pass",
            "        self.stubs.Set(vmops.VMOps, '_inject_instance_metadata',",
            "                       fake_inject_instance_metadata)",
            "",
            "        def fake_safe_copy_vdi(session, sr_ref, instance, vdi_to_copy_ref):",
            "            name_label = \"fakenamelabel\"",
            "            disk_type = \"fakedisktype\"",
            "            virtual_size = 777",
            "            return vm_utils.create_vdi(",
            "                    session, sr_ref, instance, name_label, disk_type,",
            "                    virtual_size)",
            "        self.stubs.Set(vm_utils, '_safe_copy_vdi', fake_safe_copy_vdi)",
            "",
            "    def tearDown(self):",
            "        fake_image.FakeImageService_reset()",
            "        super(XenAPIVMTestCase, self).tearDown()",
            "",
            "    def test_init_host(self):",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        vm = vm_utils._get_this_vm_ref(session)",
            "        # Local root disk",
            "        vdi0 = xenapi_fake.create_vdi('compute', None)",
            "        vbd0 = xenapi_fake.create_vbd(vm, vdi0)",
            "        # Instance VDI",
            "        vdi1 = xenapi_fake.create_vdi('instance-aaaa', None,",
            "                other_config={'nova_instance_uuid': 'aaaa'})",
            "        vbd1 = xenapi_fake.create_vbd(vm, vdi1)",
            "        # Only looks like instance VDI",
            "        vdi2 = xenapi_fake.create_vdi('instance-bbbb', None)",
            "        vbd2 = xenapi_fake.create_vbd(vm, vdi2)",
            "",
            "        self.conn.init_host(None)",
            "        self.assertEquals(set(xenapi_fake.get_all('VBD')), set([vbd0, vbd2]))",
            "",
            "    def test_instance_exists(self):",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        vm_utils.lookup(mox.IgnoreArg(), 'foo').AndReturn(True)",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertTrue(self.conn.instance_exists('foo'))",
            "",
            "    def test_instance_not_exists(self):",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        vm_utils.lookup(mox.IgnoreArg(), 'bar').AndReturn(None)",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertFalse(self.conn.instance_exists('bar'))",
            "",
            "    def test_list_instances_0(self):",
            "        instances = self.conn.list_instances()",
            "        self.assertEquals(instances, [])",
            "",
            "    def test_list_instance_uuids_0(self):",
            "        instance_uuids = self.conn.list_instance_uuids()",
            "        self.assertEquals(instance_uuids, [])",
            "",
            "    def test_list_instance_uuids(self):",
            "        uuids = []",
            "        for x in xrange(1, 4):",
            "            instance = self._create_instance(x)",
            "            uuids.append(instance['uuid'])",
            "        instance_uuids = self.conn.list_instance_uuids()",
            "        self.assertEqual(len(uuids), len(instance_uuids))",
            "        self.assertEqual(set(uuids), set(instance_uuids))",
            "",
            "    def test_get_rrd_server(self):",
            "        self.flags(xenapi_connection_url='myscheme://myaddress/')",
            "        server_info = vm_utils._get_rrd_server()",
            "        self.assertEqual(server_info[0], 'myscheme')",
            "        self.assertEqual(server_info[1], 'myaddress')",
            "",
            "    def test_get_diagnostics(self):",
            "        def fake_get_rrd(host, vm_uuid):",
            "            path = os.path.dirname(os.path.realpath(__file__))",
            "            with open(os.path.join(path, 'vm_rrd.xml')) as f:",
            "                return re.sub(r'\\s', '', f.read())",
            "        self.stubs.Set(vm_utils, '_get_rrd', fake_get_rrd)",
            "",
            "        fake_diagnostics = {",
            "            'vbd_xvdb_write': '0.0',",
            "            'memory_target': '4294967296.0000',",
            "            'memory_internal_free': '1415564.0000',",
            "            'memory': '4294967296.0000',",
            "            'vbd_xvda_write': '0.0',",
            "            'cpu0': '0.0042',",
            "            'vif_0_tx': '287.4134',",
            "            'vbd_xvda_read': '0.0',",
            "            'vif_0_rx': '1816.0144',",
            "            'vif_2_rx': '0.0',",
            "            'vif_2_tx': '0.0',",
            "            'vbd_xvdb_read': '0.0',",
            "            'last_update': '1328795567',",
            "        }",
            "        instance = self._create_instance()",
            "        expected = self.conn.get_diagnostics(instance)",
            "        self.assertThat(fake_diagnostics, matchers.DictMatches(expected))",
            "",
            "    def test_get_vnc_console(self):",
            "        instance = self._create_instance()",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                fake.FakeVirtAPI())",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vm_ref = vm_utils.lookup(session, instance['name'])",
            "",
            "        console = conn.get_vnc_console(instance)",
            "",
            "        # Note(sulo): We dont care about session id in test",
            "        # they will always differ so strip that out",
            "        actual_path = console['internal_access_path'].split('&')[0]",
            "        expected_path = \"/console?ref=%s\" % str(vm_ref)",
            "",
            "        self.assertEqual(expected_path, actual_path)",
            "",
            "    def test_get_vnc_console_for_rescue(self):",
            "        instance = self._create_instance()",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        rescue_vm = xenapi_fake.create_vm(instance['name'] + '-rescue',",
            "                                          'Running')",
            "        # Set instance state to rescued",
            "        instance['vm_state'] = 'rescued'",
            "",
            "        console = conn.get_vnc_console(instance)",
            "",
            "        # Note(sulo): We dont care about session id in test",
            "        # they will always differ so strip that out",
            "        actual_path = console['internal_access_path'].split('&')[0]",
            "        expected_path = \"/console?ref=%s\" % str(rescue_vm)",
            "",
            "        self.assertEqual(expected_path, actual_path)",
            "",
            "    def test_get_vnc_console_instance_not_ready(self):",
            "        instance = {}",
            "        # set instance name and state",
            "        instance['name'] = 'fake-instance'",
            "        instance['uuid'] = '00000000-0000-0000-0000-000000000000'",
            "        instance['vm_state'] = 'building'",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.InstanceNotFound,",
            "                          conn.get_vnc_console, instance)",
            "",
            "    def test_get_vnc_console_rescue_not_ready(self):",
            "        instance = {}",
            "        instance['name'] = 'fake-rescue'",
            "        instance['uuid'] = '00000000-0000-0000-0000-000000000001'",
            "        instance['vm_state'] = 'rescued'",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.InstanceNotReady,",
            "                          conn.get_vnc_console, instance)",
            "",
            "    def test_instance_snapshot_fails_with_no_primary_vdi(self):",
            "",
            "        def create_bad_vbd(session, vm_ref, vdi_ref, userdevice,",
            "                           vbd_type='disk', read_only=False, bootable=False,",
            "                           osvol=False):",
            "            vbd_rec = {'VM': vm_ref,",
            "               'VDI': vdi_ref,",
            "               'userdevice': 'fake',",
            "               'currently_attached': False}",
            "            vbd_ref = xenapi_fake._create_object('VBD', vbd_rec)",
            "            xenapi_fake.after_VBD_create(vbd_ref, vbd_rec)",
            "            return vbd_ref",
            "",
            "        self.stubs.Set(vm_utils, 'create_vbd', create_bad_vbd)",
            "        stubs.stubout_instance_snapshot(self.stubs)",
            "        # Stubbing out firewall driver as previous stub sets alters",
            "        # xml rpc result parsing",
            "        stubs.stubout_firewall_driver(self.stubs, self.conn)",
            "        instance = self._create_instance()",
            "",
            "        image_id = \"my_snapshot_id\"",
            "        self.assertRaises(exception.NovaException, self.conn.snapshot,",
            "                          self.context, instance, image_id,",
            "                          lambda *args, **kwargs: None)",
            "",
            "    def test_instance_snapshot(self):",
            "        expected_calls = [",
            "            {'args': (),",
            "             'kwargs':",
            "                 {'task_state': task_states.IMAGE_PENDING_UPLOAD}},",
            "            {'args': (),",
            "             'kwargs':",
            "                 {'task_state': task_states.IMAGE_UPLOADING,",
            "                  'expected_state': task_states.IMAGE_PENDING_UPLOAD}}]",
            "        func_call_matcher = matchers.FunctionCallMatcher(expected_calls)",
            "        image_id = \"my_snapshot_id\"",
            "",
            "        stubs.stubout_instance_snapshot(self.stubs)",
            "        stubs.stubout_is_snapshot(self.stubs)",
            "        # Stubbing out firewall driver as previous stub sets alters",
            "        # xml rpc result parsing",
            "        stubs.stubout_firewall_driver(self.stubs, self.conn)",
            "",
            "        instance = self._create_instance()",
            "",
            "        self.fake_upload_called = False",
            "",
            "        def fake_image_upload(_self, ctx, session, inst, vdi_uuids,",
            "                              img_id):",
            "            self.fake_upload_called = True",
            "            self.assertEqual(ctx, self.context)",
            "            self.assertEqual(inst, instance)",
            "            self.assertTrue(isinstance(vdi_uuids, list))",
            "            self.assertEqual(img_id, image_id)",
            "",
            "        self.stubs.Set(glance.GlanceStore, 'upload_image',",
            "                       fake_image_upload)",
            "",
            "        self.conn.snapshot(self.context, instance, image_id,",
            "                           func_call_matcher.call)",
            "",
            "        # Ensure VM was torn down",
            "        vm_labels = []",
            "        for vm_ref in xenapi_fake.get_all('VM'):",
            "            vm_rec = xenapi_fake.get_record('VM', vm_ref)",
            "            if not vm_rec[\"is_control_domain\"]:",
            "                vm_labels.append(vm_rec[\"name_label\"])",
            "",
            "        self.assertEquals(vm_labels, [instance['name']])",
            "",
            "        # Ensure VBDs were torn down",
            "        vbd_labels = []",
            "        for vbd_ref in xenapi_fake.get_all('VBD'):",
            "            vbd_rec = xenapi_fake.get_record('VBD', vbd_ref)",
            "            vbd_labels.append(vbd_rec[\"vm_name_label\"])",
            "",
            "        self.assertEquals(vbd_labels, [instance['name']])",
            "",
            "        # Ensure task states changed in correct order",
            "        self.assertIsNone(func_call_matcher.match())",
            "",
            "        # Ensure VDIs were torn down",
            "        for vdi_ref in xenapi_fake.get_all('VDI'):",
            "            vdi_rec = xenapi_fake.get_record('VDI', vdi_ref)",
            "            name_label = vdi_rec[\"name_label\"]",
            "            self.assert_(not name_label.endswith('snapshot'))",
            "",
            "        self.assertTrue(self.fake_upload_called)",
            "",
            "    def create_vm_record(self, conn, os_type, name):",
            "        instances = conn.list_instances()",
            "        self.assertEquals(instances, [name])",
            "",
            "        # Get Nova record for VM",
            "        vm_info = conn.get_info({'name': name})",
            "        # Get XenAPI record for VM",
            "        vms = [rec for ref, rec",
            "               in xenapi_fake.get_all_records('VM').iteritems()",
            "               if not rec['is_control_domain']]",
            "        vm = vms[0]",
            "        self.vm_info = vm_info",
            "        self.vm = vm",
            "",
            "    def check_vm_record(self, conn, instance_type_id, check_injection):",
            "        instance_type = db.flavor_get(conn, instance_type_id)",
            "        mem_kib = long(instance_type['memory_mb']) << 10",
            "        mem_bytes = str(mem_kib << 10)",
            "        vcpus = instance_type['vcpus']",
            "        vcpu_weight = instance_type['vcpu_weight']",
            "",
            "        self.assertEquals(self.vm_info['max_mem'], mem_kib)",
            "        self.assertEquals(self.vm_info['mem'], mem_kib)",
            "        self.assertEquals(self.vm['memory_static_max'], mem_bytes)",
            "        self.assertEquals(self.vm['memory_dynamic_max'], mem_bytes)",
            "        self.assertEquals(self.vm['memory_dynamic_min'], mem_bytes)",
            "        self.assertEquals(self.vm['VCPUs_max'], str(vcpus))",
            "        self.assertEquals(self.vm['VCPUs_at_startup'], str(vcpus))",
            "        if vcpu_weight == None:",
            "            self.assertEquals(self.vm['VCPUs_params'], {})",
            "        else:",
            "            self.assertEquals(self.vm['VCPUs_params'],",
            "                              {'weight': str(vcpu_weight)})",
            "",
            "        # Check that the VM is running according to Nova",
            "        self.assertEquals(self.vm_info['state'], power_state.RUNNING)",
            "",
            "        # Check that the VM is running according to XenAPI.",
            "        self.assertEquals(self.vm['power_state'], 'Running')",
            "",
            "        if check_injection:",
            "            xenstore_data = self.vm['xenstore_data']",
            "            self.assertFalse('vm-data/hostname' in xenstore_data)",
            "            key = 'vm-data/networking/DEADBEEF0001'",
            "            xenstore_value = xenstore_data[key]",
            "            tcpip_data = ast.literal_eval(xenstore_value)",
            "            self.assertEquals(tcpip_data,",
            "                              {'broadcast': '192.168.1.255',",
            "                               'dns': ['192.168.1.4', '192.168.1.3'],",
            "                               'gateway': '192.168.1.1',",
            "                               'gateway_v6': '2001:db8:0:1::1',",
            "                               'ip6s': [{'enabled': '1',",
            "                                         'ip': '2001:db8:0:1::1',",
            "                                         'netmask': 64,",
            "                                         'gateway': '2001:db8:0:1::1'}],",
            "                               'ips': [{'enabled': '1',",
            "                                        'ip': '192.168.1.100',",
            "                                        'netmask': '255.255.255.0',",
            "                                        'gateway': '192.168.1.1'},",
            "                                       {'enabled': '1',",
            "                                        'ip': '192.168.1.101',",
            "                                        'netmask': '255.255.255.0',",
            "                                        'gateway': '192.168.1.1'}],",
            "                               'label': 'test1',",
            "                               'mac': 'DE:AD:BE:EF:00:01'})",
            "",
            "    def check_vm_params_for_windows(self):",
            "        self.assertEquals(self.vm['platform']['nx'], 'true')",
            "        self.assertEquals(self.vm['HVM_boot_params'], {'order': 'dc'})",
            "        self.assertEquals(self.vm['HVM_boot_policy'], 'BIOS order')",
            "",
            "        # check that these are not set",
            "        self.assertEquals(self.vm['PV_args'], '')",
            "        self.assertEquals(self.vm['PV_bootloader'], '')",
            "        self.assertEquals(self.vm['PV_kernel'], '')",
            "        self.assertEquals(self.vm['PV_ramdisk'], '')",
            "",
            "    def check_vm_params_for_linux(self):",
            "        self.assertEquals(self.vm['platform']['nx'], 'false')",
            "        self.assertEquals(self.vm['PV_args'], '')",
            "        self.assertEquals(self.vm['PV_bootloader'], 'pygrub')",
            "",
            "        # check that these are not set",
            "        self.assertEquals(self.vm['PV_kernel'], '')",
            "        self.assertEquals(self.vm['PV_ramdisk'], '')",
            "        self.assertEquals(self.vm['HVM_boot_params'], {})",
            "        self.assertEquals(self.vm['HVM_boot_policy'], '')",
            "",
            "    def check_vm_params_for_linux_with_external_kernel(self):",
            "        self.assertEquals(self.vm['platform']['nx'], 'false')",
            "        self.assertEquals(self.vm['PV_args'], 'root=/dev/xvda1')",
            "        self.assertNotEquals(self.vm['PV_kernel'], '')",
            "        self.assertNotEquals(self.vm['PV_ramdisk'], '')",
            "",
            "        # check that these are not set",
            "        self.assertEquals(self.vm['HVM_boot_params'], {})",
            "        self.assertEquals(self.vm['HVM_boot_policy'], '')",
            "",
            "    def _list_vdis(self):",
            "        url = CONF.xenapi_connection_url",
            "        username = CONF.xenapi_connection_username",
            "        password = CONF.xenapi_connection_password",
            "        session = xenapi_conn.XenAPISession(url, username, password,",
            "                                            fake.FakeVirtAPI())",
            "        return session.call_xenapi('VDI.get_all')",
            "",
            "    def _list_vms(self):",
            "        url = CONF.xenapi_connection_url",
            "        username = CONF.xenapi_connection_username",
            "        password = CONF.xenapi_connection_password",
            "        session = xenapi_conn.XenAPISession(url, username, password,",
            "                                            fake.FakeVirtAPI())",
            "        return session.call_xenapi('VM.get_all')",
            "",
            "    def _check_vdis(self, start_list, end_list):",
            "        for vdi_ref in end_list:",
            "            if vdi_ref not in start_list:",
            "                vdi_rec = xenapi_fake.get_record('VDI', vdi_ref)",
            "                # If the cache is turned on then the base disk will be",
            "                # there even after the cleanup",
            "                if 'other_config' in vdi_rec:",
            "                    if 'image-id' not in vdi_rec['other_config']:",
            "                        self.fail('Found unexpected VDI:%s' % vdi_ref)",
            "                else:",
            "                    self.fail('Found unexpected VDI:%s' % vdi_ref)",
            "",
            "    def _test_spawn(self, image_ref, kernel_id, ramdisk_id,",
            "                    instance_type_id=\"3\", os_type=\"linux\",",
            "                    hostname=\"test\", architecture=\"x86-64\", instance_id=1,",
            "                    injected_files=None, check_injection=False,",
            "                    create_record=True, empty_dns=False,",
            "                    block_device_info=None,",
            "                    key_data=None):",
            "        if injected_files is None:",
            "            injected_files = []",
            "",
            "        # Fake out inject_instance_metadata",
            "        def fake_inject_instance_metadata(self, instance, vm):",
            "            pass",
            "        self.stubs.Set(vmops.VMOps, '_inject_instance_metadata',",
            "                       fake_inject_instance_metadata)",
            "",
            "        if create_record:",
            "            instance_values = {'id': instance_id,",
            "                               'project_id': self.project_id,",
            "                               'user_id': self.user_id,",
            "                               'image_ref': image_ref,",
            "                               'kernel_id': kernel_id,",
            "                               'ramdisk_id': ramdisk_id,",
            "                               'root_gb': 20,",
            "                               'instance_type_id': instance_type_id,",
            "                               'os_type': os_type,",
            "                               'hostname': hostname,",
            "                               'key_data': key_data,",
            "                               'architecture': architecture}",
            "            instance = create_instance_with_system_metadata(self.context,",
            "                                                            instance_values)",
            "        else:",
            "            instance = db.instance_get(self.context, instance_id)",
            "",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        if empty_dns:",
            "            # NOTE(tr3buchet): this is a terrible way to do this...",
            "            network_info[0]['network']['subnets'][0]['dns'] = []",
            "",
            "        image_meta = {}",
            "        if image_ref:",
            "            image_meta = IMAGE_FIXTURES[image_ref][\"image_meta\"]",
            "        self.conn.spawn(self.context, instance, image_meta, injected_files,",
            "                        'herp', network_info, block_device_info)",
            "        self.create_vm_record(self.conn, os_type, instance['name'])",
            "        self.check_vm_record(self.conn, instance_type_id, check_injection)",
            "        self.assertTrue(instance['os_type'])",
            "        self.assertTrue(instance['architecture'])",
            "",
            "    def test_spawn_ipxe_iso_success(self):",
            "        self.mox.StubOutWithMock(vm_utils, 'get_sr_path')",
            "        vm_utils.get_sr_path(mox.IgnoreArg()).AndReturn('/sr/path')",
            "",
            "        self.flags(xenapi_ipxe_network_name='test1',",
            "                   xenapi_ipxe_boot_menu_url='http://boot.example.com',",
            "                   xenapi_ipxe_mkisofs_cmd='/root/mkisofs')",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_plugin_serialized')",
            "        self.conn._session.call_plugin_serialized(",
            "            'ipxe', 'inject', '/sr/path', mox.IgnoreArg(),",
            "            'http://boot.example.com', '192.168.1.100', '255.255.255.0',",
            "            '192.168.1.1', '192.168.1.3', '/root/mkisofs')",
            "",
            "        self.mox.ReplayAll()",
            "        self._test_spawn(IMAGE_IPXE_ISO, None, None)",
            "",
            "    def test_spawn_ipxe_iso_no_network_name(self):",
            "        self.flags(xenapi_ipxe_network_name=None,",
            "                   xenapi_ipxe_boot_menu_url='http://boot.example.com')",
            "",
            "        # call_plugin_serialized shouldn't be called",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_plugin_serialized')",
            "",
            "        self.mox.ReplayAll()",
            "        self._test_spawn(IMAGE_IPXE_ISO, None, None)",
            "",
            "    def test_spawn_ipxe_iso_no_boot_menu_url(self):",
            "        self.flags(xenapi_ipxe_network_name='test1',",
            "                   xenapi_ipxe_boot_menu_url=None)",
            "",
            "        # call_plugin_serialized shouldn't be called",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_plugin_serialized')",
            "",
            "        self.mox.ReplayAll()",
            "        self._test_spawn(IMAGE_IPXE_ISO, None, None)",
            "",
            "    def test_spawn_ipxe_iso_unknown_network_name(self):",
            "        self.flags(xenapi_ipxe_network_name='test2',",
            "                   xenapi_ipxe_boot_menu_url='http://boot.example.com')",
            "",
            "        # call_plugin_serialized shouldn't be called",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_plugin_serialized')",
            "",
            "        self.mox.ReplayAll()",
            "        self._test_spawn(IMAGE_IPXE_ISO, None, None)",
            "",
            "    def test_spawn_empty_dns(self):",
            "        # Test spawning with an empty dns list.",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\",",
            "                         empty_dns=True)",
            "        self.check_vm_params_for_linux()",
            "",
            "    def test_spawn_not_enough_memory(self):",
            "        self.assertRaises(exception.InsufficientFreeMemory,",
            "                          self._test_spawn,",
            "                          '1', 2, 3, \"4\")  # m1.xlarge",
            "",
            "    def test_spawn_fail_cleanup_1(self):",
            "        \"\"\"Simulates an error while downloading an image.",
            "",
            "        Verifies that the VM and VDIs created are properly cleaned up.",
            "        \"\"\"",
            "        vdi_recs_start = self._list_vdis()",
            "        start_vms = self._list_vms()",
            "        stubs.stubout_fetch_disk_image(self.stubs, raise_failure=True)",
            "        self.assertRaises(xenapi_fake.Failure,",
            "                          self._test_spawn, '1', 2, 3)",
            "        # No additional VDI should be found.",
            "        vdi_recs_end = self._list_vdis()",
            "        end_vms = self._list_vms()",
            "        self._check_vdis(vdi_recs_start, vdi_recs_end)",
            "        # No additional VMs should be found.",
            "        self.assertEqual(start_vms, end_vms)",
            "",
            "    def test_spawn_fail_cleanup_2(self):",
            "        \"\"\"Simulates an error while creating VM record.",
            "",
            "        Verifies that the VM and VDIs created are properly cleaned up.",
            "        \"\"\"",
            "        vdi_recs_start = self._list_vdis()",
            "        start_vms = self._list_vms()",
            "        stubs.stubout_create_vm(self.stubs)",
            "        self.assertRaises(xenapi_fake.Failure,",
            "                          self._test_spawn, '1', 2, 3)",
            "        # No additional VDI should be found.",
            "        vdi_recs_end = self._list_vdis()",
            "        end_vms = self._list_vms()",
            "        self._check_vdis(vdi_recs_start, vdi_recs_end)",
            "        # No additional VMs should be found.",
            "        self.assertEqual(start_vms, end_vms)",
            "",
            "    def test_spawn_fail_cleanup_3(self):",
            "        \"\"\"Simulates an error while attaching disks.",
            "",
            "        Verifies that the VM and VDIs created are properly cleaned up.",
            "        \"\"\"",
            "        stubs.stubout_attach_disks(self.stubs)",
            "        vdi_recs_start = self._list_vdis()",
            "        start_vms = self._list_vms()",
            "        self.assertRaises(xenapi_fake.Failure,",
            "                          self._test_spawn, '1', 2, 3)",
            "        # No additional VDI should be found.",
            "        vdi_recs_end = self._list_vdis()",
            "        end_vms = self._list_vms()",
            "        self._check_vdis(vdi_recs_start, vdi_recs_end)",
            "        # No additional VMs should be found.",
            "        self.assertEqual(start_vms, end_vms)",
            "",
            "    def test_spawn_raw_glance(self):",
            "        self._test_spawn(IMAGE_RAW, None, None)",
            "        self.check_vm_params_for_windows()",
            "",
            "    def test_spawn_vhd_glance_linux(self):",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\")",
            "        self.check_vm_params_for_linux()",
            "",
            "    def test_spawn_vhd_glance_windows(self):",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"windows\", architecture=\"i386\",",
            "                         instance_type_id=5)",
            "        self.check_vm_params_for_windows()",
            "",
            "    def test_spawn_iso_glance(self):",
            "        self._test_spawn(IMAGE_ISO, None, None,",
            "                         os_type=\"windows\", architecture=\"i386\")",
            "        self.check_vm_params_for_windows()",
            "",
            "    def test_spawn_glance(self):",
            "",
            "        def fake_fetch_disk_image(context, session, instance, name_label,",
            "                                  image_id, image_type):",
            "            sr_ref = vm_utils.safe_find_sr(session)",
            "            image_type_str = vm_utils.ImageType.to_string(image_type)",
            "            vdi_ref = vm_utils.create_vdi(session, sr_ref, instance,",
            "                name_label, image_type_str, \"20\")",
            "            vdi_role = vm_utils.ImageType.get_role(image_type)",
            "            vdi_uuid = session.call_xenapi(\"VDI.get_uuid\", vdi_ref)",
            "            return {vdi_role: dict(uuid=vdi_uuid, file=None)}",
            "        self.stubs.Set(vm_utils, '_fetch_disk_image',",
            "                       fake_fetch_disk_image)",
            "",
            "        self._test_spawn(IMAGE_MACHINE,",
            "                         IMAGE_KERNEL,",
            "                         IMAGE_RAMDISK)",
            "        self.check_vm_params_for_linux_with_external_kernel()",
            "",
            "    def test_spawn_boot_from_volume_no_image_meta(self):",
            "        dev_info = get_fake_device_info()",
            "        self._test_spawn(None, None, None,",
            "                         block_device_info=dev_info)",
            "",
            "    def test_spawn_boot_from_volume_with_image_meta(self):",
            "        dev_info = get_fake_device_info()",
            "        self._test_spawn(None, None, None,",
            "                         block_device_info=dev_info)",
            "",
            "    def test_spawn_netinject_file(self):",
            "        self.flags(flat_injected=True)",
            "        db_fakes.stub_out_db_instance_api(self.stubs, injected=True)",
            "",
            "        self._tee_executed = False",
            "",
            "        def _tee_handler(cmd, **kwargs):",
            "            input = kwargs.get('process_input', None)",
            "            self.assertNotEqual(input, None)",
            "            config = [line.strip() for line in input.split(\"\\n\")]",
            "            # Find the start of eth0 configuration and check it",
            "            index = config.index('auto eth0')",
            "            self.assertEquals(config[index + 1:index + 8], [",
            "                'iface eth0 inet static',",
            "                'address 192.168.1.100',",
            "                'netmask 255.255.255.0',",
            "                'broadcast 192.168.1.255',",
            "                'gateway 192.168.1.1',",
            "                'dns-nameservers 192.168.1.3 192.168.1.4',",
            "                ''])",
            "            self._tee_executed = True",
            "            return '', ''",
            "",
            "        def _readlink_handler(cmd_parts, **kwargs):",
            "            return os.path.realpath(cmd_parts[2]), ''",
            "",
            "        fake_processutils.fake_execute_set_repliers([",
            "            # Capture the tee .../etc/network/interfaces command",
            "            (r'tee.*interfaces', _tee_handler),",
            "            (r'readlink -nm.*', _readlink_handler),",
            "        ])",
            "        self._test_spawn(IMAGE_MACHINE,",
            "                         IMAGE_KERNEL,",
            "                         IMAGE_RAMDISK,",
            "                         check_injection=True)",
            "        self.assertTrue(self._tee_executed)",
            "",
            "    def test_spawn_netinject_xenstore(self):",
            "        db_fakes.stub_out_db_instance_api(self.stubs, injected=True)",
            "",
            "        self._tee_executed = False",
            "",
            "        def _mount_handler(cmd, *ignore_args, **ignore_kwargs):",
            "            # When mounting, create real files under the mountpoint to simulate",
            "            # files in the mounted filesystem",
            "",
            "            # mount point will be the last item of the command list",
            "            self._tmpdir = cmd[len(cmd) - 1]",
            "            LOG.debug(_('Creating files in %s to simulate guest agent'),",
            "                      self._tmpdir)",
            "            os.makedirs(os.path.join(self._tmpdir, 'usr', 'sbin'))",
            "            # Touch the file using open",
            "            open(os.path.join(self._tmpdir, 'usr', 'sbin',",
            "                'xe-update-networking'), 'w').close()",
            "            return '', ''",
            "",
            "        def _umount_handler(cmd, *ignore_args, **ignore_kwargs):",
            "            # Umount would normall make files in the m,ounted filesystem",
            "            # disappear, so do that here",
            "            LOG.debug(_('Removing simulated guest agent files in %s'),",
            "                      self._tmpdir)",
            "            os.remove(os.path.join(self._tmpdir, 'usr', 'sbin',",
            "                'xe-update-networking'))",
            "            os.rmdir(os.path.join(self._tmpdir, 'usr', 'sbin'))",
            "            os.rmdir(os.path.join(self._tmpdir, 'usr'))",
            "            return '', ''",
            "",
            "        def _tee_handler(cmd, *ignore_args, **ignore_kwargs):",
            "            self._tee_executed = True",
            "            return '', ''",
            "",
            "        fake_processutils.fake_execute_set_repliers([",
            "            (r'mount', _mount_handler),",
            "            (r'umount', _umount_handler),",
            "            (r'tee.*interfaces', _tee_handler)])",
            "        self._test_spawn('1', 2, 3, check_injection=True)",
            "",
            "        # tee must not run in this case, where an injection-capable",
            "        # guest agent is detected",
            "        self.assertFalse(self._tee_executed)",
            "",
            "    def test_spawn_injects_auto_disk_config_to_xenstore(self):",
            "        instance = self._create_instance(spawn=False)",
            "        self.mox.StubOutWithMock(self.conn._vmops, '_inject_auto_disk_config')",
            "        self.conn._vmops._inject_auto_disk_config(instance, mox.IgnoreArg())",
            "        self.mox.ReplayAll()",
            "        self.conn.spawn(self.context, instance,",
            "                        IMAGE_FIXTURES['1'][\"image_meta\"], [], 'herp', '')",
            "",
            "    def test_spawn_vlanmanager(self):",
            "        self.flags(network_manager='nova.network.manager.VlanManager',",
            "                   vlan_interface='fake0')",
            "",
            "        def dummy(*args, **kwargs):",
            "            pass",
            "",
            "        self.stubs.Set(vmops.VMOps, '_create_vifs', dummy)",
            "        # Reset network table",
            "        xenapi_fake.reset_table('network')",
            "        # Instance id = 2 will use vlan network (see db/fakes.py)",
            "        ctxt = self.context.elevated()",
            "        instance = self._create_instance(2, False)",
            "        networks = self.network.db.network_get_all(ctxt)",
            "        for network in networks:",
            "            self.network.set_network_host(ctxt, network)",
            "",
            "        self.network.allocate_for_instance(ctxt,",
            "                          instance_id=2,",
            "                          instance_uuid='00000000-0000-0000-0000-000000000002',",
            "                          host=CONF.host,",
            "                          vpn=None,",
            "                          rxtx_factor=3,",
            "                          project_id=self.project_id,",
            "                          macs=None)",
            "        self._test_spawn(IMAGE_MACHINE,",
            "                         IMAGE_KERNEL,",
            "                         IMAGE_RAMDISK,",
            "                         instance_id=2,",
            "                         create_record=False)",
            "        # TODO(salvatore-orlando): a complete test here would require",
            "        # a check for making sure the bridge for the VM's VIF is",
            "        # consistent with bridge specified in nova db",
            "",
            "    def test_spawn_with_network_qos(self):",
            "        self._create_instance()",
            "        for vif_ref in xenapi_fake.get_all('VIF'):",
            "            vif_rec = xenapi_fake.get_record('VIF', vif_ref)",
            "            self.assertEquals(vif_rec['qos_algorithm_type'], 'ratelimit')",
            "            self.assertEquals(vif_rec['qos_algorithm_params']['kbps'],",
            "                              str(3 * 10 * 1024))",
            "",
            "    def test_spawn_ssh_key_injection(self):",
            "        # Test spawning with key_data on an instance.  Should use",
            "        # agent file injection.",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_inject_file(self, method, args):",
            "            path = base64.b64decode(args['b64_path'])",
            "            contents = base64.b64decode(args['b64_contents'])",
            "            actual_injected_files.append((path, contents))",
            "            return jsonutils.dumps({'returncode': '0', 'message': 'success'})",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_inject_file', fake_inject_file)",
            "",
            "        def fake_encrypt_text(sshkey, new_pass):",
            "            self.assertEqual(\"ssh-rsa fake_keydata\", sshkey)",
            "            return \"fake\"",
            "",
            "        self.stubs.Set(crypto, 'ssh_encrypt_text', fake_encrypt_text)",
            "",
            "        expected_data = ('\\n# The following ssh key was injected by '",
            "                         'Nova\\nssh-rsa fake_keydata\\n')",
            "",
            "        injected_files = [('/root/.ssh/authorized_keys', expected_data)]",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\",",
            "                         key_data='ssh-rsa fake_keydata')",
            "        self.assertEquals(actual_injected_files, injected_files)",
            "",
            "    def test_spawn_ssh_key_injection_non_rsa(self):",
            "        # Test spawning with key_data on an instance.  Should use",
            "        # agent file injection.",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_inject_file(self, method, args):",
            "            path = base64.b64decode(args['b64_path'])",
            "            contents = base64.b64decode(args['b64_contents'])",
            "            actual_injected_files.append((path, contents))",
            "            return jsonutils.dumps({'returncode': '0', 'message': 'success'})",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_inject_file', fake_inject_file)",
            "",
            "        def fake_encrypt_text(sshkey, new_pass):",
            "            raise NotImplementedError(\"Should not be called\")",
            "",
            "        self.stubs.Set(crypto, 'ssh_encrypt_text', fake_encrypt_text)",
            "",
            "        expected_data = ('\\n# The following ssh key was injected by '",
            "                         'Nova\\nssh-dsa fake_keydata\\n')",
            "",
            "        injected_files = [('/root/.ssh/authorized_keys', expected_data)]",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\",",
            "                         key_data='ssh-dsa fake_keydata')",
            "        self.assertEquals(actual_injected_files, injected_files)",
            "",
            "    def test_spawn_injected_files(self):",
            "        # Test spawning with injected_files.",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_inject_file(self, method, args):",
            "            path = base64.b64decode(args['b64_path'])",
            "            contents = base64.b64decode(args['b64_contents'])",
            "            actual_injected_files.append((path, contents))",
            "            return jsonutils.dumps({'returncode': '0', 'message': 'success'})",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_inject_file', fake_inject_file)",
            "",
            "        injected_files = [('/tmp/foo', 'foobar')]",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\",",
            "                         injected_files=injected_files)",
            "        self.check_vm_params_for_linux()",
            "        self.assertEquals(actual_injected_files, injected_files)",
            "",
            "    def test_spawn_agent_upgrade(self):",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_build(_self, *args):",
            "            return {\"version\": \"1.1.0\", \"architecture\": \"x86-64\",",
            "                    \"hypervisor\": \"xen\", \"os\": \"windows\",",
            "                    \"url\": \"url\", \"md5hash\": \"asdf\"}",
            "",
            "        self.stubs.Set(self.conn.virtapi, 'agent_build_get_by_triple',",
            "                       fake_agent_build)",
            "",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def test_spawn_agent_upgrade_fails_silently(self):",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_build(_self, *args):",
            "            return {\"version\": \"1.1.0\", \"architecture\": \"x86-64\",",
            "                    \"hypervisor\": \"xen\", \"os\": \"windows\",",
            "                    \"url\": \"url\", \"md5hash\": \"asdf\"}",
            "",
            "        self.stubs.Set(self.conn.virtapi, 'agent_build_get_by_triple',",
            "                       fake_agent_build)",
            "",
            "        def fake_agent_update(self, method, args):",
            "            raise xenapi_fake.Failure([\"fake_error\"])",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_agentupdate', fake_agent_update)",
            "",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def _test_spawn_fails_with(self, trigger, expected_exception):",
            "        self.flags(xenapi_use_agent_default=True)",
            "        self.flags(agent_version_timeout=0)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_version(self, method, args):",
            "            raise xenapi_fake.Failure([trigger])",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_version', fake_agent_version)",
            "",
            "        self.assertRaises(expected_exception, self._test_spawn,",
            "                IMAGE_VHD, None, None, os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def test_spawn_fails_with_agent_timeout(self):",
            "        self._test_spawn_fails_with(\"TIMEOUT:fake\", exception.AgentTimeout)",
            "",
            "    def test_spawn_fails_with_agent_not_implemented(self):",
            "        self._test_spawn_fails_with(\"NOT IMPLEMENTED:fake\",",
            "                                    exception.AgentNotImplemented)",
            "",
            "    def test_spawn_fails_with_agent_error(self):",
            "        self._test_spawn_fails_with(\"fake_error\", exception.AgentError)",
            "",
            "    def test_spawn_fails_with_agent_bad_return(self):",
            "        self.flags(xenapi_use_agent_default=True)",
            "        self.flags(agent_version_timeout=0)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_version(self, method, args):",
            "            return xenapi_fake.as_json(returncode='-1', message='fake')",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_version', fake_agent_version)",
            "",
            "        self.assertRaises(exception.AgentError, self._test_spawn,",
            "                IMAGE_VHD, None, None, os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def test_spawn_fails_agent_not_implemented(self):",
            "        # Test spawning with injected_files.",
            "        self.flags(xenapi_use_agent_default=True)",
            "        self.flags(agent_version_timeout=0)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_version(self, method, args):",
            "            raise xenapi_fake.Failure([\"NOT IMPLEMENTED:fake\"])",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_version', fake_agent_version)",
            "",
            "        self.assertRaises(exception.AgentNotImplemented, self._test_spawn,",
            "                IMAGE_VHD, None, None, os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def test_rescue(self):",
            "        instance = self._create_instance()",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        vm_ref = vm_utils.lookup(session, instance['name'])",
            "",
            "        swap_vdi_ref = xenapi_fake.create_vdi('swap', None)",
            "        root_vdi_ref = xenapi_fake.create_vdi('root', None)",
            "",
            "        xenapi_fake.create_vbd(vm_ref, swap_vdi_ref, userdevice=1)",
            "        xenapi_fake.create_vbd(vm_ref, root_vdi_ref, userdevice=0)",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        image_meta = {'id': IMAGE_VHD,",
            "                      'disk_format': 'vhd'}",
            "        conn.rescue(self.context, instance, [], image_meta, '')",
            "",
            "        vm = xenapi_fake.get_record('VM', vm_ref)",
            "        rescue_name = \"%s-rescue\" % vm[\"name_label\"]",
            "        rescue_ref = vm_utils.lookup(session, rescue_name)",
            "        rescue_vm = xenapi_fake.get_record('VM', rescue_ref)",
            "",
            "        vdi_uuids = []",
            "        for vbd_uuid in rescue_vm[\"VBDs\"]:",
            "            vdi_uuids.append(xenapi_fake.get_record('VBD', vbd_uuid)[\"VDI\"])",
            "        self.assertTrue(\"swap\" not in vdi_uuids)",
            "",
            "    def test_rescue_preserve_disk_on_failure(self):",
            "        # test that the original disk is preserved if rescue setup fails",
            "        # bug #1227898",
            "        instance = self._create_instance()",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        image_meta = {'id': IMAGE_VHD,",
            "                      'disk_format': 'vhd'}",
            "",
            "        vm_ref = vm_utils.lookup(session, instance['name'])",
            "        vdi_ref, vdi_rec = vm_utils.get_vdi_for_vm_safely(session, vm_ref)",
            "",
            "        # raise an error in the spawn setup process and trigger the",
            "        # undo manager logic:",
            "        def fake_start(*args, **kwargs):",
            "            raise test.TestingException('Start Error')",
            "",
            "        self.stubs.Set(self.conn._vmops, '_start', fake_start)",
            "",
            "        self.assertRaises(test.TestingException, self.conn.rescue,",
            "                          self.context, instance, [], image_meta, '')",
            "",
            "        # confirm original disk still exists:",
            "        vdi_ref2, vdi_rec2 = vm_utils.get_vdi_for_vm_safely(session, vm_ref)",
            "        self.assertEqual(vdi_ref, vdi_ref2)",
            "        self.assertEqual(vdi_rec['uuid'], vdi_rec2['uuid'])",
            "",
            "    def test_unrescue(self):",
            "        instance = self._create_instance()",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        # Unrescue expects the original instance to be powered off",
            "        conn.power_off(instance)",
            "        rescue_vm = xenapi_fake.create_vm(instance['name'] + '-rescue',",
            "                'Running')",
            "        conn.unrescue(instance, None)",
            "",
            "    def test_unrescue_not_in_rescue(self):",
            "        instance = self._create_instance()",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        # Ensure that it will not unrescue a non-rescued instance.",
            "        self.assertRaises(exception.InstanceNotInRescueMode, conn.unrescue,",
            "                          instance, None)",
            "",
            "    def test_finish_revert_migration(self):",
            "        instance = self._create_instance()",
            "",
            "        class VMOpsMock():",
            "",
            "            def __init__(self):",
            "                self.finish_revert_migration_called = False",
            "",
            "            def finish_revert_migration(self, instance, block_info,",
            "                                        power_on):",
            "                self.finish_revert_migration_called = True",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn._vmops = VMOpsMock()",
            "        conn.finish_revert_migration(instance, None)",
            "        self.assertTrue(conn._vmops.finish_revert_migration_called)",
            "",
            "    def test_reboot_hard(self):",
            "        instance = self._create_instance()",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.reboot(self.context, instance, None, \"HARD\")",
            "",
            "    def test_poll_rebooting_instances(self):",
            "        self.mox.StubOutWithMock(compute_api.API, 'reboot')",
            "        compute_api.API.reboot(mox.IgnoreArg(), mox.IgnoreArg(),",
            "                               mox.IgnoreArg())",
            "        self.mox.ReplayAll()",
            "        instance = self._create_instance()",
            "        instances = [instance]",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.poll_rebooting_instances(60, instances)",
            "",
            "    def test_reboot_soft(self):",
            "        instance = self._create_instance()",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.reboot(self.context, instance, None, \"SOFT\")",
            "",
            "    def test_reboot_halted(self):",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        instance = self._create_instance(spawn=False)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        xenapi_fake.create_vm(instance['name'], 'Halted')",
            "        conn.reboot(self.context, instance, None, \"SOFT\")",
            "        vm_ref = vm_utils.lookup(session, instance['name'])",
            "        vm = xenapi_fake.get_record('VM', vm_ref)",
            "        self.assertEquals(vm['power_state'], 'Running')",
            "",
            "    def test_reboot_unknown_state(self):",
            "        instance = self._create_instance(spawn=False)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        xenapi_fake.create_vm(instance['name'], 'Unknown')",
            "        self.assertRaises(xenapi_fake.Failure, conn.reboot, self.context,",
            "                          instance, None, \"SOFT\")",
            "",
            "    def test_reboot_rescued(self):",
            "        instance = self._create_instance()",
            "        instance['vm_state'] = vm_states.RESCUED",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        real_result = vm_utils.lookup(conn._session, instance['name'])",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        vm_utils.lookup(conn._session, instance['name'],",
            "                        True).AndReturn(real_result)",
            "        self.mox.ReplayAll()",
            "",
            "        conn.reboot(self.context, instance, None, \"SOFT\")",
            "",
            "    def test_get_console_output_succeeds(self):",
            "",
            "        def fake_get_console_output(instance):",
            "            self.assertEqual(\"instance\", instance)",
            "            return \"console_log\"",
            "        self.stubs.Set(self.conn._vmops, 'get_console_output',",
            "                       fake_get_console_output)",
            "",
            "        self.assertEqual(self.conn.get_console_output(\"instance\"),",
            "                         \"console_log\")",
            "",
            "    def _test_maintenance_mode(self, find_host, find_aggregate):",
            "        real_call_xenapi = self.conn._session.call_xenapi",
            "        instance = self._create_instance(spawn=True)",
            "        api_calls = {}",
            "",
            "        # Record all the xenapi calls, and return a fake list of hosts",
            "        # for the host.get_all call",
            "        def fake_call_xenapi(method, *args):",
            "            api_calls[method] = args",
            "            if method == 'host.get_all':",
            "                return ['foo', 'bar', 'baz']",
            "            return real_call_xenapi(method, *args)",
            "        self.stubs.Set(self.conn._session, 'call_xenapi', fake_call_xenapi)",
            "",
            "        def fake_aggregate_get(context, host, key):",
            "            if find_aggregate:",
            "                return [{'fake': 'aggregate'}]",
            "            else:",
            "                return []",
            "        self.stubs.Set(self.conn.virtapi, 'aggregate_get_by_host',",
            "                       fake_aggregate_get)",
            "",
            "        def fake_host_find(context, session, src, dst):",
            "            if find_host:",
            "                return 'bar'",
            "            else:",
            "                raise exception.NoValidHost(\"I saw this one coming...\")",
            "        self.stubs.Set(host, '_host_find', fake_host_find)",
            "",
            "        result = self.conn.host_maintenance_mode('bar', 'on_maintenance')",
            "        self.assertEqual(result, 'on_maintenance')",
            "",
            "        # We expect the VM.pool_migrate call to have been called to",
            "        # migrate our instance to the 'bar' host",
            "        vm_ref = vm_utils.lookup(self.conn._session, instance['name'])",
            "        host_ref = \"foo\"",
            "        expected = (vm_ref, host_ref, {\"live\": \"true\"})",
            "        self.assertEqual(api_calls.get('VM.pool_migrate'), expected)",
            "",
            "        instance = db.instance_get_by_uuid(self.context, instance['uuid'])",
            "        self.assertEqual(instance['vm_state'], vm_states.ACTIVE)",
            "        self.assertEqual(instance['task_state'], task_states.MIGRATING)",
            "",
            "    def test_maintenance_mode(self):",
            "        self._test_maintenance_mode(True, True)",
            "",
            "    def test_maintenance_mode_no_host(self):",
            "        self.assertRaises(exception.NoValidHost,",
            "                          self._test_maintenance_mode, False, True)",
            "",
            "    def test_maintenance_mode_no_aggregate(self):",
            "        self.assertRaises(exception.NotFound,",
            "                          self._test_maintenance_mode, True, False)",
            "",
            "    def test_uuid_find(self):",
            "        self.mox.StubOutWithMock(db, 'instance_get_all_by_host')",
            "        fake_inst = fake_instance.fake_db_instance(id=123)",
            "        fake_inst2 = fake_instance.fake_db_instance(id=456)",
            "        db.instance_get_all_by_host(self.context, fake_inst['host'],",
            "                                    columns_to_join=None",
            "                                    ).AndReturn([fake_inst, fake_inst2])",
            "        self.mox.ReplayAll()",
            "        expected_name = CONF.instance_name_template % fake_inst['id']",
            "        inst_uuid = host._uuid_find(self.context, fake_inst['host'],",
            "                                    expected_name)",
            "        self.assertEqual(inst_uuid, fake_inst['uuid'])",
            "",
            "    def test_session_virtapi(self):",
            "        was = {'called': False}",
            "",
            "        def fake_aggregate_get_by_host(self, *args, **kwargs):",
            "            was['called'] = True",
            "            raise test.TestingException()",
            "        self.stubs.Set(self.conn._session._virtapi, \"aggregate_get_by_host\",",
            "                       fake_aggregate_get_by_host)",
            "",
            "        self.stubs.Set(self.conn._session, \"is_slave\", True)",
            "",
            "        self.assertRaises(test.TestingException,",
            "                self.conn._session._get_host_uuid)",
            "        self.assertTrue(was['called'])",
            "",
            "    def test_per_instance_usage_running(self):",
            "        instance = self._create_instance(spawn=True)",
            "        instance_type = flavors.get_flavor(3)",
            "",
            "        expected = {instance['uuid']: {'memory_mb': instance_type['memory_mb'],",
            "                                       'uuid': instance['uuid']}}",
            "        actual = self.conn.get_per_instance_usage()",
            "        self.assertEqual(expected, actual)",
            "",
            "        # Paused instances still consume resources:",
            "        self.conn.pause(instance)",
            "        actual = self.conn.get_per_instance_usage()",
            "        self.assertEqual(expected, actual)",
            "",
            "    def test_per_instance_usage_suspended(self):",
            "        # Suspended instances do not consume memory:",
            "        instance = self._create_instance(spawn=True)",
            "        self.conn.suspend(instance)",
            "        actual = self.conn.get_per_instance_usage()",
            "        self.assertEqual({}, actual)",
            "",
            "    def test_per_instance_usage_halted(self):",
            "        instance = self._create_instance(spawn=True)",
            "        self.conn.power_off(instance)",
            "        actual = self.conn.get_per_instance_usage()",
            "        self.assertEqual({}, actual)",
            "",
            "    def _create_instance(self, instance_id=1, spawn=True):",
            "        \"\"\"Creates and spawns a test instance.\"\"\"",
            "        instance_values = {",
            "            'id': instance_id,",
            "            'uuid': '00000000-0000-0000-0000-00000000000%d' % instance_id,",
            "            'display_name': 'host-%d' % instance_id,",
            "            'project_id': self.project_id,",
            "            'user_id': self.user_id,",
            "            'image_ref': 1,",
            "            'kernel_id': 2,",
            "            'ramdisk_id': 3,",
            "            'root_gb': 20,",
            "            'instance_type_id': '3',  # m1.large",
            "            'os_type': 'linux',",
            "            'vm_mode': 'hvm',",
            "            'architecture': 'x86-64'}",
            "",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        instance_values)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        image_meta = {'id': IMAGE_VHD,",
            "                      'disk_format': 'vhd'}",
            "        if spawn:",
            "            self.conn.spawn(self.context, instance, image_meta, [], 'herp',",
            "                            network_info)",
            "        return instance",
            "",
            "    def test_destroy_clean_up_kernel_and_ramdisk(self):",
            "        def fake_lookup_kernel_ramdisk(session, vm_ref):",
            "            return \"kernel\", \"ramdisk\"",
            "",
            "        self.stubs.Set(vm_utils, \"lookup_kernel_ramdisk\",",
            "                       fake_lookup_kernel_ramdisk)",
            "",
            "        def fake_destroy_kernel_ramdisk(session, instance, kernel, ramdisk):",
            "            fake_destroy_kernel_ramdisk.called = True",
            "            self.assertEqual(\"kernel\", kernel)",
            "            self.assertEqual(\"ramdisk\", ramdisk)",
            "",
            "        fake_destroy_kernel_ramdisk.called = False",
            "",
            "        self.stubs.Set(vm_utils, \"destroy_kernel_ramdisk\",",
            "                       fake_destroy_kernel_ramdisk)",
            "",
            "        instance = self._create_instance(spawn=True)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        self.conn.destroy(instance, network_info)",
            "",
            "        vm_ref = vm_utils.lookup(self.conn._session, instance['name'])",
            "        self.assertTrue(vm_ref is None)",
            "        self.assertTrue(fake_destroy_kernel_ramdisk.called)",
            "",
            "",
            "class XenAPIDiffieHellmanTestCase(test.NoDBTestCase):",
            "    \"\"\"Unit tests for Diffie-Hellman code.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIDiffieHellmanTestCase, self).setUp()",
            "        self.alice = agent.SimpleDH()",
            "        self.bob = agent.SimpleDH()",
            "",
            "    def test_shared(self):",
            "        alice_pub = self.alice.get_public()",
            "        bob_pub = self.bob.get_public()",
            "        alice_shared = self.alice.compute_shared(bob_pub)",
            "        bob_shared = self.bob.compute_shared(alice_pub)",
            "        self.assertEquals(alice_shared, bob_shared)",
            "",
            "    def _test_encryption(self, message):",
            "        enc = self.alice.encrypt(message)",
            "        self.assertFalse(enc.endswith('\\n'))",
            "        dec = self.bob.decrypt(enc)",
            "        self.assertEquals(dec, message)",
            "",
            "    def test_encrypt_simple_message(self):",
            "        self._test_encryption('This is a simple message.')",
            "",
            "    def test_encrypt_message_with_newlines_at_end(self):",
            "        self._test_encryption('This message has a newline at the end.\\n')",
            "",
            "    def test_encrypt_many_newlines_at_end(self):",
            "        self._test_encryption('Message with lotsa newlines.\\n\\n\\n')",
            "",
            "    def test_encrypt_newlines_inside_message(self):",
            "        self._test_encryption('Message\\nwith\\ninterior\\nnewlines.')",
            "",
            "    def test_encrypt_with_leading_newlines(self):",
            "        self._test_encryption('\\n\\nMessage with leading newlines.')",
            "",
            "    def test_encrypt_really_long_message(self):",
            "        self._test_encryption(''.join(['abcd' for i in xrange(1024)]))",
            "",
            "",
            "class XenAPIMigrateInstance(stubs.XenAPITestBase):",
            "    \"\"\"Unit test for verifying migration-related actions.\"\"\"",
            "",
            "    def setUp(self):",
            "        super(XenAPIMigrateInstance, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        xenapi_fake.create_network('fake', 'fake_br1')",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "        self.instance_values = {'id': 1,",
            "                  'project_id': self.project_id,",
            "                  'user_id': self.user_id,",
            "                  'image_ref': 1,",
            "                  'kernel_id': None,",
            "                  'ramdisk_id': None,",
            "                  'root_gb': 5,",
            "                  'instance_type_id': '3',  # m1.large",
            "                  'os_type': 'linux',",
            "                  'architecture': 'x86-64'}",
            "",
            "        migration_values = {",
            "            'source_compute': 'nova-compute',",
            "            'dest_compute': 'nova-compute',",
            "            'dest_host': '10.127.5.114',",
            "            'status': 'post-migrating',",
            "            'instance_uuid': '15f23e6a-cc6e-4d22-b651-d9bdaac316f7',",
            "            'old_instance_type_id': 5,",
            "            'new_instance_type_id': 1",
            "        }",
            "        self.migration = db.migration_create(",
            "            context.get_admin_context(), migration_values)",
            "",
            "        fake_processutils.stub_out_processutils_execute(self.stubs)",
            "        stubs.stub_out_migration_methods(self.stubs)",
            "        stubs.stubout_get_this_vm_uuid(self.stubs)",
            "",
            "        def fake_inject_instance_metadata(self, instance, vm):",
            "            pass",
            "        self.stubs.Set(vmops.VMOps, '_inject_instance_metadata',",
            "                       fake_inject_instance_metadata)",
            "",
            "    def test_resize_xenserver_6(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        called = {'resize': False}",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            called['resize'] = True",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize\", fake_vdi_resize)",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests,",
            "                              product_version=(6, 0, 0),",
            "                              product_brand='XenServer')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vdi_ref = xenapi_fake.create_vdi('hurr', 'fake')",
            "        vdi_uuid = xenapi_fake.get_record('VDI', vdi_ref)['uuid']",
            "        conn._vmops._resize_up_root_vdi(instance,",
            "                                        {'uuid': vdi_uuid, 'ref': vdi_ref})",
            "        self.assertEqual(called['resize'], True)",
            "",
            "    def test_resize_xcp(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        called = {'resize': False}",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            called['resize'] = True",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize\", fake_vdi_resize)",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests,",
            "                              product_version=(1, 4, 99),",
            "                              product_brand='XCP')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vdi_ref = xenapi_fake.create_vdi('hurr', 'fake')",
            "        vdi_uuid = xenapi_fake.get_record('VDI', vdi_ref)['uuid']",
            "        conn._vmops._resize_up_root_vdi(instance,",
            "                                        {'uuid': vdi_uuid, 'ref': vdi_ref})",
            "        self.assertEqual(called['resize'], True)",
            "",
            "    def test_migrate_disk_and_power_off(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.large')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.migrate_disk_and_power_off(self.context, instance,",
            "                                        '127.0.0.1', instance_type, None)",
            "",
            "    def test_migrate_disk_and_power_off_passes_exceptions(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.large')",
            "",
            "        def fake_raise(*args, **kwargs):",
            "            raise exception.MigrationError(reason='test failure')",
            "        self.stubs.Set(vmops.VMOps, \"_migrate_vhd\", fake_raise)",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.MigrationError,",
            "                          conn.migrate_disk_and_power_off,",
            "                          self.context, instance,",
            "                          '127.0.0.1', instance_type, None)",
            "",
            "    def test_migrate_disk_and_power_off_throws_on_zero_gb_resize_down(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        instance_type = {\"root_gb\": 0}",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.ResizeError,",
            "                          conn.migrate_disk_and_power_off,",
            "                          self.context, instance,",
            "                          'fake_dest', instance_type, None)",
            "",
            "    def test_migrate_disk_and_power_off_with_zero_gb_old_and_new_works(self):",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.tiny')",
            "        instance_type[\"root_gb\"] = 0",
            "        values = copy.copy(self.instance_values)",
            "        values[\"root_gb\"] = 0",
            "        values[\"instance_type\"] = instance_type['id']",
            "        instance = db.instance_create(self.context, values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.migrate_disk_and_power_off(self.context, instance,",
            "                                        '127.0.0.1', instance_type, None)",
            "",
            "    def _test_revert_migrate(self, power_on):",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "        self.called = False",
            "        self.fake_vm_start_called = False",
            "        self.fake_finish_revert_migration_called = False",
            "",
            "        def fake_vm_start(*args, **kwargs):",
            "            self.fake_vm_start_called = True",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            self.called = True",
            "",
            "        def fake_finish_revert_migration(*args, **kwargs):",
            "            self.fake_finish_revert_migration_called = True",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize_online\", fake_vdi_resize)",
            "        self.stubs.Set(vmops.VMOps, '_start', fake_vm_start)",
            "        self.stubs.Set(vmops.VMOps, 'finish_revert_migration',",
            "                       fake_finish_revert_migration)",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests,",
            "                              product_version=(4, 0, 0),",
            "                              product_brand='XenServer')",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        image_meta = {'id': instance['image_ref'], 'disk_format': 'vhd'}",
            "        base = xenapi_fake.create_vdi('hurr', 'fake')",
            "        base_uuid = xenapi_fake.get_record('VDI', base)['uuid']",
            "        cow = xenapi_fake.create_vdi('durr', 'fake')",
            "        cow_uuid = xenapi_fake.get_record('VDI', cow)['uuid']",
            "        conn.finish_migration(self.context, self.migration, instance,",
            "                              dict(base_copy=base_uuid, cow=cow_uuid),",
            "                              network_info, image_meta, resize_instance=True,",
            "                              block_device_info=None, power_on=power_on)",
            "        self.assertEqual(self.called, True)",
            "        self.assertEqual(self.fake_vm_start_called, power_on)",
            "",
            "        conn.finish_revert_migration(instance, network_info)",
            "        self.assertEqual(self.fake_finish_revert_migration_called, True)",
            "",
            "    def test_revert_migrate_power_on(self):",
            "        self._test_revert_migrate(True)",
            "",
            "    def test_revert_migrate_power_off(self):",
            "        self._test_revert_migrate(False)",
            "",
            "    def _test_finish_migrate(self, power_on):",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "        self.called = False",
            "        self.fake_vm_start_called = False",
            "",
            "        def fake_vm_start(*args, **kwargs):",
            "            self.fake_vm_start_called = True",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            self.called = True",
            "",
            "        self.stubs.Set(vmops.VMOps, '_start', fake_vm_start)",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize_online\", fake_vdi_resize)",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests,",
            "                              product_version=(4, 0, 0),",
            "                              product_brand='XenServer')",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        image_meta = {'id': instance['image_ref'], 'disk_format': 'vhd'}",
            "        conn.finish_migration(self.context, self.migration, instance,",
            "                              dict(base_copy='hurr', cow='durr'),",
            "                              network_info, image_meta, resize_instance=True,",
            "                              block_device_info=None, power_on=power_on)",
            "        self.assertEqual(self.called, True)",
            "        self.assertEqual(self.fake_vm_start_called, power_on)",
            "",
            "    def test_finish_migrate_power_on(self):",
            "        self._test_finish_migrate(True)",
            "",
            "    def test_finish_migrate_power_off(self):",
            "        self._test_finish_migrate(False)",
            "",
            "    def test_finish_migrate_no_local_storage(self):",
            "        tiny_type = flavors.get_flavor_by_name('m1.tiny')",
            "        tiny_type_id = tiny_type['id']",
            "        self.instance_values.update({'instance_type_id': tiny_type_id,",
            "                                     'root_gb': 0})",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            raise Exception(\"This shouldn't be called\")",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize_online\", fake_vdi_resize)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        image_meta = {'id': instance['image_ref'], 'disk_format': 'vhd'}",
            "        conn.finish_migration(self.context, self.migration, instance,",
            "                              dict(base_copy='hurr', cow='durr'),",
            "                              network_info, image_meta, resize_instance=True)",
            "",
            "    def test_finish_migrate_no_resize_vdi(self):",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            raise Exception(\"This shouldn't be called\")",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize_online\", fake_vdi_resize)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        # Resize instance would be determined by the compute call",
            "        image_meta = {'id': instance['image_ref'], 'disk_format': 'vhd'}",
            "        conn.finish_migration(self.context, self.migration, instance,",
            "                              dict(base_copy='hurr', cow='durr'),",
            "                              network_info, image_meta, resize_instance=False)",
            "",
            "    @stub_vm_utils_with_vdi_attached_here",
            "    def test_migrate_too_many_partitions_no_resize_down(self):",
            "        instance_values = self.instance_values",
            "        instance_values['root_gb'] = 40",
            "        instance = db.instance_create(self.context, instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.small')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_partitions(partition):",
            "            return [(1, 2, 3, 4), (1, 2, 3, 4)]",
            "",
            "        self.stubs.Set(vm_utils, '_get_partitions', fake_get_partitions)",
            "",
            "        self.assertRaises(exception.InstanceFaultRollback,",
            "                          conn.migrate_disk_and_power_off,",
            "                          self.context, instance,",
            "                          '127.0.0.1', instance_type, None)",
            "",
            "    @stub_vm_utils_with_vdi_attached_here",
            "    def test_migrate_bad_fs_type_no_resize_down(self):",
            "        instance_values = self.instance_values",
            "        instance_values['root_gb'] = 40",
            "        instance = db.instance_create(self.context, instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.small')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_partitions(partition):",
            "            return [(1, 2, 3, \"ext2\")]",
            "",
            "        self.stubs.Set(vm_utils, '_get_partitions', fake_get_partitions)",
            "",
            "        self.assertRaises(exception.InstanceFaultRollback,",
            "                          conn.migrate_disk_and_power_off,",
            "                          self.context, instance,",
            "                          '127.0.0.1', instance_type, None)",
            "",
            "    def test_migrate_rollback_when_resize_down_fs_fails(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        virtapi = vmops._virtapi",
            "",
            "        self.mox.StubOutWithMock(vmops, '_resize_ensure_vm_is_shutdown')",
            "        self.mox.StubOutWithMock(vmops, '_apply_orig_vm_name_label')",
            "        self.mox.StubOutWithMock(vm_utils, 'resize_disk')",
            "        self.mox.StubOutWithMock(vmops, '_migrate_vhd')",
            "        self.mox.StubOutWithMock(vm_utils, 'destroy_vdi')",
            "        self.mox.StubOutWithMock(vm_utils, 'get_vdi_for_vm_safely')",
            "        self.mox.StubOutWithMock(vmops, '_restore_orig_vm_and_cleanup_orphan')",
            "        self.mox.StubOutWithMock(virtapi, 'instance_update')",
            "",
            "        instance = {'auto_disk_config': True, 'uuid': 'uuid'}",
            "        vm_ref = \"vm_ref\"",
            "        dest = \"dest\"",
            "        instance_type = \"type\"",
            "        sr_path = \"sr_path\"",
            "",
            "        virtapi.instance_update(self.context, 'uuid', {'progress': 20.0})",
            "        vmops._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "        vmops._apply_orig_vm_name_label(instance, vm_ref)",
            "        old_vdi_ref = \"old_ref\"",
            "        vm_utils.get_vdi_for_vm_safely(vmops._session, vm_ref).AndReturn(",
            "            (old_vdi_ref, None))",
            "        virtapi.instance_update(self.context, 'uuid', {'progress': 40.0})",
            "        new_vdi_ref = \"new_ref\"",
            "        new_vdi_uuid = \"new_uuid\"",
            "        vm_utils.resize_disk(vmops._session, instance, old_vdi_ref,",
            "            instance_type).AndReturn((new_vdi_ref, new_vdi_uuid))",
            "        virtapi.instance_update(self.context, 'uuid', {'progress': 60.0})",
            "        vmops._migrate_vhd(instance, new_vdi_uuid, dest,",
            "                           sr_path, 0).AndRaise(",
            "                                exception.ResizeError(reason=\"asdf\"))",
            "",
            "        vm_utils.destroy_vdi(vmops._session, new_vdi_ref)",
            "        vmops._restore_orig_vm_and_cleanup_orphan(instance, None)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertRaises(exception.InstanceFaultRollback,",
            "                          vmops._migrate_disk_resizing_down, self.context,",
            "                          instance, dest, instance_type, vm_ref, sr_path)",
            "",
            "    def test_resize_ensure_vm_is_shutdown_cleanly(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        fake_instance = {'uuid': 'uuid'}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')",
            "        self.mox.StubOutWithMock(vm_utils, 'clean_shutdown_vm')",
            "        self.mox.StubOutWithMock(vm_utils, 'hard_shutdown_vm')",
            "",
            "        vm_utils.is_vm_shutdown(vmops._session, \"ref\").AndReturn(False)",
            "        vm_utils.clean_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(True)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        vmops._resize_ensure_vm_is_shutdown(fake_instance, \"ref\")",
            "",
            "    def test_resize_ensure_vm_is_shutdown_forced(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        fake_instance = {'uuid': 'uuid'}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')",
            "        self.mox.StubOutWithMock(vm_utils, 'clean_shutdown_vm')",
            "        self.mox.StubOutWithMock(vm_utils, 'hard_shutdown_vm')",
            "",
            "        vm_utils.is_vm_shutdown(vmops._session, \"ref\").AndReturn(False)",
            "        vm_utils.clean_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(False)",
            "        vm_utils.hard_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(True)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        vmops._resize_ensure_vm_is_shutdown(fake_instance, \"ref\")",
            "",
            "    def test_resize_ensure_vm_is_shutdown_fails(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        fake_instance = {'uuid': 'uuid'}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')",
            "        self.mox.StubOutWithMock(vm_utils, 'clean_shutdown_vm')",
            "        self.mox.StubOutWithMock(vm_utils, 'hard_shutdown_vm')",
            "",
            "        vm_utils.is_vm_shutdown(vmops._session, \"ref\").AndReturn(False)",
            "        vm_utils.clean_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(False)",
            "        vm_utils.hard_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(False)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertRaises(exception.ResizeError,",
            "            vmops._resize_ensure_vm_is_shutdown, fake_instance, \"ref\")",
            "",
            "    def test_resize_ensure_vm_is_shutdown_already_shutdown(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        fake_instance = {'uuid': 'uuid'}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')",
            "        self.mox.StubOutWithMock(vm_utils, 'clean_shutdown_vm')",
            "        self.mox.StubOutWithMock(vm_utils, 'hard_shutdown_vm')",
            "",
            "        vm_utils.is_vm_shutdown(vmops._session, \"ref\").AndReturn(True)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        vmops._resize_ensure_vm_is_shutdown(fake_instance, \"ref\")",
            "",
            "",
            "class XenAPIImageTypeTestCase(test.NoDBTestCase):",
            "    \"\"\"Test ImageType class.\"\"\"",
            "",
            "    def test_to_string(self):",
            "        # Can convert from type id to type string.",
            "        self.assertEquals(",
            "            vm_utils.ImageType.to_string(vm_utils.ImageType.KERNEL),",
            "            vm_utils.ImageType.KERNEL_STR)",
            "",
            "    def _assert_role(self, expected_role, image_type_id):",
            "        self.assertEquals(",
            "            expected_role,",
            "            vm_utils.ImageType.get_role(image_type_id))",
            "",
            "    def test_get_image_role_kernel(self):",
            "        self._assert_role('kernel', vm_utils.ImageType.KERNEL)",
            "",
            "    def test_get_image_role_ramdisk(self):",
            "        self._assert_role('ramdisk', vm_utils.ImageType.RAMDISK)",
            "",
            "    def test_get_image_role_disk(self):",
            "        self._assert_role('root', vm_utils.ImageType.DISK)",
            "",
            "    def test_get_image_role_disk_raw(self):",
            "        self._assert_role('root', vm_utils.ImageType.DISK_RAW)",
            "",
            "    def test_get_image_role_disk_vhd(self):",
            "        self._assert_role('root', vm_utils.ImageType.DISK_VHD)",
            "",
            "",
            "class XenAPIDetermineDiskImageTestCase(test.NoDBTestCase):",
            "    \"\"\"Unit tests for code that detects the ImageType.\"\"\"",
            "    def assert_disk_type(self, image_meta, expected_disk_type):",
            "        actual = vm_utils.determine_disk_image_type(image_meta)",
            "        self.assertEqual(expected_disk_type, actual)",
            "",
            "    def test_machine(self):",
            "        image_meta = {'id': 'a', 'disk_format': 'ami'}",
            "        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK)",
            "",
            "    def test_raw(self):",
            "        image_meta = {'id': 'a', 'disk_format': 'raw'}",
            "        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_RAW)",
            "",
            "    def test_vhd(self):",
            "        image_meta = {'id': 'a', 'disk_format': 'vhd'}",
            "        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_VHD)",
            "",
            "    def test_none(self):",
            "        image_meta = None",
            "        self.assert_disk_type(image_meta, None)",
            "",
            "",
            "class XenAPIDetermineIsPVTestCase(test.NoDBTestCase):",
            "    \"\"\"Unit tests for code that detects the PV status based on ImageType.\"\"\"",
            "    def assert_pv_status(self, disk_image_type, os_type, expected_pv_status):",
            "        session = None",
            "        vdi_ref = None",
            "        actual = vm_utils.determine_is_pv(session, vdi_ref,",
            "                                          disk_image_type, os_type)",
            "        self.assertEqual(expected_pv_status, actual)",
            "",
            "    def test_windows_vhd(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK_VHD, 'windows', False)",
            "",
            "    def test_linux_vhd(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK_VHD, 'linux', True)",
            "",
            "    def test_raw(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK_RAW, 'linux', False)",
            "",
            "    def test_disk(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK, None, True)",
            "",
            "    def test_iso(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK_ISO, None, False)",
            "",
            "    def test_none(self):",
            "        self.assert_pv_status(None, None, False)",
            "",
            "",
            "class CompareVersionTestCase(test.NoDBTestCase):",
            "    def test_less_than(self):",
            "        # Test that cmp_version compares a as less than b.",
            "        self.assertTrue(vmops.cmp_version('1.2.3.4', '1.2.3.5') < 0)",
            "",
            "    def test_greater_than(self):",
            "        # Test that cmp_version compares a as greater than b.",
            "        self.assertTrue(vmops.cmp_version('1.2.3.5', '1.2.3.4') > 0)",
            "",
            "    def test_equal(self):",
            "        # Test that cmp_version compares a as equal to b.",
            "        self.assertTrue(vmops.cmp_version('1.2.3.4', '1.2.3.4') == 0)",
            "",
            "    def test_non_lexical(self):",
            "        # Test that cmp_version compares non-lexically.",
            "        self.assertTrue(vmops.cmp_version('1.2.3.10', '1.2.3.4') > 0)",
            "",
            "    def test_length(self):",
            "        # Test that cmp_version compares by length as last resort.",
            "        self.assertTrue(vmops.cmp_version('1.2.3', '1.2.3.4') < 0)",
            "",
            "",
            "class XenAPIHostTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Tests HostState, which holds metrics from XenServer that get",
            "    reported back to the Schedulers.",
            "    \"\"\"",
            "",
            "    def setUp(self):",
            "        super(XenAPIHostTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.context = context.get_admin_context()",
            "        self.flags(use_local=True, group='conductor')",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "    def test_host_state(self):",
            "        stats = self.conn.get_host_stats()",
            "        self.assertEquals(stats['disk_total'], 40000)",
            "        self.assertEquals(stats['disk_used'], 20000)",
            "        self.assertEquals(stats['host_memory_total'], 10)",
            "        self.assertEquals(stats['host_memory_overhead'], 20)",
            "        self.assertEquals(stats['host_memory_free'], 30)",
            "        self.assertEquals(stats['host_memory_free_computed'], 40)",
            "        self.assertEquals(stats['hypervisor_hostname'], 'fake-xenhost')",
            "",
            "    def test_host_state_missing_sr(self):",
            "        def fake_safe_find_sr(session):",
            "            raise exception.StorageRepositoryNotFound('not there')",
            "",
            "        self.stubs.Set(vm_utils, 'safe_find_sr', fake_safe_find_sr)",
            "        self.assertRaises(exception.StorageRepositoryNotFound,",
            "                          self.conn.get_host_stats)",
            "",
            "    def _test_host_action(self, method, action, expected=None):",
            "        result = method('host', action)",
            "        if not expected:",
            "            expected = action",
            "        self.assertEqual(result, expected)",
            "",
            "    def test_host_reboot(self):",
            "        self._test_host_action(self.conn.host_power_action, 'reboot')",
            "",
            "    def test_host_shutdown(self):",
            "        self._test_host_action(self.conn.host_power_action, 'shutdown')",
            "",
            "    def test_host_startup(self):",
            "        self.assertRaises(NotImplementedError,",
            "                          self.conn.host_power_action, 'host', 'startup')",
            "",
            "    def test_host_maintenance_on(self):",
            "        self._test_host_action(self.conn.host_maintenance_mode,",
            "                               True, 'on_maintenance')",
            "",
            "    def test_host_maintenance_off(self):",
            "        self._test_host_action(self.conn.host_maintenance_mode,",
            "                               False, 'off_maintenance')",
            "",
            "    def test_set_enable_host_enable(self):",
            "        values = _create_service_entries(self.context, values={'nova':",
            "            ['host']})",
            "        self._test_host_action(self.conn.set_host_enabled, True, 'enabled')",
            "        service = db.service_get_by_args(self.context, 'host', 'nova-compute')",
            "        self.assertEquals(service.disabled, False)",
            "",
            "    def test_set_enable_host_disable(self):",
            "        values = _create_service_entries(self.context, values={'nova':",
            "            ['host']})",
            "        self._test_host_action(self.conn.set_host_enabled, False, 'disabled')",
            "        service = db.service_get_by_args(self.context, 'host', 'nova-compute')",
            "        self.assertEquals(service.disabled, True)",
            "",
            "    def test_get_host_uptime(self):",
            "        result = self.conn.get_host_uptime('host')",
            "        self.assertEqual(result, 'fake uptime')",
            "",
            "    def test_supported_instances_is_included_in_host_state(self):",
            "        stats = self.conn.get_host_stats()",
            "        self.assertTrue('supported_instances' in stats)",
            "",
            "    def test_supported_instances_is_calculated_by_to_supported_instances(self):",
            "",
            "        def to_supported_instances(somedata):",
            "            self.assertEquals(None, somedata)",
            "            return \"SOMERETURNVALUE\"",
            "        self.stubs.Set(host, 'to_supported_instances', to_supported_instances)",
            "",
            "        stats = self.conn.get_host_stats()",
            "        self.assertEquals(\"SOMERETURNVALUE\", stats['supported_instances'])",
            "",
            "    def test_update_stats_caches_hostname(self):",
            "        self.mox.StubOutWithMock(host, 'call_xenhost')",
            "        self.mox.StubOutWithMock(vm_utils, 'safe_find_sr')",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_xenapi')",
            "        data = {'disk_total': 0,",
            "                'disk_used': 0,",
            "                'disk_available': 0,",
            "                'supported_instances': 0,",
            "                'host_capabilities': [],",
            "                'host_hostname': 'foo',",
            "                }",
            "        sr_rec = {",
            "            'physical_size': 0,",
            "            'physical_utilisation': 0,",
            "            }",
            "",
            "        for i in range(3):",
            "            host.call_xenhost(mox.IgnoreArg(), 'host_data', {}).AndReturn(data)",
            "            vm_utils.safe_find_sr(self.conn._session).AndReturn(None)",
            "            self.conn._session.call_xenapi('SR.scan', None)",
            "            self.conn._session.call_xenapi('SR.get_record', None).AndReturn(",
            "                sr_rec)",
            "            if i == 2:",
            "                # On the third call (the second below) change the hostname",
            "                data = dict(data, host_hostname='bar')",
            "",
            "        self.mox.ReplayAll()",
            "        stats = self.conn.get_host_stats(refresh=True)",
            "        self.assertEqual('foo', stats['hypervisor_hostname'])",
            "        stats = self.conn.get_host_stats(refresh=True)",
            "        self.assertEqual('foo', stats['hypervisor_hostname'])",
            "",
            "",
            "class ToSupportedInstancesTestCase(test.NoDBTestCase):",
            "    def test_default_return_value(self):",
            "        self.assertEquals([],",
            "            host.to_supported_instances(None))",
            "",
            "    def test_return_value(self):",
            "        self.assertEquals([('x86_64', 'xapi', 'xen')],",
            "             host.to_supported_instances([u'xen-3.0-x86_64']))",
            "",
            "    def test_invalid_values_do_not_break(self):",
            "        self.assertEquals([('x86_64', 'xapi', 'xen')],",
            "             host.to_supported_instances([u'xen-3.0-x86_64', 'spam']))",
            "",
            "    def test_multiple_values(self):",
            "        self.assertEquals(",
            "            [",
            "                ('x86_64', 'xapi', 'xen'),",
            "                ('x86_32', 'xapi', 'hvm')",
            "            ],",
            "            host.to_supported_instances([u'xen-3.0-x86_64', 'hvm-3.0-x86_32'])",
            "        )",
            "",
            "",
            "class XenAPIAutoDiskConfigTestCase(stubs.XenAPITestBase):",
            "    def setUp(self):",
            "        super(XenAPIAutoDiskConfigTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "",
            "        self.instance_values = {'id': 1,",
            "                  'project_id': self.project_id,",
            "                  'user_id': self.user_id,",
            "                  'image_ref': 1,",
            "                  'kernel_id': 2,",
            "                  'ramdisk_id': 3,",
            "                  'root_gb': 20,",
            "                  'instance_type_id': '3',  # m1.large",
            "                  'os_type': 'linux',",
            "                  'architecture': 'x86-64'}",
            "",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "",
            "        def fake_create_vbd(session, vm_ref, vdi_ref, userdevice,",
            "                            vbd_type='disk', read_only=False, bootable=True,",
            "                            osvol=False):",
            "            pass",
            "",
            "        self.stubs.Set(vm_utils, 'create_vbd', fake_create_vbd)",
            "",
            "    def assertIsPartitionCalled(self, called):",
            "        marker = {\"partition_called\": False}",
            "",
            "        def fake_resize_part_and_fs(dev, start, old, new):",
            "            marker[\"partition_called\"] = True",
            "        self.stubs.Set(vm_utils, \"_resize_part_and_fs\",",
            "                       fake_resize_part_and_fs)",
            "",
            "        ctx = context.RequestContext(self.user_id, self.project_id)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "",
            "        disk_image_type = vm_utils.ImageType.DISK_VHD",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "        vm_ref = xenapi_fake.create_vm(instance['name'], 'Halted')",
            "        vdi_ref = xenapi_fake.create_vdi(instance['name'], 'fake')",
            "",
            "        vdi_uuid = session.call_xenapi('VDI.get_record', vdi_ref)['uuid']",
            "        vdis = {'root': {'uuid': vdi_uuid, 'ref': vdi_ref}}",
            "",
            "        self.conn._vmops._attach_disks(instance, vm_ref, instance['name'],",
            "                                       vdis, disk_image_type)",
            "",
            "        self.assertEqual(marker[\"partition_called\"], called)",
            "",
            "    def test_instance_not_auto_disk_config(self):",
            "        \"\"\"Should not partition unless instance is marked as",
            "        auto_disk_config.",
            "        \"\"\"",
            "        self.instance_values['auto_disk_config'] = False",
            "        self.assertIsPartitionCalled(False)",
            "",
            "    @stub_vm_utils_with_vdi_attached_here",
            "    def test_instance_auto_disk_config_doesnt_pass_fail_safes(self):",
            "        # Should not partition unless fail safes pass.",
            "        self.instance_values['auto_disk_config'] = True",
            "",
            "        def fake_get_partitions(dev):",
            "            return [(1, 0, 100, 'ext4'), (2, 100, 200, 'ext4')]",
            "        self.stubs.Set(vm_utils, \"_get_partitions\",",
            "                       fake_get_partitions)",
            "",
            "        self.assertIsPartitionCalled(False)",
            "",
            "    @stub_vm_utils_with_vdi_attached_here",
            "    def test_instance_auto_disk_config_passes_fail_safes(self):",
            "        \"\"\"Should partition if instance is marked as auto_disk_config=True and",
            "        virt-layer specific fail-safe checks pass.",
            "        \"\"\"",
            "        self.instance_values['auto_disk_config'] = True",
            "",
            "        def fake_get_partitions(dev):",
            "            return [(1, 0, 100, 'ext4')]",
            "        self.stubs.Set(vm_utils, \"_get_partitions\",",
            "                       fake_get_partitions)",
            "",
            "        self.assertIsPartitionCalled(True)",
            "",
            "",
            "class XenAPIGenerateLocal(stubs.XenAPITestBase):",
            "    \"\"\"Test generating of local disks, like swap and ephemeral.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIGenerateLocal, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "",
            "        self.instance_values = {'id': 1,",
            "                  'project_id': self.project_id,",
            "                  'user_id': self.user_id,",
            "                  'image_ref': 1,",
            "                  'kernel_id': 2,",
            "                  'ramdisk_id': 3,",
            "                  'root_gb': 20,",
            "                  'instance_type_id': '3',  # m1.large",
            "                  'os_type': 'linux',",
            "                  'architecture': 'x86-64'}",
            "",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "",
            "        def fake_create_vbd(session, vm_ref, vdi_ref, userdevice,",
            "                            vbd_type='disk', read_only=False, bootable=True,",
            "                            osvol=False, empty=False, unpluggable=True):",
            "            return session.call_xenapi('VBD.create', {'VM': vm_ref,",
            "                                                      'VDI': vdi_ref})",
            "",
            "        self.stubs.Set(vm_utils, 'create_vbd', fake_create_vbd)",
            "",
            "    def assertCalled(self, instance,",
            "                     disk_image_type=vm_utils.ImageType.DISK_VHD):",
            "        ctx = context.RequestContext(self.user_id, self.project_id)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "",
            "        vm_ref = xenapi_fake.create_vm(instance['name'], 'Halted')",
            "        vdi_ref = xenapi_fake.create_vdi(instance['name'], 'fake')",
            "",
            "        vdi_uuid = session.call_xenapi('VDI.get_record', vdi_ref)['uuid']",
            "",
            "        vdi_key = 'root'",
            "        if disk_image_type == vm_utils.ImageType.DISK_ISO:",
            "            vdi_key = 'iso'",
            "        vdis = {vdi_key: {'uuid': vdi_uuid, 'ref': vdi_ref}}",
            "",
            "        self.called = False",
            "        self.conn._vmops._attach_disks(instance, vm_ref, instance['name'],",
            "                                       vdis, disk_image_type)",
            "        self.assertTrue(self.called)",
            "",
            "    def test_generate_swap(self):",
            "        # Test swap disk generation.",
            "        instance_values = dict(self.instance_values, instance_type_id=5)",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        instance_values)",
            "",
            "        def fake_generate_swap(*args, **kwargs):",
            "            self.called = True",
            "        self.stubs.Set(vm_utils, 'generate_swap', fake_generate_swap)",
            "",
            "        self.assertCalled(instance)",
            "",
            "    def test_generate_ephemeral(self):",
            "        # Test ephemeral disk generation.",
            "        instance_values = dict(self.instance_values, instance_type_id=4)",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        instance_values)",
            "",
            "        def fake_generate_ephemeral(*args):",
            "            self.called = True",
            "        self.stubs.Set(vm_utils, 'generate_ephemeral', fake_generate_ephemeral)",
            "",
            "        self.assertCalled(instance)",
            "",
            "    def test_generate_iso_blank_root_disk(self):",
            "        instance_values = dict(self.instance_values, instance_type_id=4)",
            "        instance_values.pop('kernel_id')",
            "        instance_values.pop('ramdisk_id')",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        instance_values)",
            "",
            "        def fake_generate_ephemeral(*args):",
            "            pass",
            "        self.stubs.Set(vm_utils, 'generate_ephemeral', fake_generate_ephemeral)",
            "",
            "        def fake_generate_iso(*args):",
            "            self.called = True",
            "        self.stubs.Set(vm_utils, 'generate_iso_blank_root_disk',",
            "            fake_generate_iso)",
            "",
            "        self.assertCalled(instance, vm_utils.ImageType.DISK_ISO)",
            "",
            "",
            "class XenAPIBWCountersTestCase(stubs.XenAPITestBase):",
            "    FAKE_VMS = {'test1:ref': dict(name_label='test1',",
            "                                   other_config=dict(nova_uuid='hash'),",
            "                                   domid='12',",
            "                                   _vifmap={'0': \"a:b:c:d...\",",
            "                                           '1': \"e:f:12:q...\"}),",
            "                'test2:ref': dict(name_label='test2',",
            "                                   other_config=dict(nova_uuid='hash'),",
            "                                   domid='42',",
            "                                   _vifmap={'0': \"a:3:c:d...\",",
            "                                           '1': \"e:f:42:q...\"}),",
            "               }",
            "",
            "    def setUp(self):",
            "        super(XenAPIBWCountersTestCase, self).setUp()",
            "        self.stubs.Set(vm_utils, 'list_vms',",
            "                       XenAPIBWCountersTestCase._fake_list_vms)",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def _fake_get_vif_device_map(vm_rec):",
            "            return vm_rec['_vifmap']",
            "",
            "        self.stubs.Set(self.conn._vmops, \"_get_vif_device_map\",",
            "                                         _fake_get_vif_device_map)",
            "",
            "    @classmethod",
            "    def _fake_list_vms(cls, session):",
            "        return cls.FAKE_VMS.iteritems()",
            "",
            "    @staticmethod",
            "    def _fake_fetch_bandwidth_mt(session):",
            "        return {}",
            "",
            "    @staticmethod",
            "    def _fake_fetch_bandwidth(session):",
            "        return {'42':",
            "                    {'0': {'bw_in': 21024, 'bw_out': 22048},",
            "                     '1': {'bw_in': 231337, 'bw_out': 221212121}},",
            "                '12':",
            "                    {'0': {'bw_in': 1024, 'bw_out': 2048},",
            "                     '1': {'bw_in': 31337, 'bw_out': 21212121}},",
            "                }",
            "",
            "    def test_get_all_bw_counters(self):",
            "        instances = [dict(name='test1', uuid='1-2-3'),",
            "                     dict(name='test2', uuid='4-5-6')]",
            "",
            "        self.stubs.Set(vm_utils, 'fetch_bandwidth',",
            "                       self._fake_fetch_bandwidth)",
            "        result = self.conn.get_all_bw_counters(instances)",
            "        self.assertEqual(len(result), 4)",
            "        self.assertIn(dict(uuid='1-2-3',",
            "                           mac_address=\"a:b:c:d...\",",
            "                           bw_in=1024,",
            "                           bw_out=2048), result)",
            "        self.assertIn(dict(uuid='1-2-3',",
            "                           mac_address=\"e:f:12:q...\",",
            "                           bw_in=31337,",
            "                           bw_out=21212121), result)",
            "",
            "        self.assertIn(dict(uuid='4-5-6',",
            "                           mac_address=\"a:3:c:d...\",",
            "                           bw_in=21024,",
            "                           bw_out=22048), result)",
            "        self.assertIn(dict(uuid='4-5-6',",
            "                           mac_address=\"e:f:42:q...\",",
            "                           bw_in=231337,",
            "                           bw_out=221212121), result)",
            "",
            "    def test_get_all_bw_counters_in_failure_case(self):",
            "        \"\"\"Test that get_all_bw_conters returns an empty list when",
            "        no data returned from Xenserver.  c.f. bug #910045.",
            "        \"\"\"",
            "        instances = [dict(name='instance-0001', uuid='1-2-3-4-5')]",
            "",
            "        self.stubs.Set(vm_utils, 'fetch_bandwidth',",
            "                       self._fake_fetch_bandwidth_mt)",
            "        result = self.conn.get_all_bw_counters(instances)",
            "        self.assertEqual(result, [])",
            "",
            "",
            "# TODO(salvatore-orlando): this class and",
            "# nova.tests.virt.test_libvirt.IPTablesFirewallDriverTestCase share a lot of",
            "# code.  Consider abstracting common code in a base class for firewall driver",
            "# testing.",
            "class XenAPIDom0IptablesFirewallTestCase(stubs.XenAPITestBase):",
            "",
            "    _in_rules = [",
            "      '# Generated by iptables-save v1.4.10 on Sat Feb 19 00:03:19 2011',",
            "      '*nat',",
            "      ':PREROUTING ACCEPT [1170:189210]',",
            "      ':INPUT ACCEPT [844:71028]',",
            "      ':OUTPUT ACCEPT [5149:405186]',",
            "      ':POSTROUTING ACCEPT [5063:386098]',",
            "      '# Completed on Mon Dec  6 11:54:13 2010',",
            "      '# Generated by iptables-save v1.4.4 on Mon Dec  6 11:54:13 2010',",
            "      '*mangle',",
            "      ':INPUT ACCEPT [969615:281627771]',",
            "      ':FORWARD ACCEPT [0:0]',",
            "      ':OUTPUT ACCEPT [915599:63811649]',",
            "      ':nova-block-ipv4 - [0:0]',",
            "      '[0:0] -A INPUT -i virbr0 -p tcp -m tcp --dport 67 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -d 192.168.122.0/24 -o virbr0 -m state --state RELATED'",
            "      ',ESTABLISHED -j ACCEPT ',",
            "      '[0:0] -A FORWARD -s 192.168.122.0/24 -i virbr0 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -i virbr0 -o virbr0 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -o virbr0 -j REJECT '",
            "      '--reject-with icmp-port-unreachable ',",
            "      '[0:0] -A FORWARD -i virbr0 -j REJECT '",
            "      '--reject-with icmp-port-unreachable ',",
            "      'COMMIT',",
            "      '# Completed on Mon Dec  6 11:54:13 2010',",
            "      '# Generated by iptables-save v1.4.4 on Mon Dec  6 11:54:13 2010',",
            "      '*filter',",
            "      ':INPUT ACCEPT [969615:281627771]',",
            "      ':FORWARD ACCEPT [0:0]',",
            "      ':OUTPUT ACCEPT [915599:63811649]',",
            "      ':nova-block-ipv4 - [0:0]',",
            "      '[0:0] -A INPUT -i virbr0 -p tcp -m tcp --dport 67 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -d 192.168.122.0/24 -o virbr0 -m state --state RELATED'",
            "      ',ESTABLISHED -j ACCEPT ',",
            "      '[0:0] -A FORWARD -s 192.168.122.0/24 -i virbr0 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -i virbr0 -o virbr0 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -o virbr0 -j REJECT '",
            "      '--reject-with icmp-port-unreachable ',",
            "      '[0:0] -A FORWARD -i virbr0 -j REJECT '",
            "      '--reject-with icmp-port-unreachable ',",
            "      'COMMIT',",
            "      '# Completed on Mon Dec  6 11:54:13 2010',",
            "    ]",
            "",
            "    _in6_filter_rules = [",
            "      '# Generated by ip6tables-save v1.4.4 on Tue Jan 18 23:47:56 2011',",
            "      '*filter',",
            "      ':INPUT ACCEPT [349155:75810423]',",
            "      ':FORWARD ACCEPT [0:0]',",
            "      ':OUTPUT ACCEPT [349256:75777230]',",
            "      'COMMIT',",
            "      '# Completed on Tue Jan 18 23:47:56 2011',",
            "    ]",
            "",
            "    def setUp(self):",
            "        super(XenAPIDom0IptablesFirewallTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   instance_name_template='%d',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        self.user_id = 'mappin'",
            "        self.project_id = 'fake'",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForFirewallTests,",
            "                              test_case=self)",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "        self.network = importutils.import_object(CONF.network_manager)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.fw = self.conn._vmops.firewall_driver",
            "",
            "    def _create_instance_ref(self):",
            "        return db.instance_create(self.context,",
            "                                  {'user_id': self.user_id,",
            "                                   'project_id': self.project_id,",
            "                                   'instance_type_id': 1})",
            "",
            "    def _create_test_security_group(self):",
            "        admin_ctxt = context.get_admin_context()",
            "        secgroup = db.security_group_create(admin_ctxt,",
            "                                {'user_id': self.user_id,",
            "                                 'project_id': self.project_id,",
            "                                 'name': 'testgroup',",
            "                                 'description': 'test group'})",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'icmp',",
            "                                       'from_port': -1,",
            "                                       'to_port': -1,",
            "                                       'cidr': '192.168.11.0/24'})",
            "",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'icmp',",
            "                                       'from_port': 8,",
            "                                       'to_port': -1,",
            "                                       'cidr': '192.168.11.0/24'})",
            "",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'tcp',",
            "                                       'from_port': 80,",
            "                                       'to_port': 81,",
            "                                       'cidr': '192.168.10.0/24'})",
            "        return secgroup",
            "",
            "    def _validate_security_group(self):",
            "        in_rules = filter(lambda l: not l.startswith('#'),",
            "                          self._in_rules)",
            "        for rule in in_rules:",
            "            if 'nova' not in rule:",
            "                self.assertTrue(rule in self._out_rules,",
            "                                'Rule went missing: %s' % rule)",
            "",
            "        instance_chain = None",
            "        for rule in self._out_rules:",
            "            # This is pretty crude, but it'll do for now",
            "            # last two octets change",
            "            if re.search('-d 192.168.[0-9]{1,3}.[0-9]{1,3} -j', rule):",
            "                instance_chain = rule.split(' ')[-1]",
            "                break",
            "        self.assertTrue(instance_chain, \"The instance chain wasn't added\")",
            "        security_group_chain = None",
            "        for rule in self._out_rules:",
            "            # This is pretty crude, but it'll do for now",
            "            if '-A %s -j' % instance_chain in rule:",
            "                security_group_chain = rule.split(' ')[-1]",
            "                break",
            "        self.assertTrue(security_group_chain,",
            "                        \"The security group chain wasn't added\")",
            "",
            "        regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p icmp'",
            "                           ' -s 192.168.11.0/24')",
            "        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                        \"ICMP acceptance rule wasn't added\")",
            "",
            "        regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p icmp -m icmp'",
            "                           ' --icmp-type 8 -s 192.168.11.0/24')",
            "        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                        \"ICMP Echo Request acceptance rule wasn't added\")",
            "",
            "        regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p tcp --dport 80:81'",
            "                           ' -s 192.168.10.0/24')",
            "        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                        \"TCP port 80/81 acceptance rule wasn't added\")",
            "",
            "    def test_static_filters(self):",
            "        instance_ref = self._create_instance_ref()",
            "        src_instance_ref = self._create_instance_ref()",
            "        admin_ctxt = context.get_admin_context()",
            "        secgroup = self._create_test_security_group()",
            "",
            "        src_secgroup = db.security_group_create(admin_ctxt,",
            "                                                {'user_id': self.user_id,",
            "                                                 'project_id': self.project_id,",
            "                                                 'name': 'testsourcegroup',",
            "                                                 'description': 'src group'})",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'tcp',",
            "                                       'from_port': 80,",
            "                                       'to_port': 81,",
            "                                       'group_id': src_secgroup['id']})",
            "",
            "        db.instance_add_security_group(admin_ctxt, instance_ref['uuid'],",
            "                                       secgroup['id'])",
            "        db.instance_add_security_group(admin_ctxt, src_instance_ref['uuid'],",
            "                                       src_secgroup['id'])",
            "        instance_ref = db.instance_get(admin_ctxt, instance_ref['id'])",
            "        src_instance_ref = db.instance_get(admin_ctxt, src_instance_ref['id'])",
            "",
            "        network_model = fake_network.fake_get_instance_nw_info(self.stubs, 1)",
            "",
            "        from nova.compute import utils as compute_utils",
            "        self.stubs.Set(compute_utils, 'get_nw_info_for_instance',",
            "                       lambda instance: network_model)",
            "",
            "        self.fw.prepare_instance_filter(instance_ref, network_model)",
            "        self.fw.apply_instance_filter(instance_ref, network_model)",
            "",
            "        self._validate_security_group()",
            "        # Extra test for TCP acceptance rules",
            "        for ip in network_model.fixed_ips():",
            "            if ip['version'] != 4:",
            "                continue",
            "            regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p tcp'",
            "                               ' --dport 80:81 -s %s' % ip['address'])",
            "            self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                            \"TCP port 80/81 acceptance rule wasn't added\")",
            "",
            "        db.instance_destroy(admin_ctxt, instance_ref['uuid'])",
            "",
            "    def test_filters_for_instance_with_ip_v6(self):",
            "        self.flags(use_ipv6=True)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)",
            "        rulesv4, rulesv6 = self.fw._filters_for_instance(\"fake\", network_info)",
            "        self.assertEquals(len(rulesv4), 2)",
            "        self.assertEquals(len(rulesv6), 1)",
            "",
            "    def test_filters_for_instance_without_ip_v6(self):",
            "        self.flags(use_ipv6=False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)",
            "        rulesv4, rulesv6 = self.fw._filters_for_instance(\"fake\", network_info)",
            "        self.assertEquals(len(rulesv4), 2)",
            "        self.assertEquals(len(rulesv6), 0)",
            "",
            "    def test_multinic_iptables(self):",
            "        ipv4_rules_per_addr = 1",
            "        ipv4_addr_per_network = 2",
            "        ipv6_rules_per_addr = 1",
            "        ipv6_addr_per_network = 1",
            "        networks_count = 5",
            "        instance_ref = self._create_instance_ref()",
            "        _get_instance_nw_info = fake_network.fake_get_instance_nw_info",
            "        network_info = _get_instance_nw_info(self.stubs,",
            "                                             networks_count,",
            "                                             ipv4_addr_per_network)",
            "        network_info[0]['network']['subnets'][0]['meta']['dhcp_server'] = \\",
            "            '1.1.1.1'",
            "        ipv4_len = len(self.fw.iptables.ipv4['filter'].rules)",
            "        ipv6_len = len(self.fw.iptables.ipv6['filter'].rules)",
            "        inst_ipv4, inst_ipv6 = self.fw.instance_rules(instance_ref,",
            "                                                      network_info)",
            "        self.fw.prepare_instance_filter(instance_ref, network_info)",
            "        ipv4 = self.fw.iptables.ipv4['filter'].rules",
            "        ipv6 = self.fw.iptables.ipv6['filter'].rules",
            "        ipv4_network_rules = len(ipv4) - len(inst_ipv4) - ipv4_len",
            "        ipv6_network_rules = len(ipv6) - len(inst_ipv6) - ipv6_len",
            "        # Extra rules are for the DHCP request",
            "        rules = (ipv4_rules_per_addr * ipv4_addr_per_network *",
            "                 networks_count) + 2",
            "        self.assertEquals(ipv4_network_rules, rules)",
            "        self.assertEquals(ipv6_network_rules,",
            "                  ipv6_rules_per_addr * ipv6_addr_per_network * networks_count)",
            "",
            "    def test_do_refresh_security_group_rules(self):",
            "        admin_ctxt = context.get_admin_context()",
            "        instance_ref = self._create_instance_ref()",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1, 1)",
            "        secgroup = self._create_test_security_group()",
            "        db.instance_add_security_group(admin_ctxt, instance_ref['uuid'],",
            "                                       secgroup['id'])",
            "        self.fw.prepare_instance_filter(instance_ref, network_info)",
            "        self.fw.instances[instance_ref['id']] = instance_ref",
            "        self._validate_security_group()",
            "        # add a rule to the security group",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'udp',",
            "                                       'from_port': 200,",
            "                                       'to_port': 299,",
            "                                       'cidr': '192.168.99.0/24'})",
            "        #validate the extra rule",
            "        self.fw.refresh_security_group_rules(secgroup)",
            "        regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p udp --dport 200:299'",
            "                           ' -s 192.168.99.0/24')",
            "        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                        \"Rules were not updated properly.\"",
            "                        \"The rule for UDP acceptance is missing\")",
            "",
            "    def test_provider_firewall_rules(self):",
            "        # setup basic instance data",
            "        instance_ref = self._create_instance_ref()",
            "        # FRAGILE: as in libvirt tests",
            "        # peeks at how the firewall names chains",
            "        chain_name = 'inst-%s' % instance_ref['id']",
            "",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1, 1)",
            "        self.fw.prepare_instance_filter(instance_ref, network_info)",
            "        self.assertTrue('provider' in self.fw.iptables.ipv4['filter'].chains)",
            "        rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                      if rule.chain == 'provider']",
            "        self.assertEqual(0, len(rules))",
            "",
            "        admin_ctxt = context.get_admin_context()",
            "        # add a rule and send the update message, check for 1 rule",
            "        provider_fw0 = db.provider_fw_rule_create(admin_ctxt,",
            "                                                  {'protocol': 'tcp',",
            "                                                   'cidr': '10.99.99.99/32',",
            "                                                   'from_port': 1,",
            "                                                   'to_port': 65535})",
            "        self.fw.refresh_provider_fw_rules()",
            "        rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                      if rule.chain == 'provider']",
            "        self.assertEqual(1, len(rules))",
            "",
            "        # Add another, refresh, and make sure number of rules goes to two",
            "        provider_fw1 = db.provider_fw_rule_create(admin_ctxt,",
            "                                                  {'protocol': 'udp',",
            "                                                   'cidr': '10.99.99.99/32',",
            "                                                   'from_port': 1,",
            "                                                   'to_port': 65535})",
            "        self.fw.refresh_provider_fw_rules()",
            "        rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                      if rule.chain == 'provider']",
            "        self.assertEqual(2, len(rules))",
            "",
            "        # create the instance filter and make sure it has a jump rule",
            "        self.fw.prepare_instance_filter(instance_ref, network_info)",
            "        self.fw.apply_instance_filter(instance_ref, network_info)",
            "        inst_rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                           if rule.chain == chain_name]",
            "        jump_rules = [rule for rule in inst_rules if '-j' in rule.rule]",
            "        provjump_rules = []",
            "        # IptablesTable doesn't make rules unique internally",
            "        for rule in jump_rules:",
            "            if 'provider' in rule.rule and rule not in provjump_rules:",
            "                provjump_rules.append(rule)",
            "        self.assertEqual(1, len(provjump_rules))",
            "",
            "        # remove a rule from the db, cast to compute to refresh rule",
            "        db.provider_fw_rule_destroy(admin_ctxt, provider_fw1['id'])",
            "        self.fw.refresh_provider_fw_rules()",
            "        rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                      if rule.chain == 'provider']",
            "        self.assertEqual(1, len(rules))",
            "",
            "",
            "class XenAPISRSelectionTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for testing we find the right SR.\"\"\"",
            "    def test_safe_find_sr_raise_exception(self):",
            "        # Ensure StorageRepositoryNotFound is raise when wrong filter.",
            "        self.flags(sr_matching_filter='yadayadayada')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        self.assertRaises(exception.StorageRepositoryNotFound,",
            "                          vm_utils.safe_find_sr, session)",
            "",
            "    def test_safe_find_sr_local_storage(self):",
            "        # Ensure the default local-storage is found.",
            "        self.flags(sr_matching_filter='other-config:i18n-key=local-storage')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        # This test is only guaranteed if there is one host in the pool",
            "        self.assertEqual(len(xenapi_fake.get_all('host')), 1)",
            "        host_ref = xenapi_fake.get_all('host')[0]",
            "        pbd_refs = xenapi_fake.get_all('PBD')",
            "        for pbd_ref in pbd_refs:",
            "            pbd_rec = xenapi_fake.get_record('PBD', pbd_ref)",
            "            if pbd_rec['host'] != host_ref:",
            "                continue",
            "            sr_rec = xenapi_fake.get_record('SR', pbd_rec['SR'])",
            "            if sr_rec['other_config']['i18n-key'] == 'local-storage':",
            "                local_sr = pbd_rec['SR']",
            "        expected = vm_utils.safe_find_sr(session)",
            "        self.assertEqual(local_sr, expected)",
            "",
            "    def test_safe_find_sr_by_other_criteria(self):",
            "        # Ensure the SR is found when using a different filter.",
            "        self.flags(sr_matching_filter='other-config:my_fake_sr=true')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        host_ref = xenapi_fake.get_all('host')[0]",
            "        local_sr = xenapi_fake.create_sr(name_label='Fake Storage',",
            "                                         type='lvm',",
            "                                         other_config={'my_fake_sr': 'true'},",
            "                                         host_ref=host_ref)",
            "        expected = vm_utils.safe_find_sr(session)",
            "        self.assertEqual(local_sr, expected)",
            "",
            "    def test_safe_find_sr_default(self):",
            "        # Ensure the default SR is found regardless of other-config.",
            "        self.flags(sr_matching_filter='default-sr:true')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        pool_ref = session.call_xenapi('pool.get_all')[0]",
            "        expected = vm_utils.safe_find_sr(session)",
            "        self.assertEqual(session.call_xenapi('pool.get_default_SR', pool_ref),",
            "                         expected)",
            "",
            "",
            "def _create_service_entries(context, values={'avail_zone1': ['fake_host1',",
            "                                                         'fake_host2'],",
            "                                         'avail_zone2': ['fake_host3'], }):",
            "    for avail_zone, hosts in values.iteritems():",
            "        for host in hosts:",
            "            db.service_create(context,",
            "                              {'host': host,",
            "                               'binary': 'nova-compute',",
            "                               'topic': 'compute',",
            "                               'report_count': 0})",
            "    return values",
            "",
            "",
            "class XenAPIAggregateTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for aggregate operations.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIAggregateTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='http://test_url',",
            "                   xenapi_connection_username='test_user',",
            "                   xenapi_connection_password='test_pass',",
            "                   instance_name_template='%d',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver',",
            "                   host='host',",
            "                   compute_driver='xenapi.XenAPIDriver',",
            "                   default_availability_zone='avail_zone1')",
            "        self.flags(use_local=True, group='conductor')",
            "        host_ref = xenapi_fake.get_all('host')[0]",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.context = context.get_admin_context()",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.compute = importutils.import_object(CONF.compute_manager)",
            "        self.api = compute_api.AggregateAPI()",
            "        values = {'name': 'test_aggr',",
            "                  'metadata': {'availability_zone': 'test_zone',",
            "                  pool_states.POOL_FLAG: 'XenAPI'}}",
            "        self.aggr = db.aggregate_create(self.context, values)",
            "        self.fake_metadata = {pool_states.POOL_FLAG: 'XenAPI',",
            "                              'master_compute': 'host',",
            "                              'availability_zone': 'fake_zone',",
            "                              pool_states.KEY: pool_states.ACTIVE,",
            "                              'host': xenapi_fake.get_record('host',",
            "                                                             host_ref)['uuid']}",
            "",
            "    def test_pool_add_to_aggregate_called_by_driver(self):",
            "",
            "        calls = []",
            "",
            "        def pool_add_to_aggregate(context, aggregate, host, slave_info=None):",
            "            self.assertEquals(\"CONTEXT\", context)",
            "            self.assertEquals(\"AGGREGATE\", aggregate)",
            "            self.assertEquals(\"HOST\", host)",
            "            self.assertEquals(\"SLAVEINFO\", slave_info)",
            "            calls.append(pool_add_to_aggregate)",
            "        self.stubs.Set(self.conn._pool,",
            "                       \"add_to_aggregate\",",
            "                       pool_add_to_aggregate)",
            "",
            "        self.conn.add_to_aggregate(\"CONTEXT\", \"AGGREGATE\", \"HOST\",",
            "                                   slave_info=\"SLAVEINFO\")",
            "",
            "        self.assertTrue(pool_add_to_aggregate in calls)",
            "",
            "    def test_pool_remove_from_aggregate_called_by_driver(self):",
            "",
            "        calls = []",
            "",
            "        def pool_remove_from_aggregate(context, aggregate, host,",
            "                                       slave_info=None):",
            "            self.assertEquals(\"CONTEXT\", context)",
            "            self.assertEquals(\"AGGREGATE\", aggregate)",
            "            self.assertEquals(\"HOST\", host)",
            "            self.assertEquals(\"SLAVEINFO\", slave_info)",
            "            calls.append(pool_remove_from_aggregate)",
            "        self.stubs.Set(self.conn._pool,",
            "                       \"remove_from_aggregate\",",
            "                       pool_remove_from_aggregate)",
            "",
            "        self.conn.remove_from_aggregate(\"CONTEXT\", \"AGGREGATE\", \"HOST\",",
            "                                        slave_info=\"SLAVEINFO\")",
            "",
            "        self.assertTrue(pool_remove_from_aggregate in calls)",
            "",
            "    def test_add_to_aggregate_for_first_host_sets_metadata(self):",
            "        def fake_init_pool(id, name):",
            "            fake_init_pool.called = True",
            "        self.stubs.Set(self.conn._pool, \"_init_pool\", fake_init_pool)",
            "",
            "        aggregate = self._aggregate_setup()",
            "        self.conn._pool.add_to_aggregate(self.context, aggregate, \"host\")",
            "        result = db.aggregate_get(self.context, aggregate['id'])",
            "        self.assertTrue(fake_init_pool.called)",
            "        self.assertThat(self.fake_metadata,",
            "                        matchers.DictMatches(result['metadetails']))",
            "",
            "    def test_join_slave(self):",
            "        # Ensure join_slave gets called when the request gets to master.",
            "        def fake_join_slave(id, compute_uuid, host, url, user, password):",
            "            fake_join_slave.called = True",
            "        self.stubs.Set(self.conn._pool, \"_join_slave\", fake_join_slave)",
            "",
            "        aggregate = self._aggregate_setup(hosts=['host', 'host2'],",
            "                                          metadata=self.fake_metadata)",
            "        self.conn._pool.add_to_aggregate(self.context, aggregate, \"host2\",",
            "                                         dict(compute_uuid='fake_uuid',",
            "                                         url='fake_url',",
            "                                         user='fake_user',",
            "                                         passwd='fake_pass',",
            "                                         xenhost_uuid='fake_uuid'))",
            "        self.assertTrue(fake_join_slave.called)",
            "",
            "    def test_add_to_aggregate_first_host(self):",
            "        def fake_pool_set_name_label(self, session, pool_ref, name):",
            "            fake_pool_set_name_label.called = True",
            "        self.stubs.Set(xenapi_fake.SessionBase, \"pool_set_name_label\",",
            "                       fake_pool_set_name_label)",
            "        self.conn._session.call_xenapi(\"pool.create\", {\"name\": \"asdf\"})",
            "",
            "        values = {\"name\": 'fake_aggregate',",
            "                  'metadata': {'availability_zone': 'fake_zone'}}",
            "        result = db.aggregate_create(self.context, values)",
            "        metadata = {'availability_zone': 'fake_zone',",
            "                    pool_states.POOL_FLAG: \"XenAPI\",",
            "                    pool_states.KEY: pool_states.CREATED}",
            "        db.aggregate_metadata_add(self.context, result['id'], metadata)",
            "",
            "        db.aggregate_host_add(self.context, result['id'], \"host\")",
            "        aggregate = db.aggregate_get(self.context, result['id'])",
            "        self.assertEqual([\"host\"], aggregate['hosts'])",
            "        self.assertEqual(metadata, aggregate['metadetails'])",
            "",
            "        self.conn._pool.add_to_aggregate(self.context, aggregate, \"host\")",
            "        self.assertTrue(fake_pool_set_name_label.called)",
            "",
            "    def test_remove_from_aggregate_called(self):",
            "        def fake_remove_from_aggregate(context, aggregate, host):",
            "            fake_remove_from_aggregate.called = True",
            "        self.stubs.Set(self.conn._pool,",
            "                       \"remove_from_aggregate\",",
            "                       fake_remove_from_aggregate)",
            "",
            "        self.conn.remove_from_aggregate(None, None, None)",
            "        self.assertTrue(fake_remove_from_aggregate.called)",
            "",
            "    def test_remove_from_empty_aggregate(self):",
            "        result = self._aggregate_setup()",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn._pool.remove_from_aggregate,",
            "                          self.context, result, \"test_host\")",
            "",
            "    def test_remove_slave(self):",
            "        # Ensure eject slave gets called.",
            "        def fake_eject_slave(id, compute_uuid, host_uuid):",
            "            fake_eject_slave.called = True",
            "        self.stubs.Set(self.conn._pool, \"_eject_slave\", fake_eject_slave)",
            "",
            "        self.fake_metadata['host2'] = 'fake_host2_uuid'",
            "        aggregate = self._aggregate_setup(hosts=['host', 'host2'],",
            "                metadata=self.fake_metadata, aggr_state=pool_states.ACTIVE)",
            "        self.conn._pool.remove_from_aggregate(self.context, aggregate, \"host2\")",
            "        self.assertTrue(fake_eject_slave.called)",
            "",
            "    def test_remove_master_solo(self):",
            "        # Ensure metadata are cleared after removal.",
            "        def fake_clear_pool(id):",
            "            fake_clear_pool.called = True",
            "        self.stubs.Set(self.conn._pool, \"_clear_pool\", fake_clear_pool)",
            "",
            "        aggregate = self._aggregate_setup(metadata=self.fake_metadata)",
            "        self.conn._pool.remove_from_aggregate(self.context, aggregate, \"host\")",
            "        result = db.aggregate_get(self.context, aggregate['id'])",
            "        self.assertTrue(fake_clear_pool.called)",
            "        self.assertThat({'availability_zone': 'fake_zone',",
            "                pool_states.POOL_FLAG: 'XenAPI',",
            "                pool_states.KEY: pool_states.ACTIVE},",
            "                matchers.DictMatches(result['metadetails']))",
            "",
            "    def test_remote_master_non_empty_pool(self):",
            "        # Ensure AggregateError is raised if removing the master.",
            "        aggregate = self._aggregate_setup(hosts=['host', 'host2'],",
            "                                          metadata=self.fake_metadata)",
            "",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn._pool.remove_from_aggregate,",
            "                          self.context, aggregate, \"host\")",
            "",
            "    def _aggregate_setup(self, aggr_name='fake_aggregate',",
            "                         aggr_zone='fake_zone',",
            "                         aggr_state=pool_states.CREATED,",
            "                         hosts=['host'], metadata=None):",
            "        values = {\"name\": aggr_name}",
            "        result = db.aggregate_create(self.context, values,",
            "                metadata={'availability_zone': aggr_zone})",
            "        pool_flag = {pool_states.POOL_FLAG: \"XenAPI\",",
            "                    pool_states.KEY: aggr_state}",
            "        db.aggregate_metadata_add(self.context, result['id'], pool_flag)",
            "",
            "        for host in hosts:",
            "            db.aggregate_host_add(self.context, result['id'], host)",
            "        if metadata:",
            "            db.aggregate_metadata_add(self.context, result['id'], metadata)",
            "        return db.aggregate_get(self.context, result['id'])",
            "",
            "    def test_add_host_to_aggregate_invalid_changing_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when adding host while",
            "        aggregate is not ready.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.CHANGING)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.add_to_aggregate, self.context,",
            "                          aggregate, 'host')",
            "",
            "    def test_add_host_to_aggregate_invalid_dismissed_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when aggregate is",
            "        deleted.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.DISMISSED)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.add_to_aggregate, self.context,",
            "                          aggregate, 'fake_host')",
            "",
            "    def test_add_host_to_aggregate_invalid_error_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when aggregate is",
            "        in error.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.ERROR)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.add_to_aggregate, self.context,",
            "                          aggregate, 'fake_host')",
            "",
            "    def test_remove_host_from_aggregate_error(self):",
            "        # Ensure we can remove a host from an aggregate even if in error.",
            "        values = _create_service_entries(self.context)",
            "        fake_zone = values.keys()[0]",
            "        aggr = self.api.create_aggregate(self.context,",
            "                                         'fake_aggregate', fake_zone)",
            "        # let's mock the fact that the aggregate is ready!",
            "        metadata = {pool_states.POOL_FLAG: \"XenAPI\",",
            "                    pool_states.KEY: pool_states.ACTIVE}",
            "        db.aggregate_metadata_add(self.context, aggr['id'], metadata)",
            "        for host in values[fake_zone]:",
            "            aggr = self.api.add_host_to_aggregate(self.context,",
            "                                                  aggr['id'], host)",
            "        # let's mock the fact that the aggregate is in error!",
            "        status = {'operational_state': pool_states.ERROR}",
            "        expected = self.api.remove_host_from_aggregate(self.context,",
            "                                                       aggr['id'],",
            "                                                       values[fake_zone][0])",
            "        self.assertEqual(len(aggr['hosts']) - 1, len(expected['hosts']))",
            "        self.assertEqual(expected['metadata'][pool_states.KEY],",
            "                         pool_states.ACTIVE)",
            "",
            "    def test_remove_host_from_aggregate_invalid_dismissed_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when aggregate is",
            "        deleted.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.DISMISSED)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.remove_from_aggregate, self.context,",
            "                          aggregate, 'fake_host')",
            "",
            "    def test_remove_host_from_aggregate_invalid_changing_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when aggregate is",
            "        changing.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.CHANGING)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.remove_from_aggregate, self.context,",
            "                          aggregate, 'fake_host')",
            "",
            "    def test_add_aggregate_host_raise_err(self):",
            "        # Ensure the undo operation works correctly on add.",
            "        def fake_driver_add_to_aggregate(context, aggregate, host, **_ignore):",
            "            raise exception.AggregateError(",
            "                    aggregate_id='', action='', reason='')",
            "        self.stubs.Set(self.compute.driver, \"add_to_aggregate\",",
            "                       fake_driver_add_to_aggregate)",
            "        metadata = {pool_states.POOL_FLAG: \"XenAPI\",",
            "                    pool_states.KEY: pool_states.ACTIVE}",
            "        db.aggregate_metadata_add(self.context, self.aggr['id'], metadata)",
            "        db.aggregate_host_add(self.context, self.aggr['id'], 'fake_host')",
            "",
            "        self.assertRaises(exception.AggregateError,",
            "                          self.compute.add_aggregate_host,",
            "                          self.context, \"fake_host\",",
            "                          aggregate=jsonutils.to_primitive(self.aggr))",
            "        excepted = db.aggregate_get(self.context, self.aggr['id'])",
            "        self.assertEqual(excepted['metadetails'][pool_states.KEY],",
            "                pool_states.ERROR)",
            "        self.assertEqual(excepted['hosts'], [])",
            "",
            "",
            "class MockComputeAPI(object):",
            "    def __init__(self):",
            "        self._mock_calls = []",
            "",
            "    def add_aggregate_host(self, ctxt, aggregate,",
            "                                     host_param, host, slave_info):",
            "        self._mock_calls.append((",
            "            self.add_aggregate_host, ctxt, aggregate,",
            "            host_param, host, slave_info))",
            "",
            "    def remove_aggregate_host(self, ctxt, aggregate_id, host_param,",
            "                              host, slave_info):",
            "        self._mock_calls.append((",
            "            self.remove_aggregate_host, ctxt, aggregate_id,",
            "            host_param, host, slave_info))",
            "",
            "",
            "class StubDependencies(object):",
            "    \"\"\"Stub dependencies for ResourcePool.\"\"\"",
            "",
            "    def __init__(self):",
            "        self.compute_rpcapi = MockComputeAPI()",
            "",
            "    def _is_hv_pool(self, *_ignore):",
            "        return True",
            "",
            "    def _get_metadata(self, *_ignore):",
            "        return {",
            "            pool_states.KEY: {},",
            "            'master_compute': 'master'",
            "        }",
            "",
            "    def _create_slave_info(self, *ignore):",
            "        return \"SLAVE_INFO\"",
            "",
            "",
            "class ResourcePoolWithStubs(StubDependencies, pool.ResourcePool):",
            "    \"\"\"A ResourcePool, use stub dependencies.\"\"\"",
            "",
            "",
            "class HypervisorPoolTestCase(test.NoDBTestCase):",
            "",
            "    fake_aggregate = {",
            "        'id': 98,",
            "        'hosts': [],",
            "        'metadetails': {",
            "            'master_compute': 'master',",
            "            pool_states.POOL_FLAG: {},",
            "            pool_states.KEY: {}",
            "            }",
            "        }",
            "",
            "    def test_slave_asks_master_to_add_slave_to_pool(self):",
            "        slave = ResourcePoolWithStubs()",
            "",
            "        slave.add_to_aggregate(\"CONTEXT\", self.fake_aggregate, \"slave\")",
            "",
            "        self.assertIn(",
            "            (slave.compute_rpcapi.add_aggregate_host,",
            "            \"CONTEXT\", jsonutils.to_primitive(self.fake_aggregate),",
            "            \"slave\", \"master\", \"SLAVE_INFO\"),",
            "            slave.compute_rpcapi._mock_calls)",
            "",
            "    def test_slave_asks_master_to_remove_slave_from_pool(self):",
            "        slave = ResourcePoolWithStubs()",
            "",
            "        slave.remove_from_aggregate(\"CONTEXT\", self.fake_aggregate, \"slave\")",
            "",
            "        self.assertIn(",
            "            (slave.compute_rpcapi.remove_aggregate_host,",
            "            \"CONTEXT\", 98, \"slave\", \"master\", \"SLAVE_INFO\"),",
            "            slave.compute_rpcapi._mock_calls)",
            "",
            "",
            "class SwapXapiHostTestCase(test.NoDBTestCase):",
            "",
            "    def test_swapping(self):",
            "        self.assertEquals(",
            "            \"http://otherserver:8765/somepath\",",
            "            pool.swap_xapi_host(",
            "                \"http://someserver:8765/somepath\", 'otherserver'))",
            "",
            "    def test_no_port(self):",
            "        self.assertEquals(",
            "            \"http://otherserver/somepath\",",
            "            pool.swap_xapi_host(",
            "                \"http://someserver/somepath\", 'otherserver'))",
            "",
            "    def test_no_path(self):",
            "        self.assertEquals(",
            "            \"http://otherserver\",",
            "            pool.swap_xapi_host(",
            "                \"http://someserver\", 'otherserver'))",
            "",
            "",
            "class XenAPILiveMigrateTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for live_migration.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPILiveMigrateTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver',",
            "                   host='host')",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        self.context = context.get_admin_context()",
            "",
            "    def test_live_migration_calls_vmops(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_live_migrate(context, instance_ref, dest, post_method,",
            "                              recover_method, block_migration, migrate_data):",
            "            fake_live_migrate.called = True",
            "",
            "        self.stubs.Set(self.conn._vmops, \"live_migrate\", fake_live_migrate)",
            "",
            "        self.conn.live_migration(None, None, None, None, None)",
            "        self.assertTrue(fake_live_migrate.called)",
            "",
            "    def test_pre_live_migration(self):",
            "        # ensure method is present",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.conn.pre_live_migration(None, None, None, None, None)",
            "",
            "    def test_post_live_migration_at_destination(self):",
            "        # ensure method is present",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.conn.post_live_migration_at_destination(None, None, None, None)",
            "",
            "    def test_check_can_live_migrate_destination_with_block_migration(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self.stubs.Set(vm_utils, \"safe_find_sr\", lambda _x: \"asdf\")",
            "",
            "        expected = {'block_migration': True,",
            "                    'migrate_data': {",
            "                        'migrate_send_data': \"fake_migrate_data\",",
            "                        'destination_sr_ref': 'asdf'",
            "                        }",
            "                    }",
            "        result = self.conn.check_can_live_migrate_destination(self.context,",
            "                              {'host': 'host'},",
            "                              {}, {},",
            "                              True, False)",
            "        self.assertEqual(expected, result)",
            "",
            "    def test_check_can_live_migrate_destination_block_migration_fails(self):",
            "        stubs.stubout_session(self.stubs,",
            "                              stubs.FakeSessionForFailedMigrateTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.check_can_live_migrate_destination,",
            "                          self.context, {'host': 'host'},",
            "                          {}, {},",
            "                          True, False)",
            "",
            "    def _add_default_live_migrate_stubs(self, conn):",
            "        def fake_generate_vdi_map(destination_sr_ref, _vm_ref):",
            "            pass",
            "",
            "        def fake_get_iscsi_srs(destination_sr_ref, _vm_ref):",
            "            return []",
            "",
            "        def fake_get_vm_opaque_ref(instance):",
            "            return \"fake_vm\"",
            "",
            "        self.stubs.Set(conn._vmops, \"_generate_vdi_map\",",
            "                       fake_generate_vdi_map)",
            "        self.stubs.Set(conn._vmops, \"_get_iscsi_srs\",",
            "                       fake_get_iscsi_srs)",
            "        self.stubs.Set(conn._vmops, \"_get_vm_opaque_ref\",",
            "                       fake_get_vm_opaque_ref)",
            "",
            "    def test_check_can_live_migrate_source_with_block_migrate(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        dest_check_data = {'block_migration': True,",
            "                           'migrate_data': {",
            "                            'destination_sr_ref': None,",
            "                            'migrate_send_data': None",
            "                           }}",
            "        result = self.conn.check_can_live_migrate_source(self.context,",
            "                                                         {'host': 'host'},",
            "                                                         dest_check_data)",
            "        self.assertEqual(dest_check_data, result)",
            "",
            "    def test_check_can_live_migrate_source_with_block_migrate_iscsi(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def fake_get_iscsi_srs(destination_sr_ref, _vm_ref):",
            "            return ['sr_ref']",
            "        self.stubs.Set(self.conn._vmops, \"_get_iscsi_srs\",",
            "                       fake_get_iscsi_srs)",
            "",
            "        def fake_make_plugin_call(plugin, method, **args):",
            "            return \"true\"",
            "        self.stubs.Set(self.conn._vmops, \"_make_plugin_call\",",
            "                       fake_make_plugin_call)",
            "",
            "        dest_check_data = {'block_migration': True,",
            "                           'migrate_data': {",
            "                            'destination_sr_ref': None,",
            "                            'migrate_send_data': None",
            "                           }}",
            "        result = self.conn.check_can_live_migrate_source(self.context,",
            "                                                         {'host': 'host'},",
            "                                                         dest_check_data)",
            "        self.assertEqual(dest_check_data, result)",
            "",
            "    def test_check_can_live_migrate_source_with_block_iscsi_fails(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def fake_get_iscsi_srs(destination_sr_ref, _vm_ref):",
            "            return ['sr_ref']",
            "        self.stubs.Set(self.conn._vmops, \"_get_iscsi_srs\",",
            "                       fake_get_iscsi_srs)",
            "",
            "        def fake_make_plugin_call(plugin, method, **args):",
            "            return {'returncode': 'error', 'message': 'Plugin not found'}",
            "        self.stubs.Set(self.conn._vmops, \"_make_plugin_call\",",
            "                       fake_make_plugin_call)",
            "",
            "        dest_check_data = {'block_migration': True,",
            "                           'migrate_data': {",
            "                            'destination_sr_ref': None,",
            "                            'migrate_send_data': None",
            "                           }}",
            "",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.check_can_live_migrate_source,",
            "                          self.context, {'host': 'host'},",
            "                          {})",
            "",
            "    def test_check_can_live_migrate_source_with_block_migrate_fails(self):",
            "        stubs.stubout_session(self.stubs,",
            "                              stubs.FakeSessionForFailedMigrateTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        dest_check_data = {'block_migration': True,",
            "                           'migrate_data': {",
            "                            'destination_sr_ref': None,",
            "                            'migrate_send_data': None",
            "                           }}",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.check_can_live_migrate_source,",
            "                          self.context,",
            "                          {'host': 'host'},",
            "                          dest_check_data)",
            "",
            "    def test_check_can_live_migrate_works(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        class fake_aggregate:",
            "            def __init__(self):",
            "                self.metadetails = {\"host\": \"test_host_uuid\"}",
            "",
            "        def fake_aggregate_get_by_host(context, host, key=None):",
            "            self.assertEqual(CONF.host, host)",
            "            return [fake_aggregate()]",
            "",
            "        self.stubs.Set(db, \"aggregate_get_by_host\",",
            "                fake_aggregate_get_by_host)",
            "        self.conn.check_can_live_migrate_destination(self.context,",
            "                {'host': 'host'}, False, False)",
            "",
            "    def test_check_can_live_migrate_fails(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        class fake_aggregate:",
            "            def __init__(self):",
            "                self.metadetails = {\"dest_other\": \"test_host_uuid\"}",
            "",
            "        def fake_aggregate_get_by_host(context, host, key=None):",
            "            self.assertEqual(CONF.host, host)",
            "            return [fake_aggregate()]",
            "",
            "        self.stubs.Set(db, \"aggregate_get_by_host\",",
            "                      fake_aggregate_get_by_host)",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.check_can_live_migrate_destination,",
            "                          self.context, {'host': 'host'}, None, None)",
            "",
            "    def test_live_migration(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_vm_opaque_ref(instance):",
            "            return \"fake_vm\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_vm_opaque_ref\",",
            "                       fake_get_vm_opaque_ref)",
            "",
            "        def fake_get_host_opaque_ref(context, destination_hostname):",
            "            return \"fake_host\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_host_opaque_ref\",",
            "                       fake_get_host_opaque_ref)",
            "",
            "        def post_method(context, instance, destination_hostname,",
            "                        block_migration):",
            "            post_method.called = True",
            "",
            "        self.conn.live_migration(self.conn, None, None, post_method, None)",
            "",
            "        self.assertTrue(post_method.called, \"post_method.called\")",
            "",
            "    def test_live_migration_on_failure(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_vm_opaque_ref(instance):",
            "            return \"fake_vm\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_vm_opaque_ref\",",
            "                       fake_get_vm_opaque_ref)",
            "",
            "        def fake_get_host_opaque_ref(context, destination_hostname):",
            "            return \"fake_host\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_host_opaque_ref\",",
            "                       fake_get_host_opaque_ref)",
            "",
            "        def fake_call_xenapi(*args):",
            "            raise NotImplementedError()",
            "        self.stubs.Set(self.conn._vmops._session, \"call_xenapi\",",
            "                       fake_call_xenapi)",
            "",
            "        def recover_method(context, instance, destination_hostname,",
            "                        block_migration):",
            "            recover_method.called = True",
            "",
            "        self.assertRaises(NotImplementedError, self.conn.live_migration,",
            "                          self.conn, None, None, None, recover_method)",
            "        self.assertTrue(recover_method.called, \"recover_method.called\")",
            "",
            "    def test_live_migration_calls_post_migration(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def post_method(context, instance, destination_hostname,",
            "                        block_migration):",
            "            post_method.called = True",
            "",
            "        # pass block_migration = True and migrate data",
            "        migrate_data = {\"destination_sr_ref\": \"foo\",",
            "                        \"migrate_send_data\": \"bar\"}",
            "        self.conn.live_migration(self.conn, None, None, post_method, None,",
            "                                 True, migrate_data)",
            "        self.assertTrue(post_method.called, \"post_method.called\")",
            "",
            "    def test_live_migration_block_cleans_srs(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def fake_get_iscsi_srs(context, instance):",
            "            return ['sr_ref']",
            "        self.stubs.Set(self.conn._vmops, \"_get_iscsi_srs\",",
            "                       fake_get_iscsi_srs)",
            "",
            "        def fake_forget_sr(context, instance):",
            "            fake_forget_sr.called = True",
            "        self.stubs.Set(volume_utils, \"forget_sr\",",
            "                       fake_forget_sr)",
            "",
            "        def post_method(context, instance, destination_hostname,",
            "                        block_migration):",
            "            post_method.called = True",
            "",
            "        migrate_data = {\"destination_sr_ref\": \"foo\",",
            "                        \"migrate_send_data\": \"bar\"}",
            "        self.conn.live_migration(self.conn, None, None, post_method, None,",
            "                                 True, migrate_data)",
            "",
            "        self.assertTrue(post_method.called, \"post_method.called\")",
            "        self.assertTrue(fake_forget_sr.called, \"forget_sr.called\")",
            "",
            "    def test_live_migration_with_block_migration_raises_invalid_param(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_vm_opaque_ref(instance):",
            "            return \"fake_vm\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_vm_opaque_ref\",",
            "                       fake_get_vm_opaque_ref)",
            "",
            "        def recover_method(context, instance, destination_hostname,",
            "                           block_migration):",
            "            recover_method.called = True",
            "        # pass block_migration = True and no migrate data",
            "        self.assertRaises(exception.InvalidParameterValue,",
            "                          self.conn.live_migration, self.conn,",
            "                          None, None, None, recover_method, True, None)",
            "        self.assertTrue(recover_method.called, \"recover_method.called\")",
            "",
            "    def test_live_migration_with_block_migration_fails_migrate_send(self):",
            "        stubs.stubout_session(self.stubs,",
            "                              stubs.FakeSessionForFailedMigrateTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def recover_method(context, instance, destination_hostname,",
            "                           block_migration):",
            "            recover_method.called = True",
            "        # pass block_migration = True and migrate data",
            "        migrate_data = dict(destination_sr_ref='foo', migrate_send_data='bar')",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.live_migration, self.conn,",
            "                          None, None, None, recover_method, True, migrate_data)",
            "        self.assertTrue(recover_method.called, \"recover_method.called\")",
            "",
            "    def test_live_migrate_block_migration_xapi_call_parameters(self):",
            "",
            "        fake_vdi_map = object()",
            "",
            "        class Session(xenapi_fake.SessionBase):",
            "            def VM_migrate_send(self_, session, vmref, migrate_data, islive,",
            "                                vdi_map, vif_map, options):",
            "                self.assertEquals('SOMEDATA', migrate_data)",
            "                self.assertEquals(fake_vdi_map, vdi_map)",
            "",
            "        stubs.stubout_session(self.stubs, Session)",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(conn)",
            "",
            "        def fake_generate_vdi_map(destination_sr_ref, _vm_ref):",
            "            return fake_vdi_map",
            "",
            "        self.stubs.Set(conn._vmops, \"_generate_vdi_map\",",
            "                       fake_generate_vdi_map)",
            "",
            "        def dummy_callback(*args, **kwargs):",
            "            pass",
            "",
            "        conn.live_migration(",
            "            self.context, instance_ref=dict(name='ignore'), dest=None,",
            "            post_method=dummy_callback, recover_method=dummy_callback,",
            "            block_migration=\"SOMEDATA\",",
            "            migrate_data=dict(migrate_send_data='SOMEDATA',",
            "                              destination_sr_ref=\"TARGET_SR_OPAQUE_REF\"))",
            "",
            "    def test_live_migrate_pool_migration_xapi_call_parameters(self):",
            "",
            "        class Session(xenapi_fake.SessionBase):",
            "            def VM_pool_migrate(self_, session, vm_ref, host_ref, options):",
            "                self.assertEquals(\"fake_ref\", host_ref)",
            "                self.assertEquals({\"live\": \"true\"}, options)",
            "                raise IOError()",
            "",
            "        stubs.stubout_session(self.stubs, Session)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self._add_default_live_migrate_stubs(conn)",
            "",
            "        def fake_get_host_opaque_ref(context, destination):",
            "            return \"fake_ref\"",
            "",
            "        self.stubs.Set(conn._vmops, \"_get_host_opaque_ref\",",
            "                       fake_get_host_opaque_ref)",
            "",
            "        def dummy_callback(*args, **kwargs):",
            "            pass",
            "",
            "        self.assertRaises(IOError, conn.live_migration,",
            "            self.context, instance_ref=dict(name='ignore'), dest=None,",
            "            post_method=dummy_callback, recover_method=dummy_callback,",
            "            block_migration=False, migrate_data={})",
            "",
            "    def test_generate_vdi_map(self):",
            "        stubs.stubout_session(self.stubs, xenapi_fake.SessionBase)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        vm_ref = \"fake_vm_ref\"",
            "",
            "        def fake_find_sr(_session):",
            "            self.assertEquals(conn._session, _session)",
            "            return \"source_sr_ref\"",
            "        self.stubs.Set(vm_utils, \"safe_find_sr\", fake_find_sr)",
            "",
            "        def fake_get_instance_vdis_for_sr(_session, _vm_ref, _sr_ref):",
            "            self.assertEquals(conn._session, _session)",
            "            self.assertEquals(vm_ref, _vm_ref)",
            "            self.assertEquals(\"source_sr_ref\", _sr_ref)",
            "            return [\"vdi0\", \"vdi1\"]",
            "",
            "        self.stubs.Set(vm_utils, \"get_instance_vdis_for_sr\",",
            "                       fake_get_instance_vdis_for_sr)",
            "",
            "        result = conn._vmops._generate_vdi_map(\"dest_sr_ref\", vm_ref)",
            "",
            "        self.assertEquals({\"vdi0\": \"dest_sr_ref\",",
            "                           \"vdi1\": \"dest_sr_ref\"}, result)",
            "",
            "",
            "class XenAPIInjectMetadataTestCase(stubs.XenAPITestBase):",
            "    def setUp(self):",
            "        super(XenAPIInjectMetadataTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self.xenstore = dict(persist={}, ephem={})",
            "",
            "        self.called_fake_get_vm_opaque_ref = False",
            "",
            "        def fake_get_vm_opaque_ref(inst, instance):",
            "            self.called_fake_get_vm_opaque_ref = True",
            "            if instance[\"uuid\"] == \"not_found\":",
            "                raise exception.NotFound",
            "            self.assertEqual(instance, {'uuid': 'fake'})",
            "            return 'vm_ref'",
            "",
            "        def fake_add_to_param_xenstore(inst, vm_ref, key, val):",
            "            self.assertEqual(vm_ref, 'vm_ref')",
            "            self.xenstore['persist'][key] = val",
            "",
            "        def fake_remove_from_param_xenstore(inst, vm_ref, key):",
            "            self.assertEqual(vm_ref, 'vm_ref')",
            "            if key in self.xenstore['persist']:",
            "                del self.xenstore['persist'][key]",
            "",
            "        def fake_write_to_xenstore(inst, instance, path, value, vm_ref=None):",
            "            self.assertEqual(instance, {'uuid': 'fake'})",
            "            self.assertEqual(vm_ref, 'vm_ref')",
            "            self.xenstore['ephem'][path] = jsonutils.dumps(value)",
            "",
            "        def fake_delete_from_xenstore(inst, instance, path, vm_ref=None):",
            "            self.assertEqual(instance, {'uuid': 'fake'})",
            "            self.assertEqual(vm_ref, 'vm_ref')",
            "            if path in self.xenstore['ephem']:",
            "                del self.xenstore['ephem'][path]",
            "",
            "        self.stubs.Set(vmops.VMOps, '_get_vm_opaque_ref',",
            "                       fake_get_vm_opaque_ref)",
            "        self.stubs.Set(vmops.VMOps, '_add_to_param_xenstore',",
            "                       fake_add_to_param_xenstore)",
            "        self.stubs.Set(vmops.VMOps, '_remove_from_param_xenstore',",
            "                       fake_remove_from_param_xenstore)",
            "        self.stubs.Set(vmops.VMOps, '_write_to_xenstore',",
            "                       fake_write_to_xenstore)",
            "        self.stubs.Set(vmops.VMOps, '_delete_from_xenstore',",
            "                       fake_delete_from_xenstore)",
            "",
            "    def test_inject_instance_metadata(self):",
            "",
            "        # Add some system_metadata to ensure it doesn't get added",
            "        # to xenstore",
            "        instance = dict(metadata=[{'key': 'a', 'value': 1},",
            "                                  {'key': 'b', 'value': 2},",
            "                                  {'key': 'c', 'value': 3},",
            "                                  # Check xenstore key sanitizing",
            "                                  {'key': 'hi.there', 'value': 4},",
            "                                  {'key': 'hi!t.e/e', 'value': 5}],",
            "                                  # Check xenstore key sanitizing",
            "                        system_metadata=[{'key': 'sys_a', 'value': 1},",
            "                                         {'key': 'sys_b', 'value': 2},",
            "                                         {'key': 'sys_c', 'value': 3}],",
            "                        uuid='fake')",
            "        self.conn._vmops._inject_instance_metadata(instance, 'vm_ref')",
            "",
            "        self.assertEqual(self.xenstore, {",
            "                'persist': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '2',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    'vm-data/user-metadata/hi_there': '4',",
            "                    'vm-data/user-metadata/hi_t_e_e': '5',",
            "                    },",
            "                'ephem': {},",
            "                })",
            "",
            "    def test_change_instance_metadata_add(self):",
            "        # Test XenStore key sanitizing here, too.",
            "        diff = {'test.key': ['+', 4]}",
            "        instance = {'uuid': 'fake'}",
            "        self.xenstore = {",
            "            'persist': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            'ephem': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            }",
            "",
            "        self.conn._vmops.change_instance_metadata(instance, diff)",
            "",
            "        self.assertEqual(self.xenstore, {",
            "                'persist': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '2',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    'vm-data/user-metadata/test_key': '4',",
            "                    },",
            "                'ephem': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '2',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    'vm-data/user-metadata/test_key': '4',",
            "                    },",
            "                })",
            "",
            "    def test_change_instance_metadata_update(self):",
            "        diff = dict(b=['+', 4])",
            "        instance = {'uuid': 'fake'}",
            "        self.xenstore = {",
            "            'persist': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            'ephem': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            }",
            "",
            "        self.conn._vmops.change_instance_metadata(instance, diff)",
            "",
            "        self.assertEqual(self.xenstore, {",
            "                'persist': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '4',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    },",
            "                'ephem': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '4',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    },",
            "                })",
            "",
            "    def test_change_instance_metadata_delete(self):",
            "        diff = dict(b=['-'])",
            "        instance = {'uuid': 'fake'}",
            "        self.xenstore = {",
            "            'persist': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            'ephem': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            }",
            "",
            "        self.conn._vmops.change_instance_metadata(instance, diff)",
            "",
            "        self.assertEqual(self.xenstore, {",
            "                'persist': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    },",
            "                'ephem': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    },",
            "                })",
            "",
            "    def test_change_instance_metadata_not_found(self):",
            "        instance = {'uuid': 'not_found'}",
            "        self.conn._vmops.change_instance_metadata(instance, \"fake_diff\")",
            "        self.assertTrue(self.called_fake_get_vm_opaque_ref)",
            "",
            "",
            "class XenAPISessionTestCase(test.NoDBTestCase):",
            "    def _get_mock_xapisession(self, software_version):",
            "        class MockXapiSession(xenapi_conn.XenAPISession):",
            "            def __init__(_ignore):",
            "                \"Skip the superclass's dirty init\"",
            "",
            "            def _get_software_version(_ignore):",
            "                return software_version",
            "",
            "        return MockXapiSession()",
            "",
            "    def test_local_session(self):",
            "        session = self._get_mock_xapisession({})",
            "        session.is_local_connection = True",
            "        session.XenAPI = self.mox.CreateMockAnything()",
            "        session.XenAPI.xapi_local().AndReturn(\"local_connection\")",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEqual(\"local_connection\",",
            "                         session._create_session(\"unix://local\"))",
            "",
            "    def test_remote_session(self):",
            "        session = self._get_mock_xapisession({})",
            "        session.is_local_connection = False",
            "        session.XenAPI = self.mox.CreateMockAnything()",
            "        session.XenAPI.Session(\"url\").AndReturn(\"remote_connection\")",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEqual(\"remote_connection\", session._create_session(\"url\"))",
            "",
            "    def test_get_product_version_product_brand_does_not_fail(self):",
            "        session = self._get_mock_xapisession({",
            "                    'build_number': '0',",
            "                    'date': '2012-08-03',",
            "                    'hostname': 'komainu',",
            "                    'linux': '3.2.0-27-generic',",
            "                    'network_backend': 'bridge',",
            "                    'platform_name': 'XCP_Kronos',",
            "                    'platform_version': '1.6.0',",
            "                    'xapi': '1.3',",
            "                    'xen': '4.1.2',",
            "                    'xencenter_max': '1.10',",
            "                    'xencenter_min': '1.10'",
            "                })",
            "",
            "        self.assertEquals(",
            "            ((1, 6, 0), None),",
            "            session._get_product_version_and_brand()",
            "        )",
            "",
            "    def test_get_product_version_product_brand_xs_6(self):",
            "        session = self._get_mock_xapisession({",
            "                    'product_brand': 'XenServer',",
            "                    'product_version': '6.0.50',",
            "                    'platform_version': '0.0.1'",
            "                })",
            "",
            "        self.assertEquals(",
            "            ((6, 0, 50), 'XenServer'),",
            "            session._get_product_version_and_brand()",
            "        )",
            "",
            "",
            "class XenAPIFakeTestCase(test.NoDBTestCase):",
            "    def test_query_matches(self):",
            "        record = {'a': '1', 'b': '2', 'c_d': '3'}",
            "",
            "        tests = {'field \"a\"=\"1\"': True,",
            "                 'field \"b\"=\"2\"': True,",
            "                 'field \"b\"=\"4\"': False,",
            "                 'not field \"b\"=\"4\"': True,",
            "                 'field \"a\"=\"1\" and field \"b\"=\"4\"': False,",
            "                 'field \"a\"=\"1\" or field \"b\"=\"4\"': True,",
            "                 'field \"c__d\"=\"3\"': True,",
            "                 'field \\'b\\'=\\'2\\'': True,",
            "                 }",
            "",
            "        for query in tests.keys():",
            "            expected = tests[query]",
            "            fail_msg = \"for test '%s'\" % query",
            "            self.assertEqual(xenapi_fake._query_matches(record, query),",
            "                             expected, fail_msg)",
            "",
            "    def test_query_bad_format(self):",
            "        record = {'a': '1', 'b': '2', 'c': '3'}",
            "",
            "        tests = ['\"a\"=\"1\" or \"b\"=\"4\"',",
            "                 'a=1',",
            "                 ]",
            "",
            "        for query in tests:",
            "            fail_msg = \"for test '%s'\" % query",
            "            self.assertFalse(xenapi_fake._query_matches(record, query),",
            "                             fail_msg)"
        ],
        "afterPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "#    Copyright (c) 2010 Citrix Systems, Inc.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"Test suite for XenAPI.\"\"\"",
            "",
            "import ast",
            "import base64",
            "import contextlib",
            "import copy",
            "import functools",
            "import mox",
            "import os",
            "import re",
            "",
            "import mox",
            "from oslo.config import cfg",
            "",
            "from nova.compute import api as compute_api",
            "from nova.compute import flavors",
            "from nova.compute import power_state",
            "from nova.compute import task_states",
            "from nova.compute import vm_states",
            "from nova import context",
            "from nova import crypto",
            "from nova import db",
            "from nova import exception",
            "from nova.openstack.common.gettextutils import _",
            "from nova.openstack.common import importutils",
            "from nova.openstack.common import jsonutils",
            "from nova.openstack.common import log as logging",
            "from nova import test",
            "from nova.tests.db import fakes as db_fakes",
            "from nova.tests import fake_instance",
            "from nova.tests import fake_network",
            "from nova.tests import fake_processutils",
            "import nova.tests.image.fake as fake_image",
            "from nova.tests import matchers",
            "from nova.tests.virt.xenapi import stubs",
            "from nova.virt import fake",
            "from nova.virt.xenapi import agent",
            "from nova.virt.xenapi import driver as xenapi_conn",
            "from nova.virt.xenapi import fake as xenapi_fake",
            "from nova.virt.xenapi import host",
            "from nova.virt.xenapi.image import glance",
            "from nova.virt.xenapi import pool",
            "from nova.virt.xenapi import pool_states",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import vmops",
            "from nova.virt.xenapi import volume_utils",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "CONF = cfg.CONF",
            "CONF.import_opt('compute_manager', 'nova.service')",
            "CONF.import_opt('network_manager', 'nova.service')",
            "CONF.import_opt('compute_driver', 'nova.virt.driver')",
            "CONF.import_opt('host', 'nova.netconf')",
            "CONF.import_opt('default_availability_zone', 'nova.availability_zones')",
            "",
            "IMAGE_MACHINE = '1'",
            "IMAGE_KERNEL = '2'",
            "IMAGE_RAMDISK = '3'",
            "IMAGE_RAW = '4'",
            "IMAGE_VHD = '5'",
            "IMAGE_ISO = '6'",
            "IMAGE_IPXE_ISO = '7'",
            "",
            "IMAGE_FIXTURES = {",
            "    IMAGE_MACHINE: {",
            "        'image_meta': {'name': 'fakemachine', 'size': 0,",
            "                       'disk_format': 'ami',",
            "                       'container_format': 'ami'},",
            "    },",
            "    IMAGE_KERNEL: {",
            "        'image_meta': {'name': 'fakekernel', 'size': 0,",
            "                       'disk_format': 'aki',",
            "                       'container_format': 'aki'},",
            "    },",
            "    IMAGE_RAMDISK: {",
            "        'image_meta': {'name': 'fakeramdisk', 'size': 0,",
            "                       'disk_format': 'ari',",
            "                       'container_format': 'ari'},",
            "    },",
            "    IMAGE_RAW: {",
            "        'image_meta': {'name': 'fakeraw', 'size': 0,",
            "                       'disk_format': 'raw',",
            "                       'container_format': 'bare'},",
            "    },",
            "    IMAGE_VHD: {",
            "        'image_meta': {'name': 'fakevhd', 'size': 0,",
            "                       'disk_format': 'vhd',",
            "                       'container_format': 'ovf'},",
            "    },",
            "    IMAGE_ISO: {",
            "        'image_meta': {'name': 'fakeiso', 'size': 0,",
            "                       'disk_format': 'iso',",
            "                       'container_format': 'bare'},",
            "    },",
            "    IMAGE_IPXE_ISO: {",
            "        'image_meta': {'name': 'fake_ipxe_iso', 'size': 0,",
            "                       'disk_format': 'iso',",
            "                       'container_format': 'bare',",
            "                       'properties': {'ipxe_boot': 'true'}},",
            "    },",
            "}",
            "",
            "",
            "def set_image_fixtures():",
            "    image_service = fake_image.FakeImageService()",
            "    image_service.images.clear()",
            "    for image_id, image_meta in IMAGE_FIXTURES.items():",
            "        image_meta = image_meta['image_meta']",
            "        image_meta['id'] = image_id",
            "        image_service.create(None, image_meta)",
            "",
            "",
            "def get_fake_device_info():",
            "    # FIXME: 'sr_uuid', 'introduce_sr_keys', sr_type and vdi_uuid",
            "    # can be removed from the dict when LP bug #1087308 is fixed",
            "    fake_vdi_ref = xenapi_fake.create_vdi('fake-vdi', None)",
            "    fake_vdi_uuid = xenapi_fake.get_record('VDI', fake_vdi_ref)['uuid']",
            "    fake = {'block_device_mapping':",
            "              [{'connection_info': {'driver_volume_type': 'iscsi',",
            "                                    'data': {'sr_uuid': 'falseSR',",
            "                                             'introduce_sr_keys': ['sr_type'],",
            "                                             'sr_type': 'iscsi',",
            "                                             'vdi_uuid': fake_vdi_uuid,",
            "                                             'target_discovered': False,",
            "                                             'target_iqn': 'foo_iqn:foo_volid',",
            "                                             'target_portal': 'localhost:3260',",
            "                                             'volume_id': 'foo_volid',",
            "                                             'target_lun': 1,",
            "                                             'auth_password': 'my-p@55w0rd',",
            "                                             'auth_username': 'johndoe',",
            "                                             'auth_method': u'CHAP'}, },",
            "                'mount_device': 'vda',",
            "                'delete_on_termination': False}, ],",
            "            'root_device_name': '/dev/sda',",
            "            'ephemerals': [],",
            "            'swap': None, }",
            "    return fake",
            "",
            "",
            "def stub_vm_utils_with_vdi_attached_here(function):",
            "    \"\"\"",
            "    vm_utils.with_vdi_attached_here needs to be stubbed out because it",
            "    calls down to the filesystem to attach a vdi. This provides a",
            "    decorator to handle that.",
            "    \"\"\"",
            "    @functools.wraps(function)",
            "    def decorated_function(self, *args, **kwargs):",
            "        @contextlib.contextmanager",
            "        def fake_vdi_attached_here(*args, **kwargs):",
            "            fake_dev = 'fakedev'",
            "            yield fake_dev",
            "",
            "        def fake_image_download(*args, **kwargs):",
            "            pass",
            "",
            "        orig_vdi_attached_here = vm_utils.vdi_attached_here",
            "        orig_image_download = fake_image._FakeImageService.download",
            "        try:",
            "            vm_utils.vdi_attached_here = fake_vdi_attached_here",
            "            fake_image._FakeImageService.download = fake_image_download",
            "            return function(self, *args, **kwargs)",
            "        finally:",
            "            fake_image._FakeImageService.download = orig_image_download",
            "            vm_utils.vdi_attached_here = orig_vdi_attached_here",
            "",
            "    return decorated_function",
            "",
            "",
            "def create_instance_with_system_metadata(context, instance_values):",
            "    instance_type = db.flavor_get(context,",
            "                                         instance_values['instance_type_id'])",
            "    sys_meta = flavors.save_flavor_info({},",
            "                                                      instance_type)",
            "    instance_values['system_metadata'] = sys_meta",
            "    return db.instance_create(context, instance_values)",
            "",
            "",
            "class XenAPIVolumeTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for Volume operations.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIVolumeTestCase, self).setUp()",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "        self.flags(disable_process_locking=True,",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver',",
            "                   xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass')",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        self.instance_values = {'id': 1,",
            "                  'project_id': self.user_id,",
            "                  'user_id': 'fake',",
            "                  'image_ref': 1,",
            "                  'kernel_id': 2,",
            "                  'ramdisk_id': 3,",
            "                  'root_gb': 20,",
            "                  'instance_type_id': '3',  # m1.large",
            "                  'os_type': 'linux',",
            "                  'architecture': 'x86-64'}",
            "",
            "    def _create_volume(self, size=0):",
            "        \"\"\"Create a volume object.\"\"\"",
            "        vol = {}",
            "        vol['size'] = size",
            "        vol['user_id'] = 'fake'",
            "        vol['project_id'] = 'fake'",
            "        vol['host'] = 'localhost'",
            "        vol['availability_zone'] = CONF.default_availability_zone",
            "        vol['status'] = \"creating\"",
            "        vol['attach_status'] = \"detached\"",
            "        return db.volume_create(self.context, vol)",
            "",
            "    @staticmethod",
            "    def _make_connection_data():",
            "        return {",
            "            'volume_id': 1,",
            "            'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',",
            "            'target_portal': '127.0.0.1:3260,fake',",
            "            'target_lun': None,",
            "            'auth_method': 'CHAP',",
            "            'auth_username': 'username',",
            "            'auth_password': 'password',",
            "        }",
            "",
            "    @classmethod",
            "    def _make_connection_info(cls):",
            "        return {",
            "            'driver_volume_type': 'iscsi',",
            "            'data': cls._make_connection_data()",
            "        }",
            "",
            "    def test_mountpoint_to_number(self):",
            "        cases = {",
            "            'sda': 0,",
            "            'sdp': 15,",
            "            'hda': 0,",
            "            'hdp': 15,",
            "            'vda': 0,",
            "            'xvda': 0,",
            "            '0': 0,",
            "            '10': 10,",
            "            'vdq': -1,",
            "            'sdq': -1,",
            "            'hdq': -1,",
            "            'xvdq': -1,",
            "        }",
            "",
            "        for (input, expected) in cases.iteritems():",
            "            actual = volume_utils.mountpoint_to_number(input)",
            "            self.assertEqual(actual, expected,",
            "                    '%s yielded %s, not %s' % (input, actual, expected))",
            "",
            "    def test_parse_volume_info_parsing_auth_details(self):",
            "        result = volume_utils.parse_volume_info(",
            "            self._make_connection_data())",
            "",
            "        self.assertEquals('username', result['chapuser'])",
            "        self.assertEquals('password', result['chappassword'])",
            "",
            "    def test_get_device_number_raise_exception_on_wrong_mountpoint(self):",
            "        self.assertRaises(",
            "            volume_utils.StorageError,",
            "            volume_utils.get_device_number,",
            "            'dev/sd')",
            "",
            "    def test_attach_volume(self):",
            "        # This shows how to test Ops classes' methods.",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVolumeTests)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        vm = xenapi_fake.create_vm(instance['name'], 'Running')",
            "        result = conn.attach_volume(None, self._make_connection_info(),",
            "                                    instance, '/dev/sdc')",
            "",
            "        # check that the VM has a VBD attached to it",
            "        # Get XenAPI record for VBD",
            "        vbds = xenapi_fake.get_all('VBD')",
            "        vbd = xenapi_fake.get_record('VBD', vbds[0])",
            "        vm_ref = vbd['VM']",
            "        self.assertEqual(vm_ref, vm)",
            "",
            "    def test_attach_volume_raise_exception(self):",
            "        # This shows how to test when exceptions are raised.",
            "        stubs.stubout_session(self.stubs,",
            "                              stubs.FakeSessionForVolumeFailedTests)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        self.assertRaises(exception.VolumeDriverNotFound,",
            "                          conn.attach_volume,",
            "                          None, {'driver_volume_type': 'nonexist'},",
            "                          instance, '/dev/sdc')",
            "",
            "",
            "class XenAPIVMTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for VM operations.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIVMTestCase, self).setUp()",
            "        self.useFixture(test.SampleNetworks())",
            "        self.network = importutils.import_object(CONF.network_manager)",
            "        self.flags(disable_process_locking=True,",
            "                   instance_name_template='%d',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver',",
            "                   xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',)",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        xenapi_fake.create_network('fake', 'fake_br1')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        stubs.stubout_get_this_vm_uuid(self.stubs)",
            "        stubs.stub_out_vm_methods(self.stubs)",
            "        fake_processutils.stub_out_processutils_execute(self.stubs)",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.conn._session.is_local_connection = False",
            "",
            "        self.stubs.Set(fake.FakeVirtAPI, 'instance_update',",
            "                       lambda *args, **kwargs: ('fake-oldref', 'fake-newref'))",
            "        fake_image.stub_out_image_service(self.stubs)",
            "        set_image_fixtures()",
            "        stubs.stubout_image_service_download(self.stubs)",
            "        stubs.stubout_stream_disk(self.stubs)",
            "",
            "        def fake_inject_instance_metadata(self, instance, vm):",
            "            pass",
            "        self.stubs.Set(vmops.VMOps, '_inject_instance_metadata',",
            "                       fake_inject_instance_metadata)",
            "",
            "        def fake_safe_copy_vdi(session, sr_ref, instance, vdi_to_copy_ref):",
            "            name_label = \"fakenamelabel\"",
            "            disk_type = \"fakedisktype\"",
            "            virtual_size = 777",
            "            return vm_utils.create_vdi(",
            "                    session, sr_ref, instance, name_label, disk_type,",
            "                    virtual_size)",
            "        self.stubs.Set(vm_utils, '_safe_copy_vdi', fake_safe_copy_vdi)",
            "",
            "    def tearDown(self):",
            "        fake_image.FakeImageService_reset()",
            "        super(XenAPIVMTestCase, self).tearDown()",
            "",
            "    def test_init_host(self):",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        vm = vm_utils._get_this_vm_ref(session)",
            "        # Local root disk",
            "        vdi0 = xenapi_fake.create_vdi('compute', None)",
            "        vbd0 = xenapi_fake.create_vbd(vm, vdi0)",
            "        # Instance VDI",
            "        vdi1 = xenapi_fake.create_vdi('instance-aaaa', None,",
            "                other_config={'nova_instance_uuid': 'aaaa'})",
            "        vbd1 = xenapi_fake.create_vbd(vm, vdi1)",
            "        # Only looks like instance VDI",
            "        vdi2 = xenapi_fake.create_vdi('instance-bbbb', None)",
            "        vbd2 = xenapi_fake.create_vbd(vm, vdi2)",
            "",
            "        self.conn.init_host(None)",
            "        self.assertEquals(set(xenapi_fake.get_all('VBD')), set([vbd0, vbd2]))",
            "",
            "    def test_instance_exists(self):",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        vm_utils.lookup(mox.IgnoreArg(), 'foo').AndReturn(True)",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertTrue(self.conn.instance_exists('foo'))",
            "",
            "    def test_instance_not_exists(self):",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        vm_utils.lookup(mox.IgnoreArg(), 'bar').AndReturn(None)",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertFalse(self.conn.instance_exists('bar'))",
            "",
            "    def test_list_instances_0(self):",
            "        instances = self.conn.list_instances()",
            "        self.assertEquals(instances, [])",
            "",
            "    def test_list_instance_uuids_0(self):",
            "        instance_uuids = self.conn.list_instance_uuids()",
            "        self.assertEquals(instance_uuids, [])",
            "",
            "    def test_list_instance_uuids(self):",
            "        uuids = []",
            "        for x in xrange(1, 4):",
            "            instance = self._create_instance(x)",
            "            uuids.append(instance['uuid'])",
            "        instance_uuids = self.conn.list_instance_uuids()",
            "        self.assertEqual(len(uuids), len(instance_uuids))",
            "        self.assertEqual(set(uuids), set(instance_uuids))",
            "",
            "    def test_get_rrd_server(self):",
            "        self.flags(xenapi_connection_url='myscheme://myaddress/')",
            "        server_info = vm_utils._get_rrd_server()",
            "        self.assertEqual(server_info[0], 'myscheme')",
            "        self.assertEqual(server_info[1], 'myaddress')",
            "",
            "    def test_get_diagnostics(self):",
            "        def fake_get_rrd(host, vm_uuid):",
            "            path = os.path.dirname(os.path.realpath(__file__))",
            "            with open(os.path.join(path, 'vm_rrd.xml')) as f:",
            "                return re.sub(r'\\s', '', f.read())",
            "        self.stubs.Set(vm_utils, '_get_rrd', fake_get_rrd)",
            "",
            "        fake_diagnostics = {",
            "            'vbd_xvdb_write': '0.0',",
            "            'memory_target': '4294967296.0000',",
            "            'memory_internal_free': '1415564.0000',",
            "            'memory': '4294967296.0000',",
            "            'vbd_xvda_write': '0.0',",
            "            'cpu0': '0.0042',",
            "            'vif_0_tx': '287.4134',",
            "            'vbd_xvda_read': '0.0',",
            "            'vif_0_rx': '1816.0144',",
            "            'vif_2_rx': '0.0',",
            "            'vif_2_tx': '0.0',",
            "            'vbd_xvdb_read': '0.0',",
            "            'last_update': '1328795567',",
            "        }",
            "        instance = self._create_instance()",
            "        expected = self.conn.get_diagnostics(instance)",
            "        self.assertThat(fake_diagnostics, matchers.DictMatches(expected))",
            "",
            "    def test_get_vnc_console(self):",
            "        instance = self._create_instance()",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                fake.FakeVirtAPI())",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vm_ref = vm_utils.lookup(session, instance['name'])",
            "",
            "        console = conn.get_vnc_console(instance)",
            "",
            "        # Note(sulo): We dont care about session id in test",
            "        # they will always differ so strip that out",
            "        actual_path = console['internal_access_path'].split('&')[0]",
            "        expected_path = \"/console?ref=%s\" % str(vm_ref)",
            "",
            "        self.assertEqual(expected_path, actual_path)",
            "",
            "    def test_get_vnc_console_for_rescue(self):",
            "        instance = self._create_instance()",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        rescue_vm = xenapi_fake.create_vm(instance['name'] + '-rescue',",
            "                                          'Running')",
            "        # Set instance state to rescued",
            "        instance['vm_state'] = 'rescued'",
            "",
            "        console = conn.get_vnc_console(instance)",
            "",
            "        # Note(sulo): We dont care about session id in test",
            "        # they will always differ so strip that out",
            "        actual_path = console['internal_access_path'].split('&')[0]",
            "        expected_path = \"/console?ref=%s\" % str(rescue_vm)",
            "",
            "        self.assertEqual(expected_path, actual_path)",
            "",
            "    def test_get_vnc_console_instance_not_ready(self):",
            "        instance = {}",
            "        # set instance name and state",
            "        instance['name'] = 'fake-instance'",
            "        instance['uuid'] = '00000000-0000-0000-0000-000000000000'",
            "        instance['vm_state'] = 'building'",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.InstanceNotFound,",
            "                          conn.get_vnc_console, instance)",
            "",
            "    def test_get_vnc_console_rescue_not_ready(self):",
            "        instance = {}",
            "        instance['name'] = 'fake-rescue'",
            "        instance['uuid'] = '00000000-0000-0000-0000-000000000001'",
            "        instance['vm_state'] = 'rescued'",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.InstanceNotReady,",
            "                          conn.get_vnc_console, instance)",
            "",
            "    def test_instance_snapshot_fails_with_no_primary_vdi(self):",
            "",
            "        def create_bad_vbd(session, vm_ref, vdi_ref, userdevice,",
            "                           vbd_type='disk', read_only=False, bootable=False,",
            "                           osvol=False):",
            "            vbd_rec = {'VM': vm_ref,",
            "               'VDI': vdi_ref,",
            "               'userdevice': 'fake',",
            "               'currently_attached': False}",
            "            vbd_ref = xenapi_fake._create_object('VBD', vbd_rec)",
            "            xenapi_fake.after_VBD_create(vbd_ref, vbd_rec)",
            "            return vbd_ref",
            "",
            "        self.stubs.Set(vm_utils, 'create_vbd', create_bad_vbd)",
            "        stubs.stubout_instance_snapshot(self.stubs)",
            "        # Stubbing out firewall driver as previous stub sets alters",
            "        # xml rpc result parsing",
            "        stubs.stubout_firewall_driver(self.stubs, self.conn)",
            "        instance = self._create_instance()",
            "",
            "        image_id = \"my_snapshot_id\"",
            "        self.assertRaises(exception.NovaException, self.conn.snapshot,",
            "                          self.context, instance, image_id,",
            "                          lambda *args, **kwargs: None)",
            "",
            "    def test_instance_snapshot(self):",
            "        expected_calls = [",
            "            {'args': (),",
            "             'kwargs':",
            "                 {'task_state': task_states.IMAGE_PENDING_UPLOAD}},",
            "            {'args': (),",
            "             'kwargs':",
            "                 {'task_state': task_states.IMAGE_UPLOADING,",
            "                  'expected_state': task_states.IMAGE_PENDING_UPLOAD}}]",
            "        func_call_matcher = matchers.FunctionCallMatcher(expected_calls)",
            "        image_id = \"my_snapshot_id\"",
            "",
            "        stubs.stubout_instance_snapshot(self.stubs)",
            "        stubs.stubout_is_snapshot(self.stubs)",
            "        # Stubbing out firewall driver as previous stub sets alters",
            "        # xml rpc result parsing",
            "        stubs.stubout_firewall_driver(self.stubs, self.conn)",
            "",
            "        instance = self._create_instance()",
            "",
            "        self.fake_upload_called = False",
            "",
            "        def fake_image_upload(_self, ctx, session, inst, vdi_uuids,",
            "                              img_id):",
            "            self.fake_upload_called = True",
            "            self.assertEqual(ctx, self.context)",
            "            self.assertEqual(inst, instance)",
            "            self.assertTrue(isinstance(vdi_uuids, list))",
            "            self.assertEqual(img_id, image_id)",
            "",
            "        self.stubs.Set(glance.GlanceStore, 'upload_image',",
            "                       fake_image_upload)",
            "",
            "        self.conn.snapshot(self.context, instance, image_id,",
            "                           func_call_matcher.call)",
            "",
            "        # Ensure VM was torn down",
            "        vm_labels = []",
            "        for vm_ref in xenapi_fake.get_all('VM'):",
            "            vm_rec = xenapi_fake.get_record('VM', vm_ref)",
            "            if not vm_rec[\"is_control_domain\"]:",
            "                vm_labels.append(vm_rec[\"name_label\"])",
            "",
            "        self.assertEquals(vm_labels, [instance['name']])",
            "",
            "        # Ensure VBDs were torn down",
            "        vbd_labels = []",
            "        for vbd_ref in xenapi_fake.get_all('VBD'):",
            "            vbd_rec = xenapi_fake.get_record('VBD', vbd_ref)",
            "            vbd_labels.append(vbd_rec[\"vm_name_label\"])",
            "",
            "        self.assertEquals(vbd_labels, [instance['name']])",
            "",
            "        # Ensure task states changed in correct order",
            "        self.assertIsNone(func_call_matcher.match())",
            "",
            "        # Ensure VDIs were torn down",
            "        for vdi_ref in xenapi_fake.get_all('VDI'):",
            "            vdi_rec = xenapi_fake.get_record('VDI', vdi_ref)",
            "            name_label = vdi_rec[\"name_label\"]",
            "            self.assert_(not name_label.endswith('snapshot'))",
            "",
            "        self.assertTrue(self.fake_upload_called)",
            "",
            "    def create_vm_record(self, conn, os_type, name):",
            "        instances = conn.list_instances()",
            "        self.assertEquals(instances, [name])",
            "",
            "        # Get Nova record for VM",
            "        vm_info = conn.get_info({'name': name})",
            "        # Get XenAPI record for VM",
            "        vms = [rec for ref, rec",
            "               in xenapi_fake.get_all_records('VM').iteritems()",
            "               if not rec['is_control_domain']]",
            "        vm = vms[0]",
            "        self.vm_info = vm_info",
            "        self.vm = vm",
            "",
            "    def check_vm_record(self, conn, instance_type_id, check_injection):",
            "        instance_type = db.flavor_get(conn, instance_type_id)",
            "        mem_kib = long(instance_type['memory_mb']) << 10",
            "        mem_bytes = str(mem_kib << 10)",
            "        vcpus = instance_type['vcpus']",
            "        vcpu_weight = instance_type['vcpu_weight']",
            "",
            "        self.assertEquals(self.vm_info['max_mem'], mem_kib)",
            "        self.assertEquals(self.vm_info['mem'], mem_kib)",
            "        self.assertEquals(self.vm['memory_static_max'], mem_bytes)",
            "        self.assertEquals(self.vm['memory_dynamic_max'], mem_bytes)",
            "        self.assertEquals(self.vm['memory_dynamic_min'], mem_bytes)",
            "        self.assertEquals(self.vm['VCPUs_max'], str(vcpus))",
            "        self.assertEquals(self.vm['VCPUs_at_startup'], str(vcpus))",
            "        if vcpu_weight == None:",
            "            self.assertEquals(self.vm['VCPUs_params'], {})",
            "        else:",
            "            self.assertEquals(self.vm['VCPUs_params'],",
            "                              {'weight': str(vcpu_weight)})",
            "",
            "        # Check that the VM is running according to Nova",
            "        self.assertEquals(self.vm_info['state'], power_state.RUNNING)",
            "",
            "        # Check that the VM is running according to XenAPI.",
            "        self.assertEquals(self.vm['power_state'], 'Running')",
            "",
            "        if check_injection:",
            "            xenstore_data = self.vm['xenstore_data']",
            "            self.assertFalse('vm-data/hostname' in xenstore_data)",
            "            key = 'vm-data/networking/DEADBEEF0001'",
            "            xenstore_value = xenstore_data[key]",
            "            tcpip_data = ast.literal_eval(xenstore_value)",
            "            self.assertEquals(tcpip_data,",
            "                              {'broadcast': '192.168.1.255',",
            "                               'dns': ['192.168.1.4', '192.168.1.3'],",
            "                               'gateway': '192.168.1.1',",
            "                               'gateway_v6': '2001:db8:0:1::1',",
            "                               'ip6s': [{'enabled': '1',",
            "                                         'ip': '2001:db8:0:1::1',",
            "                                         'netmask': 64,",
            "                                         'gateway': '2001:db8:0:1::1'}],",
            "                               'ips': [{'enabled': '1',",
            "                                        'ip': '192.168.1.100',",
            "                                        'netmask': '255.255.255.0',",
            "                                        'gateway': '192.168.1.1'},",
            "                                       {'enabled': '1',",
            "                                        'ip': '192.168.1.101',",
            "                                        'netmask': '255.255.255.0',",
            "                                        'gateway': '192.168.1.1'}],",
            "                               'label': 'test1',",
            "                               'mac': 'DE:AD:BE:EF:00:01'})",
            "",
            "    def check_vm_params_for_windows(self):",
            "        self.assertEquals(self.vm['platform']['nx'], 'true')",
            "        self.assertEquals(self.vm['HVM_boot_params'], {'order': 'dc'})",
            "        self.assertEquals(self.vm['HVM_boot_policy'], 'BIOS order')",
            "",
            "        # check that these are not set",
            "        self.assertEquals(self.vm['PV_args'], '')",
            "        self.assertEquals(self.vm['PV_bootloader'], '')",
            "        self.assertEquals(self.vm['PV_kernel'], '')",
            "        self.assertEquals(self.vm['PV_ramdisk'], '')",
            "",
            "    def check_vm_params_for_linux(self):",
            "        self.assertEquals(self.vm['platform']['nx'], 'false')",
            "        self.assertEquals(self.vm['PV_args'], '')",
            "        self.assertEquals(self.vm['PV_bootloader'], 'pygrub')",
            "",
            "        # check that these are not set",
            "        self.assertEquals(self.vm['PV_kernel'], '')",
            "        self.assertEquals(self.vm['PV_ramdisk'], '')",
            "        self.assertEquals(self.vm['HVM_boot_params'], {})",
            "        self.assertEquals(self.vm['HVM_boot_policy'], '')",
            "",
            "    def check_vm_params_for_linux_with_external_kernel(self):",
            "        self.assertEquals(self.vm['platform']['nx'], 'false')",
            "        self.assertEquals(self.vm['PV_args'], 'root=/dev/xvda1')",
            "        self.assertNotEquals(self.vm['PV_kernel'], '')",
            "        self.assertNotEquals(self.vm['PV_ramdisk'], '')",
            "",
            "        # check that these are not set",
            "        self.assertEquals(self.vm['HVM_boot_params'], {})",
            "        self.assertEquals(self.vm['HVM_boot_policy'], '')",
            "",
            "    def _list_vdis(self):",
            "        url = CONF.xenapi_connection_url",
            "        username = CONF.xenapi_connection_username",
            "        password = CONF.xenapi_connection_password",
            "        session = xenapi_conn.XenAPISession(url, username, password,",
            "                                            fake.FakeVirtAPI())",
            "        return session.call_xenapi('VDI.get_all')",
            "",
            "    def _list_vms(self):",
            "        url = CONF.xenapi_connection_url",
            "        username = CONF.xenapi_connection_username",
            "        password = CONF.xenapi_connection_password",
            "        session = xenapi_conn.XenAPISession(url, username, password,",
            "                                            fake.FakeVirtAPI())",
            "        return session.call_xenapi('VM.get_all')",
            "",
            "    def _check_vdis(self, start_list, end_list):",
            "        for vdi_ref in end_list:",
            "            if vdi_ref not in start_list:",
            "                vdi_rec = xenapi_fake.get_record('VDI', vdi_ref)",
            "                # If the cache is turned on then the base disk will be",
            "                # there even after the cleanup",
            "                if 'other_config' in vdi_rec:",
            "                    if 'image-id' not in vdi_rec['other_config']:",
            "                        self.fail('Found unexpected VDI:%s' % vdi_ref)",
            "                else:",
            "                    self.fail('Found unexpected VDI:%s' % vdi_ref)",
            "",
            "    def _test_spawn(self, image_ref, kernel_id, ramdisk_id,",
            "                    instance_type_id=\"3\", os_type=\"linux\",",
            "                    hostname=\"test\", architecture=\"x86-64\", instance_id=1,",
            "                    injected_files=None, check_injection=False,",
            "                    create_record=True, empty_dns=False,",
            "                    block_device_info=None,",
            "                    key_data=None):",
            "        if injected_files is None:",
            "            injected_files = []",
            "",
            "        # Fake out inject_instance_metadata",
            "        def fake_inject_instance_metadata(self, instance, vm):",
            "            pass",
            "        self.stubs.Set(vmops.VMOps, '_inject_instance_metadata',",
            "                       fake_inject_instance_metadata)",
            "",
            "        if create_record:",
            "            instance_values = {'id': instance_id,",
            "                               'project_id': self.project_id,",
            "                               'user_id': self.user_id,",
            "                               'image_ref': image_ref,",
            "                               'kernel_id': kernel_id,",
            "                               'ramdisk_id': ramdisk_id,",
            "                               'root_gb': 20,",
            "                               'instance_type_id': instance_type_id,",
            "                               'os_type': os_type,",
            "                               'hostname': hostname,",
            "                               'key_data': key_data,",
            "                               'architecture': architecture}",
            "            instance = create_instance_with_system_metadata(self.context,",
            "                                                            instance_values)",
            "        else:",
            "            instance = db.instance_get(self.context, instance_id)",
            "",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        if empty_dns:",
            "            # NOTE(tr3buchet): this is a terrible way to do this...",
            "            network_info[0]['network']['subnets'][0]['dns'] = []",
            "",
            "        image_meta = {}",
            "        if image_ref:",
            "            image_meta = IMAGE_FIXTURES[image_ref][\"image_meta\"]",
            "        self.conn.spawn(self.context, instance, image_meta, injected_files,",
            "                        'herp', network_info, block_device_info)",
            "        self.create_vm_record(self.conn, os_type, instance['name'])",
            "        self.check_vm_record(self.conn, instance_type_id, check_injection)",
            "        self.assertTrue(instance['os_type'])",
            "        self.assertTrue(instance['architecture'])",
            "",
            "    def test_spawn_ipxe_iso_success(self):",
            "        self.mox.StubOutWithMock(vm_utils, 'get_sr_path')",
            "        vm_utils.get_sr_path(mox.IgnoreArg()).AndReturn('/sr/path')",
            "",
            "        self.flags(xenapi_ipxe_network_name='test1',",
            "                   xenapi_ipxe_boot_menu_url='http://boot.example.com',",
            "                   xenapi_ipxe_mkisofs_cmd='/root/mkisofs')",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_plugin_serialized')",
            "        self.conn._session.call_plugin_serialized(",
            "            'ipxe', 'inject', '/sr/path', mox.IgnoreArg(),",
            "            'http://boot.example.com', '192.168.1.100', '255.255.255.0',",
            "            '192.168.1.1', '192.168.1.3', '/root/mkisofs')",
            "",
            "        self.mox.ReplayAll()",
            "        self._test_spawn(IMAGE_IPXE_ISO, None, None)",
            "",
            "    def test_spawn_ipxe_iso_no_network_name(self):",
            "        self.flags(xenapi_ipxe_network_name=None,",
            "                   xenapi_ipxe_boot_menu_url='http://boot.example.com')",
            "",
            "        # call_plugin_serialized shouldn't be called",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_plugin_serialized')",
            "",
            "        self.mox.ReplayAll()",
            "        self._test_spawn(IMAGE_IPXE_ISO, None, None)",
            "",
            "    def test_spawn_ipxe_iso_no_boot_menu_url(self):",
            "        self.flags(xenapi_ipxe_network_name='test1',",
            "                   xenapi_ipxe_boot_menu_url=None)",
            "",
            "        # call_plugin_serialized shouldn't be called",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_plugin_serialized')",
            "",
            "        self.mox.ReplayAll()",
            "        self._test_spawn(IMAGE_IPXE_ISO, None, None)",
            "",
            "    def test_spawn_ipxe_iso_unknown_network_name(self):",
            "        self.flags(xenapi_ipxe_network_name='test2',",
            "                   xenapi_ipxe_boot_menu_url='http://boot.example.com')",
            "",
            "        # call_plugin_serialized shouldn't be called",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_plugin_serialized')",
            "",
            "        self.mox.ReplayAll()",
            "        self._test_spawn(IMAGE_IPXE_ISO, None, None)",
            "",
            "    def test_spawn_empty_dns(self):",
            "        # Test spawning with an empty dns list.",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\",",
            "                         empty_dns=True)",
            "        self.check_vm_params_for_linux()",
            "",
            "    def test_spawn_not_enough_memory(self):",
            "        self.assertRaises(exception.InsufficientFreeMemory,",
            "                          self._test_spawn,",
            "                          '1', 2, 3, \"4\")  # m1.xlarge",
            "",
            "    def test_spawn_fail_cleanup_1(self):",
            "        \"\"\"Simulates an error while downloading an image.",
            "",
            "        Verifies that the VM and VDIs created are properly cleaned up.",
            "        \"\"\"",
            "        vdi_recs_start = self._list_vdis()",
            "        start_vms = self._list_vms()",
            "        stubs.stubout_fetch_disk_image(self.stubs, raise_failure=True)",
            "        self.assertRaises(xenapi_fake.Failure,",
            "                          self._test_spawn, '1', 2, 3)",
            "        # No additional VDI should be found.",
            "        vdi_recs_end = self._list_vdis()",
            "        end_vms = self._list_vms()",
            "        self._check_vdis(vdi_recs_start, vdi_recs_end)",
            "        # No additional VMs should be found.",
            "        self.assertEqual(start_vms, end_vms)",
            "",
            "    def test_spawn_fail_cleanup_2(self):",
            "        \"\"\"Simulates an error while creating VM record.",
            "",
            "        Verifies that the VM and VDIs created are properly cleaned up.",
            "        \"\"\"",
            "        vdi_recs_start = self._list_vdis()",
            "        start_vms = self._list_vms()",
            "        stubs.stubout_create_vm(self.stubs)",
            "        self.assertRaises(xenapi_fake.Failure,",
            "                          self._test_spawn, '1', 2, 3)",
            "        # No additional VDI should be found.",
            "        vdi_recs_end = self._list_vdis()",
            "        end_vms = self._list_vms()",
            "        self._check_vdis(vdi_recs_start, vdi_recs_end)",
            "        # No additional VMs should be found.",
            "        self.assertEqual(start_vms, end_vms)",
            "",
            "    def test_spawn_fail_cleanup_3(self):",
            "        \"\"\"Simulates an error while attaching disks.",
            "",
            "        Verifies that the VM and VDIs created are properly cleaned up.",
            "        \"\"\"",
            "        stubs.stubout_attach_disks(self.stubs)",
            "        vdi_recs_start = self._list_vdis()",
            "        start_vms = self._list_vms()",
            "        self.assertRaises(xenapi_fake.Failure,",
            "                          self._test_spawn, '1', 2, 3)",
            "        # No additional VDI should be found.",
            "        vdi_recs_end = self._list_vdis()",
            "        end_vms = self._list_vms()",
            "        self._check_vdis(vdi_recs_start, vdi_recs_end)",
            "        # No additional VMs should be found.",
            "        self.assertEqual(start_vms, end_vms)",
            "",
            "    def test_spawn_raw_glance(self):",
            "        self._test_spawn(IMAGE_RAW, None, None)",
            "        self.check_vm_params_for_windows()",
            "",
            "    def test_spawn_vhd_glance_linux(self):",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\")",
            "        self.check_vm_params_for_linux()",
            "",
            "    def test_spawn_vhd_glance_windows(self):",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"windows\", architecture=\"i386\",",
            "                         instance_type_id=5)",
            "        self.check_vm_params_for_windows()",
            "",
            "    def test_spawn_iso_glance(self):",
            "        self._test_spawn(IMAGE_ISO, None, None,",
            "                         os_type=\"windows\", architecture=\"i386\")",
            "        self.check_vm_params_for_windows()",
            "",
            "    def test_spawn_glance(self):",
            "",
            "        def fake_fetch_disk_image(context, session, instance, name_label,",
            "                                  image_id, image_type):",
            "            sr_ref = vm_utils.safe_find_sr(session)",
            "            image_type_str = vm_utils.ImageType.to_string(image_type)",
            "            vdi_ref = vm_utils.create_vdi(session, sr_ref, instance,",
            "                name_label, image_type_str, \"20\")",
            "            vdi_role = vm_utils.ImageType.get_role(image_type)",
            "            vdi_uuid = session.call_xenapi(\"VDI.get_uuid\", vdi_ref)",
            "            return {vdi_role: dict(uuid=vdi_uuid, file=None)}",
            "        self.stubs.Set(vm_utils, '_fetch_disk_image',",
            "                       fake_fetch_disk_image)",
            "",
            "        self._test_spawn(IMAGE_MACHINE,",
            "                         IMAGE_KERNEL,",
            "                         IMAGE_RAMDISK)",
            "        self.check_vm_params_for_linux_with_external_kernel()",
            "",
            "    def test_spawn_boot_from_volume_no_image_meta(self):",
            "        dev_info = get_fake_device_info()",
            "        self._test_spawn(None, None, None,",
            "                         block_device_info=dev_info)",
            "",
            "    def test_spawn_boot_from_volume_with_image_meta(self):",
            "        dev_info = get_fake_device_info()",
            "        self._test_spawn(None, None, None,",
            "                         block_device_info=dev_info)",
            "",
            "    def test_spawn_netinject_file(self):",
            "        self.flags(flat_injected=True)",
            "        db_fakes.stub_out_db_instance_api(self.stubs, injected=True)",
            "",
            "        self._tee_executed = False",
            "",
            "        def _tee_handler(cmd, **kwargs):",
            "            input = kwargs.get('process_input', None)",
            "            self.assertNotEqual(input, None)",
            "            config = [line.strip() for line in input.split(\"\\n\")]",
            "            # Find the start of eth0 configuration and check it",
            "            index = config.index('auto eth0')",
            "            self.assertEquals(config[index + 1:index + 8], [",
            "                'iface eth0 inet static',",
            "                'address 192.168.1.100',",
            "                'netmask 255.255.255.0',",
            "                'broadcast 192.168.1.255',",
            "                'gateway 192.168.1.1',",
            "                'dns-nameservers 192.168.1.3 192.168.1.4',",
            "                ''])",
            "            self._tee_executed = True",
            "            return '', ''",
            "",
            "        def _readlink_handler(cmd_parts, **kwargs):",
            "            return os.path.realpath(cmd_parts[2]), ''",
            "",
            "        fake_processutils.fake_execute_set_repliers([",
            "            # Capture the tee .../etc/network/interfaces command",
            "            (r'tee.*interfaces', _tee_handler),",
            "            (r'readlink -nm.*', _readlink_handler),",
            "        ])",
            "        self._test_spawn(IMAGE_MACHINE,",
            "                         IMAGE_KERNEL,",
            "                         IMAGE_RAMDISK,",
            "                         check_injection=True)",
            "        self.assertTrue(self._tee_executed)",
            "",
            "    def test_spawn_netinject_xenstore(self):",
            "        db_fakes.stub_out_db_instance_api(self.stubs, injected=True)",
            "",
            "        self._tee_executed = False",
            "",
            "        def _mount_handler(cmd, *ignore_args, **ignore_kwargs):",
            "            # When mounting, create real files under the mountpoint to simulate",
            "            # files in the mounted filesystem",
            "",
            "            # mount point will be the last item of the command list",
            "            self._tmpdir = cmd[len(cmd) - 1]",
            "            LOG.debug(_('Creating files in %s to simulate guest agent'),",
            "                      self._tmpdir)",
            "            os.makedirs(os.path.join(self._tmpdir, 'usr', 'sbin'))",
            "            # Touch the file using open",
            "            open(os.path.join(self._tmpdir, 'usr', 'sbin',",
            "                'xe-update-networking'), 'w').close()",
            "            return '', ''",
            "",
            "        def _umount_handler(cmd, *ignore_args, **ignore_kwargs):",
            "            # Umount would normall make files in the m,ounted filesystem",
            "            # disappear, so do that here",
            "            LOG.debug(_('Removing simulated guest agent files in %s'),",
            "                      self._tmpdir)",
            "            os.remove(os.path.join(self._tmpdir, 'usr', 'sbin',",
            "                'xe-update-networking'))",
            "            os.rmdir(os.path.join(self._tmpdir, 'usr', 'sbin'))",
            "            os.rmdir(os.path.join(self._tmpdir, 'usr'))",
            "            return '', ''",
            "",
            "        def _tee_handler(cmd, *ignore_args, **ignore_kwargs):",
            "            self._tee_executed = True",
            "            return '', ''",
            "",
            "        fake_processutils.fake_execute_set_repliers([",
            "            (r'mount', _mount_handler),",
            "            (r'umount', _umount_handler),",
            "            (r'tee.*interfaces', _tee_handler)])",
            "        self._test_spawn('1', 2, 3, check_injection=True)",
            "",
            "        # tee must not run in this case, where an injection-capable",
            "        # guest agent is detected",
            "        self.assertFalse(self._tee_executed)",
            "",
            "    def test_spawn_injects_auto_disk_config_to_xenstore(self):",
            "        instance = self._create_instance(spawn=False)",
            "        self.mox.StubOutWithMock(self.conn._vmops, '_inject_auto_disk_config')",
            "        self.conn._vmops._inject_auto_disk_config(instance, mox.IgnoreArg())",
            "        self.mox.ReplayAll()",
            "        self.conn.spawn(self.context, instance,",
            "                        IMAGE_FIXTURES['1'][\"image_meta\"], [], 'herp', '')",
            "",
            "    def test_spawn_vlanmanager(self):",
            "        self.flags(network_manager='nova.network.manager.VlanManager',",
            "                   vlan_interface='fake0')",
            "",
            "        def dummy(*args, **kwargs):",
            "            pass",
            "",
            "        self.stubs.Set(vmops.VMOps, '_create_vifs', dummy)",
            "        # Reset network table",
            "        xenapi_fake.reset_table('network')",
            "        # Instance id = 2 will use vlan network (see db/fakes.py)",
            "        ctxt = self.context.elevated()",
            "        instance = self._create_instance(2, False)",
            "        networks = self.network.db.network_get_all(ctxt)",
            "        for network in networks:",
            "            self.network.set_network_host(ctxt, network)",
            "",
            "        self.network.allocate_for_instance(ctxt,",
            "                          instance_id=2,",
            "                          instance_uuid='00000000-0000-0000-0000-000000000002',",
            "                          host=CONF.host,",
            "                          vpn=None,",
            "                          rxtx_factor=3,",
            "                          project_id=self.project_id,",
            "                          macs=None)",
            "        self._test_spawn(IMAGE_MACHINE,",
            "                         IMAGE_KERNEL,",
            "                         IMAGE_RAMDISK,",
            "                         instance_id=2,",
            "                         create_record=False)",
            "        # TODO(salvatore-orlando): a complete test here would require",
            "        # a check for making sure the bridge for the VM's VIF is",
            "        # consistent with bridge specified in nova db",
            "",
            "    def test_spawn_with_network_qos(self):",
            "        self._create_instance()",
            "        for vif_ref in xenapi_fake.get_all('VIF'):",
            "            vif_rec = xenapi_fake.get_record('VIF', vif_ref)",
            "            self.assertEquals(vif_rec['qos_algorithm_type'], 'ratelimit')",
            "            self.assertEquals(vif_rec['qos_algorithm_params']['kbps'],",
            "                              str(3 * 10 * 1024))",
            "",
            "    def test_spawn_ssh_key_injection(self):",
            "        # Test spawning with key_data on an instance.  Should use",
            "        # agent file injection.",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_inject_file(self, method, args):",
            "            path = base64.b64decode(args['b64_path'])",
            "            contents = base64.b64decode(args['b64_contents'])",
            "            actual_injected_files.append((path, contents))",
            "            return jsonutils.dumps({'returncode': '0', 'message': 'success'})",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_inject_file', fake_inject_file)",
            "",
            "        def fake_encrypt_text(sshkey, new_pass):",
            "            self.assertEqual(\"ssh-rsa fake_keydata\", sshkey)",
            "            return \"fake\"",
            "",
            "        self.stubs.Set(crypto, 'ssh_encrypt_text', fake_encrypt_text)",
            "",
            "        expected_data = ('\\n# The following ssh key was injected by '",
            "                         'Nova\\nssh-rsa fake_keydata\\n')",
            "",
            "        injected_files = [('/root/.ssh/authorized_keys', expected_data)]",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\",",
            "                         key_data='ssh-rsa fake_keydata')",
            "        self.assertEquals(actual_injected_files, injected_files)",
            "",
            "    def test_spawn_ssh_key_injection_non_rsa(self):",
            "        # Test spawning with key_data on an instance.  Should use",
            "        # agent file injection.",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_inject_file(self, method, args):",
            "            path = base64.b64decode(args['b64_path'])",
            "            contents = base64.b64decode(args['b64_contents'])",
            "            actual_injected_files.append((path, contents))",
            "            return jsonutils.dumps({'returncode': '0', 'message': 'success'})",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_inject_file', fake_inject_file)",
            "",
            "        def fake_encrypt_text(sshkey, new_pass):",
            "            raise NotImplementedError(\"Should not be called\")",
            "",
            "        self.stubs.Set(crypto, 'ssh_encrypt_text', fake_encrypt_text)",
            "",
            "        expected_data = ('\\n# The following ssh key was injected by '",
            "                         'Nova\\nssh-dsa fake_keydata\\n')",
            "",
            "        injected_files = [('/root/.ssh/authorized_keys', expected_data)]",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\",",
            "                         key_data='ssh-dsa fake_keydata')",
            "        self.assertEquals(actual_injected_files, injected_files)",
            "",
            "    def test_spawn_injected_files(self):",
            "        # Test spawning with injected_files.",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_inject_file(self, method, args):",
            "            path = base64.b64decode(args['b64_path'])",
            "            contents = base64.b64decode(args['b64_contents'])",
            "            actual_injected_files.append((path, contents))",
            "            return jsonutils.dumps({'returncode': '0', 'message': 'success'})",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_inject_file', fake_inject_file)",
            "",
            "        injected_files = [('/tmp/foo', 'foobar')]",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\",",
            "                         injected_files=injected_files)",
            "        self.check_vm_params_for_linux()",
            "        self.assertEquals(actual_injected_files, injected_files)",
            "",
            "    def test_spawn_agent_upgrade(self):",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_build(_self, *args):",
            "            return {\"version\": \"1.1.0\", \"architecture\": \"x86-64\",",
            "                    \"hypervisor\": \"xen\", \"os\": \"windows\",",
            "                    \"url\": \"url\", \"md5hash\": \"asdf\"}",
            "",
            "        self.stubs.Set(self.conn.virtapi, 'agent_build_get_by_triple',",
            "                       fake_agent_build)",
            "",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def test_spawn_agent_upgrade_fails_silently(self):",
            "        self.flags(xenapi_use_agent_default=True)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_build(_self, *args):",
            "            return {\"version\": \"1.1.0\", \"architecture\": \"x86-64\",",
            "                    \"hypervisor\": \"xen\", \"os\": \"windows\",",
            "                    \"url\": \"url\", \"md5hash\": \"asdf\"}",
            "",
            "        self.stubs.Set(self.conn.virtapi, 'agent_build_get_by_triple',",
            "                       fake_agent_build)",
            "",
            "        def fake_agent_update(self, method, args):",
            "            raise xenapi_fake.Failure([\"fake_error\"])",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_agentupdate', fake_agent_update)",
            "",
            "        self._test_spawn(IMAGE_VHD, None, None,",
            "                         os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def _test_spawn_fails_with(self, trigger, expected_exception):",
            "        self.flags(xenapi_use_agent_default=True)",
            "        self.flags(agent_version_timeout=0)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_version(self, method, args):",
            "            raise xenapi_fake.Failure([trigger])",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_version', fake_agent_version)",
            "",
            "        self.assertRaises(expected_exception, self._test_spawn,",
            "                IMAGE_VHD, None, None, os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def test_spawn_fails_with_agent_timeout(self):",
            "        self._test_spawn_fails_with(\"TIMEOUT:fake\", exception.AgentTimeout)",
            "",
            "    def test_spawn_fails_with_agent_not_implemented(self):",
            "        self._test_spawn_fails_with(\"NOT IMPLEMENTED:fake\",",
            "                                    exception.AgentNotImplemented)",
            "",
            "    def test_spawn_fails_with_agent_error(self):",
            "        self._test_spawn_fails_with(\"fake_error\", exception.AgentError)",
            "",
            "    def test_spawn_fails_with_agent_bad_return(self):",
            "        self.flags(xenapi_use_agent_default=True)",
            "        self.flags(agent_version_timeout=0)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_version(self, method, args):",
            "            return xenapi_fake.as_json(returncode='-1', message='fake')",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_version', fake_agent_version)",
            "",
            "        self.assertRaises(exception.AgentError, self._test_spawn,",
            "                IMAGE_VHD, None, None, os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def test_spawn_fails_agent_not_implemented(self):",
            "        # Test spawning with injected_files.",
            "        self.flags(xenapi_use_agent_default=True)",
            "        self.flags(agent_version_timeout=0)",
            "        actual_injected_files = []",
            "",
            "        def fake_agent_version(self, method, args):",
            "            raise xenapi_fake.Failure([\"NOT IMPLEMENTED:fake\"])",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       '_plugin_agent_version', fake_agent_version)",
            "",
            "        self.assertRaises(exception.AgentNotImplemented, self._test_spawn,",
            "                IMAGE_VHD, None, None, os_type=\"linux\", architecture=\"x86-64\")",
            "",
            "    def test_rescue(self):",
            "        instance = self._create_instance()",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        vm_ref = vm_utils.lookup(session, instance['name'])",
            "",
            "        swap_vdi_ref = xenapi_fake.create_vdi('swap', None)",
            "        root_vdi_ref = xenapi_fake.create_vdi('root', None)",
            "",
            "        xenapi_fake.create_vbd(vm_ref, swap_vdi_ref, userdevice=1)",
            "        xenapi_fake.create_vbd(vm_ref, root_vdi_ref, userdevice=0)",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        image_meta = {'id': IMAGE_VHD,",
            "                      'disk_format': 'vhd'}",
            "        conn.rescue(self.context, instance, [], image_meta, '')",
            "",
            "        vm = xenapi_fake.get_record('VM', vm_ref)",
            "        rescue_name = \"%s-rescue\" % vm[\"name_label\"]",
            "        rescue_ref = vm_utils.lookup(session, rescue_name)",
            "        rescue_vm = xenapi_fake.get_record('VM', rescue_ref)",
            "",
            "        vdi_uuids = []",
            "        for vbd_uuid in rescue_vm[\"VBDs\"]:",
            "            vdi_uuids.append(xenapi_fake.get_record('VBD', vbd_uuid)[\"VDI\"])",
            "        self.assertTrue(\"swap\" not in vdi_uuids)",
            "",
            "    def test_rescue_preserve_disk_on_failure(self):",
            "        # test that the original disk is preserved if rescue setup fails",
            "        # bug #1227898",
            "        instance = self._create_instance()",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        image_meta = {'id': IMAGE_VHD,",
            "                      'disk_format': 'vhd'}",
            "",
            "        vm_ref = vm_utils.lookup(session, instance['name'])",
            "        vdi_ref, vdi_rec = vm_utils.get_vdi_for_vm_safely(session, vm_ref)",
            "",
            "        # raise an error in the spawn setup process and trigger the",
            "        # undo manager logic:",
            "        def fake_start(*args, **kwargs):",
            "            raise test.TestingException('Start Error')",
            "",
            "        self.stubs.Set(self.conn._vmops, '_start', fake_start)",
            "",
            "        self.assertRaises(test.TestingException, self.conn.rescue,",
            "                          self.context, instance, [], image_meta, '')",
            "",
            "        # confirm original disk still exists:",
            "        vdi_ref2, vdi_rec2 = vm_utils.get_vdi_for_vm_safely(session, vm_ref)",
            "        self.assertEqual(vdi_ref, vdi_ref2)",
            "        self.assertEqual(vdi_rec['uuid'], vdi_rec2['uuid'])",
            "",
            "    def test_unrescue(self):",
            "        instance = self._create_instance()",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        # Unrescue expects the original instance to be powered off",
            "        conn.power_off(instance)",
            "        rescue_vm = xenapi_fake.create_vm(instance['name'] + '-rescue',",
            "                'Running')",
            "        conn.unrescue(instance, None)",
            "",
            "    def test_unrescue_not_in_rescue(self):",
            "        instance = self._create_instance()",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        # Ensure that it will not unrescue a non-rescued instance.",
            "        self.assertRaises(exception.InstanceNotInRescueMode, conn.unrescue,",
            "                          instance, None)",
            "",
            "    def test_finish_revert_migration(self):",
            "        instance = self._create_instance()",
            "",
            "        class VMOpsMock():",
            "",
            "            def __init__(self):",
            "                self.finish_revert_migration_called = False",
            "",
            "            def finish_revert_migration(self, instance, block_info,",
            "                                        power_on):",
            "                self.finish_revert_migration_called = True",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn._vmops = VMOpsMock()",
            "        conn.finish_revert_migration(instance, None)",
            "        self.assertTrue(conn._vmops.finish_revert_migration_called)",
            "",
            "    def test_reboot_hard(self):",
            "        instance = self._create_instance()",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.reboot(self.context, instance, None, \"HARD\")",
            "",
            "    def test_poll_rebooting_instances(self):",
            "        self.mox.StubOutWithMock(compute_api.API, 'reboot')",
            "        compute_api.API.reboot(mox.IgnoreArg(), mox.IgnoreArg(),",
            "                               mox.IgnoreArg())",
            "        self.mox.ReplayAll()",
            "        instance = self._create_instance()",
            "        instances = [instance]",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.poll_rebooting_instances(60, instances)",
            "",
            "    def test_reboot_soft(self):",
            "        instance = self._create_instance()",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.reboot(self.context, instance, None, \"SOFT\")",
            "",
            "    def test_reboot_halted(self):",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        instance = self._create_instance(spawn=False)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        xenapi_fake.create_vm(instance['name'], 'Halted')",
            "        conn.reboot(self.context, instance, None, \"SOFT\")",
            "        vm_ref = vm_utils.lookup(session, instance['name'])",
            "        vm = xenapi_fake.get_record('VM', vm_ref)",
            "        self.assertEquals(vm['power_state'], 'Running')",
            "",
            "    def test_reboot_unknown_state(self):",
            "        instance = self._create_instance(spawn=False)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        xenapi_fake.create_vm(instance['name'], 'Unknown')",
            "        self.assertRaises(xenapi_fake.Failure, conn.reboot, self.context,",
            "                          instance, None, \"SOFT\")",
            "",
            "    def test_reboot_rescued(self):",
            "        instance = self._create_instance()",
            "        instance['vm_state'] = vm_states.RESCUED",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        real_result = vm_utils.lookup(conn._session, instance['name'])",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        vm_utils.lookup(conn._session, instance['name'],",
            "                        True).AndReturn(real_result)",
            "        self.mox.ReplayAll()",
            "",
            "        conn.reboot(self.context, instance, None, \"SOFT\")",
            "",
            "    def test_get_console_output_succeeds(self):",
            "",
            "        def fake_get_console_output(instance):",
            "            self.assertEqual(\"instance\", instance)",
            "            return \"console_log\"",
            "        self.stubs.Set(self.conn._vmops, 'get_console_output',",
            "                       fake_get_console_output)",
            "",
            "        self.assertEqual(self.conn.get_console_output(\"instance\"),",
            "                         \"console_log\")",
            "",
            "    def _test_maintenance_mode(self, find_host, find_aggregate):",
            "        real_call_xenapi = self.conn._session.call_xenapi",
            "        instance = self._create_instance(spawn=True)",
            "        api_calls = {}",
            "",
            "        # Record all the xenapi calls, and return a fake list of hosts",
            "        # for the host.get_all call",
            "        def fake_call_xenapi(method, *args):",
            "            api_calls[method] = args",
            "            if method == 'host.get_all':",
            "                return ['foo', 'bar', 'baz']",
            "            return real_call_xenapi(method, *args)",
            "        self.stubs.Set(self.conn._session, 'call_xenapi', fake_call_xenapi)",
            "",
            "        def fake_aggregate_get(context, host, key):",
            "            if find_aggregate:",
            "                return [{'fake': 'aggregate'}]",
            "            else:",
            "                return []",
            "        self.stubs.Set(self.conn.virtapi, 'aggregate_get_by_host',",
            "                       fake_aggregate_get)",
            "",
            "        def fake_host_find(context, session, src, dst):",
            "            if find_host:",
            "                return 'bar'",
            "            else:",
            "                raise exception.NoValidHost(\"I saw this one coming...\")",
            "        self.stubs.Set(host, '_host_find', fake_host_find)",
            "",
            "        result = self.conn.host_maintenance_mode('bar', 'on_maintenance')",
            "        self.assertEqual(result, 'on_maintenance')",
            "",
            "        # We expect the VM.pool_migrate call to have been called to",
            "        # migrate our instance to the 'bar' host",
            "        vm_ref = vm_utils.lookup(self.conn._session, instance['name'])",
            "        host_ref = \"foo\"",
            "        expected = (vm_ref, host_ref, {\"live\": \"true\"})",
            "        self.assertEqual(api_calls.get('VM.pool_migrate'), expected)",
            "",
            "        instance = db.instance_get_by_uuid(self.context, instance['uuid'])",
            "        self.assertEqual(instance['vm_state'], vm_states.ACTIVE)",
            "        self.assertEqual(instance['task_state'], task_states.MIGRATING)",
            "",
            "    def test_maintenance_mode(self):",
            "        self._test_maintenance_mode(True, True)",
            "",
            "    def test_maintenance_mode_no_host(self):",
            "        self.assertRaises(exception.NoValidHost,",
            "                          self._test_maintenance_mode, False, True)",
            "",
            "    def test_maintenance_mode_no_aggregate(self):",
            "        self.assertRaises(exception.NotFound,",
            "                          self._test_maintenance_mode, True, False)",
            "",
            "    def test_uuid_find(self):",
            "        self.mox.StubOutWithMock(db, 'instance_get_all_by_host')",
            "        fake_inst = fake_instance.fake_db_instance(id=123)",
            "        fake_inst2 = fake_instance.fake_db_instance(id=456)",
            "        db.instance_get_all_by_host(self.context, fake_inst['host'],",
            "                                    columns_to_join=None",
            "                                    ).AndReturn([fake_inst, fake_inst2])",
            "        self.mox.ReplayAll()",
            "        expected_name = CONF.instance_name_template % fake_inst['id']",
            "        inst_uuid = host._uuid_find(self.context, fake_inst['host'],",
            "                                    expected_name)",
            "        self.assertEqual(inst_uuid, fake_inst['uuid'])",
            "",
            "    def test_session_virtapi(self):",
            "        was = {'called': False}",
            "",
            "        def fake_aggregate_get_by_host(self, *args, **kwargs):",
            "            was['called'] = True",
            "            raise test.TestingException()",
            "        self.stubs.Set(self.conn._session._virtapi, \"aggregate_get_by_host\",",
            "                       fake_aggregate_get_by_host)",
            "",
            "        self.stubs.Set(self.conn._session, \"is_slave\", True)",
            "",
            "        self.assertRaises(test.TestingException,",
            "                self.conn._session._get_host_uuid)",
            "        self.assertTrue(was['called'])",
            "",
            "    def test_per_instance_usage_running(self):",
            "        instance = self._create_instance(spawn=True)",
            "        instance_type = flavors.get_flavor(3)",
            "",
            "        expected = {instance['uuid']: {'memory_mb': instance_type['memory_mb'],",
            "                                       'uuid': instance['uuid']}}",
            "        actual = self.conn.get_per_instance_usage()",
            "        self.assertEqual(expected, actual)",
            "",
            "        # Paused instances still consume resources:",
            "        self.conn.pause(instance)",
            "        actual = self.conn.get_per_instance_usage()",
            "        self.assertEqual(expected, actual)",
            "",
            "    def test_per_instance_usage_suspended(self):",
            "        # Suspended instances do not consume memory:",
            "        instance = self._create_instance(spawn=True)",
            "        self.conn.suspend(instance)",
            "        actual = self.conn.get_per_instance_usage()",
            "        self.assertEqual({}, actual)",
            "",
            "    def test_per_instance_usage_halted(self):",
            "        instance = self._create_instance(spawn=True)",
            "        self.conn.power_off(instance)",
            "        actual = self.conn.get_per_instance_usage()",
            "        self.assertEqual({}, actual)",
            "",
            "    def _create_instance(self, instance_id=1, spawn=True):",
            "        \"\"\"Creates and spawns a test instance.\"\"\"",
            "        instance_values = {",
            "            'id': instance_id,",
            "            'uuid': '00000000-0000-0000-0000-00000000000%d' % instance_id,",
            "            'display_name': 'host-%d' % instance_id,",
            "            'project_id': self.project_id,",
            "            'user_id': self.user_id,",
            "            'image_ref': 1,",
            "            'kernel_id': 2,",
            "            'ramdisk_id': 3,",
            "            'root_gb': 20,",
            "            'instance_type_id': '3',  # m1.large",
            "            'os_type': 'linux',",
            "            'vm_mode': 'hvm',",
            "            'architecture': 'x86-64'}",
            "",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        instance_values)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        image_meta = {'id': IMAGE_VHD,",
            "                      'disk_format': 'vhd'}",
            "        if spawn:",
            "            self.conn.spawn(self.context, instance, image_meta, [], 'herp',",
            "                            network_info)",
            "        return instance",
            "",
            "    def test_destroy_clean_up_kernel_and_ramdisk(self):",
            "        def fake_lookup_kernel_ramdisk(session, vm_ref):",
            "            return \"kernel\", \"ramdisk\"",
            "",
            "        self.stubs.Set(vm_utils, \"lookup_kernel_ramdisk\",",
            "                       fake_lookup_kernel_ramdisk)",
            "",
            "        def fake_destroy_kernel_ramdisk(session, instance, kernel, ramdisk):",
            "            fake_destroy_kernel_ramdisk.called = True",
            "            self.assertEqual(\"kernel\", kernel)",
            "            self.assertEqual(\"ramdisk\", ramdisk)",
            "",
            "        fake_destroy_kernel_ramdisk.called = False",
            "",
            "        self.stubs.Set(vm_utils, \"destroy_kernel_ramdisk\",",
            "                       fake_destroy_kernel_ramdisk)",
            "",
            "        instance = self._create_instance(spawn=True)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        self.conn.destroy(instance, network_info)",
            "",
            "        vm_ref = vm_utils.lookup(self.conn._session, instance['name'])",
            "        self.assertTrue(vm_ref is None)",
            "        self.assertTrue(fake_destroy_kernel_ramdisk.called)",
            "",
            "",
            "class XenAPIDiffieHellmanTestCase(test.NoDBTestCase):",
            "    \"\"\"Unit tests for Diffie-Hellman code.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIDiffieHellmanTestCase, self).setUp()",
            "        self.alice = agent.SimpleDH()",
            "        self.bob = agent.SimpleDH()",
            "",
            "    def test_shared(self):",
            "        alice_pub = self.alice.get_public()",
            "        bob_pub = self.bob.get_public()",
            "        alice_shared = self.alice.compute_shared(bob_pub)",
            "        bob_shared = self.bob.compute_shared(alice_pub)",
            "        self.assertEquals(alice_shared, bob_shared)",
            "",
            "    def _test_encryption(self, message):",
            "        enc = self.alice.encrypt(message)",
            "        self.assertFalse(enc.endswith('\\n'))",
            "        dec = self.bob.decrypt(enc)",
            "        self.assertEquals(dec, message)",
            "",
            "    def test_encrypt_simple_message(self):",
            "        self._test_encryption('This is a simple message.')",
            "",
            "    def test_encrypt_message_with_newlines_at_end(self):",
            "        self._test_encryption('This message has a newline at the end.\\n')",
            "",
            "    def test_encrypt_many_newlines_at_end(self):",
            "        self._test_encryption('Message with lotsa newlines.\\n\\n\\n')",
            "",
            "    def test_encrypt_newlines_inside_message(self):",
            "        self._test_encryption('Message\\nwith\\ninterior\\nnewlines.')",
            "",
            "    def test_encrypt_with_leading_newlines(self):",
            "        self._test_encryption('\\n\\nMessage with leading newlines.')",
            "",
            "    def test_encrypt_really_long_message(self):",
            "        self._test_encryption(''.join(['abcd' for i in xrange(1024)]))",
            "",
            "",
            "class XenAPIMigrateInstance(stubs.XenAPITestBase):",
            "    \"\"\"Unit test for verifying migration-related actions.\"\"\"",
            "",
            "    def setUp(self):",
            "        super(XenAPIMigrateInstance, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        xenapi_fake.create_network('fake', 'fake_br1')",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "        self.instance_values = {'id': 1,",
            "                  'project_id': self.project_id,",
            "                  'user_id': self.user_id,",
            "                  'image_ref': 1,",
            "                  'kernel_id': None,",
            "                  'ramdisk_id': None,",
            "                  'root_gb': 5,",
            "                  'instance_type_id': '3',  # m1.large",
            "                  'os_type': 'linux',",
            "                  'architecture': 'x86-64'}",
            "",
            "        migration_values = {",
            "            'source_compute': 'nova-compute',",
            "            'dest_compute': 'nova-compute',",
            "            'dest_host': '10.127.5.114',",
            "            'status': 'post-migrating',",
            "            'instance_uuid': '15f23e6a-cc6e-4d22-b651-d9bdaac316f7',",
            "            'old_instance_type_id': 5,",
            "            'new_instance_type_id': 1",
            "        }",
            "        self.migration = db.migration_create(",
            "            context.get_admin_context(), migration_values)",
            "",
            "        fake_processutils.stub_out_processutils_execute(self.stubs)",
            "        stubs.stub_out_migration_methods(self.stubs)",
            "        stubs.stubout_get_this_vm_uuid(self.stubs)",
            "",
            "        def fake_inject_instance_metadata(self, instance, vm):",
            "            pass",
            "        self.stubs.Set(vmops.VMOps, '_inject_instance_metadata',",
            "                       fake_inject_instance_metadata)",
            "",
            "    def test_resize_xenserver_6(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        called = {'resize': False}",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            called['resize'] = True",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize\", fake_vdi_resize)",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests,",
            "                              product_version=(6, 0, 0),",
            "                              product_brand='XenServer')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vdi_ref = xenapi_fake.create_vdi('hurr', 'fake')",
            "        vdi_uuid = xenapi_fake.get_record('VDI', vdi_ref)['uuid']",
            "        conn._vmops._resize_up_root_vdi(instance,",
            "                                        {'uuid': vdi_uuid, 'ref': vdi_ref})",
            "        self.assertEqual(called['resize'], True)",
            "",
            "    def test_resize_xcp(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        called = {'resize': False}",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            called['resize'] = True",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize\", fake_vdi_resize)",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests,",
            "                              product_version=(1, 4, 99),",
            "                              product_brand='XCP')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vdi_ref = xenapi_fake.create_vdi('hurr', 'fake')",
            "        vdi_uuid = xenapi_fake.get_record('VDI', vdi_ref)['uuid']",
            "        conn._vmops._resize_up_root_vdi(instance,",
            "                                        {'uuid': vdi_uuid, 'ref': vdi_ref})",
            "        self.assertEqual(called['resize'], True)",
            "",
            "    def test_migrate_disk_and_power_off(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.large')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.migrate_disk_and_power_off(self.context, instance,",
            "                                        '127.0.0.1', instance_type, None)",
            "",
            "    def test_migrate_disk_and_power_off_passes_exceptions(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.large')",
            "",
            "        def fake_raise(*args, **kwargs):",
            "            raise exception.MigrationError(reason='test failure')",
            "        self.stubs.Set(vmops.VMOps, \"_migrate_vhd\", fake_raise)",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.MigrationError,",
            "                          conn.migrate_disk_and_power_off,",
            "                          self.context, instance,",
            "                          '127.0.0.1', instance_type, None)",
            "",
            "    def test_migrate_disk_and_power_off_throws_on_zero_gb_resize_down(self):",
            "        instance = db.instance_create(self.context, self.instance_values)",
            "        instance_type = {\"root_gb\": 0}",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.ResizeError,",
            "                          conn.migrate_disk_and_power_off,",
            "                          self.context, instance,",
            "                          'fake_dest', instance_type, None)",
            "",
            "    def test_migrate_disk_and_power_off_with_zero_gb_old_and_new_works(self):",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.tiny')",
            "        instance_type[\"root_gb\"] = 0",
            "        values = copy.copy(self.instance_values)",
            "        values[\"root_gb\"] = 0",
            "        values[\"instance_type\"] = instance_type['id']",
            "        instance = db.instance_create(self.context, values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        conn.migrate_disk_and_power_off(self.context, instance,",
            "                                        '127.0.0.1', instance_type, None)",
            "",
            "    def _test_revert_migrate(self, power_on):",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "        self.called = False",
            "        self.fake_vm_start_called = False",
            "        self.fake_finish_revert_migration_called = False",
            "",
            "        def fake_vm_start(*args, **kwargs):",
            "            self.fake_vm_start_called = True",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            self.called = True",
            "",
            "        def fake_finish_revert_migration(*args, **kwargs):",
            "            self.fake_finish_revert_migration_called = True",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize_online\", fake_vdi_resize)",
            "        self.stubs.Set(vmops.VMOps, '_start', fake_vm_start)",
            "        self.stubs.Set(vmops.VMOps, 'finish_revert_migration',",
            "                       fake_finish_revert_migration)",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests,",
            "                              product_version=(4, 0, 0),",
            "                              product_brand='XenServer')",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        image_meta = {'id': instance['image_ref'], 'disk_format': 'vhd'}",
            "        base = xenapi_fake.create_vdi('hurr', 'fake')",
            "        base_uuid = xenapi_fake.get_record('VDI', base)['uuid']",
            "        cow = xenapi_fake.create_vdi('durr', 'fake')",
            "        cow_uuid = xenapi_fake.get_record('VDI', cow)['uuid']",
            "        conn.finish_migration(self.context, self.migration, instance,",
            "                              dict(base_copy=base_uuid, cow=cow_uuid),",
            "                              network_info, image_meta, resize_instance=True,",
            "                              block_device_info=None, power_on=power_on)",
            "        self.assertEqual(self.called, True)",
            "        self.assertEqual(self.fake_vm_start_called, power_on)",
            "",
            "        conn.finish_revert_migration(instance, network_info)",
            "        self.assertEqual(self.fake_finish_revert_migration_called, True)",
            "",
            "    def test_revert_migrate_power_on(self):",
            "        self._test_revert_migrate(True)",
            "",
            "    def test_revert_migrate_power_off(self):",
            "        self._test_revert_migrate(False)",
            "",
            "    def _test_finish_migrate(self, power_on):",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "        self.called = False",
            "        self.fake_vm_start_called = False",
            "",
            "        def fake_vm_start(*args, **kwargs):",
            "            self.fake_vm_start_called = True",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            self.called = True",
            "",
            "        self.stubs.Set(vmops.VMOps, '_start', fake_vm_start)",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize_online\", fake_vdi_resize)",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests,",
            "                              product_version=(4, 0, 0),",
            "                              product_brand='XenServer')",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        image_meta = {'id': instance['image_ref'], 'disk_format': 'vhd'}",
            "        conn.finish_migration(self.context, self.migration, instance,",
            "                              dict(base_copy='hurr', cow='durr'),",
            "                              network_info, image_meta, resize_instance=True,",
            "                              block_device_info=None, power_on=power_on)",
            "        self.assertEqual(self.called, True)",
            "        self.assertEqual(self.fake_vm_start_called, power_on)",
            "",
            "    def test_finish_migrate_power_on(self):",
            "        self._test_finish_migrate(True)",
            "",
            "    def test_finish_migrate_power_off(self):",
            "        self._test_finish_migrate(False)",
            "",
            "    def test_finish_migrate_no_local_storage(self):",
            "        tiny_type = flavors.get_flavor_by_name('m1.tiny')",
            "        tiny_type_id = tiny_type['id']",
            "        self.instance_values.update({'instance_type_id': tiny_type_id,",
            "                                     'root_gb': 0})",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            raise Exception(\"This shouldn't be called\")",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize_online\", fake_vdi_resize)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        image_meta = {'id': instance['image_ref'], 'disk_format': 'vhd'}",
            "        conn.finish_migration(self.context, self.migration, instance,",
            "                              dict(base_copy='hurr', cow='durr'),",
            "                              network_info, image_meta, resize_instance=True)",
            "",
            "    def test_finish_migrate_no_resize_vdi(self):",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "",
            "        def fake_vdi_resize(*args, **kwargs):",
            "            raise Exception(\"This shouldn't be called\")",
            "",
            "        self.stubs.Set(stubs.FakeSessionForVMTests,",
            "                       \"VDI_resize_online\", fake_vdi_resize)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs)",
            "        # Resize instance would be determined by the compute call",
            "        image_meta = {'id': instance['image_ref'], 'disk_format': 'vhd'}",
            "        conn.finish_migration(self.context, self.migration, instance,",
            "                              dict(base_copy='hurr', cow='durr'),",
            "                              network_info, image_meta, resize_instance=False)",
            "",
            "    @stub_vm_utils_with_vdi_attached_here",
            "    def test_migrate_too_many_partitions_no_resize_down(self):",
            "        instance_values = self.instance_values",
            "        instance_values['root_gb'] = 40",
            "        instance = db.instance_create(self.context, instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.small')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_partitions(partition):",
            "            return [(1, 2, 3, 4), (1, 2, 3, 4)]",
            "",
            "        self.stubs.Set(vm_utils, '_get_partitions', fake_get_partitions)",
            "",
            "        self.assertRaises(exception.InstanceFaultRollback,",
            "                          conn.migrate_disk_and_power_off,",
            "                          self.context, instance,",
            "                          '127.0.0.1', instance_type, None)",
            "",
            "    @stub_vm_utils_with_vdi_attached_here",
            "    def test_migrate_bad_fs_type_no_resize_down(self):",
            "        instance_values = self.instance_values",
            "        instance_values['root_gb'] = 40",
            "        instance = db.instance_create(self.context, instance_values)",
            "        xenapi_fake.create_vm(instance['name'], 'Running')",
            "        instance_type = db.flavor_get_by_name(self.context, 'm1.small')",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_partitions(partition):",
            "            return [(1, 2, 3, \"ext2\")]",
            "",
            "        self.stubs.Set(vm_utils, '_get_partitions', fake_get_partitions)",
            "",
            "        self.assertRaises(exception.InstanceFaultRollback,",
            "                          conn.migrate_disk_and_power_off,",
            "                          self.context, instance,",
            "                          '127.0.0.1', instance_type, None)",
            "",
            "    def test_migrate_rollback_when_resize_down_fs_fails(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        virtapi = vmops._virtapi",
            "",
            "        self.mox.StubOutWithMock(vmops, '_resize_ensure_vm_is_shutdown')",
            "        self.mox.StubOutWithMock(vmops, '_apply_orig_vm_name_label')",
            "        self.mox.StubOutWithMock(vm_utils, 'resize_disk')",
            "        self.mox.StubOutWithMock(vmops, '_migrate_vhd')",
            "        self.mox.StubOutWithMock(vm_utils, 'destroy_vdi')",
            "        self.mox.StubOutWithMock(vm_utils, 'get_vdi_for_vm_safely')",
            "        self.mox.StubOutWithMock(vmops, '_restore_orig_vm_and_cleanup_orphan')",
            "        self.mox.StubOutWithMock(virtapi, 'instance_update')",
            "",
            "        instance = {'auto_disk_config': True, 'uuid': 'uuid'}",
            "        vm_ref = \"vm_ref\"",
            "        dest = \"dest\"",
            "        instance_type = \"type\"",
            "        sr_path = \"sr_path\"",
            "",
            "        virtapi.instance_update(self.context, 'uuid', {'progress': 20.0})",
            "        vmops._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "        vmops._apply_orig_vm_name_label(instance, vm_ref)",
            "        old_vdi_ref = \"old_ref\"",
            "        vm_utils.get_vdi_for_vm_safely(vmops._session, vm_ref).AndReturn(",
            "            (old_vdi_ref, None))",
            "        virtapi.instance_update(self.context, 'uuid', {'progress': 40.0})",
            "        new_vdi_ref = \"new_ref\"",
            "        new_vdi_uuid = \"new_uuid\"",
            "        vm_utils.resize_disk(vmops._session, instance, old_vdi_ref,",
            "            instance_type).AndReturn((new_vdi_ref, new_vdi_uuid))",
            "        virtapi.instance_update(self.context, 'uuid', {'progress': 60.0})",
            "        vmops._migrate_vhd(instance, new_vdi_uuid, dest,",
            "                           sr_path, 0).AndRaise(",
            "                                exception.ResizeError(reason=\"asdf\"))",
            "",
            "        vm_utils.destroy_vdi(vmops._session, new_vdi_ref)",
            "        vmops._restore_orig_vm_and_cleanup_orphan(instance, None)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertRaises(exception.InstanceFaultRollback,",
            "                          vmops._migrate_disk_resizing_down, self.context,",
            "                          instance, dest, instance_type, vm_ref, sr_path)",
            "",
            "    def test_resize_ensure_vm_is_shutdown_cleanly(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        fake_instance = {'uuid': 'uuid'}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')",
            "        self.mox.StubOutWithMock(vm_utils, 'clean_shutdown_vm')",
            "        self.mox.StubOutWithMock(vm_utils, 'hard_shutdown_vm')",
            "",
            "        vm_utils.is_vm_shutdown(vmops._session, \"ref\").AndReturn(False)",
            "        vm_utils.clean_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(True)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        vmops._resize_ensure_vm_is_shutdown(fake_instance, \"ref\")",
            "",
            "    def test_resize_ensure_vm_is_shutdown_forced(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        fake_instance = {'uuid': 'uuid'}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')",
            "        self.mox.StubOutWithMock(vm_utils, 'clean_shutdown_vm')",
            "        self.mox.StubOutWithMock(vm_utils, 'hard_shutdown_vm')",
            "",
            "        vm_utils.is_vm_shutdown(vmops._session, \"ref\").AndReturn(False)",
            "        vm_utils.clean_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(False)",
            "        vm_utils.hard_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(True)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        vmops._resize_ensure_vm_is_shutdown(fake_instance, \"ref\")",
            "",
            "    def test_resize_ensure_vm_is_shutdown_fails(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        fake_instance = {'uuid': 'uuid'}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')",
            "        self.mox.StubOutWithMock(vm_utils, 'clean_shutdown_vm')",
            "        self.mox.StubOutWithMock(vm_utils, 'hard_shutdown_vm')",
            "",
            "        vm_utils.is_vm_shutdown(vmops._session, \"ref\").AndReturn(False)",
            "        vm_utils.clean_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(False)",
            "        vm_utils.hard_shutdown_vm(vmops._session, fake_instance,",
            "            \"ref\").AndReturn(False)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertRaises(exception.ResizeError,",
            "            vmops._resize_ensure_vm_is_shutdown, fake_instance, \"ref\")",
            "",
            "    def test_resize_ensure_vm_is_shutdown_already_shutdown(self):",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        vmops = conn._vmops",
            "        fake_instance = {'uuid': 'uuid'}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')",
            "        self.mox.StubOutWithMock(vm_utils, 'clean_shutdown_vm')",
            "        self.mox.StubOutWithMock(vm_utils, 'hard_shutdown_vm')",
            "",
            "        vm_utils.is_vm_shutdown(vmops._session, \"ref\").AndReturn(True)",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        vmops._resize_ensure_vm_is_shutdown(fake_instance, \"ref\")",
            "",
            "",
            "class XenAPIImageTypeTestCase(test.NoDBTestCase):",
            "    \"\"\"Test ImageType class.\"\"\"",
            "",
            "    def test_to_string(self):",
            "        # Can convert from type id to type string.",
            "        self.assertEquals(",
            "            vm_utils.ImageType.to_string(vm_utils.ImageType.KERNEL),",
            "            vm_utils.ImageType.KERNEL_STR)",
            "",
            "    def _assert_role(self, expected_role, image_type_id):",
            "        self.assertEquals(",
            "            expected_role,",
            "            vm_utils.ImageType.get_role(image_type_id))",
            "",
            "    def test_get_image_role_kernel(self):",
            "        self._assert_role('kernel', vm_utils.ImageType.KERNEL)",
            "",
            "    def test_get_image_role_ramdisk(self):",
            "        self._assert_role('ramdisk', vm_utils.ImageType.RAMDISK)",
            "",
            "    def test_get_image_role_disk(self):",
            "        self._assert_role('root', vm_utils.ImageType.DISK)",
            "",
            "    def test_get_image_role_disk_raw(self):",
            "        self._assert_role('root', vm_utils.ImageType.DISK_RAW)",
            "",
            "    def test_get_image_role_disk_vhd(self):",
            "        self._assert_role('root', vm_utils.ImageType.DISK_VHD)",
            "",
            "",
            "class XenAPIDetermineDiskImageTestCase(test.NoDBTestCase):",
            "    \"\"\"Unit tests for code that detects the ImageType.\"\"\"",
            "    def assert_disk_type(self, image_meta, expected_disk_type):",
            "        actual = vm_utils.determine_disk_image_type(image_meta)",
            "        self.assertEqual(expected_disk_type, actual)",
            "",
            "    def test_machine(self):",
            "        image_meta = {'id': 'a', 'disk_format': 'ami'}",
            "        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK)",
            "",
            "    def test_raw(self):",
            "        image_meta = {'id': 'a', 'disk_format': 'raw'}",
            "        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_RAW)",
            "",
            "    def test_vhd(self):",
            "        image_meta = {'id': 'a', 'disk_format': 'vhd'}",
            "        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_VHD)",
            "",
            "    def test_none(self):",
            "        image_meta = None",
            "        self.assert_disk_type(image_meta, None)",
            "",
            "",
            "class XenAPIDetermineIsPVTestCase(test.NoDBTestCase):",
            "    \"\"\"Unit tests for code that detects the PV status based on ImageType.\"\"\"",
            "    def assert_pv_status(self, disk_image_type, os_type, expected_pv_status):",
            "        session = None",
            "        vdi_ref = None",
            "        actual = vm_utils.determine_is_pv(session, vdi_ref,",
            "                                          disk_image_type, os_type)",
            "        self.assertEqual(expected_pv_status, actual)",
            "",
            "    def test_windows_vhd(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK_VHD, 'windows', False)",
            "",
            "    def test_linux_vhd(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK_VHD, 'linux', True)",
            "",
            "    def test_raw(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK_RAW, 'linux', False)",
            "",
            "    def test_disk(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK, None, True)",
            "",
            "    def test_iso(self):",
            "        self.assert_pv_status(vm_utils.ImageType.DISK_ISO, None, False)",
            "",
            "    def test_none(self):",
            "        self.assert_pv_status(None, None, False)",
            "",
            "",
            "class CompareVersionTestCase(test.NoDBTestCase):",
            "    def test_less_than(self):",
            "        # Test that cmp_version compares a as less than b.",
            "        self.assertTrue(vmops.cmp_version('1.2.3.4', '1.2.3.5') < 0)",
            "",
            "    def test_greater_than(self):",
            "        # Test that cmp_version compares a as greater than b.",
            "        self.assertTrue(vmops.cmp_version('1.2.3.5', '1.2.3.4') > 0)",
            "",
            "    def test_equal(self):",
            "        # Test that cmp_version compares a as equal to b.",
            "        self.assertTrue(vmops.cmp_version('1.2.3.4', '1.2.3.4') == 0)",
            "",
            "    def test_non_lexical(self):",
            "        # Test that cmp_version compares non-lexically.",
            "        self.assertTrue(vmops.cmp_version('1.2.3.10', '1.2.3.4') > 0)",
            "",
            "    def test_length(self):",
            "        # Test that cmp_version compares by length as last resort.",
            "        self.assertTrue(vmops.cmp_version('1.2.3', '1.2.3.4') < 0)",
            "",
            "",
            "class XenAPIHostTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Tests HostState, which holds metrics from XenServer that get",
            "    reported back to the Schedulers.",
            "    \"\"\"",
            "",
            "    def setUp(self):",
            "        super(XenAPIHostTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.context = context.get_admin_context()",
            "        self.flags(use_local=True, group='conductor')",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "    def test_host_state(self):",
            "        stats = self.conn.get_host_stats()",
            "        self.assertEquals(stats['disk_total'], 40000)",
            "        self.assertEquals(stats['disk_used'], 20000)",
            "        self.assertEquals(stats['host_memory_total'], 10)",
            "        self.assertEquals(stats['host_memory_overhead'], 20)",
            "        self.assertEquals(stats['host_memory_free'], 30)",
            "        self.assertEquals(stats['host_memory_free_computed'], 40)",
            "        self.assertEquals(stats['hypervisor_hostname'], 'fake-xenhost')",
            "",
            "    def test_host_state_missing_sr(self):",
            "        def fake_safe_find_sr(session):",
            "            raise exception.StorageRepositoryNotFound('not there')",
            "",
            "        self.stubs.Set(vm_utils, 'safe_find_sr', fake_safe_find_sr)",
            "        self.assertRaises(exception.StorageRepositoryNotFound,",
            "                          self.conn.get_host_stats)",
            "",
            "    def _test_host_action(self, method, action, expected=None):",
            "        result = method('host', action)",
            "        if not expected:",
            "            expected = action",
            "        self.assertEqual(result, expected)",
            "",
            "    def test_host_reboot(self):",
            "        self._test_host_action(self.conn.host_power_action, 'reboot')",
            "",
            "    def test_host_shutdown(self):",
            "        self._test_host_action(self.conn.host_power_action, 'shutdown')",
            "",
            "    def test_host_startup(self):",
            "        self.assertRaises(NotImplementedError,",
            "                          self.conn.host_power_action, 'host', 'startup')",
            "",
            "    def test_host_maintenance_on(self):",
            "        self._test_host_action(self.conn.host_maintenance_mode,",
            "                               True, 'on_maintenance')",
            "",
            "    def test_host_maintenance_off(self):",
            "        self._test_host_action(self.conn.host_maintenance_mode,",
            "                               False, 'off_maintenance')",
            "",
            "    def test_set_enable_host_enable(self):",
            "        values = _create_service_entries(self.context, values={'nova':",
            "            ['host']})",
            "        self._test_host_action(self.conn.set_host_enabled, True, 'enabled')",
            "        service = db.service_get_by_args(self.context, 'host', 'nova-compute')",
            "        self.assertEquals(service.disabled, False)",
            "",
            "    def test_set_enable_host_disable(self):",
            "        values = _create_service_entries(self.context, values={'nova':",
            "            ['host']})",
            "        self._test_host_action(self.conn.set_host_enabled, False, 'disabled')",
            "        service = db.service_get_by_args(self.context, 'host', 'nova-compute')",
            "        self.assertEquals(service.disabled, True)",
            "",
            "    def test_get_host_uptime(self):",
            "        result = self.conn.get_host_uptime('host')",
            "        self.assertEqual(result, 'fake uptime')",
            "",
            "    def test_supported_instances_is_included_in_host_state(self):",
            "        stats = self.conn.get_host_stats()",
            "        self.assertTrue('supported_instances' in stats)",
            "",
            "    def test_supported_instances_is_calculated_by_to_supported_instances(self):",
            "",
            "        def to_supported_instances(somedata):",
            "            self.assertEquals(None, somedata)",
            "            return \"SOMERETURNVALUE\"",
            "        self.stubs.Set(host, 'to_supported_instances', to_supported_instances)",
            "",
            "        stats = self.conn.get_host_stats()",
            "        self.assertEquals(\"SOMERETURNVALUE\", stats['supported_instances'])",
            "",
            "    def test_update_stats_caches_hostname(self):",
            "        self.mox.StubOutWithMock(host, 'call_xenhost')",
            "        self.mox.StubOutWithMock(vm_utils, 'safe_find_sr')",
            "        self.mox.StubOutWithMock(self.conn._session, 'call_xenapi')",
            "        data = {'disk_total': 0,",
            "                'disk_used': 0,",
            "                'disk_available': 0,",
            "                'supported_instances': 0,",
            "                'host_capabilities': [],",
            "                'host_hostname': 'foo',",
            "                }",
            "        sr_rec = {",
            "            'physical_size': 0,",
            "            'physical_utilisation': 0,",
            "            }",
            "",
            "        for i in range(3):",
            "            host.call_xenhost(mox.IgnoreArg(), 'host_data', {}).AndReturn(data)",
            "            vm_utils.safe_find_sr(self.conn._session).AndReturn(None)",
            "            self.conn._session.call_xenapi('SR.scan', None)",
            "            self.conn._session.call_xenapi('SR.get_record', None).AndReturn(",
            "                sr_rec)",
            "            if i == 2:",
            "                # On the third call (the second below) change the hostname",
            "                data = dict(data, host_hostname='bar')",
            "",
            "        self.mox.ReplayAll()",
            "        stats = self.conn.get_host_stats(refresh=True)",
            "        self.assertEqual('foo', stats['hypervisor_hostname'])",
            "        stats = self.conn.get_host_stats(refresh=True)",
            "        self.assertEqual('foo', stats['hypervisor_hostname'])",
            "",
            "",
            "class ToSupportedInstancesTestCase(test.NoDBTestCase):",
            "    def test_default_return_value(self):",
            "        self.assertEquals([],",
            "            host.to_supported_instances(None))",
            "",
            "    def test_return_value(self):",
            "        self.assertEquals([('x86_64', 'xapi', 'xen')],",
            "             host.to_supported_instances([u'xen-3.0-x86_64']))",
            "",
            "    def test_invalid_values_do_not_break(self):",
            "        self.assertEquals([('x86_64', 'xapi', 'xen')],",
            "             host.to_supported_instances([u'xen-3.0-x86_64', 'spam']))",
            "",
            "    def test_multiple_values(self):",
            "        self.assertEquals(",
            "            [",
            "                ('x86_64', 'xapi', 'xen'),",
            "                ('x86_32', 'xapi', 'hvm')",
            "            ],",
            "            host.to_supported_instances([u'xen-3.0-x86_64', 'hvm-3.0-x86_32'])",
            "        )",
            "",
            "",
            "class XenAPIAutoDiskConfigTestCase(stubs.XenAPITestBase):",
            "    def setUp(self):",
            "        super(XenAPIAutoDiskConfigTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "",
            "        self.instance_values = {'id': 1,",
            "                  'project_id': self.project_id,",
            "                  'user_id': self.user_id,",
            "                  'image_ref': 1,",
            "                  'kernel_id': 2,",
            "                  'ramdisk_id': 3,",
            "                  'root_gb': 20,",
            "                  'instance_type_id': '3',  # m1.large",
            "                  'os_type': 'linux',",
            "                  'architecture': 'x86-64'}",
            "",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "",
            "        def fake_create_vbd(session, vm_ref, vdi_ref, userdevice,",
            "                            vbd_type='disk', read_only=False, bootable=True,",
            "                            osvol=False):",
            "            pass",
            "",
            "        self.stubs.Set(vm_utils, 'create_vbd', fake_create_vbd)",
            "",
            "    def assertIsPartitionCalled(self, called):",
            "        marker = {\"partition_called\": False}",
            "",
            "        def fake_resize_part_and_fs(dev, start, old, new):",
            "            marker[\"partition_called\"] = True",
            "        self.stubs.Set(vm_utils, \"_resize_part_and_fs\",",
            "                       fake_resize_part_and_fs)",
            "",
            "        ctx = context.RequestContext(self.user_id, self.project_id)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "",
            "        disk_image_type = vm_utils.ImageType.DISK_VHD",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        self.instance_values)",
            "        vm_ref = xenapi_fake.create_vm(instance['name'], 'Halted')",
            "        vdi_ref = xenapi_fake.create_vdi(instance['name'], 'fake')",
            "",
            "        vdi_uuid = session.call_xenapi('VDI.get_record', vdi_ref)['uuid']",
            "        vdis = {'root': {'uuid': vdi_uuid, 'ref': vdi_ref}}",
            "",
            "        self.conn._vmops._attach_disks(instance, vm_ref, instance['name'],",
            "                                       vdis, disk_image_type)",
            "",
            "        self.assertEqual(marker[\"partition_called\"], called)",
            "",
            "    def test_instance_not_auto_disk_config(self):",
            "        \"\"\"Should not partition unless instance is marked as",
            "        auto_disk_config.",
            "        \"\"\"",
            "        self.instance_values['auto_disk_config'] = False",
            "        self.assertIsPartitionCalled(False)",
            "",
            "    @stub_vm_utils_with_vdi_attached_here",
            "    def test_instance_auto_disk_config_doesnt_pass_fail_safes(self):",
            "        # Should not partition unless fail safes pass.",
            "        self.instance_values['auto_disk_config'] = True",
            "",
            "        def fake_get_partitions(dev):",
            "            return [(1, 0, 100, 'ext4'), (2, 100, 200, 'ext4')]",
            "        self.stubs.Set(vm_utils, \"_get_partitions\",",
            "                       fake_get_partitions)",
            "",
            "        self.assertIsPartitionCalled(False)",
            "",
            "    @stub_vm_utils_with_vdi_attached_here",
            "    def test_instance_auto_disk_config_passes_fail_safes(self):",
            "        \"\"\"Should partition if instance is marked as auto_disk_config=True and",
            "        virt-layer specific fail-safe checks pass.",
            "        \"\"\"",
            "        self.instance_values['auto_disk_config'] = True",
            "",
            "        def fake_get_partitions(dev):",
            "            return [(1, 0, 100, 'ext4')]",
            "        self.stubs.Set(vm_utils, \"_get_partitions\",",
            "                       fake_get_partitions)",
            "",
            "        self.assertIsPartitionCalled(True)",
            "",
            "",
            "class XenAPIGenerateLocal(stubs.XenAPITestBase):",
            "    \"\"\"Test generating of local disks, like swap and ephemeral.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIGenerateLocal, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self.user_id = 'fake'",
            "        self.project_id = 'fake'",
            "",
            "        self.instance_values = {'id': 1,",
            "                  'project_id': self.project_id,",
            "                  'user_id': self.user_id,",
            "                  'image_ref': 1,",
            "                  'kernel_id': 2,",
            "                  'ramdisk_id': 3,",
            "                  'root_gb': 20,",
            "                  'instance_type_id': '3',  # m1.large",
            "                  'os_type': 'linux',",
            "                  'architecture': 'x86-64'}",
            "",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "",
            "        def fake_create_vbd(session, vm_ref, vdi_ref, userdevice,",
            "                            vbd_type='disk', read_only=False, bootable=True,",
            "                            osvol=False, empty=False, unpluggable=True):",
            "            return session.call_xenapi('VBD.create', {'VM': vm_ref,",
            "                                                      'VDI': vdi_ref})",
            "",
            "        self.stubs.Set(vm_utils, 'create_vbd', fake_create_vbd)",
            "",
            "    def assertCalled(self, instance,",
            "                     disk_image_type=vm_utils.ImageType.DISK_VHD):",
            "        ctx = context.RequestContext(self.user_id, self.project_id)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "",
            "        vm_ref = xenapi_fake.create_vm(instance['name'], 'Halted')",
            "        vdi_ref = xenapi_fake.create_vdi(instance['name'], 'fake')",
            "",
            "        vdi_uuid = session.call_xenapi('VDI.get_record', vdi_ref)['uuid']",
            "",
            "        vdi_key = 'root'",
            "        if disk_image_type == vm_utils.ImageType.DISK_ISO:",
            "            vdi_key = 'iso'",
            "        vdis = {vdi_key: {'uuid': vdi_uuid, 'ref': vdi_ref}}",
            "",
            "        self.called = False",
            "        self.conn._vmops._attach_disks(instance, vm_ref, instance['name'],",
            "                                       vdis, disk_image_type)",
            "        self.assertTrue(self.called)",
            "",
            "    def test_generate_swap(self):",
            "        # Test swap disk generation.",
            "        instance_values = dict(self.instance_values, instance_type_id=5)",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        instance_values)",
            "",
            "        def fake_generate_swap(*args, **kwargs):",
            "            self.called = True",
            "        self.stubs.Set(vm_utils, 'generate_swap', fake_generate_swap)",
            "",
            "        self.assertCalled(instance)",
            "",
            "    def test_generate_ephemeral(self):",
            "        # Test ephemeral disk generation.",
            "        instance_values = dict(self.instance_values, instance_type_id=4)",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        instance_values)",
            "",
            "        def fake_generate_ephemeral(*args):",
            "            self.called = True",
            "        self.stubs.Set(vm_utils, 'generate_ephemeral', fake_generate_ephemeral)",
            "",
            "        self.assertCalled(instance)",
            "",
            "    def test_generate_iso_blank_root_disk(self):",
            "        instance_values = dict(self.instance_values, instance_type_id=4)",
            "        instance_values.pop('kernel_id')",
            "        instance_values.pop('ramdisk_id')",
            "        instance = create_instance_with_system_metadata(self.context,",
            "                                                        instance_values)",
            "",
            "        def fake_generate_ephemeral(*args):",
            "            pass",
            "        self.stubs.Set(vm_utils, 'generate_ephemeral', fake_generate_ephemeral)",
            "",
            "        def fake_generate_iso(*args):",
            "            self.called = True",
            "        self.stubs.Set(vm_utils, 'generate_iso_blank_root_disk',",
            "            fake_generate_iso)",
            "",
            "        self.assertCalled(instance, vm_utils.ImageType.DISK_ISO)",
            "",
            "",
            "class XenAPIBWCountersTestCase(stubs.XenAPITestBase):",
            "    FAKE_VMS = {'test1:ref': dict(name_label='test1',",
            "                                   other_config=dict(nova_uuid='hash'),",
            "                                   domid='12',",
            "                                   _vifmap={'0': \"a:b:c:d...\",",
            "                                           '1': \"e:f:12:q...\"}),",
            "                'test2:ref': dict(name_label='test2',",
            "                                   other_config=dict(nova_uuid='hash'),",
            "                                   domid='42',",
            "                                   _vifmap={'0': \"a:3:c:d...\",",
            "                                           '1': \"e:f:42:q...\"}),",
            "               }",
            "",
            "    def setUp(self):",
            "        super(XenAPIBWCountersTestCase, self).setUp()",
            "        self.stubs.Set(vm_utils, 'list_vms',",
            "                       XenAPIBWCountersTestCase._fake_list_vms)",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def _fake_get_vif_device_map(vm_rec):",
            "            return vm_rec['_vifmap']",
            "",
            "        self.stubs.Set(self.conn._vmops, \"_get_vif_device_map\",",
            "                                         _fake_get_vif_device_map)",
            "",
            "    @classmethod",
            "    def _fake_list_vms(cls, session):",
            "        return cls.FAKE_VMS.iteritems()",
            "",
            "    @staticmethod",
            "    def _fake_fetch_bandwidth_mt(session):",
            "        return {}",
            "",
            "    @staticmethod",
            "    def _fake_fetch_bandwidth(session):",
            "        return {'42':",
            "                    {'0': {'bw_in': 21024, 'bw_out': 22048},",
            "                     '1': {'bw_in': 231337, 'bw_out': 221212121}},",
            "                '12':",
            "                    {'0': {'bw_in': 1024, 'bw_out': 2048},",
            "                     '1': {'bw_in': 31337, 'bw_out': 21212121}},",
            "                }",
            "",
            "    def test_get_all_bw_counters(self):",
            "        instances = [dict(name='test1', uuid='1-2-3'),",
            "                     dict(name='test2', uuid='4-5-6')]",
            "",
            "        self.stubs.Set(vm_utils, 'fetch_bandwidth',",
            "                       self._fake_fetch_bandwidth)",
            "        result = self.conn.get_all_bw_counters(instances)",
            "        self.assertEqual(len(result), 4)",
            "        self.assertIn(dict(uuid='1-2-3',",
            "                           mac_address=\"a:b:c:d...\",",
            "                           bw_in=1024,",
            "                           bw_out=2048), result)",
            "        self.assertIn(dict(uuid='1-2-3',",
            "                           mac_address=\"e:f:12:q...\",",
            "                           bw_in=31337,",
            "                           bw_out=21212121), result)",
            "",
            "        self.assertIn(dict(uuid='4-5-6',",
            "                           mac_address=\"a:3:c:d...\",",
            "                           bw_in=21024,",
            "                           bw_out=22048), result)",
            "        self.assertIn(dict(uuid='4-5-6',",
            "                           mac_address=\"e:f:42:q...\",",
            "                           bw_in=231337,",
            "                           bw_out=221212121), result)",
            "",
            "    def test_get_all_bw_counters_in_failure_case(self):",
            "        \"\"\"Test that get_all_bw_conters returns an empty list when",
            "        no data returned from Xenserver.  c.f. bug #910045.",
            "        \"\"\"",
            "        instances = [dict(name='instance-0001', uuid='1-2-3-4-5')]",
            "",
            "        self.stubs.Set(vm_utils, 'fetch_bandwidth',",
            "                       self._fake_fetch_bandwidth_mt)",
            "        result = self.conn.get_all_bw_counters(instances)",
            "        self.assertEqual(result, [])",
            "",
            "",
            "# TODO(salvatore-orlando): this class and",
            "# nova.tests.virt.test_libvirt.IPTablesFirewallDriverTestCase share a lot of",
            "# code.  Consider abstracting common code in a base class for firewall driver",
            "# testing.",
            "class XenAPIDom0IptablesFirewallTestCase(stubs.XenAPITestBase):",
            "",
            "    _in_rules = [",
            "      '# Generated by iptables-save v1.4.10 on Sat Feb 19 00:03:19 2011',",
            "      '*nat',",
            "      ':PREROUTING ACCEPT [1170:189210]',",
            "      ':INPUT ACCEPT [844:71028]',",
            "      ':OUTPUT ACCEPT [5149:405186]',",
            "      ':POSTROUTING ACCEPT [5063:386098]',",
            "      '# Completed on Mon Dec  6 11:54:13 2010',",
            "      '# Generated by iptables-save v1.4.4 on Mon Dec  6 11:54:13 2010',",
            "      '*mangle',",
            "      ':INPUT ACCEPT [969615:281627771]',",
            "      ':FORWARD ACCEPT [0:0]',",
            "      ':OUTPUT ACCEPT [915599:63811649]',",
            "      ':nova-block-ipv4 - [0:0]',",
            "      '[0:0] -A INPUT -i virbr0 -p tcp -m tcp --dport 67 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -d 192.168.122.0/24 -o virbr0 -m state --state RELATED'",
            "      ',ESTABLISHED -j ACCEPT ',",
            "      '[0:0] -A FORWARD -s 192.168.122.0/24 -i virbr0 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -i virbr0 -o virbr0 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -o virbr0 -j REJECT '",
            "      '--reject-with icmp-port-unreachable ',",
            "      '[0:0] -A FORWARD -i virbr0 -j REJECT '",
            "      '--reject-with icmp-port-unreachable ',",
            "      'COMMIT',",
            "      '# Completed on Mon Dec  6 11:54:13 2010',",
            "      '# Generated by iptables-save v1.4.4 on Mon Dec  6 11:54:13 2010',",
            "      '*filter',",
            "      ':INPUT ACCEPT [969615:281627771]',",
            "      ':FORWARD ACCEPT [0:0]',",
            "      ':OUTPUT ACCEPT [915599:63811649]',",
            "      ':nova-block-ipv4 - [0:0]',",
            "      '[0:0] -A INPUT -i virbr0 -p tcp -m tcp --dport 67 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -d 192.168.122.0/24 -o virbr0 -m state --state RELATED'",
            "      ',ESTABLISHED -j ACCEPT ',",
            "      '[0:0] -A FORWARD -s 192.168.122.0/24 -i virbr0 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -i virbr0 -o virbr0 -j ACCEPT ',",
            "      '[0:0] -A FORWARD -o virbr0 -j REJECT '",
            "      '--reject-with icmp-port-unreachable ',",
            "      '[0:0] -A FORWARD -i virbr0 -j REJECT '",
            "      '--reject-with icmp-port-unreachable ',",
            "      'COMMIT',",
            "      '# Completed on Mon Dec  6 11:54:13 2010',",
            "    ]",
            "",
            "    _in6_filter_rules = [",
            "      '# Generated by ip6tables-save v1.4.4 on Tue Jan 18 23:47:56 2011',",
            "      '*filter',",
            "      ':INPUT ACCEPT [349155:75810423]',",
            "      ':FORWARD ACCEPT [0:0]',",
            "      ':OUTPUT ACCEPT [349256:75777230]',",
            "      'COMMIT',",
            "      '# Completed on Tue Jan 18 23:47:56 2011',",
            "    ]",
            "",
            "    def setUp(self):",
            "        super(XenAPIDom0IptablesFirewallTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   instance_name_template='%d',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        self.user_id = 'mappin'",
            "        self.project_id = 'fake'",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForFirewallTests,",
            "                              test_case=self)",
            "        self.context = context.RequestContext(self.user_id, self.project_id)",
            "        self.network = importutils.import_object(CONF.network_manager)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.fw = self.conn._vmops.firewall_driver",
            "",
            "    def _create_instance_ref(self):",
            "        return db.instance_create(self.context,",
            "                                  {'user_id': self.user_id,",
            "                                   'project_id': self.project_id,",
            "                                   'instance_type_id': 1})",
            "",
            "    def _create_test_security_group(self):",
            "        admin_ctxt = context.get_admin_context()",
            "        secgroup = db.security_group_create(admin_ctxt,",
            "                                {'user_id': self.user_id,",
            "                                 'project_id': self.project_id,",
            "                                 'name': 'testgroup',",
            "                                 'description': 'test group'})",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'icmp',",
            "                                       'from_port': -1,",
            "                                       'to_port': -1,",
            "                                       'cidr': '192.168.11.0/24'})",
            "",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'icmp',",
            "                                       'from_port': 8,",
            "                                       'to_port': -1,",
            "                                       'cidr': '192.168.11.0/24'})",
            "",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'tcp',",
            "                                       'from_port': 80,",
            "                                       'to_port': 81,",
            "                                       'cidr': '192.168.10.0/24'})",
            "        return secgroup",
            "",
            "    def _validate_security_group(self):",
            "        in_rules = filter(lambda l: not l.startswith('#'),",
            "                          self._in_rules)",
            "        for rule in in_rules:",
            "            if 'nova' not in rule:",
            "                self.assertTrue(rule in self._out_rules,",
            "                                'Rule went missing: %s' % rule)",
            "",
            "        instance_chain = None",
            "        for rule in self._out_rules:",
            "            # This is pretty crude, but it'll do for now",
            "            # last two octets change",
            "            if re.search('-d 192.168.[0-9]{1,3}.[0-9]{1,3} -j', rule):",
            "                instance_chain = rule.split(' ')[-1]",
            "                break",
            "        self.assertTrue(instance_chain, \"The instance chain wasn't added\")",
            "        security_group_chain = None",
            "        for rule in self._out_rules:",
            "            # This is pretty crude, but it'll do for now",
            "            if '-A %s -j' % instance_chain in rule:",
            "                security_group_chain = rule.split(' ')[-1]",
            "                break",
            "        self.assertTrue(security_group_chain,",
            "                        \"The security group chain wasn't added\")",
            "",
            "        regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p icmp'",
            "                           ' -s 192.168.11.0/24')",
            "        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                        \"ICMP acceptance rule wasn't added\")",
            "",
            "        regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p icmp -m icmp'",
            "                           ' --icmp-type 8 -s 192.168.11.0/24')",
            "        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                        \"ICMP Echo Request acceptance rule wasn't added\")",
            "",
            "        regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p tcp --dport 80:81'",
            "                           ' -s 192.168.10.0/24')",
            "        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                        \"TCP port 80/81 acceptance rule wasn't added\")",
            "",
            "    def test_static_filters(self):",
            "        instance_ref = self._create_instance_ref()",
            "        src_instance_ref = self._create_instance_ref()",
            "        admin_ctxt = context.get_admin_context()",
            "        secgroup = self._create_test_security_group()",
            "",
            "        src_secgroup = db.security_group_create(admin_ctxt,",
            "                                                {'user_id': self.user_id,",
            "                                                 'project_id': self.project_id,",
            "                                                 'name': 'testsourcegroup',",
            "                                                 'description': 'src group'})",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'tcp',",
            "                                       'from_port': 80,",
            "                                       'to_port': 81,",
            "                                       'group_id': src_secgroup['id']})",
            "",
            "        db.instance_add_security_group(admin_ctxt, instance_ref['uuid'],",
            "                                       secgroup['id'])",
            "        db.instance_add_security_group(admin_ctxt, src_instance_ref['uuid'],",
            "                                       src_secgroup['id'])",
            "        instance_ref = db.instance_get(admin_ctxt, instance_ref['id'])",
            "        src_instance_ref = db.instance_get(admin_ctxt, src_instance_ref['id'])",
            "",
            "        network_model = fake_network.fake_get_instance_nw_info(self.stubs, 1)",
            "",
            "        from nova.compute import utils as compute_utils",
            "        self.stubs.Set(compute_utils, 'get_nw_info_for_instance',",
            "                       lambda instance: network_model)",
            "",
            "        self.fw.prepare_instance_filter(instance_ref, network_model)",
            "        self.fw.apply_instance_filter(instance_ref, network_model)",
            "",
            "        self._validate_security_group()",
            "        # Extra test for TCP acceptance rules",
            "        for ip in network_model.fixed_ips():",
            "            if ip['version'] != 4:",
            "                continue",
            "            regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p tcp'",
            "                               ' --dport 80:81 -s %s' % ip['address'])",
            "            self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                            \"TCP port 80/81 acceptance rule wasn't added\")",
            "",
            "        db.instance_destroy(admin_ctxt, instance_ref['uuid'])",
            "",
            "    def test_filters_for_instance_with_ip_v6(self):",
            "        self.flags(use_ipv6=True)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)",
            "        rulesv4, rulesv6 = self.fw._filters_for_instance(\"fake\", network_info)",
            "        self.assertEquals(len(rulesv4), 2)",
            "        self.assertEquals(len(rulesv6), 1)",
            "",
            "    def test_filters_for_instance_without_ip_v6(self):",
            "        self.flags(use_ipv6=False)",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)",
            "        rulesv4, rulesv6 = self.fw._filters_for_instance(\"fake\", network_info)",
            "        self.assertEquals(len(rulesv4), 2)",
            "        self.assertEquals(len(rulesv6), 0)",
            "",
            "    def test_multinic_iptables(self):",
            "        ipv4_rules_per_addr = 1",
            "        ipv4_addr_per_network = 2",
            "        ipv6_rules_per_addr = 1",
            "        ipv6_addr_per_network = 1",
            "        networks_count = 5",
            "        instance_ref = self._create_instance_ref()",
            "        _get_instance_nw_info = fake_network.fake_get_instance_nw_info",
            "        network_info = _get_instance_nw_info(self.stubs,",
            "                                             networks_count,",
            "                                             ipv4_addr_per_network)",
            "        network_info[0]['network']['subnets'][0]['meta']['dhcp_server'] = \\",
            "            '1.1.1.1'",
            "        ipv4_len = len(self.fw.iptables.ipv4['filter'].rules)",
            "        ipv6_len = len(self.fw.iptables.ipv6['filter'].rules)",
            "        inst_ipv4, inst_ipv6 = self.fw.instance_rules(instance_ref,",
            "                                                      network_info)",
            "        self.fw.prepare_instance_filter(instance_ref, network_info)",
            "        ipv4 = self.fw.iptables.ipv4['filter'].rules",
            "        ipv6 = self.fw.iptables.ipv6['filter'].rules",
            "        ipv4_network_rules = len(ipv4) - len(inst_ipv4) - ipv4_len",
            "        ipv6_network_rules = len(ipv6) - len(inst_ipv6) - ipv6_len",
            "        # Extra rules are for the DHCP request",
            "        rules = (ipv4_rules_per_addr * ipv4_addr_per_network *",
            "                 networks_count) + 2",
            "        self.assertEquals(ipv4_network_rules, rules)",
            "        self.assertEquals(ipv6_network_rules,",
            "                  ipv6_rules_per_addr * ipv6_addr_per_network * networks_count)",
            "",
            "    def test_do_refresh_security_group_rules(self):",
            "        admin_ctxt = context.get_admin_context()",
            "        instance_ref = self._create_instance_ref()",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1, 1)",
            "        secgroup = self._create_test_security_group()",
            "        db.instance_add_security_group(admin_ctxt, instance_ref['uuid'],",
            "                                       secgroup['id'])",
            "        self.fw.prepare_instance_filter(instance_ref, network_info)",
            "        self.fw.instances[instance_ref['id']] = instance_ref",
            "        self._validate_security_group()",
            "        # add a rule to the security group",
            "        db.security_group_rule_create(admin_ctxt,",
            "                                      {'parent_group_id': secgroup['id'],",
            "                                       'protocol': 'udp',",
            "                                       'from_port': 200,",
            "                                       'to_port': 299,",
            "                                       'cidr': '192.168.99.0/24'})",
            "        #validate the extra rule",
            "        self.fw.refresh_security_group_rules(secgroup)",
            "        regex = re.compile('\\[0\\:0\\] -A .* -j ACCEPT -p udp --dport 200:299'",
            "                           ' -s 192.168.99.0/24')",
            "        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,",
            "                        \"Rules were not updated properly.\"",
            "                        \"The rule for UDP acceptance is missing\")",
            "",
            "    def test_provider_firewall_rules(self):",
            "        # setup basic instance data",
            "        instance_ref = self._create_instance_ref()",
            "        # FRAGILE: as in libvirt tests",
            "        # peeks at how the firewall names chains",
            "        chain_name = 'inst-%s' % instance_ref['id']",
            "",
            "        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1, 1)",
            "        self.fw.prepare_instance_filter(instance_ref, network_info)",
            "        self.assertTrue('provider' in self.fw.iptables.ipv4['filter'].chains)",
            "        rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                      if rule.chain == 'provider']",
            "        self.assertEqual(0, len(rules))",
            "",
            "        admin_ctxt = context.get_admin_context()",
            "        # add a rule and send the update message, check for 1 rule",
            "        provider_fw0 = db.provider_fw_rule_create(admin_ctxt,",
            "                                                  {'protocol': 'tcp',",
            "                                                   'cidr': '10.99.99.99/32',",
            "                                                   'from_port': 1,",
            "                                                   'to_port': 65535})",
            "        self.fw.refresh_provider_fw_rules()",
            "        rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                      if rule.chain == 'provider']",
            "        self.assertEqual(1, len(rules))",
            "",
            "        # Add another, refresh, and make sure number of rules goes to two",
            "        provider_fw1 = db.provider_fw_rule_create(admin_ctxt,",
            "                                                  {'protocol': 'udp',",
            "                                                   'cidr': '10.99.99.99/32',",
            "                                                   'from_port': 1,",
            "                                                   'to_port': 65535})",
            "        self.fw.refresh_provider_fw_rules()",
            "        rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                      if rule.chain == 'provider']",
            "        self.assertEqual(2, len(rules))",
            "",
            "        # create the instance filter and make sure it has a jump rule",
            "        self.fw.prepare_instance_filter(instance_ref, network_info)",
            "        self.fw.apply_instance_filter(instance_ref, network_info)",
            "        inst_rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                           if rule.chain == chain_name]",
            "        jump_rules = [rule for rule in inst_rules if '-j' in rule.rule]",
            "        provjump_rules = []",
            "        # IptablesTable doesn't make rules unique internally",
            "        for rule in jump_rules:",
            "            if 'provider' in rule.rule and rule not in provjump_rules:",
            "                provjump_rules.append(rule)",
            "        self.assertEqual(1, len(provjump_rules))",
            "",
            "        # remove a rule from the db, cast to compute to refresh rule",
            "        db.provider_fw_rule_destroy(admin_ctxt, provider_fw1['id'])",
            "        self.fw.refresh_provider_fw_rules()",
            "        rules = [rule for rule in self.fw.iptables.ipv4['filter'].rules",
            "                      if rule.chain == 'provider']",
            "        self.assertEqual(1, len(rules))",
            "",
            "",
            "class XenAPISRSelectionTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for testing we find the right SR.\"\"\"",
            "    def test_safe_find_sr_raise_exception(self):",
            "        # Ensure StorageRepositoryNotFound is raise when wrong filter.",
            "        self.flags(sr_matching_filter='yadayadayada')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        self.assertRaises(exception.StorageRepositoryNotFound,",
            "                          vm_utils.safe_find_sr, session)",
            "",
            "    def test_safe_find_sr_local_storage(self):",
            "        # Ensure the default local-storage is found.",
            "        self.flags(sr_matching_filter='other-config:i18n-key=local-storage')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        # This test is only guaranteed if there is one host in the pool",
            "        self.assertEqual(len(xenapi_fake.get_all('host')), 1)",
            "        host_ref = xenapi_fake.get_all('host')[0]",
            "        pbd_refs = xenapi_fake.get_all('PBD')",
            "        for pbd_ref in pbd_refs:",
            "            pbd_rec = xenapi_fake.get_record('PBD', pbd_ref)",
            "            if pbd_rec['host'] != host_ref:",
            "                continue",
            "            sr_rec = xenapi_fake.get_record('SR', pbd_rec['SR'])",
            "            if sr_rec['other_config']['i18n-key'] == 'local-storage':",
            "                local_sr = pbd_rec['SR']",
            "        expected = vm_utils.safe_find_sr(session)",
            "        self.assertEqual(local_sr, expected)",
            "",
            "    def test_safe_find_sr_by_other_criteria(self):",
            "        # Ensure the SR is found when using a different filter.",
            "        self.flags(sr_matching_filter='other-config:my_fake_sr=true')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        host_ref = xenapi_fake.get_all('host')[0]",
            "        local_sr = xenapi_fake.create_sr(name_label='Fake Storage',",
            "                                         type='lvm',",
            "                                         other_config={'my_fake_sr': 'true'},",
            "                                         host_ref=host_ref)",
            "        expected = vm_utils.safe_find_sr(session)",
            "        self.assertEqual(local_sr, expected)",
            "",
            "    def test_safe_find_sr_default(self):",
            "        # Ensure the default SR is found regardless of other-config.",
            "        self.flags(sr_matching_filter='default-sr:true')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass',",
            "                                            fake.FakeVirtAPI())",
            "        pool_ref = session.call_xenapi('pool.get_all')[0]",
            "        expected = vm_utils.safe_find_sr(session)",
            "        self.assertEqual(session.call_xenapi('pool.get_default_SR', pool_ref),",
            "                         expected)",
            "",
            "",
            "def _create_service_entries(context, values={'avail_zone1': ['fake_host1',",
            "                                                         'fake_host2'],",
            "                                         'avail_zone2': ['fake_host3'], }):",
            "    for avail_zone, hosts in values.iteritems():",
            "        for host in hosts:",
            "            db.service_create(context,",
            "                              {'host': host,",
            "                               'binary': 'nova-compute',",
            "                               'topic': 'compute',",
            "                               'report_count': 0})",
            "    return values",
            "",
            "",
            "class XenAPIAggregateTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for aggregate operations.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPIAggregateTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='http://test_url',",
            "                   xenapi_connection_username='test_user',",
            "                   xenapi_connection_password='test_pass',",
            "                   instance_name_template='%d',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver',",
            "                   host='host',",
            "                   compute_driver='xenapi.XenAPIDriver',",
            "                   default_availability_zone='avail_zone1')",
            "        self.flags(use_local=True, group='conductor')",
            "        host_ref = xenapi_fake.get_all('host')[0]",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.context = context.get_admin_context()",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.compute = importutils.import_object(CONF.compute_manager)",
            "        self.api = compute_api.AggregateAPI()",
            "        values = {'name': 'test_aggr',",
            "                  'metadata': {'availability_zone': 'test_zone',",
            "                  pool_states.POOL_FLAG: 'XenAPI'}}",
            "        self.aggr = db.aggregate_create(self.context, values)",
            "        self.fake_metadata = {pool_states.POOL_FLAG: 'XenAPI',",
            "                              'master_compute': 'host',",
            "                              'availability_zone': 'fake_zone',",
            "                              pool_states.KEY: pool_states.ACTIVE,",
            "                              'host': xenapi_fake.get_record('host',",
            "                                                             host_ref)['uuid']}",
            "",
            "    def test_pool_add_to_aggregate_called_by_driver(self):",
            "",
            "        calls = []",
            "",
            "        def pool_add_to_aggregate(context, aggregate, host, slave_info=None):",
            "            self.assertEquals(\"CONTEXT\", context)",
            "            self.assertEquals(\"AGGREGATE\", aggregate)",
            "            self.assertEquals(\"HOST\", host)",
            "            self.assertEquals(\"SLAVEINFO\", slave_info)",
            "            calls.append(pool_add_to_aggregate)",
            "        self.stubs.Set(self.conn._pool,",
            "                       \"add_to_aggregate\",",
            "                       pool_add_to_aggregate)",
            "",
            "        self.conn.add_to_aggregate(\"CONTEXT\", \"AGGREGATE\", \"HOST\",",
            "                                   slave_info=\"SLAVEINFO\")",
            "",
            "        self.assertTrue(pool_add_to_aggregate in calls)",
            "",
            "    def test_pool_remove_from_aggregate_called_by_driver(self):",
            "",
            "        calls = []",
            "",
            "        def pool_remove_from_aggregate(context, aggregate, host,",
            "                                       slave_info=None):",
            "            self.assertEquals(\"CONTEXT\", context)",
            "            self.assertEquals(\"AGGREGATE\", aggregate)",
            "            self.assertEquals(\"HOST\", host)",
            "            self.assertEquals(\"SLAVEINFO\", slave_info)",
            "            calls.append(pool_remove_from_aggregate)",
            "        self.stubs.Set(self.conn._pool,",
            "                       \"remove_from_aggregate\",",
            "                       pool_remove_from_aggregate)",
            "",
            "        self.conn.remove_from_aggregate(\"CONTEXT\", \"AGGREGATE\", \"HOST\",",
            "                                        slave_info=\"SLAVEINFO\")",
            "",
            "        self.assertTrue(pool_remove_from_aggregate in calls)",
            "",
            "    def test_add_to_aggregate_for_first_host_sets_metadata(self):",
            "        def fake_init_pool(id, name):",
            "            fake_init_pool.called = True",
            "        self.stubs.Set(self.conn._pool, \"_init_pool\", fake_init_pool)",
            "",
            "        aggregate = self._aggregate_setup()",
            "        self.conn._pool.add_to_aggregate(self.context, aggregate, \"host\")",
            "        result = db.aggregate_get(self.context, aggregate['id'])",
            "        self.assertTrue(fake_init_pool.called)",
            "        self.assertThat(self.fake_metadata,",
            "                        matchers.DictMatches(result['metadetails']))",
            "",
            "    def test_join_slave(self):",
            "        # Ensure join_slave gets called when the request gets to master.",
            "        def fake_join_slave(id, compute_uuid, host, url, user, password):",
            "            fake_join_slave.called = True",
            "        self.stubs.Set(self.conn._pool, \"_join_slave\", fake_join_slave)",
            "",
            "        aggregate = self._aggregate_setup(hosts=['host', 'host2'],",
            "                                          metadata=self.fake_metadata)",
            "        self.conn._pool.add_to_aggregate(self.context, aggregate, \"host2\",",
            "                                         dict(compute_uuid='fake_uuid',",
            "                                         url='fake_url',",
            "                                         user='fake_user',",
            "                                         passwd='fake_pass',",
            "                                         xenhost_uuid='fake_uuid'))",
            "        self.assertTrue(fake_join_slave.called)",
            "",
            "    def test_add_to_aggregate_first_host(self):",
            "        def fake_pool_set_name_label(self, session, pool_ref, name):",
            "            fake_pool_set_name_label.called = True",
            "        self.stubs.Set(xenapi_fake.SessionBase, \"pool_set_name_label\",",
            "                       fake_pool_set_name_label)",
            "        self.conn._session.call_xenapi(\"pool.create\", {\"name\": \"asdf\"})",
            "",
            "        values = {\"name\": 'fake_aggregate',",
            "                  'metadata': {'availability_zone': 'fake_zone'}}",
            "        result = db.aggregate_create(self.context, values)",
            "        metadata = {'availability_zone': 'fake_zone',",
            "                    pool_states.POOL_FLAG: \"XenAPI\",",
            "                    pool_states.KEY: pool_states.CREATED}",
            "        db.aggregate_metadata_add(self.context, result['id'], metadata)",
            "",
            "        db.aggregate_host_add(self.context, result['id'], \"host\")",
            "        aggregate = db.aggregate_get(self.context, result['id'])",
            "        self.assertEqual([\"host\"], aggregate['hosts'])",
            "        self.assertEqual(metadata, aggregate['metadetails'])",
            "",
            "        self.conn._pool.add_to_aggregate(self.context, aggregate, \"host\")",
            "        self.assertTrue(fake_pool_set_name_label.called)",
            "",
            "    def test_remove_from_aggregate_called(self):",
            "        def fake_remove_from_aggregate(context, aggregate, host):",
            "            fake_remove_from_aggregate.called = True",
            "        self.stubs.Set(self.conn._pool,",
            "                       \"remove_from_aggregate\",",
            "                       fake_remove_from_aggregate)",
            "",
            "        self.conn.remove_from_aggregate(None, None, None)",
            "        self.assertTrue(fake_remove_from_aggregate.called)",
            "",
            "    def test_remove_from_empty_aggregate(self):",
            "        result = self._aggregate_setup()",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn._pool.remove_from_aggregate,",
            "                          self.context, result, \"test_host\")",
            "",
            "    def test_remove_slave(self):",
            "        # Ensure eject slave gets called.",
            "        def fake_eject_slave(id, compute_uuid, host_uuid):",
            "            fake_eject_slave.called = True",
            "        self.stubs.Set(self.conn._pool, \"_eject_slave\", fake_eject_slave)",
            "",
            "        self.fake_metadata['host2'] = 'fake_host2_uuid'",
            "        aggregate = self._aggregate_setup(hosts=['host', 'host2'],",
            "                metadata=self.fake_metadata, aggr_state=pool_states.ACTIVE)",
            "        self.conn._pool.remove_from_aggregate(self.context, aggregate, \"host2\")",
            "        self.assertTrue(fake_eject_slave.called)",
            "",
            "    def test_remove_master_solo(self):",
            "        # Ensure metadata are cleared after removal.",
            "        def fake_clear_pool(id):",
            "            fake_clear_pool.called = True",
            "        self.stubs.Set(self.conn._pool, \"_clear_pool\", fake_clear_pool)",
            "",
            "        aggregate = self._aggregate_setup(metadata=self.fake_metadata)",
            "        self.conn._pool.remove_from_aggregate(self.context, aggregate, \"host\")",
            "        result = db.aggregate_get(self.context, aggregate['id'])",
            "        self.assertTrue(fake_clear_pool.called)",
            "        self.assertThat({'availability_zone': 'fake_zone',",
            "                pool_states.POOL_FLAG: 'XenAPI',",
            "                pool_states.KEY: pool_states.ACTIVE},",
            "                matchers.DictMatches(result['metadetails']))",
            "",
            "    def test_remote_master_non_empty_pool(self):",
            "        # Ensure AggregateError is raised if removing the master.",
            "        aggregate = self._aggregate_setup(hosts=['host', 'host2'],",
            "                                          metadata=self.fake_metadata)",
            "",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn._pool.remove_from_aggregate,",
            "                          self.context, aggregate, \"host\")",
            "",
            "    def _aggregate_setup(self, aggr_name='fake_aggregate',",
            "                         aggr_zone='fake_zone',",
            "                         aggr_state=pool_states.CREATED,",
            "                         hosts=['host'], metadata=None):",
            "        values = {\"name\": aggr_name}",
            "        result = db.aggregate_create(self.context, values,",
            "                metadata={'availability_zone': aggr_zone})",
            "        pool_flag = {pool_states.POOL_FLAG: \"XenAPI\",",
            "                    pool_states.KEY: aggr_state}",
            "        db.aggregate_metadata_add(self.context, result['id'], pool_flag)",
            "",
            "        for host in hosts:",
            "            db.aggregate_host_add(self.context, result['id'], host)",
            "        if metadata:",
            "            db.aggregate_metadata_add(self.context, result['id'], metadata)",
            "        return db.aggregate_get(self.context, result['id'])",
            "",
            "    def test_add_host_to_aggregate_invalid_changing_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when adding host while",
            "        aggregate is not ready.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.CHANGING)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.add_to_aggregate, self.context,",
            "                          aggregate, 'host')",
            "",
            "    def test_add_host_to_aggregate_invalid_dismissed_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when aggregate is",
            "        deleted.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.DISMISSED)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.add_to_aggregate, self.context,",
            "                          aggregate, 'fake_host')",
            "",
            "    def test_add_host_to_aggregate_invalid_error_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when aggregate is",
            "        in error.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.ERROR)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.add_to_aggregate, self.context,",
            "                          aggregate, 'fake_host')",
            "",
            "    def test_remove_host_from_aggregate_error(self):",
            "        # Ensure we can remove a host from an aggregate even if in error.",
            "        values = _create_service_entries(self.context)",
            "        fake_zone = values.keys()[0]",
            "        aggr = self.api.create_aggregate(self.context,",
            "                                         'fake_aggregate', fake_zone)",
            "        # let's mock the fact that the aggregate is ready!",
            "        metadata = {pool_states.POOL_FLAG: \"XenAPI\",",
            "                    pool_states.KEY: pool_states.ACTIVE}",
            "        db.aggregate_metadata_add(self.context, aggr['id'], metadata)",
            "        for host in values[fake_zone]:",
            "            aggr = self.api.add_host_to_aggregate(self.context,",
            "                                                  aggr['id'], host)",
            "        # let's mock the fact that the aggregate is in error!",
            "        status = {'operational_state': pool_states.ERROR}",
            "        expected = self.api.remove_host_from_aggregate(self.context,",
            "                                                       aggr['id'],",
            "                                                       values[fake_zone][0])",
            "        self.assertEqual(len(aggr['hosts']) - 1, len(expected['hosts']))",
            "        self.assertEqual(expected['metadata'][pool_states.KEY],",
            "                         pool_states.ACTIVE)",
            "",
            "    def test_remove_host_from_aggregate_invalid_dismissed_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when aggregate is",
            "        deleted.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.DISMISSED)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.remove_from_aggregate, self.context,",
            "                          aggregate, 'fake_host')",
            "",
            "    def test_remove_host_from_aggregate_invalid_changing_status(self):",
            "        \"\"\"Ensure InvalidAggregateAction is raised when aggregate is",
            "        changing.",
            "        \"\"\"",
            "        aggregate = self._aggregate_setup(aggr_state=pool_states.CHANGING)",
            "        self.assertRaises(exception.InvalidAggregateAction,",
            "                          self.conn.remove_from_aggregate, self.context,",
            "                          aggregate, 'fake_host')",
            "",
            "    def test_add_aggregate_host_raise_err(self):",
            "        # Ensure the undo operation works correctly on add.",
            "        def fake_driver_add_to_aggregate(context, aggregate, host, **_ignore):",
            "            raise exception.AggregateError(",
            "                    aggregate_id='', action='', reason='')",
            "        self.stubs.Set(self.compute.driver, \"add_to_aggregate\",",
            "                       fake_driver_add_to_aggregate)",
            "        metadata = {pool_states.POOL_FLAG: \"XenAPI\",",
            "                    pool_states.KEY: pool_states.ACTIVE}",
            "        db.aggregate_metadata_add(self.context, self.aggr['id'], metadata)",
            "        db.aggregate_host_add(self.context, self.aggr['id'], 'fake_host')",
            "",
            "        self.assertRaises(exception.AggregateError,",
            "                          self.compute.add_aggregate_host,",
            "                          self.context, \"fake_host\",",
            "                          aggregate=jsonutils.to_primitive(self.aggr))",
            "        excepted = db.aggregate_get(self.context, self.aggr['id'])",
            "        self.assertEqual(excepted['metadetails'][pool_states.KEY],",
            "                pool_states.ERROR)",
            "        self.assertEqual(excepted['hosts'], [])",
            "",
            "",
            "class MockComputeAPI(object):",
            "    def __init__(self):",
            "        self._mock_calls = []",
            "",
            "    def add_aggregate_host(self, ctxt, aggregate,",
            "                                     host_param, host, slave_info):",
            "        self._mock_calls.append((",
            "            self.add_aggregate_host, ctxt, aggregate,",
            "            host_param, host, slave_info))",
            "",
            "    def remove_aggregate_host(self, ctxt, aggregate_id, host_param,",
            "                              host, slave_info):",
            "        self._mock_calls.append((",
            "            self.remove_aggregate_host, ctxt, aggregate_id,",
            "            host_param, host, slave_info))",
            "",
            "",
            "class StubDependencies(object):",
            "    \"\"\"Stub dependencies for ResourcePool.\"\"\"",
            "",
            "    def __init__(self):",
            "        self.compute_rpcapi = MockComputeAPI()",
            "",
            "    def _is_hv_pool(self, *_ignore):",
            "        return True",
            "",
            "    def _get_metadata(self, *_ignore):",
            "        return {",
            "            pool_states.KEY: {},",
            "            'master_compute': 'master'",
            "        }",
            "",
            "    def _create_slave_info(self, *ignore):",
            "        return \"SLAVE_INFO\"",
            "",
            "",
            "class ResourcePoolWithStubs(StubDependencies, pool.ResourcePool):",
            "    \"\"\"A ResourcePool, use stub dependencies.\"\"\"",
            "",
            "",
            "class HypervisorPoolTestCase(test.NoDBTestCase):",
            "",
            "    fake_aggregate = {",
            "        'id': 98,",
            "        'hosts': [],",
            "        'metadetails': {",
            "            'master_compute': 'master',",
            "            pool_states.POOL_FLAG: {},",
            "            pool_states.KEY: {}",
            "            }",
            "        }",
            "",
            "    def test_slave_asks_master_to_add_slave_to_pool(self):",
            "        slave = ResourcePoolWithStubs()",
            "",
            "        slave.add_to_aggregate(\"CONTEXT\", self.fake_aggregate, \"slave\")",
            "",
            "        self.assertIn(",
            "            (slave.compute_rpcapi.add_aggregate_host,",
            "            \"CONTEXT\", jsonutils.to_primitive(self.fake_aggregate),",
            "            \"slave\", \"master\", \"SLAVE_INFO\"),",
            "            slave.compute_rpcapi._mock_calls)",
            "",
            "    def test_slave_asks_master_to_remove_slave_from_pool(self):",
            "        slave = ResourcePoolWithStubs()",
            "",
            "        slave.remove_from_aggregate(\"CONTEXT\", self.fake_aggregate, \"slave\")",
            "",
            "        self.assertIn(",
            "            (slave.compute_rpcapi.remove_aggregate_host,",
            "            \"CONTEXT\", 98, \"slave\", \"master\", \"SLAVE_INFO\"),",
            "            slave.compute_rpcapi._mock_calls)",
            "",
            "",
            "class SwapXapiHostTestCase(test.NoDBTestCase):",
            "",
            "    def test_swapping(self):",
            "        self.assertEquals(",
            "            \"http://otherserver:8765/somepath\",",
            "            pool.swap_xapi_host(",
            "                \"http://someserver:8765/somepath\", 'otherserver'))",
            "",
            "    def test_no_port(self):",
            "        self.assertEquals(",
            "            \"http://otherserver/somepath\",",
            "            pool.swap_xapi_host(",
            "                \"http://someserver/somepath\", 'otherserver'))",
            "",
            "    def test_no_path(self):",
            "        self.assertEquals(",
            "            \"http://otherserver\",",
            "            pool.swap_xapi_host(",
            "                \"http://someserver\", 'otherserver'))",
            "",
            "",
            "class XenAPILiveMigrateTestCase(stubs.XenAPITestBase):",
            "    \"\"\"Unit tests for live_migration.\"\"\"",
            "    def setUp(self):",
            "        super(XenAPILiveMigrateTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver',",
            "                   host='host')",
            "        db_fakes.stub_out_db_instance_api(self.stubs)",
            "        self.context = context.get_admin_context()",
            "",
            "    def test_live_migration_calls_vmops(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_live_migrate(context, instance_ref, dest, post_method,",
            "                              recover_method, block_migration, migrate_data):",
            "            fake_live_migrate.called = True",
            "",
            "        self.stubs.Set(self.conn._vmops, \"live_migrate\", fake_live_migrate)",
            "",
            "        self.conn.live_migration(None, None, None, None, None)",
            "        self.assertTrue(fake_live_migrate.called)",
            "",
            "    def test_pre_live_migration(self):",
            "        # ensure method is present",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.conn.pre_live_migration(None, None, None, None, None)",
            "",
            "    def test_post_live_migration_at_destination(self):",
            "        # ensure method is present",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        fake_instance = \"instance\"",
            "        fake_network_info = \"network_info\"",
            "",
            "        def fake_fw(instance, network_info):",
            "            self.assertEquals(instance, fake_instance)",
            "            self.assertEquals(network_info, fake_network_info)",
            "            fake_fw.called += 1",
            "",
            "        fake_fw.called = 0",
            "        _vmops = self.conn._vmops",
            "        self.stubs.Set(_vmops.firewall_driver,",
            "                       'setup_basic_filtering', fake_fw)",
            "        self.stubs.Set(_vmops.firewall_driver,",
            "                       'prepare_instance_filter', fake_fw)",
            "        self.stubs.Set(_vmops.firewall_driver,",
            "                       'apply_instance_filter', fake_fw)",
            "",
            "        self.conn.post_live_migration_at_destination(None, fake_instance,",
            "                                                     fake_network_info, None)",
            "        self.assertEqual(fake_fw.called, 3)",
            "",
            "    def test_check_can_live_migrate_destination_with_block_migration(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self.stubs.Set(vm_utils, \"safe_find_sr\", lambda _x: \"asdf\")",
            "",
            "        expected = {'block_migration': True,",
            "                    'migrate_data': {",
            "                        'migrate_send_data': \"fake_migrate_data\",",
            "                        'destination_sr_ref': 'asdf'",
            "                        }",
            "                    }",
            "        result = self.conn.check_can_live_migrate_destination(self.context,",
            "                              {'host': 'host'},",
            "                              {}, {},",
            "                              True, False)",
            "        self.assertEqual(expected, result)",
            "",
            "    def test_check_can_live_migrate_destination_block_migration_fails(self):",
            "        stubs.stubout_session(self.stubs,",
            "                              stubs.FakeSessionForFailedMigrateTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.check_can_live_migrate_destination,",
            "                          self.context, {'host': 'host'},",
            "                          {}, {},",
            "                          True, False)",
            "",
            "    def _add_default_live_migrate_stubs(self, conn):",
            "        def fake_generate_vdi_map(destination_sr_ref, _vm_ref):",
            "            pass",
            "",
            "        def fake_get_iscsi_srs(destination_sr_ref, _vm_ref):",
            "            return []",
            "",
            "        def fake_get_vm_opaque_ref(instance):",
            "            return \"fake_vm\"",
            "",
            "        self.stubs.Set(conn._vmops, \"_generate_vdi_map\",",
            "                       fake_generate_vdi_map)",
            "        self.stubs.Set(conn._vmops, \"_get_iscsi_srs\",",
            "                       fake_get_iscsi_srs)",
            "        self.stubs.Set(conn._vmops, \"_get_vm_opaque_ref\",",
            "                       fake_get_vm_opaque_ref)",
            "",
            "    def test_check_can_live_migrate_source_with_block_migrate(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        dest_check_data = {'block_migration': True,",
            "                           'migrate_data': {",
            "                            'destination_sr_ref': None,",
            "                            'migrate_send_data': None",
            "                           }}",
            "        result = self.conn.check_can_live_migrate_source(self.context,",
            "                                                         {'host': 'host'},",
            "                                                         dest_check_data)",
            "        self.assertEqual(dest_check_data, result)",
            "",
            "    def test_check_can_live_migrate_source_with_block_migrate_iscsi(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def fake_get_iscsi_srs(destination_sr_ref, _vm_ref):",
            "            return ['sr_ref']",
            "        self.stubs.Set(self.conn._vmops, \"_get_iscsi_srs\",",
            "                       fake_get_iscsi_srs)",
            "",
            "        def fake_make_plugin_call(plugin, method, **args):",
            "            return \"true\"",
            "        self.stubs.Set(self.conn._vmops, \"_make_plugin_call\",",
            "                       fake_make_plugin_call)",
            "",
            "        dest_check_data = {'block_migration': True,",
            "                           'migrate_data': {",
            "                            'destination_sr_ref': None,",
            "                            'migrate_send_data': None",
            "                           }}",
            "        result = self.conn.check_can_live_migrate_source(self.context,",
            "                                                         {'host': 'host'},",
            "                                                         dest_check_data)",
            "        self.assertEqual(dest_check_data, result)",
            "",
            "    def test_check_can_live_migrate_source_with_block_iscsi_fails(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def fake_get_iscsi_srs(destination_sr_ref, _vm_ref):",
            "            return ['sr_ref']",
            "        self.stubs.Set(self.conn._vmops, \"_get_iscsi_srs\",",
            "                       fake_get_iscsi_srs)",
            "",
            "        def fake_make_plugin_call(plugin, method, **args):",
            "            return {'returncode': 'error', 'message': 'Plugin not found'}",
            "        self.stubs.Set(self.conn._vmops, \"_make_plugin_call\",",
            "                       fake_make_plugin_call)",
            "",
            "        dest_check_data = {'block_migration': True,",
            "                           'migrate_data': {",
            "                            'destination_sr_ref': None,",
            "                            'migrate_send_data': None",
            "                           }}",
            "",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.check_can_live_migrate_source,",
            "                          self.context, {'host': 'host'},",
            "                          {})",
            "",
            "    def test_check_can_live_migrate_source_with_block_migrate_fails(self):",
            "        stubs.stubout_session(self.stubs,",
            "                              stubs.FakeSessionForFailedMigrateTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        dest_check_data = {'block_migration': True,",
            "                           'migrate_data': {",
            "                            'destination_sr_ref': None,",
            "                            'migrate_send_data': None",
            "                           }}",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.check_can_live_migrate_source,",
            "                          self.context,",
            "                          {'host': 'host'},",
            "                          dest_check_data)",
            "",
            "    def test_check_can_live_migrate_works(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        class fake_aggregate:",
            "            def __init__(self):",
            "                self.metadetails = {\"host\": \"test_host_uuid\"}",
            "",
            "        def fake_aggregate_get_by_host(context, host, key=None):",
            "            self.assertEqual(CONF.host, host)",
            "            return [fake_aggregate()]",
            "",
            "        self.stubs.Set(db, \"aggregate_get_by_host\",",
            "                fake_aggregate_get_by_host)",
            "        self.conn.check_can_live_migrate_destination(self.context,",
            "                {'host': 'host'}, False, False)",
            "",
            "    def test_check_can_live_migrate_fails(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        class fake_aggregate:",
            "            def __init__(self):",
            "                self.metadetails = {\"dest_other\": \"test_host_uuid\"}",
            "",
            "        def fake_aggregate_get_by_host(context, host, key=None):",
            "            self.assertEqual(CONF.host, host)",
            "            return [fake_aggregate()]",
            "",
            "        self.stubs.Set(db, \"aggregate_get_by_host\",",
            "                      fake_aggregate_get_by_host)",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.check_can_live_migrate_destination,",
            "                          self.context, {'host': 'host'}, None, None)",
            "",
            "    def test_live_migration(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_vm_opaque_ref(instance):",
            "            return \"fake_vm\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_vm_opaque_ref\",",
            "                       fake_get_vm_opaque_ref)",
            "",
            "        def fake_get_host_opaque_ref(context, destination_hostname):",
            "            return \"fake_host\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_host_opaque_ref\",",
            "                       fake_get_host_opaque_ref)",
            "",
            "        def post_method(context, instance, destination_hostname,",
            "                        block_migration):",
            "            post_method.called = True",
            "",
            "        self.conn.live_migration(self.conn, None, None, post_method, None)",
            "",
            "        self.assertTrue(post_method.called, \"post_method.called\")",
            "",
            "    def test_live_migration_on_failure(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_vm_opaque_ref(instance):",
            "            return \"fake_vm\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_vm_opaque_ref\",",
            "                       fake_get_vm_opaque_ref)",
            "",
            "        def fake_get_host_opaque_ref(context, destination_hostname):",
            "            return \"fake_host\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_host_opaque_ref\",",
            "                       fake_get_host_opaque_ref)",
            "",
            "        def fake_call_xenapi(*args):",
            "            raise NotImplementedError()",
            "        self.stubs.Set(self.conn._vmops._session, \"call_xenapi\",",
            "                       fake_call_xenapi)",
            "",
            "        def recover_method(context, instance, destination_hostname,",
            "                        block_migration):",
            "            recover_method.called = True",
            "",
            "        self.assertRaises(NotImplementedError, self.conn.live_migration,",
            "                          self.conn, None, None, None, recover_method)",
            "        self.assertTrue(recover_method.called, \"recover_method.called\")",
            "",
            "    def test_live_migration_calls_post_migration(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def post_method(context, instance, destination_hostname,",
            "                        block_migration):",
            "            post_method.called = True",
            "",
            "        # pass block_migration = True and migrate data",
            "        migrate_data = {\"destination_sr_ref\": \"foo\",",
            "                        \"migrate_send_data\": \"bar\"}",
            "        self.conn.live_migration(self.conn, None, None, post_method, None,",
            "                                 True, migrate_data)",
            "        self.assertTrue(post_method.called, \"post_method.called\")",
            "",
            "    def test_live_migration_block_cleans_srs(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def fake_get_iscsi_srs(context, instance):",
            "            return ['sr_ref']",
            "        self.stubs.Set(self.conn._vmops, \"_get_iscsi_srs\",",
            "                       fake_get_iscsi_srs)",
            "",
            "        def fake_forget_sr(context, instance):",
            "            fake_forget_sr.called = True",
            "        self.stubs.Set(volume_utils, \"forget_sr\",",
            "                       fake_forget_sr)",
            "",
            "        def post_method(context, instance, destination_hostname,",
            "                        block_migration):",
            "            post_method.called = True",
            "",
            "        migrate_data = {\"destination_sr_ref\": \"foo\",",
            "                        \"migrate_send_data\": \"bar\"}",
            "        self.conn.live_migration(self.conn, None, None, post_method, None,",
            "                                 True, migrate_data)",
            "",
            "        self.assertTrue(post_method.called, \"post_method.called\")",
            "        self.assertTrue(fake_forget_sr.called, \"forget_sr.called\")",
            "",
            "    def test_live_migration_with_block_migration_raises_invalid_param(self):",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        def fake_get_vm_opaque_ref(instance):",
            "            return \"fake_vm\"",
            "        self.stubs.Set(self.conn._vmops, \"_get_vm_opaque_ref\",",
            "                       fake_get_vm_opaque_ref)",
            "",
            "        def recover_method(context, instance, destination_hostname,",
            "                           block_migration):",
            "            recover_method.called = True",
            "        # pass block_migration = True and no migrate data",
            "        self.assertRaises(exception.InvalidParameterValue,",
            "                          self.conn.live_migration, self.conn,",
            "                          None, None, None, recover_method, True, None)",
            "        self.assertTrue(recover_method.called, \"recover_method.called\")",
            "",
            "    def test_live_migration_with_block_migration_fails_migrate_send(self):",
            "        stubs.stubout_session(self.stubs,",
            "                              stubs.FakeSessionForFailedMigrateTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(self.conn)",
            "",
            "        def recover_method(context, instance, destination_hostname,",
            "                           block_migration):",
            "            recover_method.called = True",
            "        # pass block_migration = True and migrate data",
            "        migrate_data = dict(destination_sr_ref='foo', migrate_send_data='bar')",
            "        self.assertRaises(exception.MigrationError,",
            "                          self.conn.live_migration, self.conn,",
            "                          None, None, None, recover_method, True, migrate_data)",
            "        self.assertTrue(recover_method.called, \"recover_method.called\")",
            "",
            "    def test_live_migrate_block_migration_xapi_call_parameters(self):",
            "",
            "        fake_vdi_map = object()",
            "",
            "        class Session(xenapi_fake.SessionBase):",
            "            def VM_migrate_send(self_, session, vmref, migrate_data, islive,",
            "                                vdi_map, vif_map, options):",
            "                self.assertEquals('SOMEDATA', migrate_data)",
            "                self.assertEquals(fake_vdi_map, vdi_map)",
            "",
            "        stubs.stubout_session(self.stubs, Session)",
            "",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self._add_default_live_migrate_stubs(conn)",
            "",
            "        def fake_generate_vdi_map(destination_sr_ref, _vm_ref):",
            "            return fake_vdi_map",
            "",
            "        self.stubs.Set(conn._vmops, \"_generate_vdi_map\",",
            "                       fake_generate_vdi_map)",
            "",
            "        def dummy_callback(*args, **kwargs):",
            "            pass",
            "",
            "        conn.live_migration(",
            "            self.context, instance_ref=dict(name='ignore'), dest=None,",
            "            post_method=dummy_callback, recover_method=dummy_callback,",
            "            block_migration=\"SOMEDATA\",",
            "            migrate_data=dict(migrate_send_data='SOMEDATA',",
            "                              destination_sr_ref=\"TARGET_SR_OPAQUE_REF\"))",
            "",
            "    def test_live_migrate_pool_migration_xapi_call_parameters(self):",
            "",
            "        class Session(xenapi_fake.SessionBase):",
            "            def VM_pool_migrate(self_, session, vm_ref, host_ref, options):",
            "                self.assertEquals(\"fake_ref\", host_ref)",
            "                self.assertEquals({\"live\": \"true\"}, options)",
            "                raise IOError()",
            "",
            "        stubs.stubout_session(self.stubs, Session)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "        self._add_default_live_migrate_stubs(conn)",
            "",
            "        def fake_get_host_opaque_ref(context, destination):",
            "            return \"fake_ref\"",
            "",
            "        self.stubs.Set(conn._vmops, \"_get_host_opaque_ref\",",
            "                       fake_get_host_opaque_ref)",
            "",
            "        def dummy_callback(*args, **kwargs):",
            "            pass",
            "",
            "        self.assertRaises(IOError, conn.live_migration,",
            "            self.context, instance_ref=dict(name='ignore'), dest=None,",
            "            post_method=dummy_callback, recover_method=dummy_callback,",
            "            block_migration=False, migrate_data={})",
            "",
            "    def test_generate_vdi_map(self):",
            "        stubs.stubout_session(self.stubs, xenapi_fake.SessionBase)",
            "        conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        vm_ref = \"fake_vm_ref\"",
            "",
            "        def fake_find_sr(_session):",
            "            self.assertEquals(conn._session, _session)",
            "            return \"source_sr_ref\"",
            "        self.stubs.Set(vm_utils, \"safe_find_sr\", fake_find_sr)",
            "",
            "        def fake_get_instance_vdis_for_sr(_session, _vm_ref, _sr_ref):",
            "            self.assertEquals(conn._session, _session)",
            "            self.assertEquals(vm_ref, _vm_ref)",
            "            self.assertEquals(\"source_sr_ref\", _sr_ref)",
            "            return [\"vdi0\", \"vdi1\"]",
            "",
            "        self.stubs.Set(vm_utils, \"get_instance_vdis_for_sr\",",
            "                       fake_get_instance_vdis_for_sr)",
            "",
            "        result = conn._vmops._generate_vdi_map(\"dest_sr_ref\", vm_ref)",
            "",
            "        self.assertEquals({\"vdi0\": \"dest_sr_ref\",",
            "                           \"vdi1\": \"dest_sr_ref\"}, result)",
            "",
            "",
            "class XenAPIInjectMetadataTestCase(stubs.XenAPITestBase):",
            "    def setUp(self):",
            "        super(XenAPIInjectMetadataTestCase, self).setUp()",
            "        self.flags(xenapi_connection_url='test_url',",
            "                   xenapi_connection_password='test_pass',",
            "                   firewall_driver='nova.virt.xenapi.firewall.'",
            "                                   'Dom0IptablesFirewallDriver')",
            "        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)",
            "        self.conn = xenapi_conn.XenAPIDriver(fake.FakeVirtAPI(), False)",
            "",
            "        self.xenstore = dict(persist={}, ephem={})",
            "",
            "        self.called_fake_get_vm_opaque_ref = False",
            "",
            "        def fake_get_vm_opaque_ref(inst, instance):",
            "            self.called_fake_get_vm_opaque_ref = True",
            "            if instance[\"uuid\"] == \"not_found\":",
            "                raise exception.NotFound",
            "            self.assertEqual(instance, {'uuid': 'fake'})",
            "            return 'vm_ref'",
            "",
            "        def fake_add_to_param_xenstore(inst, vm_ref, key, val):",
            "            self.assertEqual(vm_ref, 'vm_ref')",
            "            self.xenstore['persist'][key] = val",
            "",
            "        def fake_remove_from_param_xenstore(inst, vm_ref, key):",
            "            self.assertEqual(vm_ref, 'vm_ref')",
            "            if key in self.xenstore['persist']:",
            "                del self.xenstore['persist'][key]",
            "",
            "        def fake_write_to_xenstore(inst, instance, path, value, vm_ref=None):",
            "            self.assertEqual(instance, {'uuid': 'fake'})",
            "            self.assertEqual(vm_ref, 'vm_ref')",
            "            self.xenstore['ephem'][path] = jsonutils.dumps(value)",
            "",
            "        def fake_delete_from_xenstore(inst, instance, path, vm_ref=None):",
            "            self.assertEqual(instance, {'uuid': 'fake'})",
            "            self.assertEqual(vm_ref, 'vm_ref')",
            "            if path in self.xenstore['ephem']:",
            "                del self.xenstore['ephem'][path]",
            "",
            "        self.stubs.Set(vmops.VMOps, '_get_vm_opaque_ref',",
            "                       fake_get_vm_opaque_ref)",
            "        self.stubs.Set(vmops.VMOps, '_add_to_param_xenstore',",
            "                       fake_add_to_param_xenstore)",
            "        self.stubs.Set(vmops.VMOps, '_remove_from_param_xenstore',",
            "                       fake_remove_from_param_xenstore)",
            "        self.stubs.Set(vmops.VMOps, '_write_to_xenstore',",
            "                       fake_write_to_xenstore)",
            "        self.stubs.Set(vmops.VMOps, '_delete_from_xenstore',",
            "                       fake_delete_from_xenstore)",
            "",
            "    def test_inject_instance_metadata(self):",
            "",
            "        # Add some system_metadata to ensure it doesn't get added",
            "        # to xenstore",
            "        instance = dict(metadata=[{'key': 'a', 'value': 1},",
            "                                  {'key': 'b', 'value': 2},",
            "                                  {'key': 'c', 'value': 3},",
            "                                  # Check xenstore key sanitizing",
            "                                  {'key': 'hi.there', 'value': 4},",
            "                                  {'key': 'hi!t.e/e', 'value': 5}],",
            "                                  # Check xenstore key sanitizing",
            "                        system_metadata=[{'key': 'sys_a', 'value': 1},",
            "                                         {'key': 'sys_b', 'value': 2},",
            "                                         {'key': 'sys_c', 'value': 3}],",
            "                        uuid='fake')",
            "        self.conn._vmops._inject_instance_metadata(instance, 'vm_ref')",
            "",
            "        self.assertEqual(self.xenstore, {",
            "                'persist': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '2',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    'vm-data/user-metadata/hi_there': '4',",
            "                    'vm-data/user-metadata/hi_t_e_e': '5',",
            "                    },",
            "                'ephem': {},",
            "                })",
            "",
            "    def test_change_instance_metadata_add(self):",
            "        # Test XenStore key sanitizing here, too.",
            "        diff = {'test.key': ['+', 4]}",
            "        instance = {'uuid': 'fake'}",
            "        self.xenstore = {",
            "            'persist': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            'ephem': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            }",
            "",
            "        self.conn._vmops.change_instance_metadata(instance, diff)",
            "",
            "        self.assertEqual(self.xenstore, {",
            "                'persist': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '2',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    'vm-data/user-metadata/test_key': '4',",
            "                    },",
            "                'ephem': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '2',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    'vm-data/user-metadata/test_key': '4',",
            "                    },",
            "                })",
            "",
            "    def test_change_instance_metadata_update(self):",
            "        diff = dict(b=['+', 4])",
            "        instance = {'uuid': 'fake'}",
            "        self.xenstore = {",
            "            'persist': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            'ephem': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            }",
            "",
            "        self.conn._vmops.change_instance_metadata(instance, diff)",
            "",
            "        self.assertEqual(self.xenstore, {",
            "                'persist': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '4',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    },",
            "                'ephem': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/b': '4',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    },",
            "                })",
            "",
            "    def test_change_instance_metadata_delete(self):",
            "        diff = dict(b=['-'])",
            "        instance = {'uuid': 'fake'}",
            "        self.xenstore = {",
            "            'persist': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            'ephem': {",
            "                'vm-data/user-metadata/a': '1',",
            "                'vm-data/user-metadata/b': '2',",
            "                'vm-data/user-metadata/c': '3',",
            "                },",
            "            }",
            "",
            "        self.conn._vmops.change_instance_metadata(instance, diff)",
            "",
            "        self.assertEqual(self.xenstore, {",
            "                'persist': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    },",
            "                'ephem': {",
            "                    'vm-data/user-metadata/a': '1',",
            "                    'vm-data/user-metadata/c': '3',",
            "                    },",
            "                })",
            "",
            "    def test_change_instance_metadata_not_found(self):",
            "        instance = {'uuid': 'not_found'}",
            "        self.conn._vmops.change_instance_metadata(instance, \"fake_diff\")",
            "        self.assertTrue(self.called_fake_get_vm_opaque_ref)",
            "",
            "",
            "class XenAPISessionTestCase(test.NoDBTestCase):",
            "    def _get_mock_xapisession(self, software_version):",
            "        class MockXapiSession(xenapi_conn.XenAPISession):",
            "            def __init__(_ignore):",
            "                \"Skip the superclass's dirty init\"",
            "",
            "            def _get_software_version(_ignore):",
            "                return software_version",
            "",
            "        return MockXapiSession()",
            "",
            "    def test_local_session(self):",
            "        session = self._get_mock_xapisession({})",
            "        session.is_local_connection = True",
            "        session.XenAPI = self.mox.CreateMockAnything()",
            "        session.XenAPI.xapi_local().AndReturn(\"local_connection\")",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEqual(\"local_connection\",",
            "                         session._create_session(\"unix://local\"))",
            "",
            "    def test_remote_session(self):",
            "        session = self._get_mock_xapisession({})",
            "        session.is_local_connection = False",
            "        session.XenAPI = self.mox.CreateMockAnything()",
            "        session.XenAPI.Session(\"url\").AndReturn(\"remote_connection\")",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEqual(\"remote_connection\", session._create_session(\"url\"))",
            "",
            "    def test_get_product_version_product_brand_does_not_fail(self):",
            "        session = self._get_mock_xapisession({",
            "                    'build_number': '0',",
            "                    'date': '2012-08-03',",
            "                    'hostname': 'komainu',",
            "                    'linux': '3.2.0-27-generic',",
            "                    'network_backend': 'bridge',",
            "                    'platform_name': 'XCP_Kronos',",
            "                    'platform_version': '1.6.0',",
            "                    'xapi': '1.3',",
            "                    'xen': '4.1.2',",
            "                    'xencenter_max': '1.10',",
            "                    'xencenter_min': '1.10'",
            "                })",
            "",
            "        self.assertEquals(",
            "            ((1, 6, 0), None),",
            "            session._get_product_version_and_brand()",
            "        )",
            "",
            "    def test_get_product_version_product_brand_xs_6(self):",
            "        session = self._get_mock_xapisession({",
            "                    'product_brand': 'XenServer',",
            "                    'product_version': '6.0.50',",
            "                    'platform_version': '0.0.1'",
            "                })",
            "",
            "        self.assertEquals(",
            "            ((6, 0, 50), 'XenServer'),",
            "            session._get_product_version_and_brand()",
            "        )",
            "",
            "",
            "class XenAPIFakeTestCase(test.NoDBTestCase):",
            "    def test_query_matches(self):",
            "        record = {'a': '1', 'b': '2', 'c_d': '3'}",
            "",
            "        tests = {'field \"a\"=\"1\"': True,",
            "                 'field \"b\"=\"2\"': True,",
            "                 'field \"b\"=\"4\"': False,",
            "                 'not field \"b\"=\"4\"': True,",
            "                 'field \"a\"=\"1\" and field \"b\"=\"4\"': False,",
            "                 'field \"a\"=\"1\" or field \"b\"=\"4\"': True,",
            "                 'field \"c__d\"=\"3\"': True,",
            "                 'field \\'b\\'=\\'2\\'': True,",
            "                 }",
            "",
            "        for query in tests.keys():",
            "            expected = tests[query]",
            "            fail_msg = \"for test '%s'\" % query",
            "            self.assertEqual(xenapi_fake._query_matches(record, query),",
            "                             expected, fail_msg)",
            "",
            "    def test_query_bad_format(self):",
            "        record = {'a': '1', 'b': '2', 'c': '3'}",
            "",
            "        tests = ['\"a\"=\"1\" or \"b\"=\"4\"',",
            "                 'a=1',",
            "                 ]",
            "",
            "        for query in tests:",
            "            fail_msg = \"for test '%s'\" % query",
            "            self.assertFalse(xenapi_fake._query_matches(record, query),",
            "                             fail_msg)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "3301": [
                "XenAPILiveMigrateTestCase",
                "test_post_live_migration_at_destination"
            ]
        },
        "addLocation": []
    },
    "nova/virt/xenapi/driver.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-# vim: tabstop=4 shiftwidth=4 softtabstop=4"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " # Copyright (c) 2010 Citrix Systems, Inc."
            },
            "3": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " # Copyright 2010 OpenStack Foundation"
            },
            "4": {
                "beforePatchRowNumber": 555,
                "afterPatchRowNumber": 554,
                "PatchRowcode": "         :params network_info: instance network information"
            },
            "5": {
                "beforePatchRowNumber": 556,
                "afterPatchRowNumber": 555,
                "PatchRowcode": "         :params : block_migration: if true, post operation of block_migraiton."
            },
            "6": {
                "beforePatchRowNumber": 557,
                "afterPatchRowNumber": 556,
                "PatchRowcode": "         \"\"\""
            },
            "7": {
                "beforePatchRowNumber": 558,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # TODO(JohnGarbutt) look at moving/downloading ramdisk and kernel"
            },
            "8": {
                "beforePatchRowNumber": 559,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        pass"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 557,
                "PatchRowcode": "+        self._vmops.post_live_migration_at_destination(ctxt, instance_ref,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 558,
                "PatchRowcode": "+                network_info, block_device_info, block_device_info)"
            },
            "11": {
                "beforePatchRowNumber": 560,
                "afterPatchRowNumber": 559,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 561,
                "afterPatchRowNumber": 560,
                "PatchRowcode": "     def unfilter_instance(self, instance_ref, network_info):"
            },
            "13": {
                "beforePatchRowNumber": 562,
                "afterPatchRowNumber": 561,
                "PatchRowcode": "         \"\"\"Removes security groups configured for an instance.\"\"\""
            }
        },
        "frontPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright (c) 2010 Citrix Systems, Inc.",
            "# Copyright 2010 OpenStack Foundation",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "A driver for XenServer or Xen Cloud Platform.",
            "",
            "**Related Flags**",
            "",
            ":xenapi_connection_url:  URL for connection to XenServer/Xen Cloud Platform.",
            ":xenapi_connection_username:  Username for connection to XenServer/Xen Cloud",
            "                              Platform (default: root).",
            ":xenapi_connection_password:  Password for connection to XenServer/Xen Cloud",
            "                              Platform.",
            ":target_host:                the iSCSI Target Host IP address, i.e. the IP",
            "                             address for the nova-volume host",
            ":target_port:                iSCSI Target Port, 3260 Default",
            ":iqn_prefix:                 IQN Prefix, e.g. 'iqn.2010-10.org.openstack'",
            "",
            "**Variable Naming Scheme**",
            "",
            "- suffix \"_ref\" for opaque references",
            "- suffix \"_uuid\" for UUIDs",
            "- suffix \"_rec\" for record objects",
            "\"\"\"",
            "",
            "import contextlib",
            "import cPickle as pickle",
            "import math",
            "import time",
            "import urlparse",
            "import xmlrpclib",
            "",
            "from eventlet import queue",
            "from eventlet import timeout",
            "from oslo.config import cfg",
            "",
            "from nova import context",
            "from nova import exception",
            "from nova.openstack.common.gettextutils import _",
            "from nova.openstack.common import jsonutils",
            "from nova.openstack.common import log as logging",
            "from nova import utils",
            "from nova.virt import driver",
            "from nova.virt.xenapi import host",
            "from nova.virt.xenapi import pool",
            "from nova.virt.xenapi import pool_states",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import vmops",
            "from nova.virt.xenapi import volumeops",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "xenapi_opts = [",
            "    cfg.StrOpt('xenapi_connection_url',",
            "               help='URL for connection to XenServer/Xen Cloud Platform. '",
            "                    'A special value of unix://local can be used to connect '",
            "                    'to the local unix socket.  '",
            "                    'Required if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.StrOpt('xenapi_connection_username',",
            "               default='root',",
            "               help='Username for connection to XenServer/Xen Cloud Platform. '",
            "                    'Used only if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.StrOpt('xenapi_connection_password',",
            "               help='Password for connection to XenServer/Xen Cloud Platform. '",
            "                    'Used only if compute_driver=xenapi.XenAPIDriver',",
            "               secret=True),",
            "    cfg.IntOpt('xenapi_connection_concurrent',",
            "               default=5,",
            "               help='Maximum number of concurrent XenAPI connections. '",
            "                    'Used only if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.FloatOpt('xenapi_vhd_coalesce_poll_interval',",
            "                 default=5.0,",
            "                 help='The interval used for polling of coalescing vhds. '",
            "                      'Used only if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.BoolOpt('xenapi_check_host',",
            "                default=True,",
            "                help='Ensure compute service is running on host XenAPI '",
            "                     'connects to.'),",
            "    cfg.IntOpt('xenapi_vhd_coalesce_max_attempts',",
            "               default=5,",
            "               help='Max number of times to poll for VHD to coalesce. '",
            "                    'Used only if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.StrOpt('xenapi_sr_base_path',",
            "               default='/var/run/sr-mount',",
            "               help='Base path to the storage repository'),",
            "    cfg.StrOpt('target_host',",
            "               help='iSCSI Target Host'),",
            "    cfg.StrOpt('target_port',",
            "               default='3260',",
            "               help='iSCSI Target Port, 3260 Default'),",
            "    cfg.StrOpt('iqn_prefix',",
            "               default='iqn.2010-10.org.openstack',",
            "               help='IQN Prefix'),",
            "    # NOTE(sirp): This is a work-around for a bug in Ubuntu Maverick,",
            "    # when we pull support for it, we should remove this",
            "    cfg.BoolOpt('xenapi_remap_vbd_dev',",
            "                default=False,",
            "                help='Used to enable the remapping of VBD dev '",
            "                     '(Works around an issue in Ubuntu Maverick)'),",
            "    cfg.StrOpt('xenapi_remap_vbd_dev_prefix',",
            "               default='sd',",
            "               help='Specify prefix to remap VBD dev to '",
            "                    '(ex. /dev/xvdb -> /dev/sdb)'),",
            "    cfg.IntOpt('xenapi_login_timeout',",
            "               default=10,",
            "               help='Timeout in seconds for XenAPI login.'),",
            "    ]",
            "",
            "CONF = cfg.CONF",
            "CONF.register_opts(xenapi_opts)",
            "CONF.import_opt('host', 'nova.netconf')",
            "",
            "",
            "class XenAPIDriver(driver.ComputeDriver):",
            "    \"\"\"A connection to XenServer or Xen Cloud Platform.\"\"\"",
            "",
            "    def __init__(self, virtapi, read_only=False):",
            "        super(XenAPIDriver, self).__init__(virtapi)",
            "",
            "        url = CONF.xenapi_connection_url",
            "        username = CONF.xenapi_connection_username",
            "        password = CONF.xenapi_connection_password",
            "        if not url or password is None:",
            "            raise Exception(_('Must specify xenapi_connection_url, '",
            "                              'xenapi_connection_username (optionally), and '",
            "                              'xenapi_connection_password to use '",
            "                              'compute_driver=xenapi.XenAPIDriver'))",
            "",
            "        self._session = XenAPISession(url, username, password, self.virtapi)",
            "        self._volumeops = volumeops.VolumeOps(self._session)",
            "        self._host_state = None",
            "        self._host = host.Host(self._session, self.virtapi)",
            "        self._vmops = vmops.VMOps(self._session, self.virtapi)",
            "        self._initiator = None",
            "        self._hypervisor_hostname = None",
            "        self._pool = pool.ResourcePool(self._session, self.virtapi)",
            "",
            "    @property",
            "    def host_state(self):",
            "        if not self._host_state:",
            "            self._host_state = host.HostState(self._session)",
            "        return self._host_state",
            "",
            "    def init_host(self, host):",
            "        if CONF.xenapi_check_host:",
            "            vm_utils.ensure_correct_host(self._session)",
            "",
            "        try:",
            "            vm_utils.cleanup_attached_vdis(self._session)",
            "        except Exception:",
            "            LOG.exception(_('Failure while cleaning up attached VDIs'))",
            "",
            "    def instance_exists(self, instance_name):",
            "        \"\"\"Checks existence of an instance on the host.",
            "",
            "        :param instance_name: The name of the instance to lookup",
            "",
            "        Returns True if an instance with the supplied name exists on",
            "        the host, False otherwise.",
            "",
            "        NOTE(belliott): This is an override of the base method for",
            "        efficiency.",
            "        \"\"\"",
            "        return self._vmops.instance_exists(instance_name)",
            "",
            "    def estimate_instance_overhead(self, instance_info):",
            "        \"\"\"Get virtualization overhead required to build an instance of the",
            "        given flavor.",
            "",
            "        :param instance_info: Instance/flavor to calculate overhead for.",
            "        :returns: Overhead memory in MB.",
            "        \"\"\"",
            "",
            "        # XenServer memory overhead is proportional to the size of the",
            "        # VM.  Larger flavor VMs become more efficient with respect to",
            "        # overhead.",
            "",
            "        # interpolated formula to predict overhead required per vm.",
            "        # based on data from:",
            "        # https://wiki.openstack.org/wiki/XenServer/Overhead",
            "        base = 3  # MB",
            "        per_mb = 0.0081  # MB",
            "",
            "        memory_mb = instance_info['memory_mb']",
            "        overhead = memory_mb * per_mb + base",
            "        overhead = math.ceil(overhead)",
            "        return {'memory_mb': overhead}",
            "",
            "    def list_instances(self):",
            "        \"\"\"List VM instances.\"\"\"",
            "        return self._vmops.list_instances()",
            "",
            "    def list_instance_uuids(self):",
            "        \"\"\"Get the list of nova instance uuids for VMs found on the",
            "        hypervisor.",
            "        \"\"\"",
            "        return self._vmops.list_instance_uuids()",
            "",
            "    def spawn(self, context, instance, image_meta, injected_files,",
            "              admin_password, network_info=None, block_device_info=None):",
            "        \"\"\"Create VM instance.\"\"\"",
            "        self._vmops.spawn(context, instance, image_meta, injected_files,",
            "                          admin_password, network_info, block_device_info)",
            "",
            "    def confirm_migration(self, migration, instance, network_info):",
            "        \"\"\"Confirms a resize, destroying the source VM.\"\"\"",
            "        # TODO(Vek): Need to pass context in for access to auth_token",
            "        self._vmops.confirm_migration(migration, instance, network_info)",
            "",
            "    def finish_revert_migration(self, instance, network_info,",
            "                                block_device_info=None, power_on=True):",
            "        \"\"\"Finish reverting a resize.\"\"\"",
            "        # NOTE(vish): Xen currently does not use network info.",
            "        self._vmops.finish_revert_migration(instance, block_device_info,",
            "                                            power_on)",
            "",
            "    def finish_migration(self, context, migration, instance, disk_info,",
            "                         network_info, image_meta, resize_instance=False,",
            "                         block_device_info=None, power_on=True):",
            "        \"\"\"Completes a resize, turning on the migrated instance.\"\"\"",
            "        self._vmops.finish_migration(context, migration, instance, disk_info,",
            "                                     network_info, image_meta, resize_instance,",
            "                                     block_device_info, power_on)",
            "",
            "    def snapshot(self, context, instance, image_id, update_task_state):",
            "        \"\"\"Create snapshot from a running VM instance.\"\"\"",
            "        self._vmops.snapshot(context, instance, image_id, update_task_state)",
            "",
            "    def reboot(self, context, instance, network_info, reboot_type,",
            "               block_device_info=None, bad_volumes_callback=None):",
            "        \"\"\"Reboot VM instance.\"\"\"",
            "        self._vmops.reboot(instance, reboot_type,",
            "                           bad_volumes_callback=bad_volumes_callback)",
            "",
            "    def set_admin_password(self, instance, new_pass):",
            "        \"\"\"Set the root/admin password on the VM instance.\"\"\"",
            "        self._vmops.set_admin_password(instance, new_pass)",
            "",
            "    def inject_file(self, instance, b64_path, b64_contents):",
            "        \"\"\"Create a file on the VM instance. The file path and contents",
            "        should be base64-encoded.",
            "        \"\"\"",
            "        self._vmops.inject_file(instance, b64_path, b64_contents)",
            "",
            "    def change_instance_metadata(self, context, instance, diff):",
            "        \"\"\"Apply a diff to the instance metadata.\"\"\"",
            "        self._vmops.change_instance_metadata(instance, diff)",
            "",
            "    def destroy(self, instance, network_info, block_device_info=None,",
            "                destroy_disks=True, context=None):",
            "        \"\"\"Destroy VM instance.\"\"\"",
            "        self._vmops.destroy(instance, network_info, block_device_info,",
            "                            destroy_disks)",
            "",
            "    def pause(self, instance):",
            "        \"\"\"Pause VM instance.\"\"\"",
            "        self._vmops.pause(instance)",
            "",
            "    def unpause(self, instance):",
            "        \"\"\"Unpause paused VM instance.\"\"\"",
            "        self._vmops.unpause(instance)",
            "",
            "    def migrate_disk_and_power_off(self, context, instance, dest,",
            "                                   instance_type, network_info,",
            "                                   block_device_info=None):",
            "        \"\"\"Transfers the VHD of a running instance to another host, then shuts",
            "        off the instance copies over the COW disk",
            "        \"\"\"",
            "        # NOTE(vish): Xen currently does not use network info.",
            "        return self._vmops.migrate_disk_and_power_off(context, instance,",
            "                    dest, instance_type, block_device_info)",
            "",
            "    def suspend(self, instance):",
            "        \"\"\"suspend the specified instance.\"\"\"",
            "        self._vmops.suspend(instance)",
            "",
            "    def resume(self, instance, network_info, block_device_info=None):",
            "        \"\"\"resume the specified instance.\"\"\"",
            "        self._vmops.resume(instance)",
            "",
            "    def rescue(self, context, instance, network_info, image_meta,",
            "               rescue_password):",
            "        \"\"\"Rescue the specified instance.\"\"\"",
            "        self._vmops.rescue(context, instance, network_info, image_meta,",
            "                           rescue_password)",
            "",
            "    def unrescue(self, instance, network_info):",
            "        \"\"\"Unrescue the specified instance.\"\"\"",
            "        self._vmops.unrescue(instance)",
            "",
            "    def power_off(self, instance):",
            "        \"\"\"Power off the specified instance.\"\"\"",
            "        self._vmops.power_off(instance)",
            "",
            "    def power_on(self, context, instance, network_info,",
            "                 block_device_info=None):",
            "        \"\"\"Power on the specified instance.\"\"\"",
            "        self._vmops.power_on(instance)",
            "",
            "    def soft_delete(self, instance):",
            "        \"\"\"Soft delete the specified instance.\"\"\"",
            "        self._vmops.soft_delete(instance)",
            "",
            "    def restore(self, instance):",
            "        \"\"\"Restore the specified instance.\"\"\"",
            "        self._vmops.restore(instance)",
            "",
            "    def poll_rebooting_instances(self, timeout, instances):",
            "        \"\"\"Poll for rebooting instances.\"\"\"",
            "        self._vmops.poll_rebooting_instances(timeout, instances)",
            "",
            "    def reset_network(self, instance):",
            "        \"\"\"reset networking for specified instance.\"\"\"",
            "        self._vmops.reset_network(instance)",
            "",
            "    def inject_network_info(self, instance, network_info):",
            "        \"\"\"inject network info for specified instance.\"\"\"",
            "        self._vmops.inject_network_info(instance, network_info)",
            "",
            "    def plug_vifs(self, instance_ref, network_info):",
            "        \"\"\"Plug VIFs into networks.\"\"\"",
            "        self._vmops.plug_vifs(instance_ref, network_info)",
            "",
            "    def unplug_vifs(self, instance_ref, network_info):",
            "        \"\"\"Unplug VIFs from networks.\"\"\"",
            "        self._vmops.unplug_vifs(instance_ref, network_info)",
            "",
            "    def get_info(self, instance):",
            "        \"\"\"Return data about VM instance.\"\"\"",
            "        return self._vmops.get_info(instance)",
            "",
            "    def get_diagnostics(self, instance):",
            "        \"\"\"Return data about VM diagnostics.\"\"\"",
            "        return self._vmops.get_diagnostics(instance)",
            "",
            "    def get_all_bw_counters(self, instances):",
            "        \"\"\"Return bandwidth usage counters for each interface on each",
            "           running VM.",
            "        \"\"\"",
            "",
            "        # we only care about VMs that correspond to a nova-managed",
            "        # instance:",
            "        imap = dict([(inst['name'], inst['uuid']) for inst in instances])",
            "        bwcounters = []",
            "",
            "        # get a dictionary of instance names.  values are dictionaries",
            "        # of mac addresses with values that are the bw counters:",
            "        # e.g. {'instance-001' : { 12:34:56:78:90:12 : {'bw_in': 0, ....}}",
            "        all_counters = self._vmops.get_all_bw_counters()",
            "        for instance_name, counters in all_counters.iteritems():",
            "            if instance_name in imap:",
            "                # yes these are stats for a nova-managed vm",
            "                # correlate the stats with the nova instance uuid:",
            "                for vif_counter in counters.values():",
            "                    vif_counter['uuid'] = imap[instance_name]",
            "                    bwcounters.append(vif_counter)",
            "        return bwcounters",
            "",
            "    def get_console_output(self, instance):",
            "        \"\"\"Return snapshot of console.\"\"\"",
            "        return self._vmops.get_console_output(instance)",
            "",
            "    def get_vnc_console(self, instance):",
            "        \"\"\"Return link to instance's VNC console.\"\"\"",
            "        return self._vmops.get_vnc_console(instance)",
            "",
            "    def get_volume_connector(self, instance):",
            "        \"\"\"Return volume connector information.\"\"\"",
            "        if not self._initiator or not self._hypervisor_hostname:",
            "            stats = self.get_host_stats(refresh=True)",
            "            try:",
            "                self._initiator = stats['host_other-config']['iscsi_iqn']",
            "                self._hypervisor_hostname = stats['host_hostname']",
            "            except (TypeError, KeyError) as err:",
            "                LOG.warn(_('Could not determine key: %s') % err,",
            "                         instance=instance)",
            "                self._initiator = None",
            "        return {",
            "            'ip': self.get_host_ip_addr(),",
            "            'initiator': self._initiator,",
            "            'host': self._hypervisor_hostname",
            "        }",
            "",
            "    @staticmethod",
            "    def get_host_ip_addr():",
            "        xs_url = urlparse.urlparse(CONF.xenapi_connection_url)",
            "        return xs_url.netloc",
            "",
            "    def attach_volume(self, context, connection_info, instance, mountpoint,",
            "                      encryption=None):",
            "        \"\"\"Attach volume storage to VM instance.\"\"\"",
            "        return self._volumeops.attach_volume(connection_info,",
            "                                             instance['name'],",
            "                                             mountpoint)",
            "",
            "    def detach_volume(self, connection_info, instance, mountpoint,",
            "                      encryption=None):",
            "        \"\"\"Detach volume storage from VM instance.\"\"\"",
            "        return self._volumeops.detach_volume(connection_info,",
            "                                             instance['name'],",
            "                                             mountpoint)",
            "",
            "    def get_console_pool_info(self, console_type):",
            "        xs_url = urlparse.urlparse(CONF.xenapi_connection_url)",
            "        return {'address': xs_url.netloc,",
            "                'username': CONF.xenapi_connection_username,",
            "                'password': CONF.xenapi_connection_password}",
            "",
            "    def get_available_resource(self, nodename):",
            "        \"\"\"Retrieve resource information.",
            "",
            "        This method is called when nova-compute launches, and",
            "        as part of a periodic task that records the results in the DB.",
            "",
            "        :param nodename: ignored in this driver",
            "        :returns: dictionary describing resources",
            "",
            "        \"\"\"",
            "        host_stats = self.get_host_stats(refresh=True)",
            "",
            "        # Updating host information",
            "        total_ram_mb = host_stats['host_memory_total'] / (1024 * 1024)",
            "        # NOTE(belliott) memory-free-computed is a value provided by XenServer",
            "        # for gauging free memory more conservatively than memory-free.",
            "        free_ram_mb = host_stats['host_memory_free_computed'] / (1024 * 1024)",
            "        total_disk_gb = host_stats['disk_total'] / (1024 * 1024 * 1024)",
            "        used_disk_gb = host_stats['disk_used'] / (1024 * 1024 * 1024)",
            "        hyper_ver = utils.convert_version_to_int(self._session.product_version)",
            "        dic = {'vcpus': 0,",
            "               'memory_mb': total_ram_mb,",
            "               'local_gb': total_disk_gb,",
            "               'vcpus_used': 0,",
            "               'memory_mb_used': total_ram_mb - free_ram_mb,",
            "               'local_gb_used': used_disk_gb,",
            "               'hypervisor_type': 'xen',",
            "               'hypervisor_version': hyper_ver,",
            "               'hypervisor_hostname': host_stats['host_hostname'],",
            "               'cpu_info': host_stats['host_cpu_info']['cpu_count'],",
            "               'supported_instances': jsonutils.dumps(",
            "                   host_stats['supported_instances'])}",
            "",
            "        return dic",
            "",
            "    def ensure_filtering_rules_for_instance(self, instance_ref, network_info):",
            "        # NOTE(salvatore-orlando): it enforces security groups on",
            "        # host initialization and live migration.",
            "        # In XenAPI we do not assume instances running upon host initialization",
            "        return",
            "",
            "    def check_can_live_migrate_destination(self, ctxt, instance_ref,",
            "                src_compute_info, dst_compute_info,",
            "                block_migration=False, disk_over_commit=False):",
            "        \"\"\"Check if it is possible to execute live migration.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param block_migration: if true, prepare for block migration",
            "        :param disk_over_commit: if true, allow disk over commit",
            "",
            "        \"\"\"",
            "        return self._vmops.check_can_live_migrate_destination(ctxt,",
            "                                                              instance_ref,",
            "                                                              block_migration,",
            "                                                              disk_over_commit)",
            "",
            "    def check_can_live_migrate_destination_cleanup(self, ctxt,",
            "                                                   dest_check_data):",
            "        \"\"\"Do required cleanup on dest host after check_can_live_migrate calls",
            "",
            "        :param ctxt: security context",
            "        :param disk_over_commit: if true, allow disk over commit",
            "        \"\"\"",
            "        pass",
            "",
            "    def check_can_live_migrate_source(self, ctxt, instance_ref,",
            "                                      dest_check_data):",
            "        \"\"\"Check if it is possible to execute live migration.",
            "",
            "        This checks if the live migration can succeed, based on the",
            "        results from check_can_live_migrate_destination.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance",
            "        :param dest_check_data: result of check_can_live_migrate_destination",
            "                                includes the block_migration flag",
            "        \"\"\"",
            "        return self._vmops.check_can_live_migrate_source(ctxt, instance_ref,",
            "                                                         dest_check_data)",
            "",
            "    def get_instance_disk_info(self, instance_name):",
            "        \"\"\"Used by libvirt for live migration. We rely on xenapi",
            "        checks to do this for us.",
            "        \"\"\"",
            "        pass",
            "",
            "    def live_migration(self, ctxt, instance_ref, dest,",
            "                       post_method, recover_method, block_migration=False,",
            "                       migrate_data=None):",
            "        \"\"\"Performs the live migration of the specified instance.",
            "",
            "        :params ctxt: security context",
            "        :params instance_ref:",
            "            nova.db.sqlalchemy.models.Instance object",
            "            instance object that is migrated.",
            "        :params dest: destination host",
            "        :params post_method:",
            "            post operation method.",
            "            expected nova.compute.manager.post_live_migration.",
            "        :params recover_method:",
            "            recovery method when any exception occurs.",
            "            expected nova.compute.manager.recover_live_migration.",
            "        :params block_migration: if true, migrate VM disk.",
            "        :params migrate_data: implementation specific params",
            "        \"\"\"",
            "        self._vmops.live_migrate(ctxt, instance_ref, dest, post_method,",
            "                                 recover_method, block_migration, migrate_data)",
            "",
            "    def pre_live_migration(self, context, instance_ref, block_device_info,",
            "                           network_info, data, migrate_data=None):",
            "        \"\"\"Preparation live migration.",
            "",
            "        :params block_device_info:",
            "            It must be the result of _get_instance_volume_bdms()",
            "            at compute manager.",
            "        \"\"\"",
            "        # TODO(JohnGarbutt) look again when boot-from-volume hits trunk",
            "        pre_live_migration_result = {}",
            "        pre_live_migration_result['sr_uuid_map'] = \\",
            "                 self._vmops.attach_block_device_volumes(block_device_info)",
            "        return pre_live_migration_result",
            "",
            "    def post_live_migration_at_destination(self, ctxt, instance_ref,",
            "                                           network_info, block_migration,",
            "                                           block_device_info=None):",
            "        \"\"\"Post operation of live migration at destination host.",
            "",
            "        :params ctxt: security context",
            "        :params instance_ref:",
            "            nova.db.sqlalchemy.models.Instance object",
            "            instance object that is migrated.",
            "        :params network_info: instance network information",
            "        :params : block_migration: if true, post operation of block_migraiton.",
            "        \"\"\"",
            "        # TODO(JohnGarbutt) look at moving/downloading ramdisk and kernel",
            "        pass",
            "",
            "    def unfilter_instance(self, instance_ref, network_info):",
            "        \"\"\"Removes security groups configured for an instance.\"\"\"",
            "        return self._vmops.unfilter_instance(instance_ref, network_info)",
            "",
            "    def refresh_security_group_rules(self, security_group_id):",
            "        \"\"\"Updates security group rules for all instances associated with a",
            "        given security group.",
            "",
            "        Invoked when security group rules are updated.",
            "        \"\"\"",
            "        return self._vmops.refresh_security_group_rules(security_group_id)",
            "",
            "    def refresh_security_group_members(self, security_group_id):",
            "        \"\"\"Updates security group rules for all instances associated with a",
            "        given security group.",
            "",
            "        Invoked when instances are added/removed to a security group.",
            "        \"\"\"",
            "        return self._vmops.refresh_security_group_members(security_group_id)",
            "",
            "    def refresh_instance_security_rules(self, instance):",
            "        \"\"\"Updates security group rules for specified instance.",
            "",
            "        Invoked when instances are added/removed to a security group",
            "        or when a rule is added/removed to a security group.",
            "        \"\"\"",
            "        return self._vmops.refresh_instance_security_rules(instance)",
            "",
            "    def refresh_provider_fw_rules(self):",
            "        return self._vmops.refresh_provider_fw_rules()",
            "",
            "    def get_host_stats(self, refresh=False):",
            "        \"\"\"Return the current state of the host.",
            "",
            "           If 'refresh' is True, run the update first.",
            "         \"\"\"",
            "        return self.host_state.get_host_stats(refresh=refresh)",
            "",
            "    def host_power_action(self, host, action):",
            "        \"\"\"The only valid values for 'action' on XenServer are 'reboot' or",
            "        'shutdown', even though the API also accepts 'startup'. As this is",
            "        not technically possible on XenServer, since the host is the same",
            "        physical machine as the hypervisor, if this is requested, we need to",
            "        raise an exception.",
            "        \"\"\"",
            "        if action in (\"reboot\", \"shutdown\"):",
            "            return self._host.host_power_action(host, action)",
            "        else:",
            "            msg = _(\"Host startup on XenServer is not supported.\")",
            "            raise NotImplementedError(msg)",
            "",
            "    def set_host_enabled(self, host, enabled):",
            "        \"\"\"Sets the specified host's ability to accept new instances.\"\"\"",
            "        return self._host.set_host_enabled(host, enabled)",
            "",
            "    def get_host_uptime(self, host):",
            "        \"\"\"Returns the result of calling \"uptime\" on the target host.\"\"\"",
            "        return self._host.get_host_uptime(host)",
            "",
            "    def host_maintenance_mode(self, host, mode):",
            "        \"\"\"Start/Stop host maintenance window. On start, it triggers",
            "        guest VMs evacuation.",
            "        \"\"\"",
            "        return self._host.host_maintenance_mode(host, mode)",
            "",
            "    def add_to_aggregate(self, context, aggregate, host, **kwargs):",
            "        \"\"\"Add a compute host to an aggregate.\"\"\"",
            "        return self._pool.add_to_aggregate(context, aggregate, host, **kwargs)",
            "",
            "    def remove_from_aggregate(self, context, aggregate, host, **kwargs):",
            "        \"\"\"Remove a compute host from an aggregate.\"\"\"",
            "        return self._pool.remove_from_aggregate(context,",
            "                                                aggregate, host, **kwargs)",
            "",
            "    def undo_aggregate_operation(self, context, op, aggregate,",
            "                                  host, set_error=True):",
            "        \"\"\"Undo aggregate operation when pool error raised.\"\"\"",
            "        return self._pool.undo_aggregate_operation(context, op,",
            "                aggregate, host, set_error)",
            "",
            "    def resume_state_on_host_boot(self, context, instance, network_info,",
            "                                  block_device_info=None):",
            "        \"\"\"resume guest state when a host is booted.\"\"\"",
            "        self._vmops.power_on(instance)",
            "",
            "    def get_per_instance_usage(self):",
            "        \"\"\"Get information about instance resource usage.",
            "",
            "        :returns: dict of  nova uuid => dict of usage",
            "        info",
            "        \"\"\"",
            "        return self._vmops.get_per_instance_usage()",
            "",
            "",
            "class XenAPISession(object):",
            "    \"\"\"The session to invoke XenAPI SDK calls.\"\"\"",
            "",
            "    def __init__(self, url, user, pw, virtapi):",
            "        import XenAPI",
            "        self.XenAPI = XenAPI",
            "        self._sessions = queue.Queue()",
            "        self.is_slave = False",
            "        exception = self.XenAPI.Failure(_(\"Unable to log in to XenAPI \"",
            "                                          \"(is the Dom0 disk full?)\"))",
            "        url = self._create_first_session(url, user, pw, exception)",
            "        self._populate_session_pool(url, user, pw, exception)",
            "        self.host_uuid = self._get_host_uuid()",
            "        self.product_version, self.product_brand = \\",
            "            self._get_product_version_and_brand()",
            "        self._virtapi = virtapi",
            "",
            "    def _create_first_session(self, url, user, pw, exception):",
            "        try:",
            "            session = self._create_session(url)",
            "            with timeout.Timeout(CONF.xenapi_login_timeout, exception):",
            "                session.login_with_password(user, pw)",
            "        except self.XenAPI.Failure as e:",
            "            # if user and pw of the master are different, we're doomed!",
            "            if e.details[0] == 'HOST_IS_SLAVE':",
            "                master = e.details[1]",
            "                url = pool.swap_xapi_host(url, master)",
            "                session = self.XenAPI.Session(url)",
            "                session.login_with_password(user, pw)",
            "                self.is_slave = True",
            "            else:",
            "                raise",
            "        self._sessions.put(session)",
            "        return url",
            "",
            "    def _populate_session_pool(self, url, user, pw, exception):",
            "        for i in xrange(CONF.xenapi_connection_concurrent - 1):",
            "            session = self._create_session(url)",
            "            with timeout.Timeout(CONF.xenapi_login_timeout, exception):",
            "                session.login_with_password(user, pw)",
            "            self._sessions.put(session)",
            "",
            "    def _get_host_uuid(self):",
            "        if self.is_slave:",
            "            aggr = self._virtapi.aggregate_get_by_host(",
            "                context.get_admin_context(),",
            "                CONF.host, key=pool_states.POOL_FLAG)[0]",
            "            if not aggr:",
            "                LOG.error(_('Host is member of a pool, but DB '",
            "                                'says otherwise'))",
            "                raise exception.AggregateHostNotFound()",
            "            return aggr.metadetails[CONF.host]",
            "        else:",
            "            with self._get_session() as session:",
            "                host_ref = session.xenapi.session.get_this_host(session.handle)",
            "                return session.xenapi.host.get_uuid(host_ref)",
            "",
            "    def _get_product_version_and_brand(self):",
            "        \"\"\"Return a tuple of (major, minor, rev) for the host version and",
            "        a string of the product brand.",
            "        \"\"\"",
            "        software_version = self._get_software_version()",
            "",
            "        product_version_str = software_version.get('product_version')",
            "        # Product version is only set in some cases (e.g. XCP, XenServer) and",
            "        # not in others (e.g. xenserver-core, XAPI-XCP).",
            "        # In these cases, the platform version is the best number to use.",
            "        if product_version_str is None:",
            "            product_version_str = software_version.get('platform_version',",
            "                                                       '0.0.0')",
            "        product_brand = software_version.get('product_brand')",
            "",
            "        product_version = tuple(int(part) for part in",
            "                                product_version_str.split('.'))",
            "",
            "        return product_version, product_brand",
            "",
            "    def _get_software_version(self):",
            "        host = self.get_xenapi_host()",
            "        return self.call_xenapi('host.get_software_version', host)",
            "",
            "    def get_session_id(self):",
            "        \"\"\"Return a string session_id.  Used for vnc consoles.\"\"\"",
            "        with self._get_session() as session:",
            "            return str(session._session)",
            "",
            "    @contextlib.contextmanager",
            "    def _get_session(self):",
            "        \"\"\"Return exclusive session for scope of with statement.\"\"\"",
            "        session = self._sessions.get()",
            "        try:",
            "            yield session",
            "        finally:",
            "            self._sessions.put(session)",
            "",
            "    def get_xenapi_host(self):",
            "        \"\"\"Return the xenapi host on which nova-compute runs on.\"\"\"",
            "        with self._get_session() as session:",
            "            return session.xenapi.host.get_by_uuid(self.host_uuid)",
            "",
            "    def call_xenapi(self, method, *args):",
            "        \"\"\"Call the specified XenAPI method on a background thread.\"\"\"",
            "        with self._get_session() as session:",
            "            return session.xenapi_request(method, args)",
            "",
            "    def call_plugin(self, plugin, fn, args):",
            "        \"\"\"Call host.call_plugin on a background thread.\"\"\"",
            "        # NOTE(johannes): Fetch host before we acquire a session. Since",
            "        # get_xenapi_host() acquires a session too, it can result in a",
            "        # deadlock if multiple greenthreads race with each other. See",
            "        # bug 924918",
            "        host = self.get_xenapi_host()",
            "",
            "        # NOTE(armando): pass the host uuid along with the args so that",
            "        # the plugin gets executed on the right host when using XS pools",
            "        args['host_uuid'] = self.host_uuid",
            "",
            "        with self._get_session() as session:",
            "            return self._unwrap_plugin_exceptions(",
            "                                 session.xenapi.host.call_plugin,",
            "                                 host, plugin, fn, args)",
            "",
            "    def call_plugin_serialized(self, plugin, fn, *args, **kwargs):",
            "        params = {'params': pickle.dumps(dict(args=args, kwargs=kwargs))}",
            "        rv = self.call_plugin(plugin, fn, params)",
            "        return pickle.loads(rv)",
            "",
            "    def call_plugin_serialized_with_retry(self, plugin, fn, num_retries,",
            "                                          callback, *args, **kwargs):",
            "        \"\"\"Allows a plugin to raise RetryableError so we can try again.\"\"\"",
            "        attempts = num_retries + 1",
            "        sleep_time = 0.5",
            "        for attempt in xrange(1, attempts + 1):",
            "            LOG.info(_('%(plugin)s.%(fn)s attempt %(attempt)d/%(attempts)d'),",
            "                     {'plugin': plugin, 'fn': fn, 'attempt': attempt,",
            "                      'attempts': attempts})",
            "            try:",
            "                if callback:",
            "                    callback(kwargs)",
            "                return self.call_plugin_serialized(plugin, fn, *args, **kwargs)",
            "            except self.XenAPI.Failure as exc:",
            "                if self._is_retryable_exception(exc):",
            "                    LOG.warn(_('%(plugin)s.%(fn)s failed. Retrying call.')",
            "                             % {'plugin': plugin, 'fn': fn})",
            "                else:",
            "                    raise",
            "",
            "            time.sleep(sleep_time)",
            "            sleep_time = min(2 * sleep_time, 15)",
            "",
            "        raise exception.PluginRetriesExceeded(num_retries=num_retries)",
            "",
            "    def _is_retryable_exception(self, exc):",
            "        _type, method, error = exc.details[:3]",
            "        if error == 'RetryableError':",
            "            LOG.debug(_(\"RetryableError, so retrying upload_vhd\"),",
            "                      exc_info=True)",
            "            return True",
            "        elif \"signal\" in method:",
            "            LOG.debug(_(\"Error due to a signal, retrying upload_vhd\"),",
            "                      exc_info=True)",
            "            return True",
            "        else:",
            "            return False",
            "",
            "    def _create_session(self, url):",
            "        \"\"\"Stubout point. This can be replaced with a mock session.\"\"\"",
            "        self.is_local_connection = url == \"unix://local\"",
            "        if self.is_local_connection:",
            "            return self.XenAPI.xapi_local()",
            "        return self.XenAPI.Session(url)",
            "",
            "    def _unwrap_plugin_exceptions(self, func, *args, **kwargs):",
            "        \"\"\"Parse exception details.\"\"\"",
            "        try:",
            "            return func(*args, **kwargs)",
            "        except self.XenAPI.Failure as exc:",
            "            LOG.debug(_(\"Got exception: %s\"), exc)",
            "            if (len(exc.details) == 4 and",
            "                exc.details[0] == 'XENAPI_PLUGIN_EXCEPTION' and",
            "                    exc.details[2] == 'Failure'):",
            "                params = None",
            "                try:",
            "                    # FIXME(comstud): eval is evil.",
            "                    params = eval(exc.details[3])",
            "                except Exception:",
            "                    raise exc",
            "                raise self.XenAPI.Failure(params)",
            "            else:",
            "                raise",
            "        except xmlrpclib.ProtocolError as exc:",
            "            LOG.debug(_(\"Got exception: %s\"), exc)",
            "            raise",
            "",
            "    def get_rec(self, record_type, ref):",
            "        try:",
            "            return self.call_xenapi('%s.get_record' % record_type, ref)",
            "        except self.XenAPI.Failure as e:",
            "            if e.details[0] != 'HANDLE_INVALID':",
            "                raise",
            "",
            "        return None",
            "",
            "    def get_all_refs_and_recs(self, record_type):",
            "        \"\"\"Retrieve all refs and recs for a Xen record type.",
            "",
            "        Handles race-conditions where the record may be deleted between",
            "        the `get_all` call and the `get_record` call.",
            "        \"\"\"",
            "",
            "        for ref in self.call_xenapi('%s.get_all' % record_type):",
            "            rec = self.get_rec(record_type, ref)",
            "            # Check to make sure the record still exists. It may have",
            "            # been deleted between the get_all call and get_record call",
            "            if rec:",
            "                yield ref, rec"
        ],
        "afterPatchFile": [
            "",
            "# Copyright (c) 2010 Citrix Systems, Inc.",
            "# Copyright 2010 OpenStack Foundation",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "A driver for XenServer or Xen Cloud Platform.",
            "",
            "**Related Flags**",
            "",
            ":xenapi_connection_url:  URL for connection to XenServer/Xen Cloud Platform.",
            ":xenapi_connection_username:  Username for connection to XenServer/Xen Cloud",
            "                              Platform (default: root).",
            ":xenapi_connection_password:  Password for connection to XenServer/Xen Cloud",
            "                              Platform.",
            ":target_host:                the iSCSI Target Host IP address, i.e. the IP",
            "                             address for the nova-volume host",
            ":target_port:                iSCSI Target Port, 3260 Default",
            ":iqn_prefix:                 IQN Prefix, e.g. 'iqn.2010-10.org.openstack'",
            "",
            "**Variable Naming Scheme**",
            "",
            "- suffix \"_ref\" for opaque references",
            "- suffix \"_uuid\" for UUIDs",
            "- suffix \"_rec\" for record objects",
            "\"\"\"",
            "",
            "import contextlib",
            "import cPickle as pickle",
            "import math",
            "import time",
            "import urlparse",
            "import xmlrpclib",
            "",
            "from eventlet import queue",
            "from eventlet import timeout",
            "from oslo.config import cfg",
            "",
            "from nova import context",
            "from nova import exception",
            "from nova.openstack.common.gettextutils import _",
            "from nova.openstack.common import jsonutils",
            "from nova.openstack.common import log as logging",
            "from nova import utils",
            "from nova.virt import driver",
            "from nova.virt.xenapi import host",
            "from nova.virt.xenapi import pool",
            "from nova.virt.xenapi import pool_states",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import vmops",
            "from nova.virt.xenapi import volumeops",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "xenapi_opts = [",
            "    cfg.StrOpt('xenapi_connection_url',",
            "               help='URL for connection to XenServer/Xen Cloud Platform. '",
            "                    'A special value of unix://local can be used to connect '",
            "                    'to the local unix socket.  '",
            "                    'Required if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.StrOpt('xenapi_connection_username',",
            "               default='root',",
            "               help='Username for connection to XenServer/Xen Cloud Platform. '",
            "                    'Used only if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.StrOpt('xenapi_connection_password',",
            "               help='Password for connection to XenServer/Xen Cloud Platform. '",
            "                    'Used only if compute_driver=xenapi.XenAPIDriver',",
            "               secret=True),",
            "    cfg.IntOpt('xenapi_connection_concurrent',",
            "               default=5,",
            "               help='Maximum number of concurrent XenAPI connections. '",
            "                    'Used only if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.FloatOpt('xenapi_vhd_coalesce_poll_interval',",
            "                 default=5.0,",
            "                 help='The interval used for polling of coalescing vhds. '",
            "                      'Used only if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.BoolOpt('xenapi_check_host',",
            "                default=True,",
            "                help='Ensure compute service is running on host XenAPI '",
            "                     'connects to.'),",
            "    cfg.IntOpt('xenapi_vhd_coalesce_max_attempts',",
            "               default=5,",
            "               help='Max number of times to poll for VHD to coalesce. '",
            "                    'Used only if compute_driver=xenapi.XenAPIDriver'),",
            "    cfg.StrOpt('xenapi_sr_base_path',",
            "               default='/var/run/sr-mount',",
            "               help='Base path to the storage repository'),",
            "    cfg.StrOpt('target_host',",
            "               help='iSCSI Target Host'),",
            "    cfg.StrOpt('target_port',",
            "               default='3260',",
            "               help='iSCSI Target Port, 3260 Default'),",
            "    cfg.StrOpt('iqn_prefix',",
            "               default='iqn.2010-10.org.openstack',",
            "               help='IQN Prefix'),",
            "    # NOTE(sirp): This is a work-around for a bug in Ubuntu Maverick,",
            "    # when we pull support for it, we should remove this",
            "    cfg.BoolOpt('xenapi_remap_vbd_dev',",
            "                default=False,",
            "                help='Used to enable the remapping of VBD dev '",
            "                     '(Works around an issue in Ubuntu Maverick)'),",
            "    cfg.StrOpt('xenapi_remap_vbd_dev_prefix',",
            "               default='sd',",
            "               help='Specify prefix to remap VBD dev to '",
            "                    '(ex. /dev/xvdb -> /dev/sdb)'),",
            "    cfg.IntOpt('xenapi_login_timeout',",
            "               default=10,",
            "               help='Timeout in seconds for XenAPI login.'),",
            "    ]",
            "",
            "CONF = cfg.CONF",
            "CONF.register_opts(xenapi_opts)",
            "CONF.import_opt('host', 'nova.netconf')",
            "",
            "",
            "class XenAPIDriver(driver.ComputeDriver):",
            "    \"\"\"A connection to XenServer or Xen Cloud Platform.\"\"\"",
            "",
            "    def __init__(self, virtapi, read_only=False):",
            "        super(XenAPIDriver, self).__init__(virtapi)",
            "",
            "        url = CONF.xenapi_connection_url",
            "        username = CONF.xenapi_connection_username",
            "        password = CONF.xenapi_connection_password",
            "        if not url or password is None:",
            "            raise Exception(_('Must specify xenapi_connection_url, '",
            "                              'xenapi_connection_username (optionally), and '",
            "                              'xenapi_connection_password to use '",
            "                              'compute_driver=xenapi.XenAPIDriver'))",
            "",
            "        self._session = XenAPISession(url, username, password, self.virtapi)",
            "        self._volumeops = volumeops.VolumeOps(self._session)",
            "        self._host_state = None",
            "        self._host = host.Host(self._session, self.virtapi)",
            "        self._vmops = vmops.VMOps(self._session, self.virtapi)",
            "        self._initiator = None",
            "        self._hypervisor_hostname = None",
            "        self._pool = pool.ResourcePool(self._session, self.virtapi)",
            "",
            "    @property",
            "    def host_state(self):",
            "        if not self._host_state:",
            "            self._host_state = host.HostState(self._session)",
            "        return self._host_state",
            "",
            "    def init_host(self, host):",
            "        if CONF.xenapi_check_host:",
            "            vm_utils.ensure_correct_host(self._session)",
            "",
            "        try:",
            "            vm_utils.cleanup_attached_vdis(self._session)",
            "        except Exception:",
            "            LOG.exception(_('Failure while cleaning up attached VDIs'))",
            "",
            "    def instance_exists(self, instance_name):",
            "        \"\"\"Checks existence of an instance on the host.",
            "",
            "        :param instance_name: The name of the instance to lookup",
            "",
            "        Returns True if an instance with the supplied name exists on",
            "        the host, False otherwise.",
            "",
            "        NOTE(belliott): This is an override of the base method for",
            "        efficiency.",
            "        \"\"\"",
            "        return self._vmops.instance_exists(instance_name)",
            "",
            "    def estimate_instance_overhead(self, instance_info):",
            "        \"\"\"Get virtualization overhead required to build an instance of the",
            "        given flavor.",
            "",
            "        :param instance_info: Instance/flavor to calculate overhead for.",
            "        :returns: Overhead memory in MB.",
            "        \"\"\"",
            "",
            "        # XenServer memory overhead is proportional to the size of the",
            "        # VM.  Larger flavor VMs become more efficient with respect to",
            "        # overhead.",
            "",
            "        # interpolated formula to predict overhead required per vm.",
            "        # based on data from:",
            "        # https://wiki.openstack.org/wiki/XenServer/Overhead",
            "        base = 3  # MB",
            "        per_mb = 0.0081  # MB",
            "",
            "        memory_mb = instance_info['memory_mb']",
            "        overhead = memory_mb * per_mb + base",
            "        overhead = math.ceil(overhead)",
            "        return {'memory_mb': overhead}",
            "",
            "    def list_instances(self):",
            "        \"\"\"List VM instances.\"\"\"",
            "        return self._vmops.list_instances()",
            "",
            "    def list_instance_uuids(self):",
            "        \"\"\"Get the list of nova instance uuids for VMs found on the",
            "        hypervisor.",
            "        \"\"\"",
            "        return self._vmops.list_instance_uuids()",
            "",
            "    def spawn(self, context, instance, image_meta, injected_files,",
            "              admin_password, network_info=None, block_device_info=None):",
            "        \"\"\"Create VM instance.\"\"\"",
            "        self._vmops.spawn(context, instance, image_meta, injected_files,",
            "                          admin_password, network_info, block_device_info)",
            "",
            "    def confirm_migration(self, migration, instance, network_info):",
            "        \"\"\"Confirms a resize, destroying the source VM.\"\"\"",
            "        # TODO(Vek): Need to pass context in for access to auth_token",
            "        self._vmops.confirm_migration(migration, instance, network_info)",
            "",
            "    def finish_revert_migration(self, instance, network_info,",
            "                                block_device_info=None, power_on=True):",
            "        \"\"\"Finish reverting a resize.\"\"\"",
            "        # NOTE(vish): Xen currently does not use network info.",
            "        self._vmops.finish_revert_migration(instance, block_device_info,",
            "                                            power_on)",
            "",
            "    def finish_migration(self, context, migration, instance, disk_info,",
            "                         network_info, image_meta, resize_instance=False,",
            "                         block_device_info=None, power_on=True):",
            "        \"\"\"Completes a resize, turning on the migrated instance.\"\"\"",
            "        self._vmops.finish_migration(context, migration, instance, disk_info,",
            "                                     network_info, image_meta, resize_instance,",
            "                                     block_device_info, power_on)",
            "",
            "    def snapshot(self, context, instance, image_id, update_task_state):",
            "        \"\"\"Create snapshot from a running VM instance.\"\"\"",
            "        self._vmops.snapshot(context, instance, image_id, update_task_state)",
            "",
            "    def reboot(self, context, instance, network_info, reboot_type,",
            "               block_device_info=None, bad_volumes_callback=None):",
            "        \"\"\"Reboot VM instance.\"\"\"",
            "        self._vmops.reboot(instance, reboot_type,",
            "                           bad_volumes_callback=bad_volumes_callback)",
            "",
            "    def set_admin_password(self, instance, new_pass):",
            "        \"\"\"Set the root/admin password on the VM instance.\"\"\"",
            "        self._vmops.set_admin_password(instance, new_pass)",
            "",
            "    def inject_file(self, instance, b64_path, b64_contents):",
            "        \"\"\"Create a file on the VM instance. The file path and contents",
            "        should be base64-encoded.",
            "        \"\"\"",
            "        self._vmops.inject_file(instance, b64_path, b64_contents)",
            "",
            "    def change_instance_metadata(self, context, instance, diff):",
            "        \"\"\"Apply a diff to the instance metadata.\"\"\"",
            "        self._vmops.change_instance_metadata(instance, diff)",
            "",
            "    def destroy(self, instance, network_info, block_device_info=None,",
            "                destroy_disks=True, context=None):",
            "        \"\"\"Destroy VM instance.\"\"\"",
            "        self._vmops.destroy(instance, network_info, block_device_info,",
            "                            destroy_disks)",
            "",
            "    def pause(self, instance):",
            "        \"\"\"Pause VM instance.\"\"\"",
            "        self._vmops.pause(instance)",
            "",
            "    def unpause(self, instance):",
            "        \"\"\"Unpause paused VM instance.\"\"\"",
            "        self._vmops.unpause(instance)",
            "",
            "    def migrate_disk_and_power_off(self, context, instance, dest,",
            "                                   instance_type, network_info,",
            "                                   block_device_info=None):",
            "        \"\"\"Transfers the VHD of a running instance to another host, then shuts",
            "        off the instance copies over the COW disk",
            "        \"\"\"",
            "        # NOTE(vish): Xen currently does not use network info.",
            "        return self._vmops.migrate_disk_and_power_off(context, instance,",
            "                    dest, instance_type, block_device_info)",
            "",
            "    def suspend(self, instance):",
            "        \"\"\"suspend the specified instance.\"\"\"",
            "        self._vmops.suspend(instance)",
            "",
            "    def resume(self, instance, network_info, block_device_info=None):",
            "        \"\"\"resume the specified instance.\"\"\"",
            "        self._vmops.resume(instance)",
            "",
            "    def rescue(self, context, instance, network_info, image_meta,",
            "               rescue_password):",
            "        \"\"\"Rescue the specified instance.\"\"\"",
            "        self._vmops.rescue(context, instance, network_info, image_meta,",
            "                           rescue_password)",
            "",
            "    def unrescue(self, instance, network_info):",
            "        \"\"\"Unrescue the specified instance.\"\"\"",
            "        self._vmops.unrescue(instance)",
            "",
            "    def power_off(self, instance):",
            "        \"\"\"Power off the specified instance.\"\"\"",
            "        self._vmops.power_off(instance)",
            "",
            "    def power_on(self, context, instance, network_info,",
            "                 block_device_info=None):",
            "        \"\"\"Power on the specified instance.\"\"\"",
            "        self._vmops.power_on(instance)",
            "",
            "    def soft_delete(self, instance):",
            "        \"\"\"Soft delete the specified instance.\"\"\"",
            "        self._vmops.soft_delete(instance)",
            "",
            "    def restore(self, instance):",
            "        \"\"\"Restore the specified instance.\"\"\"",
            "        self._vmops.restore(instance)",
            "",
            "    def poll_rebooting_instances(self, timeout, instances):",
            "        \"\"\"Poll for rebooting instances.\"\"\"",
            "        self._vmops.poll_rebooting_instances(timeout, instances)",
            "",
            "    def reset_network(self, instance):",
            "        \"\"\"reset networking for specified instance.\"\"\"",
            "        self._vmops.reset_network(instance)",
            "",
            "    def inject_network_info(self, instance, network_info):",
            "        \"\"\"inject network info for specified instance.\"\"\"",
            "        self._vmops.inject_network_info(instance, network_info)",
            "",
            "    def plug_vifs(self, instance_ref, network_info):",
            "        \"\"\"Plug VIFs into networks.\"\"\"",
            "        self._vmops.plug_vifs(instance_ref, network_info)",
            "",
            "    def unplug_vifs(self, instance_ref, network_info):",
            "        \"\"\"Unplug VIFs from networks.\"\"\"",
            "        self._vmops.unplug_vifs(instance_ref, network_info)",
            "",
            "    def get_info(self, instance):",
            "        \"\"\"Return data about VM instance.\"\"\"",
            "        return self._vmops.get_info(instance)",
            "",
            "    def get_diagnostics(self, instance):",
            "        \"\"\"Return data about VM diagnostics.\"\"\"",
            "        return self._vmops.get_diagnostics(instance)",
            "",
            "    def get_all_bw_counters(self, instances):",
            "        \"\"\"Return bandwidth usage counters for each interface on each",
            "           running VM.",
            "        \"\"\"",
            "",
            "        # we only care about VMs that correspond to a nova-managed",
            "        # instance:",
            "        imap = dict([(inst['name'], inst['uuid']) for inst in instances])",
            "        bwcounters = []",
            "",
            "        # get a dictionary of instance names.  values are dictionaries",
            "        # of mac addresses with values that are the bw counters:",
            "        # e.g. {'instance-001' : { 12:34:56:78:90:12 : {'bw_in': 0, ....}}",
            "        all_counters = self._vmops.get_all_bw_counters()",
            "        for instance_name, counters in all_counters.iteritems():",
            "            if instance_name in imap:",
            "                # yes these are stats for a nova-managed vm",
            "                # correlate the stats with the nova instance uuid:",
            "                for vif_counter in counters.values():",
            "                    vif_counter['uuid'] = imap[instance_name]",
            "                    bwcounters.append(vif_counter)",
            "        return bwcounters",
            "",
            "    def get_console_output(self, instance):",
            "        \"\"\"Return snapshot of console.\"\"\"",
            "        return self._vmops.get_console_output(instance)",
            "",
            "    def get_vnc_console(self, instance):",
            "        \"\"\"Return link to instance's VNC console.\"\"\"",
            "        return self._vmops.get_vnc_console(instance)",
            "",
            "    def get_volume_connector(self, instance):",
            "        \"\"\"Return volume connector information.\"\"\"",
            "        if not self._initiator or not self._hypervisor_hostname:",
            "            stats = self.get_host_stats(refresh=True)",
            "            try:",
            "                self._initiator = stats['host_other-config']['iscsi_iqn']",
            "                self._hypervisor_hostname = stats['host_hostname']",
            "            except (TypeError, KeyError) as err:",
            "                LOG.warn(_('Could not determine key: %s') % err,",
            "                         instance=instance)",
            "                self._initiator = None",
            "        return {",
            "            'ip': self.get_host_ip_addr(),",
            "            'initiator': self._initiator,",
            "            'host': self._hypervisor_hostname",
            "        }",
            "",
            "    @staticmethod",
            "    def get_host_ip_addr():",
            "        xs_url = urlparse.urlparse(CONF.xenapi_connection_url)",
            "        return xs_url.netloc",
            "",
            "    def attach_volume(self, context, connection_info, instance, mountpoint,",
            "                      encryption=None):",
            "        \"\"\"Attach volume storage to VM instance.\"\"\"",
            "        return self._volumeops.attach_volume(connection_info,",
            "                                             instance['name'],",
            "                                             mountpoint)",
            "",
            "    def detach_volume(self, connection_info, instance, mountpoint,",
            "                      encryption=None):",
            "        \"\"\"Detach volume storage from VM instance.\"\"\"",
            "        return self._volumeops.detach_volume(connection_info,",
            "                                             instance['name'],",
            "                                             mountpoint)",
            "",
            "    def get_console_pool_info(self, console_type):",
            "        xs_url = urlparse.urlparse(CONF.xenapi_connection_url)",
            "        return {'address': xs_url.netloc,",
            "                'username': CONF.xenapi_connection_username,",
            "                'password': CONF.xenapi_connection_password}",
            "",
            "    def get_available_resource(self, nodename):",
            "        \"\"\"Retrieve resource information.",
            "",
            "        This method is called when nova-compute launches, and",
            "        as part of a periodic task that records the results in the DB.",
            "",
            "        :param nodename: ignored in this driver",
            "        :returns: dictionary describing resources",
            "",
            "        \"\"\"",
            "        host_stats = self.get_host_stats(refresh=True)",
            "",
            "        # Updating host information",
            "        total_ram_mb = host_stats['host_memory_total'] / (1024 * 1024)",
            "        # NOTE(belliott) memory-free-computed is a value provided by XenServer",
            "        # for gauging free memory more conservatively than memory-free.",
            "        free_ram_mb = host_stats['host_memory_free_computed'] / (1024 * 1024)",
            "        total_disk_gb = host_stats['disk_total'] / (1024 * 1024 * 1024)",
            "        used_disk_gb = host_stats['disk_used'] / (1024 * 1024 * 1024)",
            "        hyper_ver = utils.convert_version_to_int(self._session.product_version)",
            "        dic = {'vcpus': 0,",
            "               'memory_mb': total_ram_mb,",
            "               'local_gb': total_disk_gb,",
            "               'vcpus_used': 0,",
            "               'memory_mb_used': total_ram_mb - free_ram_mb,",
            "               'local_gb_used': used_disk_gb,",
            "               'hypervisor_type': 'xen',",
            "               'hypervisor_version': hyper_ver,",
            "               'hypervisor_hostname': host_stats['host_hostname'],",
            "               'cpu_info': host_stats['host_cpu_info']['cpu_count'],",
            "               'supported_instances': jsonutils.dumps(",
            "                   host_stats['supported_instances'])}",
            "",
            "        return dic",
            "",
            "    def ensure_filtering_rules_for_instance(self, instance_ref, network_info):",
            "        # NOTE(salvatore-orlando): it enforces security groups on",
            "        # host initialization and live migration.",
            "        # In XenAPI we do not assume instances running upon host initialization",
            "        return",
            "",
            "    def check_can_live_migrate_destination(self, ctxt, instance_ref,",
            "                src_compute_info, dst_compute_info,",
            "                block_migration=False, disk_over_commit=False):",
            "        \"\"\"Check if it is possible to execute live migration.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param block_migration: if true, prepare for block migration",
            "        :param disk_over_commit: if true, allow disk over commit",
            "",
            "        \"\"\"",
            "        return self._vmops.check_can_live_migrate_destination(ctxt,",
            "                                                              instance_ref,",
            "                                                              block_migration,",
            "                                                              disk_over_commit)",
            "",
            "    def check_can_live_migrate_destination_cleanup(self, ctxt,",
            "                                                   dest_check_data):",
            "        \"\"\"Do required cleanup on dest host after check_can_live_migrate calls",
            "",
            "        :param ctxt: security context",
            "        :param disk_over_commit: if true, allow disk over commit",
            "        \"\"\"",
            "        pass",
            "",
            "    def check_can_live_migrate_source(self, ctxt, instance_ref,",
            "                                      dest_check_data):",
            "        \"\"\"Check if it is possible to execute live migration.",
            "",
            "        This checks if the live migration can succeed, based on the",
            "        results from check_can_live_migrate_destination.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance",
            "        :param dest_check_data: result of check_can_live_migrate_destination",
            "                                includes the block_migration flag",
            "        \"\"\"",
            "        return self._vmops.check_can_live_migrate_source(ctxt, instance_ref,",
            "                                                         dest_check_data)",
            "",
            "    def get_instance_disk_info(self, instance_name):",
            "        \"\"\"Used by libvirt for live migration. We rely on xenapi",
            "        checks to do this for us.",
            "        \"\"\"",
            "        pass",
            "",
            "    def live_migration(self, ctxt, instance_ref, dest,",
            "                       post_method, recover_method, block_migration=False,",
            "                       migrate_data=None):",
            "        \"\"\"Performs the live migration of the specified instance.",
            "",
            "        :params ctxt: security context",
            "        :params instance_ref:",
            "            nova.db.sqlalchemy.models.Instance object",
            "            instance object that is migrated.",
            "        :params dest: destination host",
            "        :params post_method:",
            "            post operation method.",
            "            expected nova.compute.manager.post_live_migration.",
            "        :params recover_method:",
            "            recovery method when any exception occurs.",
            "            expected nova.compute.manager.recover_live_migration.",
            "        :params block_migration: if true, migrate VM disk.",
            "        :params migrate_data: implementation specific params",
            "        \"\"\"",
            "        self._vmops.live_migrate(ctxt, instance_ref, dest, post_method,",
            "                                 recover_method, block_migration, migrate_data)",
            "",
            "    def pre_live_migration(self, context, instance_ref, block_device_info,",
            "                           network_info, data, migrate_data=None):",
            "        \"\"\"Preparation live migration.",
            "",
            "        :params block_device_info:",
            "            It must be the result of _get_instance_volume_bdms()",
            "            at compute manager.",
            "        \"\"\"",
            "        # TODO(JohnGarbutt) look again when boot-from-volume hits trunk",
            "        pre_live_migration_result = {}",
            "        pre_live_migration_result['sr_uuid_map'] = \\",
            "                 self._vmops.attach_block_device_volumes(block_device_info)",
            "        return pre_live_migration_result",
            "",
            "    def post_live_migration_at_destination(self, ctxt, instance_ref,",
            "                                           network_info, block_migration,",
            "                                           block_device_info=None):",
            "        \"\"\"Post operation of live migration at destination host.",
            "",
            "        :params ctxt: security context",
            "        :params instance_ref:",
            "            nova.db.sqlalchemy.models.Instance object",
            "            instance object that is migrated.",
            "        :params network_info: instance network information",
            "        :params : block_migration: if true, post operation of block_migraiton.",
            "        \"\"\"",
            "        self._vmops.post_live_migration_at_destination(ctxt, instance_ref,",
            "                network_info, block_device_info, block_device_info)",
            "",
            "    def unfilter_instance(self, instance_ref, network_info):",
            "        \"\"\"Removes security groups configured for an instance.\"\"\"",
            "        return self._vmops.unfilter_instance(instance_ref, network_info)",
            "",
            "    def refresh_security_group_rules(self, security_group_id):",
            "        \"\"\"Updates security group rules for all instances associated with a",
            "        given security group.",
            "",
            "        Invoked when security group rules are updated.",
            "        \"\"\"",
            "        return self._vmops.refresh_security_group_rules(security_group_id)",
            "",
            "    def refresh_security_group_members(self, security_group_id):",
            "        \"\"\"Updates security group rules for all instances associated with a",
            "        given security group.",
            "",
            "        Invoked when instances are added/removed to a security group.",
            "        \"\"\"",
            "        return self._vmops.refresh_security_group_members(security_group_id)",
            "",
            "    def refresh_instance_security_rules(self, instance):",
            "        \"\"\"Updates security group rules for specified instance.",
            "",
            "        Invoked when instances are added/removed to a security group",
            "        or when a rule is added/removed to a security group.",
            "        \"\"\"",
            "        return self._vmops.refresh_instance_security_rules(instance)",
            "",
            "    def refresh_provider_fw_rules(self):",
            "        return self._vmops.refresh_provider_fw_rules()",
            "",
            "    def get_host_stats(self, refresh=False):",
            "        \"\"\"Return the current state of the host.",
            "",
            "           If 'refresh' is True, run the update first.",
            "         \"\"\"",
            "        return self.host_state.get_host_stats(refresh=refresh)",
            "",
            "    def host_power_action(self, host, action):",
            "        \"\"\"The only valid values for 'action' on XenServer are 'reboot' or",
            "        'shutdown', even though the API also accepts 'startup'. As this is",
            "        not technically possible on XenServer, since the host is the same",
            "        physical machine as the hypervisor, if this is requested, we need to",
            "        raise an exception.",
            "        \"\"\"",
            "        if action in (\"reboot\", \"shutdown\"):",
            "            return self._host.host_power_action(host, action)",
            "        else:",
            "            msg = _(\"Host startup on XenServer is not supported.\")",
            "            raise NotImplementedError(msg)",
            "",
            "    def set_host_enabled(self, host, enabled):",
            "        \"\"\"Sets the specified host's ability to accept new instances.\"\"\"",
            "        return self._host.set_host_enabled(host, enabled)",
            "",
            "    def get_host_uptime(self, host):",
            "        \"\"\"Returns the result of calling \"uptime\" on the target host.\"\"\"",
            "        return self._host.get_host_uptime(host)",
            "",
            "    def host_maintenance_mode(self, host, mode):",
            "        \"\"\"Start/Stop host maintenance window. On start, it triggers",
            "        guest VMs evacuation.",
            "        \"\"\"",
            "        return self._host.host_maintenance_mode(host, mode)",
            "",
            "    def add_to_aggregate(self, context, aggregate, host, **kwargs):",
            "        \"\"\"Add a compute host to an aggregate.\"\"\"",
            "        return self._pool.add_to_aggregate(context, aggregate, host, **kwargs)",
            "",
            "    def remove_from_aggregate(self, context, aggregate, host, **kwargs):",
            "        \"\"\"Remove a compute host from an aggregate.\"\"\"",
            "        return self._pool.remove_from_aggregate(context,",
            "                                                aggregate, host, **kwargs)",
            "",
            "    def undo_aggregate_operation(self, context, op, aggregate,",
            "                                  host, set_error=True):",
            "        \"\"\"Undo aggregate operation when pool error raised.\"\"\"",
            "        return self._pool.undo_aggregate_operation(context, op,",
            "                aggregate, host, set_error)",
            "",
            "    def resume_state_on_host_boot(self, context, instance, network_info,",
            "                                  block_device_info=None):",
            "        \"\"\"resume guest state when a host is booted.\"\"\"",
            "        self._vmops.power_on(instance)",
            "",
            "    def get_per_instance_usage(self):",
            "        \"\"\"Get information about instance resource usage.",
            "",
            "        :returns: dict of  nova uuid => dict of usage",
            "        info",
            "        \"\"\"",
            "        return self._vmops.get_per_instance_usage()",
            "",
            "",
            "class XenAPISession(object):",
            "    \"\"\"The session to invoke XenAPI SDK calls.\"\"\"",
            "",
            "    def __init__(self, url, user, pw, virtapi):",
            "        import XenAPI",
            "        self.XenAPI = XenAPI",
            "        self._sessions = queue.Queue()",
            "        self.is_slave = False",
            "        exception = self.XenAPI.Failure(_(\"Unable to log in to XenAPI \"",
            "                                          \"(is the Dom0 disk full?)\"))",
            "        url = self._create_first_session(url, user, pw, exception)",
            "        self._populate_session_pool(url, user, pw, exception)",
            "        self.host_uuid = self._get_host_uuid()",
            "        self.product_version, self.product_brand = \\",
            "            self._get_product_version_and_brand()",
            "        self._virtapi = virtapi",
            "",
            "    def _create_first_session(self, url, user, pw, exception):",
            "        try:",
            "            session = self._create_session(url)",
            "            with timeout.Timeout(CONF.xenapi_login_timeout, exception):",
            "                session.login_with_password(user, pw)",
            "        except self.XenAPI.Failure as e:",
            "            # if user and pw of the master are different, we're doomed!",
            "            if e.details[0] == 'HOST_IS_SLAVE':",
            "                master = e.details[1]",
            "                url = pool.swap_xapi_host(url, master)",
            "                session = self.XenAPI.Session(url)",
            "                session.login_with_password(user, pw)",
            "                self.is_slave = True",
            "            else:",
            "                raise",
            "        self._sessions.put(session)",
            "        return url",
            "",
            "    def _populate_session_pool(self, url, user, pw, exception):",
            "        for i in xrange(CONF.xenapi_connection_concurrent - 1):",
            "            session = self._create_session(url)",
            "            with timeout.Timeout(CONF.xenapi_login_timeout, exception):",
            "                session.login_with_password(user, pw)",
            "            self._sessions.put(session)",
            "",
            "    def _get_host_uuid(self):",
            "        if self.is_slave:",
            "            aggr = self._virtapi.aggregate_get_by_host(",
            "                context.get_admin_context(),",
            "                CONF.host, key=pool_states.POOL_FLAG)[0]",
            "            if not aggr:",
            "                LOG.error(_('Host is member of a pool, but DB '",
            "                                'says otherwise'))",
            "                raise exception.AggregateHostNotFound()",
            "            return aggr.metadetails[CONF.host]",
            "        else:",
            "            with self._get_session() as session:",
            "                host_ref = session.xenapi.session.get_this_host(session.handle)",
            "                return session.xenapi.host.get_uuid(host_ref)",
            "",
            "    def _get_product_version_and_brand(self):",
            "        \"\"\"Return a tuple of (major, minor, rev) for the host version and",
            "        a string of the product brand.",
            "        \"\"\"",
            "        software_version = self._get_software_version()",
            "",
            "        product_version_str = software_version.get('product_version')",
            "        # Product version is only set in some cases (e.g. XCP, XenServer) and",
            "        # not in others (e.g. xenserver-core, XAPI-XCP).",
            "        # In these cases, the platform version is the best number to use.",
            "        if product_version_str is None:",
            "            product_version_str = software_version.get('platform_version',",
            "                                                       '0.0.0')",
            "        product_brand = software_version.get('product_brand')",
            "",
            "        product_version = tuple(int(part) for part in",
            "                                product_version_str.split('.'))",
            "",
            "        return product_version, product_brand",
            "",
            "    def _get_software_version(self):",
            "        host = self.get_xenapi_host()",
            "        return self.call_xenapi('host.get_software_version', host)",
            "",
            "    def get_session_id(self):",
            "        \"\"\"Return a string session_id.  Used for vnc consoles.\"\"\"",
            "        with self._get_session() as session:",
            "            return str(session._session)",
            "",
            "    @contextlib.contextmanager",
            "    def _get_session(self):",
            "        \"\"\"Return exclusive session for scope of with statement.\"\"\"",
            "        session = self._sessions.get()",
            "        try:",
            "            yield session",
            "        finally:",
            "            self._sessions.put(session)",
            "",
            "    def get_xenapi_host(self):",
            "        \"\"\"Return the xenapi host on which nova-compute runs on.\"\"\"",
            "        with self._get_session() as session:",
            "            return session.xenapi.host.get_by_uuid(self.host_uuid)",
            "",
            "    def call_xenapi(self, method, *args):",
            "        \"\"\"Call the specified XenAPI method on a background thread.\"\"\"",
            "        with self._get_session() as session:",
            "            return session.xenapi_request(method, args)",
            "",
            "    def call_plugin(self, plugin, fn, args):",
            "        \"\"\"Call host.call_plugin on a background thread.\"\"\"",
            "        # NOTE(johannes): Fetch host before we acquire a session. Since",
            "        # get_xenapi_host() acquires a session too, it can result in a",
            "        # deadlock if multiple greenthreads race with each other. See",
            "        # bug 924918",
            "        host = self.get_xenapi_host()",
            "",
            "        # NOTE(armando): pass the host uuid along with the args so that",
            "        # the plugin gets executed on the right host when using XS pools",
            "        args['host_uuid'] = self.host_uuid",
            "",
            "        with self._get_session() as session:",
            "            return self._unwrap_plugin_exceptions(",
            "                                 session.xenapi.host.call_plugin,",
            "                                 host, plugin, fn, args)",
            "",
            "    def call_plugin_serialized(self, plugin, fn, *args, **kwargs):",
            "        params = {'params': pickle.dumps(dict(args=args, kwargs=kwargs))}",
            "        rv = self.call_plugin(plugin, fn, params)",
            "        return pickle.loads(rv)",
            "",
            "    def call_plugin_serialized_with_retry(self, plugin, fn, num_retries,",
            "                                          callback, *args, **kwargs):",
            "        \"\"\"Allows a plugin to raise RetryableError so we can try again.\"\"\"",
            "        attempts = num_retries + 1",
            "        sleep_time = 0.5",
            "        for attempt in xrange(1, attempts + 1):",
            "            LOG.info(_('%(plugin)s.%(fn)s attempt %(attempt)d/%(attempts)d'),",
            "                     {'plugin': plugin, 'fn': fn, 'attempt': attempt,",
            "                      'attempts': attempts})",
            "            try:",
            "                if callback:",
            "                    callback(kwargs)",
            "                return self.call_plugin_serialized(plugin, fn, *args, **kwargs)",
            "            except self.XenAPI.Failure as exc:",
            "                if self._is_retryable_exception(exc):",
            "                    LOG.warn(_('%(plugin)s.%(fn)s failed. Retrying call.')",
            "                             % {'plugin': plugin, 'fn': fn})",
            "                else:",
            "                    raise",
            "",
            "            time.sleep(sleep_time)",
            "            sleep_time = min(2 * sleep_time, 15)",
            "",
            "        raise exception.PluginRetriesExceeded(num_retries=num_retries)",
            "",
            "    def _is_retryable_exception(self, exc):",
            "        _type, method, error = exc.details[:3]",
            "        if error == 'RetryableError':",
            "            LOG.debug(_(\"RetryableError, so retrying upload_vhd\"),",
            "                      exc_info=True)",
            "            return True",
            "        elif \"signal\" in method:",
            "            LOG.debug(_(\"Error due to a signal, retrying upload_vhd\"),",
            "                      exc_info=True)",
            "            return True",
            "        else:",
            "            return False",
            "",
            "    def _create_session(self, url):",
            "        \"\"\"Stubout point. This can be replaced with a mock session.\"\"\"",
            "        self.is_local_connection = url == \"unix://local\"",
            "        if self.is_local_connection:",
            "            return self.XenAPI.xapi_local()",
            "        return self.XenAPI.Session(url)",
            "",
            "    def _unwrap_plugin_exceptions(self, func, *args, **kwargs):",
            "        \"\"\"Parse exception details.\"\"\"",
            "        try:",
            "            return func(*args, **kwargs)",
            "        except self.XenAPI.Failure as exc:",
            "            LOG.debug(_(\"Got exception: %s\"), exc)",
            "            if (len(exc.details) == 4 and",
            "                exc.details[0] == 'XENAPI_PLUGIN_EXCEPTION' and",
            "                    exc.details[2] == 'Failure'):",
            "                params = None",
            "                try:",
            "                    # FIXME(comstud): eval is evil.",
            "                    params = eval(exc.details[3])",
            "                except Exception:",
            "                    raise exc",
            "                raise self.XenAPI.Failure(params)",
            "            else:",
            "                raise",
            "        except xmlrpclib.ProtocolError as exc:",
            "            LOG.debug(_(\"Got exception: %s\"), exc)",
            "            raise",
            "",
            "    def get_rec(self, record_type, ref):",
            "        try:",
            "            return self.call_xenapi('%s.get_record' % record_type, ref)",
            "        except self.XenAPI.Failure as e:",
            "            if e.details[0] != 'HANDLE_INVALID':",
            "                raise",
            "",
            "        return None",
            "",
            "    def get_all_refs_and_recs(self, record_type):",
            "        \"\"\"Retrieve all refs and recs for a Xen record type.",
            "",
            "        Handles race-conditions where the record may be deleted between",
            "        the `get_all` call and the `get_record` call.",
            "        \"\"\"",
            "",
            "        for ref in self.call_xenapi('%s.get_all' % record_type):",
            "            rec = self.get_rec(record_type, ref)",
            "            # Check to make sure the record still exists. It may have",
            "            # been deleted between the get_all call and get_record call",
            "            if rec:",
            "                yield ref, rec"
        ],
        "action": [
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1": [],
            "558": [
                "XenAPIDriver",
                "post_live_migration_at_destination"
            ],
            "559": [
                "XenAPIDriver",
                "post_live_migration_at_destination"
            ]
        },
        "addLocation": []
    },
    "nova/virt/xenapi/vmops.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 451,
                "afterPatchRowNumber": 451,
                "PatchRowcode": "         @step"
            },
            "1": {
                "beforePatchRowNumber": 452,
                "afterPatchRowNumber": 452,
                "PatchRowcode": "         def setup_network_step(undo_mgr, vm_ref):"
            },
            "2": {
                "beforePatchRowNumber": 453,
                "afterPatchRowNumber": 453,
                "PatchRowcode": "             self._create_vifs(instance, vm_ref, network_info)"
            },
            "3": {
                "beforePatchRowNumber": 454,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "4": {
                "beforePatchRowNumber": 455,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            try:"
            },
            "5": {
                "beforePatchRowNumber": 456,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.firewall_driver.setup_basic_filtering("
            },
            "6": {
                "beforePatchRowNumber": 457,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        instance, network_info)"
            },
            "7": {
                "beforePatchRowNumber": 458,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            except NotImplementedError:"
            },
            "8": {
                "beforePatchRowNumber": 459,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # NOTE(salvatore-orlando): setup_basic_filtering might be"
            },
            "9": {
                "beforePatchRowNumber": 460,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # empty or not implemented at all, as basic filter could"
            },
            "10": {
                "beforePatchRowNumber": 461,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # be implemented with VIF rules created by xapi plugin"
            },
            "11": {
                "beforePatchRowNumber": 462,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                pass"
            },
            "12": {
                "beforePatchRowNumber": 463,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "13": {
                "beforePatchRowNumber": 464,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.firewall_driver.prepare_instance_filter(instance,"
            },
            "14": {
                "beforePatchRowNumber": 465,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                                         network_info)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 454,
                "PatchRowcode": "+            self._prepare_instance_filter(instance, network_info)"
            },
            "16": {
                "beforePatchRowNumber": 466,
                "afterPatchRowNumber": 455,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 467,
                "afterPatchRowNumber": 456,
                "PatchRowcode": "         @step"
            },
            "18": {
                "beforePatchRowNumber": 468,
                "afterPatchRowNumber": 457,
                "PatchRowcode": "         def boot_instance_step(undo_mgr, vm_ref):"
            },
            "19": {
                "beforePatchRowNumber": 690,
                "afterPatchRowNumber": 679,
                "PatchRowcode": "             # Reset network config"
            },
            "20": {
                "beforePatchRowNumber": 691,
                "afterPatchRowNumber": 680,
                "PatchRowcode": "             agent.resetnetwork()"
            },
            "21": {
                "beforePatchRowNumber": 692,
                "afterPatchRowNumber": 681,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 682,
                "PatchRowcode": "+    def _prepare_instance_filter(self, instance, network_info):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 683,
                "PatchRowcode": "+        try:"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 684,
                "PatchRowcode": "+            self.firewall_driver.setup_basic_filtering("
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 685,
                "PatchRowcode": "+                    instance, network_info)"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 686,
                "PatchRowcode": "+        except NotImplementedError:"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 687,
                "PatchRowcode": "+            # NOTE(salvatore-orlando): setup_basic_filtering might be"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 688,
                "PatchRowcode": "+            # empty or not implemented at all, as basic filter could"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 689,
                "PatchRowcode": "+            # be implemented with VIF rules created by xapi plugin"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 690,
                "PatchRowcode": "+            pass"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 691,
                "PatchRowcode": "+"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 692,
                "PatchRowcode": "+        self.firewall_driver.prepare_instance_filter(instance,"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 693,
                "PatchRowcode": "+                                                     network_info)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 694,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": 693,
                "afterPatchRowNumber": 695,
                "PatchRowcode": "     def _get_vm_opaque_ref(self, instance, check_rescue=False):"
            },
            "36": {
                "beforePatchRowNumber": 694,
                "afterPatchRowNumber": 696,
                "PatchRowcode": "         \"\"\"Get xapi OpaqueRef from a db record."
            },
            "37": {
                "beforePatchRowNumber": 695,
                "afterPatchRowNumber": 697,
                "PatchRowcode": "         :param check_rescue: if True will return the 'name'-rescue vm if it"
            },
            "38": {
                "beforePatchRowNumber": 1904,
                "afterPatchRowNumber": 1906,
                "PatchRowcode": "                 recover_method(context, instance, destination_hostname,"
            },
            "39": {
                "beforePatchRowNumber": 1905,
                "afterPatchRowNumber": 1907,
                "PatchRowcode": "                                block_migration)"
            },
            "40": {
                "beforePatchRowNumber": 1906,
                "afterPatchRowNumber": 1908,
                "PatchRowcode": " "
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1909,
                "PatchRowcode": "+    def post_live_migration_at_destination(self, context, instance,"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1910,
                "PatchRowcode": "+                                           network_info, block_migration,"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1911,
                "PatchRowcode": "+                                           block_device_info):"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1912,
                "PatchRowcode": "+        # FIXME(johngarbutt): we should block all traffic until we have"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1913,
                "PatchRowcode": "+        # applied security groups, however this requires changes to XenServer"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1914,
                "PatchRowcode": "+        self._prepare_instance_filter(instance, network_info)"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1915,
                "PatchRowcode": "+        self.firewall_driver.apply_instance_filter(instance, network_info)"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1916,
                "PatchRowcode": "+"
            },
            "49": {
                "beforePatchRowNumber": 1907,
                "afterPatchRowNumber": 1917,
                "PatchRowcode": "     def get_per_instance_usage(self):"
            },
            "50": {
                "beforePatchRowNumber": 1908,
                "afterPatchRowNumber": 1918,
                "PatchRowcode": "         \"\"\"Get usage info about each active instance.\"\"\""
            },
            "51": {
                "beforePatchRowNumber": 1909,
                "afterPatchRowNumber": 1919,
                "PatchRowcode": "         usage = {}"
            }
        },
        "frontPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright (c) 2010 Citrix Systems, Inc.",
            "# Copyright 2010 OpenStack Foundation",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "Management class for VM-related functions (spawn, reboot, etc).",
            "\"\"\"",
            "",
            "import base64",
            "import functools",
            "import itertools",
            "import time",
            "import zlib",
            "",
            "from eventlet import greenthread",
            "import netaddr",
            "from oslo.config import cfg",
            "",
            "from nova import block_device",
            "from nova import compute",
            "from nova.compute import flavors",
            "from nova.compute import power_state",
            "from nova.compute import task_states",
            "from nova.compute import vm_mode",
            "from nova.compute import vm_states",
            "from nova import context as nova_context",
            "from nova import exception",
            "from nova.openstack.common import excutils",
            "from nova.openstack.common.gettextutils import _",
            "from nova.openstack.common import importutils",
            "from nova.openstack.common import jsonutils",
            "from nova.openstack.common import log as logging",
            "from nova.openstack.common import strutils",
            "from nova.openstack.common import timeutils",
            "from nova import utils",
            "from nova.virt import configdrive",
            "from nova.virt import driver as virt_driver",
            "from nova.virt import firewall",
            "from nova.virt.xenapi import agent as xapi_agent",
            "from nova.virt.xenapi import pool_states",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import volume_utils",
            "from nova.virt.xenapi import volumeops",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "xenapi_vmops_opts = [",
            "    cfg.IntOpt('xenapi_running_timeout',",
            "               default=60,",
            "               help='number of seconds to wait for instance '",
            "                    'to go to running state'),",
            "    cfg.StrOpt('xenapi_vif_driver',",
            "               default='nova.virt.xenapi.vif.XenAPIBridgeDriver',",
            "               help='The XenAPI VIF driver using XenServer Network APIs.'),",
            "    cfg.StrOpt('xenapi_image_upload_handler',",
            "                default='nova.virt.xenapi.image.glance.GlanceStore',",
            "                help='Dom0 plugin driver used to handle image uploads.'),",
            "    ]",
            "",
            "CONF = cfg.CONF",
            "CONF.register_opts(xenapi_vmops_opts)",
            "CONF.import_opt('host', 'nova.netconf')",
            "CONF.import_opt('vncserver_proxyclient_address', 'nova.vnc')",
            "",
            "DEFAULT_FIREWALL_DRIVER = \"%s.%s\" % (",
            "    firewall.__name__,",
            "    firewall.IptablesFirewallDriver.__name__)",
            "",
            "RESIZE_TOTAL_STEPS = 5",
            "",
            "DEVICE_ROOT = '0'",
            "DEVICE_RESCUE = '1'",
            "DEVICE_SWAP = '2'",
            "DEVICE_CONFIGDRIVE = '3'",
            "# Note(johngarbutt) HVM guests only support four devices",
            "# until the PV tools activate, when others before available",
            "# As such, ephemeral disk only available once PV tools load",
            "# Note(johngarbutt) When very large ephemeral storage is required,",
            "# multiple disks may be added. In this case the device id below",
            "# is the used for the first disk. The second disk will be given",
            "# next device id, i.e. 5, and so on, until enough space is added.",
            "DEVICE_EPHEMERAL = '4'",
            "# Note(johngarbutt) Currently don't support ISO boot during rescue",
            "# and we must have the ISO visible before the PV drivers start",
            "DEVICE_CD = '1'",
            "",
            "",
            "def cmp_version(a, b):",
            "    \"\"\"Compare two version strings (eg 0.0.1.10 > 0.0.1.9).\"\"\"",
            "    a = a.split('.')",
            "    b = b.split('.')",
            "",
            "    # Compare each individual portion of both version strings",
            "    for va, vb in zip(a, b):",
            "        ret = int(va) - int(vb)",
            "        if ret:",
            "            return ret",
            "",
            "    # Fallback to comparing length last",
            "    return len(a) - len(b)",
            "",
            "",
            "def make_step_decorator(context, instance, update_instance_progress):",
            "    \"\"\"Factory to create a decorator that records instance progress as a series",
            "    of discrete steps.",
            "",
            "    Each time the decorator is invoked we bump the total-step-count, so after::",
            "",
            "        @step",
            "        def step1():",
            "            ...",
            "",
            "        @step",
            "        def step2():",
            "            ...",
            "",
            "    we have a total-step-count of 2.",
            "",
            "    Each time the step-function (not the step-decorator!) is invoked, we bump",
            "    the current-step-count by 1, so after::",
            "",
            "        step1()",
            "",
            "    the current-step-count would be 1 giving a progress of ``1 / 2 *",
            "    100`` or 50%.",
            "    \"\"\"",
            "    step_info = dict(total=0, current=0)",
            "",
            "    def bump_progress():",
            "        step_info['current'] += 1",
            "        update_instance_progress(context, instance,",
            "                                 step_info['current'], step_info['total'])",
            "",
            "    def step_decorator(f):",
            "        step_info['total'] += 1",
            "",
            "        @functools.wraps(f)",
            "        def inner(*args, **kwargs):",
            "            rv = f(*args, **kwargs)",
            "            bump_progress()",
            "            return rv",
            "",
            "        return inner",
            "",
            "    return step_decorator",
            "",
            "",
            "class VMOps(object):",
            "    \"\"\"",
            "    Management class for VM-related tasks",
            "    \"\"\"",
            "    def __init__(self, session, virtapi):",
            "        self.compute_api = compute.API()",
            "        self._session = session",
            "        self._virtapi = virtapi",
            "        self._volumeops = volumeops.VolumeOps(self._session)",
            "        self.firewall_driver = firewall.load_driver(",
            "            DEFAULT_FIREWALL_DRIVER,",
            "            self._virtapi,",
            "            xenapi_session=self._session)",
            "        vif_impl = importutils.import_class(CONF.xenapi_vif_driver)",
            "        self.vif_driver = vif_impl(xenapi_session=self._session)",
            "        self.default_root_dev = '/dev/sda'",
            "",
            "        LOG.debug(_(\"Importing image upload handler: %s\"),",
            "                  CONF.xenapi_image_upload_handler)",
            "        self.image_upload_handler = importutils.import_object(",
            "                                CONF.xenapi_image_upload_handler)",
            "",
            "    def agent_enabled(self, instance):",
            "        if CONF.xenapi_disable_agent:",
            "            return False",
            "",
            "        return xapi_agent.should_use_agent(instance)",
            "",
            "    def _get_agent(self, instance, vm_ref):",
            "        if self.agent_enabled(instance):",
            "            return xapi_agent.XenAPIBasedAgent(self._session, self._virtapi,",
            "                                               instance, vm_ref)",
            "        raise exception.NovaException(_(\"Error: Agent is disabled\"))",
            "",
            "    def instance_exists(self, name_label):",
            "        return vm_utils.lookup(self._session, name_label) is not None",
            "",
            "    def list_instances(self):",
            "        \"\"\"List VM instances.\"\"\"",
            "        # TODO(justinsb): Should we just always use the details method?",
            "        #  Seems to be the same number of API calls..",
            "        name_labels = []",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            name_labels.append(vm_rec[\"name_label\"])",
            "",
            "        return name_labels",
            "",
            "    def list_instance_uuids(self):",
            "        \"\"\"Get the list of nova instance uuids for VMs found on the",
            "        hypervisor.",
            "        \"\"\"",
            "        nova_uuids = []",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            other_config = vm_rec['other_config']",
            "            nova_uuid = other_config.get('nova_uuid')",
            "            if nova_uuid:",
            "                nova_uuids.append(nova_uuid)",
            "        return nova_uuids",
            "",
            "    def confirm_migration(self, migration, instance, network_info):",
            "        self._destroy_orig_vm(instance, network_info)",
            "",
            "    def _destroy_orig_vm(self, instance, network_info):",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "        return self._destroy(instance, vm_ref, network_info=network_info)",
            "",
            "    def _attach_mapped_block_devices(self, instance, block_device_info):",
            "        # We are attaching these volumes before start (no hotplugging)",
            "        # because some guests (windows) don't load PV drivers quickly",
            "        block_device_mapping = virt_driver.block_device_info_get_mapping(",
            "                block_device_info)",
            "        for vol in block_device_mapping:",
            "            connection_info = vol['connection_info']",
            "            mount_device = vol['mount_device'].rpartition(\"/\")[2]",
            "            self._volumeops.attach_volume(connection_info,",
            "                                          instance['name'],",
            "                                          mount_device,",
            "                                          hotplug=False)",
            "",
            "    def finish_revert_migration(self, instance, block_device_info=None,",
            "                                power_on=True):",
            "        self._restore_orig_vm_and_cleanup_orphan(instance, block_device_info,",
            "                                                 power_on)",
            "",
            "    def _restore_orig_vm_and_cleanup_orphan(self, instance,",
            "                                            block_device_info, power_on=True):",
            "        # NOTE(sirp): the original vm was suffixed with '-orig'; find it using",
            "        # the old suffix, remove the suffix, then power it back on.",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "",
            "        # NOTE(danms): if we're reverting migration in the failure case,",
            "        # make sure we don't have a conflicting vm still running here,",
            "        # as might be the case in a failed migrate-to-same-host situation",
            "        new_ref = vm_utils.lookup(self._session, instance['name'])",
            "        if vm_ref is not None:",
            "            if new_ref is not None:",
            "                self._destroy(instance, new_ref)",
            "            # Remove the '-orig' suffix (which was added in case the",
            "            # resized VM ends up on the source host, common during",
            "            # testing)",
            "            name_label = instance['name']",
            "            vm_utils.set_vm_name_label(self._session, vm_ref, name_label)",
            "            self._attach_mapped_block_devices(instance, block_device_info)",
            "        elif new_ref is not None:",
            "            # We crashed before the -orig backup was made",
            "            vm_ref = new_ref",
            "",
            "        if power_on:",
            "            self._start(instance, vm_ref)",
            "",
            "    def finish_migration(self, context, migration, instance, disk_info,",
            "                         network_info, image_meta, resize_instance,",
            "                         block_device_info=None, power_on=True):",
            "",
            "        def null_step_decorator(f):",
            "            return f",
            "",
            "        def create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                              name_label):",
            "            #TODO(johngarbutt) clean up the move_disks if this is not run",
            "            root_vdi = vm_utils.move_disks(self._session, instance, disk_info)",
            "",
            "            def undo_create_disks():",
            "                vm_utils.safe_destroy_vdis(self._session, [root_vdi['ref']])",
            "",
            "            undo_mgr.undo_with(undo_create_disks)",
            "            return {'root': root_vdi}",
            "",
            "        def completed_callback():",
            "            self._update_instance_progress(context, instance,",
            "                                           step=5,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "        self._spawn(context, instance, image_meta, null_step_decorator,",
            "                    create_disks_step, first_boot=False, injected_files=None,",
            "                    admin_password=None, network_info=network_info,",
            "                    block_device_info=block_device_info, name_label=None,",
            "                    rescue=False, power_on=power_on, resize=resize_instance,",
            "                    completed_callback=completed_callback)",
            "",
            "    def _start(self, instance, vm_ref=None, bad_volumes_callback=None):",
            "        \"\"\"Power on a VM instance.\"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        LOG.debug(_(\"Starting instance\"), instance=instance)",
            "",
            "        # Attached volumes that have become non-responsive will prevent a VM",
            "        # from starting, so scan for these before attempting to start",
            "        #",
            "        # In order to make sure this detach is consistent (virt, BDM, cinder),",
            "        # we only detach in the virt-layer if a callback is provided.",
            "        if bad_volumes_callback:",
            "            bad_devices = self._volumeops.find_bad_volumes(vm_ref)",
            "            for device_name in bad_devices:",
            "                self._volumeops.detach_volume(",
            "                        None, instance['name'], device_name)",
            "",
            "        self._session.call_xenapi('VM.start_on', vm_ref,",
            "                                  self._session.get_xenapi_host(),",
            "                                  False, False)",
            "",
            "        # Allow higher-layers a chance to detach bad-volumes as well (in order",
            "        # to cleanup BDM entries and detach in Cinder)",
            "        if bad_volumes_callback and bad_devices:",
            "            bad_volumes_callback(bad_devices)",
            "",
            "    def spawn(self, context, instance, image_meta, injected_files,",
            "              admin_password, network_info=None, block_device_info=None,",
            "              name_label=None, rescue=False):",
            "",
            "        if block_device_info:",
            "            LOG.debug(_(\"Block device information present: %s\")",
            "                      % block_device_info, instance=instance)",
            "        if block_device_info and not block_device_info['root_device_name']:",
            "            block_device_info['root_device_name'] = self.default_root_dev",
            "",
            "        step = make_step_decorator(context, instance,",
            "                                   self._update_instance_progress)",
            "",
            "        @step",
            "        def create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                              name_label):",
            "            vdis = vm_utils.get_vdis_for_instance(context, self._session,",
            "                        instance, name_label, image_meta.get('id'),",
            "                        disk_image_type, block_device_info=block_device_info)",
            "",
            "            def undo_create_disks():",
            "                vdi_refs = [vdi['ref'] for vdi in vdis.values()",
            "                        if not vdi.get('osvol')]",
            "                vm_utils.safe_destroy_vdis(self._session, vdi_refs)",
            "",
            "            undo_mgr.undo_with(undo_create_disks)",
            "            return vdis",
            "",
            "        self._spawn(context, instance, image_meta, step, create_disks_step,",
            "                    True, injected_files, admin_password,",
            "                    network_info, block_device_info, name_label, rescue)",
            "",
            "    def _spawn(self, context, instance, image_meta, step, create_disks_step,",
            "               first_boot, injected_files=None, admin_password=None,",
            "               network_info=None, block_device_info=None,",
            "               name_label=None, rescue=False, power_on=True, resize=True,",
            "               completed_callback=None):",
            "        if name_label is None:",
            "            name_label = instance['name']",
            "",
            "        self._ensure_instance_name_unique(name_label)",
            "        self._ensure_enough_free_mem(instance)",
            "",
            "        @step",
            "        def determine_disk_image_type_step(undo_mgr):",
            "            return vm_utils.determine_disk_image_type(image_meta)",
            "",
            "        @step",
            "        def create_kernel_ramdisk_step(undo_mgr):",
            "            kernel_file, ramdisk_file = vm_utils.create_kernel_and_ramdisk(",
            "                    context, self._session, instance, name_label)",
            "",
            "            def undo_create_kernel_ramdisk():",
            "                vm_utils.destroy_kernel_ramdisk(self._session, instance,",
            "                        kernel_file, ramdisk_file)",
            "",
            "            undo_mgr.undo_with(undo_create_kernel_ramdisk)",
            "            return kernel_file, ramdisk_file",
            "",
            "        @step",
            "        def create_vm_record_step(undo_mgr, vdis, disk_image_type,",
            "                kernel_file, ramdisk_file):",
            "            vm_ref = self._create_vm_record(context, instance, name_label,",
            "                    vdis, disk_image_type, kernel_file, ramdisk_file)",
            "",
            "            def undo_create_vm():",
            "                self._destroy(instance, vm_ref, network_info=network_info)",
            "",
            "            undo_mgr.undo_with(undo_create_vm)",
            "            return vm_ref",
            "",
            "        @step",
            "        def attach_disks_step(undo_mgr, vm_ref, vdis, disk_image_type):",
            "            try:",
            "                ipxe_boot = strutils.bool_from_string(",
            "                        image_meta['properties']['ipxe_boot'])",
            "            except KeyError:",
            "                ipxe_boot = False",
            "",
            "            if ipxe_boot:",
            "                if 'iso' in vdis:",
            "                    vm_utils.handle_ipxe_iso(",
            "                        self._session, instance, vdis['iso'], network_info)",
            "                else:",
            "                    LOG.warning(_('ipxe_boot is True but no ISO image found'),",
            "                                instance=instance)",
            "",
            "            root_vdi = vdis.get('root')",
            "            if root_vdi and resize:",
            "                self._resize_up_root_vdi(instance, root_vdi)",
            "",
            "            self._attach_disks(instance, vm_ref, name_label, vdis,",
            "                               disk_image_type, admin_password,",
            "                               injected_files)",
            "            if not first_boot:",
            "                self._attach_mapped_block_devices(instance,",
            "                                                  block_device_info)",
            "",
            "        if rescue:",
            "            # NOTE(johannes): Attach root disk to rescue VM now, before",
            "            # booting the VM, since we can't hotplug block devices",
            "            # on non-PV guests",
            "            @step",
            "            def attach_root_disk_step(undo_mgr, vm_ref):",
            "                vbd_ref = self._attach_orig_disk_for_rescue(instance, vm_ref)",
            "",
            "                def undo_attach_root_disk():",
            "                    # destroy the vbd in preparation to re-attach the VDI",
            "                    # to its original VM.  (does not delete VDI)",
            "                    vm_utils.destroy_vbd(self._session, vbd_ref)",
            "",
            "                undo_mgr.undo_with(undo_attach_root_disk)",
            "",
            "        @step",
            "        def inject_instance_data_step(undo_mgr, vm_ref, vdis):",
            "            self._inject_instance_metadata(instance, vm_ref)",
            "            self._inject_auto_disk_config(instance, vm_ref)",
            "            if first_boot:",
            "                self._inject_hostname(instance, vm_ref, rescue)",
            "            self._file_inject_vm_settings(instance, vm_ref, vdis, network_info)",
            "            self.inject_network_info(instance, network_info, vm_ref)",
            "",
            "        @step",
            "        def setup_network_step(undo_mgr, vm_ref):",
            "            self._create_vifs(instance, vm_ref, network_info)",
            "",
            "            try:",
            "                self.firewall_driver.setup_basic_filtering(",
            "                        instance, network_info)",
            "            except NotImplementedError:",
            "                # NOTE(salvatore-orlando): setup_basic_filtering might be",
            "                # empty or not implemented at all, as basic filter could",
            "                # be implemented with VIF rules created by xapi plugin",
            "                pass",
            "",
            "            self.firewall_driver.prepare_instance_filter(instance,",
            "                                                         network_info)",
            "",
            "        @step",
            "        def boot_instance_step(undo_mgr, vm_ref):",
            "            if power_on:",
            "                self._start(instance, vm_ref)",
            "                self._wait_for_instance_to_start(instance, vm_ref)",
            "",
            "        @step",
            "        def configure_booted_instance_step(undo_mgr, vm_ref):",
            "            if first_boot:",
            "                self._configure_new_instance_with_agent(instance, vm_ref,",
            "                        injected_files, admin_password)",
            "                self._remove_hostname(instance, vm_ref)",
            "",
            "        @step",
            "        def apply_security_group_filters_step(undo_mgr):",
            "            self.firewall_driver.apply_instance_filter(instance, network_info)",
            "",
            "        undo_mgr = utils.UndoManager()",
            "        try:",
            "            # NOTE(sirp): The create_disks() step will potentially take a",
            "            # *very* long time to complete since it has to fetch the image",
            "            # over the network and images can be several gigs in size. To",
            "            # avoid progress remaining at 0% for too long, make sure the",
            "            # first step is something that completes rather quickly.",
            "            disk_image_type = determine_disk_image_type_step(undo_mgr)",
            "",
            "            vdis = create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                                     name_label)",
            "            kernel_file, ramdisk_file = create_kernel_ramdisk_step(undo_mgr)",
            "",
            "            vm_ref = create_vm_record_step(undo_mgr, vdis, disk_image_type,",
            "                    kernel_file, ramdisk_file)",
            "            attach_disks_step(undo_mgr, vm_ref, vdis, disk_image_type)",
            "",
            "            inject_instance_data_step(undo_mgr, vm_ref, vdis)",
            "            setup_network_step(undo_mgr, vm_ref)",
            "",
            "            if rescue:",
            "                attach_root_disk_step(undo_mgr, vm_ref)",
            "",
            "            boot_instance_step(undo_mgr, vm_ref)",
            "",
            "            configure_booted_instance_step(undo_mgr, vm_ref)",
            "            apply_security_group_filters_step(undo_mgr)",
            "",
            "            if completed_callback:",
            "                completed_callback()",
            "        except Exception:",
            "            msg = _(\"Failed to spawn, rolling back\")",
            "            undo_mgr.rollback_and_reraise(msg=msg, instance=instance)",
            "",
            "    def _attach_orig_disk_for_rescue(self, instance, vm_ref):",
            "        orig_vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "        vdi_ref = self._find_root_vdi_ref(orig_vm_ref)",
            "        return vm_utils.create_vbd(self._session, vm_ref, vdi_ref,",
            "                                   DEVICE_RESCUE, bootable=False)",
            "",
            "    def _file_inject_vm_settings(self, instance, vm_ref, vdis, network_info):",
            "        if CONF.flat_injected:",
            "            vm_utils.preconfigure_instance(self._session, instance,",
            "                                           vdis['root']['ref'], network_info)",
            "",
            "    def _ensure_instance_name_unique(self, name_label):",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "        if vm_ref is not None:",
            "            raise exception.InstanceExists(name=name_label)",
            "",
            "    def _ensure_enough_free_mem(self, instance):",
            "        if not vm_utils.is_enough_free_mem(self._session, instance):",
            "            raise exception.InsufficientFreeMemory(uuid=instance['uuid'])",
            "",
            "    def _create_vm_record(self, context, instance, name_label, vdis,",
            "            disk_image_type, kernel_file, ramdisk_file):",
            "        \"\"\"Create the VM record in Xen, making sure that we do not create",
            "        a duplicate name-label.  Also do a rough sanity check on memory",
            "        to try to short-circuit a potential failure later.  (The memory",
            "        check only accounts for running VMs, so it can miss other builds",
            "        that are in progress.)",
            "        \"\"\"",
            "        mode = self._determine_vm_mode(instance, vdis, disk_image_type)",
            "        if instance['vm_mode'] != mode:",
            "            # Update database with normalized (or determined) value",
            "            self._virtapi.instance_update(context,",
            "                                          instance['uuid'], {'vm_mode': mode})",
            "",
            "        use_pv_kernel = (mode == vm_mode.XEN)",
            "        vm_ref = vm_utils.create_vm(self._session, instance, name_label,",
            "                                    kernel_file, ramdisk_file, use_pv_kernel)",
            "        return vm_ref",
            "",
            "    def _determine_vm_mode(self, instance, vdis, disk_image_type):",
            "        current_mode = vm_mode.get_from_instance(instance)",
            "        if current_mode == vm_mode.XEN or current_mode == vm_mode.HVM:",
            "            return current_mode",
            "",
            "        is_pv = False",
            "        if 'root' in vdis:",
            "            os_type = instance['os_type']",
            "            vdi_ref = vdis['root']['ref']",
            "            is_pv = vm_utils.determine_is_pv(self._session, vdi_ref,",
            "                                             disk_image_type, os_type)",
            "        if is_pv:",
            "            return vm_mode.XEN",
            "        else:",
            "            return vm_mode.HVM",
            "",
            "    def _attach_disks(self, instance, vm_ref, name_label, vdis,",
            "                      disk_image_type, admin_password=None, files=None):",
            "        ctx = nova_context.get_admin_context()",
            "        instance_type = flavors.extract_flavor(instance)",
            "",
            "        # Attach (required) root disk",
            "        if disk_image_type == vm_utils.ImageType.DISK_ISO:",
            "            # DISK_ISO needs two VBDs: the ISO disk and a blank RW disk",
            "            root_disk_size = instance_type['root_gb']",
            "            if root_disk_size > 0:",
            "                vm_utils.generate_iso_blank_root_disk(self._session, instance,",
            "                    vm_ref, DEVICE_ROOT, name_label, root_disk_size)",
            "",
            "            cd_vdi = vdis.pop('iso')",
            "            vm_utils.attach_cd(self._session, vm_ref, cd_vdi['ref'],",
            "                               DEVICE_CD)",
            "        else:",
            "            root_vdi = vdis['root']",
            "",
            "            if instance['auto_disk_config']:",
            "                LOG.debug(_(\"Auto configuring disk, attempting to \"",
            "                            \"resize partition...\"), instance=instance)",
            "                vm_utils.try_auto_configure_disk(self._session,",
            "                                                 root_vdi['ref'],",
            "                                                 instance_type['root_gb'])",
            "",
            "            vm_utils.create_vbd(self._session, vm_ref, root_vdi['ref'],",
            "                                DEVICE_ROOT, bootable=True,",
            "                                osvol=root_vdi.get('osvol'))",
            "",
            "        # Attach (optional) additional block-devices",
            "        for type_, vdi_info in vdis.items():",
            "            # Additional block-devices for boot use their device-name as the",
            "            # type.",
            "            if not type_.startswith('/dev'):",
            "                continue",
            "",
            "            # Convert device name to userdevice number, e.g. /dev/xvdb -> 1",
            "            userdevice = ord(block_device.strip_prefix(type_)) - ord('a')",
            "            vm_utils.create_vbd(self._session, vm_ref, vdi_info['ref'],",
            "                                userdevice, bootable=False,",
            "                                osvol=vdi_info.get('osvol'))",
            "",
            "        # Attach (optional) swap disk",
            "        swap_mb = instance_type['swap']",
            "        if swap_mb:",
            "            vm_utils.generate_swap(self._session, instance, vm_ref,",
            "                                   DEVICE_SWAP, name_label, swap_mb)",
            "",
            "        # Attach (optional) ephemeral disk",
            "        ephemeral_gb = instance_type['ephemeral_gb']",
            "        if ephemeral_gb:",
            "            vm_utils.generate_ephemeral(self._session, instance, vm_ref,",
            "                                        DEVICE_EPHEMERAL, name_label,",
            "                                        ephemeral_gb)",
            "",
            "        # Attach (optional) configdrive v2 disk",
            "        if configdrive.required_by(instance):",
            "            vm_utils.generate_configdrive(self._session, instance, vm_ref,",
            "                                          DEVICE_CONFIGDRIVE,",
            "                                          admin_password=admin_password,",
            "                                          files=files)",
            "",
            "    def _wait_for_instance_to_start(self, instance, vm_ref):",
            "        LOG.debug(_('Waiting for instance state to become running'),",
            "                  instance=instance)",
            "        expiration = time.time() + CONF.xenapi_running_timeout",
            "        while time.time() < expiration:",
            "            state = self.get_info(instance, vm_ref)['state']",
            "            if state == power_state.RUNNING:",
            "                break",
            "            greenthread.sleep(0.5)",
            "",
            "    def _configure_new_instance_with_agent(self, instance, vm_ref,",
            "                                           injected_files, admin_password):",
            "        if self.agent_enabled(instance):",
            "            ctx = nova_context.get_admin_context()",
            "            agent_build = self._virtapi.agent_build_get_by_triple(",
            "                ctx, 'xen', instance['os_type'], instance['architecture'])",
            "            if agent_build:",
            "                LOG.info(_('Latest agent build for %(hypervisor)s/%(os)s'",
            "                           '/%(architecture)s is %(version)s') % agent_build)",
            "            else:",
            "                LOG.info(_('No agent build found for %(hypervisor)s/%(os)s'",
            "                           '/%(architecture)s') % {",
            "                            'hypervisor': 'xen',",
            "                            'os': instance['os_type'],",
            "                            'architecture': instance['architecture']})",
            "",
            "            # Update agent, if necessary",
            "            # This also waits until the agent starts",
            "            agent = self._get_agent(instance, vm_ref)",
            "            version = agent.get_agent_version()",
            "            if version:",
            "                LOG.info(_('Instance agent version: %s'), version,",
            "                         instance=instance)",
            "",
            "            if (version and agent_build and",
            "                    cmp_version(version, agent_build['version']) < 0):",
            "                agent.agent_update(agent_build)",
            "",
            "            # if the guest agent is not available, configure the",
            "            # instance, but skip the admin password configuration",
            "            no_agent = version is None",
            "",
            "            # Inject ssh key.",
            "            agent.inject_ssh_key()",
            "",
            "            # Inject files, if necessary",
            "            if injected_files:",
            "                # Inject any files, if specified",
            "                agent.inject_files(injected_files)",
            "",
            "            # Set admin password, if necessary",
            "            if admin_password and not no_agent:",
            "                agent.set_admin_password(admin_password)",
            "",
            "            # Reset network config",
            "            agent.resetnetwork()",
            "",
            "    def _get_vm_opaque_ref(self, instance, check_rescue=False):",
            "        \"\"\"Get xapi OpaqueRef from a db record.",
            "        :param check_rescue: if True will return the 'name'-rescue vm if it",
            "                             exists, instead of just 'name'",
            "        \"\"\"",
            "        vm_ref = vm_utils.lookup(self._session, instance['name'], check_rescue)",
            "        if vm_ref is None:",
            "            raise exception.InstanceNotFound(instance_id=instance['name'])",
            "        return vm_ref",
            "",
            "    def _acquire_bootlock(self, vm):",
            "        \"\"\"Prevent an instance from booting.\"\"\"",
            "        self._session.call_xenapi(",
            "            \"VM.set_blocked_operations\",",
            "            vm,",
            "            {\"start\": \"\"})",
            "",
            "    def _release_bootlock(self, vm):",
            "        \"\"\"Allow an instance to boot.\"\"\"",
            "        self._session.call_xenapi(",
            "            \"VM.remove_from_blocked_operations\",",
            "            vm,",
            "            \"start\")",
            "",
            "    def snapshot(self, context, instance, image_id, update_task_state):",
            "        \"\"\"Create snapshot from a running VM instance.",
            "",
            "        :param context: request context",
            "        :param instance: instance to be snapshotted",
            "        :param image_id: id of image to upload to",
            "",
            "        Steps involved in a XenServer snapshot:",
            "",
            "        1. XAPI-Snapshot: Snapshotting the instance using XenAPI. This",
            "           creates: Snapshot (Template) VM, Snapshot VBD, Snapshot VDI,",
            "           Snapshot VHD",
            "",
            "        2. Wait-for-coalesce: The Snapshot VDI and Instance VDI both point to",
            "           a 'base-copy' VDI.  The base_copy is immutable and may be chained",
            "           with other base_copies.  If chained, the base_copies",
            "           coalesce together, so, we must wait for this coalescing to occur to",
            "           get a stable representation of the data on disk.",
            "",
            "        3. Push-to-data-store: Once coalesced, we call",
            "           'xenapi_image_upload_handler' to upload the images.",
            "",
            "        \"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        label = \"%s-snapshot\" % instance['name']",
            "",
            "        with vm_utils.snapshot_attached_here(",
            "                self._session, instance, vm_ref, label,",
            "                update_task_state) as vdi_uuids:",
            "            update_task_state(task_state=task_states.IMAGE_UPLOADING,",
            "                              expected_state=task_states.IMAGE_PENDING_UPLOAD)",
            "            self.image_upload_handler.upload_image(context,",
            "                                                   self._session,",
            "                                                   instance,",
            "                                                   vdi_uuids,",
            "                                                   image_id)",
            "",
            "        LOG.debug(_(\"Finished snapshot and upload for VM\"),",
            "                  instance=instance)",
            "",
            "    def _migrate_vhd(self, instance, vdi_uuid, dest, sr_path, seq_num):",
            "        LOG.debug(_(\"Migrating VHD '%(vdi_uuid)s' with seq_num %(seq_num)d\"),",
            "                  {'vdi_uuid': vdi_uuid, 'seq_num': seq_num},",
            "                  instance=instance)",
            "        instance_uuid = instance['uuid']",
            "        try:",
            "            self._session.call_plugin_serialized('migration', 'transfer_vhd',",
            "                    instance_uuid=instance_uuid, host=dest, vdi_uuid=vdi_uuid,",
            "                    sr_path=sr_path, seq_num=seq_num)",
            "        except self._session.XenAPI.Failure:",
            "            msg = _(\"Failed to transfer vhd to new host\")",
            "            raise exception.MigrationError(reason=msg)",
            "",
            "    def _get_orig_vm_name_label(self, instance):",
            "        return instance['name'] + '-orig'",
            "",
            "    def _update_instance_progress(self, context, instance, step, total_steps):",
            "        \"\"\"Update instance progress percent to reflect current step number",
            "        \"\"\"",
            "        # FIXME(sirp): for now we're taking a KISS approach to instance",
            "        # progress:",
            "        # Divide the action's workflow into discrete steps and \"bump\" the",
            "        # instance's progress field as each step is completed.",
            "        #",
            "        # For a first cut this should be fine, however, for large VM images,",
            "        # the get_vdis_for_instance step begins to dominate the equation. A",
            "        # better approximation would use the percentage of the VM image that",
            "        # has been streamed to the destination host.",
            "        progress = round(float(step) / total_steps * 100)",
            "        LOG.debug(_(\"Updating progress to %d\"), progress,",
            "                  instance=instance)",
            "        self._virtapi.instance_update(context, instance['uuid'],",
            "                                      {'progress': progress})",
            "",
            "    def _resize_ensure_vm_is_shutdown(self, instance, vm_ref):",
            "        if vm_utils.is_vm_shutdown(self._session, vm_ref):",
            "            LOG.debug(_(\"VM was already shutdown.\"), instance=instance)",
            "            return",
            "",
            "        if not vm_utils.clean_shutdown_vm(self._session, instance, vm_ref):",
            "            LOG.debug(_(\"Clean shutdown did not complete successfully, \"",
            "                        \"trying hard shutdown.\"), instance=instance)",
            "            if not vm_utils.hard_shutdown_vm(self._session, instance, vm_ref):",
            "                raise exception.ResizeError(",
            "                    reason=_(\"Unable to terminate instance.\"))",
            "",
            "    def _migrate_disk_resizing_down(self, context, instance, dest,",
            "                                    instance_type, vm_ref, sr_path):",
            "        step = make_step_decorator(context, instance,",
            "                                   self._update_instance_progress)",
            "",
            "        @step",
            "        def fake_step_to_match_resizing_up():",
            "            pass",
            "",
            "        @step",
            "        def rename_and_power_off_vm(undo_mgr):",
            "            self._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "            self._apply_orig_vm_name_label(instance, vm_ref)",
            "",
            "            def restore_orig_vm():",
            "                # Do not need to restore block devices, not yet been removed",
            "                self._restore_orig_vm_and_cleanup_orphan(instance, None)",
            "",
            "            undo_mgr.undo_with(restore_orig_vm)",
            "",
            "        @step",
            "        def create_copy_vdi_and_resize(undo_mgr, old_vdi_ref):",
            "            new_vdi_ref, new_vdi_uuid = vm_utils.resize_disk(self._session,",
            "                instance, old_vdi_ref, instance_type)",
            "",
            "            def cleanup_vdi_copy():",
            "                vm_utils.destroy_vdi(self._session, new_vdi_ref)",
            "",
            "            undo_mgr.undo_with(cleanup_vdi_copy)",
            "",
            "            return new_vdi_ref, new_vdi_uuid",
            "",
            "        @step",
            "        def transfer_vhd_to_dest(new_vdi_ref, new_vdi_uuid):",
            "            self._migrate_vhd(instance, new_vdi_uuid, dest, sr_path, 0)",
            "            # Clean up VDI now that it's been copied",
            "            vm_utils.destroy_vdi(self._session, new_vdi_ref)",
            "",
            "        @step",
            "        def fake_step_to_be_executed_by_finish_migration():",
            "            pass",
            "",
            "        undo_mgr = utils.UndoManager()",
            "        try:",
            "            fake_step_to_match_resizing_up()",
            "            rename_and_power_off_vm(undo_mgr)",
            "            old_vdi_ref, _ignore = vm_utils.get_vdi_for_vm_safely(",
            "                self._session, vm_ref)",
            "            new_vdi_ref, new_vdi_uuid = create_copy_vdi_and_resize(",
            "                undo_mgr, old_vdi_ref)",
            "            transfer_vhd_to_dest(new_vdi_ref, new_vdi_uuid)",
            "        except Exception as error:",
            "            LOG.exception(_(\"_migrate_disk_resizing_down failed. \"",
            "                            \"Restoring orig vm due_to: %s.\"), error,",
            "                          instance=instance)",
            "            undo_mgr._rollback()",
            "            raise exception.InstanceFaultRollback(error)",
            "",
            "    def _migrate_disk_resizing_up(self, context, instance, dest, vm_ref,",
            "                                  sr_path):",
            "        self._apply_orig_vm_name_label(instance, vm_ref)",
            "",
            "        # 1. Create Snapshot",
            "        label = \"%s-snapshot\" % instance['name']",
            "        with vm_utils.snapshot_attached_here(",
            "                self._session, instance, vm_ref, label) as vdi_uuids:",
            "            self._update_instance_progress(context, instance,",
            "                                           step=1,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 2. Transfer the immutable VHDs (base-copies)",
            "            #",
            "            # The first VHD will be the leaf (aka COW) that is being used by",
            "            # the VM. For this step, we're only interested in the immutable",
            "            # VHDs which are all of the parents of the leaf VHD.",
            "            for seq_num, vdi_uuid in itertools.islice(",
            "                    enumerate(vdi_uuids), 1, None):",
            "                self._migrate_vhd(instance, vdi_uuid, dest, sr_path, seq_num)",
            "                self._update_instance_progress(context, instance,",
            "                                               step=2,",
            "                                               total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 3. Now power down the instance",
            "            self._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "            self._update_instance_progress(context, instance,",
            "                                           step=3,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 4. Transfer the COW VHD",
            "            vdi_ref, vm_vdi_rec = vm_utils.get_vdi_for_vm_safely(",
            "                self._session, vm_ref)",
            "            cow_uuid = vm_vdi_rec['uuid']",
            "            self._migrate_vhd(instance, cow_uuid, dest, sr_path, 0)",
            "            self._update_instance_progress(context, instance,",
            "                                           step=4,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "    def _apply_orig_vm_name_label(self, instance, vm_ref):",
            "        # NOTE(sirp): in case we're resizing to the same host (for dev",
            "        # purposes), apply a suffix to name-label so the two VM records",
            "        # extant until a confirm_resize don't collide.",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_utils.set_vm_name_label(self._session, vm_ref, name_label)",
            "",
            "    def migrate_disk_and_power_off(self, context, instance, dest,",
            "                                   instance_type, block_device_info):",
            "        \"\"\"Copies a VHD from one host machine to another, possibly",
            "        resizing filesystem before hand.",
            "",
            "        :param instance: the instance that owns the VHD in question.",
            "        :param dest: the destination host machine.",
            "        :param instance_type: instance_type to resize to",
            "        \"\"\"",
            "        # 0. Zero out the progress to begin",
            "        self._update_instance_progress(context, instance,",
            "                                       step=0,",
            "                                       total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "        old_gb = instance['root_gb']",
            "        new_gb = instance_type['root_gb']",
            "        resize_down = old_gb > new_gb",
            "",
            "        if new_gb == 0 and old_gb != 0:",
            "            reason = _(\"Can't resize a disk to 0 GB.\")",
            "            raise exception.ResizeError(reason=reason)",
            "",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        sr_path = vm_utils.get_sr_path(self._session)",
            "",
            "        if resize_down:",
            "            self._migrate_disk_resizing_down(",
            "                    context, instance, dest, instance_type, vm_ref, sr_path)",
            "        else:",
            "            self._migrate_disk_resizing_up(",
            "                    context, instance, dest, vm_ref, sr_path)",
            "",
            "        self._detach_block_devices_from_orig_vm(instance, block_device_info)",
            "",
            "        # NOTE(sirp): disk_info isn't used by the xenapi driver, instead it",
            "        # uses a staging-area (/images/instance<uuid>) and sequence-numbered",
            "        # VHDs to figure out how to reconstruct the VDI chain after syncing",
            "        disk_info = {}",
            "        return disk_info",
            "",
            "    def _detach_block_devices_from_orig_vm(self, instance, block_device_info):",
            "        block_device_mapping = virt_driver.block_device_info_get_mapping(",
            "                block_device_info)",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        for vol in block_device_mapping:",
            "            connection_info = vol['connection_info']",
            "            mount_device = vol['mount_device'].rpartition(\"/\")[2]",
            "            self._volumeops.detach_volume(connection_info, name_label,",
            "                                          mount_device)",
            "",
            "    def _resize_up_root_vdi(self, instance, root_vdi):",
            "        \"\"\"Resize an instances root disk.\"\"\"",
            "",
            "        new_disk_size = instance['root_gb'] * 1024 * 1024 * 1024",
            "        if not new_disk_size:",
            "            return",
            "",
            "        # Get current size of VDI",
            "        virtual_size = self._session.call_xenapi('VDI.get_virtual_size',",
            "                                                 root_vdi['ref'])",
            "        virtual_size = int(virtual_size)",
            "",
            "        old_gb = virtual_size / (1024 * 1024 * 1024)",
            "        new_gb = instance['root_gb']",
            "",
            "        if virtual_size < new_disk_size:",
            "            # Resize up. Simple VDI resize will do the trick",
            "            vdi_uuid = root_vdi['uuid']",
            "            LOG.debug(_(\"Resizing up VDI %(vdi_uuid)s from %(old_gb)dGB to \"",
            "                        \"%(new_gb)dGB\"),",
            "                      {'vdi_uuid': vdi_uuid, 'old_gb': old_gb,",
            "                       'new_gb': new_gb}, instance=instance)",
            "            resize_func_name = self.check_resize_func_name()",
            "            self._session.call_xenapi(resize_func_name, root_vdi['ref'],",
            "                    str(new_disk_size))",
            "            LOG.debug(_(\"Resize complete\"), instance=instance)",
            "",
            "    def check_resize_func_name(self):",
            "        \"\"\"Check the function name used to resize an instance based",
            "        on product_brand and product_version.",
            "        \"\"\"",
            "",
            "        brand = self._session.product_brand",
            "        version = self._session.product_version",
            "",
            "        # To maintain backwards compatibility. All recent versions",
            "        # should use VDI.resize",
            "        if bool(version) and bool(brand):",
            "            xcp = brand == 'XCP'",
            "            r1_2_or_above = (",
            "                (",
            "                    version[0] == 1",
            "                    and version[1] > 1",
            "                )",
            "                or version[0] > 1)",
            "",
            "            xenserver = brand == 'XenServer'",
            "            r6_or_above = version[0] > 5",
            "",
            "            if (xcp and not r1_2_or_above) or (xenserver and not r6_or_above):",
            "                return 'VDI.resize_online'",
            "",
            "        return 'VDI.resize'",
            "",
            "    def reboot(self, instance, reboot_type, bad_volumes_callback=None):",
            "        \"\"\"Reboot VM instance.\"\"\"",
            "        # Note (salvatore-orlando): security group rules are not re-enforced",
            "        # upon reboot, since this action on the XenAPI drivers does not",
            "        # remove existing filters",
            "        vm_ref = self._get_vm_opaque_ref(instance, check_rescue=True)",
            "",
            "        try:",
            "            if reboot_type == \"HARD\":",
            "                self._session.call_xenapi('VM.hard_reboot', vm_ref)",
            "            else:",
            "                self._session.call_xenapi('VM.clean_reboot', vm_ref)",
            "        except self._session.XenAPI.Failure as exc:",
            "            details = exc.details",
            "            if (details[0] == 'VM_BAD_POWER_STATE' and",
            "                    details[-1] == 'halted'):",
            "                LOG.info(_(\"Starting halted instance found during reboot\"),",
            "                    instance=instance)",
            "                self._start(instance, vm_ref=vm_ref,",
            "                            bad_volumes_callback=bad_volumes_callback)",
            "                return",
            "            elif details[0] == 'SR_BACKEND_FAILURE_46':",
            "                LOG.warn(_(\"Reboot failed due to bad volumes, detaching bad\"",
            "                           \" volumes and starting halted instance\"),",
            "                         instance=instance)",
            "                self._start(instance, vm_ref=vm_ref,",
            "                            bad_volumes_callback=bad_volumes_callback)",
            "                return",
            "            else:",
            "                raise",
            "",
            "    def set_admin_password(self, instance, new_pass):",
            "        \"\"\"Set the root/admin password on the VM instance.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.set_admin_password(new_pass)",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    def inject_file(self, instance, path, contents):",
            "        \"\"\"Write a file to the VM instance.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.inject_file(path, contents)",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    @staticmethod",
            "    def _sanitize_xenstore_key(key):",
            "        \"\"\"",
            "        Xenstore only allows the following characters as keys:",
            "",
            "        ABCDEFGHIJKLMNOPQRSTUVWXYZ",
            "        abcdefghijklmnopqrstuvwxyz",
            "        0123456789-/_@",
            "",
            "        So convert the others to _",
            "",
            "        Also convert / to _, because that is somewhat like a path",
            "        separator.",
            "        \"\"\"",
            "        allowed_chars = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"",
            "                         \"abcdefghijklmnopqrstuvwxyz\"",
            "                         \"0123456789-_@\")",
            "        return ''.join([x in allowed_chars and x or '_' for x in key])",
            "",
            "    def _inject_instance_metadata(self, instance, vm_ref):",
            "        \"\"\"Inject instance metadata into xenstore.\"\"\"",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def store_meta(topdir, data_dict):",
            "            for key, value in data_dict.items():",
            "                key = self._sanitize_xenstore_key(key)",
            "                value = value or ''",
            "                self._add_to_param_xenstore(vm_ref, '%s/%s' % (topdir, key),",
            "                                            jsonutils.dumps(value))",
            "",
            "        # Store user metadata",
            "        store_meta('vm-data/user-metadata', utils.instance_meta(instance))",
            "",
            "    def _inject_auto_disk_config(self, instance, vm_ref):",
            "        \"\"\"Inject instance's auto_disk_config attribute into xenstore.\"\"\"",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def store_auto_disk_config(key, value):",
            "            value = value and True or False",
            "            self._add_to_param_xenstore(vm_ref, key, str(value))",
            "",
            "        store_auto_disk_config('vm-data/auto-disk-config',",
            "                               instance['auto_disk_config'])",
            "",
            "    def change_instance_metadata(self, instance, diff):",
            "        \"\"\"Apply changes to instance metadata to xenstore.\"\"\"",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "        except exception.NotFound:",
            "            # NOTE(johngarbutt) race conditions mean we can still get here",
            "            # during operations where the VM is not present, like resize.",
            "            # Skip the update when not possible, as the updated metadata will",
            "            # get added when the VM is being booted up at the end of the",
            "            # resize or rebuild.",
            "            LOG.warn(_(\"Unable to update metadata, VM not found.\"),",
            "                     instance=instance, exc_info=True)",
            "            return",
            "",
            "        def process_change(location, change):",
            "            if change[0] == '-':",
            "                self._remove_from_param_xenstore(vm_ref, location)",
            "                try:",
            "                    self._delete_from_xenstore(instance, location,",
            "                                               vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "            elif change[0] == '+':",
            "                self._add_to_param_xenstore(vm_ref, location,",
            "                                            jsonutils.dumps(change[1]))",
            "                try:",
            "                    self._write_to_xenstore(instance, location, change[1],",
            "                                            vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def update_meta():",
            "            for key, change in diff.items():",
            "                key = self._sanitize_xenstore_key(key)",
            "                location = 'vm-data/user-metadata/%s' % key",
            "                process_change(location, change)",
            "        update_meta()",
            "",
            "    def _find_root_vdi_ref(self, vm_ref):",
            "        \"\"\"Find and return the root vdi ref for a VM.\"\"\"",
            "        if not vm_ref:",
            "            return None",
            "",
            "        vbd_refs = self._session.call_xenapi(\"VM.get_VBDs\", vm_ref)",
            "",
            "        for vbd_uuid in vbd_refs:",
            "            vbd = self._session.call_xenapi(\"VBD.get_record\", vbd_uuid)",
            "            if vbd[\"userdevice\"] == DEVICE_ROOT:",
            "                return vbd[\"VDI\"]",
            "",
            "        raise exception.NotFound(_(\"Unable to find root VBD/VDI for VM\"))",
            "",
            "    def _destroy_vdis(self, instance, vm_ref):",
            "        \"\"\"Destroys all VDIs associated with a VM.\"\"\"",
            "        LOG.debug(_(\"Destroying VDIs\"), instance=instance)",
            "",
            "        vdi_refs = vm_utils.lookup_vm_vdis(self._session, vm_ref)",
            "        if not vdi_refs:",
            "            return",
            "        for vdi_ref in vdi_refs:",
            "            try:",
            "                vm_utils.destroy_vdi(self._session, vdi_ref)",
            "            except volume_utils.StorageError as exc:",
            "                LOG.error(exc)",
            "",
            "    def _destroy_kernel_ramdisk(self, instance, vm_ref):",
            "        \"\"\"Three situations can occur:",
            "",
            "            1. We have neither a ramdisk nor a kernel, in which case we are a",
            "               RAW image and can omit this step",
            "",
            "            2. We have one or the other, in which case, we should flag as an",
            "               error",
            "",
            "            3. We have both, in which case we safely remove both the kernel",
            "               and the ramdisk.",
            "",
            "        \"\"\"",
            "        instance_uuid = instance['uuid']",
            "        if not instance['kernel_id'] and not instance['ramdisk_id']:",
            "            # 1. No kernel or ramdisk",
            "            LOG.debug(_(\"Using RAW or VHD, skipping kernel and ramdisk \"",
            "                        \"deletion\"), instance=instance)",
            "            return",
            "",
            "        if not (instance['kernel_id'] and instance['ramdisk_id']):",
            "            # 2. We only have kernel xor ramdisk",
            "            raise exception.InstanceUnacceptable(instance_id=instance_uuid,",
            "               reason=_(\"instance has a kernel or ramdisk but not both\"))",
            "",
            "        # 3. We have both kernel and ramdisk",
            "        (kernel, ramdisk) = vm_utils.lookup_kernel_ramdisk(self._session,",
            "                                                           vm_ref)",
            "        if kernel or ramdisk:",
            "            vm_utils.destroy_kernel_ramdisk(self._session, instance,",
            "                                            kernel, ramdisk)",
            "            LOG.debug(_(\"kernel/ramdisk files removed\"), instance=instance)",
            "",
            "    def _destroy_rescue_instance(self, rescue_vm_ref, original_vm_ref):",
            "        \"\"\"Destroy a rescue instance.\"\"\"",
            "        # Shutdown Rescue VM",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", rescue_vm_ref)",
            "        state = vm_utils.compile_info(vm_rec)['state']",
            "        if state != power_state.SHUTDOWN:",
            "            self._session.call_xenapi(\"VM.hard_shutdown\", rescue_vm_ref)",
            "",
            "        # Destroy Rescue VDIs",
            "        vdi_refs = vm_utils.lookup_vm_vdis(self._session, rescue_vm_ref)",
            "        root_vdi_ref = self._find_root_vdi_ref(original_vm_ref)",
            "        vdi_refs = [vdi_ref for vdi_ref in vdi_refs if vdi_ref != root_vdi_ref]",
            "        vm_utils.safe_destroy_vdis(self._session, vdi_refs)",
            "",
            "        # Destroy Rescue VM",
            "        self._session.call_xenapi(\"VM.destroy\", rescue_vm_ref)",
            "",
            "    def destroy(self, instance, network_info, block_device_info=None,",
            "                destroy_disks=True):",
            "        \"\"\"Destroy VM instance.",
            "",
            "        This is the method exposed by xenapi_conn.destroy(). The rest of the",
            "        destroy_* methods are internal.",
            "",
            "        \"\"\"",
            "        LOG.info(_(\"Destroying VM\"), instance=instance)",
            "",
            "        # We don't use _get_vm_opaque_ref because the instance may",
            "        # truly not exist because of a failure during build. A valid",
            "        # vm_ref is checked correctly where necessary.",
            "        vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "",
            "        rescue_vm_ref = vm_utils.lookup(self._session,",
            "                                        \"%s-rescue\" % instance['name'])",
            "        if rescue_vm_ref:",
            "            self._destroy_rescue_instance(rescue_vm_ref, vm_ref)",
            "",
            "        # NOTE(sirp): `block_device_info` is not used, information about which",
            "        # volumes should be detached is determined by the",
            "        # VBD.other_config['osvol'] attribute",
            "        return self._destroy(instance, vm_ref, network_info=network_info,",
            "                             destroy_disks=destroy_disks)",
            "",
            "    def _destroy(self, instance, vm_ref, network_info=None,",
            "                 destroy_disks=True):",
            "        \"\"\"Destroys VM instance by performing:",
            "",
            "            1. A shutdown",
            "            2. Destroying associated VDIs.",
            "            3. Destroying kernel and ramdisk files (if necessary).",
            "            4. Destroying that actual VM record.",
            "",
            "        \"\"\"",
            "        if vm_ref is None:",
            "            LOG.warning(_(\"VM is not present, skipping destroy...\"),",
            "                        instance=instance)",
            "            return",
            "",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "",
            "        if destroy_disks:",
            "            self._volumeops.detach_all(vm_ref)",
            "            self._destroy_vdis(instance, vm_ref)",
            "            self._destroy_kernel_ramdisk(instance, vm_ref)",
            "",
            "        vm_utils.destroy_vm(self._session, instance, vm_ref)",
            "",
            "        self.unplug_vifs(instance, network_info)",
            "        self.firewall_driver.unfilter_instance(",
            "                instance, network_info=network_info)",
            "",
            "    def pause(self, instance):",
            "        \"\"\"Pause VM instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._session.call_xenapi('VM.pause', vm_ref)",
            "",
            "    def unpause(self, instance):",
            "        \"\"\"Unpause VM instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._session.call_xenapi('VM.unpause', vm_ref)",
            "",
            "    def suspend(self, instance):",
            "        \"\"\"Suspend the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._acquire_bootlock(vm_ref)",
            "        self._session.call_xenapi('VM.suspend', vm_ref)",
            "",
            "    def resume(self, instance):",
            "        \"\"\"Resume the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._release_bootlock(vm_ref)",
            "        self._session.call_xenapi('VM.resume', vm_ref, False, True)",
            "",
            "    def rescue(self, context, instance, network_info, image_meta,",
            "               rescue_password):",
            "        \"\"\"Rescue the specified instance.",
            "",
            "            - shutdown the instance VM.",
            "            - set 'bootlock' to prevent the instance from starting in rescue.",
            "            - spawn a rescue VM (the vm name-label will be instance-N-rescue).",
            "",
            "        \"\"\"",
            "        rescue_name_label = '%s-rescue' % instance['name']",
            "        rescue_vm_ref = vm_utils.lookup(self._session, rescue_name_label)",
            "        if rescue_vm_ref:",
            "            raise RuntimeError(_(\"Instance is already in Rescue Mode: %s\")",
            "                               % instance['name'])",
            "",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "        self._acquire_bootlock(vm_ref)",
            "        self.spawn(context, instance, image_meta, [], rescue_password,",
            "                   network_info, name_label=rescue_name_label, rescue=True)",
            "",
            "    def unrescue(self, instance):",
            "        \"\"\"Unrescue the specified instance.",
            "",
            "            - unplug the instance VM's disk from the rescue VM.",
            "            - teardown the rescue VM.",
            "            - release the bootlock to allow the instance VM to start.",
            "",
            "        \"\"\"",
            "        rescue_vm_ref = vm_utils.lookup(self._session,",
            "                                        \"%s-rescue\" % instance['name'])",
            "        if not rescue_vm_ref:",
            "            raise exception.InstanceNotInRescueMode(",
            "                    instance_id=instance['uuid'])",
            "",
            "        original_vm_ref = self._get_vm_opaque_ref(instance)",
            "",
            "        self._destroy_rescue_instance(rescue_vm_ref, original_vm_ref)",
            "        self._release_bootlock(original_vm_ref)",
            "        self._start(instance, original_vm_ref)",
            "",
            "    def soft_delete(self, instance):",
            "        \"\"\"Soft delete the specified instance.\"\"\"",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "        except exception.NotFound:",
            "            LOG.warning(_(\"VM is not present, skipping soft delete...\"),",
            "                        instance=instance)",
            "        else:",
            "            vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "            self._acquire_bootlock(vm_ref)",
            "",
            "    def restore(self, instance):",
            "        \"\"\"Restore the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._release_bootlock(vm_ref)",
            "        self._start(instance, vm_ref)",
            "",
            "    def power_off(self, instance):",
            "        \"\"\"Power off the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "",
            "    def power_on(self, instance):",
            "        \"\"\"Power on the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._start(instance, vm_ref)",
            "",
            "    def _cancel_stale_tasks(self, timeout, task):",
            "        \"\"\"Cancel the given tasks that are older than the given timeout.\"\"\"",
            "        task_refs = self._session.call_xenapi(\"task.get_by_name_label\", task)",
            "        for task_ref in task_refs:",
            "            task_rec = self._session.call_xenapi(\"task.get_record\", task_ref)",
            "            task_created = timeutils.parse_strtime(task_rec[\"created\"].value,",
            "                                                   \"%Y%m%dT%H:%M:%SZ\")",
            "",
            "            if timeutils.is_older_than(task_created, timeout):",
            "                self._session.call_xenapi(\"task.cancel\", task_ref)",
            "",
            "    def poll_rebooting_instances(self, timeout, instances):",
            "        \"\"\"Look for expirable rebooting instances.",
            "",
            "            - issue a \"hard\" reboot to any instance that has been stuck in a",
            "              reboot state for >= the given timeout",
            "        \"\"\"",
            "        # NOTE(jk0): All existing clean_reboot tasks must be cancelled before",
            "        # we can kick off the hard_reboot tasks.",
            "        self._cancel_stale_tasks(timeout, 'VM.clean_reboot')",
            "",
            "        ctxt = nova_context.get_admin_context()",
            "",
            "        instances_info = dict(instance_count=len(instances),",
            "                timeout=timeout)",
            "",
            "        if instances_info[\"instance_count\"] > 0:",
            "            LOG.info(_(\"Found %(instance_count)d hung reboots \"",
            "                       \"older than %(timeout)d seconds\") % instances_info)",
            "",
            "        for instance in instances:",
            "            LOG.info(_(\"Automatically hard rebooting\"), instance=instance)",
            "            self.compute_api.reboot(ctxt, instance, \"HARD\")",
            "",
            "    def get_info(self, instance, vm_ref=None):",
            "        \"\"\"Return data about VM instance.\"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_utils.compile_info(vm_rec)",
            "",
            "    def get_diagnostics(self, instance):",
            "        \"\"\"Return data about VM diagnostics.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_utils.compile_diagnostics(vm_rec)",
            "",
            "    def _get_vif_device_map(self, vm_rec):",
            "        vif_map = {}",
            "        for vif in [self._session.call_xenapi(\"VIF.get_record\", vrec)",
            "                    for vrec in vm_rec['VIFs']]:",
            "            vif_map[vif['device']] = vif['MAC']",
            "        return vif_map",
            "",
            "    def get_all_bw_counters(self):",
            "        \"\"\"Return running bandwidth counter for each interface on each",
            "           running VM.",
            "        \"\"\"",
            "        counters = vm_utils.fetch_bandwidth(self._session)",
            "        bw = {}",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            vif_map = self._get_vif_device_map(vm_rec)",
            "            name = vm_rec['name_label']",
            "            if 'nova_uuid' not in vm_rec['other_config']:",
            "                continue",
            "            dom = vm_rec.get('domid')",
            "            if dom is None or dom not in counters:",
            "                continue",
            "            vifs_bw = bw.setdefault(name, {})",
            "            for vif_num, vif_data in counters[dom].iteritems():",
            "                mac = vif_map[vif_num]",
            "                vif_data['mac_address'] = mac",
            "                vifs_bw[mac] = vif_data",
            "        return bw",
            "",
            "    def get_console_output(self, instance):",
            "        \"\"\"Return last few lines of instance console.\"\"\"",
            "        dom_id = self._get_dom_id(instance, check_rescue=True)",
            "",
            "        try:",
            "            raw_console_data = self._session.call_plugin('console',",
            "                    'get_console_log', {'dom_id': dom_id})",
            "        except self._session.XenAPI.Failure as exc:",
            "            LOG.exception(exc)",
            "            msg = _(\"Guest does not have a console available\")",
            "            raise exception.NovaException(msg)",
            "",
            "        return zlib.decompress(base64.b64decode(raw_console_data))",
            "",
            "    def get_vnc_console(self, instance):",
            "        \"\"\"Return connection info for a vnc console.\"\"\"",
            "        if instance['vm_state'] == vm_states.RESCUED:",
            "            name = '%s-rescue' % instance['name']",
            "            vm_ref = vm_utils.lookup(self._session, name)",
            "            if vm_ref is None:",
            "                # The rescue instance might not be ready at this point.",
            "                raise exception.InstanceNotReady(instance_id=instance['uuid'])",
            "        else:",
            "            vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "            if vm_ref is None:",
            "                # The compute manager expects InstanceNotFound for this case.",
            "                raise exception.InstanceNotFound(instance_id=instance['uuid'])",
            "",
            "        session_id = self._session.get_session_id()",
            "        path = \"/console?ref=%s&session_id=%s\" % (str(vm_ref), session_id)",
            "",
            "        # NOTE: XS5.6sp2+ use http over port 80 for xenapi com",
            "        return {'host': CONF.vncserver_proxyclient_address, 'port': 80,",
            "                'internal_access_path': path}",
            "",
            "    def _vif_xenstore_data(self, vif):",
            "        \"\"\"convert a network info vif to injectable instance data.\"\"\"",
            "",
            "        def get_ip(ip):",
            "            if not ip:",
            "                return None",
            "            return ip['address']",
            "",
            "        def fixed_ip_dict(ip, subnet):",
            "            if ip['version'] == 4:",
            "                netmask = str(subnet.as_netaddr().netmask)",
            "            else:",
            "                netmask = subnet.as_netaddr()._prefixlen",
            "",
            "            return {'ip': ip['address'],",
            "                    'enabled': '1',",
            "                    'netmask': netmask,",
            "                    'gateway': get_ip(subnet['gateway'])}",
            "",
            "        def convert_route(route):",
            "            return {'route': str(netaddr.IPNetwork(route['cidr']).network),",
            "                    'netmask': str(netaddr.IPNetwork(route['cidr']).netmask),",
            "                    'gateway': get_ip(route['gateway'])}",
            "",
            "        network = vif['network']",
            "        v4_subnets = [subnet for subnet in network['subnets']",
            "                             if subnet['version'] == 4]",
            "        v6_subnets = [subnet for subnet in network['subnets']",
            "                             if subnet['version'] == 6]",
            "",
            "        # NOTE(tr3buchet): routes and DNS come from all subnets",
            "        routes = [convert_route(route) for subnet in network['subnets']",
            "                                       for route in subnet['routes']]",
            "        dns = [get_ip(ip) for subnet in network['subnets']",
            "                          for ip in subnet['dns']]",
            "",
            "        info_dict = {'label': network['label'],",
            "                     'mac': vif['address']}",
            "",
            "        if v4_subnets:",
            "            # NOTE(tr3buchet): gateway and broadcast from first subnet",
            "            #                  primary IP will be from first subnet",
            "            #                  subnets are generally unordered :(",
            "            info_dict['gateway'] = get_ip(v4_subnets[0]['gateway'])",
            "            info_dict['broadcast'] = str(v4_subnets[0].as_netaddr().broadcast)",
            "            info_dict['ips'] = [fixed_ip_dict(ip, subnet)",
            "                                for subnet in v4_subnets",
            "                                for ip in subnet['ips']]",
            "        if v6_subnets:",
            "            # NOTE(tr3buchet): gateway from first subnet",
            "            #                  primary IP will be from first subnet",
            "            #                  subnets are generally unordered :(",
            "            info_dict['gateway_v6'] = get_ip(v6_subnets[0]['gateway'])",
            "            info_dict['ip6s'] = [fixed_ip_dict(ip, subnet)",
            "                                 for subnet in v6_subnets",
            "                                 for ip in subnet['ips']]",
            "        if routes:",
            "            info_dict['routes'] = routes",
            "",
            "        if dns:",
            "            info_dict['dns'] = list(set(dns))",
            "",
            "        return info_dict",
            "",
            "    def inject_network_info(self, instance, network_info, vm_ref=None):",
            "        \"\"\"",
            "        Generate the network info and make calls to place it into the",
            "        xenstore and the xenstore param list.",
            "        vm_ref can be passed in because it will sometimes be different than",
            "        what vm_utils.lookup(session, instance['name']) will find (ex: rescue)",
            "        \"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        LOG.debug(_(\"Injecting network info to xenstore\"), instance=instance)",
            "",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def update_nwinfo():",
            "            for vif in network_info:",
            "                xs_data = self._vif_xenstore_data(vif)",
            "                location = ('vm-data/networking/%s' %",
            "                            vif['address'].replace(':', ''))",
            "                self._add_to_param_xenstore(vm_ref,",
            "                                            location,",
            "                                            jsonutils.dumps(xs_data))",
            "                try:",
            "                    self._write_to_xenstore(instance, location, xs_data,",
            "                                            vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "        update_nwinfo()",
            "",
            "    def _create_vifs(self, instance, vm_ref, network_info):",
            "        \"\"\"Creates vifs for an instance.\"\"\"",
            "",
            "        LOG.debug(_(\"Creating vifs\"), instance=instance)",
            "",
            "        # this function raises if vm_ref is not a vm_opaque_ref",
            "        self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "",
            "        for device, vif in enumerate(network_info):",
            "            vif_rec = self.vif_driver.plug(instance, vif,",
            "                                           vm_ref=vm_ref, device=device)",
            "            network_ref = vif_rec['network']",
            "            LOG.debug(_('Creating VIF for network %s'),",
            "                      network_ref, instance=instance)",
            "            vif_ref = self._session.call_xenapi('VIF.create', vif_rec)",
            "            LOG.debug(_('Created VIF %(vif_ref)s, network %(network_ref)s'),",
            "                      {'vif_ref': vif_ref, 'network_ref': network_ref},",
            "                      instance=instance)",
            "",
            "    def plug_vifs(self, instance, network_info):",
            "        \"\"\"Set up VIF networking on the host.\"\"\"",
            "        for device, vif in enumerate(network_info):",
            "            self.vif_driver.plug(instance, vif, device=device)",
            "",
            "    def unplug_vifs(self, instance, network_info):",
            "        if network_info:",
            "            for vif in network_info:",
            "                self.vif_driver.unplug(instance, vif)",
            "",
            "    def reset_network(self, instance):",
            "        \"\"\"Calls resetnetwork method in agent.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.resetnetwork()",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    def _inject_hostname(self, instance, vm_ref, rescue):",
            "        \"\"\"Inject the hostname of the instance into the xenstore.\"\"\"",
            "        hostname = instance['hostname']",
            "        if rescue:",
            "            hostname = 'RESCUE-%s' % hostname",
            "",
            "        if instance['os_type'] == \"windows\":",
            "            # NOTE(jk0): Windows hostnames can only be <= 15 chars.",
            "            hostname = hostname[:15]",
            "",
            "        LOG.debug(_(\"Injecting hostname to xenstore\"), instance=instance)",
            "        self._add_to_param_xenstore(vm_ref, 'vm-data/hostname', hostname)",
            "",
            "    def _remove_hostname(self, instance, vm_ref):",
            "        LOG.debug(_(\"Removing hostname from xenstore\"), instance=instance)",
            "        self._remove_from_param_xenstore(vm_ref, 'vm-data/hostname')",
            "",
            "    def _write_to_xenstore(self, instance, path, value, vm_ref=None):",
            "        \"\"\"",
            "        Writes the passed value to the xenstore record for the given VM",
            "        at the specified location. A XenAPIPlugin.PluginError will be raised",
            "        if any error is encountered in the write process.",
            "        \"\"\"",
            "        return self._make_plugin_call('xenstore.py', 'write_record', instance,",
            "                                      vm_ref=vm_ref, path=path,",
            "                                      value=jsonutils.dumps(value))",
            "",
            "    def _delete_from_xenstore(self, instance, path, vm_ref=None):",
            "        \"\"\"",
            "        Deletes the value from the xenstore record for the given VM at",
            "        the specified location.  A XenAPIPlugin.PluginError will be",
            "        raised if any error is encountered in the delete process.",
            "        \"\"\"",
            "        return self._make_plugin_call('xenstore.py', 'delete_record', instance,",
            "                                      vm_ref=vm_ref, path=path)",
            "",
            "    def _make_plugin_call(self, plugin, method, instance=None, vm_ref=None,",
            "                          **addl_args):",
            "        \"\"\"",
            "        Abstracts out the process of calling a method of a xenapi plugin.",
            "        Any errors raised by the plugin will in turn raise a RuntimeError here.",
            "        \"\"\"",
            "        args = {}",
            "        if instance or vm_ref:",
            "            args['dom_id'] = self._get_dom_id(instance, vm_ref)",
            "        args.update(addl_args)",
            "        try:",
            "            return self._session.call_plugin(plugin, method, args)",
            "        except self._session.XenAPI.Failure as e:",
            "            err_msg = e.details[-1].splitlines()[-1]",
            "            if 'TIMEOUT:' in err_msg:",
            "                LOG.error(_('TIMEOUT: The call to %(method)s timed out. '",
            "                            'args=%(args)r'),",
            "                          {'method': method, 'args': args}, instance=instance)",
            "                return {'returncode': 'timeout', 'message': err_msg}",
            "            elif 'NOT IMPLEMENTED:' in err_msg:",
            "                LOG.error(_('NOT IMPLEMENTED: The call to %(method)s is not'",
            "                            ' supported by the agent. args=%(args)r'),",
            "                          {'method': method, 'args': args}, instance=instance)",
            "                return {'returncode': 'notimplemented', 'message': err_msg}",
            "            else:",
            "                LOG.error(_('The call to %(method)s returned an error: %(e)s. '",
            "                            'args=%(args)r'),",
            "                          {'method': method, 'args': args, 'e': e},",
            "                          instance=instance)",
            "                return {'returncode': 'error', 'message': err_msg}",
            "",
            "    def _get_dom_id(self, instance=None, vm_ref=None, check_rescue=False):",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance, check_rescue)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_rec['domid']",
            "",
            "    def _add_to_param_xenstore(self, vm_ref, key, val):",
            "        \"\"\"",
            "        Takes a key/value pair and adds it to the xenstore parameter",
            "        record for the given vm instance. If the key exists in xenstore,",
            "        it is overwritten",
            "        \"\"\"",
            "        self._remove_from_param_xenstore(vm_ref, key)",
            "        self._session.call_xenapi('VM.add_to_xenstore_data', vm_ref, key, val)",
            "",
            "    def _remove_from_param_xenstore(self, vm_ref, key):",
            "        \"\"\"",
            "        Takes a single key and removes it from the xenstore parameter",
            "        record data for the given VM.",
            "        If the key doesn't exist, the request is ignored.",
            "        \"\"\"",
            "        self._session.call_xenapi('VM.remove_from_xenstore_data', vm_ref, key)",
            "",
            "    def refresh_security_group_rules(self, security_group_id):",
            "        \"\"\"recreates security group rules for every instance.\"\"\"",
            "        self.firewall_driver.refresh_security_group_rules(security_group_id)",
            "",
            "    def refresh_security_group_members(self, security_group_id):",
            "        \"\"\"recreates security group rules for every instance.\"\"\"",
            "        self.firewall_driver.refresh_security_group_members(security_group_id)",
            "",
            "    def refresh_instance_security_rules(self, instance):",
            "        \"\"\"recreates security group rules for specified instance.\"\"\"",
            "        self.firewall_driver.refresh_instance_security_rules(instance)",
            "",
            "    def refresh_provider_fw_rules(self):",
            "        self.firewall_driver.refresh_provider_fw_rules()",
            "",
            "    def unfilter_instance(self, instance_ref, network_info):",
            "        \"\"\"Removes filters for each VIF of the specified instance.\"\"\"",
            "        self.firewall_driver.unfilter_instance(instance_ref,",
            "                                               network_info=network_info)",
            "",
            "    def _get_host_uuid_from_aggregate(self, context, hostname):",
            "        current_aggregate = self._virtapi.aggregate_get_by_host(",
            "            context, CONF.host, key=pool_states.POOL_FLAG)[0]",
            "        if not current_aggregate:",
            "            raise exception.AggregateHostNotFound(host=CONF.host)",
            "        try:",
            "            return current_aggregate.metadetails[hostname]",
            "        except KeyError:",
            "            reason = _('Destination host:%s must be in the same '",
            "                       'aggregate as the source server') % hostname",
            "            raise exception.MigrationPreCheckError(reason=reason)",
            "",
            "    def _ensure_host_in_aggregate(self, context, hostname):",
            "        self._get_host_uuid_from_aggregate(context, hostname)",
            "",
            "    def _get_host_opaque_ref(self, context, hostname):",
            "        host_uuid = self._get_host_uuid_from_aggregate(context, hostname)",
            "        return self._session.call_xenapi(\"host.get_by_uuid\", host_uuid)",
            "",
            "    def _migrate_receive(self, ctxt):",
            "        destref = self._session.get_xenapi_host()",
            "        # Get the network to for migrate.",
            "        # This is the one associated with the pif marked management. From cli:",
            "        # uuid=`xe pif-list --minimal management=true`",
            "        # xe pif-param-get param-name=network-uuid uuid=$uuid",
            "        expr = 'field \"management\" = \"true\"'",
            "        pifs = self._session.call_xenapi('PIF.get_all_records_where',",
            "                                         expr)",
            "        if len(pifs) != 1:",
            "            msg = _('No suitable network for migrate')",
            "            raise exception.MigrationPreCheckError(reason=msg)",
            "",
            "        nwref = pifs[pifs.keys()[0]]['network']",
            "        try:",
            "            options = {}",
            "            migrate_data = self._session.call_xenapi(\"host.migrate_receive\",",
            "                                                     destref,",
            "                                                     nwref,",
            "                                                     options)",
            "        except self._session.XenAPI.Failure as exc:",
            "            LOG.exception(exc)",
            "            msg = _('Migrate Receive failed')",
            "            raise exception.MigrationPreCheckError(reason=msg)",
            "        return migrate_data",
            "",
            "    def _get_iscsi_srs(self, ctxt, instance_ref):",
            "        vm_ref = self._get_vm_opaque_ref(instance_ref)",
            "        vbd_refs = self._session.call_xenapi(\"VM.get_VBDs\", vm_ref)",
            "",
            "        iscsi_srs = []",
            "",
            "        for vbd_ref in vbd_refs:",
            "            vdi_ref = self._session.call_xenapi(\"VBD.get_VDI\", vbd_ref)",
            "            # Check if it's on an iSCSI SR",
            "            sr_ref = self._session.call_xenapi(\"VDI.get_SR\", vdi_ref)",
            "            if self._session.call_xenapi(\"SR.get_type\", sr_ref) == 'iscsi':",
            "                iscsi_srs.append(sr_ref)",
            "",
            "        return iscsi_srs",
            "",
            "    def check_can_live_migrate_destination(self, ctxt, instance_ref,",
            "                                           block_migration=False,",
            "                                           disk_over_commit=False):",
            "        \"\"\"Check if it is possible to execute live migration.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param block_migration: if true, prepare for block migration",
            "        :param disk_over_commit: if true, allow disk over commit",
            "",
            "        \"\"\"",
            "        dest_check_data = {}",
            "        if block_migration:",
            "            migrate_send_data = self._migrate_receive(ctxt)",
            "            destination_sr_ref = vm_utils.safe_find_sr(self._session)",
            "            dest_check_data.update(",
            "                {\"block_migration\": block_migration,",
            "                 \"migrate_data\": {\"migrate_send_data\": migrate_send_data,",
            "                                  \"destination_sr_ref\": destination_sr_ref}})",
            "        else:",
            "            src = instance_ref['host']",
            "            self._ensure_host_in_aggregate(ctxt, src)",
            "            # TODO(johngarbutt) we currently assume",
            "            # instance is on a SR shared with other destination",
            "            # block migration work will be able to resolve this",
            "        return dest_check_data",
            "",
            "    def _is_xsm_sr_check_relaxed(self):",
            "        try:",
            "            return self.cached_xsm_sr_relaxed",
            "        except AttributeError:",
            "            config_value = None",
            "            try:",
            "                config_value = self._make_plugin_call('config_file',",
            "                                                      'get_val',",
            "                                                      key='relax-xsm-sr-check')",
            "            except Exception as exc:",
            "                LOG.exception(exc)",
            "            self.cached_xsm_sr_relaxed = config_value == \"true\"",
            "            return self.cached_xsm_sr_relaxed",
            "",
            "    def check_can_live_migrate_source(self, ctxt, instance_ref,",
            "                                      dest_check_data):",
            "        \"\"\"Check if it's possible to execute live migration on the source side.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param dest_check_data: data returned by the check on the",
            "                                destination, includes block_migration flag",
            "",
            "        \"\"\"",
            "        if len(self._get_iscsi_srs(ctxt, instance_ref)) > 0:",
            "            # XAPI must support the relaxed SR check for live migrating with",
            "            # iSCSI VBDs",
            "            if not self._is_xsm_sr_check_relaxed():",
            "                raise exception.MigrationError(_('XAPI supporting '",
            "                                'relax-xsm-sr-check=true requried'))",
            "",
            "        if 'migrate_data' in dest_check_data:",
            "            vm_ref = self._get_vm_opaque_ref(instance_ref)",
            "            migrate_data = dest_check_data['migrate_data']",
            "            try:",
            "                self._call_live_migrate_command(",
            "                    \"VM.assert_can_migrate\", vm_ref, migrate_data)",
            "            except self._session.XenAPI.Failure as exc:",
            "                LOG.exception(exc)",
            "                msg = _('VM.assert_can_migrate failed')",
            "                raise exception.MigrationPreCheckError(reason=msg)",
            "        return dest_check_data",
            "",
            "    def _generate_vdi_map(self, destination_sr_ref, vm_ref, sr_ref=None):",
            "        \"\"\"generate a vdi_map for _call_live_migrate_command.\"\"\"",
            "        if sr_ref is None:",
            "            sr_ref = vm_utils.safe_find_sr(self._session)",
            "        vm_vdis = vm_utils.get_instance_vdis_for_sr(self._session,",
            "                                                    vm_ref, sr_ref)",
            "        return dict((vdi, destination_sr_ref) for vdi in vm_vdis)",
            "",
            "    def _call_live_migrate_command(self, command_name, vm_ref, migrate_data):",
            "        \"\"\"unpack xapi specific parameters, and call a live migrate command.\"\"\"",
            "        destination_sr_ref = migrate_data['destination_sr_ref']",
            "        migrate_send_data = migrate_data['migrate_send_data']",
            "",
            "        vdi_map = self._generate_vdi_map(destination_sr_ref, vm_ref)",
            "",
            "        # Add destination SR refs for all of the VDIs that we created",
            "        # as part of the pre migration callback",
            "        if 'pre_live_migration_result' in migrate_data:",
            "            pre_migrate_data = migrate_data['pre_live_migration_result']",
            "            sr_uuid_map = pre_migrate_data.get('sr_uuid_map', [])",
            "            for sr_uuid in sr_uuid_map:",
            "                # Source and destination SRs have the same UUID, so get the",
            "                # reference for the local SR",
            "                sr_ref = self._session.call_xenapi(\"SR.get_by_uuid\", sr_uuid)",
            "                vdi_map.update(",
            "                    self._generate_vdi_map(",
            "                        sr_uuid_map[sr_uuid], vm_ref, sr_ref))",
            "        vif_map = {}",
            "        options = {}",
            "        self._session.call_xenapi(command_name, vm_ref,",
            "                                  migrate_send_data, True,",
            "                                  vdi_map, vif_map, options)",
            "",
            "    def live_migrate(self, context, instance, destination_hostname,",
            "                     post_method, recover_method, block_migration,",
            "                     migrate_data=None):",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            if block_migration:",
            "                if not migrate_data:",
            "                    raise exception.InvalidParameterValue('Block Migration '",
            "                                    'requires migrate data from destination')",
            "",
            "                iscsi_srs = self._get_iscsi_srs(context, instance)",
            "                try:",
            "                    self._call_live_migrate_command(",
            "                        \"VM.migrate_send\", vm_ref, migrate_data)",
            "                except self._session.XenAPI.Failure as exc:",
            "                    LOG.exception(exc)",
            "                    raise exception.MigrationError(_('Migrate Send failed'))",
            "",
            "                # Tidy up the iSCSI SRs",
            "                for sr_ref in iscsi_srs:",
            "                    volume_utils.forget_sr(self._session, sr_ref)",
            "            else:",
            "                host_ref = self._get_host_opaque_ref(context,",
            "                                                     destination_hostname)",
            "                self._session.call_xenapi(\"VM.pool_migrate\", vm_ref,",
            "                                          host_ref, {\"live\": \"true\"})",
            "            post_method(context, instance, destination_hostname,",
            "                        block_migration)",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                recover_method(context, instance, destination_hostname,",
            "                               block_migration)",
            "",
            "    def get_per_instance_usage(self):",
            "        \"\"\"Get usage info about each active instance.\"\"\"",
            "        usage = {}",
            "",
            "        def _is_active(vm_rec):",
            "            power_state = vm_rec['power_state'].lower()",
            "            return power_state in ['running', 'paused']",
            "",
            "        def _get_uuid(vm_rec):",
            "            other_config = vm_rec['other_config']",
            "            return other_config.get('nova_uuid', None)",
            "",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            uuid = _get_uuid(vm_rec)",
            "",
            "            if _is_active(vm_rec) and uuid is not None:",
            "                memory_mb = int(vm_rec['memory_static_max']) / 1024 / 1024",
            "                usage[uuid] = {'memory_mb': memory_mb, 'uuid': uuid}",
            "",
            "        return usage",
            "",
            "    def attach_block_device_volumes(self, block_device_info):",
            "        sr_uuid_map = {}",
            "        try:",
            "            if block_device_info is not None:",
            "                for block_device_map in block_device_info[",
            "                                                'block_device_mapping']:",
            "                    sr_uuid, _ = self._volumeops.attach_volume(",
            "                        block_device_map['connection_info'],",
            "                        None,",
            "                        block_device_map['mount_device'],",
            "                        hotplug=False)",
            "",
            "                    sr_ref = self._session.call_xenapi('SR.get_by_uuid',",
            "                                                       sr_uuid)",
            "                    sr_uuid_map[sr_uuid] = sr_ref",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                # Disconnect the volumes we just connected",
            "                for sr in sr_uuid_map:",
            "                    volume_utils.forget_sr(self._session, sr_uuid_map[sr_ref])",
            "",
            "        return sr_uuid_map"
        ],
        "afterPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright (c) 2010 Citrix Systems, Inc.",
            "# Copyright 2010 OpenStack Foundation",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "Management class for VM-related functions (spawn, reboot, etc).",
            "\"\"\"",
            "",
            "import base64",
            "import functools",
            "import itertools",
            "import time",
            "import zlib",
            "",
            "from eventlet import greenthread",
            "import netaddr",
            "from oslo.config import cfg",
            "",
            "from nova import block_device",
            "from nova import compute",
            "from nova.compute import flavors",
            "from nova.compute import power_state",
            "from nova.compute import task_states",
            "from nova.compute import vm_mode",
            "from nova.compute import vm_states",
            "from nova import context as nova_context",
            "from nova import exception",
            "from nova.openstack.common import excutils",
            "from nova.openstack.common.gettextutils import _",
            "from nova.openstack.common import importutils",
            "from nova.openstack.common import jsonutils",
            "from nova.openstack.common import log as logging",
            "from nova.openstack.common import strutils",
            "from nova.openstack.common import timeutils",
            "from nova import utils",
            "from nova.virt import configdrive",
            "from nova.virt import driver as virt_driver",
            "from nova.virt import firewall",
            "from nova.virt.xenapi import agent as xapi_agent",
            "from nova.virt.xenapi import pool_states",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import volume_utils",
            "from nova.virt.xenapi import volumeops",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "xenapi_vmops_opts = [",
            "    cfg.IntOpt('xenapi_running_timeout',",
            "               default=60,",
            "               help='number of seconds to wait for instance '",
            "                    'to go to running state'),",
            "    cfg.StrOpt('xenapi_vif_driver',",
            "               default='nova.virt.xenapi.vif.XenAPIBridgeDriver',",
            "               help='The XenAPI VIF driver using XenServer Network APIs.'),",
            "    cfg.StrOpt('xenapi_image_upload_handler',",
            "                default='nova.virt.xenapi.image.glance.GlanceStore',",
            "                help='Dom0 plugin driver used to handle image uploads.'),",
            "    ]",
            "",
            "CONF = cfg.CONF",
            "CONF.register_opts(xenapi_vmops_opts)",
            "CONF.import_opt('host', 'nova.netconf')",
            "CONF.import_opt('vncserver_proxyclient_address', 'nova.vnc')",
            "",
            "DEFAULT_FIREWALL_DRIVER = \"%s.%s\" % (",
            "    firewall.__name__,",
            "    firewall.IptablesFirewallDriver.__name__)",
            "",
            "RESIZE_TOTAL_STEPS = 5",
            "",
            "DEVICE_ROOT = '0'",
            "DEVICE_RESCUE = '1'",
            "DEVICE_SWAP = '2'",
            "DEVICE_CONFIGDRIVE = '3'",
            "# Note(johngarbutt) HVM guests only support four devices",
            "# until the PV tools activate, when others before available",
            "# As such, ephemeral disk only available once PV tools load",
            "# Note(johngarbutt) When very large ephemeral storage is required,",
            "# multiple disks may be added. In this case the device id below",
            "# is the used for the first disk. The second disk will be given",
            "# next device id, i.e. 5, and so on, until enough space is added.",
            "DEVICE_EPHEMERAL = '4'",
            "# Note(johngarbutt) Currently don't support ISO boot during rescue",
            "# and we must have the ISO visible before the PV drivers start",
            "DEVICE_CD = '1'",
            "",
            "",
            "def cmp_version(a, b):",
            "    \"\"\"Compare two version strings (eg 0.0.1.10 > 0.0.1.9).\"\"\"",
            "    a = a.split('.')",
            "    b = b.split('.')",
            "",
            "    # Compare each individual portion of both version strings",
            "    for va, vb in zip(a, b):",
            "        ret = int(va) - int(vb)",
            "        if ret:",
            "            return ret",
            "",
            "    # Fallback to comparing length last",
            "    return len(a) - len(b)",
            "",
            "",
            "def make_step_decorator(context, instance, update_instance_progress):",
            "    \"\"\"Factory to create a decorator that records instance progress as a series",
            "    of discrete steps.",
            "",
            "    Each time the decorator is invoked we bump the total-step-count, so after::",
            "",
            "        @step",
            "        def step1():",
            "            ...",
            "",
            "        @step",
            "        def step2():",
            "            ...",
            "",
            "    we have a total-step-count of 2.",
            "",
            "    Each time the step-function (not the step-decorator!) is invoked, we bump",
            "    the current-step-count by 1, so after::",
            "",
            "        step1()",
            "",
            "    the current-step-count would be 1 giving a progress of ``1 / 2 *",
            "    100`` or 50%.",
            "    \"\"\"",
            "    step_info = dict(total=0, current=0)",
            "",
            "    def bump_progress():",
            "        step_info['current'] += 1",
            "        update_instance_progress(context, instance,",
            "                                 step_info['current'], step_info['total'])",
            "",
            "    def step_decorator(f):",
            "        step_info['total'] += 1",
            "",
            "        @functools.wraps(f)",
            "        def inner(*args, **kwargs):",
            "            rv = f(*args, **kwargs)",
            "            bump_progress()",
            "            return rv",
            "",
            "        return inner",
            "",
            "    return step_decorator",
            "",
            "",
            "class VMOps(object):",
            "    \"\"\"",
            "    Management class for VM-related tasks",
            "    \"\"\"",
            "    def __init__(self, session, virtapi):",
            "        self.compute_api = compute.API()",
            "        self._session = session",
            "        self._virtapi = virtapi",
            "        self._volumeops = volumeops.VolumeOps(self._session)",
            "        self.firewall_driver = firewall.load_driver(",
            "            DEFAULT_FIREWALL_DRIVER,",
            "            self._virtapi,",
            "            xenapi_session=self._session)",
            "        vif_impl = importutils.import_class(CONF.xenapi_vif_driver)",
            "        self.vif_driver = vif_impl(xenapi_session=self._session)",
            "        self.default_root_dev = '/dev/sda'",
            "",
            "        LOG.debug(_(\"Importing image upload handler: %s\"),",
            "                  CONF.xenapi_image_upload_handler)",
            "        self.image_upload_handler = importutils.import_object(",
            "                                CONF.xenapi_image_upload_handler)",
            "",
            "    def agent_enabled(self, instance):",
            "        if CONF.xenapi_disable_agent:",
            "            return False",
            "",
            "        return xapi_agent.should_use_agent(instance)",
            "",
            "    def _get_agent(self, instance, vm_ref):",
            "        if self.agent_enabled(instance):",
            "            return xapi_agent.XenAPIBasedAgent(self._session, self._virtapi,",
            "                                               instance, vm_ref)",
            "        raise exception.NovaException(_(\"Error: Agent is disabled\"))",
            "",
            "    def instance_exists(self, name_label):",
            "        return vm_utils.lookup(self._session, name_label) is not None",
            "",
            "    def list_instances(self):",
            "        \"\"\"List VM instances.\"\"\"",
            "        # TODO(justinsb): Should we just always use the details method?",
            "        #  Seems to be the same number of API calls..",
            "        name_labels = []",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            name_labels.append(vm_rec[\"name_label\"])",
            "",
            "        return name_labels",
            "",
            "    def list_instance_uuids(self):",
            "        \"\"\"Get the list of nova instance uuids for VMs found on the",
            "        hypervisor.",
            "        \"\"\"",
            "        nova_uuids = []",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            other_config = vm_rec['other_config']",
            "            nova_uuid = other_config.get('nova_uuid')",
            "            if nova_uuid:",
            "                nova_uuids.append(nova_uuid)",
            "        return nova_uuids",
            "",
            "    def confirm_migration(self, migration, instance, network_info):",
            "        self._destroy_orig_vm(instance, network_info)",
            "",
            "    def _destroy_orig_vm(self, instance, network_info):",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "        return self._destroy(instance, vm_ref, network_info=network_info)",
            "",
            "    def _attach_mapped_block_devices(self, instance, block_device_info):",
            "        # We are attaching these volumes before start (no hotplugging)",
            "        # because some guests (windows) don't load PV drivers quickly",
            "        block_device_mapping = virt_driver.block_device_info_get_mapping(",
            "                block_device_info)",
            "        for vol in block_device_mapping:",
            "            connection_info = vol['connection_info']",
            "            mount_device = vol['mount_device'].rpartition(\"/\")[2]",
            "            self._volumeops.attach_volume(connection_info,",
            "                                          instance['name'],",
            "                                          mount_device,",
            "                                          hotplug=False)",
            "",
            "    def finish_revert_migration(self, instance, block_device_info=None,",
            "                                power_on=True):",
            "        self._restore_orig_vm_and_cleanup_orphan(instance, block_device_info,",
            "                                                 power_on)",
            "",
            "    def _restore_orig_vm_and_cleanup_orphan(self, instance,",
            "                                            block_device_info, power_on=True):",
            "        # NOTE(sirp): the original vm was suffixed with '-orig'; find it using",
            "        # the old suffix, remove the suffix, then power it back on.",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "",
            "        # NOTE(danms): if we're reverting migration in the failure case,",
            "        # make sure we don't have a conflicting vm still running here,",
            "        # as might be the case in a failed migrate-to-same-host situation",
            "        new_ref = vm_utils.lookup(self._session, instance['name'])",
            "        if vm_ref is not None:",
            "            if new_ref is not None:",
            "                self._destroy(instance, new_ref)",
            "            # Remove the '-orig' suffix (which was added in case the",
            "            # resized VM ends up on the source host, common during",
            "            # testing)",
            "            name_label = instance['name']",
            "            vm_utils.set_vm_name_label(self._session, vm_ref, name_label)",
            "            self._attach_mapped_block_devices(instance, block_device_info)",
            "        elif new_ref is not None:",
            "            # We crashed before the -orig backup was made",
            "            vm_ref = new_ref",
            "",
            "        if power_on:",
            "            self._start(instance, vm_ref)",
            "",
            "    def finish_migration(self, context, migration, instance, disk_info,",
            "                         network_info, image_meta, resize_instance,",
            "                         block_device_info=None, power_on=True):",
            "",
            "        def null_step_decorator(f):",
            "            return f",
            "",
            "        def create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                              name_label):",
            "            #TODO(johngarbutt) clean up the move_disks if this is not run",
            "            root_vdi = vm_utils.move_disks(self._session, instance, disk_info)",
            "",
            "            def undo_create_disks():",
            "                vm_utils.safe_destroy_vdis(self._session, [root_vdi['ref']])",
            "",
            "            undo_mgr.undo_with(undo_create_disks)",
            "            return {'root': root_vdi}",
            "",
            "        def completed_callback():",
            "            self._update_instance_progress(context, instance,",
            "                                           step=5,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "        self._spawn(context, instance, image_meta, null_step_decorator,",
            "                    create_disks_step, first_boot=False, injected_files=None,",
            "                    admin_password=None, network_info=network_info,",
            "                    block_device_info=block_device_info, name_label=None,",
            "                    rescue=False, power_on=power_on, resize=resize_instance,",
            "                    completed_callback=completed_callback)",
            "",
            "    def _start(self, instance, vm_ref=None, bad_volumes_callback=None):",
            "        \"\"\"Power on a VM instance.\"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        LOG.debug(_(\"Starting instance\"), instance=instance)",
            "",
            "        # Attached volumes that have become non-responsive will prevent a VM",
            "        # from starting, so scan for these before attempting to start",
            "        #",
            "        # In order to make sure this detach is consistent (virt, BDM, cinder),",
            "        # we only detach in the virt-layer if a callback is provided.",
            "        if bad_volumes_callback:",
            "            bad_devices = self._volumeops.find_bad_volumes(vm_ref)",
            "            for device_name in bad_devices:",
            "                self._volumeops.detach_volume(",
            "                        None, instance['name'], device_name)",
            "",
            "        self._session.call_xenapi('VM.start_on', vm_ref,",
            "                                  self._session.get_xenapi_host(),",
            "                                  False, False)",
            "",
            "        # Allow higher-layers a chance to detach bad-volumes as well (in order",
            "        # to cleanup BDM entries and detach in Cinder)",
            "        if bad_volumes_callback and bad_devices:",
            "            bad_volumes_callback(bad_devices)",
            "",
            "    def spawn(self, context, instance, image_meta, injected_files,",
            "              admin_password, network_info=None, block_device_info=None,",
            "              name_label=None, rescue=False):",
            "",
            "        if block_device_info:",
            "            LOG.debug(_(\"Block device information present: %s\")",
            "                      % block_device_info, instance=instance)",
            "        if block_device_info and not block_device_info['root_device_name']:",
            "            block_device_info['root_device_name'] = self.default_root_dev",
            "",
            "        step = make_step_decorator(context, instance,",
            "                                   self._update_instance_progress)",
            "",
            "        @step",
            "        def create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                              name_label):",
            "            vdis = vm_utils.get_vdis_for_instance(context, self._session,",
            "                        instance, name_label, image_meta.get('id'),",
            "                        disk_image_type, block_device_info=block_device_info)",
            "",
            "            def undo_create_disks():",
            "                vdi_refs = [vdi['ref'] for vdi in vdis.values()",
            "                        if not vdi.get('osvol')]",
            "                vm_utils.safe_destroy_vdis(self._session, vdi_refs)",
            "",
            "            undo_mgr.undo_with(undo_create_disks)",
            "            return vdis",
            "",
            "        self._spawn(context, instance, image_meta, step, create_disks_step,",
            "                    True, injected_files, admin_password,",
            "                    network_info, block_device_info, name_label, rescue)",
            "",
            "    def _spawn(self, context, instance, image_meta, step, create_disks_step,",
            "               first_boot, injected_files=None, admin_password=None,",
            "               network_info=None, block_device_info=None,",
            "               name_label=None, rescue=False, power_on=True, resize=True,",
            "               completed_callback=None):",
            "        if name_label is None:",
            "            name_label = instance['name']",
            "",
            "        self._ensure_instance_name_unique(name_label)",
            "        self._ensure_enough_free_mem(instance)",
            "",
            "        @step",
            "        def determine_disk_image_type_step(undo_mgr):",
            "            return vm_utils.determine_disk_image_type(image_meta)",
            "",
            "        @step",
            "        def create_kernel_ramdisk_step(undo_mgr):",
            "            kernel_file, ramdisk_file = vm_utils.create_kernel_and_ramdisk(",
            "                    context, self._session, instance, name_label)",
            "",
            "            def undo_create_kernel_ramdisk():",
            "                vm_utils.destroy_kernel_ramdisk(self._session, instance,",
            "                        kernel_file, ramdisk_file)",
            "",
            "            undo_mgr.undo_with(undo_create_kernel_ramdisk)",
            "            return kernel_file, ramdisk_file",
            "",
            "        @step",
            "        def create_vm_record_step(undo_mgr, vdis, disk_image_type,",
            "                kernel_file, ramdisk_file):",
            "            vm_ref = self._create_vm_record(context, instance, name_label,",
            "                    vdis, disk_image_type, kernel_file, ramdisk_file)",
            "",
            "            def undo_create_vm():",
            "                self._destroy(instance, vm_ref, network_info=network_info)",
            "",
            "            undo_mgr.undo_with(undo_create_vm)",
            "            return vm_ref",
            "",
            "        @step",
            "        def attach_disks_step(undo_mgr, vm_ref, vdis, disk_image_type):",
            "            try:",
            "                ipxe_boot = strutils.bool_from_string(",
            "                        image_meta['properties']['ipxe_boot'])",
            "            except KeyError:",
            "                ipxe_boot = False",
            "",
            "            if ipxe_boot:",
            "                if 'iso' in vdis:",
            "                    vm_utils.handle_ipxe_iso(",
            "                        self._session, instance, vdis['iso'], network_info)",
            "                else:",
            "                    LOG.warning(_('ipxe_boot is True but no ISO image found'),",
            "                                instance=instance)",
            "",
            "            root_vdi = vdis.get('root')",
            "            if root_vdi and resize:",
            "                self._resize_up_root_vdi(instance, root_vdi)",
            "",
            "            self._attach_disks(instance, vm_ref, name_label, vdis,",
            "                               disk_image_type, admin_password,",
            "                               injected_files)",
            "            if not first_boot:",
            "                self._attach_mapped_block_devices(instance,",
            "                                                  block_device_info)",
            "",
            "        if rescue:",
            "            # NOTE(johannes): Attach root disk to rescue VM now, before",
            "            # booting the VM, since we can't hotplug block devices",
            "            # on non-PV guests",
            "            @step",
            "            def attach_root_disk_step(undo_mgr, vm_ref):",
            "                vbd_ref = self._attach_orig_disk_for_rescue(instance, vm_ref)",
            "",
            "                def undo_attach_root_disk():",
            "                    # destroy the vbd in preparation to re-attach the VDI",
            "                    # to its original VM.  (does not delete VDI)",
            "                    vm_utils.destroy_vbd(self._session, vbd_ref)",
            "",
            "                undo_mgr.undo_with(undo_attach_root_disk)",
            "",
            "        @step",
            "        def inject_instance_data_step(undo_mgr, vm_ref, vdis):",
            "            self._inject_instance_metadata(instance, vm_ref)",
            "            self._inject_auto_disk_config(instance, vm_ref)",
            "            if first_boot:",
            "                self._inject_hostname(instance, vm_ref, rescue)",
            "            self._file_inject_vm_settings(instance, vm_ref, vdis, network_info)",
            "            self.inject_network_info(instance, network_info, vm_ref)",
            "",
            "        @step",
            "        def setup_network_step(undo_mgr, vm_ref):",
            "            self._create_vifs(instance, vm_ref, network_info)",
            "            self._prepare_instance_filter(instance, network_info)",
            "",
            "        @step",
            "        def boot_instance_step(undo_mgr, vm_ref):",
            "            if power_on:",
            "                self._start(instance, vm_ref)",
            "                self._wait_for_instance_to_start(instance, vm_ref)",
            "",
            "        @step",
            "        def configure_booted_instance_step(undo_mgr, vm_ref):",
            "            if first_boot:",
            "                self._configure_new_instance_with_agent(instance, vm_ref,",
            "                        injected_files, admin_password)",
            "                self._remove_hostname(instance, vm_ref)",
            "",
            "        @step",
            "        def apply_security_group_filters_step(undo_mgr):",
            "            self.firewall_driver.apply_instance_filter(instance, network_info)",
            "",
            "        undo_mgr = utils.UndoManager()",
            "        try:",
            "            # NOTE(sirp): The create_disks() step will potentially take a",
            "            # *very* long time to complete since it has to fetch the image",
            "            # over the network and images can be several gigs in size. To",
            "            # avoid progress remaining at 0% for too long, make sure the",
            "            # first step is something that completes rather quickly.",
            "            disk_image_type = determine_disk_image_type_step(undo_mgr)",
            "",
            "            vdis = create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                                     name_label)",
            "            kernel_file, ramdisk_file = create_kernel_ramdisk_step(undo_mgr)",
            "",
            "            vm_ref = create_vm_record_step(undo_mgr, vdis, disk_image_type,",
            "                    kernel_file, ramdisk_file)",
            "            attach_disks_step(undo_mgr, vm_ref, vdis, disk_image_type)",
            "",
            "            inject_instance_data_step(undo_mgr, vm_ref, vdis)",
            "            setup_network_step(undo_mgr, vm_ref)",
            "",
            "            if rescue:",
            "                attach_root_disk_step(undo_mgr, vm_ref)",
            "",
            "            boot_instance_step(undo_mgr, vm_ref)",
            "",
            "            configure_booted_instance_step(undo_mgr, vm_ref)",
            "            apply_security_group_filters_step(undo_mgr)",
            "",
            "            if completed_callback:",
            "                completed_callback()",
            "        except Exception:",
            "            msg = _(\"Failed to spawn, rolling back\")",
            "            undo_mgr.rollback_and_reraise(msg=msg, instance=instance)",
            "",
            "    def _attach_orig_disk_for_rescue(self, instance, vm_ref):",
            "        orig_vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "        vdi_ref = self._find_root_vdi_ref(orig_vm_ref)",
            "        return vm_utils.create_vbd(self._session, vm_ref, vdi_ref,",
            "                                   DEVICE_RESCUE, bootable=False)",
            "",
            "    def _file_inject_vm_settings(self, instance, vm_ref, vdis, network_info):",
            "        if CONF.flat_injected:",
            "            vm_utils.preconfigure_instance(self._session, instance,",
            "                                           vdis['root']['ref'], network_info)",
            "",
            "    def _ensure_instance_name_unique(self, name_label):",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "        if vm_ref is not None:",
            "            raise exception.InstanceExists(name=name_label)",
            "",
            "    def _ensure_enough_free_mem(self, instance):",
            "        if not vm_utils.is_enough_free_mem(self._session, instance):",
            "            raise exception.InsufficientFreeMemory(uuid=instance['uuid'])",
            "",
            "    def _create_vm_record(self, context, instance, name_label, vdis,",
            "            disk_image_type, kernel_file, ramdisk_file):",
            "        \"\"\"Create the VM record in Xen, making sure that we do not create",
            "        a duplicate name-label.  Also do a rough sanity check on memory",
            "        to try to short-circuit a potential failure later.  (The memory",
            "        check only accounts for running VMs, so it can miss other builds",
            "        that are in progress.)",
            "        \"\"\"",
            "        mode = self._determine_vm_mode(instance, vdis, disk_image_type)",
            "        if instance['vm_mode'] != mode:",
            "            # Update database with normalized (or determined) value",
            "            self._virtapi.instance_update(context,",
            "                                          instance['uuid'], {'vm_mode': mode})",
            "",
            "        use_pv_kernel = (mode == vm_mode.XEN)",
            "        vm_ref = vm_utils.create_vm(self._session, instance, name_label,",
            "                                    kernel_file, ramdisk_file, use_pv_kernel)",
            "        return vm_ref",
            "",
            "    def _determine_vm_mode(self, instance, vdis, disk_image_type):",
            "        current_mode = vm_mode.get_from_instance(instance)",
            "        if current_mode == vm_mode.XEN or current_mode == vm_mode.HVM:",
            "            return current_mode",
            "",
            "        is_pv = False",
            "        if 'root' in vdis:",
            "            os_type = instance['os_type']",
            "            vdi_ref = vdis['root']['ref']",
            "            is_pv = vm_utils.determine_is_pv(self._session, vdi_ref,",
            "                                             disk_image_type, os_type)",
            "        if is_pv:",
            "            return vm_mode.XEN",
            "        else:",
            "            return vm_mode.HVM",
            "",
            "    def _attach_disks(self, instance, vm_ref, name_label, vdis,",
            "                      disk_image_type, admin_password=None, files=None):",
            "        ctx = nova_context.get_admin_context()",
            "        instance_type = flavors.extract_flavor(instance)",
            "",
            "        # Attach (required) root disk",
            "        if disk_image_type == vm_utils.ImageType.DISK_ISO:",
            "            # DISK_ISO needs two VBDs: the ISO disk and a blank RW disk",
            "            root_disk_size = instance_type['root_gb']",
            "            if root_disk_size > 0:",
            "                vm_utils.generate_iso_blank_root_disk(self._session, instance,",
            "                    vm_ref, DEVICE_ROOT, name_label, root_disk_size)",
            "",
            "            cd_vdi = vdis.pop('iso')",
            "            vm_utils.attach_cd(self._session, vm_ref, cd_vdi['ref'],",
            "                               DEVICE_CD)",
            "        else:",
            "            root_vdi = vdis['root']",
            "",
            "            if instance['auto_disk_config']:",
            "                LOG.debug(_(\"Auto configuring disk, attempting to \"",
            "                            \"resize partition...\"), instance=instance)",
            "                vm_utils.try_auto_configure_disk(self._session,",
            "                                                 root_vdi['ref'],",
            "                                                 instance_type['root_gb'])",
            "",
            "            vm_utils.create_vbd(self._session, vm_ref, root_vdi['ref'],",
            "                                DEVICE_ROOT, bootable=True,",
            "                                osvol=root_vdi.get('osvol'))",
            "",
            "        # Attach (optional) additional block-devices",
            "        for type_, vdi_info in vdis.items():",
            "            # Additional block-devices for boot use their device-name as the",
            "            # type.",
            "            if not type_.startswith('/dev'):",
            "                continue",
            "",
            "            # Convert device name to userdevice number, e.g. /dev/xvdb -> 1",
            "            userdevice = ord(block_device.strip_prefix(type_)) - ord('a')",
            "            vm_utils.create_vbd(self._session, vm_ref, vdi_info['ref'],",
            "                                userdevice, bootable=False,",
            "                                osvol=vdi_info.get('osvol'))",
            "",
            "        # Attach (optional) swap disk",
            "        swap_mb = instance_type['swap']",
            "        if swap_mb:",
            "            vm_utils.generate_swap(self._session, instance, vm_ref,",
            "                                   DEVICE_SWAP, name_label, swap_mb)",
            "",
            "        # Attach (optional) ephemeral disk",
            "        ephemeral_gb = instance_type['ephemeral_gb']",
            "        if ephemeral_gb:",
            "            vm_utils.generate_ephemeral(self._session, instance, vm_ref,",
            "                                        DEVICE_EPHEMERAL, name_label,",
            "                                        ephemeral_gb)",
            "",
            "        # Attach (optional) configdrive v2 disk",
            "        if configdrive.required_by(instance):",
            "            vm_utils.generate_configdrive(self._session, instance, vm_ref,",
            "                                          DEVICE_CONFIGDRIVE,",
            "                                          admin_password=admin_password,",
            "                                          files=files)",
            "",
            "    def _wait_for_instance_to_start(self, instance, vm_ref):",
            "        LOG.debug(_('Waiting for instance state to become running'),",
            "                  instance=instance)",
            "        expiration = time.time() + CONF.xenapi_running_timeout",
            "        while time.time() < expiration:",
            "            state = self.get_info(instance, vm_ref)['state']",
            "            if state == power_state.RUNNING:",
            "                break",
            "            greenthread.sleep(0.5)",
            "",
            "    def _configure_new_instance_with_agent(self, instance, vm_ref,",
            "                                           injected_files, admin_password):",
            "        if self.agent_enabled(instance):",
            "            ctx = nova_context.get_admin_context()",
            "            agent_build = self._virtapi.agent_build_get_by_triple(",
            "                ctx, 'xen', instance['os_type'], instance['architecture'])",
            "            if agent_build:",
            "                LOG.info(_('Latest agent build for %(hypervisor)s/%(os)s'",
            "                           '/%(architecture)s is %(version)s') % agent_build)",
            "            else:",
            "                LOG.info(_('No agent build found for %(hypervisor)s/%(os)s'",
            "                           '/%(architecture)s') % {",
            "                            'hypervisor': 'xen',",
            "                            'os': instance['os_type'],",
            "                            'architecture': instance['architecture']})",
            "",
            "            # Update agent, if necessary",
            "            # This also waits until the agent starts",
            "            agent = self._get_agent(instance, vm_ref)",
            "            version = agent.get_agent_version()",
            "            if version:",
            "                LOG.info(_('Instance agent version: %s'), version,",
            "                         instance=instance)",
            "",
            "            if (version and agent_build and",
            "                    cmp_version(version, agent_build['version']) < 0):",
            "                agent.agent_update(agent_build)",
            "",
            "            # if the guest agent is not available, configure the",
            "            # instance, but skip the admin password configuration",
            "            no_agent = version is None",
            "",
            "            # Inject ssh key.",
            "            agent.inject_ssh_key()",
            "",
            "            # Inject files, if necessary",
            "            if injected_files:",
            "                # Inject any files, if specified",
            "                agent.inject_files(injected_files)",
            "",
            "            # Set admin password, if necessary",
            "            if admin_password and not no_agent:",
            "                agent.set_admin_password(admin_password)",
            "",
            "            # Reset network config",
            "            agent.resetnetwork()",
            "",
            "    def _prepare_instance_filter(self, instance, network_info):",
            "        try:",
            "            self.firewall_driver.setup_basic_filtering(",
            "                    instance, network_info)",
            "        except NotImplementedError:",
            "            # NOTE(salvatore-orlando): setup_basic_filtering might be",
            "            # empty or not implemented at all, as basic filter could",
            "            # be implemented with VIF rules created by xapi plugin",
            "            pass",
            "",
            "        self.firewall_driver.prepare_instance_filter(instance,",
            "                                                     network_info)",
            "",
            "    def _get_vm_opaque_ref(self, instance, check_rescue=False):",
            "        \"\"\"Get xapi OpaqueRef from a db record.",
            "        :param check_rescue: if True will return the 'name'-rescue vm if it",
            "                             exists, instead of just 'name'",
            "        \"\"\"",
            "        vm_ref = vm_utils.lookup(self._session, instance['name'], check_rescue)",
            "        if vm_ref is None:",
            "            raise exception.InstanceNotFound(instance_id=instance['name'])",
            "        return vm_ref",
            "",
            "    def _acquire_bootlock(self, vm):",
            "        \"\"\"Prevent an instance from booting.\"\"\"",
            "        self._session.call_xenapi(",
            "            \"VM.set_blocked_operations\",",
            "            vm,",
            "            {\"start\": \"\"})",
            "",
            "    def _release_bootlock(self, vm):",
            "        \"\"\"Allow an instance to boot.\"\"\"",
            "        self._session.call_xenapi(",
            "            \"VM.remove_from_blocked_operations\",",
            "            vm,",
            "            \"start\")",
            "",
            "    def snapshot(self, context, instance, image_id, update_task_state):",
            "        \"\"\"Create snapshot from a running VM instance.",
            "",
            "        :param context: request context",
            "        :param instance: instance to be snapshotted",
            "        :param image_id: id of image to upload to",
            "",
            "        Steps involved in a XenServer snapshot:",
            "",
            "        1. XAPI-Snapshot: Snapshotting the instance using XenAPI. This",
            "           creates: Snapshot (Template) VM, Snapshot VBD, Snapshot VDI,",
            "           Snapshot VHD",
            "",
            "        2. Wait-for-coalesce: The Snapshot VDI and Instance VDI both point to",
            "           a 'base-copy' VDI.  The base_copy is immutable and may be chained",
            "           with other base_copies.  If chained, the base_copies",
            "           coalesce together, so, we must wait for this coalescing to occur to",
            "           get a stable representation of the data on disk.",
            "",
            "        3. Push-to-data-store: Once coalesced, we call",
            "           'xenapi_image_upload_handler' to upload the images.",
            "",
            "        \"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        label = \"%s-snapshot\" % instance['name']",
            "",
            "        with vm_utils.snapshot_attached_here(",
            "                self._session, instance, vm_ref, label,",
            "                update_task_state) as vdi_uuids:",
            "            update_task_state(task_state=task_states.IMAGE_UPLOADING,",
            "                              expected_state=task_states.IMAGE_PENDING_UPLOAD)",
            "            self.image_upload_handler.upload_image(context,",
            "                                                   self._session,",
            "                                                   instance,",
            "                                                   vdi_uuids,",
            "                                                   image_id)",
            "",
            "        LOG.debug(_(\"Finished snapshot and upload for VM\"),",
            "                  instance=instance)",
            "",
            "    def _migrate_vhd(self, instance, vdi_uuid, dest, sr_path, seq_num):",
            "        LOG.debug(_(\"Migrating VHD '%(vdi_uuid)s' with seq_num %(seq_num)d\"),",
            "                  {'vdi_uuid': vdi_uuid, 'seq_num': seq_num},",
            "                  instance=instance)",
            "        instance_uuid = instance['uuid']",
            "        try:",
            "            self._session.call_plugin_serialized('migration', 'transfer_vhd',",
            "                    instance_uuid=instance_uuid, host=dest, vdi_uuid=vdi_uuid,",
            "                    sr_path=sr_path, seq_num=seq_num)",
            "        except self._session.XenAPI.Failure:",
            "            msg = _(\"Failed to transfer vhd to new host\")",
            "            raise exception.MigrationError(reason=msg)",
            "",
            "    def _get_orig_vm_name_label(self, instance):",
            "        return instance['name'] + '-orig'",
            "",
            "    def _update_instance_progress(self, context, instance, step, total_steps):",
            "        \"\"\"Update instance progress percent to reflect current step number",
            "        \"\"\"",
            "        # FIXME(sirp): for now we're taking a KISS approach to instance",
            "        # progress:",
            "        # Divide the action's workflow into discrete steps and \"bump\" the",
            "        # instance's progress field as each step is completed.",
            "        #",
            "        # For a first cut this should be fine, however, for large VM images,",
            "        # the get_vdis_for_instance step begins to dominate the equation. A",
            "        # better approximation would use the percentage of the VM image that",
            "        # has been streamed to the destination host.",
            "        progress = round(float(step) / total_steps * 100)",
            "        LOG.debug(_(\"Updating progress to %d\"), progress,",
            "                  instance=instance)",
            "        self._virtapi.instance_update(context, instance['uuid'],",
            "                                      {'progress': progress})",
            "",
            "    def _resize_ensure_vm_is_shutdown(self, instance, vm_ref):",
            "        if vm_utils.is_vm_shutdown(self._session, vm_ref):",
            "            LOG.debug(_(\"VM was already shutdown.\"), instance=instance)",
            "            return",
            "",
            "        if not vm_utils.clean_shutdown_vm(self._session, instance, vm_ref):",
            "            LOG.debug(_(\"Clean shutdown did not complete successfully, \"",
            "                        \"trying hard shutdown.\"), instance=instance)",
            "            if not vm_utils.hard_shutdown_vm(self._session, instance, vm_ref):",
            "                raise exception.ResizeError(",
            "                    reason=_(\"Unable to terminate instance.\"))",
            "",
            "    def _migrate_disk_resizing_down(self, context, instance, dest,",
            "                                    instance_type, vm_ref, sr_path):",
            "        step = make_step_decorator(context, instance,",
            "                                   self._update_instance_progress)",
            "",
            "        @step",
            "        def fake_step_to_match_resizing_up():",
            "            pass",
            "",
            "        @step",
            "        def rename_and_power_off_vm(undo_mgr):",
            "            self._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "            self._apply_orig_vm_name_label(instance, vm_ref)",
            "",
            "            def restore_orig_vm():",
            "                # Do not need to restore block devices, not yet been removed",
            "                self._restore_orig_vm_and_cleanup_orphan(instance, None)",
            "",
            "            undo_mgr.undo_with(restore_orig_vm)",
            "",
            "        @step",
            "        def create_copy_vdi_and_resize(undo_mgr, old_vdi_ref):",
            "            new_vdi_ref, new_vdi_uuid = vm_utils.resize_disk(self._session,",
            "                instance, old_vdi_ref, instance_type)",
            "",
            "            def cleanup_vdi_copy():",
            "                vm_utils.destroy_vdi(self._session, new_vdi_ref)",
            "",
            "            undo_mgr.undo_with(cleanup_vdi_copy)",
            "",
            "            return new_vdi_ref, new_vdi_uuid",
            "",
            "        @step",
            "        def transfer_vhd_to_dest(new_vdi_ref, new_vdi_uuid):",
            "            self._migrate_vhd(instance, new_vdi_uuid, dest, sr_path, 0)",
            "            # Clean up VDI now that it's been copied",
            "            vm_utils.destroy_vdi(self._session, new_vdi_ref)",
            "",
            "        @step",
            "        def fake_step_to_be_executed_by_finish_migration():",
            "            pass",
            "",
            "        undo_mgr = utils.UndoManager()",
            "        try:",
            "            fake_step_to_match_resizing_up()",
            "            rename_and_power_off_vm(undo_mgr)",
            "            old_vdi_ref, _ignore = vm_utils.get_vdi_for_vm_safely(",
            "                self._session, vm_ref)",
            "            new_vdi_ref, new_vdi_uuid = create_copy_vdi_and_resize(",
            "                undo_mgr, old_vdi_ref)",
            "            transfer_vhd_to_dest(new_vdi_ref, new_vdi_uuid)",
            "        except Exception as error:",
            "            LOG.exception(_(\"_migrate_disk_resizing_down failed. \"",
            "                            \"Restoring orig vm due_to: %s.\"), error,",
            "                          instance=instance)",
            "            undo_mgr._rollback()",
            "            raise exception.InstanceFaultRollback(error)",
            "",
            "    def _migrate_disk_resizing_up(self, context, instance, dest, vm_ref,",
            "                                  sr_path):",
            "        self._apply_orig_vm_name_label(instance, vm_ref)",
            "",
            "        # 1. Create Snapshot",
            "        label = \"%s-snapshot\" % instance['name']",
            "        with vm_utils.snapshot_attached_here(",
            "                self._session, instance, vm_ref, label) as vdi_uuids:",
            "            self._update_instance_progress(context, instance,",
            "                                           step=1,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 2. Transfer the immutable VHDs (base-copies)",
            "            #",
            "            # The first VHD will be the leaf (aka COW) that is being used by",
            "            # the VM. For this step, we're only interested in the immutable",
            "            # VHDs which are all of the parents of the leaf VHD.",
            "            for seq_num, vdi_uuid in itertools.islice(",
            "                    enumerate(vdi_uuids), 1, None):",
            "                self._migrate_vhd(instance, vdi_uuid, dest, sr_path, seq_num)",
            "                self._update_instance_progress(context, instance,",
            "                                               step=2,",
            "                                               total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 3. Now power down the instance",
            "            self._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "            self._update_instance_progress(context, instance,",
            "                                           step=3,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 4. Transfer the COW VHD",
            "            vdi_ref, vm_vdi_rec = vm_utils.get_vdi_for_vm_safely(",
            "                self._session, vm_ref)",
            "            cow_uuid = vm_vdi_rec['uuid']",
            "            self._migrate_vhd(instance, cow_uuid, dest, sr_path, 0)",
            "            self._update_instance_progress(context, instance,",
            "                                           step=4,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "    def _apply_orig_vm_name_label(self, instance, vm_ref):",
            "        # NOTE(sirp): in case we're resizing to the same host (for dev",
            "        # purposes), apply a suffix to name-label so the two VM records",
            "        # extant until a confirm_resize don't collide.",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_utils.set_vm_name_label(self._session, vm_ref, name_label)",
            "",
            "    def migrate_disk_and_power_off(self, context, instance, dest,",
            "                                   instance_type, block_device_info):",
            "        \"\"\"Copies a VHD from one host machine to another, possibly",
            "        resizing filesystem before hand.",
            "",
            "        :param instance: the instance that owns the VHD in question.",
            "        :param dest: the destination host machine.",
            "        :param instance_type: instance_type to resize to",
            "        \"\"\"",
            "        # 0. Zero out the progress to begin",
            "        self._update_instance_progress(context, instance,",
            "                                       step=0,",
            "                                       total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "        old_gb = instance['root_gb']",
            "        new_gb = instance_type['root_gb']",
            "        resize_down = old_gb > new_gb",
            "",
            "        if new_gb == 0 and old_gb != 0:",
            "            reason = _(\"Can't resize a disk to 0 GB.\")",
            "            raise exception.ResizeError(reason=reason)",
            "",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        sr_path = vm_utils.get_sr_path(self._session)",
            "",
            "        if resize_down:",
            "            self._migrate_disk_resizing_down(",
            "                    context, instance, dest, instance_type, vm_ref, sr_path)",
            "        else:",
            "            self._migrate_disk_resizing_up(",
            "                    context, instance, dest, vm_ref, sr_path)",
            "",
            "        self._detach_block_devices_from_orig_vm(instance, block_device_info)",
            "",
            "        # NOTE(sirp): disk_info isn't used by the xenapi driver, instead it",
            "        # uses a staging-area (/images/instance<uuid>) and sequence-numbered",
            "        # VHDs to figure out how to reconstruct the VDI chain after syncing",
            "        disk_info = {}",
            "        return disk_info",
            "",
            "    def _detach_block_devices_from_orig_vm(self, instance, block_device_info):",
            "        block_device_mapping = virt_driver.block_device_info_get_mapping(",
            "                block_device_info)",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        for vol in block_device_mapping:",
            "            connection_info = vol['connection_info']",
            "            mount_device = vol['mount_device'].rpartition(\"/\")[2]",
            "            self._volumeops.detach_volume(connection_info, name_label,",
            "                                          mount_device)",
            "",
            "    def _resize_up_root_vdi(self, instance, root_vdi):",
            "        \"\"\"Resize an instances root disk.\"\"\"",
            "",
            "        new_disk_size = instance['root_gb'] * 1024 * 1024 * 1024",
            "        if not new_disk_size:",
            "            return",
            "",
            "        # Get current size of VDI",
            "        virtual_size = self._session.call_xenapi('VDI.get_virtual_size',",
            "                                                 root_vdi['ref'])",
            "        virtual_size = int(virtual_size)",
            "",
            "        old_gb = virtual_size / (1024 * 1024 * 1024)",
            "        new_gb = instance['root_gb']",
            "",
            "        if virtual_size < new_disk_size:",
            "            # Resize up. Simple VDI resize will do the trick",
            "            vdi_uuid = root_vdi['uuid']",
            "            LOG.debug(_(\"Resizing up VDI %(vdi_uuid)s from %(old_gb)dGB to \"",
            "                        \"%(new_gb)dGB\"),",
            "                      {'vdi_uuid': vdi_uuid, 'old_gb': old_gb,",
            "                       'new_gb': new_gb}, instance=instance)",
            "            resize_func_name = self.check_resize_func_name()",
            "            self._session.call_xenapi(resize_func_name, root_vdi['ref'],",
            "                    str(new_disk_size))",
            "            LOG.debug(_(\"Resize complete\"), instance=instance)",
            "",
            "    def check_resize_func_name(self):",
            "        \"\"\"Check the function name used to resize an instance based",
            "        on product_brand and product_version.",
            "        \"\"\"",
            "",
            "        brand = self._session.product_brand",
            "        version = self._session.product_version",
            "",
            "        # To maintain backwards compatibility. All recent versions",
            "        # should use VDI.resize",
            "        if bool(version) and bool(brand):",
            "            xcp = brand == 'XCP'",
            "            r1_2_or_above = (",
            "                (",
            "                    version[0] == 1",
            "                    and version[1] > 1",
            "                )",
            "                or version[0] > 1)",
            "",
            "            xenserver = brand == 'XenServer'",
            "            r6_or_above = version[0] > 5",
            "",
            "            if (xcp and not r1_2_or_above) or (xenserver and not r6_or_above):",
            "                return 'VDI.resize_online'",
            "",
            "        return 'VDI.resize'",
            "",
            "    def reboot(self, instance, reboot_type, bad_volumes_callback=None):",
            "        \"\"\"Reboot VM instance.\"\"\"",
            "        # Note (salvatore-orlando): security group rules are not re-enforced",
            "        # upon reboot, since this action on the XenAPI drivers does not",
            "        # remove existing filters",
            "        vm_ref = self._get_vm_opaque_ref(instance, check_rescue=True)",
            "",
            "        try:",
            "            if reboot_type == \"HARD\":",
            "                self._session.call_xenapi('VM.hard_reboot', vm_ref)",
            "            else:",
            "                self._session.call_xenapi('VM.clean_reboot', vm_ref)",
            "        except self._session.XenAPI.Failure as exc:",
            "            details = exc.details",
            "            if (details[0] == 'VM_BAD_POWER_STATE' and",
            "                    details[-1] == 'halted'):",
            "                LOG.info(_(\"Starting halted instance found during reboot\"),",
            "                    instance=instance)",
            "                self._start(instance, vm_ref=vm_ref,",
            "                            bad_volumes_callback=bad_volumes_callback)",
            "                return",
            "            elif details[0] == 'SR_BACKEND_FAILURE_46':",
            "                LOG.warn(_(\"Reboot failed due to bad volumes, detaching bad\"",
            "                           \" volumes and starting halted instance\"),",
            "                         instance=instance)",
            "                self._start(instance, vm_ref=vm_ref,",
            "                            bad_volumes_callback=bad_volumes_callback)",
            "                return",
            "            else:",
            "                raise",
            "",
            "    def set_admin_password(self, instance, new_pass):",
            "        \"\"\"Set the root/admin password on the VM instance.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.set_admin_password(new_pass)",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    def inject_file(self, instance, path, contents):",
            "        \"\"\"Write a file to the VM instance.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.inject_file(path, contents)",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    @staticmethod",
            "    def _sanitize_xenstore_key(key):",
            "        \"\"\"",
            "        Xenstore only allows the following characters as keys:",
            "",
            "        ABCDEFGHIJKLMNOPQRSTUVWXYZ",
            "        abcdefghijklmnopqrstuvwxyz",
            "        0123456789-/_@",
            "",
            "        So convert the others to _",
            "",
            "        Also convert / to _, because that is somewhat like a path",
            "        separator.",
            "        \"\"\"",
            "        allowed_chars = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"",
            "                         \"abcdefghijklmnopqrstuvwxyz\"",
            "                         \"0123456789-_@\")",
            "        return ''.join([x in allowed_chars and x or '_' for x in key])",
            "",
            "    def _inject_instance_metadata(self, instance, vm_ref):",
            "        \"\"\"Inject instance metadata into xenstore.\"\"\"",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def store_meta(topdir, data_dict):",
            "            for key, value in data_dict.items():",
            "                key = self._sanitize_xenstore_key(key)",
            "                value = value or ''",
            "                self._add_to_param_xenstore(vm_ref, '%s/%s' % (topdir, key),",
            "                                            jsonutils.dumps(value))",
            "",
            "        # Store user metadata",
            "        store_meta('vm-data/user-metadata', utils.instance_meta(instance))",
            "",
            "    def _inject_auto_disk_config(self, instance, vm_ref):",
            "        \"\"\"Inject instance's auto_disk_config attribute into xenstore.\"\"\"",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def store_auto_disk_config(key, value):",
            "            value = value and True or False",
            "            self._add_to_param_xenstore(vm_ref, key, str(value))",
            "",
            "        store_auto_disk_config('vm-data/auto-disk-config',",
            "                               instance['auto_disk_config'])",
            "",
            "    def change_instance_metadata(self, instance, diff):",
            "        \"\"\"Apply changes to instance metadata to xenstore.\"\"\"",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "        except exception.NotFound:",
            "            # NOTE(johngarbutt) race conditions mean we can still get here",
            "            # during operations where the VM is not present, like resize.",
            "            # Skip the update when not possible, as the updated metadata will",
            "            # get added when the VM is being booted up at the end of the",
            "            # resize or rebuild.",
            "            LOG.warn(_(\"Unable to update metadata, VM not found.\"),",
            "                     instance=instance, exc_info=True)",
            "            return",
            "",
            "        def process_change(location, change):",
            "            if change[0] == '-':",
            "                self._remove_from_param_xenstore(vm_ref, location)",
            "                try:",
            "                    self._delete_from_xenstore(instance, location,",
            "                                               vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "            elif change[0] == '+':",
            "                self._add_to_param_xenstore(vm_ref, location,",
            "                                            jsonutils.dumps(change[1]))",
            "                try:",
            "                    self._write_to_xenstore(instance, location, change[1],",
            "                                            vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def update_meta():",
            "            for key, change in diff.items():",
            "                key = self._sanitize_xenstore_key(key)",
            "                location = 'vm-data/user-metadata/%s' % key",
            "                process_change(location, change)",
            "        update_meta()",
            "",
            "    def _find_root_vdi_ref(self, vm_ref):",
            "        \"\"\"Find and return the root vdi ref for a VM.\"\"\"",
            "        if not vm_ref:",
            "            return None",
            "",
            "        vbd_refs = self._session.call_xenapi(\"VM.get_VBDs\", vm_ref)",
            "",
            "        for vbd_uuid in vbd_refs:",
            "            vbd = self._session.call_xenapi(\"VBD.get_record\", vbd_uuid)",
            "            if vbd[\"userdevice\"] == DEVICE_ROOT:",
            "                return vbd[\"VDI\"]",
            "",
            "        raise exception.NotFound(_(\"Unable to find root VBD/VDI for VM\"))",
            "",
            "    def _destroy_vdis(self, instance, vm_ref):",
            "        \"\"\"Destroys all VDIs associated with a VM.\"\"\"",
            "        LOG.debug(_(\"Destroying VDIs\"), instance=instance)",
            "",
            "        vdi_refs = vm_utils.lookup_vm_vdis(self._session, vm_ref)",
            "        if not vdi_refs:",
            "            return",
            "        for vdi_ref in vdi_refs:",
            "            try:",
            "                vm_utils.destroy_vdi(self._session, vdi_ref)",
            "            except volume_utils.StorageError as exc:",
            "                LOG.error(exc)",
            "",
            "    def _destroy_kernel_ramdisk(self, instance, vm_ref):",
            "        \"\"\"Three situations can occur:",
            "",
            "            1. We have neither a ramdisk nor a kernel, in which case we are a",
            "               RAW image and can omit this step",
            "",
            "            2. We have one or the other, in which case, we should flag as an",
            "               error",
            "",
            "            3. We have both, in which case we safely remove both the kernel",
            "               and the ramdisk.",
            "",
            "        \"\"\"",
            "        instance_uuid = instance['uuid']",
            "        if not instance['kernel_id'] and not instance['ramdisk_id']:",
            "            # 1. No kernel or ramdisk",
            "            LOG.debug(_(\"Using RAW or VHD, skipping kernel and ramdisk \"",
            "                        \"deletion\"), instance=instance)",
            "            return",
            "",
            "        if not (instance['kernel_id'] and instance['ramdisk_id']):",
            "            # 2. We only have kernel xor ramdisk",
            "            raise exception.InstanceUnacceptable(instance_id=instance_uuid,",
            "               reason=_(\"instance has a kernel or ramdisk but not both\"))",
            "",
            "        # 3. We have both kernel and ramdisk",
            "        (kernel, ramdisk) = vm_utils.lookup_kernel_ramdisk(self._session,",
            "                                                           vm_ref)",
            "        if kernel or ramdisk:",
            "            vm_utils.destroy_kernel_ramdisk(self._session, instance,",
            "                                            kernel, ramdisk)",
            "            LOG.debug(_(\"kernel/ramdisk files removed\"), instance=instance)",
            "",
            "    def _destroy_rescue_instance(self, rescue_vm_ref, original_vm_ref):",
            "        \"\"\"Destroy a rescue instance.\"\"\"",
            "        # Shutdown Rescue VM",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", rescue_vm_ref)",
            "        state = vm_utils.compile_info(vm_rec)['state']",
            "        if state != power_state.SHUTDOWN:",
            "            self._session.call_xenapi(\"VM.hard_shutdown\", rescue_vm_ref)",
            "",
            "        # Destroy Rescue VDIs",
            "        vdi_refs = vm_utils.lookup_vm_vdis(self._session, rescue_vm_ref)",
            "        root_vdi_ref = self._find_root_vdi_ref(original_vm_ref)",
            "        vdi_refs = [vdi_ref for vdi_ref in vdi_refs if vdi_ref != root_vdi_ref]",
            "        vm_utils.safe_destroy_vdis(self._session, vdi_refs)",
            "",
            "        # Destroy Rescue VM",
            "        self._session.call_xenapi(\"VM.destroy\", rescue_vm_ref)",
            "",
            "    def destroy(self, instance, network_info, block_device_info=None,",
            "                destroy_disks=True):",
            "        \"\"\"Destroy VM instance.",
            "",
            "        This is the method exposed by xenapi_conn.destroy(). The rest of the",
            "        destroy_* methods are internal.",
            "",
            "        \"\"\"",
            "        LOG.info(_(\"Destroying VM\"), instance=instance)",
            "",
            "        # We don't use _get_vm_opaque_ref because the instance may",
            "        # truly not exist because of a failure during build. A valid",
            "        # vm_ref is checked correctly where necessary.",
            "        vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "",
            "        rescue_vm_ref = vm_utils.lookup(self._session,",
            "                                        \"%s-rescue\" % instance['name'])",
            "        if rescue_vm_ref:",
            "            self._destroy_rescue_instance(rescue_vm_ref, vm_ref)",
            "",
            "        # NOTE(sirp): `block_device_info` is not used, information about which",
            "        # volumes should be detached is determined by the",
            "        # VBD.other_config['osvol'] attribute",
            "        return self._destroy(instance, vm_ref, network_info=network_info,",
            "                             destroy_disks=destroy_disks)",
            "",
            "    def _destroy(self, instance, vm_ref, network_info=None,",
            "                 destroy_disks=True):",
            "        \"\"\"Destroys VM instance by performing:",
            "",
            "            1. A shutdown",
            "            2. Destroying associated VDIs.",
            "            3. Destroying kernel and ramdisk files (if necessary).",
            "            4. Destroying that actual VM record.",
            "",
            "        \"\"\"",
            "        if vm_ref is None:",
            "            LOG.warning(_(\"VM is not present, skipping destroy...\"),",
            "                        instance=instance)",
            "            return",
            "",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "",
            "        if destroy_disks:",
            "            self._volumeops.detach_all(vm_ref)",
            "            self._destroy_vdis(instance, vm_ref)",
            "            self._destroy_kernel_ramdisk(instance, vm_ref)",
            "",
            "        vm_utils.destroy_vm(self._session, instance, vm_ref)",
            "",
            "        self.unplug_vifs(instance, network_info)",
            "        self.firewall_driver.unfilter_instance(",
            "                instance, network_info=network_info)",
            "",
            "    def pause(self, instance):",
            "        \"\"\"Pause VM instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._session.call_xenapi('VM.pause', vm_ref)",
            "",
            "    def unpause(self, instance):",
            "        \"\"\"Unpause VM instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._session.call_xenapi('VM.unpause', vm_ref)",
            "",
            "    def suspend(self, instance):",
            "        \"\"\"Suspend the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._acquire_bootlock(vm_ref)",
            "        self._session.call_xenapi('VM.suspend', vm_ref)",
            "",
            "    def resume(self, instance):",
            "        \"\"\"Resume the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._release_bootlock(vm_ref)",
            "        self._session.call_xenapi('VM.resume', vm_ref, False, True)",
            "",
            "    def rescue(self, context, instance, network_info, image_meta,",
            "               rescue_password):",
            "        \"\"\"Rescue the specified instance.",
            "",
            "            - shutdown the instance VM.",
            "            - set 'bootlock' to prevent the instance from starting in rescue.",
            "            - spawn a rescue VM (the vm name-label will be instance-N-rescue).",
            "",
            "        \"\"\"",
            "        rescue_name_label = '%s-rescue' % instance['name']",
            "        rescue_vm_ref = vm_utils.lookup(self._session, rescue_name_label)",
            "        if rescue_vm_ref:",
            "            raise RuntimeError(_(\"Instance is already in Rescue Mode: %s\")",
            "                               % instance['name'])",
            "",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "        self._acquire_bootlock(vm_ref)",
            "        self.spawn(context, instance, image_meta, [], rescue_password,",
            "                   network_info, name_label=rescue_name_label, rescue=True)",
            "",
            "    def unrescue(self, instance):",
            "        \"\"\"Unrescue the specified instance.",
            "",
            "            - unplug the instance VM's disk from the rescue VM.",
            "            - teardown the rescue VM.",
            "            - release the bootlock to allow the instance VM to start.",
            "",
            "        \"\"\"",
            "        rescue_vm_ref = vm_utils.lookup(self._session,",
            "                                        \"%s-rescue\" % instance['name'])",
            "        if not rescue_vm_ref:",
            "            raise exception.InstanceNotInRescueMode(",
            "                    instance_id=instance['uuid'])",
            "",
            "        original_vm_ref = self._get_vm_opaque_ref(instance)",
            "",
            "        self._destroy_rescue_instance(rescue_vm_ref, original_vm_ref)",
            "        self._release_bootlock(original_vm_ref)",
            "        self._start(instance, original_vm_ref)",
            "",
            "    def soft_delete(self, instance):",
            "        \"\"\"Soft delete the specified instance.\"\"\"",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "        except exception.NotFound:",
            "            LOG.warning(_(\"VM is not present, skipping soft delete...\"),",
            "                        instance=instance)",
            "        else:",
            "            vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "            self._acquire_bootlock(vm_ref)",
            "",
            "    def restore(self, instance):",
            "        \"\"\"Restore the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._release_bootlock(vm_ref)",
            "        self._start(instance, vm_ref)",
            "",
            "    def power_off(self, instance):",
            "        \"\"\"Power off the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "",
            "    def power_on(self, instance):",
            "        \"\"\"Power on the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._start(instance, vm_ref)",
            "",
            "    def _cancel_stale_tasks(self, timeout, task):",
            "        \"\"\"Cancel the given tasks that are older than the given timeout.\"\"\"",
            "        task_refs = self._session.call_xenapi(\"task.get_by_name_label\", task)",
            "        for task_ref in task_refs:",
            "            task_rec = self._session.call_xenapi(\"task.get_record\", task_ref)",
            "            task_created = timeutils.parse_strtime(task_rec[\"created\"].value,",
            "                                                   \"%Y%m%dT%H:%M:%SZ\")",
            "",
            "            if timeutils.is_older_than(task_created, timeout):",
            "                self._session.call_xenapi(\"task.cancel\", task_ref)",
            "",
            "    def poll_rebooting_instances(self, timeout, instances):",
            "        \"\"\"Look for expirable rebooting instances.",
            "",
            "            - issue a \"hard\" reboot to any instance that has been stuck in a",
            "              reboot state for >= the given timeout",
            "        \"\"\"",
            "        # NOTE(jk0): All existing clean_reboot tasks must be cancelled before",
            "        # we can kick off the hard_reboot tasks.",
            "        self._cancel_stale_tasks(timeout, 'VM.clean_reboot')",
            "",
            "        ctxt = nova_context.get_admin_context()",
            "",
            "        instances_info = dict(instance_count=len(instances),",
            "                timeout=timeout)",
            "",
            "        if instances_info[\"instance_count\"] > 0:",
            "            LOG.info(_(\"Found %(instance_count)d hung reboots \"",
            "                       \"older than %(timeout)d seconds\") % instances_info)",
            "",
            "        for instance in instances:",
            "            LOG.info(_(\"Automatically hard rebooting\"), instance=instance)",
            "            self.compute_api.reboot(ctxt, instance, \"HARD\")",
            "",
            "    def get_info(self, instance, vm_ref=None):",
            "        \"\"\"Return data about VM instance.\"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_utils.compile_info(vm_rec)",
            "",
            "    def get_diagnostics(self, instance):",
            "        \"\"\"Return data about VM diagnostics.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_utils.compile_diagnostics(vm_rec)",
            "",
            "    def _get_vif_device_map(self, vm_rec):",
            "        vif_map = {}",
            "        for vif in [self._session.call_xenapi(\"VIF.get_record\", vrec)",
            "                    for vrec in vm_rec['VIFs']]:",
            "            vif_map[vif['device']] = vif['MAC']",
            "        return vif_map",
            "",
            "    def get_all_bw_counters(self):",
            "        \"\"\"Return running bandwidth counter for each interface on each",
            "           running VM.",
            "        \"\"\"",
            "        counters = vm_utils.fetch_bandwidth(self._session)",
            "        bw = {}",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            vif_map = self._get_vif_device_map(vm_rec)",
            "            name = vm_rec['name_label']",
            "            if 'nova_uuid' not in vm_rec['other_config']:",
            "                continue",
            "            dom = vm_rec.get('domid')",
            "            if dom is None or dom not in counters:",
            "                continue",
            "            vifs_bw = bw.setdefault(name, {})",
            "            for vif_num, vif_data in counters[dom].iteritems():",
            "                mac = vif_map[vif_num]",
            "                vif_data['mac_address'] = mac",
            "                vifs_bw[mac] = vif_data",
            "        return bw",
            "",
            "    def get_console_output(self, instance):",
            "        \"\"\"Return last few lines of instance console.\"\"\"",
            "        dom_id = self._get_dom_id(instance, check_rescue=True)",
            "",
            "        try:",
            "            raw_console_data = self._session.call_plugin('console',",
            "                    'get_console_log', {'dom_id': dom_id})",
            "        except self._session.XenAPI.Failure as exc:",
            "            LOG.exception(exc)",
            "            msg = _(\"Guest does not have a console available\")",
            "            raise exception.NovaException(msg)",
            "",
            "        return zlib.decompress(base64.b64decode(raw_console_data))",
            "",
            "    def get_vnc_console(self, instance):",
            "        \"\"\"Return connection info for a vnc console.\"\"\"",
            "        if instance['vm_state'] == vm_states.RESCUED:",
            "            name = '%s-rescue' % instance['name']",
            "            vm_ref = vm_utils.lookup(self._session, name)",
            "            if vm_ref is None:",
            "                # The rescue instance might not be ready at this point.",
            "                raise exception.InstanceNotReady(instance_id=instance['uuid'])",
            "        else:",
            "            vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "            if vm_ref is None:",
            "                # The compute manager expects InstanceNotFound for this case.",
            "                raise exception.InstanceNotFound(instance_id=instance['uuid'])",
            "",
            "        session_id = self._session.get_session_id()",
            "        path = \"/console?ref=%s&session_id=%s\" % (str(vm_ref), session_id)",
            "",
            "        # NOTE: XS5.6sp2+ use http over port 80 for xenapi com",
            "        return {'host': CONF.vncserver_proxyclient_address, 'port': 80,",
            "                'internal_access_path': path}",
            "",
            "    def _vif_xenstore_data(self, vif):",
            "        \"\"\"convert a network info vif to injectable instance data.\"\"\"",
            "",
            "        def get_ip(ip):",
            "            if not ip:",
            "                return None",
            "            return ip['address']",
            "",
            "        def fixed_ip_dict(ip, subnet):",
            "            if ip['version'] == 4:",
            "                netmask = str(subnet.as_netaddr().netmask)",
            "            else:",
            "                netmask = subnet.as_netaddr()._prefixlen",
            "",
            "            return {'ip': ip['address'],",
            "                    'enabled': '1',",
            "                    'netmask': netmask,",
            "                    'gateway': get_ip(subnet['gateway'])}",
            "",
            "        def convert_route(route):",
            "            return {'route': str(netaddr.IPNetwork(route['cidr']).network),",
            "                    'netmask': str(netaddr.IPNetwork(route['cidr']).netmask),",
            "                    'gateway': get_ip(route['gateway'])}",
            "",
            "        network = vif['network']",
            "        v4_subnets = [subnet for subnet in network['subnets']",
            "                             if subnet['version'] == 4]",
            "        v6_subnets = [subnet for subnet in network['subnets']",
            "                             if subnet['version'] == 6]",
            "",
            "        # NOTE(tr3buchet): routes and DNS come from all subnets",
            "        routes = [convert_route(route) for subnet in network['subnets']",
            "                                       for route in subnet['routes']]",
            "        dns = [get_ip(ip) for subnet in network['subnets']",
            "                          for ip in subnet['dns']]",
            "",
            "        info_dict = {'label': network['label'],",
            "                     'mac': vif['address']}",
            "",
            "        if v4_subnets:",
            "            # NOTE(tr3buchet): gateway and broadcast from first subnet",
            "            #                  primary IP will be from first subnet",
            "            #                  subnets are generally unordered :(",
            "            info_dict['gateway'] = get_ip(v4_subnets[0]['gateway'])",
            "            info_dict['broadcast'] = str(v4_subnets[0].as_netaddr().broadcast)",
            "            info_dict['ips'] = [fixed_ip_dict(ip, subnet)",
            "                                for subnet in v4_subnets",
            "                                for ip in subnet['ips']]",
            "        if v6_subnets:",
            "            # NOTE(tr3buchet): gateway from first subnet",
            "            #                  primary IP will be from first subnet",
            "            #                  subnets are generally unordered :(",
            "            info_dict['gateway_v6'] = get_ip(v6_subnets[0]['gateway'])",
            "            info_dict['ip6s'] = [fixed_ip_dict(ip, subnet)",
            "                                 for subnet in v6_subnets",
            "                                 for ip in subnet['ips']]",
            "        if routes:",
            "            info_dict['routes'] = routes",
            "",
            "        if dns:",
            "            info_dict['dns'] = list(set(dns))",
            "",
            "        return info_dict",
            "",
            "    def inject_network_info(self, instance, network_info, vm_ref=None):",
            "        \"\"\"",
            "        Generate the network info and make calls to place it into the",
            "        xenstore and the xenstore param list.",
            "        vm_ref can be passed in because it will sometimes be different than",
            "        what vm_utils.lookup(session, instance['name']) will find (ex: rescue)",
            "        \"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        LOG.debug(_(\"Injecting network info to xenstore\"), instance=instance)",
            "",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def update_nwinfo():",
            "            for vif in network_info:",
            "                xs_data = self._vif_xenstore_data(vif)",
            "                location = ('vm-data/networking/%s' %",
            "                            vif['address'].replace(':', ''))",
            "                self._add_to_param_xenstore(vm_ref,",
            "                                            location,",
            "                                            jsonutils.dumps(xs_data))",
            "                try:",
            "                    self._write_to_xenstore(instance, location, xs_data,",
            "                                            vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "        update_nwinfo()",
            "",
            "    def _create_vifs(self, instance, vm_ref, network_info):",
            "        \"\"\"Creates vifs for an instance.\"\"\"",
            "",
            "        LOG.debug(_(\"Creating vifs\"), instance=instance)",
            "",
            "        # this function raises if vm_ref is not a vm_opaque_ref",
            "        self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "",
            "        for device, vif in enumerate(network_info):",
            "            vif_rec = self.vif_driver.plug(instance, vif,",
            "                                           vm_ref=vm_ref, device=device)",
            "            network_ref = vif_rec['network']",
            "            LOG.debug(_('Creating VIF for network %s'),",
            "                      network_ref, instance=instance)",
            "            vif_ref = self._session.call_xenapi('VIF.create', vif_rec)",
            "            LOG.debug(_('Created VIF %(vif_ref)s, network %(network_ref)s'),",
            "                      {'vif_ref': vif_ref, 'network_ref': network_ref},",
            "                      instance=instance)",
            "",
            "    def plug_vifs(self, instance, network_info):",
            "        \"\"\"Set up VIF networking on the host.\"\"\"",
            "        for device, vif in enumerate(network_info):",
            "            self.vif_driver.plug(instance, vif, device=device)",
            "",
            "    def unplug_vifs(self, instance, network_info):",
            "        if network_info:",
            "            for vif in network_info:",
            "                self.vif_driver.unplug(instance, vif)",
            "",
            "    def reset_network(self, instance):",
            "        \"\"\"Calls resetnetwork method in agent.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.resetnetwork()",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    def _inject_hostname(self, instance, vm_ref, rescue):",
            "        \"\"\"Inject the hostname of the instance into the xenstore.\"\"\"",
            "        hostname = instance['hostname']",
            "        if rescue:",
            "            hostname = 'RESCUE-%s' % hostname",
            "",
            "        if instance['os_type'] == \"windows\":",
            "            # NOTE(jk0): Windows hostnames can only be <= 15 chars.",
            "            hostname = hostname[:15]",
            "",
            "        LOG.debug(_(\"Injecting hostname to xenstore\"), instance=instance)",
            "        self._add_to_param_xenstore(vm_ref, 'vm-data/hostname', hostname)",
            "",
            "    def _remove_hostname(self, instance, vm_ref):",
            "        LOG.debug(_(\"Removing hostname from xenstore\"), instance=instance)",
            "        self._remove_from_param_xenstore(vm_ref, 'vm-data/hostname')",
            "",
            "    def _write_to_xenstore(self, instance, path, value, vm_ref=None):",
            "        \"\"\"",
            "        Writes the passed value to the xenstore record for the given VM",
            "        at the specified location. A XenAPIPlugin.PluginError will be raised",
            "        if any error is encountered in the write process.",
            "        \"\"\"",
            "        return self._make_plugin_call('xenstore.py', 'write_record', instance,",
            "                                      vm_ref=vm_ref, path=path,",
            "                                      value=jsonutils.dumps(value))",
            "",
            "    def _delete_from_xenstore(self, instance, path, vm_ref=None):",
            "        \"\"\"",
            "        Deletes the value from the xenstore record for the given VM at",
            "        the specified location.  A XenAPIPlugin.PluginError will be",
            "        raised if any error is encountered in the delete process.",
            "        \"\"\"",
            "        return self._make_plugin_call('xenstore.py', 'delete_record', instance,",
            "                                      vm_ref=vm_ref, path=path)",
            "",
            "    def _make_plugin_call(self, plugin, method, instance=None, vm_ref=None,",
            "                          **addl_args):",
            "        \"\"\"",
            "        Abstracts out the process of calling a method of a xenapi plugin.",
            "        Any errors raised by the plugin will in turn raise a RuntimeError here.",
            "        \"\"\"",
            "        args = {}",
            "        if instance or vm_ref:",
            "            args['dom_id'] = self._get_dom_id(instance, vm_ref)",
            "        args.update(addl_args)",
            "        try:",
            "            return self._session.call_plugin(plugin, method, args)",
            "        except self._session.XenAPI.Failure as e:",
            "            err_msg = e.details[-1].splitlines()[-1]",
            "            if 'TIMEOUT:' in err_msg:",
            "                LOG.error(_('TIMEOUT: The call to %(method)s timed out. '",
            "                            'args=%(args)r'),",
            "                          {'method': method, 'args': args}, instance=instance)",
            "                return {'returncode': 'timeout', 'message': err_msg}",
            "            elif 'NOT IMPLEMENTED:' in err_msg:",
            "                LOG.error(_('NOT IMPLEMENTED: The call to %(method)s is not'",
            "                            ' supported by the agent. args=%(args)r'),",
            "                          {'method': method, 'args': args}, instance=instance)",
            "                return {'returncode': 'notimplemented', 'message': err_msg}",
            "            else:",
            "                LOG.error(_('The call to %(method)s returned an error: %(e)s. '",
            "                            'args=%(args)r'),",
            "                          {'method': method, 'args': args, 'e': e},",
            "                          instance=instance)",
            "                return {'returncode': 'error', 'message': err_msg}",
            "",
            "    def _get_dom_id(self, instance=None, vm_ref=None, check_rescue=False):",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance, check_rescue)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_rec['domid']",
            "",
            "    def _add_to_param_xenstore(self, vm_ref, key, val):",
            "        \"\"\"",
            "        Takes a key/value pair and adds it to the xenstore parameter",
            "        record for the given vm instance. If the key exists in xenstore,",
            "        it is overwritten",
            "        \"\"\"",
            "        self._remove_from_param_xenstore(vm_ref, key)",
            "        self._session.call_xenapi('VM.add_to_xenstore_data', vm_ref, key, val)",
            "",
            "    def _remove_from_param_xenstore(self, vm_ref, key):",
            "        \"\"\"",
            "        Takes a single key and removes it from the xenstore parameter",
            "        record data for the given VM.",
            "        If the key doesn't exist, the request is ignored.",
            "        \"\"\"",
            "        self._session.call_xenapi('VM.remove_from_xenstore_data', vm_ref, key)",
            "",
            "    def refresh_security_group_rules(self, security_group_id):",
            "        \"\"\"recreates security group rules for every instance.\"\"\"",
            "        self.firewall_driver.refresh_security_group_rules(security_group_id)",
            "",
            "    def refresh_security_group_members(self, security_group_id):",
            "        \"\"\"recreates security group rules for every instance.\"\"\"",
            "        self.firewall_driver.refresh_security_group_members(security_group_id)",
            "",
            "    def refresh_instance_security_rules(self, instance):",
            "        \"\"\"recreates security group rules for specified instance.\"\"\"",
            "        self.firewall_driver.refresh_instance_security_rules(instance)",
            "",
            "    def refresh_provider_fw_rules(self):",
            "        self.firewall_driver.refresh_provider_fw_rules()",
            "",
            "    def unfilter_instance(self, instance_ref, network_info):",
            "        \"\"\"Removes filters for each VIF of the specified instance.\"\"\"",
            "        self.firewall_driver.unfilter_instance(instance_ref,",
            "                                               network_info=network_info)",
            "",
            "    def _get_host_uuid_from_aggregate(self, context, hostname):",
            "        current_aggregate = self._virtapi.aggregate_get_by_host(",
            "            context, CONF.host, key=pool_states.POOL_FLAG)[0]",
            "        if not current_aggregate:",
            "            raise exception.AggregateHostNotFound(host=CONF.host)",
            "        try:",
            "            return current_aggregate.metadetails[hostname]",
            "        except KeyError:",
            "            reason = _('Destination host:%s must be in the same '",
            "                       'aggregate as the source server') % hostname",
            "            raise exception.MigrationPreCheckError(reason=reason)",
            "",
            "    def _ensure_host_in_aggregate(self, context, hostname):",
            "        self._get_host_uuid_from_aggregate(context, hostname)",
            "",
            "    def _get_host_opaque_ref(self, context, hostname):",
            "        host_uuid = self._get_host_uuid_from_aggregate(context, hostname)",
            "        return self._session.call_xenapi(\"host.get_by_uuid\", host_uuid)",
            "",
            "    def _migrate_receive(self, ctxt):",
            "        destref = self._session.get_xenapi_host()",
            "        # Get the network to for migrate.",
            "        # This is the one associated with the pif marked management. From cli:",
            "        # uuid=`xe pif-list --minimal management=true`",
            "        # xe pif-param-get param-name=network-uuid uuid=$uuid",
            "        expr = 'field \"management\" = \"true\"'",
            "        pifs = self._session.call_xenapi('PIF.get_all_records_where',",
            "                                         expr)",
            "        if len(pifs) != 1:",
            "            msg = _('No suitable network for migrate')",
            "            raise exception.MigrationPreCheckError(reason=msg)",
            "",
            "        nwref = pifs[pifs.keys()[0]]['network']",
            "        try:",
            "            options = {}",
            "            migrate_data = self._session.call_xenapi(\"host.migrate_receive\",",
            "                                                     destref,",
            "                                                     nwref,",
            "                                                     options)",
            "        except self._session.XenAPI.Failure as exc:",
            "            LOG.exception(exc)",
            "            msg = _('Migrate Receive failed')",
            "            raise exception.MigrationPreCheckError(reason=msg)",
            "        return migrate_data",
            "",
            "    def _get_iscsi_srs(self, ctxt, instance_ref):",
            "        vm_ref = self._get_vm_opaque_ref(instance_ref)",
            "        vbd_refs = self._session.call_xenapi(\"VM.get_VBDs\", vm_ref)",
            "",
            "        iscsi_srs = []",
            "",
            "        for vbd_ref in vbd_refs:",
            "            vdi_ref = self._session.call_xenapi(\"VBD.get_VDI\", vbd_ref)",
            "            # Check if it's on an iSCSI SR",
            "            sr_ref = self._session.call_xenapi(\"VDI.get_SR\", vdi_ref)",
            "            if self._session.call_xenapi(\"SR.get_type\", sr_ref) == 'iscsi':",
            "                iscsi_srs.append(sr_ref)",
            "",
            "        return iscsi_srs",
            "",
            "    def check_can_live_migrate_destination(self, ctxt, instance_ref,",
            "                                           block_migration=False,",
            "                                           disk_over_commit=False):",
            "        \"\"\"Check if it is possible to execute live migration.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param block_migration: if true, prepare for block migration",
            "        :param disk_over_commit: if true, allow disk over commit",
            "",
            "        \"\"\"",
            "        dest_check_data = {}",
            "        if block_migration:",
            "            migrate_send_data = self._migrate_receive(ctxt)",
            "            destination_sr_ref = vm_utils.safe_find_sr(self._session)",
            "            dest_check_data.update(",
            "                {\"block_migration\": block_migration,",
            "                 \"migrate_data\": {\"migrate_send_data\": migrate_send_data,",
            "                                  \"destination_sr_ref\": destination_sr_ref}})",
            "        else:",
            "            src = instance_ref['host']",
            "            self._ensure_host_in_aggregate(ctxt, src)",
            "            # TODO(johngarbutt) we currently assume",
            "            # instance is on a SR shared with other destination",
            "            # block migration work will be able to resolve this",
            "        return dest_check_data",
            "",
            "    def _is_xsm_sr_check_relaxed(self):",
            "        try:",
            "            return self.cached_xsm_sr_relaxed",
            "        except AttributeError:",
            "            config_value = None",
            "            try:",
            "                config_value = self._make_plugin_call('config_file',",
            "                                                      'get_val',",
            "                                                      key='relax-xsm-sr-check')",
            "            except Exception as exc:",
            "                LOG.exception(exc)",
            "            self.cached_xsm_sr_relaxed = config_value == \"true\"",
            "            return self.cached_xsm_sr_relaxed",
            "",
            "    def check_can_live_migrate_source(self, ctxt, instance_ref,",
            "                                      dest_check_data):",
            "        \"\"\"Check if it's possible to execute live migration on the source side.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param dest_check_data: data returned by the check on the",
            "                                destination, includes block_migration flag",
            "",
            "        \"\"\"",
            "        if len(self._get_iscsi_srs(ctxt, instance_ref)) > 0:",
            "            # XAPI must support the relaxed SR check for live migrating with",
            "            # iSCSI VBDs",
            "            if not self._is_xsm_sr_check_relaxed():",
            "                raise exception.MigrationError(_('XAPI supporting '",
            "                                'relax-xsm-sr-check=true requried'))",
            "",
            "        if 'migrate_data' in dest_check_data:",
            "            vm_ref = self._get_vm_opaque_ref(instance_ref)",
            "            migrate_data = dest_check_data['migrate_data']",
            "            try:",
            "                self._call_live_migrate_command(",
            "                    \"VM.assert_can_migrate\", vm_ref, migrate_data)",
            "            except self._session.XenAPI.Failure as exc:",
            "                LOG.exception(exc)",
            "                msg = _('VM.assert_can_migrate failed')",
            "                raise exception.MigrationPreCheckError(reason=msg)",
            "        return dest_check_data",
            "",
            "    def _generate_vdi_map(self, destination_sr_ref, vm_ref, sr_ref=None):",
            "        \"\"\"generate a vdi_map for _call_live_migrate_command.\"\"\"",
            "        if sr_ref is None:",
            "            sr_ref = vm_utils.safe_find_sr(self._session)",
            "        vm_vdis = vm_utils.get_instance_vdis_for_sr(self._session,",
            "                                                    vm_ref, sr_ref)",
            "        return dict((vdi, destination_sr_ref) for vdi in vm_vdis)",
            "",
            "    def _call_live_migrate_command(self, command_name, vm_ref, migrate_data):",
            "        \"\"\"unpack xapi specific parameters, and call a live migrate command.\"\"\"",
            "        destination_sr_ref = migrate_data['destination_sr_ref']",
            "        migrate_send_data = migrate_data['migrate_send_data']",
            "",
            "        vdi_map = self._generate_vdi_map(destination_sr_ref, vm_ref)",
            "",
            "        # Add destination SR refs for all of the VDIs that we created",
            "        # as part of the pre migration callback",
            "        if 'pre_live_migration_result' in migrate_data:",
            "            pre_migrate_data = migrate_data['pre_live_migration_result']",
            "            sr_uuid_map = pre_migrate_data.get('sr_uuid_map', [])",
            "            for sr_uuid in sr_uuid_map:",
            "                # Source and destination SRs have the same UUID, so get the",
            "                # reference for the local SR",
            "                sr_ref = self._session.call_xenapi(\"SR.get_by_uuid\", sr_uuid)",
            "                vdi_map.update(",
            "                    self._generate_vdi_map(",
            "                        sr_uuid_map[sr_uuid], vm_ref, sr_ref))",
            "        vif_map = {}",
            "        options = {}",
            "        self._session.call_xenapi(command_name, vm_ref,",
            "                                  migrate_send_data, True,",
            "                                  vdi_map, vif_map, options)",
            "",
            "    def live_migrate(self, context, instance, destination_hostname,",
            "                     post_method, recover_method, block_migration,",
            "                     migrate_data=None):",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            if block_migration:",
            "                if not migrate_data:",
            "                    raise exception.InvalidParameterValue('Block Migration '",
            "                                    'requires migrate data from destination')",
            "",
            "                iscsi_srs = self._get_iscsi_srs(context, instance)",
            "                try:",
            "                    self._call_live_migrate_command(",
            "                        \"VM.migrate_send\", vm_ref, migrate_data)",
            "                except self._session.XenAPI.Failure as exc:",
            "                    LOG.exception(exc)",
            "                    raise exception.MigrationError(_('Migrate Send failed'))",
            "",
            "                # Tidy up the iSCSI SRs",
            "                for sr_ref in iscsi_srs:",
            "                    volume_utils.forget_sr(self._session, sr_ref)",
            "            else:",
            "                host_ref = self._get_host_opaque_ref(context,",
            "                                                     destination_hostname)",
            "                self._session.call_xenapi(\"VM.pool_migrate\", vm_ref,",
            "                                          host_ref, {\"live\": \"true\"})",
            "            post_method(context, instance, destination_hostname,",
            "                        block_migration)",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                recover_method(context, instance, destination_hostname,",
            "                               block_migration)",
            "",
            "    def post_live_migration_at_destination(self, context, instance,",
            "                                           network_info, block_migration,",
            "                                           block_device_info):",
            "        # FIXME(johngarbutt): we should block all traffic until we have",
            "        # applied security groups, however this requires changes to XenServer",
            "        self._prepare_instance_filter(instance, network_info)",
            "        self.firewall_driver.apply_instance_filter(instance, network_info)",
            "",
            "    def get_per_instance_usage(self):",
            "        \"\"\"Get usage info about each active instance.\"\"\"",
            "        usage = {}",
            "",
            "        def _is_active(vm_rec):",
            "            power_state = vm_rec['power_state'].lower()",
            "            return power_state in ['running', 'paused']",
            "",
            "        def _get_uuid(vm_rec):",
            "            other_config = vm_rec['other_config']",
            "            return other_config.get('nova_uuid', None)",
            "",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            uuid = _get_uuid(vm_rec)",
            "",
            "            if _is_active(vm_rec) and uuid is not None:",
            "                memory_mb = int(vm_rec['memory_static_max']) / 1024 / 1024",
            "                usage[uuid] = {'memory_mb': memory_mb, 'uuid': uuid}",
            "",
            "        return usage",
            "",
            "    def attach_block_device_volumes(self, block_device_info):",
            "        sr_uuid_map = {}",
            "        try:",
            "            if block_device_info is not None:",
            "                for block_device_map in block_device_info[",
            "                                                'block_device_mapping']:",
            "                    sr_uuid, _ = self._volumeops.attach_volume(",
            "                        block_device_map['connection_info'],",
            "                        None,",
            "                        block_device_map['mount_device'],",
            "                        hotplug=False)",
            "",
            "                    sr_ref = self._session.call_xenapi('SR.get_by_uuid',",
            "                                                       sr_uuid)",
            "                    sr_uuid_map[sr_uuid] = sr_ref",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                # Disconnect the volumes we just connected",
            "                for sr in sr_uuid_map:",
            "                    volume_utils.forget_sr(self._session, sr_uuid_map[sr_ref])",
            "",
            "        return sr_uuid_map"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "454": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "455": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "456": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "457": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "458": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "459": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "460": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "461": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "462": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "463": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "464": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ],
            "465": [
                "VMOps",
                "_spawn",
                "setup_network_step"
            ]
        },
        "addLocation": []
    }
}