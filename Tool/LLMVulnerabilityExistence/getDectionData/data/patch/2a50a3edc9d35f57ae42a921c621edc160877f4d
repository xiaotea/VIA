{
    "nltk/parse/malt.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "             (r\"\\)$\", \")\"),  # round brackets"
            },
            "1": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "             (r\"\\[$\", \"[\"),"
            },
            "2": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "             (r\"\\]$\", \"]\"),  # square brackets"
            },
            "3": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+            (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers"
            },
            "5": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "             (r\"(The|the|A|a|An|an)$\", \"DT\"),  # articles"
            },
            "6": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "             (r\"(He|he|She|she|It|it|I|me|Me|You|you)$\", \"PRP\"),  # pronouns"
            },
            "7": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "             (r\"(His|his|Her|her|Its|its)$\", \"PRP$\"),  # possessive"
            }
        },
        "frontPatchFile": [
            "# Natural Language Toolkit: Interface to MaltParser",
            "#",
            "# Author: Dan Garrette <dhgarrette@gmail.com>",
            "# Contributor: Liling Tan, Mustufain, osamamukhtar11",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see LICENSE.TXT",
            "",
            "import inspect",
            "import os",
            "import subprocess",
            "import sys",
            "import tempfile",
            "",
            "from nltk.data import ZipFilePathPointer",
            "from nltk.internals import find_dir, find_file, find_jars_within_path",
            "from nltk.parse.api import ParserI",
            "from nltk.parse.dependencygraph import DependencyGraph",
            "from nltk.parse.util import taggedsents_to_conll",
            "",
            "",
            "def malt_regex_tagger():",
            "    from nltk.tag import RegexpTagger",
            "",
            "    _tagger = RegexpTagger(",
            "        [",
            "            (r\"\\.$\", \".\"),",
            "            (r\"\\,$\", \",\"),",
            "            (r\"\\?$\", \"?\"),  # fullstop, comma, Qmark",
            "            (r\"\\($\", \"(\"),",
            "            (r\"\\)$\", \")\"),  # round brackets",
            "            (r\"\\[$\", \"[\"),",
            "            (r\"\\]$\", \"]\"),  # square brackets",
            "            (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers",
            "            (r\"(The|the|A|a|An|an)$\", \"DT\"),  # articles",
            "            (r\"(He|he|She|she|It|it|I|me|Me|You|you)$\", \"PRP\"),  # pronouns",
            "            (r\"(His|his|Her|her|Its|its)$\", \"PRP$\"),  # possessive",
            "            (r\"(my|Your|your|Yours|yours)$\", \"PRP$\"),  # possessive",
            "            (r\"(on|On|in|In|at|At|since|Since)$\", \"IN\"),  # time prepopsitions",
            "            (r\"(for|For|ago|Ago|before|Before)$\", \"IN\"),  # time prepopsitions",
            "            (r\"(till|Till|until|Until)$\", \"IN\"),  # time prepopsitions",
            "            (r\"(by|By|beside|Beside)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(under|Under|below|Below)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(over|Over|above|Above)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(across|Across|through|Through)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(into|Into|towards|Towards)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(onto|Onto|from|From)$\", \"IN\"),  # space prepopsitions",
            "            (r\".*able$\", \"JJ\"),  # adjectives",
            "            (r\".*ness$\", \"NN\"),  # nouns formed from adjectives",
            "            (r\".*ly$\", \"RB\"),  # adverbs",
            "            (r\".*s$\", \"NNS\"),  # plural nouns",
            "            (r\".*ing$\", \"VBG\"),  # gerunds",
            "            (r\".*ed$\", \"VBD\"),  # past tense verbs",
            "            (r\".*\", \"NN\"),  # nouns (default)",
            "        ]",
            "    )",
            "    return _tagger.tag",
            "",
            "",
            "def find_maltparser(parser_dirname):",
            "    \"\"\"",
            "    A module to find MaltParser .jar file and its dependencies.",
            "    \"\"\"",
            "    if os.path.exists(parser_dirname):  # If a full path is given.",
            "        _malt_dir = parser_dirname",
            "    else:  # Try to find path to maltparser directory in environment variables.",
            "        _malt_dir = find_dir(parser_dirname, env_vars=(\"MALT_PARSER\",))",
            "    # Checks that that the found directory contains all the necessary .jar",
            "    malt_dependencies = [\"\", \"\", \"\"]",
            "    _malt_jars = set(find_jars_within_path(_malt_dir))",
            "    _jars = {os.path.split(jar)[1] for jar in _malt_jars}",
            "    malt_dependencies = {\"log4j.jar\", \"libsvm.jar\", \"liblinear-1.8.jar\"}",
            "",
            "    assert malt_dependencies.issubset(_jars)",
            "    assert any(",
            "        filter(lambda i: i.startswith(\"maltparser-\") and i.endswith(\".jar\"), _jars)",
            "    )",
            "    return list(_malt_jars)",
            "",
            "",
            "def find_malt_model(model_filename):",
            "    \"\"\"",
            "    A module to find pre-trained MaltParser model.",
            "    \"\"\"",
            "    if model_filename is None:",
            "        return \"malt_temp.mco\"",
            "    elif os.path.exists(model_filename):  # If a full path is given.",
            "        return model_filename",
            "    else:  # Try to find path to malt model in environment variables.",
            "        return find_file(model_filename, env_vars=(\"MALT_MODEL\",), verbose=False)",
            "",
            "",
            "class MaltParser(ParserI):",
            "    \"\"\"",
            "    A class for dependency parsing with MaltParser. The input is the paths to:",
            "    - (optionally) a maltparser directory",
            "    - (optionally) the path to a pre-trained MaltParser .mco model file",
            "    - (optionally) the tagger to use for POS tagging before parsing",
            "    - (optionally) additional Java arguments",
            "",
            "    Example:",
            "        >>> from nltk.parse import malt",
            "        >>> # With MALT_PARSER and MALT_MODEL environment set.",
            "        >>> mp = malt.MaltParser(model_filename='engmalt.linear-1.7.mco') # doctest: +SKIP",
            "        >>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() # doctest: +SKIP",
            "        (shot I (elephant an) (in (pajamas my)) .)",
            "        >>> # Without MALT_PARSER and MALT_MODEL environment.",
            "        >>> mp = malt.MaltParser('/home/user/maltparser-1.9.2/', '/home/user/engmalt.linear-1.7.mco') # doctest: +SKIP",
            "        >>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() # doctest: +SKIP",
            "        (shot I (elephant an) (in (pajamas my)) .)",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        parser_dirname=\"\",",
            "        model_filename=None,",
            "        tagger=None,",
            "        additional_java_args=None,",
            "    ):",
            "        \"\"\"",
            "        An interface for parsing with the Malt Parser.",
            "",
            "        :param parser_dirname: The path to the maltparser directory that",
            "            contains the maltparser-1.x.jar",
            "        :type parser_dirname: str",
            "        :param model_filename: The name of the pre-trained model with .mco file",
            "            extension. If provided, training will not be required.",
            "            (see http://www.maltparser.org/mco/mco.html and",
            "            see http://www.patful.com/chalk/node/185)",
            "        :type model_filename: str",
            "        :param tagger: The tagger used to POS tag the raw string before",
            "            formatting to CONLL format. It should behave like `nltk.pos_tag`",
            "        :type tagger: function",
            "        :param additional_java_args: This is the additional Java arguments that",
            "            one can use when calling Maltparser, usually this is the heapsize",
            "            limits, e.g. `additional_java_args=['-Xmx1024m']`",
            "            (see https://goo.gl/mpDBvQ)",
            "        :type additional_java_args: list",
            "        \"\"\"",
            "",
            "        # Find all the necessary jar files for MaltParser.",
            "        self.malt_jars = find_maltparser(parser_dirname)",
            "        # Initialize additional java arguments.",
            "        self.additional_java_args = (",
            "            additional_java_args if additional_java_args is not None else []",
            "        )",
            "        # Initialize model.",
            "        self.model = find_malt_model(model_filename)",
            "        self._trained = self.model != \"malt_temp.mco\"",
            "        # Set the working_dir parameters i.e. `-w` from MaltParser's option.",
            "        self.working_dir = tempfile.gettempdir()",
            "        # Initialize POS tagger.",
            "        self.tagger = tagger if tagger is not None else malt_regex_tagger()",
            "",
            "    def parse_tagged_sents(self, sentences, verbose=False, top_relation_label=\"null\"):",
            "        \"\"\"",
            "        Use MaltParser to parse multiple POS tagged sentences. Takes multiple",
            "        sentences where each sentence is a list of (word, tag) tuples.",
            "        The sentences must have already been tokenized and tagged.",
            "",
            "        :param sentences: Input sentences to parse",
            "        :type sentence: list(list(tuple(str, str)))",
            "        :return: iter(iter(``DependencyGraph``)) the dependency graph",
            "            representation of each sentence",
            "        \"\"\"",
            "        if not self._trained:",
            "            raise Exception(\"Parser has not been trained. Call train() first.\")",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            prefix=\"malt_input.conll.\", dir=self.working_dir, mode=\"w\", delete=False",
            "        ) as input_file:",
            "            with tempfile.NamedTemporaryFile(",
            "                prefix=\"malt_output.conll.\",",
            "                dir=self.working_dir,",
            "                mode=\"w\",",
            "                delete=False,",
            "            ) as output_file:",
            "                # Convert list of sentences to CONLL format.",
            "                for line in taggedsents_to_conll(sentences):",
            "                    input_file.write(str(line))",
            "                input_file.close()",
            "",
            "                # Generate command to run maltparser.",
            "                cmd = self.generate_malt_command(",
            "                    input_file.name, output_file.name, mode=\"parse\"",
            "                )",
            "",
            "                # This is a maltparser quirk, it needs to be run",
            "                # where the model file is. otherwise it goes into an awkward",
            "                # missing .jars or strange -w working_dir problem.",
            "                _current_path = os.getcwd()  # Remembers the current path.",
            "                try:  # Change to modelfile path",
            "                    os.chdir(os.path.split(self.model)[0])",
            "                except:",
            "                    pass",
            "                ret = self._execute(cmd, verbose)  # Run command.",
            "                os.chdir(_current_path)  # Change back to current path.",
            "",
            "                if ret != 0:",
            "                    raise Exception(",
            "                        \"MaltParser parsing (%s) failed with exit \"",
            "                        \"code %d\" % (\" \".join(cmd), ret)",
            "                    )",
            "",
            "                # Must return iter(iter(Tree))",
            "                with open(output_file.name) as infile:",
            "                    for tree_str in infile.read().split(\"\\n\\n\"):",
            "                        yield (",
            "                            iter(",
            "                                [",
            "                                    DependencyGraph(",
            "                                        tree_str, top_relation_label=top_relation_label",
            "                                    )",
            "                                ]",
            "                            )",
            "                        )",
            "",
            "        os.remove(input_file.name)",
            "        os.remove(output_file.name)",
            "",
            "    def parse_sents(self, sentences, verbose=False, top_relation_label=\"null\"):",
            "        \"\"\"",
            "        Use MaltParser to parse multiple sentences.",
            "        Takes a list of sentences, where each sentence is a list of words.",
            "        Each sentence will be automatically tagged with this",
            "        MaltParser instance's tagger.",
            "",
            "        :param sentences: Input sentences to parse",
            "        :type sentence: list(list(str))",
            "        :return: iter(DependencyGraph)",
            "        \"\"\"",
            "        tagged_sentences = (self.tagger(sentence) for sentence in sentences)",
            "        return self.parse_tagged_sents(",
            "            tagged_sentences, verbose, top_relation_label=top_relation_label",
            "        )",
            "",
            "    def generate_malt_command(self, inputfilename, outputfilename=None, mode=None):",
            "        \"\"\"",
            "        This function generates the maltparser command use at the terminal.",
            "",
            "        :param inputfilename: path to the input file",
            "        :type inputfilename: str",
            "        :param outputfilename: path to the output file",
            "        :type outputfilename: str",
            "        \"\"\"",
            "",
            "        cmd = [\"java\"]",
            "        cmd += self.additional_java_args  # Adds additional java arguments",
            "        # Joins classpaths with \";\" if on Windows and on Linux/Mac use \":\"",
            "        classpaths_separator = \";\" if sys.platform.startswith(\"win\") else \":\"",
            "        cmd += [",
            "            \"-cp\",",
            "            classpaths_separator.join(self.malt_jars),",
            "        ]  # Adds classpaths for jars",
            "        cmd += [\"org.maltparser.Malt\"]  # Adds the main function.",
            "",
            "        # Adds the model file.",
            "        if os.path.exists(self.model):  # when parsing",
            "            cmd += [\"-c\", os.path.split(self.model)[-1]]",
            "        else:  # when learning",
            "            cmd += [\"-c\", self.model]",
            "",
            "        cmd += [\"-i\", inputfilename]",
            "        if mode == \"parse\":",
            "            cmd += [\"-o\", outputfilename]",
            "        cmd += [\"-m\", mode]  # mode use to generate parses.",
            "        return cmd",
            "",
            "    @staticmethod",
            "    def _execute(cmd, verbose=False):",
            "        output = None if verbose else subprocess.PIPE",
            "        p = subprocess.Popen(cmd, stdout=output, stderr=output)",
            "        return p.wait()",
            "",
            "    def train(self, depgraphs, verbose=False):",
            "        \"\"\"",
            "        Train MaltParser from a list of ``DependencyGraph`` objects",
            "",
            "        :param depgraphs: list of ``DependencyGraph`` objects for training input data",
            "        :type depgraphs: DependencyGraph",
            "        \"\"\"",
            "",
            "        # Write the conll_str to malt_train.conll file in /tmp/",
            "        with tempfile.NamedTemporaryFile(",
            "            prefix=\"malt_train.conll.\", dir=self.working_dir, mode=\"w\", delete=False",
            "        ) as input_file:",
            "            input_str = \"\\n\".join(dg.to_conll(10) for dg in depgraphs)",
            "            input_file.write(str(input_str))",
            "        # Trains the model with the malt_train.conll",
            "        self.train_from_file(input_file.name, verbose=verbose)",
            "        # Removes the malt_train.conll once training finishes.",
            "        os.remove(input_file.name)",
            "",
            "    def train_from_file(self, conll_file, verbose=False):",
            "        \"\"\"",
            "        Train MaltParser from a file",
            "        :param conll_file: str for the filename of the training input data",
            "        :type conll_file: str",
            "        \"\"\"",
            "",
            "        # If conll_file is a ZipFilePathPointer,",
            "        # then we need to do some extra massaging",
            "        if isinstance(conll_file, ZipFilePathPointer):",
            "            with tempfile.NamedTemporaryFile(",
            "                prefix=\"malt_train.conll.\", dir=self.working_dir, mode=\"w\", delete=False",
            "            ) as input_file:",
            "                with conll_file.open() as conll_input_file:",
            "                    conll_str = conll_input_file.read()",
            "                    input_file.write(str(conll_str))",
            "                return self.train_from_file(input_file.name, verbose=verbose)",
            "",
            "        # Generate command to run maltparser.",
            "        cmd = self.generate_malt_command(conll_file, mode=\"learn\")",
            "        ret = self._execute(cmd, verbose)",
            "        if ret != 0:",
            "            raise Exception(",
            "                \"MaltParser training (%s) failed with exit \"",
            "                \"code %d\" % (\" \".join(cmd), ret)",
            "            )",
            "        self._trained = True",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    \"\"\"",
            "    A demonstration function to show how NLTK users can use the malt parser API.",
            "",
            "    >>> from nltk import pos_tag",
            "    >>> assert 'MALT_PARSER' in os.environ, str(",
            "    ... \"Please set MALT_PARSER in your global environment, e.g.:\\n\"",
            "    ... \"$ export MALT_PARSER='/home/user/maltparser-1.9.2/'\")",
            "    >>>",
            "    >>> assert 'MALT_MODEL' in os.environ, str(",
            "    ... \"Please set MALT_MODEL in your global environment, e.g.:\\n\"",
            "    ... \"$ export MALT_MODEL='/home/user/engmalt.linear-1.7.mco'\")",
            "    >>>",
            "    >>> _dg1_str = str(\"1    John    _    NNP   _    _    2    SUBJ    _    _\\n\"",
            "    ...             \"2    sees    _    VB    _    _    0    ROOT    _    _\\n\"",
            "    ...             \"3    a       _    DT    _    _    4    SPEC    _    _\\n\"",
            "    ...             \"4    dog     _    NN    _    _    2    OBJ     _    _\\n\"",
            "    ...             \"5    .     _    .    _    _    2    PUNCT     _    _\\n\")",
            "    >>>",
            "    >>>",
            "    >>> _dg2_str  = str(\"1    John    _    NNP   _    _    2    SUBJ    _    _\\n\"",
            "    ...             \"2    walks   _    VB    _    _    0    ROOT    _    _\\n\"",
            "    ...             \"3    .     _    .    _    _    2    PUNCT     _    _\\n\")",
            "    >>> dg1 = DependencyGraph(_dg1_str)",
            "    >>> dg2 = DependencyGraph(_dg2_str)",
            "    >>> # Initialize a MaltParser object",
            "    >>> mp = MaltParser()",
            "    >>>",
            "    >>> # Trains a model.",
            "    >>> mp.train([dg1,dg2], verbose=False)",
            "    >>> sent1 = ['John','sees','Mary', '.']",
            "    >>> sent2 = ['John', 'walks', 'a', 'dog', '.']",
            "    >>>",
            "    >>> # Parse a single sentence.",
            "    >>> parsed_sent1 = mp.parse_one(sent1)",
            "    >>> parsed_sent2 = mp.parse_one(sent2)",
            "    >>> print(parsed_sent1.tree())",
            "    (sees John Mary .)",
            "    >>> print(parsed_sent2.tree())",
            "    (walks John (dog a) .)",
            "    >>>",
            "    >>> # Parsing multiple sentences.",
            "    >>> sentences = [sent1,sent2]",
            "    >>> parsed_sents = mp.parse_sents(sentences)",
            "    >>> print(next(next(parsed_sents)).tree())",
            "    (sees John Mary .)",
            "    >>> print(next(next(parsed_sents)).tree())",
            "    (walks John (dog a) .)",
            "    >>>",
            "    >>> # Initialize a MaltParser object with an English pre-trained model.",
            "    >>> parser_dirname = 'maltparser-1.9.2'",
            "    >>> model_name = 'engmalt.linear-1.7.mco'",
            "    >>> mp = MaltParser(parser_dirname=parser_dirname, model_filename=model_name, tagger=pos_tag)",
            "    >>> sent1 = 'I shot an elephant in my pajamas .'.split()",
            "    >>> sent2 = 'Time flies like banana .'.split()",
            "    >>> # Parse a single sentence.",
            "    >>> print(mp.parse_one(sent1).tree())",
            "    (shot I (elephant an) (in (pajamas my)) .)",
            "    # Parsing multiple sentences",
            "    >>> sentences = [sent1,sent2]",
            "    >>> parsed_sents = mp.parse_sents(sentences)",
            "    >>> print(next(next(parsed_sents)).tree())",
            "    (shot I (elephant an) (in (pajamas my)) .)",
            "    >>> print(next(next(parsed_sents)).tree())",
            "    (flies Time (like banana) .)",
            "    \"\"\"",
            "",
            "    import doctest",
            "",
            "    doctest.testmod()"
        ],
        "afterPatchFile": [
            "# Natural Language Toolkit: Interface to MaltParser",
            "#",
            "# Author: Dan Garrette <dhgarrette@gmail.com>",
            "# Contributor: Liling Tan, Mustufain, osamamukhtar11",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see LICENSE.TXT",
            "",
            "import inspect",
            "import os",
            "import subprocess",
            "import sys",
            "import tempfile",
            "",
            "from nltk.data import ZipFilePathPointer",
            "from nltk.internals import find_dir, find_file, find_jars_within_path",
            "from nltk.parse.api import ParserI",
            "from nltk.parse.dependencygraph import DependencyGraph",
            "from nltk.parse.util import taggedsents_to_conll",
            "",
            "",
            "def malt_regex_tagger():",
            "    from nltk.tag import RegexpTagger",
            "",
            "    _tagger = RegexpTagger(",
            "        [",
            "            (r\"\\.$\", \".\"),",
            "            (r\"\\,$\", \",\"),",
            "            (r\"\\?$\", \"?\"),  # fullstop, comma, Qmark",
            "            (r\"\\($\", \"(\"),",
            "            (r\"\\)$\", \")\"),  # round brackets",
            "            (r\"\\[$\", \"[\"),",
            "            (r\"\\]$\", \"]\"),  # square brackets",
            "            (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers",
            "            (r\"(The|the|A|a|An|an)$\", \"DT\"),  # articles",
            "            (r\"(He|he|She|she|It|it|I|me|Me|You|you)$\", \"PRP\"),  # pronouns",
            "            (r\"(His|his|Her|her|Its|its)$\", \"PRP$\"),  # possessive",
            "            (r\"(my|Your|your|Yours|yours)$\", \"PRP$\"),  # possessive",
            "            (r\"(on|On|in|In|at|At|since|Since)$\", \"IN\"),  # time prepopsitions",
            "            (r\"(for|For|ago|Ago|before|Before)$\", \"IN\"),  # time prepopsitions",
            "            (r\"(till|Till|until|Until)$\", \"IN\"),  # time prepopsitions",
            "            (r\"(by|By|beside|Beside)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(under|Under|below|Below)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(over|Over|above|Above)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(across|Across|through|Through)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(into|Into|towards|Towards)$\", \"IN\"),  # space prepopsitions",
            "            (r\"(onto|Onto|from|From)$\", \"IN\"),  # space prepopsitions",
            "            (r\".*able$\", \"JJ\"),  # adjectives",
            "            (r\".*ness$\", \"NN\"),  # nouns formed from adjectives",
            "            (r\".*ly$\", \"RB\"),  # adverbs",
            "            (r\".*s$\", \"NNS\"),  # plural nouns",
            "            (r\".*ing$\", \"VBG\"),  # gerunds",
            "            (r\".*ed$\", \"VBD\"),  # past tense verbs",
            "            (r\".*\", \"NN\"),  # nouns (default)",
            "        ]",
            "    )",
            "    return _tagger.tag",
            "",
            "",
            "def find_maltparser(parser_dirname):",
            "    \"\"\"",
            "    A module to find MaltParser .jar file and its dependencies.",
            "    \"\"\"",
            "    if os.path.exists(parser_dirname):  # If a full path is given.",
            "        _malt_dir = parser_dirname",
            "    else:  # Try to find path to maltparser directory in environment variables.",
            "        _malt_dir = find_dir(parser_dirname, env_vars=(\"MALT_PARSER\",))",
            "    # Checks that that the found directory contains all the necessary .jar",
            "    malt_dependencies = [\"\", \"\", \"\"]",
            "    _malt_jars = set(find_jars_within_path(_malt_dir))",
            "    _jars = {os.path.split(jar)[1] for jar in _malt_jars}",
            "    malt_dependencies = {\"log4j.jar\", \"libsvm.jar\", \"liblinear-1.8.jar\"}",
            "",
            "    assert malt_dependencies.issubset(_jars)",
            "    assert any(",
            "        filter(lambda i: i.startswith(\"maltparser-\") and i.endswith(\".jar\"), _jars)",
            "    )",
            "    return list(_malt_jars)",
            "",
            "",
            "def find_malt_model(model_filename):",
            "    \"\"\"",
            "    A module to find pre-trained MaltParser model.",
            "    \"\"\"",
            "    if model_filename is None:",
            "        return \"malt_temp.mco\"",
            "    elif os.path.exists(model_filename):  # If a full path is given.",
            "        return model_filename",
            "    else:  # Try to find path to malt model in environment variables.",
            "        return find_file(model_filename, env_vars=(\"MALT_MODEL\",), verbose=False)",
            "",
            "",
            "class MaltParser(ParserI):",
            "    \"\"\"",
            "    A class for dependency parsing with MaltParser. The input is the paths to:",
            "    - (optionally) a maltparser directory",
            "    - (optionally) the path to a pre-trained MaltParser .mco model file",
            "    - (optionally) the tagger to use for POS tagging before parsing",
            "    - (optionally) additional Java arguments",
            "",
            "    Example:",
            "        >>> from nltk.parse import malt",
            "        >>> # With MALT_PARSER and MALT_MODEL environment set.",
            "        >>> mp = malt.MaltParser(model_filename='engmalt.linear-1.7.mco') # doctest: +SKIP",
            "        >>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() # doctest: +SKIP",
            "        (shot I (elephant an) (in (pajamas my)) .)",
            "        >>> # Without MALT_PARSER and MALT_MODEL environment.",
            "        >>> mp = malt.MaltParser('/home/user/maltparser-1.9.2/', '/home/user/engmalt.linear-1.7.mco') # doctest: +SKIP",
            "        >>> mp.parse_one('I shot an elephant in my pajamas .'.split()).tree() # doctest: +SKIP",
            "        (shot I (elephant an) (in (pajamas my)) .)",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        parser_dirname=\"\",",
            "        model_filename=None,",
            "        tagger=None,",
            "        additional_java_args=None,",
            "    ):",
            "        \"\"\"",
            "        An interface for parsing with the Malt Parser.",
            "",
            "        :param parser_dirname: The path to the maltparser directory that",
            "            contains the maltparser-1.x.jar",
            "        :type parser_dirname: str",
            "        :param model_filename: The name of the pre-trained model with .mco file",
            "            extension. If provided, training will not be required.",
            "            (see http://www.maltparser.org/mco/mco.html and",
            "            see http://www.patful.com/chalk/node/185)",
            "        :type model_filename: str",
            "        :param tagger: The tagger used to POS tag the raw string before",
            "            formatting to CONLL format. It should behave like `nltk.pos_tag`",
            "        :type tagger: function",
            "        :param additional_java_args: This is the additional Java arguments that",
            "            one can use when calling Maltparser, usually this is the heapsize",
            "            limits, e.g. `additional_java_args=['-Xmx1024m']`",
            "            (see https://goo.gl/mpDBvQ)",
            "        :type additional_java_args: list",
            "        \"\"\"",
            "",
            "        # Find all the necessary jar files for MaltParser.",
            "        self.malt_jars = find_maltparser(parser_dirname)",
            "        # Initialize additional java arguments.",
            "        self.additional_java_args = (",
            "            additional_java_args if additional_java_args is not None else []",
            "        )",
            "        # Initialize model.",
            "        self.model = find_malt_model(model_filename)",
            "        self._trained = self.model != \"malt_temp.mco\"",
            "        # Set the working_dir parameters i.e. `-w` from MaltParser's option.",
            "        self.working_dir = tempfile.gettempdir()",
            "        # Initialize POS tagger.",
            "        self.tagger = tagger if tagger is not None else malt_regex_tagger()",
            "",
            "    def parse_tagged_sents(self, sentences, verbose=False, top_relation_label=\"null\"):",
            "        \"\"\"",
            "        Use MaltParser to parse multiple POS tagged sentences. Takes multiple",
            "        sentences where each sentence is a list of (word, tag) tuples.",
            "        The sentences must have already been tokenized and tagged.",
            "",
            "        :param sentences: Input sentences to parse",
            "        :type sentence: list(list(tuple(str, str)))",
            "        :return: iter(iter(``DependencyGraph``)) the dependency graph",
            "            representation of each sentence",
            "        \"\"\"",
            "        if not self._trained:",
            "            raise Exception(\"Parser has not been trained. Call train() first.\")",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            prefix=\"malt_input.conll.\", dir=self.working_dir, mode=\"w\", delete=False",
            "        ) as input_file:",
            "            with tempfile.NamedTemporaryFile(",
            "                prefix=\"malt_output.conll.\",",
            "                dir=self.working_dir,",
            "                mode=\"w\",",
            "                delete=False,",
            "            ) as output_file:",
            "                # Convert list of sentences to CONLL format.",
            "                for line in taggedsents_to_conll(sentences):",
            "                    input_file.write(str(line))",
            "                input_file.close()",
            "",
            "                # Generate command to run maltparser.",
            "                cmd = self.generate_malt_command(",
            "                    input_file.name, output_file.name, mode=\"parse\"",
            "                )",
            "",
            "                # This is a maltparser quirk, it needs to be run",
            "                # where the model file is. otherwise it goes into an awkward",
            "                # missing .jars or strange -w working_dir problem.",
            "                _current_path = os.getcwd()  # Remembers the current path.",
            "                try:  # Change to modelfile path",
            "                    os.chdir(os.path.split(self.model)[0])",
            "                except:",
            "                    pass",
            "                ret = self._execute(cmd, verbose)  # Run command.",
            "                os.chdir(_current_path)  # Change back to current path.",
            "",
            "                if ret != 0:",
            "                    raise Exception(",
            "                        \"MaltParser parsing (%s) failed with exit \"",
            "                        \"code %d\" % (\" \".join(cmd), ret)",
            "                    )",
            "",
            "                # Must return iter(iter(Tree))",
            "                with open(output_file.name) as infile:",
            "                    for tree_str in infile.read().split(\"\\n\\n\"):",
            "                        yield (",
            "                            iter(",
            "                                [",
            "                                    DependencyGraph(",
            "                                        tree_str, top_relation_label=top_relation_label",
            "                                    )",
            "                                ]",
            "                            )",
            "                        )",
            "",
            "        os.remove(input_file.name)",
            "        os.remove(output_file.name)",
            "",
            "    def parse_sents(self, sentences, verbose=False, top_relation_label=\"null\"):",
            "        \"\"\"",
            "        Use MaltParser to parse multiple sentences.",
            "        Takes a list of sentences, where each sentence is a list of words.",
            "        Each sentence will be automatically tagged with this",
            "        MaltParser instance's tagger.",
            "",
            "        :param sentences: Input sentences to parse",
            "        :type sentence: list(list(str))",
            "        :return: iter(DependencyGraph)",
            "        \"\"\"",
            "        tagged_sentences = (self.tagger(sentence) for sentence in sentences)",
            "        return self.parse_tagged_sents(",
            "            tagged_sentences, verbose, top_relation_label=top_relation_label",
            "        )",
            "",
            "    def generate_malt_command(self, inputfilename, outputfilename=None, mode=None):",
            "        \"\"\"",
            "        This function generates the maltparser command use at the terminal.",
            "",
            "        :param inputfilename: path to the input file",
            "        :type inputfilename: str",
            "        :param outputfilename: path to the output file",
            "        :type outputfilename: str",
            "        \"\"\"",
            "",
            "        cmd = [\"java\"]",
            "        cmd += self.additional_java_args  # Adds additional java arguments",
            "        # Joins classpaths with \";\" if on Windows and on Linux/Mac use \":\"",
            "        classpaths_separator = \";\" if sys.platform.startswith(\"win\") else \":\"",
            "        cmd += [",
            "            \"-cp\",",
            "            classpaths_separator.join(self.malt_jars),",
            "        ]  # Adds classpaths for jars",
            "        cmd += [\"org.maltparser.Malt\"]  # Adds the main function.",
            "",
            "        # Adds the model file.",
            "        if os.path.exists(self.model):  # when parsing",
            "            cmd += [\"-c\", os.path.split(self.model)[-1]]",
            "        else:  # when learning",
            "            cmd += [\"-c\", self.model]",
            "",
            "        cmd += [\"-i\", inputfilename]",
            "        if mode == \"parse\":",
            "            cmd += [\"-o\", outputfilename]",
            "        cmd += [\"-m\", mode]  # mode use to generate parses.",
            "        return cmd",
            "",
            "    @staticmethod",
            "    def _execute(cmd, verbose=False):",
            "        output = None if verbose else subprocess.PIPE",
            "        p = subprocess.Popen(cmd, stdout=output, stderr=output)",
            "        return p.wait()",
            "",
            "    def train(self, depgraphs, verbose=False):",
            "        \"\"\"",
            "        Train MaltParser from a list of ``DependencyGraph`` objects",
            "",
            "        :param depgraphs: list of ``DependencyGraph`` objects for training input data",
            "        :type depgraphs: DependencyGraph",
            "        \"\"\"",
            "",
            "        # Write the conll_str to malt_train.conll file in /tmp/",
            "        with tempfile.NamedTemporaryFile(",
            "            prefix=\"malt_train.conll.\", dir=self.working_dir, mode=\"w\", delete=False",
            "        ) as input_file:",
            "            input_str = \"\\n\".join(dg.to_conll(10) for dg in depgraphs)",
            "            input_file.write(str(input_str))",
            "        # Trains the model with the malt_train.conll",
            "        self.train_from_file(input_file.name, verbose=verbose)",
            "        # Removes the malt_train.conll once training finishes.",
            "        os.remove(input_file.name)",
            "",
            "    def train_from_file(self, conll_file, verbose=False):",
            "        \"\"\"",
            "        Train MaltParser from a file",
            "        :param conll_file: str for the filename of the training input data",
            "        :type conll_file: str",
            "        \"\"\"",
            "",
            "        # If conll_file is a ZipFilePathPointer,",
            "        # then we need to do some extra massaging",
            "        if isinstance(conll_file, ZipFilePathPointer):",
            "            with tempfile.NamedTemporaryFile(",
            "                prefix=\"malt_train.conll.\", dir=self.working_dir, mode=\"w\", delete=False",
            "            ) as input_file:",
            "                with conll_file.open() as conll_input_file:",
            "                    conll_str = conll_input_file.read()",
            "                    input_file.write(str(conll_str))",
            "                return self.train_from_file(input_file.name, verbose=verbose)",
            "",
            "        # Generate command to run maltparser.",
            "        cmd = self.generate_malt_command(conll_file, mode=\"learn\")",
            "        ret = self._execute(cmd, verbose)",
            "        if ret != 0:",
            "            raise Exception(",
            "                \"MaltParser training (%s) failed with exit \"",
            "                \"code %d\" % (\" \".join(cmd), ret)",
            "            )",
            "        self._trained = True",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    \"\"\"",
            "    A demonstration function to show how NLTK users can use the malt parser API.",
            "",
            "    >>> from nltk import pos_tag",
            "    >>> assert 'MALT_PARSER' in os.environ, str(",
            "    ... \"Please set MALT_PARSER in your global environment, e.g.:\\n\"",
            "    ... \"$ export MALT_PARSER='/home/user/maltparser-1.9.2/'\")",
            "    >>>",
            "    >>> assert 'MALT_MODEL' in os.environ, str(",
            "    ... \"Please set MALT_MODEL in your global environment, e.g.:\\n\"",
            "    ... \"$ export MALT_MODEL='/home/user/engmalt.linear-1.7.mco'\")",
            "    >>>",
            "    >>> _dg1_str = str(\"1    John    _    NNP   _    _    2    SUBJ    _    _\\n\"",
            "    ...             \"2    sees    _    VB    _    _    0    ROOT    _    _\\n\"",
            "    ...             \"3    a       _    DT    _    _    4    SPEC    _    _\\n\"",
            "    ...             \"4    dog     _    NN    _    _    2    OBJ     _    _\\n\"",
            "    ...             \"5    .     _    .    _    _    2    PUNCT     _    _\\n\")",
            "    >>>",
            "    >>>",
            "    >>> _dg2_str  = str(\"1    John    _    NNP   _    _    2    SUBJ    _    _\\n\"",
            "    ...             \"2    walks   _    VB    _    _    0    ROOT    _    _\\n\"",
            "    ...             \"3    .     _    .    _    _    2    PUNCT     _    _\\n\")",
            "    >>> dg1 = DependencyGraph(_dg1_str)",
            "    >>> dg2 = DependencyGraph(_dg2_str)",
            "    >>> # Initialize a MaltParser object",
            "    >>> mp = MaltParser()",
            "    >>>",
            "    >>> # Trains a model.",
            "    >>> mp.train([dg1,dg2], verbose=False)",
            "    >>> sent1 = ['John','sees','Mary', '.']",
            "    >>> sent2 = ['John', 'walks', 'a', 'dog', '.']",
            "    >>>",
            "    >>> # Parse a single sentence.",
            "    >>> parsed_sent1 = mp.parse_one(sent1)",
            "    >>> parsed_sent2 = mp.parse_one(sent2)",
            "    >>> print(parsed_sent1.tree())",
            "    (sees John Mary .)",
            "    >>> print(parsed_sent2.tree())",
            "    (walks John (dog a) .)",
            "    >>>",
            "    >>> # Parsing multiple sentences.",
            "    >>> sentences = [sent1,sent2]",
            "    >>> parsed_sents = mp.parse_sents(sentences)",
            "    >>> print(next(next(parsed_sents)).tree())",
            "    (sees John Mary .)",
            "    >>> print(next(next(parsed_sents)).tree())",
            "    (walks John (dog a) .)",
            "    >>>",
            "    >>> # Initialize a MaltParser object with an English pre-trained model.",
            "    >>> parser_dirname = 'maltparser-1.9.2'",
            "    >>> model_name = 'engmalt.linear-1.7.mco'",
            "    >>> mp = MaltParser(parser_dirname=parser_dirname, model_filename=model_name, tagger=pos_tag)",
            "    >>> sent1 = 'I shot an elephant in my pajamas .'.split()",
            "    >>> sent2 = 'Time flies like banana .'.split()",
            "    >>> # Parse a single sentence.",
            "    >>> print(mp.parse_one(sent1).tree())",
            "    (shot I (elephant an) (in (pajamas my)) .)",
            "    # Parsing multiple sentences",
            "    >>> sentences = [sent1,sent2]",
            "    >>> parsed_sents = mp.parse_sents(sentences)",
            "    >>> print(next(next(parsed_sents)).tree())",
            "    (shot I (elephant an) (in (pajamas my)) .)",
            "    >>> print(next(next(parsed_sents)).tree())",
            "    (flies Time (like banana) .)",
            "    \"\"\"",
            "",
            "    import doctest",
            "",
            "    doctest.testmod()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "35": [
                "malt_regex_tagger"
            ]
        },
        "addLocation": []
    },
    "nltk/sem/glue.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 703,
                "afterPatchRowNumber": 703,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 704,
                "afterPatchRowNumber": 704,
                "PatchRowcode": "         regexp_tagger = RegexpTagger("
            },
            "2": {
                "beforePatchRowNumber": 705,
                "afterPatchRowNumber": 705,
                "PatchRowcode": "             ["
            },
            "3": {
                "beforePatchRowNumber": 706,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 706,
                "PatchRowcode": "+                (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers"
            },
            "5": {
                "beforePatchRowNumber": 707,
                "afterPatchRowNumber": 707,
                "PatchRowcode": "                 (r\"(The|the|A|a|An|an)$\", \"AT\"),  # articles"
            },
            "6": {
                "beforePatchRowNumber": 708,
                "afterPatchRowNumber": 708,
                "PatchRowcode": "                 (r\".*able$\", \"JJ\"),  # adjectives"
            },
            "7": {
                "beforePatchRowNumber": 709,
                "afterPatchRowNumber": 709,
                "PatchRowcode": "                 (r\".*ness$\", \"NN\"),  # nouns formed from adjectives"
            }
        },
        "frontPatchFile": [
            "# Natural Language Toolkit: Glue Semantics",
            "#",
            "# Author: Dan Garrette <dhgarrette@gmail.com>",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see LICENSE.TXT",
            "",
            "import os",
            "from itertools import chain",
            "",
            "import nltk",
            "from nltk.internals import Counter",
            "from nltk.sem import drt, linearlogic",
            "from nltk.sem.logic import (",
            "    AbstractVariableExpression,",
            "    Expression,",
            "    LambdaExpression,",
            "    Variable,",
            "    VariableExpression,",
            ")",
            "from nltk.tag import BigramTagger, RegexpTagger, TrigramTagger, UnigramTagger",
            "",
            "SPEC_SEMTYPES = {",
            "    \"a\": \"ex_quant\",",
            "    \"an\": \"ex_quant\",",
            "    \"every\": \"univ_quant\",",
            "    \"the\": \"def_art\",",
            "    \"no\": \"no_quant\",",
            "    \"default\": \"ex_quant\",",
            "}",
            "",
            "OPTIONAL_RELATIONSHIPS = [\"nmod\", \"vmod\", \"punct\"]",
            "",
            "",
            "class GlueFormula:",
            "    def __init__(self, meaning, glue, indices=None):",
            "        if not indices:",
            "            indices = set()",
            "",
            "        if isinstance(meaning, str):",
            "            self.meaning = Expression.fromstring(meaning)",
            "        elif isinstance(meaning, Expression):",
            "            self.meaning = meaning",
            "        else:",
            "            raise RuntimeError(",
            "                \"Meaning term neither string or expression: %s, %s\"",
            "                % (meaning, meaning.__class__)",
            "            )",
            "",
            "        if isinstance(glue, str):",
            "            self.glue = linearlogic.LinearLogicParser().parse(glue)",
            "        elif isinstance(glue, linearlogic.Expression):",
            "            self.glue = glue",
            "        else:",
            "            raise RuntimeError(",
            "                \"Glue term neither string or expression: %s, %s\"",
            "                % (glue, glue.__class__)",
            "            )",
            "",
            "        self.indices = indices",
            "",
            "    def applyto(self, arg):",
            "        \"\"\"self = (\\\\x.(walk x), (subj -o f))",
            "        arg  = (john        ,  subj)",
            "        returns ((walk john),          f)",
            "        \"\"\"",
            "        if self.indices & arg.indices:  # if the sets are NOT disjoint",
            "            raise linearlogic.LinearLogicApplicationException(",
            "                f\"'{self}' applied to '{arg}'.  Indices are not disjoint.\"",
            "            )",
            "        else:  # if the sets ARE disjoint",
            "            return_indices = self.indices | arg.indices",
            "",
            "        try:",
            "            return_glue = linearlogic.ApplicationExpression(",
            "                self.glue, arg.glue, arg.indices",
            "            )",
            "        except linearlogic.LinearLogicApplicationException as e:",
            "            raise linearlogic.LinearLogicApplicationException(",
            "                f\"'{self.simplify()}' applied to '{arg.simplify()}'\"",
            "            ) from e",
            "",
            "        arg_meaning_abstracted = arg.meaning",
            "        if return_indices:",
            "            for dep in self.glue.simplify().antecedent.dependencies[",
            "                ::-1",
            "            ]:  # if self.glue is (A -o B), dep is in A.dependencies",
            "                arg_meaning_abstracted = self.make_LambdaExpression(",
            "                    Variable(\"v%s\" % dep), arg_meaning_abstracted",
            "                )",
            "        return_meaning = self.meaning.applyto(arg_meaning_abstracted)",
            "",
            "        return self.__class__(return_meaning, return_glue, return_indices)",
            "",
            "    def make_VariableExpression(self, name):",
            "        return VariableExpression(name)",
            "",
            "    def make_LambdaExpression(self, variable, term):",
            "        return LambdaExpression(variable, term)",
            "",
            "    def lambda_abstract(self, other):",
            "        assert isinstance(other, GlueFormula)",
            "        assert isinstance(other.meaning, AbstractVariableExpression)",
            "        return self.__class__(",
            "            self.make_LambdaExpression(other.meaning.variable, self.meaning),",
            "            linearlogic.ImpExpression(other.glue, self.glue),",
            "        )",
            "",
            "    def compile(self, counter=None):",
            "        \"\"\"From Iddo Lev's PhD Dissertation p108-109\"\"\"",
            "        if not counter:",
            "            counter = Counter()",
            "        (compiled_glue, new_forms) = self.glue.simplify().compile_pos(",
            "            counter, self.__class__",
            "        )",
            "        return new_forms + [",
            "            self.__class__(self.meaning, compiled_glue, {counter.get()})",
            "        ]",
            "",
            "    def simplify(self):",
            "        return self.__class__(",
            "            self.meaning.simplify(), self.glue.simplify(), self.indices",
            "        )",
            "",
            "    def __eq__(self, other):",
            "        return (",
            "            self.__class__ == other.__class__",
            "            and self.meaning == other.meaning",
            "            and self.glue == other.glue",
            "        )",
            "",
            "    def __ne__(self, other):",
            "        return not self == other",
            "",
            "    # sorting for use in doctests which must be deterministic",
            "    def __lt__(self, other):",
            "        return str(self) < str(other)",
            "",
            "    def __str__(self):",
            "        assert isinstance(self.indices, set)",
            "        accum = f\"{self.meaning} : {self.glue}\"",
            "        if self.indices:",
            "            accum += (",
            "                \" : {\" + \", \".join(str(index) for index in sorted(self.indices)) + \"}\"",
            "            )",
            "        return accum",
            "",
            "    def __repr__(self):",
            "        return \"%s\" % self",
            "",
            "",
            "class GlueDict(dict):",
            "    def __init__(self, filename, encoding=None):",
            "        self.filename = filename",
            "        self.file_encoding = encoding",
            "        self.read_file()",
            "",
            "    def read_file(self, empty_first=True):",
            "        if empty_first:",
            "            self.clear()",
            "",
            "        try:",
            "            contents = nltk.data.load(",
            "                self.filename, format=\"text\", encoding=self.file_encoding",
            "            )",
            "            # TODO: the above can't handle zip files, but this should anyway be fixed in nltk.data.load()",
            "        except LookupError as e:",
            "            try:",
            "                contents = nltk.data.load(",
            "                    \"file:\" + self.filename, format=\"text\", encoding=self.file_encoding",
            "                )",
            "            except LookupError:",
            "                raise e",
            "        lines = contents.splitlines()",
            "",
            "        for line in lines:  # example: 'n : (\\\\x.(<word> x), (v-or))'",
            "            #     lambdacalc -^  linear logic -^",
            "            line = line.strip()  # remove trailing newline",
            "            if not len(line):",
            "                continue  # skip empty lines",
            "            if line[0] == \"#\":",
            "                continue  # skip commented out lines",
            "",
            "            parts = line.split(",
            "                \" : \", 2",
            "            )  # ['verb', '(\\\\x.(<word> x), ( subj -o f ))', '[subj]']",
            "",
            "            glue_formulas = []",
            "            paren_count = 0",
            "            tuple_start = 0",
            "            tuple_comma = 0",
            "",
            "            relationships = None",
            "",
            "            if len(parts) > 1:",
            "                for (i, c) in enumerate(parts[1]):",
            "                    if c == \"(\":",
            "                        if paren_count == 0:  # if it's the first '(' of a tuple",
            "                            tuple_start = i + 1  # then save the index",
            "                        paren_count += 1",
            "                    elif c == \")\":",
            "                        paren_count -= 1",
            "                        if paren_count == 0:  # if it's the last ')' of a tuple",
            "                            meaning_term = parts[1][",
            "                                tuple_start:tuple_comma",
            "                            ]  # '\\\\x.(<word> x)'",
            "                            glue_term = parts[1][tuple_comma + 1 : i]  # '(v-r)'",
            "                            glue_formulas.append(",
            "                                [meaning_term, glue_term]",
            "                            )  # add the GlueFormula to the list",
            "                    elif c == \",\":",
            "                        if (",
            "                            paren_count == 1",
            "                        ):  # if it's a comma separating the parts of the tuple",
            "                            tuple_comma = i  # then save the index",
            "                    elif c == \"#\":  # skip comments at the ends of lines",
            "                        if (",
            "                            paren_count != 0",
            "                        ):  # if the line hasn't parsed correctly so far",
            "                            raise RuntimeError(",
            "                                \"Formula syntax is incorrect for entry \" + line",
            "                            )",
            "                        break  # break to the next line",
            "",
            "            if len(parts) > 2:  # if there is a relationship entry at the end",
            "                rel_start = parts[2].index(\"[\") + 1",
            "                rel_end = parts[2].index(\"]\")",
            "                if rel_start == rel_end:",
            "                    relationships = frozenset()",
            "                else:",
            "                    relationships = frozenset(",
            "                        r.strip() for r in parts[2][rel_start:rel_end].split(\",\")",
            "                    )",
            "",
            "            try:",
            "                start_inheritance = parts[0].index(\"(\")",
            "                end_inheritance = parts[0].index(\")\")",
            "                sem = parts[0][:start_inheritance].strip()",
            "                supertype = parts[0][start_inheritance + 1 : end_inheritance]",
            "            except:",
            "                sem = parts[0].strip()",
            "                supertype = None",
            "",
            "            if sem not in self:",
            "                self[sem] = {}",
            "",
            "            if (",
            "                relationships is None",
            "            ):  # if not specified for a specific relationship set",
            "                # add all relationship entries for parents",
            "                if supertype:",
            "                    for rels in self[supertype]:",
            "                        if rels not in self[sem]:",
            "                            self[sem][rels] = []",
            "                        glue = self[supertype][rels]",
            "                        self[sem][rels].extend(glue)",
            "                        self[sem][rels].extend(",
            "                            glue_formulas",
            "                        )  # add the glue formulas to every rel entry",
            "                else:",
            "                    if None not in self[sem]:",
            "                        self[sem][None] = []",
            "                    self[sem][None].extend(",
            "                        glue_formulas",
            "                    )  # add the glue formulas to every rel entry",
            "            else:",
            "                if relationships not in self[sem]:",
            "                    self[sem][relationships] = []",
            "                if supertype:",
            "                    self[sem][relationships].extend(self[supertype][relationships])",
            "                self[sem][relationships].extend(",
            "                    glue_formulas",
            "                )  # add the glue entry to the dictionary",
            "",
            "    def __str__(self):",
            "        accum = \"\"",
            "        for pos in self:",
            "            str_pos = \"%s\" % pos",
            "            for relset in self[pos]:",
            "                i = 1",
            "                for gf in self[pos][relset]:",
            "                    if i == 1:",
            "                        accum += str_pos + \": \"",
            "                    else:",
            "                        accum += \" \" * (len(str_pos) + 2)",
            "                    accum += \"%s\" % gf",
            "                    if relset and i == len(self[pos][relset]):",
            "                        accum += \" : %s\" % relset",
            "                    accum += \"\\n\"",
            "                    i += 1",
            "        return accum",
            "",
            "    def to_glueformula_list(self, depgraph, node=None, counter=None, verbose=False):",
            "        if node is None:",
            "            # TODO: should it be depgraph.root? Is this code tested?",
            "            top = depgraph.nodes[0]",
            "            depList = list(chain.from_iterable(top[\"deps\"].values()))",
            "            root = depgraph.nodes[depList[0]]",
            "",
            "            return self.to_glueformula_list(depgraph, root, Counter(), verbose)",
            "",
            "        glueformulas = self.lookup(node, depgraph, counter)",
            "        for dep_idx in chain.from_iterable(node[\"deps\"].values()):",
            "            dep = depgraph.nodes[dep_idx]",
            "            glueformulas.extend(",
            "                self.to_glueformula_list(depgraph, dep, counter, verbose)",
            "            )",
            "        return glueformulas",
            "",
            "    def lookup(self, node, depgraph, counter):",
            "        semtype_names = self.get_semtypes(node)",
            "",
            "        semtype = None",
            "        for name in semtype_names:",
            "            if name in self:",
            "                semtype = self[name]",
            "                break",
            "        if semtype is None:",
            "            # raise KeyError, \"There is no GlueDict entry for sem type '%s' (for '%s')\" % (sem, word)",
            "            return []",
            "",
            "        self.add_missing_dependencies(node, depgraph)",
            "",
            "        lookup = self._lookup_semtype_option(semtype, node, depgraph)",
            "",
            "        if not len(lookup):",
            "            raise KeyError(",
            "                \"There is no GlueDict entry for sem type of '%s' \"",
            "                \"with tag '%s', and rel '%s'\" % (node[\"word\"], node[\"tag\"], node[\"rel\"])",
            "            )",
            "",
            "        return self.get_glueformulas_from_semtype_entry(",
            "            lookup, node[\"word\"], node, depgraph, counter",
            "        )",
            "",
            "    def add_missing_dependencies(self, node, depgraph):",
            "        rel = node[\"rel\"].lower()",
            "",
            "        if rel == \"main\":",
            "            headnode = depgraph.nodes[node[\"head\"]]",
            "            subj = self.lookup_unique(\"subj\", headnode, depgraph)",
            "            relation = subj[\"rel\"]",
            "            node[\"deps\"].setdefault(relation, [])",
            "            node[\"deps\"][relation].append(subj[\"address\"])",
            "            # node['deps'].append(subj['address'])",
            "",
            "    def _lookup_semtype_option(self, semtype, node, depgraph):",
            "        relationships = frozenset(",
            "            depgraph.nodes[dep][\"rel\"].lower()",
            "            for dep in chain.from_iterable(node[\"deps\"].values())",
            "            if depgraph.nodes[dep][\"rel\"].lower() not in OPTIONAL_RELATIONSHIPS",
            "        )",
            "",
            "        try:",
            "            lookup = semtype[relationships]",
            "        except KeyError:",
            "            # An exact match is not found, so find the best match where",
            "            # 'best' is defined as the glue entry whose relationship set has the",
            "            # most relations of any possible relationship set that is a subset",
            "            # of the actual depgraph",
            "            best_match = frozenset()",
            "            for relset_option in set(semtype) - {None}:",
            "                if (",
            "                    len(relset_option) > len(best_match)",
            "                    and relset_option < relationships",
            "                ):",
            "                    best_match = relset_option",
            "            if not best_match:",
            "                if None in semtype:",
            "                    best_match = None",
            "                else:",
            "                    return None",
            "            lookup = semtype[best_match]",
            "",
            "        return lookup",
            "",
            "    def get_semtypes(self, node):",
            "        \"\"\"",
            "        Based on the node, return a list of plausible semtypes in order of",
            "        plausibility.",
            "        \"\"\"",
            "        rel = node[\"rel\"].lower()",
            "        word = node[\"word\"].lower()",
            "",
            "        if rel == \"spec\":",
            "            if word in SPEC_SEMTYPES:",
            "                return [SPEC_SEMTYPES[word]]",
            "            else:",
            "                return [SPEC_SEMTYPES[\"default\"]]",
            "        elif rel in [\"nmod\", \"vmod\"]:",
            "            return [node[\"tag\"], rel]",
            "        else:",
            "            return [node[\"tag\"]]",
            "",
            "    def get_glueformulas_from_semtype_entry(",
            "        self, lookup, word, node, depgraph, counter",
            "    ):",
            "        glueformulas = []",
            "",
            "        glueFormulaFactory = self.get_GlueFormula_factory()",
            "        for meaning, glue in lookup:",
            "            gf = glueFormulaFactory(self.get_meaning_formula(meaning, word), glue)",
            "            if not len(glueformulas):",
            "                gf.word = word",
            "            else:",
            "                gf.word = f\"{word}{len(glueformulas) + 1}\"",
            "",
            "            gf.glue = self.initialize_labels(gf.glue, node, depgraph, counter.get())",
            "",
            "            glueformulas.append(gf)",
            "        return glueformulas",
            "",
            "    def get_meaning_formula(self, generic, word):",
            "        \"\"\"",
            "        :param generic: A meaning formula string containing the",
            "            parameter \"<word>\"",
            "        :param word: The actual word to be replace \"<word>\"",
            "        \"\"\"",
            "        word = word.replace(\".\", \"\")",
            "        return generic.replace(\"<word>\", word)",
            "",
            "    def initialize_labels(self, expr, node, depgraph, unique_index):",
            "        if isinstance(expr, linearlogic.AtomicExpression):",
            "            name = self.find_label_name(expr.name.lower(), node, depgraph, unique_index)",
            "            if name[0].isupper():",
            "                return linearlogic.VariableExpression(name)",
            "            else:",
            "                return linearlogic.ConstantExpression(name)",
            "        else:",
            "            return linearlogic.ImpExpression(",
            "                self.initialize_labels(expr.antecedent, node, depgraph, unique_index),",
            "                self.initialize_labels(expr.consequent, node, depgraph, unique_index),",
            "            )",
            "",
            "    def find_label_name(self, name, node, depgraph, unique_index):",
            "        try:",
            "            dot = name.index(\".\")",
            "",
            "            before_dot = name[:dot]",
            "            after_dot = name[dot + 1 :]",
            "            if before_dot == \"super\":",
            "                return self.find_label_name(",
            "                    after_dot, depgraph.nodes[node[\"head\"]], depgraph, unique_index",
            "                )",
            "            else:",
            "                return self.find_label_name(",
            "                    after_dot,",
            "                    self.lookup_unique(before_dot, node, depgraph),",
            "                    depgraph,",
            "                    unique_index,",
            "                )",
            "        except ValueError:",
            "            lbl = self.get_label(node)",
            "            if name == \"f\":",
            "                return lbl",
            "            elif name == \"v\":",
            "                return \"%sv\" % lbl",
            "            elif name == \"r\":",
            "                return \"%sr\" % lbl",
            "            elif name == \"super\":",
            "                return self.get_label(depgraph.nodes[node[\"head\"]])",
            "            elif name == \"var\":",
            "                return f\"{lbl.upper()}{unique_index}\"",
            "            elif name == \"a\":",
            "                return self.get_label(self.lookup_unique(\"conja\", node, depgraph))",
            "            elif name == \"b\":",
            "                return self.get_label(self.lookup_unique(\"conjb\", node, depgraph))",
            "            else:",
            "                return self.get_label(self.lookup_unique(name, node, depgraph))",
            "",
            "    def get_label(self, node):",
            "        \"\"\"",
            "        Pick an alphabetic character as identifier for an entity in the model.",
            "",
            "        :param value: where to index into the list of characters",
            "        :type value: int",
            "        \"\"\"",
            "        value = node[\"address\"]",
            "",
            "        letter = [",
            "            \"f\",",
            "            \"g\",",
            "            \"h\",",
            "            \"i\",",
            "            \"j\",",
            "            \"k\",",
            "            \"l\",",
            "            \"m\",",
            "            \"n\",",
            "            \"o\",",
            "            \"p\",",
            "            \"q\",",
            "            \"r\",",
            "            \"s\",",
            "            \"t\",",
            "            \"u\",",
            "            \"v\",",
            "            \"w\",",
            "            \"x\",",
            "            \"y\",",
            "            \"z\",",
            "            \"a\",",
            "            \"b\",",
            "            \"c\",",
            "            \"d\",",
            "            \"e\",",
            "        ][value - 1]",
            "        num = int(value) // 26",
            "        if num > 0:",
            "            return letter + str(num)",
            "        else:",
            "            return letter",
            "",
            "    def lookup_unique(self, rel, node, depgraph):",
            "        \"\"\"",
            "        Lookup 'key'. There should be exactly one item in the associated relation.",
            "        \"\"\"",
            "        deps = [",
            "            depgraph.nodes[dep]",
            "            for dep in chain.from_iterable(node[\"deps\"].values())",
            "            if depgraph.nodes[dep][\"rel\"].lower() == rel.lower()",
            "        ]",
            "",
            "        if len(deps) == 0:",
            "            raise KeyError(",
            "                \"'{}' doesn't contain a feature '{}'\".format(node[\"word\"], rel)",
            "            )",
            "        elif len(deps) > 1:",
            "            raise KeyError(",
            "                \"'{}' should only have one feature '{}'\".format(node[\"word\"], rel)",
            "            )",
            "        else:",
            "            return deps[0]",
            "",
            "    def get_GlueFormula_factory(self):",
            "        return GlueFormula",
            "",
            "",
            "class Glue:",
            "    def __init__(",
            "        self, semtype_file=None, remove_duplicates=False, depparser=None, verbose=False",
            "    ):",
            "        self.verbose = verbose",
            "        self.remove_duplicates = remove_duplicates",
            "        self.depparser = depparser",
            "",
            "        from nltk import Prover9",
            "",
            "        self.prover = Prover9()",
            "",
            "        if semtype_file:",
            "            self.semtype_file = semtype_file",
            "        else:",
            "            self.semtype_file = os.path.join(",
            "                \"grammars\", \"sample_grammars\", \"glue.semtype\"",
            "            )",
            "",
            "    def train_depparser(self, depgraphs=None):",
            "        if depgraphs:",
            "            self.depparser.train(depgraphs)",
            "        else:",
            "            self.depparser.train_from_file(",
            "                nltk.data.find(",
            "                    os.path.join(\"grammars\", \"sample_grammars\", \"glue_train.conll\")",
            "                )",
            "            )",
            "",
            "    def parse_to_meaning(self, sentence):",
            "        readings = []",
            "        for agenda in self.parse_to_compiled(sentence):",
            "            readings.extend(self.get_readings(agenda))",
            "        return readings",
            "",
            "    def get_readings(self, agenda):",
            "        readings = []",
            "        agenda_length = len(agenda)",
            "        atomics = dict()",
            "        nonatomics = dict()",
            "        while agenda:  # is not empty",
            "            cur = agenda.pop()",
            "            glue_simp = cur.glue.simplify()",
            "            if isinstance(",
            "                glue_simp, linearlogic.ImpExpression",
            "            ):  # if cur.glue is non-atomic",
            "                for key in atomics:",
            "                    try:",
            "                        if isinstance(cur.glue, linearlogic.ApplicationExpression):",
            "                            bindings = cur.glue.bindings",
            "                        else:",
            "                            bindings = linearlogic.BindingDict()",
            "                        glue_simp.antecedent.unify(key, bindings)",
            "                        for atomic in atomics[key]:",
            "                            if not (",
            "                                cur.indices & atomic.indices",
            "                            ):  # if the sets of indices are disjoint",
            "                                try:",
            "                                    agenda.append(cur.applyto(atomic))",
            "                                except linearlogic.LinearLogicApplicationException:",
            "                                    pass",
            "                    except linearlogic.UnificationException:",
            "                        pass",
            "                try:",
            "                    nonatomics[glue_simp.antecedent].append(cur)",
            "                except KeyError:",
            "                    nonatomics[glue_simp.antecedent] = [cur]",
            "",
            "            else:  # else cur.glue is atomic",
            "                for key in nonatomics:",
            "                    for nonatomic in nonatomics[key]:",
            "                        try:",
            "                            if isinstance(",
            "                                nonatomic.glue, linearlogic.ApplicationExpression",
            "                            ):",
            "                                bindings = nonatomic.glue.bindings",
            "                            else:",
            "                                bindings = linearlogic.BindingDict()",
            "                            glue_simp.unify(key, bindings)",
            "                            if not (",
            "                                cur.indices & nonatomic.indices",
            "                            ):  # if the sets of indices are disjoint",
            "                                try:",
            "                                    agenda.append(nonatomic.applyto(cur))",
            "                                except linearlogic.LinearLogicApplicationException:",
            "                                    pass",
            "                        except linearlogic.UnificationException:",
            "                            pass",
            "                try:",
            "                    atomics[glue_simp].append(cur)",
            "                except KeyError:",
            "                    atomics[glue_simp] = [cur]",
            "",
            "        for entry in atomics:",
            "            for gf in atomics[entry]:",
            "                if len(gf.indices) == agenda_length:",
            "                    self._add_to_reading_list(gf, readings)",
            "        for entry in nonatomics:",
            "            for gf in nonatomics[entry]:",
            "                if len(gf.indices) == agenda_length:",
            "                    self._add_to_reading_list(gf, readings)",
            "        return readings",
            "",
            "    def _add_to_reading_list(self, glueformula, reading_list):",
            "        add_reading = True",
            "        if self.remove_duplicates:",
            "            for reading in reading_list:",
            "                try:",
            "                    if reading.equiv(glueformula.meaning, self.prover):",
            "                        add_reading = False",
            "                        break",
            "                except Exception as e:",
            "                    # if there is an exception, the syntax of the formula",
            "                    # may not be understandable by the prover, so don't",
            "                    # throw out the reading.",
            "                    print(\"Error when checking logical equality of statements\", e)",
            "",
            "        if add_reading:",
            "            reading_list.append(glueformula.meaning)",
            "",
            "    def parse_to_compiled(self, sentence):",
            "        gfls = [self.depgraph_to_glue(dg) for dg in self.dep_parse(sentence)]",
            "        return [self.gfl_to_compiled(gfl) for gfl in gfls]",
            "",
            "    def dep_parse(self, sentence):",
            "        \"\"\"",
            "        Return a dependency graph for the sentence.",
            "",
            "        :param sentence: the sentence to be parsed",
            "        :type sentence: list(str)",
            "        :rtype: DependencyGraph",
            "        \"\"\"",
            "",
            "        # Lazy-initialize the depparser",
            "        if self.depparser is None:",
            "            from nltk.parse import MaltParser",
            "",
            "            self.depparser = MaltParser(tagger=self.get_pos_tagger())",
            "        if not self.depparser._trained:",
            "            self.train_depparser()",
            "        return self.depparser.parse(sentence, verbose=self.verbose)",
            "",
            "    def depgraph_to_glue(self, depgraph):",
            "        return self.get_glue_dict().to_glueformula_list(depgraph)",
            "",
            "    def get_glue_dict(self):",
            "        return GlueDict(self.semtype_file)",
            "",
            "    def gfl_to_compiled(self, gfl):",
            "        index_counter = Counter()",
            "        return_list = []",
            "        for gf in gfl:",
            "            return_list.extend(gf.compile(index_counter))",
            "",
            "        if self.verbose:",
            "            print(\"Compiled Glue Premises:\")",
            "            for cgf in return_list:",
            "                print(cgf)",
            "",
            "        return return_list",
            "",
            "    def get_pos_tagger(self):",
            "        from nltk.corpus import brown",
            "",
            "        regexp_tagger = RegexpTagger(",
            "            [",
            "                (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers",
            "                (r\"(The|the|A|a|An|an)$\", \"AT\"),  # articles",
            "                (r\".*able$\", \"JJ\"),  # adjectives",
            "                (r\".*ness$\", \"NN\"),  # nouns formed from adjectives",
            "                (r\".*ly$\", \"RB\"),  # adverbs",
            "                (r\".*s$\", \"NNS\"),  # plural nouns",
            "                (r\".*ing$\", \"VBG\"),  # gerunds",
            "                (r\".*ed$\", \"VBD\"),  # past tense verbs",
            "                (r\".*\", \"NN\"),  # nouns (default)",
            "            ]",
            "        )",
            "        brown_train = brown.tagged_sents(categories=\"news\")",
            "        unigram_tagger = UnigramTagger(brown_train, backoff=regexp_tagger)",
            "        bigram_tagger = BigramTagger(brown_train, backoff=unigram_tagger)",
            "        trigram_tagger = TrigramTagger(brown_train, backoff=bigram_tagger)",
            "",
            "        # Override particular words",
            "        main_tagger = RegexpTagger(",
            "            [(r\"(A|a|An|an)$\", \"ex_quant\"), (r\"(Every|every|All|all)$\", \"univ_quant\")],",
            "            backoff=trigram_tagger,",
            "        )",
            "",
            "        return main_tagger",
            "",
            "",
            "class DrtGlueFormula(GlueFormula):",
            "    def __init__(self, meaning, glue, indices=None):",
            "        if not indices:",
            "            indices = set()",
            "",
            "        if isinstance(meaning, str):",
            "            self.meaning = drt.DrtExpression.fromstring(meaning)",
            "        elif isinstance(meaning, drt.DrtExpression):",
            "            self.meaning = meaning",
            "        else:",
            "            raise RuntimeError(",
            "                \"Meaning term neither string or expression: %s, %s\"",
            "                % (meaning, meaning.__class__)",
            "            )",
            "",
            "        if isinstance(glue, str):",
            "            self.glue = linearlogic.LinearLogicParser().parse(glue)",
            "        elif isinstance(glue, linearlogic.Expression):",
            "            self.glue = glue",
            "        else:",
            "            raise RuntimeError(",
            "                \"Glue term neither string or expression: %s, %s\"",
            "                % (glue, glue.__class__)",
            "            )",
            "",
            "        self.indices = indices",
            "",
            "    def make_VariableExpression(self, name):",
            "        return drt.DrtVariableExpression(name)",
            "",
            "    def make_LambdaExpression(self, variable, term):",
            "        return drt.DrtLambdaExpression(variable, term)",
            "",
            "",
            "class DrtGlueDict(GlueDict):",
            "    def get_GlueFormula_factory(self):",
            "        return DrtGlueFormula",
            "",
            "",
            "class DrtGlue(Glue):",
            "    def __init__(",
            "        self, semtype_file=None, remove_duplicates=False, depparser=None, verbose=False",
            "    ):",
            "        if not semtype_file:",
            "            semtype_file = os.path.join(",
            "                \"grammars\", \"sample_grammars\", \"drt_glue.semtype\"",
            "            )",
            "        Glue.__init__(self, semtype_file, remove_duplicates, depparser, verbose)",
            "",
            "    def get_glue_dict(self):",
            "        return DrtGlueDict(self.semtype_file)",
            "",
            "",
            "def demo(show_example=-1):",
            "    from nltk.parse import MaltParser",
            "",
            "    examples = [",
            "        \"David sees Mary\",",
            "        \"David eats a sandwich\",",
            "        \"every man chases a dog\",",
            "        \"every man believes a dog sleeps\",",
            "        \"John gives David a sandwich\",",
            "        \"John chases himself\",",
            "    ]",
            "    #                'John persuades David to order a pizza',",
            "    #                'John tries to go',",
            "    #                'John tries to find a unicorn',",
            "    #                'John seems to vanish',",
            "    #                'a unicorn seems to approach',",
            "    #                'every big cat leaves',",
            "    #                'every gray cat leaves',",
            "    #                'every big gray cat leaves',",
            "    #                'a former senator leaves',",
            "",
            "    print(\"============== DEMO ==============\")",
            "",
            "    tagger = RegexpTagger(",
            "        [",
            "            (\"^(David|Mary|John)$\", \"NNP\"),",
            "            (",
            "                \"^(sees|eats|chases|believes|gives|sleeps|chases|persuades|tries|seems|leaves)$\",",
            "                \"VB\",",
            "            ),",
            "            (\"^(go|order|vanish|find|approach)$\", \"VB\"),",
            "            (\"^(a)$\", \"ex_quant\"),",
            "            (\"^(every)$\", \"univ_quant\"),",
            "            (\"^(sandwich|man|dog|pizza|unicorn|cat|senator)$\", \"NN\"),",
            "            (\"^(big|gray|former)$\", \"JJ\"),",
            "            (\"^(him|himself)$\", \"PRP\"),",
            "        ]",
            "    )",
            "",
            "    depparser = MaltParser(tagger=tagger)",
            "    glue = Glue(depparser=depparser, verbose=False)",
            "",
            "    for (i, sentence) in enumerate(examples):",
            "        if i == show_example or show_example == -1:",
            "            print(f\"[[[Example {i}]]]  {sentence}\")",
            "            for reading in glue.parse_to_meaning(sentence.split()):",
            "                print(reading.simplify())",
            "            print(\"\")",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    demo()"
        ],
        "afterPatchFile": [
            "# Natural Language Toolkit: Glue Semantics",
            "#",
            "# Author: Dan Garrette <dhgarrette@gmail.com>",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see LICENSE.TXT",
            "",
            "import os",
            "from itertools import chain",
            "",
            "import nltk",
            "from nltk.internals import Counter",
            "from nltk.sem import drt, linearlogic",
            "from nltk.sem.logic import (",
            "    AbstractVariableExpression,",
            "    Expression,",
            "    LambdaExpression,",
            "    Variable,",
            "    VariableExpression,",
            ")",
            "from nltk.tag import BigramTagger, RegexpTagger, TrigramTagger, UnigramTagger",
            "",
            "SPEC_SEMTYPES = {",
            "    \"a\": \"ex_quant\",",
            "    \"an\": \"ex_quant\",",
            "    \"every\": \"univ_quant\",",
            "    \"the\": \"def_art\",",
            "    \"no\": \"no_quant\",",
            "    \"default\": \"ex_quant\",",
            "}",
            "",
            "OPTIONAL_RELATIONSHIPS = [\"nmod\", \"vmod\", \"punct\"]",
            "",
            "",
            "class GlueFormula:",
            "    def __init__(self, meaning, glue, indices=None):",
            "        if not indices:",
            "            indices = set()",
            "",
            "        if isinstance(meaning, str):",
            "            self.meaning = Expression.fromstring(meaning)",
            "        elif isinstance(meaning, Expression):",
            "            self.meaning = meaning",
            "        else:",
            "            raise RuntimeError(",
            "                \"Meaning term neither string or expression: %s, %s\"",
            "                % (meaning, meaning.__class__)",
            "            )",
            "",
            "        if isinstance(glue, str):",
            "            self.glue = linearlogic.LinearLogicParser().parse(glue)",
            "        elif isinstance(glue, linearlogic.Expression):",
            "            self.glue = glue",
            "        else:",
            "            raise RuntimeError(",
            "                \"Glue term neither string or expression: %s, %s\"",
            "                % (glue, glue.__class__)",
            "            )",
            "",
            "        self.indices = indices",
            "",
            "    def applyto(self, arg):",
            "        \"\"\"self = (\\\\x.(walk x), (subj -o f))",
            "        arg  = (john        ,  subj)",
            "        returns ((walk john),          f)",
            "        \"\"\"",
            "        if self.indices & arg.indices:  # if the sets are NOT disjoint",
            "            raise linearlogic.LinearLogicApplicationException(",
            "                f\"'{self}' applied to '{arg}'.  Indices are not disjoint.\"",
            "            )",
            "        else:  # if the sets ARE disjoint",
            "            return_indices = self.indices | arg.indices",
            "",
            "        try:",
            "            return_glue = linearlogic.ApplicationExpression(",
            "                self.glue, arg.glue, arg.indices",
            "            )",
            "        except linearlogic.LinearLogicApplicationException as e:",
            "            raise linearlogic.LinearLogicApplicationException(",
            "                f\"'{self.simplify()}' applied to '{arg.simplify()}'\"",
            "            ) from e",
            "",
            "        arg_meaning_abstracted = arg.meaning",
            "        if return_indices:",
            "            for dep in self.glue.simplify().antecedent.dependencies[",
            "                ::-1",
            "            ]:  # if self.glue is (A -o B), dep is in A.dependencies",
            "                arg_meaning_abstracted = self.make_LambdaExpression(",
            "                    Variable(\"v%s\" % dep), arg_meaning_abstracted",
            "                )",
            "        return_meaning = self.meaning.applyto(arg_meaning_abstracted)",
            "",
            "        return self.__class__(return_meaning, return_glue, return_indices)",
            "",
            "    def make_VariableExpression(self, name):",
            "        return VariableExpression(name)",
            "",
            "    def make_LambdaExpression(self, variable, term):",
            "        return LambdaExpression(variable, term)",
            "",
            "    def lambda_abstract(self, other):",
            "        assert isinstance(other, GlueFormula)",
            "        assert isinstance(other.meaning, AbstractVariableExpression)",
            "        return self.__class__(",
            "            self.make_LambdaExpression(other.meaning.variable, self.meaning),",
            "            linearlogic.ImpExpression(other.glue, self.glue),",
            "        )",
            "",
            "    def compile(self, counter=None):",
            "        \"\"\"From Iddo Lev's PhD Dissertation p108-109\"\"\"",
            "        if not counter:",
            "            counter = Counter()",
            "        (compiled_glue, new_forms) = self.glue.simplify().compile_pos(",
            "            counter, self.__class__",
            "        )",
            "        return new_forms + [",
            "            self.__class__(self.meaning, compiled_glue, {counter.get()})",
            "        ]",
            "",
            "    def simplify(self):",
            "        return self.__class__(",
            "            self.meaning.simplify(), self.glue.simplify(), self.indices",
            "        )",
            "",
            "    def __eq__(self, other):",
            "        return (",
            "            self.__class__ == other.__class__",
            "            and self.meaning == other.meaning",
            "            and self.glue == other.glue",
            "        )",
            "",
            "    def __ne__(self, other):",
            "        return not self == other",
            "",
            "    # sorting for use in doctests which must be deterministic",
            "    def __lt__(self, other):",
            "        return str(self) < str(other)",
            "",
            "    def __str__(self):",
            "        assert isinstance(self.indices, set)",
            "        accum = f\"{self.meaning} : {self.glue}\"",
            "        if self.indices:",
            "            accum += (",
            "                \" : {\" + \", \".join(str(index) for index in sorted(self.indices)) + \"}\"",
            "            )",
            "        return accum",
            "",
            "    def __repr__(self):",
            "        return \"%s\" % self",
            "",
            "",
            "class GlueDict(dict):",
            "    def __init__(self, filename, encoding=None):",
            "        self.filename = filename",
            "        self.file_encoding = encoding",
            "        self.read_file()",
            "",
            "    def read_file(self, empty_first=True):",
            "        if empty_first:",
            "            self.clear()",
            "",
            "        try:",
            "            contents = nltk.data.load(",
            "                self.filename, format=\"text\", encoding=self.file_encoding",
            "            )",
            "            # TODO: the above can't handle zip files, but this should anyway be fixed in nltk.data.load()",
            "        except LookupError as e:",
            "            try:",
            "                contents = nltk.data.load(",
            "                    \"file:\" + self.filename, format=\"text\", encoding=self.file_encoding",
            "                )",
            "            except LookupError:",
            "                raise e",
            "        lines = contents.splitlines()",
            "",
            "        for line in lines:  # example: 'n : (\\\\x.(<word> x), (v-or))'",
            "            #     lambdacalc -^  linear logic -^",
            "            line = line.strip()  # remove trailing newline",
            "            if not len(line):",
            "                continue  # skip empty lines",
            "            if line[0] == \"#\":",
            "                continue  # skip commented out lines",
            "",
            "            parts = line.split(",
            "                \" : \", 2",
            "            )  # ['verb', '(\\\\x.(<word> x), ( subj -o f ))', '[subj]']",
            "",
            "            glue_formulas = []",
            "            paren_count = 0",
            "            tuple_start = 0",
            "            tuple_comma = 0",
            "",
            "            relationships = None",
            "",
            "            if len(parts) > 1:",
            "                for (i, c) in enumerate(parts[1]):",
            "                    if c == \"(\":",
            "                        if paren_count == 0:  # if it's the first '(' of a tuple",
            "                            tuple_start = i + 1  # then save the index",
            "                        paren_count += 1",
            "                    elif c == \")\":",
            "                        paren_count -= 1",
            "                        if paren_count == 0:  # if it's the last ')' of a tuple",
            "                            meaning_term = parts[1][",
            "                                tuple_start:tuple_comma",
            "                            ]  # '\\\\x.(<word> x)'",
            "                            glue_term = parts[1][tuple_comma + 1 : i]  # '(v-r)'",
            "                            glue_formulas.append(",
            "                                [meaning_term, glue_term]",
            "                            )  # add the GlueFormula to the list",
            "                    elif c == \",\":",
            "                        if (",
            "                            paren_count == 1",
            "                        ):  # if it's a comma separating the parts of the tuple",
            "                            tuple_comma = i  # then save the index",
            "                    elif c == \"#\":  # skip comments at the ends of lines",
            "                        if (",
            "                            paren_count != 0",
            "                        ):  # if the line hasn't parsed correctly so far",
            "                            raise RuntimeError(",
            "                                \"Formula syntax is incorrect for entry \" + line",
            "                            )",
            "                        break  # break to the next line",
            "",
            "            if len(parts) > 2:  # if there is a relationship entry at the end",
            "                rel_start = parts[2].index(\"[\") + 1",
            "                rel_end = parts[2].index(\"]\")",
            "                if rel_start == rel_end:",
            "                    relationships = frozenset()",
            "                else:",
            "                    relationships = frozenset(",
            "                        r.strip() for r in parts[2][rel_start:rel_end].split(\",\")",
            "                    )",
            "",
            "            try:",
            "                start_inheritance = parts[0].index(\"(\")",
            "                end_inheritance = parts[0].index(\")\")",
            "                sem = parts[0][:start_inheritance].strip()",
            "                supertype = parts[0][start_inheritance + 1 : end_inheritance]",
            "            except:",
            "                sem = parts[0].strip()",
            "                supertype = None",
            "",
            "            if sem not in self:",
            "                self[sem] = {}",
            "",
            "            if (",
            "                relationships is None",
            "            ):  # if not specified for a specific relationship set",
            "                # add all relationship entries for parents",
            "                if supertype:",
            "                    for rels in self[supertype]:",
            "                        if rels not in self[sem]:",
            "                            self[sem][rels] = []",
            "                        glue = self[supertype][rels]",
            "                        self[sem][rels].extend(glue)",
            "                        self[sem][rels].extend(",
            "                            glue_formulas",
            "                        )  # add the glue formulas to every rel entry",
            "                else:",
            "                    if None not in self[sem]:",
            "                        self[sem][None] = []",
            "                    self[sem][None].extend(",
            "                        glue_formulas",
            "                    )  # add the glue formulas to every rel entry",
            "            else:",
            "                if relationships not in self[sem]:",
            "                    self[sem][relationships] = []",
            "                if supertype:",
            "                    self[sem][relationships].extend(self[supertype][relationships])",
            "                self[sem][relationships].extend(",
            "                    glue_formulas",
            "                )  # add the glue entry to the dictionary",
            "",
            "    def __str__(self):",
            "        accum = \"\"",
            "        for pos in self:",
            "            str_pos = \"%s\" % pos",
            "            for relset in self[pos]:",
            "                i = 1",
            "                for gf in self[pos][relset]:",
            "                    if i == 1:",
            "                        accum += str_pos + \": \"",
            "                    else:",
            "                        accum += \" \" * (len(str_pos) + 2)",
            "                    accum += \"%s\" % gf",
            "                    if relset and i == len(self[pos][relset]):",
            "                        accum += \" : %s\" % relset",
            "                    accum += \"\\n\"",
            "                    i += 1",
            "        return accum",
            "",
            "    def to_glueformula_list(self, depgraph, node=None, counter=None, verbose=False):",
            "        if node is None:",
            "            # TODO: should it be depgraph.root? Is this code tested?",
            "            top = depgraph.nodes[0]",
            "            depList = list(chain.from_iterable(top[\"deps\"].values()))",
            "            root = depgraph.nodes[depList[0]]",
            "",
            "            return self.to_glueformula_list(depgraph, root, Counter(), verbose)",
            "",
            "        glueformulas = self.lookup(node, depgraph, counter)",
            "        for dep_idx in chain.from_iterable(node[\"deps\"].values()):",
            "            dep = depgraph.nodes[dep_idx]",
            "            glueformulas.extend(",
            "                self.to_glueformula_list(depgraph, dep, counter, verbose)",
            "            )",
            "        return glueformulas",
            "",
            "    def lookup(self, node, depgraph, counter):",
            "        semtype_names = self.get_semtypes(node)",
            "",
            "        semtype = None",
            "        for name in semtype_names:",
            "            if name in self:",
            "                semtype = self[name]",
            "                break",
            "        if semtype is None:",
            "            # raise KeyError, \"There is no GlueDict entry for sem type '%s' (for '%s')\" % (sem, word)",
            "            return []",
            "",
            "        self.add_missing_dependencies(node, depgraph)",
            "",
            "        lookup = self._lookup_semtype_option(semtype, node, depgraph)",
            "",
            "        if not len(lookup):",
            "            raise KeyError(",
            "                \"There is no GlueDict entry for sem type of '%s' \"",
            "                \"with tag '%s', and rel '%s'\" % (node[\"word\"], node[\"tag\"], node[\"rel\"])",
            "            )",
            "",
            "        return self.get_glueformulas_from_semtype_entry(",
            "            lookup, node[\"word\"], node, depgraph, counter",
            "        )",
            "",
            "    def add_missing_dependencies(self, node, depgraph):",
            "        rel = node[\"rel\"].lower()",
            "",
            "        if rel == \"main\":",
            "            headnode = depgraph.nodes[node[\"head\"]]",
            "            subj = self.lookup_unique(\"subj\", headnode, depgraph)",
            "            relation = subj[\"rel\"]",
            "            node[\"deps\"].setdefault(relation, [])",
            "            node[\"deps\"][relation].append(subj[\"address\"])",
            "            # node['deps'].append(subj['address'])",
            "",
            "    def _lookup_semtype_option(self, semtype, node, depgraph):",
            "        relationships = frozenset(",
            "            depgraph.nodes[dep][\"rel\"].lower()",
            "            for dep in chain.from_iterable(node[\"deps\"].values())",
            "            if depgraph.nodes[dep][\"rel\"].lower() not in OPTIONAL_RELATIONSHIPS",
            "        )",
            "",
            "        try:",
            "            lookup = semtype[relationships]",
            "        except KeyError:",
            "            # An exact match is not found, so find the best match where",
            "            # 'best' is defined as the glue entry whose relationship set has the",
            "            # most relations of any possible relationship set that is a subset",
            "            # of the actual depgraph",
            "            best_match = frozenset()",
            "            for relset_option in set(semtype) - {None}:",
            "                if (",
            "                    len(relset_option) > len(best_match)",
            "                    and relset_option < relationships",
            "                ):",
            "                    best_match = relset_option",
            "            if not best_match:",
            "                if None in semtype:",
            "                    best_match = None",
            "                else:",
            "                    return None",
            "            lookup = semtype[best_match]",
            "",
            "        return lookup",
            "",
            "    def get_semtypes(self, node):",
            "        \"\"\"",
            "        Based on the node, return a list of plausible semtypes in order of",
            "        plausibility.",
            "        \"\"\"",
            "        rel = node[\"rel\"].lower()",
            "        word = node[\"word\"].lower()",
            "",
            "        if rel == \"spec\":",
            "            if word in SPEC_SEMTYPES:",
            "                return [SPEC_SEMTYPES[word]]",
            "            else:",
            "                return [SPEC_SEMTYPES[\"default\"]]",
            "        elif rel in [\"nmod\", \"vmod\"]:",
            "            return [node[\"tag\"], rel]",
            "        else:",
            "            return [node[\"tag\"]]",
            "",
            "    def get_glueformulas_from_semtype_entry(",
            "        self, lookup, word, node, depgraph, counter",
            "    ):",
            "        glueformulas = []",
            "",
            "        glueFormulaFactory = self.get_GlueFormula_factory()",
            "        for meaning, glue in lookup:",
            "            gf = glueFormulaFactory(self.get_meaning_formula(meaning, word), glue)",
            "            if not len(glueformulas):",
            "                gf.word = word",
            "            else:",
            "                gf.word = f\"{word}{len(glueformulas) + 1}\"",
            "",
            "            gf.glue = self.initialize_labels(gf.glue, node, depgraph, counter.get())",
            "",
            "            glueformulas.append(gf)",
            "        return glueformulas",
            "",
            "    def get_meaning_formula(self, generic, word):",
            "        \"\"\"",
            "        :param generic: A meaning formula string containing the",
            "            parameter \"<word>\"",
            "        :param word: The actual word to be replace \"<word>\"",
            "        \"\"\"",
            "        word = word.replace(\".\", \"\")",
            "        return generic.replace(\"<word>\", word)",
            "",
            "    def initialize_labels(self, expr, node, depgraph, unique_index):",
            "        if isinstance(expr, linearlogic.AtomicExpression):",
            "            name = self.find_label_name(expr.name.lower(), node, depgraph, unique_index)",
            "            if name[0].isupper():",
            "                return linearlogic.VariableExpression(name)",
            "            else:",
            "                return linearlogic.ConstantExpression(name)",
            "        else:",
            "            return linearlogic.ImpExpression(",
            "                self.initialize_labels(expr.antecedent, node, depgraph, unique_index),",
            "                self.initialize_labels(expr.consequent, node, depgraph, unique_index),",
            "            )",
            "",
            "    def find_label_name(self, name, node, depgraph, unique_index):",
            "        try:",
            "            dot = name.index(\".\")",
            "",
            "            before_dot = name[:dot]",
            "            after_dot = name[dot + 1 :]",
            "            if before_dot == \"super\":",
            "                return self.find_label_name(",
            "                    after_dot, depgraph.nodes[node[\"head\"]], depgraph, unique_index",
            "                )",
            "            else:",
            "                return self.find_label_name(",
            "                    after_dot,",
            "                    self.lookup_unique(before_dot, node, depgraph),",
            "                    depgraph,",
            "                    unique_index,",
            "                )",
            "        except ValueError:",
            "            lbl = self.get_label(node)",
            "            if name == \"f\":",
            "                return lbl",
            "            elif name == \"v\":",
            "                return \"%sv\" % lbl",
            "            elif name == \"r\":",
            "                return \"%sr\" % lbl",
            "            elif name == \"super\":",
            "                return self.get_label(depgraph.nodes[node[\"head\"]])",
            "            elif name == \"var\":",
            "                return f\"{lbl.upper()}{unique_index}\"",
            "            elif name == \"a\":",
            "                return self.get_label(self.lookup_unique(\"conja\", node, depgraph))",
            "            elif name == \"b\":",
            "                return self.get_label(self.lookup_unique(\"conjb\", node, depgraph))",
            "            else:",
            "                return self.get_label(self.lookup_unique(name, node, depgraph))",
            "",
            "    def get_label(self, node):",
            "        \"\"\"",
            "        Pick an alphabetic character as identifier for an entity in the model.",
            "",
            "        :param value: where to index into the list of characters",
            "        :type value: int",
            "        \"\"\"",
            "        value = node[\"address\"]",
            "",
            "        letter = [",
            "            \"f\",",
            "            \"g\",",
            "            \"h\",",
            "            \"i\",",
            "            \"j\",",
            "            \"k\",",
            "            \"l\",",
            "            \"m\",",
            "            \"n\",",
            "            \"o\",",
            "            \"p\",",
            "            \"q\",",
            "            \"r\",",
            "            \"s\",",
            "            \"t\",",
            "            \"u\",",
            "            \"v\",",
            "            \"w\",",
            "            \"x\",",
            "            \"y\",",
            "            \"z\",",
            "            \"a\",",
            "            \"b\",",
            "            \"c\",",
            "            \"d\",",
            "            \"e\",",
            "        ][value - 1]",
            "        num = int(value) // 26",
            "        if num > 0:",
            "            return letter + str(num)",
            "        else:",
            "            return letter",
            "",
            "    def lookup_unique(self, rel, node, depgraph):",
            "        \"\"\"",
            "        Lookup 'key'. There should be exactly one item in the associated relation.",
            "        \"\"\"",
            "        deps = [",
            "            depgraph.nodes[dep]",
            "            for dep in chain.from_iterable(node[\"deps\"].values())",
            "            if depgraph.nodes[dep][\"rel\"].lower() == rel.lower()",
            "        ]",
            "",
            "        if len(deps) == 0:",
            "            raise KeyError(",
            "                \"'{}' doesn't contain a feature '{}'\".format(node[\"word\"], rel)",
            "            )",
            "        elif len(deps) > 1:",
            "            raise KeyError(",
            "                \"'{}' should only have one feature '{}'\".format(node[\"word\"], rel)",
            "            )",
            "        else:",
            "            return deps[0]",
            "",
            "    def get_GlueFormula_factory(self):",
            "        return GlueFormula",
            "",
            "",
            "class Glue:",
            "    def __init__(",
            "        self, semtype_file=None, remove_duplicates=False, depparser=None, verbose=False",
            "    ):",
            "        self.verbose = verbose",
            "        self.remove_duplicates = remove_duplicates",
            "        self.depparser = depparser",
            "",
            "        from nltk import Prover9",
            "",
            "        self.prover = Prover9()",
            "",
            "        if semtype_file:",
            "            self.semtype_file = semtype_file",
            "        else:",
            "            self.semtype_file = os.path.join(",
            "                \"grammars\", \"sample_grammars\", \"glue.semtype\"",
            "            )",
            "",
            "    def train_depparser(self, depgraphs=None):",
            "        if depgraphs:",
            "            self.depparser.train(depgraphs)",
            "        else:",
            "            self.depparser.train_from_file(",
            "                nltk.data.find(",
            "                    os.path.join(\"grammars\", \"sample_grammars\", \"glue_train.conll\")",
            "                )",
            "            )",
            "",
            "    def parse_to_meaning(self, sentence):",
            "        readings = []",
            "        for agenda in self.parse_to_compiled(sentence):",
            "            readings.extend(self.get_readings(agenda))",
            "        return readings",
            "",
            "    def get_readings(self, agenda):",
            "        readings = []",
            "        agenda_length = len(agenda)",
            "        atomics = dict()",
            "        nonatomics = dict()",
            "        while agenda:  # is not empty",
            "            cur = agenda.pop()",
            "            glue_simp = cur.glue.simplify()",
            "            if isinstance(",
            "                glue_simp, linearlogic.ImpExpression",
            "            ):  # if cur.glue is non-atomic",
            "                for key in atomics:",
            "                    try:",
            "                        if isinstance(cur.glue, linearlogic.ApplicationExpression):",
            "                            bindings = cur.glue.bindings",
            "                        else:",
            "                            bindings = linearlogic.BindingDict()",
            "                        glue_simp.antecedent.unify(key, bindings)",
            "                        for atomic in atomics[key]:",
            "                            if not (",
            "                                cur.indices & atomic.indices",
            "                            ):  # if the sets of indices are disjoint",
            "                                try:",
            "                                    agenda.append(cur.applyto(atomic))",
            "                                except linearlogic.LinearLogicApplicationException:",
            "                                    pass",
            "                    except linearlogic.UnificationException:",
            "                        pass",
            "                try:",
            "                    nonatomics[glue_simp.antecedent].append(cur)",
            "                except KeyError:",
            "                    nonatomics[glue_simp.antecedent] = [cur]",
            "",
            "            else:  # else cur.glue is atomic",
            "                for key in nonatomics:",
            "                    for nonatomic in nonatomics[key]:",
            "                        try:",
            "                            if isinstance(",
            "                                nonatomic.glue, linearlogic.ApplicationExpression",
            "                            ):",
            "                                bindings = nonatomic.glue.bindings",
            "                            else:",
            "                                bindings = linearlogic.BindingDict()",
            "                            glue_simp.unify(key, bindings)",
            "                            if not (",
            "                                cur.indices & nonatomic.indices",
            "                            ):  # if the sets of indices are disjoint",
            "                                try:",
            "                                    agenda.append(nonatomic.applyto(cur))",
            "                                except linearlogic.LinearLogicApplicationException:",
            "                                    pass",
            "                        except linearlogic.UnificationException:",
            "                            pass",
            "                try:",
            "                    atomics[glue_simp].append(cur)",
            "                except KeyError:",
            "                    atomics[glue_simp] = [cur]",
            "",
            "        for entry in atomics:",
            "            for gf in atomics[entry]:",
            "                if len(gf.indices) == agenda_length:",
            "                    self._add_to_reading_list(gf, readings)",
            "        for entry in nonatomics:",
            "            for gf in nonatomics[entry]:",
            "                if len(gf.indices) == agenda_length:",
            "                    self._add_to_reading_list(gf, readings)",
            "        return readings",
            "",
            "    def _add_to_reading_list(self, glueformula, reading_list):",
            "        add_reading = True",
            "        if self.remove_duplicates:",
            "            for reading in reading_list:",
            "                try:",
            "                    if reading.equiv(glueformula.meaning, self.prover):",
            "                        add_reading = False",
            "                        break",
            "                except Exception as e:",
            "                    # if there is an exception, the syntax of the formula",
            "                    # may not be understandable by the prover, so don't",
            "                    # throw out the reading.",
            "                    print(\"Error when checking logical equality of statements\", e)",
            "",
            "        if add_reading:",
            "            reading_list.append(glueformula.meaning)",
            "",
            "    def parse_to_compiled(self, sentence):",
            "        gfls = [self.depgraph_to_glue(dg) for dg in self.dep_parse(sentence)]",
            "        return [self.gfl_to_compiled(gfl) for gfl in gfls]",
            "",
            "    def dep_parse(self, sentence):",
            "        \"\"\"",
            "        Return a dependency graph for the sentence.",
            "",
            "        :param sentence: the sentence to be parsed",
            "        :type sentence: list(str)",
            "        :rtype: DependencyGraph",
            "        \"\"\"",
            "",
            "        # Lazy-initialize the depparser",
            "        if self.depparser is None:",
            "            from nltk.parse import MaltParser",
            "",
            "            self.depparser = MaltParser(tagger=self.get_pos_tagger())",
            "        if not self.depparser._trained:",
            "            self.train_depparser()",
            "        return self.depparser.parse(sentence, verbose=self.verbose)",
            "",
            "    def depgraph_to_glue(self, depgraph):",
            "        return self.get_glue_dict().to_glueformula_list(depgraph)",
            "",
            "    def get_glue_dict(self):",
            "        return GlueDict(self.semtype_file)",
            "",
            "    def gfl_to_compiled(self, gfl):",
            "        index_counter = Counter()",
            "        return_list = []",
            "        for gf in gfl:",
            "            return_list.extend(gf.compile(index_counter))",
            "",
            "        if self.verbose:",
            "            print(\"Compiled Glue Premises:\")",
            "            for cgf in return_list:",
            "                print(cgf)",
            "",
            "        return return_list",
            "",
            "    def get_pos_tagger(self):",
            "        from nltk.corpus import brown",
            "",
            "        regexp_tagger = RegexpTagger(",
            "            [",
            "                (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers",
            "                (r\"(The|the|A|a|An|an)$\", \"AT\"),  # articles",
            "                (r\".*able$\", \"JJ\"),  # adjectives",
            "                (r\".*ness$\", \"NN\"),  # nouns formed from adjectives",
            "                (r\".*ly$\", \"RB\"),  # adverbs",
            "                (r\".*s$\", \"NNS\"),  # plural nouns",
            "                (r\".*ing$\", \"VBG\"),  # gerunds",
            "                (r\".*ed$\", \"VBD\"),  # past tense verbs",
            "                (r\".*\", \"NN\"),  # nouns (default)",
            "            ]",
            "        )",
            "        brown_train = brown.tagged_sents(categories=\"news\")",
            "        unigram_tagger = UnigramTagger(brown_train, backoff=regexp_tagger)",
            "        bigram_tagger = BigramTagger(brown_train, backoff=unigram_tagger)",
            "        trigram_tagger = TrigramTagger(brown_train, backoff=bigram_tagger)",
            "",
            "        # Override particular words",
            "        main_tagger = RegexpTagger(",
            "            [(r\"(A|a|An|an)$\", \"ex_quant\"), (r\"(Every|every|All|all)$\", \"univ_quant\")],",
            "            backoff=trigram_tagger,",
            "        )",
            "",
            "        return main_tagger",
            "",
            "",
            "class DrtGlueFormula(GlueFormula):",
            "    def __init__(self, meaning, glue, indices=None):",
            "        if not indices:",
            "            indices = set()",
            "",
            "        if isinstance(meaning, str):",
            "            self.meaning = drt.DrtExpression.fromstring(meaning)",
            "        elif isinstance(meaning, drt.DrtExpression):",
            "            self.meaning = meaning",
            "        else:",
            "            raise RuntimeError(",
            "                \"Meaning term neither string or expression: %s, %s\"",
            "                % (meaning, meaning.__class__)",
            "            )",
            "",
            "        if isinstance(glue, str):",
            "            self.glue = linearlogic.LinearLogicParser().parse(glue)",
            "        elif isinstance(glue, linearlogic.Expression):",
            "            self.glue = glue",
            "        else:",
            "            raise RuntimeError(",
            "                \"Glue term neither string or expression: %s, %s\"",
            "                % (glue, glue.__class__)",
            "            )",
            "",
            "        self.indices = indices",
            "",
            "    def make_VariableExpression(self, name):",
            "        return drt.DrtVariableExpression(name)",
            "",
            "    def make_LambdaExpression(self, variable, term):",
            "        return drt.DrtLambdaExpression(variable, term)",
            "",
            "",
            "class DrtGlueDict(GlueDict):",
            "    def get_GlueFormula_factory(self):",
            "        return DrtGlueFormula",
            "",
            "",
            "class DrtGlue(Glue):",
            "    def __init__(",
            "        self, semtype_file=None, remove_duplicates=False, depparser=None, verbose=False",
            "    ):",
            "        if not semtype_file:",
            "            semtype_file = os.path.join(",
            "                \"grammars\", \"sample_grammars\", \"drt_glue.semtype\"",
            "            )",
            "        Glue.__init__(self, semtype_file, remove_duplicates, depparser, verbose)",
            "",
            "    def get_glue_dict(self):",
            "        return DrtGlueDict(self.semtype_file)",
            "",
            "",
            "def demo(show_example=-1):",
            "    from nltk.parse import MaltParser",
            "",
            "    examples = [",
            "        \"David sees Mary\",",
            "        \"David eats a sandwich\",",
            "        \"every man chases a dog\",",
            "        \"every man believes a dog sleeps\",",
            "        \"John gives David a sandwich\",",
            "        \"John chases himself\",",
            "    ]",
            "    #                'John persuades David to order a pizza',",
            "    #                'John tries to go',",
            "    #                'John tries to find a unicorn',",
            "    #                'John seems to vanish',",
            "    #                'a unicorn seems to approach',",
            "    #                'every big cat leaves',",
            "    #                'every gray cat leaves',",
            "    #                'every big gray cat leaves',",
            "    #                'a former senator leaves',",
            "",
            "    print(\"============== DEMO ==============\")",
            "",
            "    tagger = RegexpTagger(",
            "        [",
            "            (\"^(David|Mary|John)$\", \"NNP\"),",
            "            (",
            "                \"^(sees|eats|chases|believes|gives|sleeps|chases|persuades|tries|seems|leaves)$\",",
            "                \"VB\",",
            "            ),",
            "            (\"^(go|order|vanish|find|approach)$\", \"VB\"),",
            "            (\"^(a)$\", \"ex_quant\"),",
            "            (\"^(every)$\", \"univ_quant\"),",
            "            (\"^(sandwich|man|dog|pizza|unicorn|cat|senator)$\", \"NN\"),",
            "            (\"^(big|gray|former)$\", \"JJ\"),",
            "            (\"^(him|himself)$\", \"PRP\"),",
            "        ]",
            "    )",
            "",
            "    depparser = MaltParser(tagger=tagger)",
            "    glue = Glue(depparser=depparser, verbose=False)",
            "",
            "    for (i, sentence) in enumerate(examples):",
            "        if i == show_example or show_example == -1:",
            "            print(f\"[[[Example {i}]]]  {sentence}\")",
            "            for reading in glue.parse_to_meaning(sentence.split()):",
            "                print(reading.simplify())",
            "            print(\"\")",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    demo()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "706": [
                "Glue",
                "get_pos_tagger"
            ]
        },
        "addLocation": []
    },
    "nltk/tag/brill.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 329,
                "afterPatchRowNumber": 329,
                "PatchRowcode": "             )"
            },
            "1": {
                "beforePatchRowNumber": 330,
                "afterPatchRowNumber": 330,
                "PatchRowcode": "             print("
            },
            "2": {
                "beforePatchRowNumber": 331,
                "afterPatchRowNumber": 331,
                "PatchRowcode": "                 \"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \""
            },
            "3": {
                "beforePatchRowNumber": 332,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+                \"final: {finalerrors:5d} {finalacc:.4f}\".format(**train_stats)"
            },
            "5": {
                "beforePatchRowNumber": 333,
                "afterPatchRowNumber": 333,
                "PatchRowcode": "             )"
            },
            "6": {
                "beforePatchRowNumber": 334,
                "afterPatchRowNumber": 334,
                "PatchRowcode": "             head = \"#ID | Score (train) |  #Rules     | Template\""
            },
            "7": {
                "beforePatchRowNumber": 335,
                "afterPatchRowNumber": 335,
                "PatchRowcode": "             print(head, \"\\n\", \"-\" * len(head), sep=\"\")"
            }
        },
        "frontPatchFile": [
            "# Natural Language Toolkit: Transformation-based learning",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# Author: Marcus Uneson <marcus.uneson@gmail.com>",
            "#   based on previous (nltk2) version by",
            "#   Christopher Maloof, Edward Loper, Steven Bird",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see  LICENSE.TXT",
            "",
            "from collections import Counter, defaultdict",
            "",
            "from nltk import jsontags",
            "from nltk.tag import TaggerI",
            "from nltk.tbl import Feature, Template",
            "",
            "######################################################################",
            "# Brill Templates",
            "######################################################################",
            "",
            "",
            "@jsontags.register_tag",
            "class Word(Feature):",
            "    \"\"\"",
            "    Feature which examines the text (word) of nearby tokens.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.brill.Word\"",
            "",
            "    @staticmethod",
            "    def extract_property(tokens, index):",
            "        \"\"\"@return: The given token's text.\"\"\"",
            "        return tokens[index][0]",
            "",
            "",
            "@jsontags.register_tag",
            "class Pos(Feature):",
            "    \"\"\"",
            "    Feature which examines the tags of nearby tokens.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.brill.Pos\"",
            "",
            "    @staticmethod",
            "    def extract_property(tokens, index):",
            "        \"\"\"@return: The given token's tag.\"\"\"",
            "        return tokens[index][1]",
            "",
            "",
            "def nltkdemo18():",
            "    \"\"\"",
            "    Return 18 templates, from the original nltk demo, in multi-feature syntax",
            "    \"\"\"",
            "    return [",
            "        Template(Pos([-1])),",
            "        Template(Pos([1])),",
            "        Template(Pos([-2])),",
            "        Template(Pos([2])),",
            "        Template(Pos([-2, -1])),",
            "        Template(Pos([1, 2])),",
            "        Template(Pos([-3, -2, -1])),",
            "        Template(Pos([1, 2, 3])),",
            "        Template(Pos([-1]), Pos([1])),",
            "        Template(Word([-1])),",
            "        Template(Word([1])),",
            "        Template(Word([-2])),",
            "        Template(Word([2])),",
            "        Template(Word([-2, -1])),",
            "        Template(Word([1, 2])),",
            "        Template(Word([-3, -2, -1])),",
            "        Template(Word([1, 2, 3])),",
            "        Template(Word([-1]), Word([1])),",
            "    ]",
            "",
            "",
            "def nltkdemo18plus():",
            "    \"\"\"",
            "    Return 18 templates, from the original nltk demo, and additionally a few",
            "    multi-feature ones (the motivation is easy comparison with nltkdemo18)",
            "    \"\"\"",
            "    return nltkdemo18() + [",
            "        Template(Word([-1]), Pos([1])),",
            "        Template(Pos([-1]), Word([1])),",
            "        Template(Word([-1]), Word([0]), Pos([1])),",
            "        Template(Pos([-1]), Word([0]), Word([1])),",
            "        Template(Pos([-1]), Word([0]), Pos([1])),",
            "    ]",
            "",
            "",
            "def fntbl37():",
            "    \"\"\"",
            "    Return 37 templates taken from the postagging task of the",
            "    fntbl distribution https://www.cs.jhu.edu/~rflorian/fntbl/",
            "    (37 is after excluding a handful which do not condition on Pos[0];",
            "    fntbl can do that but the current nltk implementation cannot.)",
            "    \"\"\"",
            "    return [",
            "        Template(Word([0]), Word([1]), Word([2])),",
            "        Template(Word([-1]), Word([0]), Word([1])),",
            "        Template(Word([0]), Word([-1])),",
            "        Template(Word([0]), Word([1])),",
            "        Template(Word([0]), Word([2])),",
            "        Template(Word([0]), Word([-2])),",
            "        Template(Word([1, 2])),",
            "        Template(Word([-2, -1])),",
            "        Template(Word([1, 2, 3])),",
            "        Template(Word([-3, -2, -1])),",
            "        Template(Word([0]), Pos([2])),",
            "        Template(Word([0]), Pos([-2])),",
            "        Template(Word([0]), Pos([1])),",
            "        Template(Word([0]), Pos([-1])),",
            "        Template(Word([0])),",
            "        Template(Word([-2])),",
            "        Template(Word([2])),",
            "        Template(Word([1])),",
            "        Template(Word([-1])),",
            "        Template(Pos([-1]), Pos([1])),",
            "        Template(Pos([1]), Pos([2])),",
            "        Template(Pos([-1]), Pos([-2])),",
            "        Template(Pos([1])),",
            "        Template(Pos([-1])),",
            "        Template(Pos([-2])),",
            "        Template(Pos([2])),",
            "        Template(Pos([1, 2, 3])),",
            "        Template(Pos([1, 2])),",
            "        Template(Pos([-3, -2, -1])),",
            "        Template(Pos([-2, -1])),",
            "        Template(Pos([1]), Word([0]), Word([1])),",
            "        Template(Pos([1]), Word([0]), Word([-1])),",
            "        Template(Pos([-1]), Word([-1]), Word([0])),",
            "        Template(Pos([-1]), Word([0]), Word([1])),",
            "        Template(Pos([-2]), Pos([-1])),",
            "        Template(Pos([1]), Pos([2])),",
            "        Template(Pos([1]), Pos([2]), Word([1])),",
            "    ]",
            "",
            "",
            "def brill24():",
            "    \"\"\"",
            "    Return 24 templates of the seminal TBL paper, Brill (1995)",
            "    \"\"\"",
            "    return [",
            "        Template(Pos([-1])),",
            "        Template(Pos([1])),",
            "        Template(Pos([-2])),",
            "        Template(Pos([2])),",
            "        Template(Pos([-2, -1])),",
            "        Template(Pos([1, 2])),",
            "        Template(Pos([-3, -2, -1])),",
            "        Template(Pos([1, 2, 3])),",
            "        Template(Pos([-1]), Pos([1])),",
            "        Template(Pos([-2]), Pos([-1])),",
            "        Template(Pos([1]), Pos([2])),",
            "        Template(Word([-1])),",
            "        Template(Word([1])),",
            "        Template(Word([-2])),",
            "        Template(Word([2])),",
            "        Template(Word([-2, -1])),",
            "        Template(Word([1, 2])),",
            "        Template(Word([-1, 0])),",
            "        Template(Word([0, 1])),",
            "        Template(Word([0])),",
            "        Template(Word([-1]), Pos([-1])),",
            "        Template(Word([1]), Pos([1])),",
            "        Template(Word([0]), Word([-1]), Pos([-1])),",
            "        Template(Word([0]), Word([1]), Pos([1])),",
            "    ]",
            "",
            "",
            "def describe_template_sets():",
            "    \"\"\"",
            "    Print the available template sets in this demo, with a short description\"",
            "    \"\"\"",
            "    import inspect",
            "    import sys",
            "",
            "    # a bit of magic to get all functions in this module",
            "    templatesets = inspect.getmembers(sys.modules[__name__], inspect.isfunction)",
            "    for (name, obj) in templatesets:",
            "        if name == \"describe_template_sets\":",
            "            continue",
            "        print(name, obj.__doc__, \"\\n\")",
            "",
            "",
            "######################################################################",
            "# The Brill Tagger",
            "######################################################################",
            "",
            "",
            "@jsontags.register_tag",
            "class BrillTagger(TaggerI):",
            "    \"\"\"",
            "    Brill's transformational rule-based tagger.  Brill taggers use an",
            "    initial tagger (such as ``tag.DefaultTagger``) to assign an initial",
            "    tag sequence to a text; and then apply an ordered list of",
            "    transformational rules to correct the tags of individual tokens.",
            "    These transformation rules are specified by the ``TagRule``",
            "    interface.",
            "",
            "    Brill taggers can be created directly, from an initial tagger and",
            "    a list of transformational rules; but more often, Brill taggers",
            "    are created by learning rules from a training corpus, using one",
            "    of the TaggerTrainers available.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.BrillTagger\"",
            "",
            "    def __init__(self, initial_tagger, rules, training_stats=None):",
            "        \"\"\"",
            "        :param initial_tagger: The initial tagger",
            "        :type initial_tagger: TaggerI",
            "",
            "        :param rules: An ordered list of transformation rules that",
            "            should be used to correct the initial tagging.",
            "        :type rules: list(TagRule)",
            "",
            "        :param training_stats: A dictionary of statistics collected",
            "            during training, for possible later use",
            "        :type training_stats: dict",
            "",
            "        \"\"\"",
            "        self._initial_tagger = initial_tagger",
            "        self._rules = tuple(rules)",
            "        self._training_stats = training_stats",
            "",
            "    def encode_json_obj(self):",
            "        return self._initial_tagger, self._rules, self._training_stats",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        _initial_tagger, _rules, _training_stats = obj",
            "        return cls(_initial_tagger, _rules, _training_stats)",
            "",
            "    def rules(self):",
            "        \"\"\"",
            "        Return the ordered list of  transformation rules that this tagger has learnt",
            "",
            "        :return: the ordered list of transformation rules that correct the initial tagging",
            "        :rtype: list of Rules",
            "        \"\"\"",
            "        return self._rules",
            "",
            "    def train_stats(self, statistic=None):",
            "        \"\"\"",
            "        Return a named statistic collected during training, or a dictionary of all",
            "        available statistics if no name given",
            "",
            "        :param statistic: name of statistic",
            "        :type statistic: str",
            "        :return: some statistic collected during training of this tagger",
            "        :rtype: any (but usually a number)",
            "        \"\"\"",
            "        if statistic is None:",
            "            return self._training_stats",
            "        else:",
            "            return self._training_stats.get(statistic)",
            "",
            "    def tag(self, tokens):",
            "        # Inherit documentation from TaggerI",
            "",
            "        # Run the initial tagger.",
            "        tagged_tokens = self._initial_tagger.tag(tokens)",
            "",
            "        # Create a dictionary that maps each tag to a list of the",
            "        # indices of tokens that have that tag.",
            "        tag_to_positions = defaultdict(set)",
            "        for i, (token, tag) in enumerate(tagged_tokens):",
            "            tag_to_positions[tag].add(i)",
            "",
            "        # Apply each rule, in order.  Only try to apply rules at",
            "        # positions that have the desired original tag.",
            "        for rule in self._rules:",
            "            # Find the positions where it might apply",
            "            positions = tag_to_positions.get(rule.original_tag, [])",
            "            # Apply the rule at those positions.",
            "            changed = rule.apply(tagged_tokens, positions)",
            "            # Update tag_to_positions with the positions of tags that",
            "            # were modified.",
            "            for i in changed:",
            "                tag_to_positions[rule.original_tag].remove(i)",
            "                tag_to_positions[rule.replacement_tag].add(i)",
            "",
            "        return tagged_tokens",
            "",
            "    def print_template_statistics(self, test_stats=None, printunused=True):",
            "        \"\"\"",
            "        Print a list of all templates, ranked according to efficiency.",
            "",
            "        If test_stats is available, the templates are ranked according to their",
            "        relative contribution (summed for all rules created from a given template,",
            "        weighted by score) to the performance on the test set. If no test_stats, then",
            "        statistics collected during training are used instead. There is also",
            "        an unweighted measure (just counting the rules). This is less informative,",
            "        though, as many low-score rules will appear towards end of training.",
            "",
            "        :param test_stats: dictionary of statistics collected during testing",
            "        :type test_stats: dict of str -> any (but usually numbers)",
            "        :param printunused: if True, print a list of all unused templates",
            "        :type printunused: bool",
            "        :return: None",
            "        :rtype: None",
            "        \"\"\"",
            "        tids = [r.templateid for r in self._rules]",
            "        train_stats = self.train_stats()",
            "",
            "        trainscores = train_stats[\"rulescores\"]",
            "        assert len(trainscores) == len(",
            "            tids",
            "        ), \"corrupt statistics: \" \"{} train scores for {} rules\".format(",
            "            trainscores, tids",
            "        )",
            "        template_counts = Counter(tids)",
            "        weighted_traincounts = Counter()",
            "        for (tid, score) in zip(tids, trainscores):",
            "            weighted_traincounts[tid] += score",
            "        tottrainscores = sum(trainscores)",
            "",
            "        # det_tplsort() is for deterministic sorting;",
            "        # the otherwise convenient Counter.most_common() unfortunately",
            "        # does not break ties deterministically",
            "        # between python versions and will break cross-version tests",
            "        def det_tplsort(tpl_value):",
            "            return (tpl_value[1], repr(tpl_value[0]))",
            "",
            "        def print_train_stats():",
            "            print(",
            "                \"TEMPLATE STATISTICS (TRAIN)  {} templates, {} rules)\".format(",
            "                    len(template_counts), len(tids)",
            "                )",
            "            )",
            "            print(",
            "                \"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"",
            "                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats)",
            "            )",
            "            head = \"#ID | Score (train) |  #Rules     | Template\"",
            "            print(head, \"\\n\", \"-\" * len(head), sep=\"\")",
            "            train_tplscores = sorted(",
            "                weighted_traincounts.items(), key=det_tplsort, reverse=True",
            "            )",
            "            for (tid, trainscore) in train_tplscores:",
            "                s = \"{} | {:5d}   {:5.3f} |{:4d}   {:.3f} | {}\".format(",
            "                    tid,",
            "                    trainscore,",
            "                    trainscore / tottrainscores,",
            "                    template_counts[tid],",
            "                    template_counts[tid] / len(tids),",
            "                    Template.ALLTEMPLATES[int(tid)],",
            "                )",
            "                print(s)",
            "",
            "        def print_testtrain_stats():",
            "            testscores = test_stats[\"rulescores\"]",
            "            print(",
            "                \"TEMPLATE STATISTICS (TEST AND TRAIN) ({} templates, {} rules)\".format(",
            "                    len(template_counts), len(tids)",
            "                )",
            "            )",
            "            print(",
            "                \"TEST  ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"",
            "                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**test_stats)",
            "            )",
            "            print(",
            "                \"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"",
            "                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats)",
            "            )",
            "            weighted_testcounts = Counter()",
            "            for (tid, score) in zip(tids, testscores):",
            "                weighted_testcounts[tid] += score",
            "            tottestscores = sum(testscores)",
            "            head = \"#ID | Score (test) | Score (train) |  #Rules     | Template\"",
            "            print(head, \"\\n\", \"-\" * len(head), sep=\"\")",
            "            test_tplscores = sorted(",
            "                weighted_testcounts.items(), key=det_tplsort, reverse=True",
            "            )",
            "            for (tid, testscore) in test_tplscores:",
            "                s = \"{:s} |{:5d}  {:6.3f} |  {:4d}   {:.3f} |{:4d}   {:.3f} | {:s}\".format(",
            "                    tid,",
            "                    testscore,",
            "                    testscore / tottestscores,",
            "                    weighted_traincounts[tid],",
            "                    weighted_traincounts[tid] / tottrainscores,",
            "                    template_counts[tid],",
            "                    template_counts[tid] / len(tids),",
            "                    Template.ALLTEMPLATES[int(tid)],",
            "                )",
            "                print(s)",
            "",
            "        def print_unused_templates():",
            "            usedtpls = {int(tid) for tid in tids}",
            "            unused = [",
            "                (tid, tpl)",
            "                for (tid, tpl) in enumerate(Template.ALLTEMPLATES)",
            "                if tid not in usedtpls",
            "            ]",
            "            print(f\"UNUSED TEMPLATES ({len(unused)})\")",
            "",
            "            for (tid, tpl) in unused:",
            "                print(f\"{tid:03d} {str(tpl):s}\")",
            "",
            "        if test_stats is None:",
            "            print_train_stats()",
            "        else:",
            "            print_testtrain_stats()",
            "        print()",
            "        if printunused:",
            "            print_unused_templates()",
            "        print()",
            "",
            "    def batch_tag_incremental(self, sequences, gold):",
            "        \"\"\"",
            "        Tags by applying each rule to the entire corpus (rather than all rules to a",
            "        single sequence). The point is to collect statistics on the test set for",
            "        individual rules.",
            "",
            "        NOTE: This is inefficient (does not build any index, so will traverse the entire",
            "        corpus N times for N rules) -- usually you would not care about statistics for",
            "        individual rules and thus use batch_tag() instead",
            "",
            "        :param sequences: lists of token sequences (sentences, in some applications) to be tagged",
            "        :type sequences: list of list of strings",
            "        :param gold: the gold standard",
            "        :type gold: list of list of strings",
            "        :returns: tuple of (tagged_sequences, ordered list of rule scores (one for each rule))",
            "        \"\"\"",
            "",
            "        def counterrors(xs):",
            "            return sum(t[1] != g[1] for pair in zip(xs, gold) for (t, g) in zip(*pair))",
            "",
            "        testing_stats = {}",
            "        testing_stats[\"tokencount\"] = sum(len(t) for t in sequences)",
            "        testing_stats[\"sequencecount\"] = len(sequences)",
            "        tagged_tokenses = [self._initial_tagger.tag(tokens) for tokens in sequences]",
            "        testing_stats[\"initialerrors\"] = counterrors(tagged_tokenses)",
            "        testing_stats[\"initialacc\"] = (",
            "            1 - testing_stats[\"initialerrors\"] / testing_stats[\"tokencount\"]",
            "        )",
            "        # Apply each rule to the entire corpus, in order",
            "        errors = [testing_stats[\"initialerrors\"]]",
            "        for rule in self._rules:",
            "            for tagged_tokens in tagged_tokenses:",
            "                rule.apply(tagged_tokens)",
            "            errors.append(counterrors(tagged_tokenses))",
            "        testing_stats[\"rulescores\"] = [",
            "            err0 - err1 for (err0, err1) in zip(errors, errors[1:])",
            "        ]",
            "        testing_stats[\"finalerrors\"] = errors[-1]",
            "        testing_stats[\"finalacc\"] = (",
            "            1 - testing_stats[\"finalerrors\"] / testing_stats[\"tokencount\"]",
            "        )",
            "        return (tagged_tokenses, testing_stats)"
        ],
        "afterPatchFile": [
            "# Natural Language Toolkit: Transformation-based learning",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# Author: Marcus Uneson <marcus.uneson@gmail.com>",
            "#   based on previous (nltk2) version by",
            "#   Christopher Maloof, Edward Loper, Steven Bird",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see  LICENSE.TXT",
            "",
            "from collections import Counter, defaultdict",
            "",
            "from nltk import jsontags",
            "from nltk.tag import TaggerI",
            "from nltk.tbl import Feature, Template",
            "",
            "######################################################################",
            "# Brill Templates",
            "######################################################################",
            "",
            "",
            "@jsontags.register_tag",
            "class Word(Feature):",
            "    \"\"\"",
            "    Feature which examines the text (word) of nearby tokens.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.brill.Word\"",
            "",
            "    @staticmethod",
            "    def extract_property(tokens, index):",
            "        \"\"\"@return: The given token's text.\"\"\"",
            "        return tokens[index][0]",
            "",
            "",
            "@jsontags.register_tag",
            "class Pos(Feature):",
            "    \"\"\"",
            "    Feature which examines the tags of nearby tokens.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.brill.Pos\"",
            "",
            "    @staticmethod",
            "    def extract_property(tokens, index):",
            "        \"\"\"@return: The given token's tag.\"\"\"",
            "        return tokens[index][1]",
            "",
            "",
            "def nltkdemo18():",
            "    \"\"\"",
            "    Return 18 templates, from the original nltk demo, in multi-feature syntax",
            "    \"\"\"",
            "    return [",
            "        Template(Pos([-1])),",
            "        Template(Pos([1])),",
            "        Template(Pos([-2])),",
            "        Template(Pos([2])),",
            "        Template(Pos([-2, -1])),",
            "        Template(Pos([1, 2])),",
            "        Template(Pos([-3, -2, -1])),",
            "        Template(Pos([1, 2, 3])),",
            "        Template(Pos([-1]), Pos([1])),",
            "        Template(Word([-1])),",
            "        Template(Word([1])),",
            "        Template(Word([-2])),",
            "        Template(Word([2])),",
            "        Template(Word([-2, -1])),",
            "        Template(Word([1, 2])),",
            "        Template(Word([-3, -2, -1])),",
            "        Template(Word([1, 2, 3])),",
            "        Template(Word([-1]), Word([1])),",
            "    ]",
            "",
            "",
            "def nltkdemo18plus():",
            "    \"\"\"",
            "    Return 18 templates, from the original nltk demo, and additionally a few",
            "    multi-feature ones (the motivation is easy comparison with nltkdemo18)",
            "    \"\"\"",
            "    return nltkdemo18() + [",
            "        Template(Word([-1]), Pos([1])),",
            "        Template(Pos([-1]), Word([1])),",
            "        Template(Word([-1]), Word([0]), Pos([1])),",
            "        Template(Pos([-1]), Word([0]), Word([1])),",
            "        Template(Pos([-1]), Word([0]), Pos([1])),",
            "    ]",
            "",
            "",
            "def fntbl37():",
            "    \"\"\"",
            "    Return 37 templates taken from the postagging task of the",
            "    fntbl distribution https://www.cs.jhu.edu/~rflorian/fntbl/",
            "    (37 is after excluding a handful which do not condition on Pos[0];",
            "    fntbl can do that but the current nltk implementation cannot.)",
            "    \"\"\"",
            "    return [",
            "        Template(Word([0]), Word([1]), Word([2])),",
            "        Template(Word([-1]), Word([0]), Word([1])),",
            "        Template(Word([0]), Word([-1])),",
            "        Template(Word([0]), Word([1])),",
            "        Template(Word([0]), Word([2])),",
            "        Template(Word([0]), Word([-2])),",
            "        Template(Word([1, 2])),",
            "        Template(Word([-2, -1])),",
            "        Template(Word([1, 2, 3])),",
            "        Template(Word([-3, -2, -1])),",
            "        Template(Word([0]), Pos([2])),",
            "        Template(Word([0]), Pos([-2])),",
            "        Template(Word([0]), Pos([1])),",
            "        Template(Word([0]), Pos([-1])),",
            "        Template(Word([0])),",
            "        Template(Word([-2])),",
            "        Template(Word([2])),",
            "        Template(Word([1])),",
            "        Template(Word([-1])),",
            "        Template(Pos([-1]), Pos([1])),",
            "        Template(Pos([1]), Pos([2])),",
            "        Template(Pos([-1]), Pos([-2])),",
            "        Template(Pos([1])),",
            "        Template(Pos([-1])),",
            "        Template(Pos([-2])),",
            "        Template(Pos([2])),",
            "        Template(Pos([1, 2, 3])),",
            "        Template(Pos([1, 2])),",
            "        Template(Pos([-3, -2, -1])),",
            "        Template(Pos([-2, -1])),",
            "        Template(Pos([1]), Word([0]), Word([1])),",
            "        Template(Pos([1]), Word([0]), Word([-1])),",
            "        Template(Pos([-1]), Word([-1]), Word([0])),",
            "        Template(Pos([-1]), Word([0]), Word([1])),",
            "        Template(Pos([-2]), Pos([-1])),",
            "        Template(Pos([1]), Pos([2])),",
            "        Template(Pos([1]), Pos([2]), Word([1])),",
            "    ]",
            "",
            "",
            "def brill24():",
            "    \"\"\"",
            "    Return 24 templates of the seminal TBL paper, Brill (1995)",
            "    \"\"\"",
            "    return [",
            "        Template(Pos([-1])),",
            "        Template(Pos([1])),",
            "        Template(Pos([-2])),",
            "        Template(Pos([2])),",
            "        Template(Pos([-2, -1])),",
            "        Template(Pos([1, 2])),",
            "        Template(Pos([-3, -2, -1])),",
            "        Template(Pos([1, 2, 3])),",
            "        Template(Pos([-1]), Pos([1])),",
            "        Template(Pos([-2]), Pos([-1])),",
            "        Template(Pos([1]), Pos([2])),",
            "        Template(Word([-1])),",
            "        Template(Word([1])),",
            "        Template(Word([-2])),",
            "        Template(Word([2])),",
            "        Template(Word([-2, -1])),",
            "        Template(Word([1, 2])),",
            "        Template(Word([-1, 0])),",
            "        Template(Word([0, 1])),",
            "        Template(Word([0])),",
            "        Template(Word([-1]), Pos([-1])),",
            "        Template(Word([1]), Pos([1])),",
            "        Template(Word([0]), Word([-1]), Pos([-1])),",
            "        Template(Word([0]), Word([1]), Pos([1])),",
            "    ]",
            "",
            "",
            "def describe_template_sets():",
            "    \"\"\"",
            "    Print the available template sets in this demo, with a short description\"",
            "    \"\"\"",
            "    import inspect",
            "    import sys",
            "",
            "    # a bit of magic to get all functions in this module",
            "    templatesets = inspect.getmembers(sys.modules[__name__], inspect.isfunction)",
            "    for (name, obj) in templatesets:",
            "        if name == \"describe_template_sets\":",
            "            continue",
            "        print(name, obj.__doc__, \"\\n\")",
            "",
            "",
            "######################################################################",
            "# The Brill Tagger",
            "######################################################################",
            "",
            "",
            "@jsontags.register_tag",
            "class BrillTagger(TaggerI):",
            "    \"\"\"",
            "    Brill's transformational rule-based tagger.  Brill taggers use an",
            "    initial tagger (such as ``tag.DefaultTagger``) to assign an initial",
            "    tag sequence to a text; and then apply an ordered list of",
            "    transformational rules to correct the tags of individual tokens.",
            "    These transformation rules are specified by the ``TagRule``",
            "    interface.",
            "",
            "    Brill taggers can be created directly, from an initial tagger and",
            "    a list of transformational rules; but more often, Brill taggers",
            "    are created by learning rules from a training corpus, using one",
            "    of the TaggerTrainers available.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.BrillTagger\"",
            "",
            "    def __init__(self, initial_tagger, rules, training_stats=None):",
            "        \"\"\"",
            "        :param initial_tagger: The initial tagger",
            "        :type initial_tagger: TaggerI",
            "",
            "        :param rules: An ordered list of transformation rules that",
            "            should be used to correct the initial tagging.",
            "        :type rules: list(TagRule)",
            "",
            "        :param training_stats: A dictionary of statistics collected",
            "            during training, for possible later use",
            "        :type training_stats: dict",
            "",
            "        \"\"\"",
            "        self._initial_tagger = initial_tagger",
            "        self._rules = tuple(rules)",
            "        self._training_stats = training_stats",
            "",
            "    def encode_json_obj(self):",
            "        return self._initial_tagger, self._rules, self._training_stats",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        _initial_tagger, _rules, _training_stats = obj",
            "        return cls(_initial_tagger, _rules, _training_stats)",
            "",
            "    def rules(self):",
            "        \"\"\"",
            "        Return the ordered list of  transformation rules that this tagger has learnt",
            "",
            "        :return: the ordered list of transformation rules that correct the initial tagging",
            "        :rtype: list of Rules",
            "        \"\"\"",
            "        return self._rules",
            "",
            "    def train_stats(self, statistic=None):",
            "        \"\"\"",
            "        Return a named statistic collected during training, or a dictionary of all",
            "        available statistics if no name given",
            "",
            "        :param statistic: name of statistic",
            "        :type statistic: str",
            "        :return: some statistic collected during training of this tagger",
            "        :rtype: any (but usually a number)",
            "        \"\"\"",
            "        if statistic is None:",
            "            return self._training_stats",
            "        else:",
            "            return self._training_stats.get(statistic)",
            "",
            "    def tag(self, tokens):",
            "        # Inherit documentation from TaggerI",
            "",
            "        # Run the initial tagger.",
            "        tagged_tokens = self._initial_tagger.tag(tokens)",
            "",
            "        # Create a dictionary that maps each tag to a list of the",
            "        # indices of tokens that have that tag.",
            "        tag_to_positions = defaultdict(set)",
            "        for i, (token, tag) in enumerate(tagged_tokens):",
            "            tag_to_positions[tag].add(i)",
            "",
            "        # Apply each rule, in order.  Only try to apply rules at",
            "        # positions that have the desired original tag.",
            "        for rule in self._rules:",
            "            # Find the positions where it might apply",
            "            positions = tag_to_positions.get(rule.original_tag, [])",
            "            # Apply the rule at those positions.",
            "            changed = rule.apply(tagged_tokens, positions)",
            "            # Update tag_to_positions with the positions of tags that",
            "            # were modified.",
            "            for i in changed:",
            "                tag_to_positions[rule.original_tag].remove(i)",
            "                tag_to_positions[rule.replacement_tag].add(i)",
            "",
            "        return tagged_tokens",
            "",
            "    def print_template_statistics(self, test_stats=None, printunused=True):",
            "        \"\"\"",
            "        Print a list of all templates, ranked according to efficiency.",
            "",
            "        If test_stats is available, the templates are ranked according to their",
            "        relative contribution (summed for all rules created from a given template,",
            "        weighted by score) to the performance on the test set. If no test_stats, then",
            "        statistics collected during training are used instead. There is also",
            "        an unweighted measure (just counting the rules). This is less informative,",
            "        though, as many low-score rules will appear towards end of training.",
            "",
            "        :param test_stats: dictionary of statistics collected during testing",
            "        :type test_stats: dict of str -> any (but usually numbers)",
            "        :param printunused: if True, print a list of all unused templates",
            "        :type printunused: bool",
            "        :return: None",
            "        :rtype: None",
            "        \"\"\"",
            "        tids = [r.templateid for r in self._rules]",
            "        train_stats = self.train_stats()",
            "",
            "        trainscores = train_stats[\"rulescores\"]",
            "        assert len(trainscores) == len(",
            "            tids",
            "        ), \"corrupt statistics: \" \"{} train scores for {} rules\".format(",
            "            trainscores, tids",
            "        )",
            "        template_counts = Counter(tids)",
            "        weighted_traincounts = Counter()",
            "        for (tid, score) in zip(tids, trainscores):",
            "            weighted_traincounts[tid] += score",
            "        tottrainscores = sum(trainscores)",
            "",
            "        # det_tplsort() is for deterministic sorting;",
            "        # the otherwise convenient Counter.most_common() unfortunately",
            "        # does not break ties deterministically",
            "        # between python versions and will break cross-version tests",
            "        def det_tplsort(tpl_value):",
            "            return (tpl_value[1], repr(tpl_value[0]))",
            "",
            "        def print_train_stats():",
            "            print(",
            "                \"TEMPLATE STATISTICS (TRAIN)  {} templates, {} rules)\".format(",
            "                    len(template_counts), len(tids)",
            "                )",
            "            )",
            "            print(",
            "                \"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"",
            "                \"final: {finalerrors:5d} {finalacc:.4f}\".format(**train_stats)",
            "            )",
            "            head = \"#ID | Score (train) |  #Rules     | Template\"",
            "            print(head, \"\\n\", \"-\" * len(head), sep=\"\")",
            "            train_tplscores = sorted(",
            "                weighted_traincounts.items(), key=det_tplsort, reverse=True",
            "            )",
            "            for (tid, trainscore) in train_tplscores:",
            "                s = \"{} | {:5d}   {:5.3f} |{:4d}   {:.3f} | {}\".format(",
            "                    tid,",
            "                    trainscore,",
            "                    trainscore / tottrainscores,",
            "                    template_counts[tid],",
            "                    template_counts[tid] / len(tids),",
            "                    Template.ALLTEMPLATES[int(tid)],",
            "                )",
            "                print(s)",
            "",
            "        def print_testtrain_stats():",
            "            testscores = test_stats[\"rulescores\"]",
            "            print(",
            "                \"TEMPLATE STATISTICS (TEST AND TRAIN) ({} templates, {} rules)\".format(",
            "                    len(template_counts), len(tids)",
            "                )",
            "            )",
            "            print(",
            "                \"TEST  ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"",
            "                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**test_stats)",
            "            )",
            "            print(",
            "                \"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"",
            "                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats)",
            "            )",
            "            weighted_testcounts = Counter()",
            "            for (tid, score) in zip(tids, testscores):",
            "                weighted_testcounts[tid] += score",
            "            tottestscores = sum(testscores)",
            "            head = \"#ID | Score (test) | Score (train) |  #Rules     | Template\"",
            "            print(head, \"\\n\", \"-\" * len(head), sep=\"\")",
            "            test_tplscores = sorted(",
            "                weighted_testcounts.items(), key=det_tplsort, reverse=True",
            "            )",
            "            for (tid, testscore) in test_tplscores:",
            "                s = \"{:s} |{:5d}  {:6.3f} |  {:4d}   {:.3f} |{:4d}   {:.3f} | {:s}\".format(",
            "                    tid,",
            "                    testscore,",
            "                    testscore / tottestscores,",
            "                    weighted_traincounts[tid],",
            "                    weighted_traincounts[tid] / tottrainscores,",
            "                    template_counts[tid],",
            "                    template_counts[tid] / len(tids),",
            "                    Template.ALLTEMPLATES[int(tid)],",
            "                )",
            "                print(s)",
            "",
            "        def print_unused_templates():",
            "            usedtpls = {int(tid) for tid in tids}",
            "            unused = [",
            "                (tid, tpl)",
            "                for (tid, tpl) in enumerate(Template.ALLTEMPLATES)",
            "                if tid not in usedtpls",
            "            ]",
            "            print(f\"UNUSED TEMPLATES ({len(unused)})\")",
            "",
            "            for (tid, tpl) in unused:",
            "                print(f\"{tid:03d} {str(tpl):s}\")",
            "",
            "        if test_stats is None:",
            "            print_train_stats()",
            "        else:",
            "            print_testtrain_stats()",
            "        print()",
            "        if printunused:",
            "            print_unused_templates()",
            "        print()",
            "",
            "    def batch_tag_incremental(self, sequences, gold):",
            "        \"\"\"",
            "        Tags by applying each rule to the entire corpus (rather than all rules to a",
            "        single sequence). The point is to collect statistics on the test set for",
            "        individual rules.",
            "",
            "        NOTE: This is inefficient (does not build any index, so will traverse the entire",
            "        corpus N times for N rules) -- usually you would not care about statistics for",
            "        individual rules and thus use batch_tag() instead",
            "",
            "        :param sequences: lists of token sequences (sentences, in some applications) to be tagged",
            "        :type sequences: list of list of strings",
            "        :param gold: the gold standard",
            "        :type gold: list of list of strings",
            "        :returns: tuple of (tagged_sequences, ordered list of rule scores (one for each rule))",
            "        \"\"\"",
            "",
            "        def counterrors(xs):",
            "            return sum(t[1] != g[1] for pair in zip(xs, gold) for (t, g) in zip(*pair))",
            "",
            "        testing_stats = {}",
            "        testing_stats[\"tokencount\"] = sum(len(t) for t in sequences)",
            "        testing_stats[\"sequencecount\"] = len(sequences)",
            "        tagged_tokenses = [self._initial_tagger.tag(tokens) for tokens in sequences]",
            "        testing_stats[\"initialerrors\"] = counterrors(tagged_tokenses)",
            "        testing_stats[\"initialacc\"] = (",
            "            1 - testing_stats[\"initialerrors\"] / testing_stats[\"tokencount\"]",
            "        )",
            "        # Apply each rule to the entire corpus, in order",
            "        errors = [testing_stats[\"initialerrors\"]]",
            "        for rule in self._rules:",
            "            for tagged_tokens in tagged_tokenses:",
            "                rule.apply(tagged_tokens)",
            "            errors.append(counterrors(tagged_tokenses))",
            "        testing_stats[\"rulescores\"] = [",
            "            err0 - err1 for (err0, err1) in zip(errors, errors[1:])",
            "        ]",
            "        testing_stats[\"finalerrors\"] = errors[-1]",
            "        testing_stats[\"finalacc\"] = (",
            "            1 - testing_stats[\"finalerrors\"] / testing_stats[\"tokencount\"]",
            "        )",
            "        return (tagged_tokenses, testing_stats)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "332": [
                "BrillTagger",
                "print_template_statistics",
                "print_train_stats"
            ]
        },
        "addLocation": []
    },
    "nltk/tag/brill_trainer.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "     # Training"
            },
            "1": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 92,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "     def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):"
            },
            "3": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+        r\"\"\""
            },
            "5": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "         Trains the Brill tagger on the corpus *train_sents*,"
            },
            "6": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "         producing at most *max_rules* transformations, each of which"
            },
            "7": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "         reduces the net number of errors in the corpus by at least"
            },
            "8": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "         >>> testing_data = [untag(s) for s in gold_data]"
            },
            "9": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 112,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "         >>> backoff = RegexpTagger(["
            },
            "11": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+        ... (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers"
            },
            "13": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "         ... (r'(The|the|A|a|An|an)$', 'AT'),   # articles"
            },
            "14": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "         ... (r'.*able$', 'JJ'),                # adjectives"
            },
            "15": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "         ... (r'.*ness$', 'NN'),                # nouns formed from adjectives"
            },
            "16": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "         >>> baseline = backoff #see NOTE1"
            },
            "17": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 126,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 127,
                "PatchRowcode": "         >>> baseline.evaluate(gold_data) #doctest: +ELLIPSIS"
            },
            "19": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        0.2450142..."
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+        0.2433862..."
            },
            "21": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 129,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "         >>> # Set up templates"
            },
            "23": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 131,
                "PatchRowcode": "         >>> Template._cleartemplates() #clear any templates created in earlier tests"
            },
            "24": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "         >>> tagger1 = tt.train(training_data, max_rules=10)"
            },
            "25": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "         TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)"
            },
            "26": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": 139,
                "PatchRowcode": "         Finding initial useful rules..."
            },
            "27": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            Found 845 useful rules."
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+            Found 847 useful rules."
            },
            "29": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "         <BLANKLINE>"
            },
            "30": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 142,
                "PatchRowcode": "                    B      |"
            },
            "31": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": 143,
                "PatchRowcode": "            S   F   r   O  |        Score = Fixed - Broken"
            },
            "32": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 150,
                "PatchRowcode": "           85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]"
            },
            "33": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": 151,
                "PatchRowcode": "           69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]"
            },
            "34": {
                "beforePatchRowNumber": 152,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "           51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]"
            },
            "35": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-          47  63  16 161  | NN->IN if Pos:NNS@[-1]"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+          47  63  16 162  | NN->IN if Pos:NNS@[-1]"
            },
            "37": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "           33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]"
            },
            "38": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "           26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]"
            },
            "39": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "           24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]"
            },
            "40": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": 162,
                "PatchRowcode": " "
            },
            "41": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": 163,
                "PatchRowcode": "         >>> train_stats = tagger1.train_stats()"
            },
            "42": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": 164,
                "PatchRowcode": "         >>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]"
            },
            "43": {
                "beforePatchRowNumber": 165,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        [1775, 1269, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 165,
                "PatchRowcode": "+        [1776, 1270, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]"
            },
            "45": {
                "beforePatchRowNumber": 166,
                "afterPatchRowNumber": 166,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 167,
                "afterPatchRowNumber": 167,
                "PatchRowcode": "         >>> tagger1.print_template_statistics(printunused=False)"
            },
            "47": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": 168,
                "PatchRowcode": "         TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)"
            },
            "48": {
                "beforePatchRowNumber": 169,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+        TRAIN (   2417 tokens) initial  1776 0.2652 final:  1270 0.4746"
            },
            "50": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 170,
                "PatchRowcode": "         #ID | Score (train) |  #Rules     | Template"
            },
            "51": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": 171,
                "PatchRowcode": "         --------------------------------------------"
            },
            "52": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 172,
                "PatchRowcode": "         001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))"
            },
            "53": {
                "beforePatchRowNumber": 175,
                "afterPatchRowNumber": 175,
                "PatchRowcode": "         <BLANKLINE>"
            },
            "54": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 176,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": 177,
                "PatchRowcode": "         >>> tagger1.evaluate(gold_data) # doctest: +ELLIPSIS"
            },
            "56": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        0.43996..."
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+        0.43833..."
            },
            "58": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 179,
                "PatchRowcode": " "
            },
            "59": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "         >>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)"
            },
            "60": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 181,
                "PatchRowcode": " "
            },
            "61": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "         True"
            },
            "62": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 186,
                "PatchRowcode": " "
            },
            "63": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "         >>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]"
            },
            "64": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        [1855, 1376, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+        [1859, 1380, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]"
            },
            "66": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 189,
                "PatchRowcode": " "
            },
            "67": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "         >>> # A high-accuracy tagger"
            },
            "68": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "         >>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99)"
            },
            "69": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 192,
                "PatchRowcode": "         TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: 0.99)"
            },
            "70": {
                "beforePatchRowNumber": 193,
                "afterPatchRowNumber": 193,
                "PatchRowcode": "         Finding initial useful rules..."
            },
            "71": {
                "beforePatchRowNumber": 194,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            Found 845 useful rules."
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+            Found 847 useful rules."
            },
            "73": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": 195,
                "PatchRowcode": "         <BLANKLINE>"
            },
            "74": {
                "beforePatchRowNumber": 196,
                "afterPatchRowNumber": 196,
                "PatchRowcode": "                    B      |"
            },
            "75": {
                "beforePatchRowNumber": 197,
                "afterPatchRowNumber": 197,
                "PatchRowcode": "            S   F   r   O  |        Score = Fixed - Broken"
            },
            "76": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": 212,
                "PatchRowcode": "           18  18   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]"
            },
            "77": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 213,
                "PatchRowcode": " "
            },
            "78": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "         >>> tagger2.evaluate(gold_data)  # doctest: +ELLIPSIS"
            },
            "79": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        0.44159544..."
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 215,
                "PatchRowcode": "+        0.43996743..."
            },
            "81": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 216,
                "PatchRowcode": "         >>> tagger2.rules()[2:4]"
            },
            "82": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 217,
                "PatchRowcode": "         (Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))"
            },
            "83": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 218,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Natural Language Toolkit: Transformation-based learning",
            "#",
            "# Copyright (C) 2001-2013 NLTK Project",
            "# Author: Marcus Uneson <marcus.uneson@gmail.com>",
            "#   based on previous (nltk2) version by",
            "#   Christopher Maloof, Edward Loper, Steven Bird",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see  LICENSE.TXT",
            "",
            "import bisect",
            "import textwrap",
            "from collections import defaultdict",
            "",
            "from nltk.tag import BrillTagger, untag",
            "",
            "######################################################################",
            "#  Brill Tagger Trainer",
            "######################################################################",
            "",
            "",
            "class BrillTaggerTrainer:",
            "    \"\"\"",
            "    A trainer for tbl taggers.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self, initial_tagger, templates, trace=0, deterministic=None, ruleformat=\"str\"",
            "    ):",
            "        \"\"\"",
            "        Construct a Brill tagger from a baseline tagger and a",
            "        set of templates",
            "",
            "        :param initial_tagger: the baseline tagger",
            "        :type initial_tagger: Tagger",
            "        :param templates: templates to be used in training",
            "        :type templates: list of Templates",
            "        :param trace: verbosity level",
            "        :type trace: int",
            "        :param deterministic: if True, adjudicate ties deterministically",
            "        :type deterministic: bool",
            "        :param ruleformat: format of reported Rules",
            "        :type ruleformat: str",
            "        :return: An untrained BrillTagger",
            "        :rtype: BrillTagger",
            "        \"\"\"",
            "",
            "        if deterministic is None:",
            "            deterministic = trace > 0",
            "        self._initial_tagger = initial_tagger",
            "        self._templates = templates",
            "        self._trace = trace",
            "        self._deterministic = deterministic",
            "        self._ruleformat = ruleformat",
            "",
            "        self._tag_positions = None",
            "        \"\"\"Mapping from tags to lists of positions that use that tag.\"\"\"",
            "",
            "        self._rules_by_position = None",
            "        \"\"\"Mapping from positions to the set of rules that are known",
            "           to occur at that position.  Position is (sentnum, wordnum).",
            "           Initially, this will only contain positions where each rule",
            "           applies in a helpful way; but when we examine a rule, we'll",
            "           extend this list to also include positions where each rule",
            "           applies in a harmful or neutral way.\"\"\"",
            "",
            "        self._positions_by_rule = None",
            "        \"\"\"Mapping from rule to position to effect, specifying the",
            "           effect that each rule has on the overall score, at each",
            "           position.  Position is (sentnum, wordnum); and effect is",
            "           -1, 0, or 1.  As with _rules_by_position, this mapping starts",
            "           out only containing rules with positive effects; but when",
            "           we examine a rule, we'll extend this mapping to include",
            "           the positions where the rule is harmful or neutral.\"\"\"",
            "",
            "        self._rules_by_score = None",
            "        \"\"\"Mapping from scores to the set of rules whose effect on the",
            "           overall score is upper bounded by that score.  Invariant:",
            "           rulesByScore[s] will contain r iff the sum of",
            "           _positions_by_rule[r] is s.\"\"\"",
            "",
            "        self._rule_scores = None",
            "        \"\"\"Mapping from rules to upper bounds on their effects on the",
            "           overall score.  This is the inverse mapping to _rules_by_score.",
            "           Invariant: ruleScores[r] = sum(_positions_by_rule[r])\"\"\"",
            "",
            "        self._first_unknown_position = None",
            "        \"\"\"Mapping from rules to the first position where we're unsure",
            "           if the rule applies.  This records the next position we",
            "           need to check to see if the rule messed anything up.\"\"\"",
            "",
            "    # Training",
            "",
            "    def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):",
            "        \"\"\"",
            "        Trains the Brill tagger on the corpus *train_sents*,",
            "        producing at most *max_rules* transformations, each of which",
            "        reduces the net number of errors in the corpus by at least",
            "        *min_score*, and each of which has accuracy not lower than",
            "        *min_acc*.",
            "",
            "        >>> # Relevant imports",
            "        >>> from nltk.tbl.template import Template",
            "        >>> from nltk.tag.brill import Pos, Word",
            "        >>> from nltk.tag import untag, RegexpTagger, BrillTaggerTrainer",
            "",
            "        >>> # Load some data",
            "        >>> from nltk.corpus import treebank",
            "        >>> training_data = treebank.tagged_sents()[:100]",
            "        >>> baseline_data = treebank.tagged_sents()[100:200]",
            "        >>> gold_data = treebank.tagged_sents()[200:300]",
            "        >>> testing_data = [untag(s) for s in gold_data]",
            "",
            "        >>> backoff = RegexpTagger([",
            "        ... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers",
            "        ... (r'(The|the|A|a|An|an)$', 'AT'),   # articles",
            "        ... (r'.*able$', 'JJ'),                # adjectives",
            "        ... (r'.*ness$', 'NN'),                # nouns formed from adjectives",
            "        ... (r'.*ly$', 'RB'),                  # adverbs",
            "        ... (r'.*s$', 'NNS'),                  # plural nouns",
            "        ... (r'.*ing$', 'VBG'),                # gerunds",
            "        ... (r'.*ed$', 'VBD'),                 # past tense verbs",
            "        ... (r'.*', 'NN')                      # nouns (default)",
            "        ... ])",
            "",
            "        >>> baseline = backoff #see NOTE1",
            "",
            "        >>> baseline.evaluate(gold_data) #doctest: +ELLIPSIS",
            "        0.2450142...",
            "",
            "        >>> # Set up templates",
            "        >>> Template._cleartemplates() #clear any templates created in earlier tests",
            "        >>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",
            "",
            "        >>> # Construct a BrillTaggerTrainer",
            "        >>> tt = BrillTaggerTrainer(baseline, templates, trace=3)",
            "",
            "        >>> tagger1 = tt.train(training_data, max_rules=10)",
            "        TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)",
            "        Finding initial useful rules...",
            "            Found 845 useful rules.",
            "        <BLANKLINE>",
            "                   B      |",
            "           S   F   r   O  |        Score = Fixed - Broken",
            "           c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct",
            "           o   x   k   h  |  u     Broken = num tags changed correct -> incorrect",
            "           r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect",
            "           e   d   n   r  |  e",
            "        ------------------+-------------------------------------------------------",
            "         132 132   0   0  | AT->DT if Pos:NN@[-1]",
            "          85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]",
            "          69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]",
            "          51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]",
            "          47  63  16 161  | NN->IN if Pos:NNS@[-1]",
            "          33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]",
            "          26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]",
            "          24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]",
            "          22  27   5  24  | NN->-NONE- if Pos:VBD@[-1]",
            "          17  17   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]",
            "",
            "        >>> tagger1.rules()[1:3]",
            "        (Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))",
            "",
            "        >>> train_stats = tagger1.train_stats()",
            "        >>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]",
            "        [1775, 1269, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]",
            "",
            "        >>> tagger1.print_template_statistics(printunused=False)",
            "        TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)",
            "        TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750",
            "        #ID | Score (train) |  #Rules     | Template",
            "        --------------------------------------------",
            "        001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))",
            "        000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))",
            "        <BLANKLINE>",
            "        <BLANKLINE>",
            "",
            "        >>> tagger1.evaluate(gold_data) # doctest: +ELLIPSIS",
            "        0.43996...",
            "",
            "        >>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)",
            "",
            "        >>> tagged[33][12:] == [('foreign', 'IN'), ('debt', 'NN'), ('of', 'IN'), ('$', 'NN'), ('64', 'CD'),",
            "        ... ('billion', 'NN'), ('*U*', 'NN'), ('--', 'NN'), ('the', 'DT'), ('third-highest', 'NN'), ('in', 'NN'),",
            "        ... ('the', 'DT'), ('developing', 'VBG'), ('world', 'NN'), ('.', '.')]",
            "        True",
            "",
            "        >>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]",
            "        [1855, 1376, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]",
            "",
            "        >>> # A high-accuracy tagger",
            "        >>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99)",
            "        TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: 0.99)",
            "        Finding initial useful rules...",
            "            Found 845 useful rules.",
            "        <BLANKLINE>",
            "                   B      |",
            "           S   F   r   O  |        Score = Fixed - Broken",
            "           c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct",
            "           o   x   k   h  |  u     Broken = num tags changed correct -> incorrect",
            "           r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect",
            "           e   d   n   r  |  e",
            "        ------------------+-------------------------------------------------------",
            "         132 132   0   0  | AT->DT if Pos:NN@[-1]",
            "          85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]",
            "          69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]",
            "          51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]",
            "          36  36   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]",
            "          26  26   0   0  | NN->. if Pos:NNS@[-1] & Word:.@[0]",
            "          24  24   0   0  | NN->, if Pos:NNS@[-1] & Word:,@[0]",
            "          19  19   0   6  | NN->VB if Pos:TO@[-1]",
            "          18  18   0   0  | CD->-NONE- if Pos:NN@[-1] & Word:0@[0]",
            "          18  18   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]",
            "",
            "        >>> tagger2.evaluate(gold_data)  # doctest: +ELLIPSIS",
            "        0.44159544...",
            "        >>> tagger2.rules()[2:4]",
            "        (Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))",
            "",
            "        # NOTE1: (!!FIXME) A far better baseline uses nltk.tag.UnigramTagger,",
            "        # with a RegexpTagger only as backoff. For instance,",
            "        # >>> baseline = UnigramTagger(baseline_data, backoff=backoff)",
            "        # However, as of Nov 2013, nltk.tag.UnigramTagger does not yield consistent results",
            "        # between python versions. The simplistic backoff above is a workaround to make doctests",
            "        # get consistent input.",
            "",
            "        :param train_sents: training data",
            "        :type train_sents: list(list(tuple))",
            "        :param max_rules: output at most max_rules rules",
            "        :type max_rules: int",
            "        :param min_score: stop training when no rules better than min_score can be found",
            "        :type min_score: int",
            "        :param min_acc: discard any rule with lower accuracy than min_acc",
            "        :type min_acc: float or None",
            "        :return: the learned tagger",
            "        :rtype: BrillTagger",
            "        \"\"\"",
            "        # FIXME: several tests are a bit too dependent on tracing format",
            "        # FIXME: tests in trainer.fast and trainer.brillorig are exact duplicates",
            "",
            "        # Basic idea: Keep track of the rules that apply at each position.",
            "        # And keep track of the positions to which each rule applies.",
            "",
            "        # Create a new copy of the training corpus, and run the",
            "        # initial tagger on it.  We will progressively update this",
            "        # test corpus to look more like the training corpus.",
            "        test_sents = [",
            "            list(self._initial_tagger.tag(untag(sent))) for sent in train_sents",
            "        ]",
            "",
            "        # Collect some statistics on the training process",
            "        trainstats = {}",
            "        trainstats[\"min_acc\"] = min_acc",
            "        trainstats[\"min_score\"] = min_score",
            "        trainstats[\"tokencount\"] = sum(len(t) for t in test_sents)",
            "        trainstats[\"sequencecount\"] = len(test_sents)",
            "        trainstats[\"templatecount\"] = len(self._templates)",
            "        trainstats[\"rulescores\"] = []",
            "        trainstats[\"initialerrors\"] = sum(",
            "            tag[1] != truth[1]",
            "            for paired in zip(test_sents, train_sents)",
            "            for (tag, truth) in zip(*paired)",
            "        )",
            "        trainstats[\"initialacc\"] = (",
            "            1 - trainstats[\"initialerrors\"] / trainstats[\"tokencount\"]",
            "        )",
            "        if self._trace > 0:",
            "            print(",
            "                \"TBL train (fast) (seqs: {sequencecount}; tokens: {tokencount}; \"",
            "                \"tpls: {templatecount}; min score: {min_score}; min acc: {min_acc})\".format(",
            "                    **trainstats",
            "                )",
            "            )",
            "",
            "        # Initialize our mappings.  This will find any errors made",
            "        # by the initial tagger, and use those to generate repair",
            "        # rules, which are added to the rule mappings.",
            "        if self._trace:",
            "            print(\"Finding initial useful rules...\")",
            "        self._init_mappings(test_sents, train_sents)",
            "        if self._trace:",
            "            print(f\"    Found {len(self._rule_scores)} useful rules.\")",
            "",
            "        # Let the user know what we're up to.",
            "        if self._trace > 2:",
            "            self._trace_header()",
            "        elif self._trace == 1:",
            "            print(\"Selecting rules...\")",
            "",
            "        # Repeatedly select the best rule, and add it to `rules`.",
            "        rules = []",
            "        try:",
            "            while len(rules) < max_rules:",
            "                # Find the best rule, and add it to our rule list.",
            "                rule = self._best_rule(train_sents, test_sents, min_score, min_acc)",
            "                if rule:",
            "                    rules.append(rule)",
            "                    score = self._rule_scores[rule]",
            "                    trainstats[\"rulescores\"].append(score)",
            "                else:",
            "                    break  # No more good rules left!",
            "",
            "                # Report the rule that we found.",
            "                if self._trace > 1:",
            "                    self._trace_rule(rule)",
            "",
            "                # Apply the new rule at the relevant sites",
            "                self._apply_rule(rule, test_sents)",
            "",
            "                # Update _tag_positions[rule.original_tag] and",
            "                # _tag_positions[rule.replacement_tag] for the affected",
            "                # positions (i.e., self._positions_by_rule[rule]).",
            "                self._update_tag_positions(rule)",
            "",
            "                # Update rules that were affected by the change.",
            "                self._update_rules(rule, train_sents, test_sents)",
            "",
            "        # The user can cancel training manually:",
            "        except KeyboardInterrupt:",
            "            print(f\"Training stopped manually -- {len(rules)} rules found\")",
            "",
            "        # Discard our tag position mapping & rule mappings.",
            "        self._clean()",
            "        trainstats[\"finalerrors\"] = trainstats[\"initialerrors\"] - sum(",
            "            trainstats[\"rulescores\"]",
            "        )",
            "        trainstats[\"finalacc\"] = (",
            "            1 - trainstats[\"finalerrors\"] / trainstats[\"tokencount\"]",
            "        )",
            "        # Create and return a tagger from the rules we found.",
            "        return BrillTagger(self._initial_tagger, rules, trainstats)",
            "",
            "    def _init_mappings(self, test_sents, train_sents):",
            "        \"\"\"",
            "        Initialize the tag position mapping & the rule related",
            "        mappings.  For each error in test_sents, find new rules that",
            "        would correct them, and add them to the rule mappings.",
            "        \"\"\"",
            "        self._tag_positions = defaultdict(list)",
            "        self._rules_by_position = defaultdict(set)",
            "        self._positions_by_rule = defaultdict(dict)",
            "        self._rules_by_score = defaultdict(set)",
            "        self._rule_scores = defaultdict(int)",
            "        self._first_unknown_position = defaultdict(int)",
            "        # Scan through the corpus, initializing the tag_positions",
            "        # mapping and all the rule-related mappings.",
            "        for sentnum, sent in enumerate(test_sents):",
            "            for wordnum, (word, tag) in enumerate(sent):",
            "",
            "                # Initialize tag_positions",
            "                self._tag_positions[tag].append((sentnum, wordnum))",
            "",
            "                # If it's an error token, update the rule-related mappings.",
            "                correct_tag = train_sents[sentnum][wordnum][1]",
            "                if tag != correct_tag:",
            "                    for rule in self._find_rules(sent, wordnum, correct_tag):",
            "                        self._update_rule_applies(rule, sentnum, wordnum, train_sents)",
            "",
            "    def _clean(self):",
            "        self._tag_positions = None",
            "        self._rules_by_position = None",
            "        self._positions_by_rule = None",
            "        self._rules_by_score = None",
            "        self._rule_scores = None",
            "        self._first_unknown_position = None",
            "",
            "    def _find_rules(self, sent, wordnum, new_tag):",
            "        \"\"\"",
            "        Use the templates to find rules that apply at index *wordnum*",
            "        in the sentence *sent* and generate the tag *new_tag*.",
            "        \"\"\"",
            "        for template in self._templates:",
            "            yield from template.applicable_rules(sent, wordnum, new_tag)",
            "",
            "    def _update_rule_applies(self, rule, sentnum, wordnum, train_sents):",
            "        \"\"\"",
            "        Update the rule data tables to reflect the fact that",
            "        *rule* applies at the position *(sentnum, wordnum)*.",
            "        \"\"\"",
            "        pos = sentnum, wordnum",
            "",
            "        # If the rule is already known to apply here, ignore.",
            "        # (This only happens if the position's tag hasn't changed.)",
            "        if pos in self._positions_by_rule[rule]:",
            "            return",
            "",
            "        # Update self._positions_by_rule.",
            "        correct_tag = train_sents[sentnum][wordnum][1]",
            "        if rule.replacement_tag == correct_tag:",
            "            self._positions_by_rule[rule][pos] = 1",
            "        elif rule.original_tag == correct_tag:",
            "            self._positions_by_rule[rule][pos] = -1",
            "        else:  # was wrong, remains wrong",
            "            self._positions_by_rule[rule][pos] = 0",
            "",
            "        # Update _rules_by_position",
            "        self._rules_by_position[pos].add(rule)",
            "",
            "        # Update _rule_scores.",
            "        old_score = self._rule_scores[rule]",
            "        self._rule_scores[rule] += self._positions_by_rule[rule][pos]",
            "",
            "        # Update _rules_by_score.",
            "        self._rules_by_score[old_score].discard(rule)",
            "        self._rules_by_score[self._rule_scores[rule]].add(rule)",
            "",
            "    def _update_rule_not_applies(self, rule, sentnum, wordnum):",
            "        \"\"\"",
            "        Update the rule data tables to reflect the fact that *rule*",
            "        does not apply at the position *(sentnum, wordnum)*.",
            "        \"\"\"",
            "        pos = sentnum, wordnum",
            "",
            "        # Update _rule_scores.",
            "        old_score = self._rule_scores[rule]",
            "        self._rule_scores[rule] -= self._positions_by_rule[rule][pos]",
            "",
            "        # Update _rules_by_score.",
            "        self._rules_by_score[old_score].discard(rule)",
            "        self._rules_by_score[self._rule_scores[rule]].add(rule)",
            "",
            "        # Update _positions_by_rule",
            "        del self._positions_by_rule[rule][pos]",
            "        self._rules_by_position[pos].remove(rule)",
            "",
            "        # Optional addition: if the rule now applies nowhere, delete",
            "        # all its dictionary entries.",
            "",
            "    def _best_rule(self, train_sents, test_sents, min_score, min_acc):",
            "        \"\"\"",
            "        Find the next best rule.  This is done by repeatedly taking a",
            "        rule with the highest score and stepping through the corpus to",
            "        see where it applies.  When it makes an error (decreasing its",
            "        score) it's bumped down, and we try a new rule with the",
            "        highest score.  When we find a rule which has the highest",
            "        score *and* which has been tested against the entire corpus, we",
            "        can conclude that it's the next best rule.",
            "        \"\"\"",
            "        for max_score in sorted(self._rules_by_score.keys(), reverse=True):",
            "            if len(self._rules_by_score) == 0:",
            "                return None",
            "            if max_score < min_score or max_score <= 0:",
            "                return None",
            "            best_rules = list(self._rules_by_score[max_score])",
            "            if self._deterministic:",
            "                best_rules.sort(key=repr)",
            "            for rule in best_rules:",
            "                positions = self._tag_positions[rule.original_tag]",
            "",
            "                unk = self._first_unknown_position.get(rule, (0, -1))",
            "                start = bisect.bisect_left(positions, unk)",
            "",
            "                for i in range(start, len(positions)):",
            "                    sentnum, wordnum = positions[i]",
            "                    if rule.applies(test_sents[sentnum], wordnum):",
            "                        self._update_rule_applies(rule, sentnum, wordnum, train_sents)",
            "                        if self._rule_scores[rule] < max_score:",
            "                            self._first_unknown_position[rule] = (sentnum, wordnum + 1)",
            "                            break  # The update demoted the rule.",
            "",
            "                if self._rule_scores[rule] == max_score:",
            "                    self._first_unknown_position[rule] = (len(train_sents) + 1, 0)",
            "                    # optimization: if no min_acc threshold given, don't bother computing accuracy",
            "                    if min_acc is None:",
            "                        return rule",
            "                    else:",
            "                        changes = self._positions_by_rule[rule].values()",
            "                        num_fixed = len([c for c in changes if c == 1])",
            "                        num_broken = len([c for c in changes if c == -1])",
            "                        # acc here is fixed/(fixed+broken); could also be",
            "                        # fixed/(fixed+broken+other) == num_fixed/len(changes)",
            "                        acc = num_fixed / (num_fixed + num_broken)",
            "                        if acc >= min_acc:",
            "                            return rule",
            "                        # else: rule too inaccurate, discard and try next",
            "",
            "            # We demoted (or skipped due to < min_acc, if that was given)",
            "            # all the rules with score==max_score.",
            "",
            "            assert min_acc is not None or not self._rules_by_score[max_score]",
            "            if not self._rules_by_score[max_score]:",
            "                del self._rules_by_score[max_score]",
            "",
            "    def _apply_rule(self, rule, test_sents):",
            "        \"\"\"",
            "        Update *test_sents* by applying *rule* everywhere where its",
            "        conditions are met.",
            "        \"\"\"",
            "        update_positions = set(self._positions_by_rule[rule])",
            "        new_tag = rule.replacement_tag",
            "",
            "        if self._trace > 3:",
            "            self._trace_apply(len(update_positions))",
            "",
            "        # Update test_sents.",
            "        for (sentnum, wordnum) in update_positions:",
            "            text = test_sents[sentnum][wordnum][0]",
            "            test_sents[sentnum][wordnum] = (text, new_tag)",
            "",
            "    def _update_tag_positions(self, rule):",
            "        \"\"\"",
            "        Update _tag_positions to reflect the changes to tags that are",
            "        made by *rule*.",
            "        \"\"\"",
            "        # Update the tag index.",
            "        for pos in self._positions_by_rule[rule]:",
            "            # Delete the old tag.",
            "            old_tag_positions = self._tag_positions[rule.original_tag]",
            "            old_index = bisect.bisect_left(old_tag_positions, pos)",
            "            del old_tag_positions[old_index]",
            "            # Insert the new tag.",
            "            new_tag_positions = self._tag_positions[rule.replacement_tag]",
            "            bisect.insort_left(new_tag_positions, pos)",
            "",
            "    def _update_rules(self, rule, train_sents, test_sents):",
            "        \"\"\"",
            "        Check if we should add or remove any rules from consideration,",
            "        given the changes made by *rule*.",
            "        \"\"\"",
            "        # Collect a list of all positions that might be affected.",
            "        neighbors = set()",
            "        for sentnum, wordnum in self._positions_by_rule[rule]:",
            "            for template in self._templates:",
            "                n = template.get_neighborhood(test_sents[sentnum], wordnum)",
            "                neighbors.update([(sentnum, i) for i in n])",
            "",
            "        # Update the rules at each position.",
            "        num_obsolete = num_new = num_unseen = 0",
            "        for sentnum, wordnum in neighbors:",
            "            test_sent = test_sents[sentnum]",
            "            correct_tag = train_sents[sentnum][wordnum][1]",
            "",
            "            # Check if the change causes any rule at this position to",
            "            # stop matching; if so, then update our rule mappings",
            "            # accordingly.",
            "            old_rules = set(self._rules_by_position[sentnum, wordnum])",
            "            for old_rule in old_rules:",
            "                if not old_rule.applies(test_sent, wordnum):",
            "                    num_obsolete += 1",
            "                    self._update_rule_not_applies(old_rule, sentnum, wordnum)",
            "",
            "            # Check if the change causes our templates to propose any",
            "            # new rules for this position.",
            "            for template in self._templates:",
            "                for new_rule in template.applicable_rules(",
            "                    test_sent, wordnum, correct_tag",
            "                ):",
            "                    if new_rule not in old_rules:",
            "                        num_new += 1",
            "                        if new_rule not in self._rule_scores:",
            "                            num_unseen += 1",
            "                        old_rules.add(new_rule)",
            "                        self._update_rule_applies(",
            "                            new_rule, sentnum, wordnum, train_sents",
            "                        )",
            "",
            "            # We may have caused other rules to match here, that are",
            "            # not proposed by our templates -- in particular, rules",
            "            # that are harmful or neutral.  We therefore need to",
            "            # update any rule whose first_unknown_position is past",
            "            # this rule.",
            "            for new_rule, pos in self._first_unknown_position.items():",
            "                if pos > (sentnum, wordnum):",
            "                    if new_rule not in old_rules:",
            "                        num_new += 1",
            "                        if new_rule.applies(test_sent, wordnum):",
            "                            self._update_rule_applies(",
            "                                new_rule, sentnum, wordnum, train_sents",
            "                            )",
            "",
            "        if self._trace > 3:",
            "            self._trace_update_rules(num_obsolete, num_new, num_unseen)",
            "",
            "    # Tracing",
            "",
            "    def _trace_header(self):",
            "        print(",
            "            \"\"\"",
            "           B      |",
            "   S   F   r   O  |        Score = Fixed - Broken",
            "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct",
            "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect",
            "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect",
            "   e   d   n   r  |  e",
            "------------------+-------------------------------------------------------",
            "        \"\"\".rstrip()",
            "        )",
            "",
            "    def _trace_rule(self, rule):",
            "        assert self._rule_scores[rule] == sum(self._positions_by_rule[rule].values())",
            "",
            "        changes = self._positions_by_rule[rule].values()",
            "        num_fixed = len([c for c in changes if c == 1])",
            "        num_broken = len([c for c in changes if c == -1])",
            "        num_other = len([c for c in changes if c == 0])",
            "        score = self._rule_scores[rule]",
            "",
            "        rulestr = rule.format(self._ruleformat)",
            "        if self._trace > 2:",
            "            print(",
            "                \"{:4d}{:4d}{:4d}{:4d}  |\".format(",
            "                    score, num_fixed, num_broken, num_other",
            "                ),",
            "                end=\" \",",
            "            )",
            "            print(",
            "                textwrap.fill(",
            "                    rulestr,",
            "                    initial_indent=\" \" * 20,",
            "                    width=79,",
            "                    subsequent_indent=\" \" * 18 + \"|   \",",
            "                ).strip()",
            "            )",
            "        else:",
            "            print(rulestr)",
            "",
            "    def _trace_apply(self, num_updates):",
            "        prefix = \" \" * 18 + \"|\"",
            "        print(prefix)",
            "        print(prefix, f\"Applying rule to {num_updates} positions.\")",
            "",
            "    def _trace_update_rules(self, num_obsolete, num_new, num_unseen):",
            "        prefix = \" \" * 18 + \"|\"",
            "        print(prefix, \"Updated rule tables:\")",
            "        print(prefix, (f\"  - {num_obsolete} rule applications removed\"))",
            "        print(",
            "            prefix,",
            "            (f\"  - {num_new} rule applications added ({num_unseen} novel)\"),",
            "        )",
            "        print(prefix)"
        ],
        "afterPatchFile": [
            "# Natural Language Toolkit: Transformation-based learning",
            "#",
            "# Copyright (C) 2001-2013 NLTK Project",
            "# Author: Marcus Uneson <marcus.uneson@gmail.com>",
            "#   based on previous (nltk2) version by",
            "#   Christopher Maloof, Edward Loper, Steven Bird",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see  LICENSE.TXT",
            "",
            "import bisect",
            "import textwrap",
            "from collections import defaultdict",
            "",
            "from nltk.tag import BrillTagger, untag",
            "",
            "######################################################################",
            "#  Brill Tagger Trainer",
            "######################################################################",
            "",
            "",
            "class BrillTaggerTrainer:",
            "    \"\"\"",
            "    A trainer for tbl taggers.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self, initial_tagger, templates, trace=0, deterministic=None, ruleformat=\"str\"",
            "    ):",
            "        \"\"\"",
            "        Construct a Brill tagger from a baseline tagger and a",
            "        set of templates",
            "",
            "        :param initial_tagger: the baseline tagger",
            "        :type initial_tagger: Tagger",
            "        :param templates: templates to be used in training",
            "        :type templates: list of Templates",
            "        :param trace: verbosity level",
            "        :type trace: int",
            "        :param deterministic: if True, adjudicate ties deterministically",
            "        :type deterministic: bool",
            "        :param ruleformat: format of reported Rules",
            "        :type ruleformat: str",
            "        :return: An untrained BrillTagger",
            "        :rtype: BrillTagger",
            "        \"\"\"",
            "",
            "        if deterministic is None:",
            "            deterministic = trace > 0",
            "        self._initial_tagger = initial_tagger",
            "        self._templates = templates",
            "        self._trace = trace",
            "        self._deterministic = deterministic",
            "        self._ruleformat = ruleformat",
            "",
            "        self._tag_positions = None",
            "        \"\"\"Mapping from tags to lists of positions that use that tag.\"\"\"",
            "",
            "        self._rules_by_position = None",
            "        \"\"\"Mapping from positions to the set of rules that are known",
            "           to occur at that position.  Position is (sentnum, wordnum).",
            "           Initially, this will only contain positions where each rule",
            "           applies in a helpful way; but when we examine a rule, we'll",
            "           extend this list to also include positions where each rule",
            "           applies in a harmful or neutral way.\"\"\"",
            "",
            "        self._positions_by_rule = None",
            "        \"\"\"Mapping from rule to position to effect, specifying the",
            "           effect that each rule has on the overall score, at each",
            "           position.  Position is (sentnum, wordnum); and effect is",
            "           -1, 0, or 1.  As with _rules_by_position, this mapping starts",
            "           out only containing rules with positive effects; but when",
            "           we examine a rule, we'll extend this mapping to include",
            "           the positions where the rule is harmful or neutral.\"\"\"",
            "",
            "        self._rules_by_score = None",
            "        \"\"\"Mapping from scores to the set of rules whose effect on the",
            "           overall score is upper bounded by that score.  Invariant:",
            "           rulesByScore[s] will contain r iff the sum of",
            "           _positions_by_rule[r] is s.\"\"\"",
            "",
            "        self._rule_scores = None",
            "        \"\"\"Mapping from rules to upper bounds on their effects on the",
            "           overall score.  This is the inverse mapping to _rules_by_score.",
            "           Invariant: ruleScores[r] = sum(_positions_by_rule[r])\"\"\"",
            "",
            "        self._first_unknown_position = None",
            "        \"\"\"Mapping from rules to the first position where we're unsure",
            "           if the rule applies.  This records the next position we",
            "           need to check to see if the rule messed anything up.\"\"\"",
            "",
            "    # Training",
            "",
            "    def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):",
            "        r\"\"\"",
            "        Trains the Brill tagger on the corpus *train_sents*,",
            "        producing at most *max_rules* transformations, each of which",
            "        reduces the net number of errors in the corpus by at least",
            "        *min_score*, and each of which has accuracy not lower than",
            "        *min_acc*.",
            "",
            "        >>> # Relevant imports",
            "        >>> from nltk.tbl.template import Template",
            "        >>> from nltk.tag.brill import Pos, Word",
            "        >>> from nltk.tag import untag, RegexpTagger, BrillTaggerTrainer",
            "",
            "        >>> # Load some data",
            "        >>> from nltk.corpus import treebank",
            "        >>> training_data = treebank.tagged_sents()[:100]",
            "        >>> baseline_data = treebank.tagged_sents()[100:200]",
            "        >>> gold_data = treebank.tagged_sents()[200:300]",
            "        >>> testing_data = [untag(s) for s in gold_data]",
            "",
            "        >>> backoff = RegexpTagger([",
            "        ... (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers",
            "        ... (r'(The|the|A|a|An|an)$', 'AT'),   # articles",
            "        ... (r'.*able$', 'JJ'),                # adjectives",
            "        ... (r'.*ness$', 'NN'),                # nouns formed from adjectives",
            "        ... (r'.*ly$', 'RB'),                  # adverbs",
            "        ... (r'.*s$', 'NNS'),                  # plural nouns",
            "        ... (r'.*ing$', 'VBG'),                # gerunds",
            "        ... (r'.*ed$', 'VBD'),                 # past tense verbs",
            "        ... (r'.*', 'NN')                      # nouns (default)",
            "        ... ])",
            "",
            "        >>> baseline = backoff #see NOTE1",
            "",
            "        >>> baseline.evaluate(gold_data) #doctest: +ELLIPSIS",
            "        0.2433862...",
            "",
            "        >>> # Set up templates",
            "        >>> Template._cleartemplates() #clear any templates created in earlier tests",
            "        >>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]",
            "",
            "        >>> # Construct a BrillTaggerTrainer",
            "        >>> tt = BrillTaggerTrainer(baseline, templates, trace=3)",
            "",
            "        >>> tagger1 = tt.train(training_data, max_rules=10)",
            "        TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)",
            "        Finding initial useful rules...",
            "            Found 847 useful rules.",
            "        <BLANKLINE>",
            "                   B      |",
            "           S   F   r   O  |        Score = Fixed - Broken",
            "           c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct",
            "           o   x   k   h  |  u     Broken = num tags changed correct -> incorrect",
            "           r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect",
            "           e   d   n   r  |  e",
            "        ------------------+-------------------------------------------------------",
            "         132 132   0   0  | AT->DT if Pos:NN@[-1]",
            "          85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]",
            "          69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]",
            "          51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]",
            "          47  63  16 162  | NN->IN if Pos:NNS@[-1]",
            "          33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]",
            "          26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]",
            "          24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]",
            "          22  27   5  24  | NN->-NONE- if Pos:VBD@[-1]",
            "          17  17   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]",
            "",
            "        >>> tagger1.rules()[1:3]",
            "        (Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))",
            "",
            "        >>> train_stats = tagger1.train_stats()",
            "        >>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]",
            "        [1776, 1270, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]",
            "",
            "        >>> tagger1.print_template_statistics(printunused=False)",
            "        TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)",
            "        TRAIN (   2417 tokens) initial  1776 0.2652 final:  1270 0.4746",
            "        #ID | Score (train) |  #Rules     | Template",
            "        --------------------------------------------",
            "        001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))",
            "        000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))",
            "        <BLANKLINE>",
            "        <BLANKLINE>",
            "",
            "        >>> tagger1.evaluate(gold_data) # doctest: +ELLIPSIS",
            "        0.43833...",
            "",
            "        >>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)",
            "",
            "        >>> tagged[33][12:] == [('foreign', 'IN'), ('debt', 'NN'), ('of', 'IN'), ('$', 'NN'), ('64', 'CD'),",
            "        ... ('billion', 'NN'), ('*U*', 'NN'), ('--', 'NN'), ('the', 'DT'), ('third-highest', 'NN'), ('in', 'NN'),",
            "        ... ('the', 'DT'), ('developing', 'VBG'), ('world', 'NN'), ('.', '.')]",
            "        True",
            "",
            "        >>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]",
            "        [1859, 1380, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]",
            "",
            "        >>> # A high-accuracy tagger",
            "        >>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99)",
            "        TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: 0.99)",
            "        Finding initial useful rules...",
            "            Found 847 useful rules.",
            "        <BLANKLINE>",
            "                   B      |",
            "           S   F   r   O  |        Score = Fixed - Broken",
            "           c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct",
            "           o   x   k   h  |  u     Broken = num tags changed correct -> incorrect",
            "           r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect",
            "           e   d   n   r  |  e",
            "        ------------------+-------------------------------------------------------",
            "         132 132   0   0  | AT->DT if Pos:NN@[-1]",
            "          85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]",
            "          69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]",
            "          51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]",
            "          36  36   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]",
            "          26  26   0   0  | NN->. if Pos:NNS@[-1] & Word:.@[0]",
            "          24  24   0   0  | NN->, if Pos:NNS@[-1] & Word:,@[0]",
            "          19  19   0   6  | NN->VB if Pos:TO@[-1]",
            "          18  18   0   0  | CD->-NONE- if Pos:NN@[-1] & Word:0@[0]",
            "          18  18   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]",
            "",
            "        >>> tagger2.evaluate(gold_data)  # doctest: +ELLIPSIS",
            "        0.43996743...",
            "        >>> tagger2.rules()[2:4]",
            "        (Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))",
            "",
            "        # NOTE1: (!!FIXME) A far better baseline uses nltk.tag.UnigramTagger,",
            "        # with a RegexpTagger only as backoff. For instance,",
            "        # >>> baseline = UnigramTagger(baseline_data, backoff=backoff)",
            "        # However, as of Nov 2013, nltk.tag.UnigramTagger does not yield consistent results",
            "        # between python versions. The simplistic backoff above is a workaround to make doctests",
            "        # get consistent input.",
            "",
            "        :param train_sents: training data",
            "        :type train_sents: list(list(tuple))",
            "        :param max_rules: output at most max_rules rules",
            "        :type max_rules: int",
            "        :param min_score: stop training when no rules better than min_score can be found",
            "        :type min_score: int",
            "        :param min_acc: discard any rule with lower accuracy than min_acc",
            "        :type min_acc: float or None",
            "        :return: the learned tagger",
            "        :rtype: BrillTagger",
            "        \"\"\"",
            "        # FIXME: several tests are a bit too dependent on tracing format",
            "        # FIXME: tests in trainer.fast and trainer.brillorig are exact duplicates",
            "",
            "        # Basic idea: Keep track of the rules that apply at each position.",
            "        # And keep track of the positions to which each rule applies.",
            "",
            "        # Create a new copy of the training corpus, and run the",
            "        # initial tagger on it.  We will progressively update this",
            "        # test corpus to look more like the training corpus.",
            "        test_sents = [",
            "            list(self._initial_tagger.tag(untag(sent))) for sent in train_sents",
            "        ]",
            "",
            "        # Collect some statistics on the training process",
            "        trainstats = {}",
            "        trainstats[\"min_acc\"] = min_acc",
            "        trainstats[\"min_score\"] = min_score",
            "        trainstats[\"tokencount\"] = sum(len(t) for t in test_sents)",
            "        trainstats[\"sequencecount\"] = len(test_sents)",
            "        trainstats[\"templatecount\"] = len(self._templates)",
            "        trainstats[\"rulescores\"] = []",
            "        trainstats[\"initialerrors\"] = sum(",
            "            tag[1] != truth[1]",
            "            for paired in zip(test_sents, train_sents)",
            "            for (tag, truth) in zip(*paired)",
            "        )",
            "        trainstats[\"initialacc\"] = (",
            "            1 - trainstats[\"initialerrors\"] / trainstats[\"tokencount\"]",
            "        )",
            "        if self._trace > 0:",
            "            print(",
            "                \"TBL train (fast) (seqs: {sequencecount}; tokens: {tokencount}; \"",
            "                \"tpls: {templatecount}; min score: {min_score}; min acc: {min_acc})\".format(",
            "                    **trainstats",
            "                )",
            "            )",
            "",
            "        # Initialize our mappings.  This will find any errors made",
            "        # by the initial tagger, and use those to generate repair",
            "        # rules, which are added to the rule mappings.",
            "        if self._trace:",
            "            print(\"Finding initial useful rules...\")",
            "        self._init_mappings(test_sents, train_sents)",
            "        if self._trace:",
            "            print(f\"    Found {len(self._rule_scores)} useful rules.\")",
            "",
            "        # Let the user know what we're up to.",
            "        if self._trace > 2:",
            "            self._trace_header()",
            "        elif self._trace == 1:",
            "            print(\"Selecting rules...\")",
            "",
            "        # Repeatedly select the best rule, and add it to `rules`.",
            "        rules = []",
            "        try:",
            "            while len(rules) < max_rules:",
            "                # Find the best rule, and add it to our rule list.",
            "                rule = self._best_rule(train_sents, test_sents, min_score, min_acc)",
            "                if rule:",
            "                    rules.append(rule)",
            "                    score = self._rule_scores[rule]",
            "                    trainstats[\"rulescores\"].append(score)",
            "                else:",
            "                    break  # No more good rules left!",
            "",
            "                # Report the rule that we found.",
            "                if self._trace > 1:",
            "                    self._trace_rule(rule)",
            "",
            "                # Apply the new rule at the relevant sites",
            "                self._apply_rule(rule, test_sents)",
            "",
            "                # Update _tag_positions[rule.original_tag] and",
            "                # _tag_positions[rule.replacement_tag] for the affected",
            "                # positions (i.e., self._positions_by_rule[rule]).",
            "                self._update_tag_positions(rule)",
            "",
            "                # Update rules that were affected by the change.",
            "                self._update_rules(rule, train_sents, test_sents)",
            "",
            "        # The user can cancel training manually:",
            "        except KeyboardInterrupt:",
            "            print(f\"Training stopped manually -- {len(rules)} rules found\")",
            "",
            "        # Discard our tag position mapping & rule mappings.",
            "        self._clean()",
            "        trainstats[\"finalerrors\"] = trainstats[\"initialerrors\"] - sum(",
            "            trainstats[\"rulescores\"]",
            "        )",
            "        trainstats[\"finalacc\"] = (",
            "            1 - trainstats[\"finalerrors\"] / trainstats[\"tokencount\"]",
            "        )",
            "        # Create and return a tagger from the rules we found.",
            "        return BrillTagger(self._initial_tagger, rules, trainstats)",
            "",
            "    def _init_mappings(self, test_sents, train_sents):",
            "        \"\"\"",
            "        Initialize the tag position mapping & the rule related",
            "        mappings.  For each error in test_sents, find new rules that",
            "        would correct them, and add them to the rule mappings.",
            "        \"\"\"",
            "        self._tag_positions = defaultdict(list)",
            "        self._rules_by_position = defaultdict(set)",
            "        self._positions_by_rule = defaultdict(dict)",
            "        self._rules_by_score = defaultdict(set)",
            "        self._rule_scores = defaultdict(int)",
            "        self._first_unknown_position = defaultdict(int)",
            "        # Scan through the corpus, initializing the tag_positions",
            "        # mapping and all the rule-related mappings.",
            "        for sentnum, sent in enumerate(test_sents):",
            "            for wordnum, (word, tag) in enumerate(sent):",
            "",
            "                # Initialize tag_positions",
            "                self._tag_positions[tag].append((sentnum, wordnum))",
            "",
            "                # If it's an error token, update the rule-related mappings.",
            "                correct_tag = train_sents[sentnum][wordnum][1]",
            "                if tag != correct_tag:",
            "                    for rule in self._find_rules(sent, wordnum, correct_tag):",
            "                        self._update_rule_applies(rule, sentnum, wordnum, train_sents)",
            "",
            "    def _clean(self):",
            "        self._tag_positions = None",
            "        self._rules_by_position = None",
            "        self._positions_by_rule = None",
            "        self._rules_by_score = None",
            "        self._rule_scores = None",
            "        self._first_unknown_position = None",
            "",
            "    def _find_rules(self, sent, wordnum, new_tag):",
            "        \"\"\"",
            "        Use the templates to find rules that apply at index *wordnum*",
            "        in the sentence *sent* and generate the tag *new_tag*.",
            "        \"\"\"",
            "        for template in self._templates:",
            "            yield from template.applicable_rules(sent, wordnum, new_tag)",
            "",
            "    def _update_rule_applies(self, rule, sentnum, wordnum, train_sents):",
            "        \"\"\"",
            "        Update the rule data tables to reflect the fact that",
            "        *rule* applies at the position *(sentnum, wordnum)*.",
            "        \"\"\"",
            "        pos = sentnum, wordnum",
            "",
            "        # If the rule is already known to apply here, ignore.",
            "        # (This only happens if the position's tag hasn't changed.)",
            "        if pos in self._positions_by_rule[rule]:",
            "            return",
            "",
            "        # Update self._positions_by_rule.",
            "        correct_tag = train_sents[sentnum][wordnum][1]",
            "        if rule.replacement_tag == correct_tag:",
            "            self._positions_by_rule[rule][pos] = 1",
            "        elif rule.original_tag == correct_tag:",
            "            self._positions_by_rule[rule][pos] = -1",
            "        else:  # was wrong, remains wrong",
            "            self._positions_by_rule[rule][pos] = 0",
            "",
            "        # Update _rules_by_position",
            "        self._rules_by_position[pos].add(rule)",
            "",
            "        # Update _rule_scores.",
            "        old_score = self._rule_scores[rule]",
            "        self._rule_scores[rule] += self._positions_by_rule[rule][pos]",
            "",
            "        # Update _rules_by_score.",
            "        self._rules_by_score[old_score].discard(rule)",
            "        self._rules_by_score[self._rule_scores[rule]].add(rule)",
            "",
            "    def _update_rule_not_applies(self, rule, sentnum, wordnum):",
            "        \"\"\"",
            "        Update the rule data tables to reflect the fact that *rule*",
            "        does not apply at the position *(sentnum, wordnum)*.",
            "        \"\"\"",
            "        pos = sentnum, wordnum",
            "",
            "        # Update _rule_scores.",
            "        old_score = self._rule_scores[rule]",
            "        self._rule_scores[rule] -= self._positions_by_rule[rule][pos]",
            "",
            "        # Update _rules_by_score.",
            "        self._rules_by_score[old_score].discard(rule)",
            "        self._rules_by_score[self._rule_scores[rule]].add(rule)",
            "",
            "        # Update _positions_by_rule",
            "        del self._positions_by_rule[rule][pos]",
            "        self._rules_by_position[pos].remove(rule)",
            "",
            "        # Optional addition: if the rule now applies nowhere, delete",
            "        # all its dictionary entries.",
            "",
            "    def _best_rule(self, train_sents, test_sents, min_score, min_acc):",
            "        \"\"\"",
            "        Find the next best rule.  This is done by repeatedly taking a",
            "        rule with the highest score and stepping through the corpus to",
            "        see where it applies.  When it makes an error (decreasing its",
            "        score) it's bumped down, and we try a new rule with the",
            "        highest score.  When we find a rule which has the highest",
            "        score *and* which has been tested against the entire corpus, we",
            "        can conclude that it's the next best rule.",
            "        \"\"\"",
            "        for max_score in sorted(self._rules_by_score.keys(), reverse=True):",
            "            if len(self._rules_by_score) == 0:",
            "                return None",
            "            if max_score < min_score or max_score <= 0:",
            "                return None",
            "            best_rules = list(self._rules_by_score[max_score])",
            "            if self._deterministic:",
            "                best_rules.sort(key=repr)",
            "            for rule in best_rules:",
            "                positions = self._tag_positions[rule.original_tag]",
            "",
            "                unk = self._first_unknown_position.get(rule, (0, -1))",
            "                start = bisect.bisect_left(positions, unk)",
            "",
            "                for i in range(start, len(positions)):",
            "                    sentnum, wordnum = positions[i]",
            "                    if rule.applies(test_sents[sentnum], wordnum):",
            "                        self._update_rule_applies(rule, sentnum, wordnum, train_sents)",
            "                        if self._rule_scores[rule] < max_score:",
            "                            self._first_unknown_position[rule] = (sentnum, wordnum + 1)",
            "                            break  # The update demoted the rule.",
            "",
            "                if self._rule_scores[rule] == max_score:",
            "                    self._first_unknown_position[rule] = (len(train_sents) + 1, 0)",
            "                    # optimization: if no min_acc threshold given, don't bother computing accuracy",
            "                    if min_acc is None:",
            "                        return rule",
            "                    else:",
            "                        changes = self._positions_by_rule[rule].values()",
            "                        num_fixed = len([c for c in changes if c == 1])",
            "                        num_broken = len([c for c in changes if c == -1])",
            "                        # acc here is fixed/(fixed+broken); could also be",
            "                        # fixed/(fixed+broken+other) == num_fixed/len(changes)",
            "                        acc = num_fixed / (num_fixed + num_broken)",
            "                        if acc >= min_acc:",
            "                            return rule",
            "                        # else: rule too inaccurate, discard and try next",
            "",
            "            # We demoted (or skipped due to < min_acc, if that was given)",
            "            # all the rules with score==max_score.",
            "",
            "            assert min_acc is not None or not self._rules_by_score[max_score]",
            "            if not self._rules_by_score[max_score]:",
            "                del self._rules_by_score[max_score]",
            "",
            "    def _apply_rule(self, rule, test_sents):",
            "        \"\"\"",
            "        Update *test_sents* by applying *rule* everywhere where its",
            "        conditions are met.",
            "        \"\"\"",
            "        update_positions = set(self._positions_by_rule[rule])",
            "        new_tag = rule.replacement_tag",
            "",
            "        if self._trace > 3:",
            "            self._trace_apply(len(update_positions))",
            "",
            "        # Update test_sents.",
            "        for (sentnum, wordnum) in update_positions:",
            "            text = test_sents[sentnum][wordnum][0]",
            "            test_sents[sentnum][wordnum] = (text, new_tag)",
            "",
            "    def _update_tag_positions(self, rule):",
            "        \"\"\"",
            "        Update _tag_positions to reflect the changes to tags that are",
            "        made by *rule*.",
            "        \"\"\"",
            "        # Update the tag index.",
            "        for pos in self._positions_by_rule[rule]:",
            "            # Delete the old tag.",
            "            old_tag_positions = self._tag_positions[rule.original_tag]",
            "            old_index = bisect.bisect_left(old_tag_positions, pos)",
            "            del old_tag_positions[old_index]",
            "            # Insert the new tag.",
            "            new_tag_positions = self._tag_positions[rule.replacement_tag]",
            "            bisect.insort_left(new_tag_positions, pos)",
            "",
            "    def _update_rules(self, rule, train_sents, test_sents):",
            "        \"\"\"",
            "        Check if we should add or remove any rules from consideration,",
            "        given the changes made by *rule*.",
            "        \"\"\"",
            "        # Collect a list of all positions that might be affected.",
            "        neighbors = set()",
            "        for sentnum, wordnum in self._positions_by_rule[rule]:",
            "            for template in self._templates:",
            "                n = template.get_neighborhood(test_sents[sentnum], wordnum)",
            "                neighbors.update([(sentnum, i) for i in n])",
            "",
            "        # Update the rules at each position.",
            "        num_obsolete = num_new = num_unseen = 0",
            "        for sentnum, wordnum in neighbors:",
            "            test_sent = test_sents[sentnum]",
            "            correct_tag = train_sents[sentnum][wordnum][1]",
            "",
            "            # Check if the change causes any rule at this position to",
            "            # stop matching; if so, then update our rule mappings",
            "            # accordingly.",
            "            old_rules = set(self._rules_by_position[sentnum, wordnum])",
            "            for old_rule in old_rules:",
            "                if not old_rule.applies(test_sent, wordnum):",
            "                    num_obsolete += 1",
            "                    self._update_rule_not_applies(old_rule, sentnum, wordnum)",
            "",
            "            # Check if the change causes our templates to propose any",
            "            # new rules for this position.",
            "            for template in self._templates:",
            "                for new_rule in template.applicable_rules(",
            "                    test_sent, wordnum, correct_tag",
            "                ):",
            "                    if new_rule not in old_rules:",
            "                        num_new += 1",
            "                        if new_rule not in self._rule_scores:",
            "                            num_unseen += 1",
            "                        old_rules.add(new_rule)",
            "                        self._update_rule_applies(",
            "                            new_rule, sentnum, wordnum, train_sents",
            "                        )",
            "",
            "            # We may have caused other rules to match here, that are",
            "            # not proposed by our templates -- in particular, rules",
            "            # that are harmful or neutral.  We therefore need to",
            "            # update any rule whose first_unknown_position is past",
            "            # this rule.",
            "            for new_rule, pos in self._first_unknown_position.items():",
            "                if pos > (sentnum, wordnum):",
            "                    if new_rule not in old_rules:",
            "                        num_new += 1",
            "                        if new_rule.applies(test_sent, wordnum):",
            "                            self._update_rule_applies(",
            "                                new_rule, sentnum, wordnum, train_sents",
            "                            )",
            "",
            "        if self._trace > 3:",
            "            self._trace_update_rules(num_obsolete, num_new, num_unseen)",
            "",
            "    # Tracing",
            "",
            "    def _trace_header(self):",
            "        print(",
            "            \"\"\"",
            "           B      |",
            "   S   F   r   O  |        Score = Fixed - Broken",
            "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct",
            "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect",
            "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect",
            "   e   d   n   r  |  e",
            "------------------+-------------------------------------------------------",
            "        \"\"\".rstrip()",
            "        )",
            "",
            "    def _trace_rule(self, rule):",
            "        assert self._rule_scores[rule] == sum(self._positions_by_rule[rule].values())",
            "",
            "        changes = self._positions_by_rule[rule].values()",
            "        num_fixed = len([c for c in changes if c == 1])",
            "        num_broken = len([c for c in changes if c == -1])",
            "        num_other = len([c for c in changes if c == 0])",
            "        score = self._rule_scores[rule]",
            "",
            "        rulestr = rule.format(self._ruleformat)",
            "        if self._trace > 2:",
            "            print(",
            "                \"{:4d}{:4d}{:4d}{:4d}  |\".format(",
            "                    score, num_fixed, num_broken, num_other",
            "                ),",
            "                end=\" \",",
            "            )",
            "            print(",
            "                textwrap.fill(",
            "                    rulestr,",
            "                    initial_indent=\" \" * 20,",
            "                    width=79,",
            "                    subsequent_indent=\" \" * 18 + \"|   \",",
            "                ).strip()",
            "            )",
            "        else:",
            "            print(rulestr)",
            "",
            "    def _trace_apply(self, num_updates):",
            "        prefix = \" \" * 18 + \"|\"",
            "        print(prefix)",
            "        print(prefix, f\"Applying rule to {num_updates} positions.\")",
            "",
            "    def _trace_update_rules(self, num_obsolete, num_new, num_unseen):",
            "        prefix = \" \" * 18 + \"|\"",
            "        print(prefix, \"Updated rule tables:\")",
            "        print(prefix, (f\"  - {num_obsolete} rule applications removed\"))",
            "        print(",
            "            prefix,",
            "            (f\"  - {num_new} rule applications added ({num_unseen} novel)\"),",
            "        )",
            "        print(prefix)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "94": [
                "BrillTaggerTrainer",
                "train"
            ],
            "114": [
                "BrillTaggerTrainer",
                "train"
            ],
            "128": [
                "BrillTaggerTrainer",
                "train"
            ],
            "140": [
                "BrillTaggerTrainer",
                "train"
            ],
            "153": [
                "BrillTaggerTrainer",
                "train"
            ],
            "165": [
                "BrillTaggerTrainer",
                "train"
            ],
            "169": [
                "BrillTaggerTrainer",
                "train"
            ],
            "178": [
                "BrillTaggerTrainer",
                "train"
            ],
            "188": [
                "BrillTaggerTrainer",
                "train"
            ],
            "194": [
                "BrillTaggerTrainer",
                "train"
            ],
            "215": [
                "BrillTaggerTrainer",
                "train"
            ]
        },
        "addLocation": []
    },
    "nltk/tag/sequential.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 337,
                "afterPatchRowNumber": 337,
                "PatchRowcode": "         >>> test_sent = brown.sents(categories='news')[0]"
            },
            "1": {
                "beforePatchRowNumber": 338,
                "afterPatchRowNumber": 338,
                "PatchRowcode": "         >>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])"
            },
            "2": {
                "beforePatchRowNumber": 339,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "         >>> for tok, tag in unigram_tagger.tag(test_sent):"
            },
            "3": {
                "beforePatchRowNumber": 340,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ...     print(\"({}, {}), \".format(tok, tag))"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 340,
                "PatchRowcode": "+        ...     print(\"({}, {}), \".format(tok, tag)) # doctest: +NORMALIZE_WHITESPACE"
            },
            "5": {
                "beforePatchRowNumber": 341,
                "afterPatchRowNumber": 341,
                "PatchRowcode": "         (The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),"
            },
            "6": {
                "beforePatchRowNumber": 342,
                "afterPatchRowNumber": 342,
                "PatchRowcode": "         (Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),"
            },
            "7": {
                "beforePatchRowNumber": 343,
                "afterPatchRowNumber": 343,
                "PatchRowcode": "         (investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),"
            },
            "8": {
                "beforePatchRowNumber": 491,
                "afterPatchRowNumber": 491,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 492,
                "afterPatchRowNumber": 492,
                "PatchRowcode": " @jsontags.register_tag"
            },
            "10": {
                "beforePatchRowNumber": 493,
                "afterPatchRowNumber": 493,
                "PatchRowcode": " class RegexpTagger(SequentialBackoffTagger):"
            },
            "11": {
                "beforePatchRowNumber": 494,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\""
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 494,
                "PatchRowcode": "+    r\"\"\""
            },
            "13": {
                "beforePatchRowNumber": 495,
                "afterPatchRowNumber": 495,
                "PatchRowcode": "     Regular Expression Tagger"
            },
            "14": {
                "beforePatchRowNumber": 496,
                "afterPatchRowNumber": 496,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 497,
                "afterPatchRowNumber": 497,
                "PatchRowcode": "     The RegexpTagger assigns tags to tokens by comparing their"
            },
            "16": {
                "beforePatchRowNumber": 503,
                "afterPatchRowNumber": 503,
                "PatchRowcode": "         >>> from nltk.tag import RegexpTagger"
            },
            "17": {
                "beforePatchRowNumber": 504,
                "afterPatchRowNumber": 504,
                "PatchRowcode": "         >>> test_sent = brown.sents(categories='news')[0]"
            },
            "18": {
                "beforePatchRowNumber": 505,
                "afterPatchRowNumber": 505,
                "PatchRowcode": "         >>> regexp_tagger = RegexpTagger("
            },
            "19": {
                "beforePatchRowNumber": 506,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ...     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 506,
                "PatchRowcode": "+        ...     [(r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers"
            },
            "21": {
                "beforePatchRowNumber": 507,
                "afterPatchRowNumber": 507,
                "PatchRowcode": "         ...      (r'(The|the|A|a|An|an)$', 'AT'),   # articles"
            },
            "22": {
                "beforePatchRowNumber": 508,
                "afterPatchRowNumber": 508,
                "PatchRowcode": "         ...      (r'.*able$', 'JJ'),                # adjectives"
            },
            "23": {
                "beforePatchRowNumber": 509,
                "afterPatchRowNumber": 509,
                "PatchRowcode": "         ...      (r'.*ness$', 'NN'),                # nouns formed from adjectives"
            },
            "24": {
                "beforePatchRowNumber": 515,
                "afterPatchRowNumber": 515,
                "PatchRowcode": "         ... ])"
            },
            "25": {
                "beforePatchRowNumber": 516,
                "afterPatchRowNumber": 516,
                "PatchRowcode": "         >>> regexp_tagger"
            },
            "26": {
                "beforePatchRowNumber": 517,
                "afterPatchRowNumber": 517,
                "PatchRowcode": "         <Regexp Tagger: size=9>"
            },
            "27": {
                "beforePatchRowNumber": 518,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        >>> regexp_tagger.tag(test_sent)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 518,
                "PatchRowcode": "+        >>> regexp_tagger.tag(test_sent) # doctest: +NORMALIZE_WHITESPACE"
            },
            "29": {
                "beforePatchRowNumber": 519,
                "afterPatchRowNumber": 519,
                "PatchRowcode": "         [('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'),"
            },
            "30": {
                "beforePatchRowNumber": 520,
                "afterPatchRowNumber": 520,
                "PatchRowcode": "         ('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'),"
            },
            "31": {
                "beforePatchRowNumber": 521,
                "afterPatchRowNumber": 521,
                "PatchRowcode": "         (\"Atlanta's\", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'),"
            }
        },
        "frontPatchFile": [
            "# Natural Language Toolkit: Sequential Backoff Taggers",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# Author: Edward Loper <edloper@gmail.com>",
            "#         Steven Bird <stevenbird1@gmail.com> (minor additions)",
            "#         Tiago Tresoldi <tresoldi@users.sf.net> (original affix tagger)",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see LICENSE.TXT",
            "",
            "\"\"\"",
            "Classes for tagging sentences sequentially, left to right.  The",
            "abstract base class SequentialBackoffTagger serves as the base",
            "class for all the taggers in this module.  Tagging of individual words",
            "is performed by the method ``choose_tag()``, which is defined by",
            "subclasses of SequentialBackoffTagger.  If a tagger is unable to",
            "determine a tag for the specified token, then its backoff tagger is",
            "consulted instead.  Any SequentialBackoffTagger may serve as a",
            "backoff tagger for any other SequentialBackoffTagger.",
            "\"\"\"",
            "import ast",
            "import re",
            "from abc import abstractmethod",
            "from typing import List, Optional, Tuple",
            "",
            "from nltk import jsontags",
            "from nltk.classify import NaiveBayesClassifier",
            "from nltk.probability import ConditionalFreqDist",
            "from nltk.tag.api import FeaturesetTaggerI, TaggerI",
            "",
            "",
            "######################################################################",
            "# Abstract Base Classes",
            "######################################################################",
            "class SequentialBackoffTagger(TaggerI):",
            "    \"\"\"",
            "    An abstract base class for taggers that tags words sequentially,",
            "    left to right.  Tagging of individual words is performed by the",
            "    ``choose_tag()`` method, which should be defined by subclasses.  If",
            "    a tagger is unable to determine a tag for the specified token,",
            "    then its backoff tagger is consulted.",
            "",
            "    :ivar _taggers: A list of all the taggers that should be tried to",
            "        tag a token (i.e., self and its backoff taggers).",
            "    \"\"\"",
            "",
            "    def __init__(self, backoff=None):",
            "        if backoff is None:",
            "            self._taggers = [self]",
            "        else:",
            "            self._taggers = [self] + backoff._taggers",
            "",
            "    @property",
            "    def backoff(self):",
            "        \"\"\"The backoff tagger for this tagger.\"\"\"",
            "        return self._taggers[1] if len(self._taggers) > 1 else None",
            "",
            "    def tag(self, tokens):",
            "        # docs inherited from TaggerI",
            "        tags = []",
            "        for i in range(len(tokens)):",
            "            tags.append(self.tag_one(tokens, i, tags))",
            "        return list(zip(tokens, tags))",
            "",
            "    def tag_one(self, tokens, index, history):",
            "        \"\"\"",
            "        Determine an appropriate tag for the specified token, and",
            "        return that tag.  If this tagger is unable to determine a tag",
            "        for the specified token, then its backoff tagger is consulted.",
            "",
            "        :rtype: str",
            "        :type tokens: list",
            "        :param tokens: The list of words that are being tagged.",
            "        :type index: int",
            "        :param index: The index of the word whose tag should be",
            "            returned.",
            "        :type history: list(str)",
            "        :param history: A list of the tags for all words before *index*.",
            "        \"\"\"",
            "        tag = None",
            "        for tagger in self._taggers:",
            "            tag = tagger.choose_tag(tokens, index, history)",
            "            if tag is not None:",
            "                break",
            "        return tag",
            "",
            "    @abstractmethod",
            "    def choose_tag(self, tokens, index, history):",
            "        \"\"\"",
            "        Decide which tag should be used for the specified token, and",
            "        return that tag.  If this tagger is unable to determine a tag",
            "        for the specified token, return None -- do not consult",
            "        the backoff tagger.  This method should be overridden by",
            "        subclasses of SequentialBackoffTagger.",
            "",
            "        :rtype: str",
            "        :type tokens: list",
            "        :param tokens: The list of words that are being tagged.",
            "        :type index: int",
            "        :param index: The index of the word whose tag should be",
            "            returned.",
            "        :type history: list(str)",
            "        :param history: A list of the tags for all words before *index*.",
            "        \"\"\"",
            "",
            "",
            "class ContextTagger(SequentialBackoffTagger):",
            "    \"\"\"",
            "    An abstract base class for sequential backoff taggers that choose",
            "    a tag for a token based on the value of its \"context\".  Different",
            "    subclasses are used to define different contexts.",
            "",
            "    A ContextTagger chooses the tag for a token by calculating the",
            "    token's context, and looking up the corresponding tag in a table.",
            "    This table can be constructed manually; or it can be automatically",
            "    constructed based on a training corpus, using the ``_train()``",
            "    factory method.",
            "",
            "    :ivar _context_to_tag: Dictionary mapping contexts to tags.",
            "    \"\"\"",
            "",
            "    def __init__(self, context_to_tag, backoff=None):",
            "        \"\"\"",
            "        :param context_to_tag: A dictionary mapping contexts to tags.",
            "        :param backoff: The backoff tagger that should be used for this tagger.",
            "        \"\"\"",
            "        super().__init__(backoff)",
            "        self._context_to_tag = context_to_tag if context_to_tag else {}",
            "",
            "    @abstractmethod",
            "    def context(self, tokens, index, history):",
            "        \"\"\"",
            "        :return: the context that should be used to look up the tag",
            "            for the specified token; or None if the specified token",
            "            should not be handled by this tagger.",
            "        :rtype: (hashable)",
            "        \"\"\"",
            "",
            "    def choose_tag(self, tokens, index, history):",
            "        context = self.context(tokens, index, history)",
            "        return self._context_to_tag.get(context)",
            "",
            "    def size(self):",
            "        \"\"\"",
            "        :return: The number of entries in the table used by this",
            "            tagger to map from contexts to tags.",
            "        \"\"\"",
            "        return len(self._context_to_tag)",
            "",
            "    def __repr__(self):",
            "        return f\"<{self.__class__.__name__}: size={self.size()}>\"",
            "",
            "    def _train(self, tagged_corpus, cutoff=0, verbose=False):",
            "        \"\"\"",
            "        Initialize this ContextTagger's ``_context_to_tag`` table",
            "        based on the given training data.  In particular, for each",
            "        context ``c`` in the training data, set",
            "        ``_context_to_tag[c]`` to the most frequent tag for that",
            "        context.  However, exclude any contexts that are already",
            "        tagged perfectly by the backoff tagger(s).",
            "",
            "        The old value of ``self._context_to_tag`` (if any) is discarded.",
            "",
            "        :param tagged_corpus: A tagged corpus.  Each item should be",
            "            a list of (word, tag tuples.",
            "        :param cutoff: If the most likely tag for a context occurs",
            "            fewer than cutoff times, then exclude it from the",
            "            context-to-tag table for the new tagger.",
            "        \"\"\"",
            "",
            "        token_count = hit_count = 0",
            "",
            "        # A context is considered 'useful' if it's not already tagged",
            "        # perfectly by the backoff tagger.",
            "        useful_contexts = set()",
            "",
            "        # Count how many times each tag occurs in each context.",
            "        fd = ConditionalFreqDist()",
            "        for sentence in tagged_corpus:",
            "            tokens, tags = zip(*sentence)",
            "            for index, (token, tag) in enumerate(sentence):",
            "                # Record the event.",
            "                token_count += 1",
            "                context = self.context(tokens, index, tags[:index])",
            "                if context is None:",
            "                    continue",
            "                fd[context][tag] += 1",
            "                # If the backoff got it wrong, this context is useful:",
            "                if self.backoff is None or tag != self.backoff.tag_one(",
            "                    tokens, index, tags[:index]",
            "                ):",
            "                    useful_contexts.add(context)",
            "",
            "        # Build the context_to_tag table -- for each context, figure",
            "        # out what the most likely tag is.  Only include contexts that",
            "        # we've seen at least `cutoff` times.",
            "        for context in useful_contexts:",
            "            best_tag = fd[context].max()",
            "            hits = fd[context][best_tag]",
            "            if hits > cutoff:",
            "                self._context_to_tag[context] = best_tag",
            "                hit_count += hits",
            "",
            "        # Display some stats, if requested.",
            "        if verbose:",
            "            size = len(self._context_to_tag)",
            "            backoff = 100 - (hit_count * 100.0) / token_count",
            "            pruning = 100 - (size * 100.0) / len(fd.conditions())",
            "            print(\"[Trained Unigram tagger:\", end=\" \")",
            "            print(",
            "                \"size={}, backoff={:.2f}%, pruning={:.2f}%]\".format(",
            "                    size, backoff, pruning",
            "                )",
            "            )",
            "",
            "",
            "######################################################################",
            "# Tagger Classes",
            "######################################################################",
            "",
            "",
            "@jsontags.register_tag",
            "class DefaultTagger(SequentialBackoffTagger):",
            "    \"\"\"",
            "    A tagger that assigns the same tag to every token.",
            "",
            "        >>> from nltk.tag import DefaultTagger",
            "        >>> default_tagger = DefaultTagger('NN')",
            "        >>> list(default_tagger.tag('This is a test'.split()))",
            "        [('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]",
            "",
            "    This tagger is recommended as a backoff tagger, in cases where",
            "    a more powerful tagger is unable to assign a tag to the word",
            "    (e.g. because the word was not seen during training).",
            "",
            "    :param tag: The tag to assign to each token",
            "    :type tag: str",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.DefaultTagger\"",
            "",
            "    def __init__(self, tag):",
            "        self._tag = tag",
            "        super().__init__(None)",
            "",
            "    def encode_json_obj(self):",
            "        return self._tag",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        tag = obj",
            "        return cls(tag)",
            "",
            "    def choose_tag(self, tokens, index, history):",
            "        return self._tag  # ignore token and history",
            "",
            "    def __repr__(self):",
            "        return f\"<DefaultTagger: tag={self._tag}>\"",
            "",
            "",
            "@jsontags.register_tag",
            "class NgramTagger(ContextTagger):",
            "    \"\"\"",
            "    A tagger that chooses a token's tag based on its word string and",
            "    on the preceding n word's tags.  In particular, a tuple",
            "    (tags[i-n:i-1], words[i]) is looked up in a table, and the",
            "    corresponding tag is returned.  N-gram taggers are typically",
            "    trained on a tagged corpus.",
            "",
            "    Train a new NgramTagger using the given training data or",
            "    the supplied model.  In particular, construct a new tagger",
            "    whose table maps from each context (tag[i-n:i-1], word[i])",
            "    to the most frequent tag for that context.  But exclude any",
            "    contexts that are already tagged perfectly by the backoff",
            "    tagger.",
            "",
            "    :param train: A tagged corpus consisting of a list of tagged",
            "        sentences, where each sentence is a list of (word, tag) tuples.",
            "    :param backoff: A backoff tagger, to be used by the new",
            "        tagger if it encounters an unknown context.",
            "    :param cutoff: If the most likely tag for a context occurs",
            "        fewer than *cutoff* times, then exclude it from the",
            "        context-to-tag table for the new tagger.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.NgramTagger\"",
            "",
            "    def __init__(",
            "        self, n, train=None, model=None, backoff=None, cutoff=0, verbose=False",
            "    ):",
            "        self._n = n",
            "        self._check_params(train, model)",
            "",
            "        super().__init__(model, backoff)",
            "",
            "        if train:",
            "            self._train(train, cutoff, verbose)",
            "",
            "    def encode_json_obj(self):",
            "        _context_to_tag = {repr(k): v for k, v in self._context_to_tag.items()}",
            "        if \"NgramTagger\" in self.__class__.__name__:",
            "            return self._n, _context_to_tag, self.backoff",
            "        else:",
            "            return _context_to_tag, self.backoff",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        try:",
            "            _n, _context_to_tag, backoff = obj",
            "        except ValueError:",
            "            _context_to_tag, backoff = obj",
            "",
            "        if not _context_to_tag:",
            "            return backoff",
            "",
            "        _context_to_tag = {ast.literal_eval(k): v for k, v in _context_to_tag.items()}",
            "",
            "        if \"NgramTagger\" in cls.__name__:",
            "            return cls(_n, model=_context_to_tag, backoff=backoff)",
            "        else:",
            "            return cls(model=_context_to_tag, backoff=backoff)",
            "",
            "    def context(self, tokens, index, history):",
            "        tag_context = tuple(history[max(0, index - self._n + 1) : index])",
            "        return tag_context, tokens[index]",
            "",
            "",
            "@jsontags.register_tag",
            "class UnigramTagger(NgramTagger):",
            "    \"\"\"",
            "    Unigram Tagger",
            "",
            "    The UnigramTagger finds the most likely tag for each word in a training",
            "    corpus, and then uses that information to assign tags to new tokens.",
            "",
            "        >>> from nltk.corpus import brown",
            "        >>> from nltk.tag import UnigramTagger",
            "        >>> test_sent = brown.sents(categories='news')[0]",
            "        >>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])",
            "        >>> for tok, tag in unigram_tagger.tag(test_sent):",
            "        ...     print(\"({}, {}), \".format(tok, tag))",
            "        (The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),",
            "        (Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),",
            "        (investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),",
            "        (primary, NN), (election, NN), (produced, VBD), (``, ``),",
            "        (no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),",
            "        (irregularities, NNS), (took, VBD), (place, NN), (., .),",
            "",
            "    :param train: The corpus of training data, a list of tagged sentences",
            "    :type train: list(list(tuple(str, str)))",
            "    :param model: The tagger model",
            "    :type model: dict",
            "    :param backoff: Another tagger which this tagger will consult when it is",
            "        unable to tag a word",
            "    :type backoff: TaggerI",
            "    :param cutoff: The number of instances of training data the tagger must see",
            "        in order not to use the backoff tagger",
            "    :type cutoff: int",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.UnigramTagger\"",
            "",
            "    def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):",
            "        super().__init__(1, train, model, backoff, cutoff, verbose)",
            "",
            "    def context(self, tokens, index, history):",
            "        return tokens[index]",
            "",
            "",
            "@jsontags.register_tag",
            "class BigramTagger(NgramTagger):",
            "    \"\"\"",
            "    A tagger that chooses a token's tag based its word string and on",
            "    the preceding words' tag.  In particular, a tuple consisting",
            "    of the previous tag and the word is looked up in a table, and",
            "    the corresponding tag is returned.",
            "",
            "    :param train: The corpus of training data, a list of tagged sentences",
            "    :type train: list(list(tuple(str, str)))",
            "    :param model: The tagger model",
            "    :type model: dict",
            "    :param backoff: Another tagger which this tagger will consult when it is",
            "        unable to tag a word",
            "    :type backoff: TaggerI",
            "    :param cutoff: The number of instances of training data the tagger must see",
            "        in order not to use the backoff tagger",
            "    :type cutoff: int",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.BigramTagger\"",
            "",
            "    def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):",
            "        super().__init__(2, train, model, backoff, cutoff, verbose)",
            "",
            "",
            "@jsontags.register_tag",
            "class TrigramTagger(NgramTagger):",
            "    \"\"\"",
            "    A tagger that chooses a token's tag based its word string and on",
            "    the preceding two words' tags.  In particular, a tuple consisting",
            "    of the previous two tags and the word is looked up in a table, and",
            "    the corresponding tag is returned.",
            "",
            "    :param train: The corpus of training data, a list of tagged sentences",
            "    :type train: list(list(tuple(str, str)))",
            "    :param model: The tagger model",
            "    :type model: dict",
            "    :param backoff: Another tagger which this tagger will consult when it is",
            "        unable to tag a word",
            "    :type backoff: TaggerI",
            "    :param cutoff: The number of instances of training data the tagger must see",
            "        in order not to use the backoff tagger",
            "    :type cutoff: int",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.TrigramTagger\"",
            "",
            "    def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):",
            "        super().__init__(3, train, model, backoff, cutoff, verbose)",
            "",
            "",
            "@jsontags.register_tag",
            "class AffixTagger(ContextTagger):",
            "    \"\"\"",
            "    A tagger that chooses a token's tag based on a leading or trailing",
            "    substring of its word string.  (It is important to note that these",
            "    substrings are not necessarily \"true\" morphological affixes).  In",
            "    particular, a fixed-length substring of the word is looked up in a",
            "    table, and the corresponding tag is returned.  Affix taggers are",
            "    typically constructed by training them on a tagged corpus.",
            "",
            "    Construct a new affix tagger.",
            "",
            "    :param affix_length: The length of the affixes that should be",
            "        considered during training and tagging.  Use negative",
            "        numbers for suffixes.",
            "    :param min_stem_length: Any words whose length is less than",
            "        min_stem_length+abs(affix_length) will be assigned a",
            "        tag of None by this tagger.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.AffixTagger\"",
            "",
            "    def __init__(",
            "        self,",
            "        train=None,",
            "        model=None,",
            "        affix_length=-3,",
            "        min_stem_length=2,",
            "        backoff=None,",
            "        cutoff=0,",
            "        verbose=False,",
            "    ):",
            "",
            "        self._check_params(train, model)",
            "",
            "        super().__init__(model, backoff)",
            "",
            "        self._affix_length = affix_length",
            "        self._min_word_length = min_stem_length + abs(affix_length)",
            "",
            "        if train:",
            "            self._train(train, cutoff, verbose)",
            "",
            "    def encode_json_obj(self):",
            "        return (",
            "            self._affix_length,",
            "            self._min_word_length,",
            "            self._context_to_tag,",
            "            self.backoff,",
            "        )",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        _affix_length, _min_word_length, _context_to_tag, backoff = obj",
            "        return cls(",
            "            affix_length=_affix_length,",
            "            min_stem_length=_min_word_length - abs(_affix_length),",
            "            model=_context_to_tag,",
            "            backoff=backoff,",
            "        )",
            "",
            "    def context(self, tokens, index, history):",
            "        token = tokens[index]",
            "        if len(token) < self._min_word_length:",
            "            return None",
            "        elif self._affix_length > 0:",
            "            return token[: self._affix_length]",
            "        else:",
            "            return token[self._affix_length :]",
            "",
            "",
            "@jsontags.register_tag",
            "class RegexpTagger(SequentialBackoffTagger):",
            "    \"\"\"",
            "    Regular Expression Tagger",
            "",
            "    The RegexpTagger assigns tags to tokens by comparing their",
            "    word strings to a series of regular expressions.  The following tagger",
            "    uses word suffixes to make guesses about the correct Brown Corpus part",
            "    of speech tag:",
            "",
            "        >>> from nltk.corpus import brown",
            "        >>> from nltk.tag import RegexpTagger",
            "        >>> test_sent = brown.sents(categories='news')[0]",
            "        >>> regexp_tagger = RegexpTagger(",
            "        ...     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers",
            "        ...      (r'(The|the|A|a|An|an)$', 'AT'),   # articles",
            "        ...      (r'.*able$', 'JJ'),                # adjectives",
            "        ...      (r'.*ness$', 'NN'),                # nouns formed from adjectives",
            "        ...      (r'.*ly$', 'RB'),                  # adverbs",
            "        ...      (r'.*s$', 'NNS'),                  # plural nouns",
            "        ...      (r'.*ing$', 'VBG'),                # gerunds",
            "        ...      (r'.*ed$', 'VBD'),                 # past tense verbs",
            "        ...      (r'.*', 'NN')                      # nouns (default)",
            "        ... ])",
            "        >>> regexp_tagger",
            "        <Regexp Tagger: size=9>",
            "        >>> regexp_tagger.tag(test_sent)",
            "        [('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'),",
            "        ('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'),",
            "        (\"Atlanta's\", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'),",
            "        ('produced', 'VBD'), ('``', 'NN'), ('no', 'NN'), ('evidence', 'NN'), (\"''\", 'NN'),",
            "        ('that', 'NN'), ('any', 'NN'), ('irregularities', 'NNS'), ('took', 'NN'),",
            "        ('place', 'NN'), ('.', 'NN')]",
            "",
            "    :type regexps: list(tuple(str, str))",
            "    :param regexps: A list of ``(regexp, tag)`` pairs, each of",
            "        which indicates that a word matching ``regexp`` should",
            "        be tagged with ``tag``.  The pairs will be evaluated in",
            "        order.  If none of the regexps match a word, then the",
            "        optional backoff tagger is invoked, else it is",
            "        assigned the tag None.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.RegexpTagger\"",
            "",
            "    def __init__(",
            "        self, regexps: List[Tuple[str, str]], backoff: Optional[TaggerI] = None",
            "    ):",
            "        super().__init__(backoff)",
            "        self._regexps = []",
            "        for regexp, tag in regexps:",
            "            try:",
            "                self._regexps.append((re.compile(regexp), tag))",
            "            except Exception as e:",
            "                raise Exception(",
            "                    f\"Invalid RegexpTagger regexp: {e}\\n- regexp: {regexp!r}\\n- tag: {tag!r}\"",
            "                ) from e",
            "",
            "    def encode_json_obj(self):",
            "        return [(regexp.pattern, tag) for regexp, tag in self._regexps], self.backoff",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        regexps, backoff = obj",
            "        return cls(regexps, backoff)",
            "",
            "    def choose_tag(self, tokens, index, history):",
            "        for regexp, tag in self._regexps:",
            "            if re.match(regexp, tokens[index]):",
            "                return tag",
            "        return None",
            "",
            "    def __repr__(self):",
            "        return f\"<Regexp Tagger: size={len(self._regexps)}>\"",
            "",
            "",
            "class ClassifierBasedTagger(SequentialBackoffTagger, FeaturesetTaggerI):",
            "    \"\"\"",
            "    A sequential tagger that uses a classifier to choose the tag for",
            "    each token in a sentence.  The featureset input for the classifier",
            "    is generated by a feature detector function::",
            "",
            "        feature_detector(tokens, index, history) -> featureset",
            "",
            "    Where tokens is the list of unlabeled tokens in the sentence;",
            "    index is the index of the token for which feature detection",
            "    should be performed; and history is list of the tags for all",
            "    tokens before index.",
            "",
            "    Construct a new classifier-based sequential tagger.",
            "",
            "    :param feature_detector: A function used to generate the",
            "        featureset input for the classifier::",
            "        feature_detector(tokens, index, history) -> featureset",
            "",
            "    :param train: A tagged corpus consisting of a list of tagged",
            "        sentences, where each sentence is a list of (word, tag) tuples.",
            "",
            "    :param backoff: A backoff tagger, to be used by the new tagger",
            "        if it encounters an unknown context.",
            "",
            "    :param classifier_builder: A function used to train a new",
            "        classifier based on the data in *train*.  It should take",
            "        one argument, a list of labeled featuresets (i.e.,",
            "        (featureset, label) tuples).",
            "",
            "    :param classifier: The classifier that should be used by the",
            "        tagger.  This is only useful if you want to manually",
            "        construct the classifier; normally, you would use *train*",
            "        instead.",
            "",
            "    :param backoff: A backoff tagger, used if this tagger is",
            "        unable to determine a tag for a given token.",
            "",
            "    :param cutoff_prob: If specified, then this tagger will fall",
            "        back on its backoff tagger if the probability of the most",
            "        likely tag is less than *cutoff_prob*.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        feature_detector=None,",
            "        train=None,",
            "        classifier_builder=NaiveBayesClassifier.train,",
            "        classifier=None,",
            "        backoff=None,",
            "        cutoff_prob=None,",
            "        verbose=False,",
            "    ):",
            "        self._check_params(train, classifier)",
            "",
            "        super().__init__(backoff)",
            "",
            "        if (train and classifier) or (not train and not classifier):",
            "            raise ValueError(",
            "                \"Must specify either training data or \" \"trained classifier.\"",
            "            )",
            "",
            "        if feature_detector is not None:",
            "            self._feature_detector = feature_detector",
            "            # The feature detector function, used to generate a featureset",
            "            # or each token: feature_detector(tokens, index, history) -> featureset",
            "",
            "        self._cutoff_prob = cutoff_prob",
            "        \"\"\"Cutoff probability for tagging -- if the probability of the",
            "           most likely tag is less than this, then use backoff.\"\"\"",
            "",
            "        self._classifier = classifier",
            "        \"\"\"The classifier used to choose a tag for each token.\"\"\"",
            "",
            "        if train:",
            "            self._train(train, classifier_builder, verbose)",
            "",
            "    def choose_tag(self, tokens, index, history):",
            "        # Use our feature detector to get the featureset.",
            "        featureset = self.feature_detector(tokens, index, history)",
            "",
            "        # Use the classifier to pick a tag.  If a cutoff probability",
            "        # was specified, then check that the tag's probability is",
            "        # higher than that cutoff first; otherwise, return None.",
            "        if self._cutoff_prob is None:",
            "            return self._classifier.classify(featureset)",
            "",
            "        pdist = self._classifier.prob_classify(featureset)",
            "        tag = pdist.max()",
            "        return tag if pdist.prob(tag) >= self._cutoff_prob else None",
            "",
            "    def _train(self, tagged_corpus, classifier_builder, verbose):",
            "        \"\"\"",
            "        Build a new classifier, based on the given training data",
            "        *tagged_corpus*.",
            "        \"\"\"",
            "",
            "        classifier_corpus = []",
            "        if verbose:",
            "            print(\"Constructing training corpus for classifier.\")",
            "",
            "        for sentence in tagged_corpus:",
            "            history = []",
            "            untagged_sentence, tags = zip(*sentence)",
            "            for index in range(len(sentence)):",
            "                featureset = self.feature_detector(untagged_sentence, index, history)",
            "                classifier_corpus.append((featureset, tags[index]))",
            "                history.append(tags[index])",
            "",
            "        if verbose:",
            "            print(f\"Training classifier ({len(classifier_corpus)} instances)\")",
            "        self._classifier = classifier_builder(classifier_corpus)",
            "",
            "    def __repr__(self):",
            "        return f\"<ClassifierBasedTagger: {self._classifier}>\"",
            "",
            "    def feature_detector(self, tokens, index, history):",
            "        \"\"\"",
            "        Return the feature detector that this tagger uses to generate",
            "        featuresets for its classifier.  The feature detector is a",
            "        function with the signature::",
            "",
            "          feature_detector(tokens, index, history) -> featureset",
            "",
            "        See ``classifier()``",
            "        \"\"\"",
            "        return self._feature_detector(tokens, index, history)",
            "",
            "    def classifier(self):",
            "        \"\"\"",
            "        Return the classifier that this tagger uses to choose a tag",
            "        for each word in a sentence.  The input for this classifier is",
            "        generated using this tagger's feature detector.",
            "        See ``feature_detector()``",
            "        \"\"\"",
            "        return self._classifier",
            "",
            "",
            "class ClassifierBasedPOSTagger(ClassifierBasedTagger):",
            "    \"\"\"",
            "    A classifier based part of speech tagger.",
            "    \"\"\"",
            "",
            "    def feature_detector(self, tokens, index, history):",
            "        word = tokens[index]",
            "        if index == 0:",
            "            prevword = prevprevword = None",
            "            prevtag = prevprevtag = None",
            "        elif index == 1:",
            "            prevword = tokens[index - 1].lower()",
            "            prevprevword = None",
            "            prevtag = history[index - 1]",
            "            prevprevtag = None",
            "        else:",
            "            prevword = tokens[index - 1].lower()",
            "            prevprevword = tokens[index - 2].lower()",
            "            prevtag = history[index - 1]",
            "            prevprevtag = history[index - 2]",
            "",
            "        if re.match(r\"[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$\", word):",
            "            shape = \"number\"",
            "        elif re.match(r\"\\W+$\", word):",
            "            shape = \"punct\"",
            "        elif re.match(\"[A-Z][a-z]+$\", word):",
            "            shape = \"upcase\"",
            "        elif re.match(\"[a-z]+$\", word):",
            "            shape = \"downcase\"",
            "        elif re.match(r\"\\w+$\", word):",
            "            shape = \"mixedcase\"",
            "        else:",
            "            shape = \"other\"",
            "",
            "        features = {",
            "            \"prevtag\": prevtag,",
            "            \"prevprevtag\": prevprevtag,",
            "            \"word\": word,",
            "            \"word.lower\": word.lower(),",
            "            \"suffix3\": word.lower()[-3:],",
            "            \"suffix2\": word.lower()[-2:],",
            "            \"suffix1\": word.lower()[-1:],",
            "            \"prevprevword\": prevprevword,",
            "            \"prevword\": prevword,",
            "            \"prevtag+word\": f\"{prevtag}+{word.lower()}\",",
            "            \"prevprevtag+word\": f\"{prevprevtag}+{word.lower()}\",",
            "            \"prevword+word\": f\"{prevword}+{word.lower()}\",",
            "            \"shape\": shape,",
            "        }",
            "        return features"
        ],
        "afterPatchFile": [
            "# Natural Language Toolkit: Sequential Backoff Taggers",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# Author: Edward Loper <edloper@gmail.com>",
            "#         Steven Bird <stevenbird1@gmail.com> (minor additions)",
            "#         Tiago Tresoldi <tresoldi@users.sf.net> (original affix tagger)",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see LICENSE.TXT",
            "",
            "\"\"\"",
            "Classes for tagging sentences sequentially, left to right.  The",
            "abstract base class SequentialBackoffTagger serves as the base",
            "class for all the taggers in this module.  Tagging of individual words",
            "is performed by the method ``choose_tag()``, which is defined by",
            "subclasses of SequentialBackoffTagger.  If a tagger is unable to",
            "determine a tag for the specified token, then its backoff tagger is",
            "consulted instead.  Any SequentialBackoffTagger may serve as a",
            "backoff tagger for any other SequentialBackoffTagger.",
            "\"\"\"",
            "import ast",
            "import re",
            "from abc import abstractmethod",
            "from typing import List, Optional, Tuple",
            "",
            "from nltk import jsontags",
            "from nltk.classify import NaiveBayesClassifier",
            "from nltk.probability import ConditionalFreqDist",
            "from nltk.tag.api import FeaturesetTaggerI, TaggerI",
            "",
            "",
            "######################################################################",
            "# Abstract Base Classes",
            "######################################################################",
            "class SequentialBackoffTagger(TaggerI):",
            "    \"\"\"",
            "    An abstract base class for taggers that tags words sequentially,",
            "    left to right.  Tagging of individual words is performed by the",
            "    ``choose_tag()`` method, which should be defined by subclasses.  If",
            "    a tagger is unable to determine a tag for the specified token,",
            "    then its backoff tagger is consulted.",
            "",
            "    :ivar _taggers: A list of all the taggers that should be tried to",
            "        tag a token (i.e., self and its backoff taggers).",
            "    \"\"\"",
            "",
            "    def __init__(self, backoff=None):",
            "        if backoff is None:",
            "            self._taggers = [self]",
            "        else:",
            "            self._taggers = [self] + backoff._taggers",
            "",
            "    @property",
            "    def backoff(self):",
            "        \"\"\"The backoff tagger for this tagger.\"\"\"",
            "        return self._taggers[1] if len(self._taggers) > 1 else None",
            "",
            "    def tag(self, tokens):",
            "        # docs inherited from TaggerI",
            "        tags = []",
            "        for i in range(len(tokens)):",
            "            tags.append(self.tag_one(tokens, i, tags))",
            "        return list(zip(tokens, tags))",
            "",
            "    def tag_one(self, tokens, index, history):",
            "        \"\"\"",
            "        Determine an appropriate tag for the specified token, and",
            "        return that tag.  If this tagger is unable to determine a tag",
            "        for the specified token, then its backoff tagger is consulted.",
            "",
            "        :rtype: str",
            "        :type tokens: list",
            "        :param tokens: The list of words that are being tagged.",
            "        :type index: int",
            "        :param index: The index of the word whose tag should be",
            "            returned.",
            "        :type history: list(str)",
            "        :param history: A list of the tags for all words before *index*.",
            "        \"\"\"",
            "        tag = None",
            "        for tagger in self._taggers:",
            "            tag = tagger.choose_tag(tokens, index, history)",
            "            if tag is not None:",
            "                break",
            "        return tag",
            "",
            "    @abstractmethod",
            "    def choose_tag(self, tokens, index, history):",
            "        \"\"\"",
            "        Decide which tag should be used for the specified token, and",
            "        return that tag.  If this tagger is unable to determine a tag",
            "        for the specified token, return None -- do not consult",
            "        the backoff tagger.  This method should be overridden by",
            "        subclasses of SequentialBackoffTagger.",
            "",
            "        :rtype: str",
            "        :type tokens: list",
            "        :param tokens: The list of words that are being tagged.",
            "        :type index: int",
            "        :param index: The index of the word whose tag should be",
            "            returned.",
            "        :type history: list(str)",
            "        :param history: A list of the tags for all words before *index*.",
            "        \"\"\"",
            "",
            "",
            "class ContextTagger(SequentialBackoffTagger):",
            "    \"\"\"",
            "    An abstract base class for sequential backoff taggers that choose",
            "    a tag for a token based on the value of its \"context\".  Different",
            "    subclasses are used to define different contexts.",
            "",
            "    A ContextTagger chooses the tag for a token by calculating the",
            "    token's context, and looking up the corresponding tag in a table.",
            "    This table can be constructed manually; or it can be automatically",
            "    constructed based on a training corpus, using the ``_train()``",
            "    factory method.",
            "",
            "    :ivar _context_to_tag: Dictionary mapping contexts to tags.",
            "    \"\"\"",
            "",
            "    def __init__(self, context_to_tag, backoff=None):",
            "        \"\"\"",
            "        :param context_to_tag: A dictionary mapping contexts to tags.",
            "        :param backoff: The backoff tagger that should be used for this tagger.",
            "        \"\"\"",
            "        super().__init__(backoff)",
            "        self._context_to_tag = context_to_tag if context_to_tag else {}",
            "",
            "    @abstractmethod",
            "    def context(self, tokens, index, history):",
            "        \"\"\"",
            "        :return: the context that should be used to look up the tag",
            "            for the specified token; or None if the specified token",
            "            should not be handled by this tagger.",
            "        :rtype: (hashable)",
            "        \"\"\"",
            "",
            "    def choose_tag(self, tokens, index, history):",
            "        context = self.context(tokens, index, history)",
            "        return self._context_to_tag.get(context)",
            "",
            "    def size(self):",
            "        \"\"\"",
            "        :return: The number of entries in the table used by this",
            "            tagger to map from contexts to tags.",
            "        \"\"\"",
            "        return len(self._context_to_tag)",
            "",
            "    def __repr__(self):",
            "        return f\"<{self.__class__.__name__}: size={self.size()}>\"",
            "",
            "    def _train(self, tagged_corpus, cutoff=0, verbose=False):",
            "        \"\"\"",
            "        Initialize this ContextTagger's ``_context_to_tag`` table",
            "        based on the given training data.  In particular, for each",
            "        context ``c`` in the training data, set",
            "        ``_context_to_tag[c]`` to the most frequent tag for that",
            "        context.  However, exclude any contexts that are already",
            "        tagged perfectly by the backoff tagger(s).",
            "",
            "        The old value of ``self._context_to_tag`` (if any) is discarded.",
            "",
            "        :param tagged_corpus: A tagged corpus.  Each item should be",
            "            a list of (word, tag tuples.",
            "        :param cutoff: If the most likely tag for a context occurs",
            "            fewer than cutoff times, then exclude it from the",
            "            context-to-tag table for the new tagger.",
            "        \"\"\"",
            "",
            "        token_count = hit_count = 0",
            "",
            "        # A context is considered 'useful' if it's not already tagged",
            "        # perfectly by the backoff tagger.",
            "        useful_contexts = set()",
            "",
            "        # Count how many times each tag occurs in each context.",
            "        fd = ConditionalFreqDist()",
            "        for sentence in tagged_corpus:",
            "            tokens, tags = zip(*sentence)",
            "            for index, (token, tag) in enumerate(sentence):",
            "                # Record the event.",
            "                token_count += 1",
            "                context = self.context(tokens, index, tags[:index])",
            "                if context is None:",
            "                    continue",
            "                fd[context][tag] += 1",
            "                # If the backoff got it wrong, this context is useful:",
            "                if self.backoff is None or tag != self.backoff.tag_one(",
            "                    tokens, index, tags[:index]",
            "                ):",
            "                    useful_contexts.add(context)",
            "",
            "        # Build the context_to_tag table -- for each context, figure",
            "        # out what the most likely tag is.  Only include contexts that",
            "        # we've seen at least `cutoff` times.",
            "        for context in useful_contexts:",
            "            best_tag = fd[context].max()",
            "            hits = fd[context][best_tag]",
            "            if hits > cutoff:",
            "                self._context_to_tag[context] = best_tag",
            "                hit_count += hits",
            "",
            "        # Display some stats, if requested.",
            "        if verbose:",
            "            size = len(self._context_to_tag)",
            "            backoff = 100 - (hit_count * 100.0) / token_count",
            "            pruning = 100 - (size * 100.0) / len(fd.conditions())",
            "            print(\"[Trained Unigram tagger:\", end=\" \")",
            "            print(",
            "                \"size={}, backoff={:.2f}%, pruning={:.2f}%]\".format(",
            "                    size, backoff, pruning",
            "                )",
            "            )",
            "",
            "",
            "######################################################################",
            "# Tagger Classes",
            "######################################################################",
            "",
            "",
            "@jsontags.register_tag",
            "class DefaultTagger(SequentialBackoffTagger):",
            "    \"\"\"",
            "    A tagger that assigns the same tag to every token.",
            "",
            "        >>> from nltk.tag import DefaultTagger",
            "        >>> default_tagger = DefaultTagger('NN')",
            "        >>> list(default_tagger.tag('This is a test'.split()))",
            "        [('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]",
            "",
            "    This tagger is recommended as a backoff tagger, in cases where",
            "    a more powerful tagger is unable to assign a tag to the word",
            "    (e.g. because the word was not seen during training).",
            "",
            "    :param tag: The tag to assign to each token",
            "    :type tag: str",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.DefaultTagger\"",
            "",
            "    def __init__(self, tag):",
            "        self._tag = tag",
            "        super().__init__(None)",
            "",
            "    def encode_json_obj(self):",
            "        return self._tag",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        tag = obj",
            "        return cls(tag)",
            "",
            "    def choose_tag(self, tokens, index, history):",
            "        return self._tag  # ignore token and history",
            "",
            "    def __repr__(self):",
            "        return f\"<DefaultTagger: tag={self._tag}>\"",
            "",
            "",
            "@jsontags.register_tag",
            "class NgramTagger(ContextTagger):",
            "    \"\"\"",
            "    A tagger that chooses a token's tag based on its word string and",
            "    on the preceding n word's tags.  In particular, a tuple",
            "    (tags[i-n:i-1], words[i]) is looked up in a table, and the",
            "    corresponding tag is returned.  N-gram taggers are typically",
            "    trained on a tagged corpus.",
            "",
            "    Train a new NgramTagger using the given training data or",
            "    the supplied model.  In particular, construct a new tagger",
            "    whose table maps from each context (tag[i-n:i-1], word[i])",
            "    to the most frequent tag for that context.  But exclude any",
            "    contexts that are already tagged perfectly by the backoff",
            "    tagger.",
            "",
            "    :param train: A tagged corpus consisting of a list of tagged",
            "        sentences, where each sentence is a list of (word, tag) tuples.",
            "    :param backoff: A backoff tagger, to be used by the new",
            "        tagger if it encounters an unknown context.",
            "    :param cutoff: If the most likely tag for a context occurs",
            "        fewer than *cutoff* times, then exclude it from the",
            "        context-to-tag table for the new tagger.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.NgramTagger\"",
            "",
            "    def __init__(",
            "        self, n, train=None, model=None, backoff=None, cutoff=0, verbose=False",
            "    ):",
            "        self._n = n",
            "        self._check_params(train, model)",
            "",
            "        super().__init__(model, backoff)",
            "",
            "        if train:",
            "            self._train(train, cutoff, verbose)",
            "",
            "    def encode_json_obj(self):",
            "        _context_to_tag = {repr(k): v for k, v in self._context_to_tag.items()}",
            "        if \"NgramTagger\" in self.__class__.__name__:",
            "            return self._n, _context_to_tag, self.backoff",
            "        else:",
            "            return _context_to_tag, self.backoff",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        try:",
            "            _n, _context_to_tag, backoff = obj",
            "        except ValueError:",
            "            _context_to_tag, backoff = obj",
            "",
            "        if not _context_to_tag:",
            "            return backoff",
            "",
            "        _context_to_tag = {ast.literal_eval(k): v for k, v in _context_to_tag.items()}",
            "",
            "        if \"NgramTagger\" in cls.__name__:",
            "            return cls(_n, model=_context_to_tag, backoff=backoff)",
            "        else:",
            "            return cls(model=_context_to_tag, backoff=backoff)",
            "",
            "    def context(self, tokens, index, history):",
            "        tag_context = tuple(history[max(0, index - self._n + 1) : index])",
            "        return tag_context, tokens[index]",
            "",
            "",
            "@jsontags.register_tag",
            "class UnigramTagger(NgramTagger):",
            "    \"\"\"",
            "    Unigram Tagger",
            "",
            "    The UnigramTagger finds the most likely tag for each word in a training",
            "    corpus, and then uses that information to assign tags to new tokens.",
            "",
            "        >>> from nltk.corpus import brown",
            "        >>> from nltk.tag import UnigramTagger",
            "        >>> test_sent = brown.sents(categories='news')[0]",
            "        >>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])",
            "        >>> for tok, tag in unigram_tagger.tag(test_sent):",
            "        ...     print(\"({}, {}), \".format(tok, tag)) # doctest: +NORMALIZE_WHITESPACE",
            "        (The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),",
            "        (Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),",
            "        (investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),",
            "        (primary, NN), (election, NN), (produced, VBD), (``, ``),",
            "        (no, AT), (evidence, NN), ('', ''), (that, CS), (any, DTI),",
            "        (irregularities, NNS), (took, VBD), (place, NN), (., .),",
            "",
            "    :param train: The corpus of training data, a list of tagged sentences",
            "    :type train: list(list(tuple(str, str)))",
            "    :param model: The tagger model",
            "    :type model: dict",
            "    :param backoff: Another tagger which this tagger will consult when it is",
            "        unable to tag a word",
            "    :type backoff: TaggerI",
            "    :param cutoff: The number of instances of training data the tagger must see",
            "        in order not to use the backoff tagger",
            "    :type cutoff: int",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.UnigramTagger\"",
            "",
            "    def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):",
            "        super().__init__(1, train, model, backoff, cutoff, verbose)",
            "",
            "    def context(self, tokens, index, history):",
            "        return tokens[index]",
            "",
            "",
            "@jsontags.register_tag",
            "class BigramTagger(NgramTagger):",
            "    \"\"\"",
            "    A tagger that chooses a token's tag based its word string and on",
            "    the preceding words' tag.  In particular, a tuple consisting",
            "    of the previous tag and the word is looked up in a table, and",
            "    the corresponding tag is returned.",
            "",
            "    :param train: The corpus of training data, a list of tagged sentences",
            "    :type train: list(list(tuple(str, str)))",
            "    :param model: The tagger model",
            "    :type model: dict",
            "    :param backoff: Another tagger which this tagger will consult when it is",
            "        unable to tag a word",
            "    :type backoff: TaggerI",
            "    :param cutoff: The number of instances of training data the tagger must see",
            "        in order not to use the backoff tagger",
            "    :type cutoff: int",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.BigramTagger\"",
            "",
            "    def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):",
            "        super().__init__(2, train, model, backoff, cutoff, verbose)",
            "",
            "",
            "@jsontags.register_tag",
            "class TrigramTagger(NgramTagger):",
            "    \"\"\"",
            "    A tagger that chooses a token's tag based its word string and on",
            "    the preceding two words' tags.  In particular, a tuple consisting",
            "    of the previous two tags and the word is looked up in a table, and",
            "    the corresponding tag is returned.",
            "",
            "    :param train: The corpus of training data, a list of tagged sentences",
            "    :type train: list(list(tuple(str, str)))",
            "    :param model: The tagger model",
            "    :type model: dict",
            "    :param backoff: Another tagger which this tagger will consult when it is",
            "        unable to tag a word",
            "    :type backoff: TaggerI",
            "    :param cutoff: The number of instances of training data the tagger must see",
            "        in order not to use the backoff tagger",
            "    :type cutoff: int",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.TrigramTagger\"",
            "",
            "    def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):",
            "        super().__init__(3, train, model, backoff, cutoff, verbose)",
            "",
            "",
            "@jsontags.register_tag",
            "class AffixTagger(ContextTagger):",
            "    \"\"\"",
            "    A tagger that chooses a token's tag based on a leading or trailing",
            "    substring of its word string.  (It is important to note that these",
            "    substrings are not necessarily \"true\" morphological affixes).  In",
            "    particular, a fixed-length substring of the word is looked up in a",
            "    table, and the corresponding tag is returned.  Affix taggers are",
            "    typically constructed by training them on a tagged corpus.",
            "",
            "    Construct a new affix tagger.",
            "",
            "    :param affix_length: The length of the affixes that should be",
            "        considered during training and tagging.  Use negative",
            "        numbers for suffixes.",
            "    :param min_stem_length: Any words whose length is less than",
            "        min_stem_length+abs(affix_length) will be assigned a",
            "        tag of None by this tagger.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.AffixTagger\"",
            "",
            "    def __init__(",
            "        self,",
            "        train=None,",
            "        model=None,",
            "        affix_length=-3,",
            "        min_stem_length=2,",
            "        backoff=None,",
            "        cutoff=0,",
            "        verbose=False,",
            "    ):",
            "",
            "        self._check_params(train, model)",
            "",
            "        super().__init__(model, backoff)",
            "",
            "        self._affix_length = affix_length",
            "        self._min_word_length = min_stem_length + abs(affix_length)",
            "",
            "        if train:",
            "            self._train(train, cutoff, verbose)",
            "",
            "    def encode_json_obj(self):",
            "        return (",
            "            self._affix_length,",
            "            self._min_word_length,",
            "            self._context_to_tag,",
            "            self.backoff,",
            "        )",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        _affix_length, _min_word_length, _context_to_tag, backoff = obj",
            "        return cls(",
            "            affix_length=_affix_length,",
            "            min_stem_length=_min_word_length - abs(_affix_length),",
            "            model=_context_to_tag,",
            "            backoff=backoff,",
            "        )",
            "",
            "    def context(self, tokens, index, history):",
            "        token = tokens[index]",
            "        if len(token) < self._min_word_length:",
            "            return None",
            "        elif self._affix_length > 0:",
            "            return token[: self._affix_length]",
            "        else:",
            "            return token[self._affix_length :]",
            "",
            "",
            "@jsontags.register_tag",
            "class RegexpTagger(SequentialBackoffTagger):",
            "    r\"\"\"",
            "    Regular Expression Tagger",
            "",
            "    The RegexpTagger assigns tags to tokens by comparing their",
            "    word strings to a series of regular expressions.  The following tagger",
            "    uses word suffixes to make guesses about the correct Brown Corpus part",
            "    of speech tag:",
            "",
            "        >>> from nltk.corpus import brown",
            "        >>> from nltk.tag import RegexpTagger",
            "        >>> test_sent = brown.sents(categories='news')[0]",
            "        >>> regexp_tagger = RegexpTagger(",
            "        ...     [(r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers",
            "        ...      (r'(The|the|A|a|An|an)$', 'AT'),   # articles",
            "        ...      (r'.*able$', 'JJ'),                # adjectives",
            "        ...      (r'.*ness$', 'NN'),                # nouns formed from adjectives",
            "        ...      (r'.*ly$', 'RB'),                  # adverbs",
            "        ...      (r'.*s$', 'NNS'),                  # plural nouns",
            "        ...      (r'.*ing$', 'VBG'),                # gerunds",
            "        ...      (r'.*ed$', 'VBD'),                 # past tense verbs",
            "        ...      (r'.*', 'NN')                      # nouns (default)",
            "        ... ])",
            "        >>> regexp_tagger",
            "        <Regexp Tagger: size=9>",
            "        >>> regexp_tagger.tag(test_sent) # doctest: +NORMALIZE_WHITESPACE",
            "        [('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'),",
            "        ('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'),",
            "        (\"Atlanta's\", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'),",
            "        ('produced', 'VBD'), ('``', 'NN'), ('no', 'NN'), ('evidence', 'NN'), (\"''\", 'NN'),",
            "        ('that', 'NN'), ('any', 'NN'), ('irregularities', 'NNS'), ('took', 'NN'),",
            "        ('place', 'NN'), ('.', 'NN')]",
            "",
            "    :type regexps: list(tuple(str, str))",
            "    :param regexps: A list of ``(regexp, tag)`` pairs, each of",
            "        which indicates that a word matching ``regexp`` should",
            "        be tagged with ``tag``.  The pairs will be evaluated in",
            "        order.  If none of the regexps match a word, then the",
            "        optional backoff tagger is invoked, else it is",
            "        assigned the tag None.",
            "    \"\"\"",
            "",
            "    json_tag = \"nltk.tag.sequential.RegexpTagger\"",
            "",
            "    def __init__(",
            "        self, regexps: List[Tuple[str, str]], backoff: Optional[TaggerI] = None",
            "    ):",
            "        super().__init__(backoff)",
            "        self._regexps = []",
            "        for regexp, tag in regexps:",
            "            try:",
            "                self._regexps.append((re.compile(regexp), tag))",
            "            except Exception as e:",
            "                raise Exception(",
            "                    f\"Invalid RegexpTagger regexp: {e}\\n- regexp: {regexp!r}\\n- tag: {tag!r}\"",
            "                ) from e",
            "",
            "    def encode_json_obj(self):",
            "        return [(regexp.pattern, tag) for regexp, tag in self._regexps], self.backoff",
            "",
            "    @classmethod",
            "    def decode_json_obj(cls, obj):",
            "        regexps, backoff = obj",
            "        return cls(regexps, backoff)",
            "",
            "    def choose_tag(self, tokens, index, history):",
            "        for regexp, tag in self._regexps:",
            "            if re.match(regexp, tokens[index]):",
            "                return tag",
            "        return None",
            "",
            "    def __repr__(self):",
            "        return f\"<Regexp Tagger: size={len(self._regexps)}>\"",
            "",
            "",
            "class ClassifierBasedTagger(SequentialBackoffTagger, FeaturesetTaggerI):",
            "    \"\"\"",
            "    A sequential tagger that uses a classifier to choose the tag for",
            "    each token in a sentence.  The featureset input for the classifier",
            "    is generated by a feature detector function::",
            "",
            "        feature_detector(tokens, index, history) -> featureset",
            "",
            "    Where tokens is the list of unlabeled tokens in the sentence;",
            "    index is the index of the token for which feature detection",
            "    should be performed; and history is list of the tags for all",
            "    tokens before index.",
            "",
            "    Construct a new classifier-based sequential tagger.",
            "",
            "    :param feature_detector: A function used to generate the",
            "        featureset input for the classifier::",
            "        feature_detector(tokens, index, history) -> featureset",
            "",
            "    :param train: A tagged corpus consisting of a list of tagged",
            "        sentences, where each sentence is a list of (word, tag) tuples.",
            "",
            "    :param backoff: A backoff tagger, to be used by the new tagger",
            "        if it encounters an unknown context.",
            "",
            "    :param classifier_builder: A function used to train a new",
            "        classifier based on the data in *train*.  It should take",
            "        one argument, a list of labeled featuresets (i.e.,",
            "        (featureset, label) tuples).",
            "",
            "    :param classifier: The classifier that should be used by the",
            "        tagger.  This is only useful if you want to manually",
            "        construct the classifier; normally, you would use *train*",
            "        instead.",
            "",
            "    :param backoff: A backoff tagger, used if this tagger is",
            "        unable to determine a tag for a given token.",
            "",
            "    :param cutoff_prob: If specified, then this tagger will fall",
            "        back on its backoff tagger if the probability of the most",
            "        likely tag is less than *cutoff_prob*.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        feature_detector=None,",
            "        train=None,",
            "        classifier_builder=NaiveBayesClassifier.train,",
            "        classifier=None,",
            "        backoff=None,",
            "        cutoff_prob=None,",
            "        verbose=False,",
            "    ):",
            "        self._check_params(train, classifier)",
            "",
            "        super().__init__(backoff)",
            "",
            "        if (train and classifier) or (not train and not classifier):",
            "            raise ValueError(",
            "                \"Must specify either training data or \" \"trained classifier.\"",
            "            )",
            "",
            "        if feature_detector is not None:",
            "            self._feature_detector = feature_detector",
            "            # The feature detector function, used to generate a featureset",
            "            # or each token: feature_detector(tokens, index, history) -> featureset",
            "",
            "        self._cutoff_prob = cutoff_prob",
            "        \"\"\"Cutoff probability for tagging -- if the probability of the",
            "           most likely tag is less than this, then use backoff.\"\"\"",
            "",
            "        self._classifier = classifier",
            "        \"\"\"The classifier used to choose a tag for each token.\"\"\"",
            "",
            "        if train:",
            "            self._train(train, classifier_builder, verbose)",
            "",
            "    def choose_tag(self, tokens, index, history):",
            "        # Use our feature detector to get the featureset.",
            "        featureset = self.feature_detector(tokens, index, history)",
            "",
            "        # Use the classifier to pick a tag.  If a cutoff probability",
            "        # was specified, then check that the tag's probability is",
            "        # higher than that cutoff first; otherwise, return None.",
            "        if self._cutoff_prob is None:",
            "            return self._classifier.classify(featureset)",
            "",
            "        pdist = self._classifier.prob_classify(featureset)",
            "        tag = pdist.max()",
            "        return tag if pdist.prob(tag) >= self._cutoff_prob else None",
            "",
            "    def _train(self, tagged_corpus, classifier_builder, verbose):",
            "        \"\"\"",
            "        Build a new classifier, based on the given training data",
            "        *tagged_corpus*.",
            "        \"\"\"",
            "",
            "        classifier_corpus = []",
            "        if verbose:",
            "            print(\"Constructing training corpus for classifier.\")",
            "",
            "        for sentence in tagged_corpus:",
            "            history = []",
            "            untagged_sentence, tags = zip(*sentence)",
            "            for index in range(len(sentence)):",
            "                featureset = self.feature_detector(untagged_sentence, index, history)",
            "                classifier_corpus.append((featureset, tags[index]))",
            "                history.append(tags[index])",
            "",
            "        if verbose:",
            "            print(f\"Training classifier ({len(classifier_corpus)} instances)\")",
            "        self._classifier = classifier_builder(classifier_corpus)",
            "",
            "    def __repr__(self):",
            "        return f\"<ClassifierBasedTagger: {self._classifier}>\"",
            "",
            "    def feature_detector(self, tokens, index, history):",
            "        \"\"\"",
            "        Return the feature detector that this tagger uses to generate",
            "        featuresets for its classifier.  The feature detector is a",
            "        function with the signature::",
            "",
            "          feature_detector(tokens, index, history) -> featureset",
            "",
            "        See ``classifier()``",
            "        \"\"\"",
            "        return self._feature_detector(tokens, index, history)",
            "",
            "    def classifier(self):",
            "        \"\"\"",
            "        Return the classifier that this tagger uses to choose a tag",
            "        for each word in a sentence.  The input for this classifier is",
            "        generated using this tagger's feature detector.",
            "        See ``feature_detector()``",
            "        \"\"\"",
            "        return self._classifier",
            "",
            "",
            "class ClassifierBasedPOSTagger(ClassifierBasedTagger):",
            "    \"\"\"",
            "    A classifier based part of speech tagger.",
            "    \"\"\"",
            "",
            "    def feature_detector(self, tokens, index, history):",
            "        word = tokens[index]",
            "        if index == 0:",
            "            prevword = prevprevword = None",
            "            prevtag = prevprevtag = None",
            "        elif index == 1:",
            "            prevword = tokens[index - 1].lower()",
            "            prevprevword = None",
            "            prevtag = history[index - 1]",
            "            prevprevtag = None",
            "        else:",
            "            prevword = tokens[index - 1].lower()",
            "            prevprevword = tokens[index - 2].lower()",
            "            prevtag = history[index - 1]",
            "            prevprevtag = history[index - 2]",
            "",
            "        if re.match(r\"[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$\", word):",
            "            shape = \"number\"",
            "        elif re.match(r\"\\W+$\", word):",
            "            shape = \"punct\"",
            "        elif re.match(\"[A-Z][a-z]+$\", word):",
            "            shape = \"upcase\"",
            "        elif re.match(\"[a-z]+$\", word):",
            "            shape = \"downcase\"",
            "        elif re.match(r\"\\w+$\", word):",
            "            shape = \"mixedcase\"",
            "        else:",
            "            shape = \"other\"",
            "",
            "        features = {",
            "            \"prevtag\": prevtag,",
            "            \"prevprevtag\": prevprevtag,",
            "            \"word\": word,",
            "            \"word.lower\": word.lower(),",
            "            \"suffix3\": word.lower()[-3:],",
            "            \"suffix2\": word.lower()[-2:],",
            "            \"suffix1\": word.lower()[-1:],",
            "            \"prevprevword\": prevprevword,",
            "            \"prevword\": prevword,",
            "            \"prevtag+word\": f\"{prevtag}+{word.lower()}\",",
            "            \"prevprevtag+word\": f\"{prevprevtag}+{word.lower()}\",",
            "            \"prevword+word\": f\"{prevword}+{word.lower()}\",",
            "            \"shape\": shape,",
            "        }",
            "        return features"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "340": [
                "UnigramTagger"
            ],
            "494": [
                "RegexpTagger"
            ],
            "506": [
                "RegexpTagger"
            ],
            "518": [
                "RegexpTagger"
            ]
        },
        "addLocation": []
    },
    "nltk/tbl/demo.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 393,
                "afterPatchRowNumber": 393,
                "PatchRowcode": "     plt.savefig(learning_curve_output)"
            },
            "1": {
                "beforePatchRowNumber": 394,
                "afterPatchRowNumber": 394,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 395,
                "afterPatchRowNumber": 395,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 396,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-NN_CD_TAGGER = RegexpTagger([(r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"), (r\".*\", \"NN\")])"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 396,
                "PatchRowcode": "+NN_CD_TAGGER = RegexpTagger([(r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"), (r\".*\", \"NN\")])"
            },
            "5": {
                "beforePatchRowNumber": 397,
                "afterPatchRowNumber": 397,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 398,
                "afterPatchRowNumber": 398,
                "PatchRowcode": " REGEXP_TAGGER = RegexpTagger("
            },
            "7": {
                "beforePatchRowNumber": 399,
                "afterPatchRowNumber": 399,
                "PatchRowcode": "     ["
            },
            "8": {
                "beforePatchRowNumber": 400,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 400,
                "PatchRowcode": "+        (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers"
            },
            "10": {
                "beforePatchRowNumber": 401,
                "afterPatchRowNumber": 401,
                "PatchRowcode": "         (r\"(The|the|A|a|An|an)$\", \"AT\"),  # articles"
            },
            "11": {
                "beforePatchRowNumber": 402,
                "afterPatchRowNumber": 402,
                "PatchRowcode": "         (r\".*able$\", \"JJ\"),  # adjectives"
            },
            "12": {
                "beforePatchRowNumber": 403,
                "afterPatchRowNumber": 403,
                "PatchRowcode": "         (r\".*ness$\", \"NN\"),  # nouns formed from adjectives"
            }
        },
        "frontPatchFile": [
            "# Natural Language Toolkit: Transformation-based learning",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# Author: Marcus Uneson <marcus.uneson@gmail.com>",
            "#   based on previous (nltk2) version by",
            "#   Christopher Maloof, Edward Loper, Steven Bird",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see  LICENSE.TXT",
            "",
            "import os",
            "import pickle",
            "import random",
            "import time",
            "",
            "from nltk.corpus import treebank",
            "from nltk.tag import BrillTaggerTrainer, RegexpTagger, UnigramTagger",
            "from nltk.tag.brill import Pos, Word",
            "from nltk.tbl import Template, error_list",
            "",
            "",
            "def demo():",
            "    \"\"\"",
            "    Run a demo with defaults. See source comments for details,",
            "    or docstrings of any of the more specific demo_* functions.",
            "    \"\"\"",
            "    postag()",
            "",
            "",
            "def demo_repr_rule_format():",
            "    \"\"\"",
            "    Exemplify repr(Rule) (see also str(Rule) and Rule.format(\"verbose\"))",
            "    \"\"\"",
            "    postag(ruleformat=\"repr\")",
            "",
            "",
            "def demo_str_rule_format():",
            "    \"\"\"",
            "    Exemplify repr(Rule) (see also str(Rule) and Rule.format(\"verbose\"))",
            "    \"\"\"",
            "    postag(ruleformat=\"str\")",
            "",
            "",
            "def demo_verbose_rule_format():",
            "    \"\"\"",
            "    Exemplify Rule.format(\"verbose\")",
            "    \"\"\"",
            "    postag(ruleformat=\"verbose\")",
            "",
            "",
            "def demo_multiposition_feature():",
            "    \"\"\"",
            "    The feature/s of a template takes a list of positions",
            "    relative to the current word where the feature should be",
            "    looked for, conceptually joined by logical OR. For instance,",
            "    Pos([-1, 1]), given a value V, will hold whenever V is found",
            "    one step to the left and/or one step to the right.",
            "",
            "    For contiguous ranges, a 2-arg form giving inclusive end",
            "    points can also be used: Pos(-3, -1) is the same as the arg",
            "    below.",
            "    \"\"\"",
            "    postag(templates=[Template(Pos([-3, -2, -1]))])",
            "",
            "",
            "def demo_multifeature_template():",
            "    \"\"\"",
            "    Templates can have more than a single feature.",
            "    \"\"\"",
            "    postag(templates=[Template(Word([0]), Pos([-2, -1]))])",
            "",
            "",
            "def demo_template_statistics():",
            "    \"\"\"",
            "    Show aggregate statistics per template. Little used templates are",
            "    candidates for deletion, much used templates may possibly be refined.",
            "",
            "    Deleting unused templates is mostly about saving time and/or space:",
            "    training is basically O(T) in the number of templates T",
            "    (also in terms of memory usage, which often will be the limiting factor).",
            "    \"\"\"",
            "    postag(incremental_stats=True, template_stats=True)",
            "",
            "",
            "def demo_generated_templates():",
            "    \"\"\"",
            "    Template.expand and Feature.expand are class methods facilitating",
            "    generating large amounts of templates. See their documentation for",
            "    details.",
            "",
            "    Note: training with 500 templates can easily fill all available",
            "    even on relatively small corpora",
            "    \"\"\"",
            "    wordtpls = Word.expand([-1, 0, 1], [1, 2], excludezero=False)",
            "    tagtpls = Pos.expand([-2, -1, 0, 1], [1, 2], excludezero=True)",
            "    templates = list(Template.expand([wordtpls, tagtpls], combinations=(1, 3)))",
            "    print(",
            "        \"Generated {} templates for transformation-based learning\".format(",
            "            len(templates)",
            "        )",
            "    )",
            "    postag(templates=templates, incremental_stats=True, template_stats=True)",
            "",
            "",
            "def demo_learning_curve():",
            "    \"\"\"",
            "    Plot a learning curve -- the contribution on tagging accuracy of",
            "    the individual rules.",
            "    Note: requires matplotlib",
            "    \"\"\"",
            "    postag(",
            "        incremental_stats=True,",
            "        separate_baseline_data=True,",
            "        learning_curve_output=\"learningcurve.png\",",
            "    )",
            "",
            "",
            "def demo_error_analysis():",
            "    \"\"\"",
            "    Writes a file with context for each erroneous word after tagging testing data",
            "    \"\"\"",
            "    postag(error_output=\"errors.txt\")",
            "",
            "",
            "def demo_serialize_tagger():",
            "    \"\"\"",
            "    Serializes the learned tagger to a file in pickle format; reloads it",
            "    and validates the process.",
            "    \"\"\"",
            "    postag(serialize_output=\"tagger.pcl\")",
            "",
            "",
            "def demo_high_accuracy_rules():",
            "    \"\"\"",
            "    Discard rules with low accuracy. This may hurt performance a bit,",
            "    but will often produce rules which are more interesting read to a human.",
            "    \"\"\"",
            "    postag(num_sents=3000, min_acc=0.96, min_score=10)",
            "",
            "",
            "def postag(",
            "    templates=None,",
            "    tagged_data=None,",
            "    num_sents=1000,",
            "    max_rules=300,",
            "    min_score=3,",
            "    min_acc=None,",
            "    train=0.8,",
            "    trace=3,",
            "    randomize=False,",
            "    ruleformat=\"str\",",
            "    incremental_stats=False,",
            "    template_stats=False,",
            "    error_output=None,",
            "    serialize_output=None,",
            "    learning_curve_output=None,",
            "    learning_curve_take=300,",
            "    baseline_backoff_tagger=None,",
            "    separate_baseline_data=False,",
            "    cache_baseline_tagger=None,",
            "):",
            "    \"\"\"",
            "    Brill Tagger Demonstration",
            "    :param templates: how many sentences of training and testing data to use",
            "    :type templates: list of Template",
            "",
            "    :param tagged_data: maximum number of rule instances to create",
            "    :type tagged_data: C{int}",
            "",
            "    :param num_sents: how many sentences of training and testing data to use",
            "    :type num_sents: C{int}",
            "",
            "    :param max_rules: maximum number of rule instances to create",
            "    :type max_rules: C{int}",
            "",
            "    :param min_score: the minimum score for a rule in order for it to be considered",
            "    :type min_score: C{int}",
            "",
            "    :param min_acc: the minimum score for a rule in order for it to be considered",
            "    :type min_acc: C{float}",
            "",
            "    :param train: the fraction of the the corpus to be used for training (1=all)",
            "    :type train: C{float}",
            "",
            "    :param trace: the level of diagnostic tracing output to produce (0-4)",
            "    :type trace: C{int}",
            "",
            "    :param randomize: whether the training data should be a random subset of the corpus",
            "    :type randomize: C{bool}",
            "",
            "    :param ruleformat: rule output format, one of \"str\", \"repr\", \"verbose\"",
            "    :type ruleformat: C{str}",
            "",
            "    :param incremental_stats: if true, will tag incrementally and collect stats for each rule (rather slow)",
            "    :type incremental_stats: C{bool}",
            "",
            "    :param template_stats: if true, will print per-template statistics collected in training and (optionally) testing",
            "    :type template_stats: C{bool}",
            "",
            "    :param error_output: the file where errors will be saved",
            "    :type error_output: C{string}",
            "",
            "    :param serialize_output: the file where the learned tbl tagger will be saved",
            "    :type serialize_output: C{string}",
            "",
            "    :param learning_curve_output: filename of plot of learning curve(s) (train and also test, if available)",
            "    :type learning_curve_output: C{string}",
            "",
            "    :param learning_curve_take: how many rules plotted",
            "    :type learning_curve_take: C{int}",
            "",
            "    :param baseline_backoff_tagger: the file where rules will be saved",
            "    :type baseline_backoff_tagger: tagger",
            "",
            "    :param separate_baseline_data: use a fraction of the training data exclusively for training baseline",
            "    :type separate_baseline_data: C{bool}",
            "",
            "    :param cache_baseline_tagger: cache baseline tagger to this file (only interesting as a temporary workaround to get",
            "                                  deterministic output from the baseline unigram tagger between python versions)",
            "    :type cache_baseline_tagger: C{string}",
            "",
            "",
            "    Note on separate_baseline_data: if True, reuse training data both for baseline and rule learner. This",
            "    is fast and fine for a demo, but is likely to generalize worse on unseen data.",
            "    Also cannot be sensibly used for learning curves on training data (the baseline will be artificially high).",
            "    \"\"\"",
            "",
            "    # defaults",
            "    baseline_backoff_tagger = baseline_backoff_tagger or REGEXP_TAGGER",
            "    if templates is None:",
            "        from nltk.tag.brill import brill24, describe_template_sets",
            "",
            "        # some pre-built template sets taken from typical systems or publications are",
            "        # available. Print a list with describe_template_sets()",
            "        # for instance:",
            "        templates = brill24()",
            "    (training_data, baseline_data, gold_data, testing_data) = _demo_prepare_data(",
            "        tagged_data, train, num_sents, randomize, separate_baseline_data",
            "    )",
            "",
            "    # creating (or reloading from cache) a baseline tagger (unigram tagger)",
            "    # this is just a mechanism for getting deterministic output from the baseline between",
            "    # python versions",
            "    if cache_baseline_tagger:",
            "        if not os.path.exists(cache_baseline_tagger):",
            "            baseline_tagger = UnigramTagger(",
            "                baseline_data, backoff=baseline_backoff_tagger",
            "            )",
            "            with open(cache_baseline_tagger, \"w\") as print_rules:",
            "                pickle.dump(baseline_tagger, print_rules)",
            "            print(",
            "                \"Trained baseline tagger, pickled it to {}\".format(",
            "                    cache_baseline_tagger",
            "                )",
            "            )",
            "        with open(cache_baseline_tagger) as print_rules:",
            "            baseline_tagger = pickle.load(print_rules)",
            "            print(f\"Reloaded pickled tagger from {cache_baseline_tagger}\")",
            "    else:",
            "        baseline_tagger = UnigramTagger(baseline_data, backoff=baseline_backoff_tagger)",
            "        print(\"Trained baseline tagger\")",
            "    if gold_data:",
            "        print(",
            "            \"    Accuracy on test set: {:0.4f}\".format(",
            "                baseline_tagger.evaluate(gold_data)",
            "            )",
            "        )",
            "",
            "    # creating a Brill tagger",
            "    tbrill = time.time()",
            "    trainer = BrillTaggerTrainer(",
            "        baseline_tagger, templates, trace, ruleformat=ruleformat",
            "    )",
            "    print(\"Training tbl tagger...\")",
            "    brill_tagger = trainer.train(training_data, max_rules, min_score, min_acc)",
            "    print(f\"Trained tbl tagger in {time.time() - tbrill:0.2f} seconds\")",
            "    if gold_data:",
            "        print(\"    Accuracy on test set: %.4f\" % brill_tagger.evaluate(gold_data))",
            "",
            "    # printing the learned rules, if learned silently",
            "    if trace == 1:",
            "        print(\"\\nLearned rules: \")",
            "        for (ruleno, rule) in enumerate(brill_tagger.rules(), 1):",
            "            print(f\"{ruleno:4d} {rule.format(ruleformat):s}\")",
            "",
            "    # printing template statistics (optionally including comparison with the training data)",
            "    # note: if not separate_baseline_data, then baseline accuracy will be artificially high",
            "    if incremental_stats:",
            "        print(",
            "            \"Incrementally tagging the test data, collecting individual rule statistics\"",
            "        )",
            "        (taggedtest, teststats) = brill_tagger.batch_tag_incremental(",
            "            testing_data, gold_data",
            "        )",
            "        print(\"    Rule statistics collected\")",
            "        if not separate_baseline_data:",
            "            print(",
            "                \"WARNING: train_stats asked for separate_baseline_data=True; the baseline \"",
            "                \"will be artificially high\"",
            "            )",
            "        trainstats = brill_tagger.train_stats()",
            "        if template_stats:",
            "            brill_tagger.print_template_statistics(teststats)",
            "        if learning_curve_output:",
            "            _demo_plot(",
            "                learning_curve_output, teststats, trainstats, take=learning_curve_take",
            "            )",
            "            print(f\"Wrote plot of learning curve to {learning_curve_output}\")",
            "    else:",
            "        print(\"Tagging the test data\")",
            "        taggedtest = brill_tagger.tag_sents(testing_data)",
            "        if template_stats:",
            "            brill_tagger.print_template_statistics()",
            "",
            "    # writing error analysis to file",
            "    if error_output is not None:",
            "        with open(error_output, \"w\") as f:",
            "            f.write(\"Errors for Brill Tagger %r\\n\\n\" % serialize_output)",
            "            f.write(\"\\n\".join(error_list(gold_data, taggedtest)).encode(\"utf-8\") + \"\\n\")",
            "        print(f\"Wrote tagger errors including context to {error_output}\")",
            "",
            "    # serializing the tagger to a pickle file and reloading (just to see it works)",
            "    if serialize_output is not None:",
            "        taggedtest = brill_tagger.tag_sents(testing_data)",
            "        with open(serialize_output, \"w\") as print_rules:",
            "            pickle.dump(brill_tagger, print_rules)",
            "        print(f\"Wrote pickled tagger to {serialize_output}\")",
            "        with open(serialize_output) as print_rules:",
            "            brill_tagger_reloaded = pickle.load(print_rules)",
            "        print(f\"Reloaded pickled tagger from {serialize_output}\")",
            "        taggedtest_reloaded = brill_tagger.tag_sents(testing_data)",
            "        if taggedtest == taggedtest_reloaded:",
            "            print(\"Reloaded tagger tried on test set, results identical\")",
            "        else:",
            "            print(\"PROBLEM: Reloaded tagger gave different results on test set\")",
            "",
            "",
            "def _demo_prepare_data(",
            "    tagged_data, train, num_sents, randomize, separate_baseline_data",
            "):",
            "    # train is the proportion of data used in training; the rest is reserved",
            "    # for testing.",
            "    if tagged_data is None:",
            "        print(\"Loading tagged data from treebank... \")",
            "        tagged_data = treebank.tagged_sents()",
            "    if num_sents is None or len(tagged_data) <= num_sents:",
            "        num_sents = len(tagged_data)",
            "    if randomize:",
            "        random.seed(len(tagged_data))",
            "        random.shuffle(tagged_data)",
            "    cutoff = int(num_sents * train)",
            "    training_data = tagged_data[:cutoff]",
            "    gold_data = tagged_data[cutoff:num_sents]",
            "    testing_data = [[t[0] for t in sent] for sent in gold_data]",
            "    if not separate_baseline_data:",
            "        baseline_data = training_data",
            "    else:",
            "        bl_cutoff = len(training_data) // 3",
            "        (baseline_data, training_data) = (",
            "            training_data[:bl_cutoff],",
            "            training_data[bl_cutoff:],",
            "        )",
            "    (trainseqs, traintokens) = corpus_size(training_data)",
            "    (testseqs, testtokens) = corpus_size(testing_data)",
            "    (bltrainseqs, bltraintokens) = corpus_size(baseline_data)",
            "    print(f\"Read testing data ({testseqs:d} sents/{testtokens:d} wds)\")",
            "    print(f\"Read training data ({trainseqs:d} sents/{traintokens:d} wds)\")",
            "    print(",
            "        \"Read baseline data ({:d} sents/{:d} wds) {:s}\".format(",
            "            bltrainseqs,",
            "            bltraintokens,",
            "            \"\" if separate_baseline_data else \"[reused the training set]\",",
            "        )",
            "    )",
            "    return (training_data, baseline_data, gold_data, testing_data)",
            "",
            "",
            "def _demo_plot(learning_curve_output, teststats, trainstats=None, take=None):",
            "    testcurve = [teststats[\"initialerrors\"]]",
            "    for rulescore in teststats[\"rulescores\"]:",
            "        testcurve.append(testcurve[-1] - rulescore)",
            "    testcurve = [1 - x / teststats[\"tokencount\"] for x in testcurve[:take]]",
            "",
            "    traincurve = [trainstats[\"initialerrors\"]]",
            "    for rulescore in trainstats[\"rulescores\"]:",
            "        traincurve.append(traincurve[-1] - rulescore)",
            "    traincurve = [1 - x / trainstats[\"tokencount\"] for x in traincurve[:take]]",
            "",
            "    import matplotlib.pyplot as plt",
            "",
            "    r = list(range(len(testcurve)))",
            "    plt.plot(r, testcurve, r, traincurve)",
            "    plt.axis([None, None, None, 1.0])",
            "    plt.savefig(learning_curve_output)",
            "",
            "",
            "NN_CD_TAGGER = RegexpTagger([(r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"), (r\".*\", \"NN\")])",
            "",
            "REGEXP_TAGGER = RegexpTagger(",
            "    [",
            "        (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers",
            "        (r\"(The|the|A|a|An|an)$\", \"AT\"),  # articles",
            "        (r\".*able$\", \"JJ\"),  # adjectives",
            "        (r\".*ness$\", \"NN\"),  # nouns formed from adjectives",
            "        (r\".*ly$\", \"RB\"),  # adverbs",
            "        (r\".*s$\", \"NNS\"),  # plural nouns",
            "        (r\".*ing$\", \"VBG\"),  # gerunds",
            "        (r\".*ed$\", \"VBD\"),  # past tense verbs",
            "        (r\".*\", \"NN\"),  # nouns (default)",
            "    ]",
            ")",
            "",
            "",
            "def corpus_size(seqs):",
            "    return (len(seqs), sum(len(x) for x in seqs))",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    demo_learning_curve()"
        ],
        "afterPatchFile": [
            "# Natural Language Toolkit: Transformation-based learning",
            "#",
            "# Copyright (C) 2001-2021 NLTK Project",
            "# Author: Marcus Uneson <marcus.uneson@gmail.com>",
            "#   based on previous (nltk2) version by",
            "#   Christopher Maloof, Edward Loper, Steven Bird",
            "# URL: <https://www.nltk.org/>",
            "# For license information, see  LICENSE.TXT",
            "",
            "import os",
            "import pickle",
            "import random",
            "import time",
            "",
            "from nltk.corpus import treebank",
            "from nltk.tag import BrillTaggerTrainer, RegexpTagger, UnigramTagger",
            "from nltk.tag.brill import Pos, Word",
            "from nltk.tbl import Template, error_list",
            "",
            "",
            "def demo():",
            "    \"\"\"",
            "    Run a demo with defaults. See source comments for details,",
            "    or docstrings of any of the more specific demo_* functions.",
            "    \"\"\"",
            "    postag()",
            "",
            "",
            "def demo_repr_rule_format():",
            "    \"\"\"",
            "    Exemplify repr(Rule) (see also str(Rule) and Rule.format(\"verbose\"))",
            "    \"\"\"",
            "    postag(ruleformat=\"repr\")",
            "",
            "",
            "def demo_str_rule_format():",
            "    \"\"\"",
            "    Exemplify repr(Rule) (see also str(Rule) and Rule.format(\"verbose\"))",
            "    \"\"\"",
            "    postag(ruleformat=\"str\")",
            "",
            "",
            "def demo_verbose_rule_format():",
            "    \"\"\"",
            "    Exemplify Rule.format(\"verbose\")",
            "    \"\"\"",
            "    postag(ruleformat=\"verbose\")",
            "",
            "",
            "def demo_multiposition_feature():",
            "    \"\"\"",
            "    The feature/s of a template takes a list of positions",
            "    relative to the current word where the feature should be",
            "    looked for, conceptually joined by logical OR. For instance,",
            "    Pos([-1, 1]), given a value V, will hold whenever V is found",
            "    one step to the left and/or one step to the right.",
            "",
            "    For contiguous ranges, a 2-arg form giving inclusive end",
            "    points can also be used: Pos(-3, -1) is the same as the arg",
            "    below.",
            "    \"\"\"",
            "    postag(templates=[Template(Pos([-3, -2, -1]))])",
            "",
            "",
            "def demo_multifeature_template():",
            "    \"\"\"",
            "    Templates can have more than a single feature.",
            "    \"\"\"",
            "    postag(templates=[Template(Word([0]), Pos([-2, -1]))])",
            "",
            "",
            "def demo_template_statistics():",
            "    \"\"\"",
            "    Show aggregate statistics per template. Little used templates are",
            "    candidates for deletion, much used templates may possibly be refined.",
            "",
            "    Deleting unused templates is mostly about saving time and/or space:",
            "    training is basically O(T) in the number of templates T",
            "    (also in terms of memory usage, which often will be the limiting factor).",
            "    \"\"\"",
            "    postag(incremental_stats=True, template_stats=True)",
            "",
            "",
            "def demo_generated_templates():",
            "    \"\"\"",
            "    Template.expand and Feature.expand are class methods facilitating",
            "    generating large amounts of templates. See their documentation for",
            "    details.",
            "",
            "    Note: training with 500 templates can easily fill all available",
            "    even on relatively small corpora",
            "    \"\"\"",
            "    wordtpls = Word.expand([-1, 0, 1], [1, 2], excludezero=False)",
            "    tagtpls = Pos.expand([-2, -1, 0, 1], [1, 2], excludezero=True)",
            "    templates = list(Template.expand([wordtpls, tagtpls], combinations=(1, 3)))",
            "    print(",
            "        \"Generated {} templates for transformation-based learning\".format(",
            "            len(templates)",
            "        )",
            "    )",
            "    postag(templates=templates, incremental_stats=True, template_stats=True)",
            "",
            "",
            "def demo_learning_curve():",
            "    \"\"\"",
            "    Plot a learning curve -- the contribution on tagging accuracy of",
            "    the individual rules.",
            "    Note: requires matplotlib",
            "    \"\"\"",
            "    postag(",
            "        incremental_stats=True,",
            "        separate_baseline_data=True,",
            "        learning_curve_output=\"learningcurve.png\",",
            "    )",
            "",
            "",
            "def demo_error_analysis():",
            "    \"\"\"",
            "    Writes a file with context for each erroneous word after tagging testing data",
            "    \"\"\"",
            "    postag(error_output=\"errors.txt\")",
            "",
            "",
            "def demo_serialize_tagger():",
            "    \"\"\"",
            "    Serializes the learned tagger to a file in pickle format; reloads it",
            "    and validates the process.",
            "    \"\"\"",
            "    postag(serialize_output=\"tagger.pcl\")",
            "",
            "",
            "def demo_high_accuracy_rules():",
            "    \"\"\"",
            "    Discard rules with low accuracy. This may hurt performance a bit,",
            "    but will often produce rules which are more interesting read to a human.",
            "    \"\"\"",
            "    postag(num_sents=3000, min_acc=0.96, min_score=10)",
            "",
            "",
            "def postag(",
            "    templates=None,",
            "    tagged_data=None,",
            "    num_sents=1000,",
            "    max_rules=300,",
            "    min_score=3,",
            "    min_acc=None,",
            "    train=0.8,",
            "    trace=3,",
            "    randomize=False,",
            "    ruleformat=\"str\",",
            "    incremental_stats=False,",
            "    template_stats=False,",
            "    error_output=None,",
            "    serialize_output=None,",
            "    learning_curve_output=None,",
            "    learning_curve_take=300,",
            "    baseline_backoff_tagger=None,",
            "    separate_baseline_data=False,",
            "    cache_baseline_tagger=None,",
            "):",
            "    \"\"\"",
            "    Brill Tagger Demonstration",
            "    :param templates: how many sentences of training and testing data to use",
            "    :type templates: list of Template",
            "",
            "    :param tagged_data: maximum number of rule instances to create",
            "    :type tagged_data: C{int}",
            "",
            "    :param num_sents: how many sentences of training and testing data to use",
            "    :type num_sents: C{int}",
            "",
            "    :param max_rules: maximum number of rule instances to create",
            "    :type max_rules: C{int}",
            "",
            "    :param min_score: the minimum score for a rule in order for it to be considered",
            "    :type min_score: C{int}",
            "",
            "    :param min_acc: the minimum score for a rule in order for it to be considered",
            "    :type min_acc: C{float}",
            "",
            "    :param train: the fraction of the the corpus to be used for training (1=all)",
            "    :type train: C{float}",
            "",
            "    :param trace: the level of diagnostic tracing output to produce (0-4)",
            "    :type trace: C{int}",
            "",
            "    :param randomize: whether the training data should be a random subset of the corpus",
            "    :type randomize: C{bool}",
            "",
            "    :param ruleformat: rule output format, one of \"str\", \"repr\", \"verbose\"",
            "    :type ruleformat: C{str}",
            "",
            "    :param incremental_stats: if true, will tag incrementally and collect stats for each rule (rather slow)",
            "    :type incremental_stats: C{bool}",
            "",
            "    :param template_stats: if true, will print per-template statistics collected in training and (optionally) testing",
            "    :type template_stats: C{bool}",
            "",
            "    :param error_output: the file where errors will be saved",
            "    :type error_output: C{string}",
            "",
            "    :param serialize_output: the file where the learned tbl tagger will be saved",
            "    :type serialize_output: C{string}",
            "",
            "    :param learning_curve_output: filename of plot of learning curve(s) (train and also test, if available)",
            "    :type learning_curve_output: C{string}",
            "",
            "    :param learning_curve_take: how many rules plotted",
            "    :type learning_curve_take: C{int}",
            "",
            "    :param baseline_backoff_tagger: the file where rules will be saved",
            "    :type baseline_backoff_tagger: tagger",
            "",
            "    :param separate_baseline_data: use a fraction of the training data exclusively for training baseline",
            "    :type separate_baseline_data: C{bool}",
            "",
            "    :param cache_baseline_tagger: cache baseline tagger to this file (only interesting as a temporary workaround to get",
            "                                  deterministic output from the baseline unigram tagger between python versions)",
            "    :type cache_baseline_tagger: C{string}",
            "",
            "",
            "    Note on separate_baseline_data: if True, reuse training data both for baseline and rule learner. This",
            "    is fast and fine for a demo, but is likely to generalize worse on unseen data.",
            "    Also cannot be sensibly used for learning curves on training data (the baseline will be artificially high).",
            "    \"\"\"",
            "",
            "    # defaults",
            "    baseline_backoff_tagger = baseline_backoff_tagger or REGEXP_TAGGER",
            "    if templates is None:",
            "        from nltk.tag.brill import brill24, describe_template_sets",
            "",
            "        # some pre-built template sets taken from typical systems or publications are",
            "        # available. Print a list with describe_template_sets()",
            "        # for instance:",
            "        templates = brill24()",
            "    (training_data, baseline_data, gold_data, testing_data) = _demo_prepare_data(",
            "        tagged_data, train, num_sents, randomize, separate_baseline_data",
            "    )",
            "",
            "    # creating (or reloading from cache) a baseline tagger (unigram tagger)",
            "    # this is just a mechanism for getting deterministic output from the baseline between",
            "    # python versions",
            "    if cache_baseline_tagger:",
            "        if not os.path.exists(cache_baseline_tagger):",
            "            baseline_tagger = UnigramTagger(",
            "                baseline_data, backoff=baseline_backoff_tagger",
            "            )",
            "            with open(cache_baseline_tagger, \"w\") as print_rules:",
            "                pickle.dump(baseline_tagger, print_rules)",
            "            print(",
            "                \"Trained baseline tagger, pickled it to {}\".format(",
            "                    cache_baseline_tagger",
            "                )",
            "            )",
            "        with open(cache_baseline_tagger) as print_rules:",
            "            baseline_tagger = pickle.load(print_rules)",
            "            print(f\"Reloaded pickled tagger from {cache_baseline_tagger}\")",
            "    else:",
            "        baseline_tagger = UnigramTagger(baseline_data, backoff=baseline_backoff_tagger)",
            "        print(\"Trained baseline tagger\")",
            "    if gold_data:",
            "        print(",
            "            \"    Accuracy on test set: {:0.4f}\".format(",
            "                baseline_tagger.evaluate(gold_data)",
            "            )",
            "        )",
            "",
            "    # creating a Brill tagger",
            "    tbrill = time.time()",
            "    trainer = BrillTaggerTrainer(",
            "        baseline_tagger, templates, trace, ruleformat=ruleformat",
            "    )",
            "    print(\"Training tbl tagger...\")",
            "    brill_tagger = trainer.train(training_data, max_rules, min_score, min_acc)",
            "    print(f\"Trained tbl tagger in {time.time() - tbrill:0.2f} seconds\")",
            "    if gold_data:",
            "        print(\"    Accuracy on test set: %.4f\" % brill_tagger.evaluate(gold_data))",
            "",
            "    # printing the learned rules, if learned silently",
            "    if trace == 1:",
            "        print(\"\\nLearned rules: \")",
            "        for (ruleno, rule) in enumerate(brill_tagger.rules(), 1):",
            "            print(f\"{ruleno:4d} {rule.format(ruleformat):s}\")",
            "",
            "    # printing template statistics (optionally including comparison with the training data)",
            "    # note: if not separate_baseline_data, then baseline accuracy will be artificially high",
            "    if incremental_stats:",
            "        print(",
            "            \"Incrementally tagging the test data, collecting individual rule statistics\"",
            "        )",
            "        (taggedtest, teststats) = brill_tagger.batch_tag_incremental(",
            "            testing_data, gold_data",
            "        )",
            "        print(\"    Rule statistics collected\")",
            "        if not separate_baseline_data:",
            "            print(",
            "                \"WARNING: train_stats asked for separate_baseline_data=True; the baseline \"",
            "                \"will be artificially high\"",
            "            )",
            "        trainstats = brill_tagger.train_stats()",
            "        if template_stats:",
            "            brill_tagger.print_template_statistics(teststats)",
            "        if learning_curve_output:",
            "            _demo_plot(",
            "                learning_curve_output, teststats, trainstats, take=learning_curve_take",
            "            )",
            "            print(f\"Wrote plot of learning curve to {learning_curve_output}\")",
            "    else:",
            "        print(\"Tagging the test data\")",
            "        taggedtest = brill_tagger.tag_sents(testing_data)",
            "        if template_stats:",
            "            brill_tagger.print_template_statistics()",
            "",
            "    # writing error analysis to file",
            "    if error_output is not None:",
            "        with open(error_output, \"w\") as f:",
            "            f.write(\"Errors for Brill Tagger %r\\n\\n\" % serialize_output)",
            "            f.write(\"\\n\".join(error_list(gold_data, taggedtest)).encode(\"utf-8\") + \"\\n\")",
            "        print(f\"Wrote tagger errors including context to {error_output}\")",
            "",
            "    # serializing the tagger to a pickle file and reloading (just to see it works)",
            "    if serialize_output is not None:",
            "        taggedtest = brill_tagger.tag_sents(testing_data)",
            "        with open(serialize_output, \"w\") as print_rules:",
            "            pickle.dump(brill_tagger, print_rules)",
            "        print(f\"Wrote pickled tagger to {serialize_output}\")",
            "        with open(serialize_output) as print_rules:",
            "            brill_tagger_reloaded = pickle.load(print_rules)",
            "        print(f\"Reloaded pickled tagger from {serialize_output}\")",
            "        taggedtest_reloaded = brill_tagger.tag_sents(testing_data)",
            "        if taggedtest == taggedtest_reloaded:",
            "            print(\"Reloaded tagger tried on test set, results identical\")",
            "        else:",
            "            print(\"PROBLEM: Reloaded tagger gave different results on test set\")",
            "",
            "",
            "def _demo_prepare_data(",
            "    tagged_data, train, num_sents, randomize, separate_baseline_data",
            "):",
            "    # train is the proportion of data used in training; the rest is reserved",
            "    # for testing.",
            "    if tagged_data is None:",
            "        print(\"Loading tagged data from treebank... \")",
            "        tagged_data = treebank.tagged_sents()",
            "    if num_sents is None or len(tagged_data) <= num_sents:",
            "        num_sents = len(tagged_data)",
            "    if randomize:",
            "        random.seed(len(tagged_data))",
            "        random.shuffle(tagged_data)",
            "    cutoff = int(num_sents * train)",
            "    training_data = tagged_data[:cutoff]",
            "    gold_data = tagged_data[cutoff:num_sents]",
            "    testing_data = [[t[0] for t in sent] for sent in gold_data]",
            "    if not separate_baseline_data:",
            "        baseline_data = training_data",
            "    else:",
            "        bl_cutoff = len(training_data) // 3",
            "        (baseline_data, training_data) = (",
            "            training_data[:bl_cutoff],",
            "            training_data[bl_cutoff:],",
            "        )",
            "    (trainseqs, traintokens) = corpus_size(training_data)",
            "    (testseqs, testtokens) = corpus_size(testing_data)",
            "    (bltrainseqs, bltraintokens) = corpus_size(baseline_data)",
            "    print(f\"Read testing data ({testseqs:d} sents/{testtokens:d} wds)\")",
            "    print(f\"Read training data ({trainseqs:d} sents/{traintokens:d} wds)\")",
            "    print(",
            "        \"Read baseline data ({:d} sents/{:d} wds) {:s}\".format(",
            "            bltrainseqs,",
            "            bltraintokens,",
            "            \"\" if separate_baseline_data else \"[reused the training set]\",",
            "        )",
            "    )",
            "    return (training_data, baseline_data, gold_data, testing_data)",
            "",
            "",
            "def _demo_plot(learning_curve_output, teststats, trainstats=None, take=None):",
            "    testcurve = [teststats[\"initialerrors\"]]",
            "    for rulescore in teststats[\"rulescores\"]:",
            "        testcurve.append(testcurve[-1] - rulescore)",
            "    testcurve = [1 - x / teststats[\"tokencount\"] for x in testcurve[:take]]",
            "",
            "    traincurve = [trainstats[\"initialerrors\"]]",
            "    for rulescore in trainstats[\"rulescores\"]:",
            "        traincurve.append(traincurve[-1] - rulescore)",
            "    traincurve = [1 - x / trainstats[\"tokencount\"] for x in traincurve[:take]]",
            "",
            "    import matplotlib.pyplot as plt",
            "",
            "    r = list(range(len(testcurve)))",
            "    plt.plot(r, testcurve, r, traincurve)",
            "    plt.axis([None, None, None, 1.0])",
            "    plt.savefig(learning_curve_output)",
            "",
            "",
            "NN_CD_TAGGER = RegexpTagger([(r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"), (r\".*\", \"NN\")])",
            "",
            "REGEXP_TAGGER = RegexpTagger(",
            "    [",
            "        (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers",
            "        (r\"(The|the|A|a|An|an)$\", \"AT\"),  # articles",
            "        (r\".*able$\", \"JJ\"),  # adjectives",
            "        (r\".*ness$\", \"NN\"),  # nouns formed from adjectives",
            "        (r\".*ly$\", \"RB\"),  # adverbs",
            "        (r\".*s$\", \"NNS\"),  # plural nouns",
            "        (r\".*ing$\", \"VBG\"),  # gerunds",
            "        (r\".*ed$\", \"VBD\"),  # past tense verbs",
            "        (r\".*\", \"NN\"),  # nouns (default)",
            "    ]",
            ")",
            "",
            "",
            "def corpus_size(seqs):",
            "    return (len(seqs), sum(len(x) for x in seqs))",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    demo_learning_curve()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "396": [
                "NN_CD_TAGGER"
            ],
            "400": []
        },
        "addLocation": []
    }
}