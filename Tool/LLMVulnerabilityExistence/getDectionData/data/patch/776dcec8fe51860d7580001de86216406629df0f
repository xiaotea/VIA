{
    "vllm/envs.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "     VLLM_DP_MASTER_IP: str = \"\""
            },
            "1": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "     VLLM_DP_MASTER_PORT: int = 0"
            },
            "2": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "     VLLM_MARLIN_USE_ATOMIC_ADD: bool = False"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+    VLLM_V0_USE_OUTLINES_CACHE: bool = False"
            },
            "4": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 99,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 100,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 101,
                "PatchRowcode": " def get_default_cache_root():"
            },
            "7": {
                "beforePatchRowNumber": 623,
                "afterPatchRowNumber": 624,
                "PatchRowcode": "     # Whether to use atomicAdd reduce in gptq/awq marlin kernel."
            },
            "8": {
                "beforePatchRowNumber": 624,
                "afterPatchRowNumber": 625,
                "PatchRowcode": "     \"VLLM_MARLIN_USE_ATOMIC_ADD\":"
            },
            "9": {
                "beforePatchRowNumber": 625,
                "afterPatchRowNumber": 626,
                "PatchRowcode": "     lambda: os.environ.get(\"VLLM_MARLIN_USE_ATOMIC_ADD\", \"0\") == \"1\","
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 627,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 628,
                "PatchRowcode": "+    # Whether to turn on the outlines cache for V0"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 629,
                "PatchRowcode": "+    # This cache is unbounded and on disk, so it's not safe to use in"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 630,
                "PatchRowcode": "+    # an environment with potentially malicious users."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 631,
                "PatchRowcode": "+    \"VLLM_V0_USE_OUTLINES_CACHE\":"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 632,
                "PatchRowcode": "+    lambda: os.environ.get(\"VLLM_V0_USE_OUTLINES_CACHE\", \"0\") == \"1\","
            },
            "16": {
                "beforePatchRowNumber": 626,
                "afterPatchRowNumber": 633,
                "PatchRowcode": " }"
            },
            "17": {
                "beforePatchRowNumber": 627,
                "afterPatchRowNumber": 634,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 628,
                "afterPatchRowNumber": 635,
                "PatchRowcode": " # end-env-vars-definition"
            }
        },
        "frontPatchFile": [
            "# SPDX-License-Identifier: Apache-2.0",
            "",
            "import os",
            "import tempfile",
            "from typing import TYPE_CHECKING, Any, Callable, Optional",
            "",
            "if TYPE_CHECKING:",
            "    VLLM_HOST_IP: str = \"\"",
            "    VLLM_PORT: Optional[int] = None",
            "    VLLM_RPC_BASE_PATH: str = tempfile.gettempdir()",
            "    VLLM_USE_MODELSCOPE: bool = False",
            "    VLLM_RINGBUFFER_WARNING_INTERVAL: int = 60",
            "    VLLM_NCCL_SO_PATH: Optional[str] = None",
            "    LD_LIBRARY_PATH: Optional[str] = None",
            "    VLLM_USE_TRITON_FLASH_ATTN: bool = False",
            "    VLLM_FLASH_ATTN_VERSION: Optional[int] = None",
            "    LOCAL_RANK: int = 0",
            "    CUDA_VISIBLE_DEVICES: Optional[str] = None",
            "    VLLM_ENGINE_ITERATION_TIMEOUT_S: int = 60",
            "    VLLM_API_KEY: Optional[str] = None",
            "    S3_ACCESS_KEY_ID: Optional[str] = None",
            "    S3_SECRET_ACCESS_KEY: Optional[str] = None",
            "    S3_ENDPOINT_URL: Optional[str] = None",
            "    VLLM_CACHE_ROOT: str = os.path.expanduser(\"~/.cache/vllm\")",
            "    VLLM_CONFIG_ROOT: str = os.path.expanduser(\"~/.config/vllm\")",
            "    VLLM_USAGE_STATS_SERVER: str = \"https://stats.vllm.ai\"",
            "    VLLM_NO_USAGE_STATS: bool = False",
            "    VLLM_DO_NOT_TRACK: bool = False",
            "    VLLM_USAGE_SOURCE: str = \"\"",
            "    VLLM_CONFIGURE_LOGGING: int = 1",
            "    VLLM_LOGGING_LEVEL: str = \"INFO\"",
            "    VLLM_LOGGING_PREFIX: str = \"\"",
            "    VLLM_LOGGING_CONFIG_PATH: Optional[str] = None",
            "    VLLM_LOGITS_PROCESSOR_THREADS: Optional[int] = None",
            "    VLLM_TRACE_FUNCTION: int = 0",
            "    VLLM_ATTENTION_BACKEND: Optional[str] = None",
            "    VLLM_USE_FLASHINFER_SAMPLER: Optional[bool] = None",
            "    VLLM_USE_FLASHINFER_REJECTION_SAMPLER: bool = False",
            "    VLLM_FLASHINFER_FORCE_TENSOR_CORES: bool = False",
            "    VLLM_PP_LAYER_PARTITION: Optional[str] = None",
            "    VLLM_CPU_KVCACHE_SPACE: int = 0",
            "    VLLM_CPU_OMP_THREADS_BIND: str = \"\"",
            "    VLLM_CPU_MOE_PREPACK: bool = True",
            "    VLLM_OPENVINO_DEVICE: str = \"CPU\"",
            "    VLLM_OPENVINO_KVCACHE_SPACE: int = 0",
            "    VLLM_OPENVINO_CPU_KV_CACHE_PRECISION: Optional[str] = None",
            "    VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS: bool = False",
            "    VLLM_XLA_CACHE_PATH: str = os.path.join(VLLM_CACHE_ROOT, \"xla_cache\")",
            "    VLLM_FUSED_MOE_CHUNK_SIZE: int = 64 * 1024",
            "    VLLM_USE_RAY_SPMD_WORKER: bool = False",
            "    VLLM_USE_RAY_COMPILED_DAG: bool = False",
            "    VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL: bool = True",
            "    VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM: bool = False",
            "    VLLM_WORKER_MULTIPROC_METHOD: str = \"fork\"",
            "    VLLM_ASSETS_CACHE: str = os.path.join(VLLM_CACHE_ROOT, \"assets\")",
            "    VLLM_IMAGE_FETCH_TIMEOUT: int = 5",
            "    VLLM_VIDEO_FETCH_TIMEOUT: int = 30",
            "    VLLM_AUDIO_FETCH_TIMEOUT: int = 10",
            "    VLLM_MM_INPUT_CACHE_SIZE: int = 256",
            "    VLLM_TARGET_DEVICE: str = \"cuda\"",
            "    MAX_JOBS: Optional[str] = None",
            "    NVCC_THREADS: Optional[str] = None",
            "    VLLM_USE_PRECOMPILED: bool = False",
            "    VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL: bool = False",
            "    VLLM_NO_DEPRECATION_WARNING: bool = False",
            "    VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = False",
            "    CMAKE_BUILD_TYPE: Optional[str] = None",
            "    VERBOSE: bool = False",
            "    VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = False",
            "    VLLM_RPC_TIMEOUT: int = 10000  # ms",
            "    VLLM_PLUGINS: Optional[list[str]] = None",
            "    VLLM_TORCH_PROFILER_DIR: Optional[str] = None",
            "    VLLM_USE_TRITON_AWQ: bool = False",
            "    VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False",
            "    VLLM_SKIP_P2P_CHECK: bool = False",
            "    VLLM_DISABLED_KERNELS: list[str] = []",
            "    VLLM_USE_V1: bool = False",
            "    VLLM_ROCM_FP8_PADDING: bool = True",
            "    VLLM_ENABLE_V1_MULTIPROCESSING: bool = True",
            "    VLLM_LOG_BATCHSIZE_INTERVAL: float = -1",
            "    VLLM_DISABLE_COMPILE_CACHE: bool = False",
            "    K_SCALE_CONSTANT: int = 200",
            "    V_SCALE_CONSTANT: int = 100",
            "    VLLM_SERVER_DEV_MODE: bool = False",
            "    VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128",
            "    VLLM_MLA_DISABLE: bool = False",
            "    VLLM_MLA_CUDA_MEM_ALIGN_KV_CACHE: bool = True",
            "    VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON: bool = False",
            "    VLLM_RAY_PER_WORKER_GPUS: float = 1.0",
            "    VLLM_RAY_BUNDLE_INDICES: str = \"\"",
            "    VLLM_CUDART_SO_PATH: Optional[str] = None",
            "    VLLM_USE_HPU_CONTIGUOUS_CACHE_FETCH: bool = True",
            "    VLLM_DP_RANK: int = 0",
            "    VLLM_DP_SIZE: int = 1",
            "    VLLM_DP_MASTER_IP: str = \"\"",
            "    VLLM_DP_MASTER_PORT: int = 0",
            "    VLLM_MARLIN_USE_ATOMIC_ADD: bool = False",
            "",
            "",
            "def get_default_cache_root():",
            "    return os.getenv(",
            "        \"XDG_CACHE_HOME\",",
            "        os.path.join(os.path.expanduser(\"~\"), \".cache\"),",
            "    )",
            "",
            "",
            "def get_default_config_root():",
            "    return os.getenv(",
            "        \"XDG_CONFIG_HOME\",",
            "        os.path.join(os.path.expanduser(\"~\"), \".config\"),",
            "    )",
            "",
            "",
            "def maybe_convert_int(value: Optional[str]) -> Optional[int]:",
            "    if value is None:",
            "        return None",
            "    return int(value)",
            "",
            "",
            "# The begin-* and end* here are used by the documentation generator",
            "# to extract the used env vars.",
            "",
            "# begin-env-vars-definition",
            "",
            "environment_variables: dict[str, Callable[[], Any]] = {",
            "",
            "    # ================== Installation Time Env Vars ==================",
            "",
            "    # Target device of vLLM, supporting [cuda (by default),",
            "    # rocm, neuron, cpu, openvino]",
            "    \"VLLM_TARGET_DEVICE\":",
            "    lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\"),",
            "",
            "    # Maximum number of compilation jobs to run in parallel.",
            "    # By default this is the number of CPUs",
            "    \"MAX_JOBS\":",
            "    lambda: os.getenv(\"MAX_JOBS\", None),",
            "",
            "    # Number of threads to use for nvcc",
            "    # By default this is 1.",
            "    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.",
            "    \"NVCC_THREADS\":",
            "    lambda: os.getenv(\"NVCC_THREADS\", None),",
            "",
            "    # If set, vllm will use precompiled binaries (*.so)",
            "    \"VLLM_USE_PRECOMPILED\":",
            "    lambda: bool(os.environ.get(\"VLLM_USE_PRECOMPILED\")) or bool(",
            "        os.environ.get(\"VLLM_PRECOMPILED_WHEEL_LOCATION\")),",
            "",
            "    # Whether to force using nightly wheel in python build.",
            "    # This is used for testing the nightly wheel in python build.",
            "    \"VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL\":",
            "    lambda: bool(int(os.getenv(\"VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL\", \"0\"))",
            "                 ),",
            "",
            "    # CMake build type",
            "    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"",
            "    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"",
            "    \"CMAKE_BUILD_TYPE\":",
            "    lambda: os.getenv(\"CMAKE_BUILD_TYPE\"),",
            "",
            "    # If set, vllm will print verbose logs during installation",
            "    \"VERBOSE\":",
            "    lambda: bool(int(os.getenv('VERBOSE', '0'))),",
            "",
            "    # Root directory for vLLM configuration files",
            "    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set",
            "    # Note that this not only affects how vllm finds its configuration files",
            "    # during runtime, but also affects how vllm installs its configuration",
            "    # files during **installation**.",
            "    \"VLLM_CONFIG_ROOT\":",
            "    lambda: os.path.expanduser(",
            "        os.getenv(",
            "            \"VLLM_CONFIG_ROOT\",",
            "            os.path.join(get_default_config_root(), \"vllm\"),",
            "        )),",
            "",
            "    # ================== Runtime Env Vars ==================",
            "",
            "    # Root directory for vLLM cache files",
            "    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set",
            "    \"VLLM_CACHE_ROOT\":",
            "    lambda: os.path.expanduser(",
            "        os.getenv(",
            "            \"VLLM_CACHE_ROOT\",",
            "            os.path.join(get_default_cache_root(), \"vllm\"),",
            "        )),",
            "",
            "    # used in distributed environment to determine the ip address",
            "    # of the current node, when the node has multiple network interfaces.",
            "    # If you are using multi-node inference, you should set this differently",
            "    # on each node.",
            "    'VLLM_HOST_IP':",
            "    lambda: os.getenv('VLLM_HOST_IP', \"\"),",
            "",
            "    # used in distributed environment to manually set the communication port",
            "    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the",
            "    # VLLM_PORT will be used as the first port, and the rest will be generated",
            "    # by incrementing the VLLM_PORT value.",
            "    # '0' is used to make mypy happy",
            "    'VLLM_PORT':",
            "    lambda: int(os.getenv('VLLM_PORT', '0'))",
            "    if 'VLLM_PORT' in os.environ else None,",
            "",
            "    # path used for ipc when the frontend api server is running in",
            "    # multi-processing mode to communicate with the backend engine process.",
            "    'VLLM_RPC_BASE_PATH':",
            "    lambda: os.getenv('VLLM_RPC_BASE_PATH', tempfile.gettempdir()),",
            "",
            "    # If true, will load models from ModelScope instead of Hugging Face Hub.",
            "    # note that the value is true or false, not numbers",
            "    \"VLLM_USE_MODELSCOPE\":",
            "    lambda: os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\",",
            "",
            "    # Interval in seconds to log a warning message when the ring buffer is full",
            "    \"VLLM_RINGBUFFER_WARNING_INTERVAL\":",
            "    lambda: int(os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")),",
            "",
            "    # path to cudatoolkit home directory, under which should be bin, include,",
            "    # and lib directories.",
            "    \"CUDA_HOME\":",
            "    lambda: os.environ.get(\"CUDA_HOME\", None),",
            "",
            "    # Path to the NCCL library file. It is needed because nccl>=2.19 brought",
            "    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234",
            "    \"VLLM_NCCL_SO_PATH\":",
            "    lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),",
            "",
            "    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl",
            "    # library file in the locations specified by `LD_LIBRARY_PATH`",
            "    \"LD_LIBRARY_PATH\":",
            "    lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),",
            "",
            "    # flag to control if vllm should use triton flash attention",
            "    \"VLLM_USE_TRITON_FLASH_ATTN\":",
            "    lambda: (os.environ.get(\"VLLM_USE_TRITON_FLASH_ATTN\", \"True\").lower() in",
            "             (\"true\", \"1\")),",
            "",
            "    # Force vllm to use a specific flash-attention version (2 or 3), only valid",
            "    # when using the flash-attention backend.",
            "    \"VLLM_FLASH_ATTN_VERSION\":",
            "    lambda: maybe_convert_int(os.environ.get(\"VLLM_FLASH_ATTN_VERSION\", None)),",
            "",
            "    # Internal flag to enable Dynamo fullgraph capture",
            "    \"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\":",
            "    lambda: bool(",
            "        os.environ.get(\"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\", \"1\") != \"0\"),",
            "",
            "    # local rank of the process in the distributed setting, used to determine",
            "    # the GPU device id",
            "    \"LOCAL_RANK\":",
            "    lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),",
            "",
            "    # used to control the visible devices in the distributed setting",
            "    \"CUDA_VISIBLE_DEVICES\":",
            "    lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),",
            "",
            "    # timeout for each iteration in the engine",
            "    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\":",
            "    lambda: int(os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")),",
            "",
            "    # API key for vLLM API server",
            "    \"VLLM_API_KEY\":",
            "    lambda: os.environ.get(\"VLLM_API_KEY\", None),",
            "",
            "    # S3 access information, used for tensorizer to load model from S3",
            "    \"S3_ACCESS_KEY_ID\":",
            "    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),",
            "    \"S3_SECRET_ACCESS_KEY\":",
            "    lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),",
            "    \"S3_ENDPOINT_URL\":",
            "    lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),",
            "",
            "    # Usage stats collection",
            "    \"VLLM_USAGE_STATS_SERVER\":",
            "    lambda: os.environ.get(\"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"),",
            "    \"VLLM_NO_USAGE_STATS\":",
            "    lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",",
            "    \"VLLM_DO_NOT_TRACK\":",
            "    lambda: (os.environ.get(\"VLLM_DO_NOT_TRACK\", None) or os.environ.get(",
            "        \"DO_NOT_TRACK\", None) or \"0\") == \"1\",",
            "    \"VLLM_USAGE_SOURCE\":",
            "    lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),",
            "",
            "    # Logging configuration",
            "    # If set to 0, vllm will not configure logging",
            "    # If set to 1, vllm will configure logging using the default configuration",
            "    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH",
            "    \"VLLM_CONFIGURE_LOGGING\":",
            "    lambda: int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\")),",
            "    \"VLLM_LOGGING_CONFIG_PATH\":",
            "    lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),",
            "",
            "    # this is used for configuring the default logging level",
            "    \"VLLM_LOGGING_LEVEL\":",
            "    lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\"),",
            "",
            "    # if set, VLLM_LOGGING_PREFIX will be prepended to all log messages",
            "    \"VLLM_LOGGING_PREFIX\":",
            "    lambda: os.getenv(\"VLLM_LOGGING_PREFIX\", \"\"),",
            "",
            "    # if set, vllm will call logits processors in a thread pool with this many",
            "    # threads. This is useful when using custom logits processors that either",
            "    # (a) launch additional CUDA kernels or (b) do significant CPU-bound work",
            "    # while not holding the python GIL, or both.",
            "    \"VLLM_LOGITS_PROCESSOR_THREADS\":",
            "    lambda: int(os.getenv(\"VLLM_LOGITS_PROCESSOR_THREADS\", \"0\"))",
            "    if \"VLLM_LOGITS_PROCESSOR_THREADS\" in os.environ else None,",
            "",
            "    # Trace function calls",
            "    # If set to 1, vllm will trace function calls",
            "    # Useful for debugging",
            "    \"VLLM_TRACE_FUNCTION\":",
            "    lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),",
            "",
            "    # Backend for attention computation",
            "    # Available options:",
            "    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention",
            "    # - \"FLASH_ATTN\": use FlashAttention",
            "    # - \"XFORMERS\": use XFormers",
            "    # - \"ROCM_FLASH\": use ROCmFlashAttention",
            "    # - \"FLASHINFER\": use flashinfer",
            "    # - \"FLASHMLA\": use FlashMLA",
            "    \"VLLM_ATTENTION_BACKEND\":",
            "    lambda: os.getenv(\"VLLM_ATTENTION_BACKEND\", None),",
            "",
            "    # If set, vllm will use flashinfer sampler",
            "    \"VLLM_USE_FLASHINFER_SAMPLER\":",
            "    lambda: bool(int(os.environ[\"VLLM_USE_FLASHINFER_SAMPLER\"]))",
            "    if \"VLLM_USE_FLASHINFER_SAMPLER\" in os.environ else None,",
            "",
            "    # If set, vllm will force flashinfer to use tensor cores;",
            "    # otherwise will use heuristic based on model architecture.",
            "    \"VLLM_FLASHINFER_FORCE_TENSOR_CORES\":",
            "    lambda: bool(int(os.getenv(\"VLLM_FLASHINFER_FORCE_TENSOR_CORES\", \"0\"))),",
            "",
            "    # Pipeline stage partition strategy",
            "    \"VLLM_PP_LAYER_PARTITION\":",
            "    lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),",
            "",
            "    # (CPU backend only) CPU key-value cache space.",
            "    # default is 4GB",
            "    \"VLLM_CPU_KVCACHE_SPACE\":",
            "    lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\")),",
            "",
            "    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",",
            "    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.",
            "    \"VLLM_CPU_OMP_THREADS_BIND\":",
            "    lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"all\"),",
            "",
            "    # (CPU backend only) whether to use prepack for MoE layer. This will be",
            "    # passed to ipex.llm.modules.GatedMLPMOE. On unsupported CPUs, you might",
            "    # need to set this to \"0\" (False).",
            "    \"VLLM_CPU_MOE_PREPACK\":",
            "    lambda: bool(int(os.getenv(\"VLLM_CPU_MOE_PREPACK\", \"1\"))),",
            "",
            "    # OpenVINO device selection",
            "    # default is CPU",
            "    \"VLLM_OPENVINO_DEVICE\":",
            "    lambda: os.getenv(\"VLLM_OPENVINO_DEVICE\", \"CPU\").upper(),",
            "",
            "    # OpenVINO key-value cache space",
            "    # default is 4GB",
            "    \"VLLM_OPENVINO_KVCACHE_SPACE\":",
            "    lambda: int(os.getenv(\"VLLM_OPENVINO_KVCACHE_SPACE\", \"0\")),",
            "",
            "    # OpenVINO KV cache precision",
            "    # default is bf16 if natively supported by platform, otherwise f16",
            "    # To enable KV cache compression, please, explicitly specify u8",
            "    \"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\":",
            "    lambda: os.getenv(\"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\", None),",
            "",
            "    # Enables weights compression during model export via HF Optimum",
            "    # default is False",
            "    \"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\":",
            "    lambda:",
            "    (os.environ.get(\"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\", \"0\").lower() in",
            "     (\"on\", \"true\", \"1\")),",
            "    # If the env var is set, then all workers will execute as separate",
            "    # processes from the engine, and we use the same mechanism to trigger",
            "    # execution on all workers.",
            "    # Run vLLM with VLLM_USE_RAY_SPMD_WORKER=1 to enable it.",
            "    \"VLLM_USE_RAY_SPMD_WORKER\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_SPMD_WORKER\", \"0\"))),",
            "",
            "    # If the env var is set, it uses the Ray's Compiled Graph",
            "    # (previously known as ADAG) API which optimizes the",
            "    # control plane overhead.",
            "    # Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.",
            "    \"VLLM_USE_RAY_COMPILED_DAG\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG\", \"0\"))),",
            "",
            "    # If the env var is set, it uses NCCL for communication in",
            "    # Ray's Compiled Graph. This flag is ignored if",
            "    # VLLM_USE_RAY_COMPILED_DAG is not set.",
            "    \"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\", \"1\"))",
            "                 ),",
            "",
            "    # If the env var is set, it enables GPU communication overlap",
            "    # (experimental feature) in Ray's Compiled Graph. This flag is ignored if",
            "    # VLLM_USE_RAY_COMPILED_DAG is not set.",
            "    \"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\", \"0\"))",
            "                 ),",
            "",
            "    # Use dedicated multiprocess context for workers.",
            "    # Both spawn and fork work",
            "    \"VLLM_WORKER_MULTIPROC_METHOD\":",
            "    lambda: os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\"),",
            "",
            "    # Path to the cache for storing downloaded assets",
            "    \"VLLM_ASSETS_CACHE\":",
            "    lambda: os.path.expanduser(",
            "        os.getenv(",
            "            \"VLLM_ASSETS_CACHE\",",
            "            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),",
            "        )),",
            "",
            "    # Timeout for fetching images when serving multimodal models",
            "    # Default is 5 seconds",
            "    \"VLLM_IMAGE_FETCH_TIMEOUT\":",
            "    lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),",
            "",
            "    # Timeout for fetching videos when serving multimodal models",
            "    # Default is 30 seconds",
            "    \"VLLM_VIDEO_FETCH_TIMEOUT\":",
            "    lambda: int(os.getenv(\"VLLM_VIDEO_FETCH_TIMEOUT\", \"30\")),",
            "",
            "    # Timeout for fetching audio when serving multimodal models",
            "    # Default is 10 seconds",
            "    \"VLLM_AUDIO_FETCH_TIMEOUT\":",
            "    lambda: int(os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"10\")),",
            "",
            "    # Cache size for multimodal feature/input cache for multimodal models",
            "    # in unit of number of multimodal data items (e.g. image, video, audio).",
            "    # Default is 256 multimodal data items.",
            "    \"VLLM_MM_INPUT_CACHE_SIZE\":",
            "    lambda: int(os.getenv(\"VLLM_MM_INPUT_CACHE_SIZE\", \"256\")),",
            "",
            "    # Path to the XLA persistent cache directory.",
            "    # Only used for XLA devices such as TPUs.",
            "    \"VLLM_XLA_CACHE_PATH\":",
            "    lambda: os.path.expanduser(",
            "        os.getenv(",
            "            \"VLLM_XLA_CACHE_PATH\",",
            "            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),",
            "        )),",
            "    \"VLLM_FUSED_MOE_CHUNK_SIZE\":",
            "    lambda: int(os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", \"32768\")),",
            "",
            "    # If set, vllm will skip the deprecation warnings.",
            "    \"VLLM_NO_DEPRECATION_WARNING\":",
            "    lambda: bool(int(os.getenv(\"VLLM_NO_DEPRECATION_WARNING\", \"0\"))),",
            "",
            "    # If set, the OpenAI API server will stay alive even after the underlying",
            "    # AsyncLLMEngine errors and stops serving requests",
            "    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\":",
            "    lambda: bool(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", 0)),",
            "",
            "    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows",
            "    # the user to specify a max sequence length greater than",
            "    # the max length derived from the model's config.json.",
            "    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.",
            "    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\":",
            "    lambda:",
            "    (os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower() in",
            "     (\"1\", \"true\")),",
            "",
            "    # If set, forces FP8 Marlin to be used for FP8 quantization regardless",
            "    # of the hardware support for FP8 compute.",
            "    \"VLLM_TEST_FORCE_FP8_MARLIN\":",
            "    lambda:",
            "    (os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower() in",
            "     (\"1\", \"true\")),",
            "    \"VLLM_TEST_FORCE_LOAD_FORMAT\":",
            "    lambda: os.getenv(\"VLLM_TEST_FORCE_LOAD_FORMAT\", \"dummy\"),",
            "",
            "    # Time in ms for the zmq client to wait for a response from the backend",
            "    # server for simple data operations",
            "    \"VLLM_RPC_TIMEOUT\":",
            "    lambda: int(os.getenv(\"VLLM_RPC_TIMEOUT\", \"10000\")),",
            "",
            "    # a list of plugin names to load, separated by commas.",
            "    # if this is not set, it means all plugins will be loaded",
            "    # if this is set to an empty string, no plugins will be loaded",
            "    \"VLLM_PLUGINS\":",
            "    lambda: None if \"VLLM_PLUGINS\" not in os.environ else os.environ[",
            "        \"VLLM_PLUGINS\"].split(\",\"),",
            "",
            "    # Enables torch profiler if set. Path to the directory where torch profiler",
            "    # traces are saved. Note that it must be an absolute path.",
            "    \"VLLM_TORCH_PROFILER_DIR\":",
            "    lambda: (None if os.getenv(\"VLLM_TORCH_PROFILER_DIR\", None) is None else os",
            "             .path.expanduser(os.getenv(\"VLLM_TORCH_PROFILER_DIR\", \".\"))),",
            "",
            "    # If set, vLLM will use Triton implementations of AWQ.",
            "    \"VLLM_USE_TRITON_AWQ\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),",
            "",
            "    # If set, allow loading or unloading lora adapters in runtime,",
            "    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\":",
            "    lambda:",
            "    (os.environ.get(\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\", \"0\").strip().lower() in",
            "     (\"1\", \"true\")),",
            "",
            "    # By default, vLLM will check the peer-to-peer capability itself,",
            "    # in case of broken drivers. See https://github.com/vllm-project/vllm/blob/a9b15c606fea67a072416ea0ea115261a2756058/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L101-L108 for details. # noqa",
            "    # If this env var is set to 1, vLLM will skip the peer-to-peer check,",
            "    # and trust the driver's peer-to-peer capability report.",
            "    \"VLLM_SKIP_P2P_CHECK\":",
            "    lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"0\") == \"1\",",
            "",
            "    # List of quantization kernels that should be disabled, used for testing",
            "    # and performance comparisons. Currently only affects MPLinearKernel",
            "    # selection",
            "    # (kernels: MacheteLinearKernel, MarlinLinearKernel, ExllamaLinearKernel)",
            "    \"VLLM_DISABLED_KERNELS\":",
            "    lambda: [] if \"VLLM_DISABLED_KERNELS\" not in os.environ else os.environ[",
            "        \"VLLM_DISABLED_KERNELS\"].split(\",\"),",
            "",
            "    # If set, use the V1 code path.",
            "    \"VLLM_USE_V1\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_V1\", \"0\"))),",
            "",
            "    # Pad the fp8 weights to 256 bytes for ROCm",
            "    \"VLLM_ROCM_FP8_PADDING\":",
            "    lambda: bool(int(os.getenv(\"VLLM_ROCM_FP8_PADDING\", \"1\"))),",
            "    # Divisor for dynamic key scale factor calculation for FP8 KV Cache",
            "    \"K_SCALE_CONSTANT\":",
            "    lambda: int(os.getenv(\"K_SCALE_CONSTANT\", \"200\")),",
            "",
            "    # Divisor for dynamic value scale factor calculation for FP8 KV Cache",
            "    \"V_SCALE_CONSTANT\":",
            "    lambda: int(os.getenv(\"V_SCALE_CONSTANT\", \"100\")),",
            "    # If set, enable multiprocessing in LLM for the V1 code path.",
            "    \"VLLM_ENABLE_V1_MULTIPROCESSING\":",
            "    lambda: bool(int(os.getenv(\"VLLM_ENABLE_V1_MULTIPROCESSING\", \"1\"))),",
            "    \"VLLM_LOG_BATCHSIZE_INTERVAL\":",
            "    lambda: float(os.getenv(\"VLLM_LOG_BATCHSIZE_INTERVAL\", \"-1\")),",
            "    \"VLLM_DISABLE_COMPILE_CACHE\":",
            "    lambda: bool(int(os.getenv(\"VLLM_DISABLE_COMPILE_CACHE\", \"0\"))),",
            "",
            "    # If set, vllm will run in development mode, which will enable",
            "    # some additional endpoints for developing and debugging,",
            "    # e.g. `/reset_prefix_cache`",
            "    \"VLLM_SERVER_DEV_MODE\":",
            "    lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),",
            "",
            "    # Controls the maximum number of requests to handle in a",
            "    # single asyncio task when processing per-token outputs in the",
            "    # V1 AsyncLLM interface. It is applicable when handling a high",
            "    # concurrency of streaming requests.",
            "    # Setting this too high can result in a higher variance of",
            "    # inter-message latencies. Setting it too low can negatively impact",
            "    # TTFT and overall throughput.",
            "    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":",
            "    lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),",
            "",
            "    # If set, vLLM will disable the MLA attention optimizations.",
            "    \"VLLM_MLA_DISABLE\":",
            "    lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE\", \"0\"))),",
            "",
            "    # If set, vLLM will use the Triton implementation of moe_align_block_size,",
            "    # i.e. moe_align_block_size_triton in fused_moe.py.",
            "    \"VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON\":",
            "    lambda: bool(int(os.getenv(\"VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON\", \"0\"))",
            "                 ),",
            "",
            "    # Number of GPUs per worker in Ray, if it is set to be a fraction,",
            "    # it allows ray to schedule multiple actors on a single GPU,",
            "    # so that users can colocate other actors on the same GPUs as vLLM.",
            "    \"VLLM_RAY_PER_WORKER_GPUS\":",
            "    lambda: float(os.getenv(\"VLLM_RAY_PER_WORKER_GPUS\", \"1.0\")),",
            "",
            "    # Bundle indices for Ray, if it is set, it can control precisely",
            "    # which indices are used for the Ray bundle, for every worker.",
            "    # Format: comma-separated list of integers, e.g. \"0,1,2,3\"",
            "    \"VLLM_RAY_BUNDLE_INDICES\":",
            "    lambda: os.getenv(\"VLLM_RAY_BUNDLE_INDICES\", \"\"),",
            "",
            "    # When on a Nvidia GPU aligns single entries (within a page) so they are 256",
            "    # byte aligned for better performance, this increases the memory usage of",
            "    # the cache. Currently this only affects MLA that results in non-256",
            "    # byte aligned entries. This matches the alignment the CUDA runtime uses",
            "    # for all allocations. Currently this primarily affects MLA, for most other",
            "    # models the alignment is already naturally aligned to 256 bytes.",
            "    \"VLLM_CUDA_MEM_ALIGN_KV_CACHE\":",
            "    lambda: bool(int(os.getenv(\"VLLM_CUDA_MEM_ALIGN_KV_CACHE\", \"1\"))),",
            "",
            "    # In some system, find_loaded_library() may not work. So we allow users to",
            "    # specify the path through environment variable VLLM_CUDART_SO_PATH.",
            "    \"VLLM_CUDART_SO_PATH\":",
            "    lambda: os.getenv(\"VLLM_CUDART_SO_PATH\", None),",
            "",
            "    # Contiguous cache fetching to avoid using costly gather operation on",
            "    # Gaudi3. This is only applicable to HPU contiguous cache. If set to true,",
            "    # contiguous cache fetch will be used.",
            "    \"VLLM_USE_HPU_CONTIGUOUS_CACHE_FETCH\":",
            "    lambda: os.environ.get(\"VLLM_CONTIGUOUS_PA\", \"true\").lower() in",
            "    (\"1\", \"true\"),",
            "",
            "    # Rank of the process in the data parallel setting",
            "    \"VLLM_DP_RANK\":",
            "    lambda: int(os.getenv(\"VLLM_DP_RANK\", \"0\")),",
            "",
            "    # World size of the data parallel setting",
            "    \"VLLM_DP_SIZE\":",
            "    lambda: int(os.getenv(\"VLLM_DP_SIZE\", \"1\")),",
            "",
            "    # IP address of the master node in the data parallel setting",
            "    \"VLLM_DP_MASTER_IP\":",
            "    lambda: os.getenv(\"VLLM_DP_MASTER_IP\", \"127.0.0.1\"),",
            "",
            "    # Port of the master node in the data parallel setting",
            "    \"VLLM_DP_MASTER_PORT\":",
            "    lambda: int(os.getenv(\"VLLM_DP_MASTER_PORT\", \"0\")),",
            "",
            "    # Whether to use S3 path for model loading in CI via RunAI Streamer",
            "    \"VLLM_CI_USE_S3\":",
            "    lambda: os.environ.get(\"VLLM_CI_USE_S3\", \"0\") == \"1\",",
            "",
            "    # Whether to use atomicAdd reduce in gptq/awq marlin kernel.",
            "    \"VLLM_MARLIN_USE_ATOMIC_ADD\":",
            "    lambda: os.environ.get(\"VLLM_MARLIN_USE_ATOMIC_ADD\", \"0\") == \"1\",",
            "}",
            "",
            "# end-env-vars-definition",
            "",
            "",
            "def __getattr__(name: str):",
            "    # lazy evaluation of environment variables",
            "    if name in environment_variables:",
            "        return environment_variables[name]()",
            "    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")",
            "",
            "",
            "def __dir__():",
            "    return list(environment_variables.keys())"
        ],
        "afterPatchFile": [
            "# SPDX-License-Identifier: Apache-2.0",
            "",
            "import os",
            "import tempfile",
            "from typing import TYPE_CHECKING, Any, Callable, Optional",
            "",
            "if TYPE_CHECKING:",
            "    VLLM_HOST_IP: str = \"\"",
            "    VLLM_PORT: Optional[int] = None",
            "    VLLM_RPC_BASE_PATH: str = tempfile.gettempdir()",
            "    VLLM_USE_MODELSCOPE: bool = False",
            "    VLLM_RINGBUFFER_WARNING_INTERVAL: int = 60",
            "    VLLM_NCCL_SO_PATH: Optional[str] = None",
            "    LD_LIBRARY_PATH: Optional[str] = None",
            "    VLLM_USE_TRITON_FLASH_ATTN: bool = False",
            "    VLLM_FLASH_ATTN_VERSION: Optional[int] = None",
            "    LOCAL_RANK: int = 0",
            "    CUDA_VISIBLE_DEVICES: Optional[str] = None",
            "    VLLM_ENGINE_ITERATION_TIMEOUT_S: int = 60",
            "    VLLM_API_KEY: Optional[str] = None",
            "    S3_ACCESS_KEY_ID: Optional[str] = None",
            "    S3_SECRET_ACCESS_KEY: Optional[str] = None",
            "    S3_ENDPOINT_URL: Optional[str] = None",
            "    VLLM_CACHE_ROOT: str = os.path.expanduser(\"~/.cache/vllm\")",
            "    VLLM_CONFIG_ROOT: str = os.path.expanduser(\"~/.config/vllm\")",
            "    VLLM_USAGE_STATS_SERVER: str = \"https://stats.vllm.ai\"",
            "    VLLM_NO_USAGE_STATS: bool = False",
            "    VLLM_DO_NOT_TRACK: bool = False",
            "    VLLM_USAGE_SOURCE: str = \"\"",
            "    VLLM_CONFIGURE_LOGGING: int = 1",
            "    VLLM_LOGGING_LEVEL: str = \"INFO\"",
            "    VLLM_LOGGING_PREFIX: str = \"\"",
            "    VLLM_LOGGING_CONFIG_PATH: Optional[str] = None",
            "    VLLM_LOGITS_PROCESSOR_THREADS: Optional[int] = None",
            "    VLLM_TRACE_FUNCTION: int = 0",
            "    VLLM_ATTENTION_BACKEND: Optional[str] = None",
            "    VLLM_USE_FLASHINFER_SAMPLER: Optional[bool] = None",
            "    VLLM_USE_FLASHINFER_REJECTION_SAMPLER: bool = False",
            "    VLLM_FLASHINFER_FORCE_TENSOR_CORES: bool = False",
            "    VLLM_PP_LAYER_PARTITION: Optional[str] = None",
            "    VLLM_CPU_KVCACHE_SPACE: int = 0",
            "    VLLM_CPU_OMP_THREADS_BIND: str = \"\"",
            "    VLLM_CPU_MOE_PREPACK: bool = True",
            "    VLLM_OPENVINO_DEVICE: str = \"CPU\"",
            "    VLLM_OPENVINO_KVCACHE_SPACE: int = 0",
            "    VLLM_OPENVINO_CPU_KV_CACHE_PRECISION: Optional[str] = None",
            "    VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS: bool = False",
            "    VLLM_XLA_CACHE_PATH: str = os.path.join(VLLM_CACHE_ROOT, \"xla_cache\")",
            "    VLLM_FUSED_MOE_CHUNK_SIZE: int = 64 * 1024",
            "    VLLM_USE_RAY_SPMD_WORKER: bool = False",
            "    VLLM_USE_RAY_COMPILED_DAG: bool = False",
            "    VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL: bool = True",
            "    VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM: bool = False",
            "    VLLM_WORKER_MULTIPROC_METHOD: str = \"fork\"",
            "    VLLM_ASSETS_CACHE: str = os.path.join(VLLM_CACHE_ROOT, \"assets\")",
            "    VLLM_IMAGE_FETCH_TIMEOUT: int = 5",
            "    VLLM_VIDEO_FETCH_TIMEOUT: int = 30",
            "    VLLM_AUDIO_FETCH_TIMEOUT: int = 10",
            "    VLLM_MM_INPUT_CACHE_SIZE: int = 256",
            "    VLLM_TARGET_DEVICE: str = \"cuda\"",
            "    MAX_JOBS: Optional[str] = None",
            "    NVCC_THREADS: Optional[str] = None",
            "    VLLM_USE_PRECOMPILED: bool = False",
            "    VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL: bool = False",
            "    VLLM_NO_DEPRECATION_WARNING: bool = False",
            "    VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = False",
            "    CMAKE_BUILD_TYPE: Optional[str] = None",
            "    VERBOSE: bool = False",
            "    VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = False",
            "    VLLM_RPC_TIMEOUT: int = 10000  # ms",
            "    VLLM_PLUGINS: Optional[list[str]] = None",
            "    VLLM_TORCH_PROFILER_DIR: Optional[str] = None",
            "    VLLM_USE_TRITON_AWQ: bool = False",
            "    VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False",
            "    VLLM_SKIP_P2P_CHECK: bool = False",
            "    VLLM_DISABLED_KERNELS: list[str] = []",
            "    VLLM_USE_V1: bool = False",
            "    VLLM_ROCM_FP8_PADDING: bool = True",
            "    VLLM_ENABLE_V1_MULTIPROCESSING: bool = True",
            "    VLLM_LOG_BATCHSIZE_INTERVAL: float = -1",
            "    VLLM_DISABLE_COMPILE_CACHE: bool = False",
            "    K_SCALE_CONSTANT: int = 200",
            "    V_SCALE_CONSTANT: int = 100",
            "    VLLM_SERVER_DEV_MODE: bool = False",
            "    VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128",
            "    VLLM_MLA_DISABLE: bool = False",
            "    VLLM_MLA_CUDA_MEM_ALIGN_KV_CACHE: bool = True",
            "    VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON: bool = False",
            "    VLLM_RAY_PER_WORKER_GPUS: float = 1.0",
            "    VLLM_RAY_BUNDLE_INDICES: str = \"\"",
            "    VLLM_CUDART_SO_PATH: Optional[str] = None",
            "    VLLM_USE_HPU_CONTIGUOUS_CACHE_FETCH: bool = True",
            "    VLLM_DP_RANK: int = 0",
            "    VLLM_DP_SIZE: int = 1",
            "    VLLM_DP_MASTER_IP: str = \"\"",
            "    VLLM_DP_MASTER_PORT: int = 0",
            "    VLLM_MARLIN_USE_ATOMIC_ADD: bool = False",
            "    VLLM_V0_USE_OUTLINES_CACHE: bool = False",
            "",
            "",
            "def get_default_cache_root():",
            "    return os.getenv(",
            "        \"XDG_CACHE_HOME\",",
            "        os.path.join(os.path.expanduser(\"~\"), \".cache\"),",
            "    )",
            "",
            "",
            "def get_default_config_root():",
            "    return os.getenv(",
            "        \"XDG_CONFIG_HOME\",",
            "        os.path.join(os.path.expanduser(\"~\"), \".config\"),",
            "    )",
            "",
            "",
            "def maybe_convert_int(value: Optional[str]) -> Optional[int]:",
            "    if value is None:",
            "        return None",
            "    return int(value)",
            "",
            "",
            "# The begin-* and end* here are used by the documentation generator",
            "# to extract the used env vars.",
            "",
            "# begin-env-vars-definition",
            "",
            "environment_variables: dict[str, Callable[[], Any]] = {",
            "",
            "    # ================== Installation Time Env Vars ==================",
            "",
            "    # Target device of vLLM, supporting [cuda (by default),",
            "    # rocm, neuron, cpu, openvino]",
            "    \"VLLM_TARGET_DEVICE\":",
            "    lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\"),",
            "",
            "    # Maximum number of compilation jobs to run in parallel.",
            "    # By default this is the number of CPUs",
            "    \"MAX_JOBS\":",
            "    lambda: os.getenv(\"MAX_JOBS\", None),",
            "",
            "    # Number of threads to use for nvcc",
            "    # By default this is 1.",
            "    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.",
            "    \"NVCC_THREADS\":",
            "    lambda: os.getenv(\"NVCC_THREADS\", None),",
            "",
            "    # If set, vllm will use precompiled binaries (*.so)",
            "    \"VLLM_USE_PRECOMPILED\":",
            "    lambda: bool(os.environ.get(\"VLLM_USE_PRECOMPILED\")) or bool(",
            "        os.environ.get(\"VLLM_PRECOMPILED_WHEEL_LOCATION\")),",
            "",
            "    # Whether to force using nightly wheel in python build.",
            "    # This is used for testing the nightly wheel in python build.",
            "    \"VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL\":",
            "    lambda: bool(int(os.getenv(\"VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL\", \"0\"))",
            "                 ),",
            "",
            "    # CMake build type",
            "    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"",
            "    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"",
            "    \"CMAKE_BUILD_TYPE\":",
            "    lambda: os.getenv(\"CMAKE_BUILD_TYPE\"),",
            "",
            "    # If set, vllm will print verbose logs during installation",
            "    \"VERBOSE\":",
            "    lambda: bool(int(os.getenv('VERBOSE', '0'))),",
            "",
            "    # Root directory for vLLM configuration files",
            "    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set",
            "    # Note that this not only affects how vllm finds its configuration files",
            "    # during runtime, but also affects how vllm installs its configuration",
            "    # files during **installation**.",
            "    \"VLLM_CONFIG_ROOT\":",
            "    lambda: os.path.expanduser(",
            "        os.getenv(",
            "            \"VLLM_CONFIG_ROOT\",",
            "            os.path.join(get_default_config_root(), \"vllm\"),",
            "        )),",
            "",
            "    # ================== Runtime Env Vars ==================",
            "",
            "    # Root directory for vLLM cache files",
            "    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set",
            "    \"VLLM_CACHE_ROOT\":",
            "    lambda: os.path.expanduser(",
            "        os.getenv(",
            "            \"VLLM_CACHE_ROOT\",",
            "            os.path.join(get_default_cache_root(), \"vllm\"),",
            "        )),",
            "",
            "    # used in distributed environment to determine the ip address",
            "    # of the current node, when the node has multiple network interfaces.",
            "    # If you are using multi-node inference, you should set this differently",
            "    # on each node.",
            "    'VLLM_HOST_IP':",
            "    lambda: os.getenv('VLLM_HOST_IP', \"\"),",
            "",
            "    # used in distributed environment to manually set the communication port",
            "    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the",
            "    # VLLM_PORT will be used as the first port, and the rest will be generated",
            "    # by incrementing the VLLM_PORT value.",
            "    # '0' is used to make mypy happy",
            "    'VLLM_PORT':",
            "    lambda: int(os.getenv('VLLM_PORT', '0'))",
            "    if 'VLLM_PORT' in os.environ else None,",
            "",
            "    # path used for ipc when the frontend api server is running in",
            "    # multi-processing mode to communicate with the backend engine process.",
            "    'VLLM_RPC_BASE_PATH':",
            "    lambda: os.getenv('VLLM_RPC_BASE_PATH', tempfile.gettempdir()),",
            "",
            "    # If true, will load models from ModelScope instead of Hugging Face Hub.",
            "    # note that the value is true or false, not numbers",
            "    \"VLLM_USE_MODELSCOPE\":",
            "    lambda: os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\",",
            "",
            "    # Interval in seconds to log a warning message when the ring buffer is full",
            "    \"VLLM_RINGBUFFER_WARNING_INTERVAL\":",
            "    lambda: int(os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")),",
            "",
            "    # path to cudatoolkit home directory, under which should be bin, include,",
            "    # and lib directories.",
            "    \"CUDA_HOME\":",
            "    lambda: os.environ.get(\"CUDA_HOME\", None),",
            "",
            "    # Path to the NCCL library file. It is needed because nccl>=2.19 brought",
            "    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234",
            "    \"VLLM_NCCL_SO_PATH\":",
            "    lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),",
            "",
            "    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl",
            "    # library file in the locations specified by `LD_LIBRARY_PATH`",
            "    \"LD_LIBRARY_PATH\":",
            "    lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),",
            "",
            "    # flag to control if vllm should use triton flash attention",
            "    \"VLLM_USE_TRITON_FLASH_ATTN\":",
            "    lambda: (os.environ.get(\"VLLM_USE_TRITON_FLASH_ATTN\", \"True\").lower() in",
            "             (\"true\", \"1\")),",
            "",
            "    # Force vllm to use a specific flash-attention version (2 or 3), only valid",
            "    # when using the flash-attention backend.",
            "    \"VLLM_FLASH_ATTN_VERSION\":",
            "    lambda: maybe_convert_int(os.environ.get(\"VLLM_FLASH_ATTN_VERSION\", None)),",
            "",
            "    # Internal flag to enable Dynamo fullgraph capture",
            "    \"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\":",
            "    lambda: bool(",
            "        os.environ.get(\"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\", \"1\") != \"0\"),",
            "",
            "    # local rank of the process in the distributed setting, used to determine",
            "    # the GPU device id",
            "    \"LOCAL_RANK\":",
            "    lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),",
            "",
            "    # used to control the visible devices in the distributed setting",
            "    \"CUDA_VISIBLE_DEVICES\":",
            "    lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),",
            "",
            "    # timeout for each iteration in the engine",
            "    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\":",
            "    lambda: int(os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")),",
            "",
            "    # API key for vLLM API server",
            "    \"VLLM_API_KEY\":",
            "    lambda: os.environ.get(\"VLLM_API_KEY\", None),",
            "",
            "    # S3 access information, used for tensorizer to load model from S3",
            "    \"S3_ACCESS_KEY_ID\":",
            "    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),",
            "    \"S3_SECRET_ACCESS_KEY\":",
            "    lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),",
            "    \"S3_ENDPOINT_URL\":",
            "    lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),",
            "",
            "    # Usage stats collection",
            "    \"VLLM_USAGE_STATS_SERVER\":",
            "    lambda: os.environ.get(\"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"),",
            "    \"VLLM_NO_USAGE_STATS\":",
            "    lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",",
            "    \"VLLM_DO_NOT_TRACK\":",
            "    lambda: (os.environ.get(\"VLLM_DO_NOT_TRACK\", None) or os.environ.get(",
            "        \"DO_NOT_TRACK\", None) or \"0\") == \"1\",",
            "    \"VLLM_USAGE_SOURCE\":",
            "    lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),",
            "",
            "    # Logging configuration",
            "    # If set to 0, vllm will not configure logging",
            "    # If set to 1, vllm will configure logging using the default configuration",
            "    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH",
            "    \"VLLM_CONFIGURE_LOGGING\":",
            "    lambda: int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\")),",
            "    \"VLLM_LOGGING_CONFIG_PATH\":",
            "    lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),",
            "",
            "    # this is used for configuring the default logging level",
            "    \"VLLM_LOGGING_LEVEL\":",
            "    lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\"),",
            "",
            "    # if set, VLLM_LOGGING_PREFIX will be prepended to all log messages",
            "    \"VLLM_LOGGING_PREFIX\":",
            "    lambda: os.getenv(\"VLLM_LOGGING_PREFIX\", \"\"),",
            "",
            "    # if set, vllm will call logits processors in a thread pool with this many",
            "    # threads. This is useful when using custom logits processors that either",
            "    # (a) launch additional CUDA kernels or (b) do significant CPU-bound work",
            "    # while not holding the python GIL, or both.",
            "    \"VLLM_LOGITS_PROCESSOR_THREADS\":",
            "    lambda: int(os.getenv(\"VLLM_LOGITS_PROCESSOR_THREADS\", \"0\"))",
            "    if \"VLLM_LOGITS_PROCESSOR_THREADS\" in os.environ else None,",
            "",
            "    # Trace function calls",
            "    # If set to 1, vllm will trace function calls",
            "    # Useful for debugging",
            "    \"VLLM_TRACE_FUNCTION\":",
            "    lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),",
            "",
            "    # Backend for attention computation",
            "    # Available options:",
            "    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention",
            "    # - \"FLASH_ATTN\": use FlashAttention",
            "    # - \"XFORMERS\": use XFormers",
            "    # - \"ROCM_FLASH\": use ROCmFlashAttention",
            "    # - \"FLASHINFER\": use flashinfer",
            "    # - \"FLASHMLA\": use FlashMLA",
            "    \"VLLM_ATTENTION_BACKEND\":",
            "    lambda: os.getenv(\"VLLM_ATTENTION_BACKEND\", None),",
            "",
            "    # If set, vllm will use flashinfer sampler",
            "    \"VLLM_USE_FLASHINFER_SAMPLER\":",
            "    lambda: bool(int(os.environ[\"VLLM_USE_FLASHINFER_SAMPLER\"]))",
            "    if \"VLLM_USE_FLASHINFER_SAMPLER\" in os.environ else None,",
            "",
            "    # If set, vllm will force flashinfer to use tensor cores;",
            "    # otherwise will use heuristic based on model architecture.",
            "    \"VLLM_FLASHINFER_FORCE_TENSOR_CORES\":",
            "    lambda: bool(int(os.getenv(\"VLLM_FLASHINFER_FORCE_TENSOR_CORES\", \"0\"))),",
            "",
            "    # Pipeline stage partition strategy",
            "    \"VLLM_PP_LAYER_PARTITION\":",
            "    lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),",
            "",
            "    # (CPU backend only) CPU key-value cache space.",
            "    # default is 4GB",
            "    \"VLLM_CPU_KVCACHE_SPACE\":",
            "    lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\")),",
            "",
            "    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",",
            "    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.",
            "    \"VLLM_CPU_OMP_THREADS_BIND\":",
            "    lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"all\"),",
            "",
            "    # (CPU backend only) whether to use prepack for MoE layer. This will be",
            "    # passed to ipex.llm.modules.GatedMLPMOE. On unsupported CPUs, you might",
            "    # need to set this to \"0\" (False).",
            "    \"VLLM_CPU_MOE_PREPACK\":",
            "    lambda: bool(int(os.getenv(\"VLLM_CPU_MOE_PREPACK\", \"1\"))),",
            "",
            "    # OpenVINO device selection",
            "    # default is CPU",
            "    \"VLLM_OPENVINO_DEVICE\":",
            "    lambda: os.getenv(\"VLLM_OPENVINO_DEVICE\", \"CPU\").upper(),",
            "",
            "    # OpenVINO key-value cache space",
            "    # default is 4GB",
            "    \"VLLM_OPENVINO_KVCACHE_SPACE\":",
            "    lambda: int(os.getenv(\"VLLM_OPENVINO_KVCACHE_SPACE\", \"0\")),",
            "",
            "    # OpenVINO KV cache precision",
            "    # default is bf16 if natively supported by platform, otherwise f16",
            "    # To enable KV cache compression, please, explicitly specify u8",
            "    \"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\":",
            "    lambda: os.getenv(\"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\", None),",
            "",
            "    # Enables weights compression during model export via HF Optimum",
            "    # default is False",
            "    \"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\":",
            "    lambda:",
            "    (os.environ.get(\"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\", \"0\").lower() in",
            "     (\"on\", \"true\", \"1\")),",
            "    # If the env var is set, then all workers will execute as separate",
            "    # processes from the engine, and we use the same mechanism to trigger",
            "    # execution on all workers.",
            "    # Run vLLM with VLLM_USE_RAY_SPMD_WORKER=1 to enable it.",
            "    \"VLLM_USE_RAY_SPMD_WORKER\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_SPMD_WORKER\", \"0\"))),",
            "",
            "    # If the env var is set, it uses the Ray's Compiled Graph",
            "    # (previously known as ADAG) API which optimizes the",
            "    # control plane overhead.",
            "    # Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.",
            "    \"VLLM_USE_RAY_COMPILED_DAG\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG\", \"0\"))),",
            "",
            "    # If the env var is set, it uses NCCL for communication in",
            "    # Ray's Compiled Graph. This flag is ignored if",
            "    # VLLM_USE_RAY_COMPILED_DAG is not set.",
            "    \"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\", \"1\"))",
            "                 ),",
            "",
            "    # If the env var is set, it enables GPU communication overlap",
            "    # (experimental feature) in Ray's Compiled Graph. This flag is ignored if",
            "    # VLLM_USE_RAY_COMPILED_DAG is not set.",
            "    \"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\", \"0\"))",
            "                 ),",
            "",
            "    # Use dedicated multiprocess context for workers.",
            "    # Both spawn and fork work",
            "    \"VLLM_WORKER_MULTIPROC_METHOD\":",
            "    lambda: os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\"),",
            "",
            "    # Path to the cache for storing downloaded assets",
            "    \"VLLM_ASSETS_CACHE\":",
            "    lambda: os.path.expanduser(",
            "        os.getenv(",
            "            \"VLLM_ASSETS_CACHE\",",
            "            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),",
            "        )),",
            "",
            "    # Timeout for fetching images when serving multimodal models",
            "    # Default is 5 seconds",
            "    \"VLLM_IMAGE_FETCH_TIMEOUT\":",
            "    lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),",
            "",
            "    # Timeout for fetching videos when serving multimodal models",
            "    # Default is 30 seconds",
            "    \"VLLM_VIDEO_FETCH_TIMEOUT\":",
            "    lambda: int(os.getenv(\"VLLM_VIDEO_FETCH_TIMEOUT\", \"30\")),",
            "",
            "    # Timeout for fetching audio when serving multimodal models",
            "    # Default is 10 seconds",
            "    \"VLLM_AUDIO_FETCH_TIMEOUT\":",
            "    lambda: int(os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"10\")),",
            "",
            "    # Cache size for multimodal feature/input cache for multimodal models",
            "    # in unit of number of multimodal data items (e.g. image, video, audio).",
            "    # Default is 256 multimodal data items.",
            "    \"VLLM_MM_INPUT_CACHE_SIZE\":",
            "    lambda: int(os.getenv(\"VLLM_MM_INPUT_CACHE_SIZE\", \"256\")),",
            "",
            "    # Path to the XLA persistent cache directory.",
            "    # Only used for XLA devices such as TPUs.",
            "    \"VLLM_XLA_CACHE_PATH\":",
            "    lambda: os.path.expanduser(",
            "        os.getenv(",
            "            \"VLLM_XLA_CACHE_PATH\",",
            "            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),",
            "        )),",
            "    \"VLLM_FUSED_MOE_CHUNK_SIZE\":",
            "    lambda: int(os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", \"32768\")),",
            "",
            "    # If set, vllm will skip the deprecation warnings.",
            "    \"VLLM_NO_DEPRECATION_WARNING\":",
            "    lambda: bool(int(os.getenv(\"VLLM_NO_DEPRECATION_WARNING\", \"0\"))),",
            "",
            "    # If set, the OpenAI API server will stay alive even after the underlying",
            "    # AsyncLLMEngine errors and stops serving requests",
            "    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\":",
            "    lambda: bool(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", 0)),",
            "",
            "    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows",
            "    # the user to specify a max sequence length greater than",
            "    # the max length derived from the model's config.json.",
            "    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.",
            "    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\":",
            "    lambda:",
            "    (os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower() in",
            "     (\"1\", \"true\")),",
            "",
            "    # If set, forces FP8 Marlin to be used for FP8 quantization regardless",
            "    # of the hardware support for FP8 compute.",
            "    \"VLLM_TEST_FORCE_FP8_MARLIN\":",
            "    lambda:",
            "    (os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower() in",
            "     (\"1\", \"true\")),",
            "    \"VLLM_TEST_FORCE_LOAD_FORMAT\":",
            "    lambda: os.getenv(\"VLLM_TEST_FORCE_LOAD_FORMAT\", \"dummy\"),",
            "",
            "    # Time in ms for the zmq client to wait for a response from the backend",
            "    # server for simple data operations",
            "    \"VLLM_RPC_TIMEOUT\":",
            "    lambda: int(os.getenv(\"VLLM_RPC_TIMEOUT\", \"10000\")),",
            "",
            "    # a list of plugin names to load, separated by commas.",
            "    # if this is not set, it means all plugins will be loaded",
            "    # if this is set to an empty string, no plugins will be loaded",
            "    \"VLLM_PLUGINS\":",
            "    lambda: None if \"VLLM_PLUGINS\" not in os.environ else os.environ[",
            "        \"VLLM_PLUGINS\"].split(\",\"),",
            "",
            "    # Enables torch profiler if set. Path to the directory where torch profiler",
            "    # traces are saved. Note that it must be an absolute path.",
            "    \"VLLM_TORCH_PROFILER_DIR\":",
            "    lambda: (None if os.getenv(\"VLLM_TORCH_PROFILER_DIR\", None) is None else os",
            "             .path.expanduser(os.getenv(\"VLLM_TORCH_PROFILER_DIR\", \".\"))),",
            "",
            "    # If set, vLLM will use Triton implementations of AWQ.",
            "    \"VLLM_USE_TRITON_AWQ\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),",
            "",
            "    # If set, allow loading or unloading lora adapters in runtime,",
            "    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\":",
            "    lambda:",
            "    (os.environ.get(\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\", \"0\").strip().lower() in",
            "     (\"1\", \"true\")),",
            "",
            "    # By default, vLLM will check the peer-to-peer capability itself,",
            "    # in case of broken drivers. See https://github.com/vllm-project/vllm/blob/a9b15c606fea67a072416ea0ea115261a2756058/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L101-L108 for details. # noqa",
            "    # If this env var is set to 1, vLLM will skip the peer-to-peer check,",
            "    # and trust the driver's peer-to-peer capability report.",
            "    \"VLLM_SKIP_P2P_CHECK\":",
            "    lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"0\") == \"1\",",
            "",
            "    # List of quantization kernels that should be disabled, used for testing",
            "    # and performance comparisons. Currently only affects MPLinearKernel",
            "    # selection",
            "    # (kernels: MacheteLinearKernel, MarlinLinearKernel, ExllamaLinearKernel)",
            "    \"VLLM_DISABLED_KERNELS\":",
            "    lambda: [] if \"VLLM_DISABLED_KERNELS\" not in os.environ else os.environ[",
            "        \"VLLM_DISABLED_KERNELS\"].split(\",\"),",
            "",
            "    # If set, use the V1 code path.",
            "    \"VLLM_USE_V1\":",
            "    lambda: bool(int(os.getenv(\"VLLM_USE_V1\", \"0\"))),",
            "",
            "    # Pad the fp8 weights to 256 bytes for ROCm",
            "    \"VLLM_ROCM_FP8_PADDING\":",
            "    lambda: bool(int(os.getenv(\"VLLM_ROCM_FP8_PADDING\", \"1\"))),",
            "    # Divisor for dynamic key scale factor calculation for FP8 KV Cache",
            "    \"K_SCALE_CONSTANT\":",
            "    lambda: int(os.getenv(\"K_SCALE_CONSTANT\", \"200\")),",
            "",
            "    # Divisor for dynamic value scale factor calculation for FP8 KV Cache",
            "    \"V_SCALE_CONSTANT\":",
            "    lambda: int(os.getenv(\"V_SCALE_CONSTANT\", \"100\")),",
            "    # If set, enable multiprocessing in LLM for the V1 code path.",
            "    \"VLLM_ENABLE_V1_MULTIPROCESSING\":",
            "    lambda: bool(int(os.getenv(\"VLLM_ENABLE_V1_MULTIPROCESSING\", \"1\"))),",
            "    \"VLLM_LOG_BATCHSIZE_INTERVAL\":",
            "    lambda: float(os.getenv(\"VLLM_LOG_BATCHSIZE_INTERVAL\", \"-1\")),",
            "    \"VLLM_DISABLE_COMPILE_CACHE\":",
            "    lambda: bool(int(os.getenv(\"VLLM_DISABLE_COMPILE_CACHE\", \"0\"))),",
            "",
            "    # If set, vllm will run in development mode, which will enable",
            "    # some additional endpoints for developing and debugging,",
            "    # e.g. `/reset_prefix_cache`",
            "    \"VLLM_SERVER_DEV_MODE\":",
            "    lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),",
            "",
            "    # Controls the maximum number of requests to handle in a",
            "    # single asyncio task when processing per-token outputs in the",
            "    # V1 AsyncLLM interface. It is applicable when handling a high",
            "    # concurrency of streaming requests.",
            "    # Setting this too high can result in a higher variance of",
            "    # inter-message latencies. Setting it too low can negatively impact",
            "    # TTFT and overall throughput.",
            "    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":",
            "    lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),",
            "",
            "    # If set, vLLM will disable the MLA attention optimizations.",
            "    \"VLLM_MLA_DISABLE\":",
            "    lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE\", \"0\"))),",
            "",
            "    # If set, vLLM will use the Triton implementation of moe_align_block_size,",
            "    # i.e. moe_align_block_size_triton in fused_moe.py.",
            "    \"VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON\":",
            "    lambda: bool(int(os.getenv(\"VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON\", \"0\"))",
            "                 ),",
            "",
            "    # Number of GPUs per worker in Ray, if it is set to be a fraction,",
            "    # it allows ray to schedule multiple actors on a single GPU,",
            "    # so that users can colocate other actors on the same GPUs as vLLM.",
            "    \"VLLM_RAY_PER_WORKER_GPUS\":",
            "    lambda: float(os.getenv(\"VLLM_RAY_PER_WORKER_GPUS\", \"1.0\")),",
            "",
            "    # Bundle indices for Ray, if it is set, it can control precisely",
            "    # which indices are used for the Ray bundle, for every worker.",
            "    # Format: comma-separated list of integers, e.g. \"0,1,2,3\"",
            "    \"VLLM_RAY_BUNDLE_INDICES\":",
            "    lambda: os.getenv(\"VLLM_RAY_BUNDLE_INDICES\", \"\"),",
            "",
            "    # When on a Nvidia GPU aligns single entries (within a page) so they are 256",
            "    # byte aligned for better performance, this increases the memory usage of",
            "    # the cache. Currently this only affects MLA that results in non-256",
            "    # byte aligned entries. This matches the alignment the CUDA runtime uses",
            "    # for all allocations. Currently this primarily affects MLA, for most other",
            "    # models the alignment is already naturally aligned to 256 bytes.",
            "    \"VLLM_CUDA_MEM_ALIGN_KV_CACHE\":",
            "    lambda: bool(int(os.getenv(\"VLLM_CUDA_MEM_ALIGN_KV_CACHE\", \"1\"))),",
            "",
            "    # In some system, find_loaded_library() may not work. So we allow users to",
            "    # specify the path through environment variable VLLM_CUDART_SO_PATH.",
            "    \"VLLM_CUDART_SO_PATH\":",
            "    lambda: os.getenv(\"VLLM_CUDART_SO_PATH\", None),",
            "",
            "    # Contiguous cache fetching to avoid using costly gather operation on",
            "    # Gaudi3. This is only applicable to HPU contiguous cache. If set to true,",
            "    # contiguous cache fetch will be used.",
            "    \"VLLM_USE_HPU_CONTIGUOUS_CACHE_FETCH\":",
            "    lambda: os.environ.get(\"VLLM_CONTIGUOUS_PA\", \"true\").lower() in",
            "    (\"1\", \"true\"),",
            "",
            "    # Rank of the process in the data parallel setting",
            "    \"VLLM_DP_RANK\":",
            "    lambda: int(os.getenv(\"VLLM_DP_RANK\", \"0\")),",
            "",
            "    # World size of the data parallel setting",
            "    \"VLLM_DP_SIZE\":",
            "    lambda: int(os.getenv(\"VLLM_DP_SIZE\", \"1\")),",
            "",
            "    # IP address of the master node in the data parallel setting",
            "    \"VLLM_DP_MASTER_IP\":",
            "    lambda: os.getenv(\"VLLM_DP_MASTER_IP\", \"127.0.0.1\"),",
            "",
            "    # Port of the master node in the data parallel setting",
            "    \"VLLM_DP_MASTER_PORT\":",
            "    lambda: int(os.getenv(\"VLLM_DP_MASTER_PORT\", \"0\")),",
            "",
            "    # Whether to use S3 path for model loading in CI via RunAI Streamer",
            "    \"VLLM_CI_USE_S3\":",
            "    lambda: os.environ.get(\"VLLM_CI_USE_S3\", \"0\") == \"1\",",
            "",
            "    # Whether to use atomicAdd reduce in gptq/awq marlin kernel.",
            "    \"VLLM_MARLIN_USE_ATOMIC_ADD\":",
            "    lambda: os.environ.get(\"VLLM_MARLIN_USE_ATOMIC_ADD\", \"0\") == \"1\",",
            "",
            "    # Whether to turn on the outlines cache for V0",
            "    # This cache is unbounded and on disk, so it's not safe to use in",
            "    # an environment with potentially malicious users.",
            "    \"VLLM_V0_USE_OUTLINES_CACHE\":",
            "    lambda: os.environ.get(\"VLLM_V0_USE_OUTLINES_CACHE\", \"0\") == \"1\",",
            "}",
            "",
            "# end-env-vars-definition",
            "",
            "",
            "def __getattr__(name: str):",
            "    # lazy evaluation of environment variables",
            "    if name in environment_variables:",
            "        return environment_variables[name]()",
            "    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")",
            "",
            "",
            "def __dir__():",
            "    return list(environment_variables.keys())"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "vllm/model_executor/guided_decoding/outlines_logits_processors.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " import numpy as np"
            },
            "1": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " import torch"
            },
            "2": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from outlines import grammars"
            },
            "3": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from outlines.caching import cache"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+from outlines.caching import cache, disable_cache"
            },
            "5": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from outlines.fsm.guide import (CFGGuide, CFGState, Generate, Guide,"
            },
            "6": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": "                                 RegexGuide, Write)"
            },
            "7": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from outlines.fsm.parsing import PartialLark"
            },
            "8": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from outlines_core.fsm.json_schema import build_regex_from_schema"
            },
            "9": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from pydantic import BaseModel"
            },
            "10": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from transformers import PreTrainedTokenizerBase"
            },
            "11": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+import vllm.envs as envs"
            },
            "13": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from vllm.logger import init_logger"
            },
            "14": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " from vllm.model_executor.guided_decoding.reasoner import Reasoner"
            },
            "15": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " from vllm.platforms import current_platform"
            },
            "16": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " logger = init_logger(__name__)"
            },
            "18": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " "
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+if envs.VLLM_V0_USE_OUTLINES_CACHE:"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+    logger.warning(\"Enabling outlines cache. This is an unbounded on-disk \""
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+                   \"cache. It may consume a lot of disk space and should \""
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+                   \"not be used with untrusted clients.\")"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+else:"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+    disable_cache()"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 49,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " class BaseLogitsProcessor:"
            },
            "28": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 51,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# SPDX-License-Identifier: Apache-2.0",
            "",
            "# Copyright 2024- the Outlines developers",
            "# This file is adapted from",
            "# https://github.com/outlines-dev/outlines/blob/main/outlines/serve/vllm.py",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import copy",
            "import json",
            "from collections import defaultdict",
            "from functools import lru_cache",
            "from typing import Callable, DefaultDict, Dict, List, Optional, Union",
            "",
            "import numpy as np",
            "import torch",
            "from outlines import grammars",
            "from outlines.caching import cache",
            "from outlines.fsm.guide import (CFGGuide, CFGState, Generate, Guide,",
            "                                RegexGuide, Write)",
            "from outlines.fsm.parsing import PartialLark",
            "from outlines_core.fsm.json_schema import build_regex_from_schema",
            "from pydantic import BaseModel",
            "from transformers import PreTrainedTokenizerBase",
            "",
            "from vllm.logger import init_logger",
            "from vllm.model_executor.guided_decoding.reasoner import Reasoner",
            "from vllm.platforms import current_platform",
            "",
            "logger = init_logger(__name__)",
            "",
            "",
            "class BaseLogitsProcessor:",
            "",
            "    def __init__(self, guide: Guide, reasoner: Optional[Reasoner]):",
            "        self._guide: Guide = guide",
            "        self._reasoner: Optional[Reasoner] = reasoner",
            "        # CFGState is used for the FSM state for CFGGuide",
            "        self._fsm_state: DefaultDict[int, Union[int,",
            "                                                CFGState]] = defaultdict(int)",
            "",
            "    def __call__(self, input_ids: List[int],",
            "                 scores: torch.Tensor) -> torch.Tensor:",
            "        \"\"\"Use the FSM to bias the logits before sampling the next token.\"\"\"",
            "",
            "        # Skip the structured logits processing if reasoning is not finished.",
            "        # reasoner is not None only when `--enable-reasoning` is set.",
            "        if self._reasoner is not None:",
            "            if not self._reasoner.is_reasoning_end(input_ids):",
            "                return scores",
            "            else:",
            "                # Remove the reasoning tokens from the input_ids",
            "                # We need this because our implementation relies on the",
            "                # hash of the input_ids to store the FSM state.",
            "                input_ids = self._reasoner.extract_content(input_ids)",
            "",
            "        seq_id = hash(tuple(input_ids))",
            "",
            "        if len(input_ids) > 0:",
            "            last_token = input_ids[-1]",
            "            last_seq_id = hash(tuple(input_ids[:-1]))",
            "            self._fsm_state[seq_id] = self._guide.get_next_state(",
            "                state=self._fsm_state[last_seq_id], token_id=last_token)",
            "        else:",
            "            # Note: this is a hack.",
            "            # Lark pickling does not work properly (silent failure),",
            "            # which breaks the RPC (which uses python pickleing).",
            "            # We need to find a better solution.",
            "            # On the first time this is called, we simply re-create",
            "            # the Lark object.",
            "            if isinstance(self._guide, CFGGuide):",
            "                self._guide.parser = PartialLark(",
            "                    self._guide.cfg_string,",
            "                    parser=\"lalr\",",
            "                    import_paths=[grammars.GRAMMAR_PATH],",
            "                )",
            "                self._fsm_state[seq_id] = CFGState(",
            "                    parser_state=self._guide.parser.parse(\"\"), prev_token=None)",
            "",
            "        instruction = self._guide.get_next_instruction(",
            "            state=self._fsm_state[seq_id])",
            "",
            "        if type(instruction) == Generate:  # noqa: E721",
            "            allowed_tokens = instruction.tokens",
            "        elif type(instruction) == Write:  # noqa: E721",
            "            # TODO: support fast forward tokens",
            "            allowed_tokens = [instruction.tokens[0]]",
            "        else:",
            "            raise TypeError(",
            "                f\"Unsupported instruction type {type(instruction)}\")",
            "",
            "        mask = torch.full((scores.shape[-1], ),",
            "                          -torch.inf,",
            "                          device=scores.device)",
            "        # The tokenizer may support more token ids than the model can generate,",
            "        # eg. Llama 3.2 Vision models have an `<|image|>` token with id 128256",
            "        # but scores.shape == torch.Size([128256])",
            "        # Using NumPy is faster for filtering token ids",
            "        allowed_tokens = np.array(allowed_tokens, dtype=np.int64)",
            "        allowed_tokens = torch.tensor(allowed_tokens, device=scores.device)",
            "        allowed_tokens = allowed_tokens.masked_select(",
            "            allowed_tokens < scores.shape[-1])",
            "        mask.index_fill_(0, allowed_tokens, 0)",
            "        if current_platform.is_hpu():",
            "            # Workaround for HPU bug where add_() raise RuntimeError:",
            "            # synNodeCreateWithId failed for node: strided_insert",
            "            # with synStatus 1 [Invalid argument], hopefully it will",
            "            # be fixed in the future releases of the HPU runtime.",
            "            scores = scores.add(mask)",
            "        else:",
            "            scores.add_(mask)",
            "        return scores",
            "",
            "",
            "class RegexLogitsProcessor(BaseLogitsProcessor):",
            "",
            "    @classmethod",
            "    @cache()",
            "    def _get_guide(cls, regex_string: str,",
            "                   tokenizer: PreTrainedTokenizerBase) -> Guide:",
            "        tokenizer = _adapt_tokenizer(tokenizer)",
            "        return RegexGuide.from_regex(regex_string, tokenizer)",
            "",
            "    def __init__(",
            "        self,",
            "        regex_string: str,",
            "        tokenizer: PreTrainedTokenizerBase,",
            "        reasoner: Optional[Reasoner],",
            "    ):",
            "        \"\"\"Compile the FSM that drives the regex-structured generation.",
            "",
            "        Parameters",
            "        ----------",
            "        regex_string",
            "            A string that represents a regular expression",
            "        tokenizer",
            "            The model's tokenizer",
            "",
            "        \"\"\"",
            "        super().__init__(",
            "            RegexLogitsProcessor._get_guide(regex_string, tokenizer), reasoner)",
            "",
            "",
            "class JSONLogitsProcessor(RegexLogitsProcessor):",
            "",
            "    def __init__(self, schema: Union[str, Dict, BaseModel],",
            "                 tokenizer: PreTrainedTokenizerBase,",
            "                 whitespace_pattern: Union[str, None],",
            "                 reasoner: Optional[Reasoner]):",
            "        \"\"\"Compile the FSM that drives the JSON-guided generation.",
            "",
            "        Parameters",
            "        ----------",
            "        schema",
            "            A JSON schema that encodes the structure we want the model to",
            "            generate",
            "        tokenizer",
            "            The model's tokenizer",
            "        whitespace_pattern",
            "            Pattern to use for JSON syntactic whitespace (doesn't impact",
            "            string literals)",
            "            Example: allow only a single space or newline with",
            "            `whitespace_pattern=r\"[\\n ]?\"`",
            "        \"\"\"",
            "        if isinstance(schema, type(BaseModel)):",
            "            schema_str = json.dumps(schema.model_json_schema())",
            "        elif isinstance(schema, Dict):",
            "            schema_str = json.dumps(schema)",
            "        elif isinstance(schema, str):",
            "            schema_str = schema",
            "        else:",
            "            raise ValueError(",
            "                f\"Cannot parse schema {schema}. The schema must be either \"",
            "                f\"a Pydantic object, a dictionary or a string that contains \"",
            "                f\"the JSON Schema specification\")",
            "        regex_string = build_regex_from_schema(schema_str, whitespace_pattern)",
            "        super().__init__(regex_string, tokenizer, reasoner)",
            "",
            "",
            "class CFGLogitsProcessor(BaseLogitsProcessor):",
            "",
            "    @classmethod",
            "    @cache()",
            "    def _get_guide(cls, cfg: str, tokenizer: PreTrainedTokenizerBase) -> Guide:",
            "        tokenizer = _adapt_tokenizer(tokenizer)",
            "        return CFGGuide(cfg, tokenizer)",
            "",
            "    def __init__(self, cfg: str, tokenizer: PreTrainedTokenizerBase,",
            "                 reasoner: Optional[Reasoner]):",
            "        \"\"\"Compile the FSM that drives the context free grammar generation.",
            "",
            "        Parameters",
            "        ----------",
            "        cfg",
            "            A string that represents a context-free grammar",
            "        tokenizer",
            "            The model's tokenizer",
            "",
            "        \"\"\"",
            "        super().__init__(CFGLogitsProcessor._get_guide(cfg, tokenizer),",
            "                         reasoner)",
            "        self._guide = self._guide.copy()",
            "",
            "",
            "@lru_cache(maxsize=32)",
            "def _adapt_tokenizer(tokenizer: PreTrainedTokenizerBase):",
            "    \"\"\"Adapt vLLM's tokenizer to use to compile the FSM.",
            "",
            "    The API of Outlines tokenizers is slightly different to that of",
            "    `transformers`. The decoder of outlines, returns a list whereas",
            "    the decode of vLLM returns an str. To sync the vLLM decoder with",
            "    outlines internal api, the decoder should be adapted. In addition",
            "    we need to handle the missing spaces to Llama's tokenizer to be",
            "    able to compile FSMs for this model.",
            "",
            "    \"\"\"",
            "    if getattr(tokenizer, \"_outlines_adapted\", False):",
            "        return tokenizer",
            "",
            "    tokenizer = copy.deepcopy(tokenizer)",
            "",
            "    tokenizer.vocabulary = tokenizer.get_vocab()",
            "    tokenizer.special_tokens = set(tokenizer.all_special_tokens)",
            "",
            "    def convert_token_to_string(token: str) -> str:",
            "        from transformers.file_utils import SPIECE_UNDERLINE",
            "",
            "        string = tokenizer.convert_tokens_to_string([token])",
            "",
            "        # A hack to handle missing spaces to HF's Llama tokenizers",
            "        if (type(token) is str and token.startswith(SPIECE_UNDERLINE)",
            "                or token == \"<0x20>\"):",
            "            return \" \" + string",
            "",
            "        return string",
            "",
            "    def change_decoder(",
            "        decoder: Callable[[List[int]],",
            "                          str]) -> Callable[[List[int]], List[str]]:",
            "        \"\"\"Sync vLLM's decoder with the outlines by returning list.\"\"\"",
            "",
            "        def new_decoder(inp_tokens: List[int]) -> List[str]:",
            "            if (isinstance(inp_tokens, list) and len(inp_tokens) == 1",
            "                    and isinstance(inp_tokens[0], list)):",
            "                inp_tokens = inp_tokens[0]",
            "            return [decoder(inp_tokens)]",
            "",
            "        return new_decoder",
            "",
            "    tokenizer.convert_token_to_string = convert_token_to_string",
            "    tokenizer.decode = change_decoder(tokenizer.decode)",
            "    setattr(tokenizer, \"_outlines_adapted\", True)  # noqa: B010",
            "",
            "    return tokenizer"
        ],
        "afterPatchFile": [
            "# SPDX-License-Identifier: Apache-2.0",
            "",
            "# Copyright 2024- the Outlines developers",
            "# This file is adapted from",
            "# https://github.com/outlines-dev/outlines/blob/main/outlines/serve/vllm.py",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import copy",
            "import json",
            "from collections import defaultdict",
            "from functools import lru_cache",
            "from typing import Callable, DefaultDict, Dict, List, Optional, Union",
            "",
            "import numpy as np",
            "import torch",
            "from outlines import grammars",
            "from outlines.caching import cache, disable_cache",
            "from outlines.fsm.guide import (CFGGuide, CFGState, Generate, Guide,",
            "                                RegexGuide, Write)",
            "from outlines.fsm.parsing import PartialLark",
            "from outlines_core.fsm.json_schema import build_regex_from_schema",
            "from pydantic import BaseModel",
            "from transformers import PreTrainedTokenizerBase",
            "",
            "import vllm.envs as envs",
            "from vllm.logger import init_logger",
            "from vllm.model_executor.guided_decoding.reasoner import Reasoner",
            "from vllm.platforms import current_platform",
            "",
            "logger = init_logger(__name__)",
            "",
            "if envs.VLLM_V0_USE_OUTLINES_CACHE:",
            "    logger.warning(\"Enabling outlines cache. This is an unbounded on-disk \"",
            "                   \"cache. It may consume a lot of disk space and should \"",
            "                   \"not be used with untrusted clients.\")",
            "else:",
            "    disable_cache()",
            "",
            "",
            "class BaseLogitsProcessor:",
            "",
            "    def __init__(self, guide: Guide, reasoner: Optional[Reasoner]):",
            "        self._guide: Guide = guide",
            "        self._reasoner: Optional[Reasoner] = reasoner",
            "        # CFGState is used for the FSM state for CFGGuide",
            "        self._fsm_state: DefaultDict[int, Union[int,",
            "                                                CFGState]] = defaultdict(int)",
            "",
            "    def __call__(self, input_ids: List[int],",
            "                 scores: torch.Tensor) -> torch.Tensor:",
            "        \"\"\"Use the FSM to bias the logits before sampling the next token.\"\"\"",
            "",
            "        # Skip the structured logits processing if reasoning is not finished.",
            "        # reasoner is not None only when `--enable-reasoning` is set.",
            "        if self._reasoner is not None:",
            "            if not self._reasoner.is_reasoning_end(input_ids):",
            "                return scores",
            "            else:",
            "                # Remove the reasoning tokens from the input_ids",
            "                # We need this because our implementation relies on the",
            "                # hash of the input_ids to store the FSM state.",
            "                input_ids = self._reasoner.extract_content(input_ids)",
            "",
            "        seq_id = hash(tuple(input_ids))",
            "",
            "        if len(input_ids) > 0:",
            "            last_token = input_ids[-1]",
            "            last_seq_id = hash(tuple(input_ids[:-1]))",
            "            self._fsm_state[seq_id] = self._guide.get_next_state(",
            "                state=self._fsm_state[last_seq_id], token_id=last_token)",
            "        else:",
            "            # Note: this is a hack.",
            "            # Lark pickling does not work properly (silent failure),",
            "            # which breaks the RPC (which uses python pickleing).",
            "            # We need to find a better solution.",
            "            # On the first time this is called, we simply re-create",
            "            # the Lark object.",
            "            if isinstance(self._guide, CFGGuide):",
            "                self._guide.parser = PartialLark(",
            "                    self._guide.cfg_string,",
            "                    parser=\"lalr\",",
            "                    import_paths=[grammars.GRAMMAR_PATH],",
            "                )",
            "                self._fsm_state[seq_id] = CFGState(",
            "                    parser_state=self._guide.parser.parse(\"\"), prev_token=None)",
            "",
            "        instruction = self._guide.get_next_instruction(",
            "            state=self._fsm_state[seq_id])",
            "",
            "        if type(instruction) == Generate:  # noqa: E721",
            "            allowed_tokens = instruction.tokens",
            "        elif type(instruction) == Write:  # noqa: E721",
            "            # TODO: support fast forward tokens",
            "            allowed_tokens = [instruction.tokens[0]]",
            "        else:",
            "            raise TypeError(",
            "                f\"Unsupported instruction type {type(instruction)}\")",
            "",
            "        mask = torch.full((scores.shape[-1], ),",
            "                          -torch.inf,",
            "                          device=scores.device)",
            "        # The tokenizer may support more token ids than the model can generate,",
            "        # eg. Llama 3.2 Vision models have an `<|image|>` token with id 128256",
            "        # but scores.shape == torch.Size([128256])",
            "        # Using NumPy is faster for filtering token ids",
            "        allowed_tokens = np.array(allowed_tokens, dtype=np.int64)",
            "        allowed_tokens = torch.tensor(allowed_tokens, device=scores.device)",
            "        allowed_tokens = allowed_tokens.masked_select(",
            "            allowed_tokens < scores.shape[-1])",
            "        mask.index_fill_(0, allowed_tokens, 0)",
            "        if current_platform.is_hpu():",
            "            # Workaround for HPU bug where add_() raise RuntimeError:",
            "            # synNodeCreateWithId failed for node: strided_insert",
            "            # with synStatus 1 [Invalid argument], hopefully it will",
            "            # be fixed in the future releases of the HPU runtime.",
            "            scores = scores.add(mask)",
            "        else:",
            "            scores.add_(mask)",
            "        return scores",
            "",
            "",
            "class RegexLogitsProcessor(BaseLogitsProcessor):",
            "",
            "    @classmethod",
            "    @cache()",
            "    def _get_guide(cls, regex_string: str,",
            "                   tokenizer: PreTrainedTokenizerBase) -> Guide:",
            "        tokenizer = _adapt_tokenizer(tokenizer)",
            "        return RegexGuide.from_regex(regex_string, tokenizer)",
            "",
            "    def __init__(",
            "        self,",
            "        regex_string: str,",
            "        tokenizer: PreTrainedTokenizerBase,",
            "        reasoner: Optional[Reasoner],",
            "    ):",
            "        \"\"\"Compile the FSM that drives the regex-structured generation.",
            "",
            "        Parameters",
            "        ----------",
            "        regex_string",
            "            A string that represents a regular expression",
            "        tokenizer",
            "            The model's tokenizer",
            "",
            "        \"\"\"",
            "        super().__init__(",
            "            RegexLogitsProcessor._get_guide(regex_string, tokenizer), reasoner)",
            "",
            "",
            "class JSONLogitsProcessor(RegexLogitsProcessor):",
            "",
            "    def __init__(self, schema: Union[str, Dict, BaseModel],",
            "                 tokenizer: PreTrainedTokenizerBase,",
            "                 whitespace_pattern: Union[str, None],",
            "                 reasoner: Optional[Reasoner]):",
            "        \"\"\"Compile the FSM that drives the JSON-guided generation.",
            "",
            "        Parameters",
            "        ----------",
            "        schema",
            "            A JSON schema that encodes the structure we want the model to",
            "            generate",
            "        tokenizer",
            "            The model's tokenizer",
            "        whitespace_pattern",
            "            Pattern to use for JSON syntactic whitespace (doesn't impact",
            "            string literals)",
            "            Example: allow only a single space or newline with",
            "            `whitespace_pattern=r\"[\\n ]?\"`",
            "        \"\"\"",
            "        if isinstance(schema, type(BaseModel)):",
            "            schema_str = json.dumps(schema.model_json_schema())",
            "        elif isinstance(schema, Dict):",
            "            schema_str = json.dumps(schema)",
            "        elif isinstance(schema, str):",
            "            schema_str = schema",
            "        else:",
            "            raise ValueError(",
            "                f\"Cannot parse schema {schema}. The schema must be either \"",
            "                f\"a Pydantic object, a dictionary or a string that contains \"",
            "                f\"the JSON Schema specification\")",
            "        regex_string = build_regex_from_schema(schema_str, whitespace_pattern)",
            "        super().__init__(regex_string, tokenizer, reasoner)",
            "",
            "",
            "class CFGLogitsProcessor(BaseLogitsProcessor):",
            "",
            "    @classmethod",
            "    @cache()",
            "    def _get_guide(cls, cfg: str, tokenizer: PreTrainedTokenizerBase) -> Guide:",
            "        tokenizer = _adapt_tokenizer(tokenizer)",
            "        return CFGGuide(cfg, tokenizer)",
            "",
            "    def __init__(self, cfg: str, tokenizer: PreTrainedTokenizerBase,",
            "                 reasoner: Optional[Reasoner]):",
            "        \"\"\"Compile the FSM that drives the context free grammar generation.",
            "",
            "        Parameters",
            "        ----------",
            "        cfg",
            "            A string that represents a context-free grammar",
            "        tokenizer",
            "            The model's tokenizer",
            "",
            "        \"\"\"",
            "        super().__init__(CFGLogitsProcessor._get_guide(cfg, tokenizer),",
            "                         reasoner)",
            "        self._guide = self._guide.copy()",
            "",
            "",
            "@lru_cache(maxsize=32)",
            "def _adapt_tokenizer(tokenizer: PreTrainedTokenizerBase):",
            "    \"\"\"Adapt vLLM's tokenizer to use to compile the FSM.",
            "",
            "    The API of Outlines tokenizers is slightly different to that of",
            "    `transformers`. The decoder of outlines, returns a list whereas",
            "    the decode of vLLM returns an str. To sync the vLLM decoder with",
            "    outlines internal api, the decoder should be adapted. In addition",
            "    we need to handle the missing spaces to Llama's tokenizer to be",
            "    able to compile FSMs for this model.",
            "",
            "    \"\"\"",
            "    if getattr(tokenizer, \"_outlines_adapted\", False):",
            "        return tokenizer",
            "",
            "    tokenizer = copy.deepcopy(tokenizer)",
            "",
            "    tokenizer.vocabulary = tokenizer.get_vocab()",
            "    tokenizer.special_tokens = set(tokenizer.all_special_tokens)",
            "",
            "    def convert_token_to_string(token: str) -> str:",
            "        from transformers.file_utils import SPIECE_UNDERLINE",
            "",
            "        string = tokenizer.convert_tokens_to_string([token])",
            "",
            "        # A hack to handle missing spaces to HF's Llama tokenizers",
            "        if (type(token) is str and token.startswith(SPIECE_UNDERLINE)",
            "                or token == \"<0x20>\"):",
            "            return \" \" + string",
            "",
            "        return string",
            "",
            "    def change_decoder(",
            "        decoder: Callable[[List[int]],",
            "                          str]) -> Callable[[List[int]], List[str]]:",
            "        \"\"\"Sync vLLM's decoder with the outlines by returning list.\"\"\"",
            "",
            "        def new_decoder(inp_tokens: List[int]) -> List[str]:",
            "            if (isinstance(inp_tokens, list) and len(inp_tokens) == 1",
            "                    and isinstance(inp_tokens[0], list)):",
            "                inp_tokens = inp_tokens[0]",
            "            return [decoder(inp_tokens)]",
            "",
            "        return new_decoder",
            "",
            "    tokenizer.convert_token_to_string = convert_token_to_string",
            "    tokenizer.decode = change_decoder(tokenizer.decode)",
            "    setattr(tokenizer, \"_outlines_adapted\", True)  # noqa: B010",
            "",
            "    return tokenizer"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "27": []
        },
        "addLocation": []
    }
}