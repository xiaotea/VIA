{
    "crmsh/bootstrap.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from . import clidisplay"
            },
            "1": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from . import term"
            },
            "2": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from . import lock"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+from . import userdir"
            },
            "4": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " LOG_FILE = \"/var/log/crmsh/ha-cluster-bootstrap.log\""
            },
            "7": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 43,
                "PatchRowcode": " SYSCONFIG_FW_CLUSTER = \"/etc/sysconfig/SuSEfirewall2.d/services/cluster\""
            },
            "8": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " PCMK_REMOTE_AUTH = \"/etc/pacemaker/authkey\""
            },
            "9": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " COROSYNC_CONF_ORIG = tmpfiles.create()[1]"
            },
            "10": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-RSA_PRIVATE_KEY = \"/root/.ssh/id_rsa\""
            },
            "11": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-RSA_PUBLIC_KEY = \"/root/.ssh/id_rsa.pub\""
            },
            "12": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-AUTHORIZED_KEYS_FILE = \"/root/.ssh/authorized_keys\""
            },
            "13": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 46,
                "PatchRowcode": " SERVICES_STOP_LIST = [\"corosync-qdevice.service\", \"corosync.service\", \"hawk.service\"]"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+USER_LIST = [\"root\", \"hacluster\"]"
            },
            "15": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 49,
                "PatchRowcode": " INIT_STAGES = (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\", \"corosync\", \"storage\", \"sbd\", \"cluster\", \"vgfs\", \"admin\", \"qdevice\")"
            },
            "17": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "         self.args = None"
            },
            "19": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "         self.ui_context = None"
            },
            "20": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "         self.interfaces_inst = None"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+        self.with_other_user = True"
            },
            "22": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "         self.default_nic_list = []"
            },
            "23": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "         self.default_ip_list = []"
            },
            "24": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "         self.local_ip_list = []"
            },
            "25": {
                "beforePatchRowNumber": 1134,
                "afterPatchRowNumber": 1134,
                "PatchRowcode": "     Configure passwordless SSH."
            },
            "26": {
                "beforePatchRowNumber": 1135,
                "afterPatchRowNumber": 1135,
                "PatchRowcode": "     \"\"\""
            },
            "27": {
                "beforePatchRowNumber": 1136,
                "afterPatchRowNumber": 1136,
                "PatchRowcode": "     utils.start_service(\"sshd.service\", enable=True)"
            },
            "28": {
                "beforePatchRowNumber": 1137,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    configure_local_ssh_key()"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1137,
                "PatchRowcode": "+    for user in USER_LIST:"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1138,
                "PatchRowcode": "+        configure_local_ssh_key(user)"
            },
            "31": {
                "beforePatchRowNumber": 1138,
                "afterPatchRowNumber": 1139,
                "PatchRowcode": " "
            },
            "32": {
                "beforePatchRowNumber": 1139,
                "afterPatchRowNumber": 1140,
                "PatchRowcode": " "
            },
            "33": {
                "beforePatchRowNumber": 1140,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def configure_local_ssh_key():"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1141,
                "PatchRowcode": "+def key_files(user):"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1142,
                "PatchRowcode": "+    \"\"\""
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1143,
                "PatchRowcode": "+    Find home directory for user and return key files with abspath"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1144,
                "PatchRowcode": "+    \"\"\""
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1145,
                "PatchRowcode": "+    keyfile_dict = {}"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1146,
                "PatchRowcode": "+    home_dir = userdir.gethomedir(user)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1147,
                "PatchRowcode": "+    keyfile_dict['private'] = \"{}/.ssh/id_rsa\".format(home_dir)"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1148,
                "PatchRowcode": "+    keyfile_dict['public'] = \"{}/.ssh/id_rsa.pub\".format(home_dir)"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1149,
                "PatchRowcode": "+    keyfile_dict['authorized'] = \"{}/.ssh/authorized_keys\".format(home_dir)"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1150,
                "PatchRowcode": "+    return keyfile_dict"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1151,
                "PatchRowcode": "+"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1152,
                "PatchRowcode": "+"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1153,
                "PatchRowcode": "+def is_nologin(user):"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1154,
                "PatchRowcode": "+    \"\"\""
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1155,
                "PatchRowcode": "+    Check if user's shell is /sbin/nologin"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1156,
                "PatchRowcode": "+    \"\"\""
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1157,
                "PatchRowcode": "+    with open(\"/etc/passwd\") as f:"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1158,
                "PatchRowcode": "+        return re.search(\"{}:.*:/sbin/nologin\".format(user), f.read())"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1159,
                "PatchRowcode": "+"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1160,
                "PatchRowcode": "+"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1161,
                "PatchRowcode": "+def change_user_shell(user):"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1162,
                "PatchRowcode": "+    \"\"\""
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1163,
                "PatchRowcode": "+    To change user's login shell"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1164,
                "PatchRowcode": "+    \"\"\""
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1165,
                "PatchRowcode": "+    if user != \"root\" and is_nologin(user):"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1166,
                "PatchRowcode": "+        if not _context.yes_to_all:"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1167,
                "PatchRowcode": "+            status(\"\"\""
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1168,
                "PatchRowcode": "+User {} will be changed the login shell as /bin/bash, and"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1169,
                "PatchRowcode": "+be setted up authorized ssh access among cluster nodes\"\"\".format(user))"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1170,
                "PatchRowcode": "+            if not confirm(\"Continue?\"):"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1171,
                "PatchRowcode": "+                _context.with_other_user = False"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1172,
                "PatchRowcode": "+                return"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1173,
                "PatchRowcode": "+        invoke(\"usermod -s /bin/bash {}\".format(user))"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1174,
                "PatchRowcode": "+"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1175,
                "PatchRowcode": "+"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1176,
                "PatchRowcode": "+def configure_local_ssh_key(user=\"root\"):"
            },
            "70": {
                "beforePatchRowNumber": 1141,
                "afterPatchRowNumber": 1177,
                "PatchRowcode": "     \"\"\""
            },
            "71": {
                "beforePatchRowNumber": 1142,
                "afterPatchRowNumber": 1178,
                "PatchRowcode": "     Configure ssh rsa key locally"
            },
            "72": {
                "beforePatchRowNumber": 1143,
                "afterPatchRowNumber": 1179,
                "PatchRowcode": " "
            },
            "73": {
                "beforePatchRowNumber": 1144,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    If /root/.ssh/id_rsa not exist, generate a new one"
            },
            "74": {
                "beforePatchRowNumber": 1145,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Add /root/.ssh/id_rsa.pub to /root/.ssh/authorized_keys anyway, make sure itself authorized"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1180,
                "PatchRowcode": "+    If <home_dir>/.ssh/id_rsa not exist, generate a new one"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1181,
                "PatchRowcode": "+    Add <home_dir>/.ssh/id_rsa.pub to <home_dir>/.ssh/authorized_keys anyway, make sure itself authorized"
            },
            "77": {
                "beforePatchRowNumber": 1146,
                "afterPatchRowNumber": 1182,
                "PatchRowcode": "     \"\"\""
            },
            "78": {
                "beforePatchRowNumber": 1147,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not os.path.exists(RSA_PRIVATE_KEY):"
            },
            "79": {
                "beforePatchRowNumber": 1148,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        status(\"Generating SSH key\")"
            },
            "80": {
                "beforePatchRowNumber": 1149,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        invoke(\"ssh-keygen -q -f {} -C 'Cluster Internal on {}' -N ''\".format(RSA_PRIVATE_KEY, utils.this_node()))"
            },
            "81": {
                "beforePatchRowNumber": 1150,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not os.path.exists(AUTHORIZED_KEYS_FILE):"
            },
            "82": {
                "beforePatchRowNumber": 1151,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        open(AUTHORIZED_KEYS_FILE, 'w').close()"
            },
            "83": {
                "beforePatchRowNumber": 1152,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    append_unique(RSA_PUBLIC_KEY, AUTHORIZED_KEYS_FILE)"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1183,
                "PatchRowcode": "+    change_user_shell(user)"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1184,
                "PatchRowcode": "+"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1185,
                "PatchRowcode": "+    private_key, public_key, authorized_file = key_files(user).values()"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1186,
                "PatchRowcode": "+    if not os.path.exists(private_key):"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1187,
                "PatchRowcode": "+        status(\"Generating SSH key for {}\".format(user))"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1188,
                "PatchRowcode": "+        cmd = \"ssh-keygen -q -f {} -C 'Cluster Internal on {}' -N ''\".format(private_key, utils.this_node())"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1189,
                "PatchRowcode": "+        cmd = utils.add_su(cmd, user)"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1190,
                "PatchRowcode": "+        rc, _, err = invoke(cmd)"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1191,
                "PatchRowcode": "+        if not rc:"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1192,
                "PatchRowcode": "+            error(\"Failed to generate ssh key for {}: {}\".format(user, err))"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1193,
                "PatchRowcode": "+"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1194,
                "PatchRowcode": "+    if not os.path.exists(authorized_file):"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1195,
                "PatchRowcode": "+        open(authorized_file, 'w').close()"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1196,
                "PatchRowcode": "+    append_unique(public_key, authorized_file)"
            },
            "98": {
                "beforePatchRowNumber": 1153,
                "afterPatchRowNumber": 1197,
                "PatchRowcode": " "
            },
            "99": {
                "beforePatchRowNumber": 1154,
                "afterPatchRowNumber": 1198,
                "PatchRowcode": " "
            },
            "100": {
                "beforePatchRowNumber": 1155,
                "afterPatchRowNumber": 1199,
                "PatchRowcode": " def init_ssh_remote():"
            },
            "101": {
                "beforePatchRowNumber": 1871,
                "afterPatchRowNumber": 1915,
                "PatchRowcode": "         error(\"No existing IP/hostname specified (use -c option)\")"
            },
            "102": {
                "beforePatchRowNumber": 1872,
                "afterPatchRowNumber": 1916,
                "PatchRowcode": " "
            },
            "103": {
                "beforePatchRowNumber": 1873,
                "afterPatchRowNumber": 1917,
                "PatchRowcode": "     utils.start_service(\"sshd.service\", enable=True)"
            },
            "104": {
                "beforePatchRowNumber": 1874,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    configure_local_ssh_key()"
            },
            "105": {
                "beforePatchRowNumber": 1875,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    swap_public_ssh_key(seed_host)"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1918,
                "PatchRowcode": "+    for user in USER_LIST:"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1919,
                "PatchRowcode": "+        configure_local_ssh_key(user)"
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1920,
                "PatchRowcode": "+        swap_public_ssh_key(seed_host, user)"
            },
            "109": {
                "beforePatchRowNumber": 1876,
                "afterPatchRowNumber": 1921,
                "PatchRowcode": " "
            },
            "110": {
                "beforePatchRowNumber": 1877,
                "afterPatchRowNumber": 1922,
                "PatchRowcode": "     # This makes sure the seed host has its own SSH keys in its own"
            },
            "111": {
                "beforePatchRowNumber": 1878,
                "afterPatchRowNumber": 1923,
                "PatchRowcode": "     # authorized_keys file (again, to help with the case where the"
            },
            "112": {
                "beforePatchRowNumber": 1883,
                "afterPatchRowNumber": 1928,
                "PatchRowcode": "         error(\"Can't invoke crm cluster init -i {} ssh_remote on {}: {}\".format(_context.default_nic_list[0], seed_host, err))"
            },
            "113": {
                "beforePatchRowNumber": 1884,
                "afterPatchRowNumber": 1929,
                "PatchRowcode": " "
            },
            "114": {
                "beforePatchRowNumber": 1885,
                "afterPatchRowNumber": 1930,
                "PatchRowcode": " "
            },
            "115": {
                "beforePatchRowNumber": 1886,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def swap_public_ssh_key(remote_node):"
            },
            "116": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1931,
                "PatchRowcode": "+def swap_public_ssh_key(remote_node, user=\"root\"):"
            },
            "117": {
                "beforePatchRowNumber": 1887,
                "afterPatchRowNumber": 1932,
                "PatchRowcode": "     \"\"\""
            },
            "118": {
                "beforePatchRowNumber": 1888,
                "afterPatchRowNumber": 1933,
                "PatchRowcode": "     Swap public ssh key between remote_node and local"
            },
            "119": {
                "beforePatchRowNumber": 1889,
                "afterPatchRowNumber": 1934,
                "PatchRowcode": "     \"\"\""
            },
            "120": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1935,
                "PatchRowcode": "+    if user != \"root\" and not _context.with_other_user:"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1936,
                "PatchRowcode": "+        return"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1937,
                "PatchRowcode": "+"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1938,
                "PatchRowcode": "+    _, public_key, authorized_file = key_files(user).values()"
            },
            "124": {
                "beforePatchRowNumber": 1890,
                "afterPatchRowNumber": 1939,
                "PatchRowcode": "     # Detect whether need password to login to remote_node"
            },
            "125": {
                "beforePatchRowNumber": 1891,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if utils.check_ssh_passwd_need(remote_node):"
            },
            "126": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1940,
                "PatchRowcode": "+    if utils.check_ssh_passwd_need(remote_node, user):"
            },
            "127": {
                "beforePatchRowNumber": 1892,
                "afterPatchRowNumber": 1941,
                "PatchRowcode": "         # If no passwordless configured, paste /root/.ssh/id_rsa.pub to remote_node's /root/.ssh/authorized_keys"
            },
            "128": {
                "beforePatchRowNumber": 1893,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        status(\"Configuring SSH passwordless with root@{}\".format(remote_node))"
            },
            "129": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1942,
                "PatchRowcode": "+        status(\"Configuring SSH passwordless with {}@{}\".format(user, remote_node))"
            },
            "130": {
                "beforePatchRowNumber": 1894,
                "afterPatchRowNumber": 1943,
                "PatchRowcode": "         # After this, login to remote_node is passwordless"
            },
            "131": {
                "beforePatchRowNumber": 1895,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        append_to_remote_file(RSA_PUBLIC_KEY, remote_node, AUTHORIZED_KEYS_FILE)"
            },
            "132": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1944,
                "PatchRowcode": "+        append_to_remote_file(public_key, remote_node, authorized_file)"
            },
            "133": {
                "beforePatchRowNumber": 1896,
                "afterPatchRowNumber": 1945,
                "PatchRowcode": " "
            },
            "134": {
                "beforePatchRowNumber": 1897,
                "afterPatchRowNumber": 1946,
                "PatchRowcode": "     try:"
            },
            "135": {
                "beforePatchRowNumber": 1898,
                "afterPatchRowNumber": 1947,
                "PatchRowcode": "         # Fetch public key file from remote_node"
            },
            "136": {
                "beforePatchRowNumber": 1899,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        public_key_file_remote = fetch_public_key_from_remote_node(remote_node)"
            },
            "137": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1948,
                "PatchRowcode": "+        public_key_file_remote = fetch_public_key_from_remote_node(remote_node, user)"
            },
            "138": {
                "beforePatchRowNumber": 1900,
                "afterPatchRowNumber": 1949,
                "PatchRowcode": "     except ValueError as err:"
            },
            "139": {
                "beforePatchRowNumber": 1901,
                "afterPatchRowNumber": 1950,
                "PatchRowcode": "         warn(err)"
            },
            "140": {
                "beforePatchRowNumber": 1902,
                "afterPatchRowNumber": 1951,
                "PatchRowcode": "         return"
            },
            "141": {
                "beforePatchRowNumber": 1903,
                "afterPatchRowNumber": 1952,
                "PatchRowcode": "     # Append public key file from remote_node to local's /root/.ssh/authorized_keys"
            },
            "142": {
                "beforePatchRowNumber": 1904,
                "afterPatchRowNumber": 1953,
                "PatchRowcode": "     # After this, login from remote_node is passwordless"
            },
            "143": {
                "beforePatchRowNumber": 1905,
                "afterPatchRowNumber": 1954,
                "PatchRowcode": "     # Should do this step even passwordless is True, to make sure we got two-way passwordless"
            },
            "144": {
                "beforePatchRowNumber": 1906,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    append_unique(public_key_file_remote, AUTHORIZED_KEYS_FILE)"
            },
            "145": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1955,
                "PatchRowcode": "+    append_unique(public_key_file_remote, authorized_file)"
            },
            "146": {
                "beforePatchRowNumber": 1907,
                "afterPatchRowNumber": 1956,
                "PatchRowcode": " "
            },
            "147": {
                "beforePatchRowNumber": 1908,
                "afterPatchRowNumber": 1957,
                "PatchRowcode": " "
            },
            "148": {
                "beforePatchRowNumber": 1909,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def fetch_public_key_from_remote_node(node):"
            },
            "149": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1958,
                "PatchRowcode": "+def fetch_public_key_from_remote_node(node, user=\"root\"):"
            },
            "150": {
                "beforePatchRowNumber": 1910,
                "afterPatchRowNumber": 1959,
                "PatchRowcode": "     \"\"\""
            },
            "151": {
                "beforePatchRowNumber": 1911,
                "afterPatchRowNumber": 1960,
                "PatchRowcode": "     Fetch public key file from remote node"
            },
            "152": {
                "beforePatchRowNumber": 1912,
                "afterPatchRowNumber": 1961,
                "PatchRowcode": "     Return a temp file contains public key"
            },
            "153": {
                "beforePatchRowNumber": 1915,
                "afterPatchRowNumber": 1964,
                "PatchRowcode": " "
            },
            "154": {
                "beforePatchRowNumber": 1916,
                "afterPatchRowNumber": 1965,
                "PatchRowcode": "     # For dsa, might need to add PubkeyAcceptedKeyTypes=+ssh-dss to config file, see"
            },
            "155": {
                "beforePatchRowNumber": 1917,
                "afterPatchRowNumber": 1966,
                "PatchRowcode": "     # https://superuser.com/questions/1016989/ssh-dsa-keys-no-longer-work-for-password-less-authentication"
            },
            "156": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1967,
                "PatchRowcode": "+    home_dir = userdir.gethomedir(user)"
            },
            "157": {
                "beforePatchRowNumber": 1918,
                "afterPatchRowNumber": 1968,
                "PatchRowcode": "     for key in (\"id_rsa\", \"id_ecdsa\", \"id_ed25519\", \"id_dsa\"):"
            },
            "158": {
                "beforePatchRowNumber": 1919,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        public_key_file = \"/root/.ssh/{}.pub\".format(key)"
            },
            "159": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1969,
                "PatchRowcode": "+        public_key_file = \"{}/.ssh/{}.pub\".format(home_dir, key)"
            },
            "160": {
                "beforePatchRowNumber": 1920,
                "afterPatchRowNumber": 1970,
                "PatchRowcode": "         cmd = \"ssh -oStrictHostKeyChecking=no root@{} 'test -f {}'\".format(node, public_key_file)"
            },
            "161": {
                "beforePatchRowNumber": 1921,
                "afterPatchRowNumber": 1971,
                "PatchRowcode": "         if not invokerc(cmd):"
            },
            "162": {
                "beforePatchRowNumber": 1922,
                "afterPatchRowNumber": 1972,
                "PatchRowcode": "             continue"
            },
            "163": {
                "beforePatchRowNumber": 2128,
                "afterPatchRowNumber": 2178,
                "PatchRowcode": " "
            },
            "164": {
                "beforePatchRowNumber": 2129,
                "afterPatchRowNumber": 2179,
                "PatchRowcode": "     # Swap ssh public key between join node and other cluster nodes"
            },
            "165": {
                "beforePatchRowNumber": 2130,
                "afterPatchRowNumber": 2180,
                "PatchRowcode": "     for node in cluster_nodes_list:"
            },
            "166": {
                "beforePatchRowNumber": 2131,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        swap_public_ssh_key(node)"
            },
            "167": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2181,
                "PatchRowcode": "+        for user in USER_LIST:"
            },
            "168": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2182,
                "PatchRowcode": "+            swap_public_ssh_key(node, user)"
            },
            "169": {
                "beforePatchRowNumber": 2132,
                "afterPatchRowNumber": 2183,
                "PatchRowcode": " "
            },
            "170": {
                "beforePatchRowNumber": 2133,
                "afterPatchRowNumber": 2184,
                "PatchRowcode": " "
            },
            "171": {
                "beforePatchRowNumber": 2134,
                "afterPatchRowNumber": 2185,
                "PatchRowcode": " def join_cluster(seed_host):"
            },
            "172": {
                "beforePatchRowNumber": 2487,
                "afterPatchRowNumber": 2538,
                "PatchRowcode": "     check_tty()"
            },
            "173": {
                "beforePatchRowNumber": 2488,
                "afterPatchRowNumber": 2539,
                "PatchRowcode": " "
            },
            "174": {
                "beforePatchRowNumber": 2489,
                "afterPatchRowNumber": 2540,
                "PatchRowcode": "     corosync_active = utils.service_is_active(\"corosync.service\")"
            },
            "175": {
                "beforePatchRowNumber": 2490,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if corosync_active:"
            },
            "176": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2541,
                "PatchRowcode": "+    if corosync_active and _context.stage != \"ssh\":"
            },
            "177": {
                "beforePatchRowNumber": 2491,
                "afterPatchRowNumber": 2542,
                "PatchRowcode": "         error(\"Abort: Cluster is currently active. Run this command on a node joining the cluster.\")"
            },
            "178": {
                "beforePatchRowNumber": 2492,
                "afterPatchRowNumber": 2543,
                "PatchRowcode": " "
            },
            "179": {
                "beforePatchRowNumber": 2493,
                "afterPatchRowNumber": 2544,
                "PatchRowcode": "     if not check_prereqs(\"join\"):"
            }
        },
        "frontPatchFile": [
            "# Copyright (C) 2016 Kristoffer Gronlund <kgronlund@suse.com>",
            "# See COPYING for license information.",
            "#",
            "# Bootstrap:",
            "#",
            "# Supersedes and replaces both the init/add/remove cluster scripts,",
            "# and the ha-cluster-bootstrap scripts.",
            "#",
            "# Implemented as a straight-forward set of python functions for",
            "# simplicity and flexibility.",
            "#",
            "# TODO: Make csync2 usage optional",
            "# TODO: Configuration file for bootstrap?",
            "",
            "import os",
            "import sys",
            "import random",
            "import re",
            "import time",
            "import readline",
            "import shutil",
            "from string import Template",
            "from lxml import etree",
            "from pathlib import Path",
            "from . import config",
            "from . import utils",
            "from . import xmlutil",
            "from .cibconfig import mkset_obj, cib_factory",
            "from . import corosync",
            "from . import tmpfiles",
            "from . import clidisplay",
            "from . import term",
            "from . import lock",
            "",
            "",
            "LOG_FILE = \"/var/log/crmsh/ha-cluster-bootstrap.log\"",
            "CSYNC2_KEY = \"/etc/csync2/key_hagroup\"",
            "CSYNC2_CFG = \"/etc/csync2/csync2.cfg\"",
            "COROSYNC_AUTH = \"/etc/corosync/authkey\"",
            "SYSCONFIG_SBD = \"/etc/sysconfig/sbd\"",
            "SYSCONFIG_FW = \"/etc/sysconfig/SuSEfirewall2\"",
            "SYSCONFIG_FW_CLUSTER = \"/etc/sysconfig/SuSEfirewall2.d/services/cluster\"",
            "PCMK_REMOTE_AUTH = \"/etc/pacemaker/authkey\"",
            "COROSYNC_CONF_ORIG = tmpfiles.create()[1]",
            "RSA_PRIVATE_KEY = \"/root/.ssh/id_rsa\"",
            "RSA_PUBLIC_KEY = \"/root/.ssh/id_rsa.pub\"",
            "AUTHORIZED_KEYS_FILE = \"/root/.ssh/authorized_keys\"",
            "SERVICES_STOP_LIST = [\"corosync-qdevice.service\", \"corosync.service\", \"hawk.service\"]",
            "",
            "INIT_STAGES = (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\", \"corosync\", \"storage\", \"sbd\", \"cluster\", \"vgfs\", \"admin\", \"qdevice\")",
            "",
            "",
            "class Context(object):",
            "    \"\"\"",
            "    Context object used to avoid having to pass these variables",
            "    to every bootstrap method.",
            "    \"\"\"",
            "    def __init__(self):",
            "        '''",
            "        Initialize attributes",
            "        '''",
            "        self.type = None # init or join",
            "        self.quiet = None",
            "        self.yes_to_all = None",
            "        self.template = None",
            "        self.cluster_name = None",
            "        self.watchdog = None",
            "        self.no_overwrite_sshkey = None",
            "        self.nic_list = None",
            "        self.unicast = None",
            "        self.admin_ip = None",
            "        self.second_heartbeat = None",
            "        self.ipv6 = None",
            "        self.qdevice_inst = None",
            "        self.qnetd_addr = None",
            "        self.qdevice_port = None",
            "        self.qdevice_algo = None",
            "        self.qdevice_tie_breaker = None",
            "        self.qdevice_tls = None",
            "        self.qdevice_heuristics = None",
            "        self.qdevice_heuristics_mode = None",
            "        self.qdevice_rm_flag = None",
            "        self.shared_device = None",
            "        self.ocfs2_device = None",
            "        self.cluster_node = None",
            "        self.cluster_node_ip = None",
            "        self.force = None",
            "        self.arbitrator = None",
            "        self.clusters = None",
            "        self.tickets = None",
            "        self.sbd_manager = None",
            "        self.sbd_devices = None",
            "        self.diskless_sbd = None",
            "        self.stage = None",
            "        self.args = None",
            "        self.ui_context = None",
            "        self.interfaces_inst = None",
            "        self.default_nic_list = []",
            "        self.default_ip_list = []",
            "        self.local_ip_list = []",
            "        self.local_network_list = []",
            "        self.rm_list = [SYSCONFIG_SBD, CSYNC2_CFG, corosync.conf(), CSYNC2_KEY,",
            "                COROSYNC_AUTH, \"/var/lib/heartbeat/crm/*\", \"/var/lib/pacemaker/cib/*\"]",
            "",
            "    @classmethod",
            "    def set_context(cls, options):",
            "        ctx = cls()",
            "        for opt in vars(options):",
            "            setattr(ctx, opt, getattr(options, opt))",
            "        return ctx",
            "",
            "    def initialize_qdevice(self):",
            "        \"\"\"",
            "        Initialize qdevice instance",
            "        \"\"\"",
            "        if not self.qnetd_addr:",
            "            return",
            "        self.qdevice_inst = corosync.QDevice(",
            "                self.qnetd_addr,",
            "                port=self.qdevice_port,",
            "                algo=self.qdevice_algo,",
            "                tie_breaker=self.qdevice_tie_breaker,",
            "                tls=self.qdevice_tls,",
            "                cmds=self.qdevice_heuristics,",
            "                mode=self.qdevice_heuristics_mode)",
            "",
            "    def validate_option(self):",
            "        \"\"\"",
            "        Validate options",
            "        \"\"\"",
            "        if self.admin_ip:",
            "            try:",
            "                Validation.valid_admin_ip(self.admin_ip)",
            "            except ValueError as err:",
            "                error(err)",
            "        if self.qdevice_inst:",
            "            try:",
            "                self.qdevice_inst.valid_attr()",
            "            except ValueError as err:",
            "                error(err)",
            "        if self.nic_list:",
            "            if len(self.nic_list) > 2:",
            "                error(\"Maximum number of interface is 2\")",
            "            if len(self.nic_list) != len(set(self.nic_list)):",
            "                error(\"Duplicated input\")",
            "        if self.no_overwrite_sshkey:",
            "            warn(\"--no-overwrite-sshkey option is deprecated since crmsh does not overwrite ssh keys by default anymore and will be removed in future versions\")",
            "        if self.type == \"join\" and self.watchdog:",
            "            warn(\"-w option is deprecated and will be removed in future versions\")",
            "",
            "    def init_sbd_manager(self):",
            "        self.sbd_manager = SBDManager(self.sbd_devices, self.diskless_sbd)",
            "",
            "",
            "class Watchdog(object):",
            "    \"\"\"",
            "    Class to find valid watchdog device name",
            "    \"\"\"",
            "    QUERY_CMD = \"sbd query-watchdog\"",
            "    DEVICE_FIND_REGREX = \"\\[[0-9]+\\] (/dev/.*)\\n.*\\nDriver: (.*)\"",
            "",
            "    def __init__(self, _input=None, peer_host=None):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self._input = _input",
            "        self._peer_host = peer_host",
            "        self._watchdog_info_dict = {}",
            "        self._watchdog_device_name = None",
            "    ",
            "    @property",
            "    def watchdog_device_name(self):",
            "        return self._watchdog_device_name",
            "",
            "    @staticmethod",
            "    def _verify_watchdog_device(dev, ignore_error=False):",
            "        \"\"\"",
            "        Use wdctl to verify watchdog device",
            "        \"\"\"",
            "        rc, _, err = utils.get_stdout_stderr(\"wdctl {}\".format(dev))",
            "        if rc != 0:",
            "            if ignore_error:",
            "                return False",
            "            else:",
            "                error(\"Invalid watchdog device {}: {}\".format(dev, err))",
            "        return True",
            "",
            "    @staticmethod",
            "    def _load_watchdog_driver(driver):",
            "        \"\"\"",
            "        Load specific watchdog driver",
            "        \"\"\"",
            "        invoke(\"echo {} > /etc/modules-load.d/watchdog.conf\".format(driver))",
            "        invoke(\"systemctl restart systemd-modules-load\")",
            "",
            "    @staticmethod",
            "    def _get_watchdog_device_from_sbd_config():",
            "        \"\"\"",
            "        Try to get watchdog device name from sbd config file",
            "        \"\"\"",
            "        conf = utils.parse_sysconfig(SYSCONFIG_SBD)",
            "        return conf.get(\"SBD_WATCHDOG_DEV\")",
            "",
            "    @staticmethod",
            "    def _driver_is_loaded(driver):",
            "        \"\"\"",
            "        Check if driver was already loaded",
            "        \"\"\"",
            "        _, out, _ = utils.get_stdout_stderr(\"lsmod\")",
            "        return re.search(\"\\n{}\\s+\".format(driver), out)",
            "",
            "    def _set_watchdog_info(self):",
            "        \"\"\"",
            "        Set watchdog info through sbd query-watchdog command",
            "        Content in self._watchdog_info_dict: {device_name: driver_name}",
            "        \"\"\"",
            "        rc, out, err = utils.get_stdout_stderr(self.QUERY_CMD)",
            "        if rc == 0 and out:",
            "            # output format might like:",
            "            #   [1] /dev/watchdog\\nIdentity: Software Watchdog\\nDriver: softdog\\n",
            "            self._watchdog_info_dict = dict(re.findall(self.DEVICE_FIND_REGREX, out))",
            "        else:",
            "            error(\"Failed to run {}: {}\".format(self.QUERY_CMD, err))",
            "",
            "    def _get_device_through_driver(self, driver_name):",
            "        \"\"\"",
            "        Get watchdog device name which has driver_name",
            "        \"\"\"",
            "        for device, driver in self._watchdog_info_dict.items():",
            "            if driver == driver_name and self._verify_watchdog_device(device):",
            "                return device",
            "        return None",
            "",
            "    def _get_driver_through_device_remotely(self, dev_name):",
            "        \"\"\"",
            "        Given watchdog device name, get driver name on remote node",
            "        \"\"\"",
            "        cmd = \"ssh -o StrictHostKeyChecking=no root@{} {}\".format(self._peer_host, self.QUERY_CMD)",
            "        rc, out, err = utils.get_stdout_stderr(cmd)",
            "        if rc == 0 and out:",
            "            # output format might like:",
            "            #   [1] /dev/watchdog\\nIdentity: Software Watchdog\\nDriver: softdog\\n",
            "            device_driver_dict = dict(re.findall(self.DEVICE_FIND_REGREX, out))",
            "            if device_driver_dict and dev_name in device_driver_dict:",
            "                return device_driver_dict[dev_name]",
            "            else:",
            "                return None",
            "        else:",
            "            error(\"Failed to run {} remotely: {}\".format(self.QUERY_CMD, err))",
            "",
            "    def _get_first_unused_device(self):",
            "        \"\"\"",
            "        Get first unused watchdog device name",
            "        \"\"\"",
            "        for dev in self._watchdog_info_dict:",
            "            if self._verify_watchdog_device(dev, ignore_error=True):",
            "                return dev",
            "        return None",
            "",
            "    def _set_input(self):",
            "        \"\"\"",
            "        If self._input was not provided by option:",
            "          1. Try to get it from sbd config file",
            "          2. Try to get the first valid device from result of sbd query-watchdog",
            "          3. Set the self._input as softdog",
            "        \"\"\"",
            "        if not self._input:",
            "            dev = self._get_watchdog_device_from_sbd_config()",
            "            if dev and self._verify_watchdog_device(dev, ignore_error=True):",
            "                self._input = dev",
            "                return",
            "            first_unused = self._get_first_unused_device()",
            "            self._input = first_unused if first_unused else \"softdog\"",
            "",
            "    def _valid_device(self, dev):",
            "        \"\"\"",
            "        Is an unused watchdog device",
            "        \"\"\"",
            "        if dev in self._watchdog_info_dict and self._verify_watchdog_device(dev):",
            "            return True",
            "        return False",
            "",
            "    def join_watchdog(self):",
            "        \"\"\"",
            "        In join proces, get watchdog device from config",
            "        If that device not exist, get driver name from init node, and load that driver",
            "        \"\"\"",
            "        self._set_watchdog_info()",
            "",
            "        res = self._get_watchdog_device_from_sbd_config()",
            "        if not res:",
            "            error(\"Failed to get watchdog device from {}\".format(SYSCONFIG_SBD))",
            "        self._input = res",
            "",
            "        if not self._valid_device(self._input):",
            "            driver = self._get_driver_through_device_remotely(self._input)",
            "            self._load_watchdog_driver(driver)",
            "",
            "    def init_watchdog(self):",
            "        \"\"\"",
            "        In init process, find valid watchdog device",
            "        \"\"\"",
            "        self._set_watchdog_info()",
            "        self._set_input()",
            "",
            "        # self._input is a device name",
            "        if self._valid_device(self._input):",
            "            self._watchdog_device_name = self._input",
            "            return",
            "",
            "        # self._input is invalid, exit",
            "        if not invokerc(\"modinfo {}\".format(self._input)):",
            "            error(\"Should provide valid watchdog device or driver name by -w option\")",
            "",
            "        # self._input is a driver name, load it if it was unloaded",
            "        if not self._driver_is_loaded(self._input):",
            "            self._load_watchdog_driver(self._input)",
            "            self._set_watchdog_info()",
            "",
            "        # self._input is a loaded driver name, find corresponding device name",
            "        res = self._get_device_through_driver(self._input)",
            "        if res:",
            "            self._watchdog_device_name = res",
            "            return",
            "",
            "",
            "class SBDManager(object):",
            "    \"\"\"",
            "    Class to manage sbd configuration and services",
            "    \"\"\"",
            "    SYSCONFIG_SBD_TEMPLATE = \"/usr/share/fillup-templates/sysconfig.sbd\"",
            "    SBD_STATUS_DESCRIPTION = \"\"\"",
            "Configure SBD:",
            "  If you have shared storage, for example a SAN or iSCSI target,",
            "  you can use it avoid split-brain scenarios by configuring SBD.",
            "  This requires a 1 MB partition, accessible to all nodes in the",
            "  cluster.  The device path must be persistent and consistent",
            "  across all nodes in the cluster, so /dev/disk/by-id/* devices",
            "  are a good choice.  Note that all data on the partition you",
            "  specify here will be destroyed.",
            "\"\"\"",
            "",
            "    def __init__(self, sbd_devices=None, diskless_sbd=False):",
            "        \"\"\"",
            "        Init function",
            "",
            "        sbd_devices is provided by '-s' option on init process",
            "        diskless_sbd is provided by '-S' option on init process",
            "        \"\"\"",
            "        self.sbd_devices_input = sbd_devices",
            "        self.diskless_sbd = diskless_sbd",
            "        self._sbd_devices = None",
            "        self._watchdog_inst = None",
            "",
            "    def _parse_sbd_device(self):",
            "        \"\"\"",
            "        Parse sbd devices, possible command line is like:",
            "          -s \"/dev/sdb1;/dev/sdb2\"",
            "          -s /dev/sdb1 -s /dev/sbd2",
            "        \"\"\"",
            "        result_list = []",
            "        for dev in self.sbd_devices_input:",
            "            if ';' in dev:",
            "                result_list.extend(dev.strip(';').split(';'))",
            "            else:",
            "                result_list.append(dev)",
            "        return result_list",
            "",
            "    @staticmethod",
            "    def _get_device_uuid(dev, node=None):",
            "        \"\"\"",
            "        Get UUID for specific device and node",
            "        \"\"\"",
            "        cmd = \"sbd -d {} dump\".format(dev)",
            "        if node:",
            "            cmd = \"ssh -o StrictHostKeyChecking=no root@{} '{}'\".format(node, cmd)",
            "",
            "        rc, out, err = utils.get_stdout_stderr(cmd)",
            "        if rc != 0 and err:",
            "            raise ValueError(\"Cannot dump sbd meta-data: {}\".format(err))",
            "        if rc == 0 and out:",
            "            res = re.search(\"UUID\\s*:\\s*(.*)\\n\", out)",
            "            if not res:",
            "                raise ValueError(\"Cannot find sbd device UUID for {}\".format(dev))",
            "            return res.group(1)",
            "",
            "    def _compare_device_uuid(self, dev, node_list):",
            "        \"\"\"",
            "        Compare local sbd device UUID with other node's sbd device UUID",
            "        \"\"\"",
            "        if not node_list:",
            "            return",
            "        local_uuid = self._get_device_uuid(dev)",
            "        for node in node_list:",
            "            remote_uuid = self._get_device_uuid(dev, node)",
            "            if local_uuid != remote_uuid:",
            "                raise ValueError(\"Device {} doesn't have the same UUID with {}\".format(dev, node))",
            "",
            "    def _verify_sbd_device(self, dev_list, compare_node_list=[]):",
            "        \"\"\"",
            "        Verify sbd device",
            "        \"\"\"",
            "        if len(dev_list) > 3:",
            "            raise ValueError(\"Maximum number of SBD device is 3\")",
            "        for dev in dev_list:",
            "            if not is_block_device(dev):",
            "                raise ValueError(\"{} doesn't look like a block device\".format(dev))",
            "            self._compare_device_uuid(dev, compare_node_list)",
            "",
            "    def _get_sbd_device_interactive(self):",
            "        \"\"\"",
            "        Get sbd device on interactive mode",
            "        \"\"\"",
            "        if _context.yes_to_all:",
            "            warn(\"Not configuring SBD ({} left untouched).\".format(SYSCONFIG_SBD))",
            "            return",
            "",
            "        status(self.SBD_STATUS_DESCRIPTION)",
            "",
            "        if not confirm(\"Do you wish to use SBD?\"):",
            "            warn(\"Not configuring SBD - STONITH will be disabled.\")",
            "            return",
            "",
            "        configured_dev_list = self._get_sbd_device_from_config()",
            "        if configured_dev_list and not confirm(\"SBD is already configured to use {} - overwrite?\".format(';'.join(configured_dev_list))):",
            "            return configured_dev_list",
            "",
            "        dev_list = []",
            "        dev_looks_sane = False",
            "        while not dev_looks_sane:",
            "            dev = prompt_for_string('Path to storage device (e.g. /dev/disk/by-id/...), or \"none\" for diskless sbd, use \";\" as separator for multi path', r'none|\\/.*')",
            "            if not dev:",
            "                continue",
            "            if dev == \"none\":",
            "                self.diskless_sbd = True",
            "                return",
            "            dev_list = dev.strip(';').split(';')",
            "            try:",
            "                self._verify_sbd_device(dev_list)",
            "            except ValueError as err_msg:",
            "                print_error_msg(str(err_msg))",
            "                continue",
            "            for dev_item in dev_list:",
            "                warn(\"All data on {} will be destroyed!\".format(dev_item))",
            "                if confirm('Are you sure you wish to use this device?'):",
            "                    dev_looks_sane = True",
            "                else:",
            "                    dev_looks_sane = False",
            "                    break",
            "",
            "        return dev_list",
            "",
            "    def _get_sbd_device(self):",
            "        \"\"\"",
            "        Get sbd device from options or interactive mode",
            "        \"\"\"",
            "        dev_list = []",
            "        if self.sbd_devices_input:",
            "            dev_list = self._parse_sbd_device()",
            "            self._verify_sbd_device(dev_list)",
            "        elif not self.diskless_sbd:",
            "            dev_list = self._get_sbd_device_interactive()",
            "        self._sbd_devices = dev_list",
            "",
            "    def _initialize_sbd(self):",
            "        \"\"\"",
            "        Initialize SBD device",
            "        \"\"\"",
            "        if self.diskless_sbd:",
            "            return",
            "        for dev in self._sbd_devices:",
            "            rc, _, err = invoke(\"sbd -d {} create\".format(dev))",
            "            if not rc:",
            "                error(\"Failed to initialize SBD device {}: {}\".format(dev, err))",
            "",
            "    def _update_configuration(self):",
            "        \"\"\"",
            "        Update /etc/sysconfig/sbd",
            "        \"\"\"",
            "        shutil.copyfile(self.SYSCONFIG_SBD_TEMPLATE, SYSCONFIG_SBD)",
            "        sbd_config_dict = {",
            "                \"SBD_PACEMAKER\": \"yes\",",
            "                \"SBD_STARTMODE\": \"always\",",
            "                \"SBD_DELAY_START\": \"no\",",
            "                \"SBD_WATCHDOG_DEV\": self._watchdog_inst.watchdog_device_name",
            "                }",
            "        if self._sbd_devices:",
            "            sbd_config_dict[\"SBD_DEVICE\"] = ';'.join(self._sbd_devices)",
            "        utils.sysconfig_set(SYSCONFIG_SBD, **sbd_config_dict)",
            "        csync2_update(SYSCONFIG_SBD)",
            "",
            "    @staticmethod",
            "    def _get_sbd_device_from_config():",
            "        \"\"\"",
            "        Gets currently configured SBD device, i.e. what's in /etc/sysconfig/sbd",
            "        \"\"\"",
            "        conf = utils.parse_sysconfig(SYSCONFIG_SBD)",
            "        res = conf.get(\"SBD_DEVICE\")",
            "        if res:",
            "            return res.strip(';').split(';')",
            "        else:",
            "            return None",
            "",
            "    def sbd_init(self):",
            "        \"\"\"",
            "        Function sbd_init includes these steps:",
            "        1. Get sbd device from options or interactive mode",
            "        2. Initialize sbd device",
            "        3. Write config file /etc/sysconfig/sbd",
            "        \"\"\"",
            "        if not utils.package_is_installed(\"sbd\"):",
            "            return",
            "        self._watchdog_inst = Watchdog(_input=_context.watchdog)",
            "        self._watchdog_inst.init_watchdog()",
            "        self._get_sbd_device()",
            "        if not self._sbd_devices and not self.diskless_sbd:",
            "            invoke(\"systemctl disable sbd.service\")",
            "            return",
            "        status_long(\"Initializing {}SBD...\".format(\"diskless \" if self.diskless_sbd else \"\"))",
            "        self._initialize_sbd()",
            "        self._update_configuration()",
            "        invoke(\"systemctl enable sbd.service\")",
            "        status_done()",
            "",
            "    def configure_sbd_resource(self):",
            "        \"\"\"",
            "        Configure stonith-sbd resource and stonith-enabled property",
            "        \"\"\"",
            "        if not utils.package_is_installed(\"sbd\"):",
            "            return",
            "        if utils.service_is_enabled(\"sbd.service\"):",
            "            if self._get_sbd_device_from_config():",
            "                if not invokerc(\"crm configure primitive stonith-sbd stonith:external/sbd pcmk_delay_max=30s\"):",
            "                    error(\"Can't create stonith-sbd primitive\")",
            "                if not invokerc(\"crm configure property stonith-enabled=true\"):",
            "                    error(\"Can't enable STONITH for SBD\")",
            "            else:",
            "                if not invokerc(\"crm configure property stonith-enabled=true stonith-watchdog-timeout=5s\"):",
            "                    error(\"Can't enable STONITH for diskless SBD\")",
            "",
            "    def join_sbd(self, peer_host):",
            "        \"\"\"",
            "        Function join_sbd running on join process only",
            "        On joining process, check whether peer node has enabled sbd.service",
            "        If so, check prerequisites of SBD and verify sbd device on join node",
            "        \"\"\"",
            "        if not utils.package_is_installed(\"sbd\"):",
            "            return",
            "        if not os.path.exists(SYSCONFIG_SBD) or not utils.service_is_enabled(\"sbd.service\", peer_host):",
            "            invoke(\"systemctl disable sbd.service\")",
            "            return",
            "        self._watchdog_inst = Watchdog(peer_host=peer_host)",
            "        self._watchdog_inst.join_watchdog()",
            "        dev_list = self._get_sbd_device_from_config()",
            "        if dev_list:",
            "            self._verify_sbd_device(dev_list, [peer_host])",
            "        status(\"Got {}SBD configuration\".format(\"\" if dev_list else \"diskless \"))",
            "        invoke(\"systemctl enable sbd.service\")",
            "",
            "    @classmethod",
            "    def verify_sbd_device(cls):",
            "        \"\"\"",
            "        This classmethod is for verifying sbd device on a running cluster",
            "        Raise ValueError for exceptions",
            "        \"\"\"",
            "        inst = cls()",
            "        dev_list = inst._get_sbd_device_from_config()",
            "        if not dev_list:",
            "            raise ValueError(\"No sbd device configured\")",
            "        inst._verify_sbd_device(dev_list, utils.list_cluster_nodes_except_me())",
            "",
            "",
            "_context = None",
            "",
            "",
            "def die(*args):",
            "    \"\"\"",
            "    Broken out as special case for log() failure.  Ordinarily you",
            "    should just use error() to terminate.",
            "    \"\"\"",
            "    raise ValueError(\" \".join([str(arg) for arg in args]))",
            "",
            "",
            "def error(*args):",
            "    \"\"\"",
            "    Log an error message and raise ValueError to bail out of",
            "    bootstrap process.",
            "    \"\"\"",
            "    log(\"ERROR: {}\".format(\" \".join([str(arg) for arg in args])))",
            "    die(*args)",
            "",
            "",
            "def print_error_msg(msg):",
            "    \"\"\"",
            "    Just print error message",
            "    \"\"\"",
            "    print(term.render(clidisplay.error(\"ERROR:\")) + \" {}\".format(msg))",
            "",
            "",
            "def warn(*args):",
            "    \"\"\"",
            "    Log and display a warning message.",
            "    \"\"\"",
            "    log(\"WARNING: {}\".format(\" \".join(str(arg) for arg in args)))",
            "    print(term.render(clidisplay.warn(\"WARNING: {}\".format(\" \".join(str(arg) for arg in args)))))",
            "",
            "",
            "@utils.memoize",
            "def log_file_fallback():",
            "    \"\"\"",
            "    If the standard log location isn't writable,",
            "    just log to the nearest temp dir.",
            "    \"\"\"",
            "    return os.path.join(utils.get_tempdir(), \"ha-cluster-bootstrap.log\")",
            "",
            "",
            "def log(*args):",
            "    global LOG_FILE",
            "    try:",
            "        Path(os.path.dirname(LOG_FILE)).mkdir(parents=True, exist_ok=True)",
            "        with open(LOG_FILE, \"ab\") as logfile:",
            "            text = \" \".join([utils.to_ascii(arg) for arg in args]) + \"\\n\"",
            "            logfile.write(text.encode('ascii', 'backslashreplace'))",
            "    except IOError:",
            "        if LOG_FILE != log_file_fallback():",
            "            LOG_FILE = log_file_fallback()",
            "            log(*args)",
            "        else:",
            "            die(\"Can't append to {} - aborting\".format(LOG_FILE))",
            "",
            "",
            "def drop_last_history():",
            "    hlen = readline.get_current_history_length()",
            "    if hlen > 0:",
            "        readline.remove_history_item(hlen - 1)",
            "",
            "",
            "def prompt_for_string(msg, match=None, default='', valid_func=None, prev_value=[]):",
            "    if _context.yes_to_all:",
            "        return default",
            "",
            "    while True:",
            "        disable_completion()",
            "        val = utils.multi_input('  %s [%s]' % (msg, default))",
            "        enable_completion()",
            "        if not val:",
            "            val = default",
            "        else:",
            "            drop_last_history()",
            "",
            "        if not val:",
            "            return None",
            "        if not match and not valid_func:",
            "            return val",
            "        if match and not re.match(match, val):",
            "            print_error_msg(\"Invalid value entered\")",
            "            continue",
            "        if valid_func:",
            "            try:",
            "                valid_func(val, prev_value)",
            "            except ValueError as err:",
            "                print_error_msg(err)",
            "                continue",
            "",
            "        return val",
            "",
            "",
            "def confirm(msg):",
            "    if _context.yes_to_all:",
            "        return True",
            "    disable_completion()",
            "    rc = utils.ask(msg)",
            "    enable_completion()",
            "    drop_last_history()",
            "    return rc",
            "",
            "",
            "def disable_completion():",
            "    if _context.ui_context:",
            "        _context.ui_context.disable_completion()",
            "",
            "",
            "def enable_completion():",
            "    if _context.ui_context:",
            "        _context.ui_context.setup_readline()",
            "",
            "",
            "def invoke(*args):",
            "    \"\"\"",
            "    Log command execution to log file.",
            "    Log output from command to log file.",
            "    Return (boolean, stdout, stderr)",
            "    \"\"\"",
            "    log(\"+ \" + \" \".join(args))",
            "    rc, stdout, stderr = utils.get_stdout_stderr(\" \".join(args))",
            "    if stdout:",
            "        log(stdout)",
            "    if stderr:",
            "        log(stderr)",
            "    return rc == 0, stdout, stderr",
            "",
            "",
            "def invokerc(*args):",
            "    \"\"\"",
            "    Calling invoke, return True/False",
            "    \"\"\"",
            "    rc, _, _ = invoke(*args)",
            "    return rc",
            "",
            "",
            "def crm_configure_load(action, configuration):",
            "    log(\": loading crm config (%s), content is:\" % (action))",
            "    log(configuration)",
            "    if not cib_factory.initialize():",
            "        error(\"Failed to load cluster configuration\")",
            "    set_obj = mkset_obj()",
            "    if action == 'replace':",
            "        cib_factory.erase()",
            "    if not set_obj.save(configuration, remove=False, method=action):",
            "        error(\"Failed to load cluster configuration\")",
            "    if not cib_factory.commit():",
            "        error(\"Failed to commit cluster configuration\")",
            "",
            "",
            "def wait_for_resource(message, resource, needle=\"running on\"):",
            "    status_long(message)",
            "    while True:",
            "        _rc, out, err = utils.get_stdout_stderr(\"crm_resource --locate --resource \" + resource)",
            "        if needle in out:",
            "            break",
            "        if needle in err:",
            "            break",
            "        status_progress()",
            "        sleep(1)",
            "    status_done()",
            "",
            "",
            "def wait_for_stop(message, resource):",
            "    return wait_for_resource(message, resource, needle=\"NOT running\")",
            "",
            "",
            "def wait_for_cluster():",
            "    status_long(\"Waiting for cluster\")",
            "    while True:",
            "        _rc, out, _err = utils.get_stdout_stderr(\"crm_mon -1\")",
            "        if is_online(out):",
            "            break",
            "        status_progress()",
            "        sleep(2)",
            "    status_done()",
            "",
            "",
            "def get_cluster_node_hostname():",
            "    \"\"\"",
            "    Get the hostname of the cluster node used during the join process if an IP address is used.",
            "    \"\"\"",
            "    peer_node = None",
            "    if _context.cluster_node:",
            "        if utils.IP.is_valid_ip(_context.cluster_node):",
            "            rc, out, err = utils.get_stdout_stderr(\"ssh {} crm_node --name\".format(_context.cluster_node))",
            "            if rc != 0:",
            "                error(err)",
            "            peer_node = out",
            "        else:",
            "            peer_node = _context.cluster_node",
            "    return peer_node",
            "",
            "",
            "def is_online(crm_mon_txt):",
            "    \"\"\"",
            "    Check whether local node is online",
            "    Besides that, in join process, check whether init node is online",
            "    \"\"\"",
            "    if not re.search(\"Online: .* {} \".format(utils.this_node()), crm_mon_txt):",
            "        return False",
            "",
            "    # if peer_node is None, this is in the init process",
            "    peer_node = get_cluster_node_hostname()",
            "    if peer_node is None:",
            "        return True",
            "    # In join process",
            "    # If the joining node is already online but can't find the init node",
            "    # The communication IP maybe mis-configured",
            "    if not re.search(\"Online: .* {} \".format(peer_node), crm_mon_txt):",
            "        shutil.copy(COROSYNC_CONF_ORIG, corosync.conf())",
            "        csync2_update(corosync.conf())",
            "        utils.stop_service(\"corosync\")",
            "        print()",
            "        error(\"Cannot see peer node \\\"{}\\\", please check the communication IP\".format(peer_node))",
            "    return True",
            "",
            "",
            "def pick_default_value(default_list, prev_list):",
            "    \"\"\"",
            "    Provide default value for function 'prompt_for_string'.",
            "    Make sure give different default value in multi-ring mode.",
            "",
            "    Parameters:",
            "    * default_list - default value list for config item",
            "    * prev_list    - previous value for config item in multi-ring mode",
            "    \"\"\"",
            "    for value in default_list:",
            "        if value not in prev_list:",
            "            return value",
            "    return \"\"",
            "",
            "",
            "def sleep(t):",
            "    \"\"\"",
            "    Sleep for t seconds.",
            "    \"\"\"",
            "    t = float(t)",
            "    time.sleep(t)",
            "",
            "",
            "def status(msg):",
            "    log(\"# \" + msg)",
            "    if not _context.quiet:",
            "        print(\"  {}\".format(msg))",
            "",
            "",
            "def status_long(msg):",
            "    log(\"# {}...\".format(msg))",
            "    if not _context.quiet:",
            "        sys.stdout.write(\"  {}...\".format(msg))",
            "        sys.stdout.flush()",
            "",
            "",
            "def status_progress():",
            "    if not _context.quiet:",
            "        sys.stdout.write(\".\")",
            "        sys.stdout.flush()",
            "",
            "",
            "def status_done():",
            "    log(\"# done\")",
            "    if not _context.quiet:",
            "        print(\"done\")",
            "",
            "",
            "def partprobe():",
            "    # This function uses fdisk to create a list of valid devices for probing",
            "    # with partprobe.  This prevents partprobe from failing on read-only mounted",
            "    # devices such as /dev/sr0 (etc) that might cause it to return an error when",
            "    # it exits.  This allows partprobe to run without forcing _die to bail out.",
            "    # -Brandon Heaton",
            "    #  ATT Training Engineer",
            "    #  Data Center Engineer",
            "    #  bheaton@suse.com",
            "    _rc, out, _err = utils.get_stdout_stderr(\"sfdisk -l\")",
            "    disks = re.findall(r'^Disk\\s*(/.+):', out, re.M)",
            "    invoke(\"partprobe\", *disks)",
            "",
            "",
            "def probe_partitions():",
            "    status_long(\"Probing for new partitions\")",
            "    partprobe()",
            "    sleep(5)",
            "    status_done()",
            "",
            "",
            "def check_tty():",
            "    \"\"\"",
            "    Check for pseudo-tty: Cannot display read prompts without a TTY (bnc#892702)",
            "    \"\"\"",
            "    if _context.yes_to_all:",
            "        return",
            "    if not sys.stdin.isatty():",
            "        error(\"No pseudo-tty detected! Use -t option to ssh if calling remotely.\")",
            "",
            "",
            "def my_hostname_resolves():",
            "    import socket",
            "    hostname = utils.this_node()",
            "    try:",
            "        socket.gethostbyname(hostname)",
            "        return True",
            "    except socket.error:",
            "        return False",
            "",
            "",
            "def check_prereqs(stage):",
            "    warned = False",
            "",
            "    if not my_hostname_resolves():",
            "        warn(\"Hostname '{}' is unresolvable. {}\".format(",
            "            utils.this_node(),",
            "            \"Please add an entry to /etc/hosts or configure DNS.\"))",
            "        warned = True",
            "",
            "    timekeepers = ('chronyd.service', 'ntp.service', 'ntpd.service')",
            "    timekeeper = None",
            "    for tk in timekeepers:",
            "        if utils.service_is_available(tk):",
            "            timekeeper = tk",
            "            break",
            "",
            "    if timekeeper is None:",
            "        warn(\"No NTP service found.\")",
            "        warned = True",
            "    elif not utils.service_is_enabled(timekeeper):",
            "        warn(\"{} is not configured to start at system boot.\".format(timekeeper))",
            "        warned = True",
            "",
            "    if warned:",
            "        if not confirm(\"Do you want to continue anyway?\"):",
            "            return False",
            "",
            "    firewall_open_basic_ports()",
            "    return True",
            "",
            "",
            "def log_start():",
            "    \"\"\"",
            "    Convenient side-effect: this will die immediately if the log file",
            "    is not writable (e.g. if not running as root)",
            "    \"\"\"",
            "    # Reload rsyslog to make sure it logs with the correct hostname",
            "    if utils.service_is_active(\"rsyslog.service\"):",
            "        invoke(\"systemctl reload rsyslog.service\")",
            "    datestr = utils.get_stdout(\"date --rfc-3339=seconds\")[1]",
            "    log('================================================================')",
            "    log(\"%s %s\" % (datestr, \" \".join(sys.argv)))",
            "    log('----------------------------------------------------------------')",
            "",
            "",
            "def init_network():",
            "    \"\"\"",
            "    Get all needed network information through utils.InterfacesInfo",
            "    \"\"\"",
            "    interfaces_inst = utils.InterfacesInfo(_context.ipv6, _context.second_heartbeat, _context.nic_list)",
            "    interfaces_inst.get_interfaces_info()",
            "    _context.default_nic_list = interfaces_inst.get_default_nic_list_from_route()",
            "    _context.default_ip_list = interfaces_inst.get_default_ip_list()",
            "",
            "    # local_ip_list and local_network_list are for validation",
            "    _context.local_ip_list = interfaces_inst.ip_list",
            "    _context.local_network_list = interfaces_inst.network_list",
            "    _context.interfaces_inst = interfaces_inst",
            "    # use two \"-i\" options equal to use \"-M\" option",
            "    if len(_context.default_nic_list) == 2 and not _context.second_heartbeat:",
            "        _context.second_heartbeat = True",
            "",
            "",
            "def configure_firewall(tcp=None, udp=None):",
            "    if tcp is None:",
            "        tcp = []",
            "    if udp is None:",
            "        udp = []",
            "",
            "    def init_firewall_suse(tcp, udp):",
            "        if os.path.exists(SYSCONFIG_FW_CLUSTER):",
            "            cluster = utils.parse_sysconfig(SYSCONFIG_FW_CLUSTER)",
            "            tcpcurr = set(cluster.get(\"TCP\", \"\").split())",
            "            tcpcurr.update(tcp)",
            "            tcp = list(tcpcurr)",
            "            udpcurr = set(cluster.get(\"UDP\", \"\").split())",
            "            udpcurr.update(udp)",
            "            udp = list(udpcurr)",
            "",
            "        utils.sysconfig_set(SYSCONFIG_FW_CLUSTER, TCP=\" \".join(tcp), UDP=\" \".join(udp))",
            "",
            "        ext = \"\"",
            "        if os.path.exists(SYSCONFIG_FW):",
            "            fw = utils.parse_sysconfig(SYSCONFIG_FW)",
            "            ext = fw.get(\"FW_CONFIGURATIONS_EXT\", \"\")",
            "            if \"cluster\" not in ext.split():",
            "                ext = ext + \" cluster\"",
            "        utils.sysconfig_set(SYSCONFIG_FW, FW_CONFIGURATIONS_EXT=ext)",
            "",
            "        # No need to do anything else if the firewall is inactive",
            "        if not utils.service_is_active(\"SuSEfirewall2\"):",
            "            return",
            "",
            "        # Firewall is active, either restart or complain if we couldn't tweak it",
            "        status(\"Restarting firewall (tcp={}, udp={})\".format(\" \".join(tcp), \" \".join(udp)))",
            "        if not invokerc(\"rcSuSEfirewall2 restart\"):",
            "            error(\"Failed to restart firewall (SuSEfirewall2)\")",
            "",
            "    def init_firewall_firewalld(tcp, udp):",
            "        has_firewalld = utils.service_is_active(\"firewalld\")",
            "        cmdbase = 'firewall-cmd --zone=public --permanent ' if has_firewalld else 'firewall-offline-cmd --zone=public '",
            "",
            "        def cmd(args):",
            "            if not invokerc(cmdbase + args):",
            "                error(\"Failed to configure firewall.\")",
            "",
            "        for p in tcp:",
            "            cmd(\"--add-port={}/tcp\".format(p))",
            "",
            "        for p in udp:",
            "            cmd(\"--add-port={}/udp\".format(p))",
            "",
            "        if has_firewalld:",
            "            if not invokerc(\"firewall-cmd --reload\"):",
            "                error(\"Failed to reload firewall configuration.\")",
            "",
            "    def init_firewall_ufw(tcp, udp):",
            "        \"\"\"",
            "        try configuring firewall with ufw",
            "        \"\"\"",
            "        for p in tcp:",
            "            if not invokerc(\"ufw allow {}/tcp\".format(p)):",
            "                error(\"Failed to configure firewall (ufw)\")",
            "        for p in udp:",
            "            if not invokerc(\"ufw allow {}/udp\".format(p)):",
            "                error(\"Failed to configure firewall (ufw)\")",
            "",
            "    if utils.package_is_installed(\"firewalld\"):",
            "        init_firewall_firewalld(tcp, udp)",
            "    elif utils.package_is_installed(\"SuSEfirewall2\"):",
            "        init_firewall_suse(tcp, udp)",
            "    elif utils.package_is_installed(\"ufw\"):",
            "        init_firewall_ufw(tcp, udp)",
            "    else:",
            "        warn(\"Failed to detect firewall: Could not open ports tcp={}, udp={}\".format(\"|\".join(tcp), \"|\".join(udp)))",
            "",
            "",
            "def firewall_open_basic_ports():",
            "    \"\"\"",
            "    Open ports for csync2, mgmtd, hawk & dlm respectively",
            "    \"\"\"",
            "    configure_firewall(tcp=[\"30865\", \"5560\", \"7630\", \"21064\"])",
            "",
            "",
            "def firewall_open_corosync_ports():",
            "    \"\"\"",
            "    Have to do this separately, as we need general firewall config early",
            "    so csync2 works, but need corosync config *after* corosync.conf has",
            "    been created/updated.",
            "",
            "    Please note corosync uses two UDP ports mcastport (for mcast",
            "    receives) and mcastport - 1 (for mcast sends).",
            "",
            "    Also open QNetd/QDevice port if configured.",
            "    \"\"\"",
            "    # all mcastports defined in corosync config",
            "    udp = corosync.get_values(\"totem.interface.mcastport\")",
            "    udp.extend([str(int(p) - 1) for p in udp])",
            "",
            "    tcp = corosync.get_values(\"totem.quorum.device.net.port\")",
            "",
            "    configure_firewall(tcp=tcp, udp=udp)",
            "",
            "",
            "def init_cluster_local():",
            "    # Caller should check this, but I'm paranoid...",
            "    if utils.service_is_active(\"corosync.service\"):",
            "        error(\"corosync service is running!\")",
            "",
            "    firewall_open_corosync_ports()",
            "",
            "    # reset password, but only if it's not already set",
            "    _rc, outp = utils.get_stdout(\"passwd -S hacluster\")",
            "    ps = outp.strip().split()[1]",
            "    pass_msg = \"\"",
            "    if ps not in (\"P\", \"PS\"):",
            "        log(': Resetting password of hacluster user')",
            "        rc, outp, errp = utils.get_stdout_stderr(\"passwd hacluster\", input_s=b\"linux\\nlinux\\n\")",
            "        if rc != 0:",
            "            warn(\"Failed to reset password of hacluster user: %s\" % (outp + errp))",
            "        else:",
            "            pass_msg = \", password 'linux'\"",
            "",
            "    # evil, but necessary",
            "    invoke(\"rm -f /var/lib/heartbeat/crm/* /var/lib/pacemaker/cib/*\")",
            "",
            "    # only try to start hawk if hawk is installed",
            "    if utils.service_is_available(\"hawk.service\"):",
            "        utils.start_service(\"hawk.service\", enable=True)",
            "        status(\"Hawk cluster interface is now running. To see cluster status, open:\")",
            "        status(\"  https://{}:7630/\".format(_context.default_ip_list[0]))",
            "        status(\"Log in with username 'hacluster'{}\".format(pass_msg))",
            "    else:",
            "        warn(\"Hawk not installed - not configuring web management interface.\")",
            "",
            "    if pass_msg:",
            "        warn(\"You should change the hacluster password to something more secure!\")",
            "",
            "    utils.start_service(\"pacemaker.service\", enable=True)",
            "    wait_for_cluster()",
            "",
            "",
            "def install_tmp(tmpfile, to):",
            "    with open(tmpfile, \"r\") as src:",
            "        with utils.open_atomic(to, \"w\") as dst:",
            "            for line in src:",
            "                dst.write(line)",
            "",
            "",
            "def append(fromfile, tofile):",
            "    log(\"+ cat %s >> %s\" % (fromfile, tofile))",
            "    with open(tofile, \"a\") as tf:",
            "        with open(fromfile, \"r\") as ff:",
            "            tf.write(ff.read())",
            "",
            "",
            "def append_unique(fromfile, tofile):",
            "    \"\"\"",
            "    Append unique content from fromfile to tofile",
            "    \"\"\"",
            "    if not utils.check_file_content_included(fromfile, tofile):",
            "        append(fromfile, tofile)",
            "",
            "",
            "def rmfile(path, ignore_errors=False):",
            "    \"\"\"",
            "    Try to remove the given file, and",
            "    report an error on failure",
            "    \"\"\"",
            "    try:",
            "        os.remove(path)",
            "    except os.error as err:",
            "        if not ignore_errors:",
            "            error(\"Failed to remove {}: {}\".format(path, err))",
            "",
            "",
            "def mkdirs_owned(dirs, mode=0o777, uid=-1, gid=-1):",
            "    \"\"\"",
            "    Create directory path, setting the mode and",
            "    ownership of the leaf directory to mode/uid/gid.",
            "    \"\"\"",
            "    if not os.path.exists(dirs):",
            "        try:",
            "            os.makedirs(dirs, mode)",
            "        except OSError as err:",
            "            error(\"Failed to create {}: {}\".format(dirs, err))",
            "        if uid != -1 or gid != -1:",
            "            utils.chown(dirs, uid, gid)",
            "",
            "",
            "def init_ssh():",
            "    \"\"\"",
            "    Configure passwordless SSH.",
            "    \"\"\"",
            "    utils.start_service(\"sshd.service\", enable=True)",
            "    configure_local_ssh_key()",
            "",
            "",
            "def configure_local_ssh_key():",
            "    \"\"\"",
            "    Configure ssh rsa key locally",
            "",
            "    If /root/.ssh/id_rsa not exist, generate a new one",
            "    Add /root/.ssh/id_rsa.pub to /root/.ssh/authorized_keys anyway, make sure itself authorized",
            "    \"\"\"",
            "    if not os.path.exists(RSA_PRIVATE_KEY):",
            "        status(\"Generating SSH key\")",
            "        invoke(\"ssh-keygen -q -f {} -C 'Cluster Internal on {}' -N ''\".format(RSA_PRIVATE_KEY, utils.this_node()))",
            "    if not os.path.exists(AUTHORIZED_KEYS_FILE):",
            "        open(AUTHORIZED_KEYS_FILE, 'w').close()",
            "    append_unique(RSA_PUBLIC_KEY, AUTHORIZED_KEYS_FILE)",
            "",
            "",
            "def init_ssh_remote():",
            "    \"\"\"",
            "    Called by ha-cluster-join",
            "    \"\"\"",
            "    authorized_keys_file = \"/root/.ssh/authorized_keys\"",
            "    if not os.path.exists(authorized_keys_file):",
            "        open(authorized_keys_file, 'w').close()",
            "    authkeys = open(authorized_keys_file, \"r+\")",
            "    authkeys_data = authkeys.read()",
            "    for key in (\"id_rsa\", \"id_dsa\", \"id_ecdsa\", \"id_ed25519\"):",
            "        fn = os.path.join(\"/root/.ssh\", key)",
            "        if not os.path.exists(fn):",
            "            continue",
            "        keydata = open(fn + \".pub\").read()",
            "        if keydata not in authkeys_data:",
            "            append(fn + \".pub\", authorized_keys_file)",
            "",
            "",
            "def append_to_remote_file(fromfile, remote_node, tofile):",
            "    \"\"\"",
            "    Append content of fromfile to tofile on remote_node",
            "    \"\"\"",
            "    err_details_string = \"\"\"",
            "    crmsh has no way to help you to setup up passwordless ssh among nodes at this time. ",
            "    As the hint, likely, `PasswordAuthentication` is 'no' in /etc/ssh/sshd_config. ",
            "    Given in this case, users must setup passwordless ssh beforehand, or change it to 'yes' and manage passwords properly",
            "    \"\"\"",
            "    cmd = \"cat {} | ssh -oStrictHostKeyChecking=no root@{} 'cat >> {}'\".format(fromfile, remote_node, tofile)",
            "    rc, _, err = invoke(cmd)",
            "    if not rc:",
            "        error(\"Failed to append contents of {} to {}:\\n\\\"{}\\\"\\n{}\".format(fromfile, remote_node, err, err_details_string))",
            "",
            "",
            "def init_csync2():",
            "    status(\"Configuring csync2\")",
            "    if os.path.exists(CSYNC2_KEY):",
            "        if not confirm(\"csync2 is already configured - overwrite?\"):",
            "            return",
            "",
            "    invoke(\"rm\", \"-f\", CSYNC2_KEY)",
            "    status_long(\"Generating csync2 shared key (this may take a while)\")",
            "    if not invokerc(\"csync2\", \"-k\", CSYNC2_KEY):",
            "        error(\"Can't create csync2 key {}\".format(CSYNC2_KEY))",
            "    status_done()",
            "",
            "    utils.str2file(\"\"\"group ha_group",
            "{",
            "key /etc/csync2/key_hagroup;",
            "host %s;",
            "include /etc/booth;",
            "include /etc/corosync/corosync.conf;",
            "include /etc/corosync/authkey;",
            "include /etc/csync2/csync2.cfg;",
            "include /etc/csync2/key_hagroup;",
            "include /etc/ctdb/nodes;",
            "include /etc/drbd.conf;",
            "include /etc/drbd.d;",
            "include /etc/ha.d/ldirectord.cf;",
            "include /etc/lvm/lvm.conf;",
            "include /etc/multipath.conf;",
            "include /etc/samba/smb.conf;",
            "include /etc/sysconfig/nfs;",
            "include /etc/sysconfig/pacemaker;",
            "include /etc/sysconfig/sbd;",
            "include /etc/pacemaker/authkey;",
            "include /etc/modules-load.d/watchdog.conf;",
            "}",
            "    \"\"\" % (utils.this_node()), CSYNC2_CFG)",
            "",
            "    utils.start_service(\"csync2.socket\", enable=True)",
            "    status_long(\"csync2 checking files\")",
            "    invoke(\"csync2\", \"-cr\", \"/\")",
            "    status_done()",
            "",
            "",
            "def csync2_update(path):",
            "    '''",
            "    Sync path to all peers",
            "",
            "    If there was a conflict, use '-f' to force this side to win",
            "    '''",
            "    invoke(\"csync2 -rm {}\".format(path))",
            "    if invokerc(\"csync2 -rxv {}\".format(path)):",
            "        return",
            "    invoke(\"csync2 -rf {}\".format(path))",
            "    if not invokerc(\"csync2 -rxv {}\".format(path)):",
            "        warn(\"{} was not synced\".format(path))",
            "",
            "",
            "def init_csync2_remote():",
            "    \"\"\"",
            "    It would be nice if we could just have csync2.cfg include a directory,",
            "    which in turn included one file per node which would be referenced via",
            "    something like \"group ha_group { ... config: /etc/csync2/hosts/*; }\"",
            "    That way, adding a new node would just mean adding a single new file",
            "    to that directory.  Unfortunately, the 'config' statement only allows",
            "    inclusion of specific individual files, not multiple files via wildcard.",
            "    So we have this function which is called by ha-cluster-join to add the new",
            "    remote node to csync2 config on some existing node.  It is intentionally",
            "    not documented in ha-cluster-init's user-visible usage information.",
            "    \"\"\"",
            "    newhost = _context.cluster_node",
            "    if not newhost:",
            "        error(\"Hostname not specified\")",
            "",
            "    curr_cfg = open(CSYNC2_CFG).read()",
            "",
            "    was_quiet = _context.quiet",
            "    try:",
            "        _context.quiet = True",
            "        # if host doesn't already exist in csync2 config, add it",
            "        if not re.search(r\"^\\s*host.*\\s+%s\\s*;\" % (newhost), curr_cfg, flags=re.M):",
            "            curr_cfg = re.sub(r\"\\bhost.*\\s+\\S+\\s*;\", r\"\\g<0>\\n\\thost %s;\" % (utils.doublequote(newhost)), curr_cfg, count=1)",
            "            utils.str2file(curr_cfg, CSYNC2_CFG)",
            "            csync2_update(\"/\")",
            "        else:",
            "            log(\": Not updating %s - remote host %s already exists\" % (CSYNC2_CFG, newhost))",
            "    finally:",
            "        _context.quiet = was_quiet",
            "",
            "",
            "def init_corosync_auth():",
            "    \"\"\"",
            "    Generate the corosync authkey",
            "    \"\"\"",
            "    if os.path.exists(COROSYNC_AUTH):",
            "        if not confirm(\"%s already exists - overwrite?\" % (COROSYNC_AUTH)):",
            "            return",
            "        rmfile(COROSYNC_AUTH)",
            "    invoke(\"corosync-keygen -l\")",
            "",
            "",
            "def init_remote_auth():",
            "    \"\"\"",
            "    Generate the pacemaker-remote authkey",
            "    \"\"\"",
            "    if os.path.exists(PCMK_REMOTE_AUTH):",
            "        if not confirm(\"%s already exists - overwrite?\" % (PCMK_REMOTE_AUTH)):",
            "            return",
            "        rmfile(PCMK_REMOTE_AUTH)",
            "",
            "    pcmk_remote_dir = os.path.dirname(PCMK_REMOTE_AUTH)",
            "    mkdirs_owned(pcmk_remote_dir, mode=0o750, gid=\"haclient\")",
            "    if not invokerc(\"dd if=/dev/urandom of={} bs=4096 count=1\".format(PCMK_REMOTE_AUTH)):",
            "        warn(\"Failed to create pacemaker authkey: {}\".format(PCMK_REMOTE_AUTH))",
            "    utils.chown(PCMK_REMOTE_AUTH, \"hacluster\", \"haclient\")",
            "    os.chmod(PCMK_REMOTE_AUTH, 0o640)",
            "",
            "",
            "class Validation(object):",
            "    \"\"\"",
            "    Class to validate values from interactive inputs",
            "    \"\"\"",
            "",
            "    def __init__(self, value, prev_value_list=[]):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self.value = value",
            "        self.prev_value_list = prev_value_list",
            "        if self.value in self.prev_value_list:",
            "            raise ValueError(\"Already in use: {}\".format(self.value))",
            "",
            "    def _is_mcast_addr(self):",
            "        \"\"\"",
            "        Check whether the address is multicast address",
            "        \"\"\"",
            "        if not utils.IP.is_mcast(self.value):",
            "            raise ValueError(\"{} is not multicast address\".format(self.value))",
            "",
            "    def _is_local_addr(self, local_addr_list):",
            "        \"\"\"",
            "        Check whether the address is in local",
            "        \"\"\"",
            "        if self.value not in local_addr_list:",
            "            raise ValueError(\"Address must be a local address (one of {})\".format(local_addr_list))",
            "",
            "    def _is_valid_port(self):",
            "        \"\"\"",
            "        Check whether the port is valid",
            "        \"\"\"",
            "        if self.prev_value_list and abs(int(self.value) - int(self.prev_value_list[0])) <= 1:",
            "            raise ValueError(\"Port {} is already in use by corosync. Leave a gap between multiple rings.\".format(self.value))",
            "        if int(self.value) <= 1024 or int(self.value) > 65535:",
            "            raise ValueError(\"Valid port range should be 1025-65535\")",
            "",
            "    @classmethod",
            "    def valid_mcast_address(cls, addr, prev_value_list=[]):",
            "        \"\"\"",
            "        Check whether the address is already in use and whether the address is for multicast",
            "        \"\"\"",
            "        cls_inst = cls(addr, prev_value_list)",
            "        cls_inst._is_mcast_addr()",
            "",
            "    @classmethod",
            "    def valid_ucast_ip(cls, addr, prev_value_list=[]):",
            "        \"\"\"",
            "        Check whether the address is already in use and whether the address exists on local",
            "        \"\"\"",
            "        cls_inst = cls(addr, prev_value_list)",
            "        cls_inst._is_local_addr(_context.local_ip_list)",
            "",
            "    @classmethod",
            "    def valid_mcast_ip(cls, addr, prev_value_list=[]):",
            "        \"\"\"",
            "        Check whether the address is already in use and whether the address exists on local address and network",
            "        \"\"\"",
            "        cls_inst = cls(addr, prev_value_list)",
            "        cls_inst._is_local_addr(_context.local_ip_list + _context.local_network_list)",
            "",
            "    @classmethod",
            "    def valid_port(cls, port, prev_value_list=[]):",
            "        \"\"\"",
            "        Check whether the port is valid",
            "        \"\"\"",
            "        cls_inst = cls(port, prev_value_list)",
            "        cls_inst._is_valid_port()",
            "",
            "    @staticmethod",
            "    def valid_admin_ip(addr, prev_value_list=[]):",
            "        \"\"\"",
            "        Validate admin IP address",
            "        \"\"\"",
            "        ipv6 = utils.IP.is_ipv6(addr)",
            "",
            "        # Check whether this IP already configured in cluster",
            "        ping_cmd = \"ping6\" if ipv6 else \"ping\"",
            "        if invokerc(\"{} -c 1 {}\".format(ping_cmd, addr)):",
            "            raise ValueError(\"Address already in use: {}\".format(addr))",
            "",
            "",
            "def init_corosync_unicast():",
            "",
            "    if _context.yes_to_all:",
            "        status(\"Configuring corosync (unicast)\")",
            "    else:",
            "        status(\"\"\"",
            "Configure Corosync (unicast):",
            "  This will configure the cluster messaging layer.  You will need",
            "  to specify a network address over which to communicate (default",
            "  is {}'s network, but you can use the network address of any",
            "  active interface).",
            "\"\"\".format(_context.default_nic_list[0]))",
            "",
            "    ringXaddr_res = []",
            "    mcastport_res = []",
            "    default_ports = [\"5405\", \"5407\"]",
            "    two_rings = False",
            "",
            "    for i in range(2):",
            "        ringXaddr = prompt_for_string(",
            "                'Address for ring{}'.format(i),",
            "                default=pick_default_value(_context.default_ip_list, ringXaddr_res),",
            "                valid_func=Validation.valid_ucast_ip,",
            "                prev_value=ringXaddr_res)",
            "        if not ringXaddr:",
            "            error(\"No value for ring{}\".format(i))",
            "        ringXaddr_res.append(ringXaddr)",
            "",
            "        mcastport = prompt_for_string(",
            "                'Port for ring{}'.format(i),",
            "                match='[0-9]+',",
            "                default=pick_default_value(default_ports, mcastport_res),",
            "                valid_func=Validation.valid_port,",
            "                prev_value=mcastport_res)",
            "        if not mcastport:",
            "            error(\"Expected a multicast port for ring{}\".format(i))",
            "        mcastport_res.append(mcastport)",
            "",
            "        if i == 1 or \\",
            "           not _context.second_heartbeat or \\",
            "           not confirm(\"\\nAdd another heartbeat line?\"):",
            "            break",
            "        two_rings = True",
            "",
            "    corosync.create_configuration(",
            "            clustername=_context.cluster_name,",
            "            ringXaddr=ringXaddr_res,",
            "            mcastport=mcastport_res,",
            "            transport=\"udpu\",",
            "            ipv6=_context.ipv6,",
            "            two_rings=two_rings)",
            "    csync2_update(corosync.conf())",
            "",
            "",
            "def init_corosync_multicast():",
            "    def gen_mcastaddr():",
            "        if _context.ipv6:",
            "            return \"ff3e::%s:%d\" % (",
            "                ''.join([random.choice('0123456789abcdef') for _ in range(4)]),",
            "                random.randint(0, 9))",
            "        return \"239.%d.%d.%d\" % (",
            "            random.randint(0, 255),",
            "            random.randint(0, 255),",
            "            random.randint(1, 255))",
            "",
            "    if _context.yes_to_all:",
            "        status(\"Configuring corosync\")",
            "    else:",
            "        status(\"\"\"",
            "Configure Corosync:",
            "  This will configure the cluster messaging layer.  You will need",
            "  to specify a network address over which to communicate (default",
            "  is {}'s network, but you can use the network address of any",
            "  active interface).",
            "\"\"\".format(_context.default_nic_list[0]))",
            "",
            "    bindnetaddr_res = []",
            "    mcastaddr_res = []",
            "    mcastport_res = []",
            "    default_ports = [\"5405\", \"5407\"]",
            "    two_rings = False",
            "",
            "    for i in range(2):",
            "        bindnetaddr = prompt_for_string(",
            "                'IP or network address to bind to',",
            "                default=pick_default_value(_context.default_ip_list, bindnetaddr_res),",
            "                valid_func=Validation.valid_mcast_ip,",
            "                prev_value=bindnetaddr_res)",
            "        if not bindnetaddr:",
            "            error(\"No value for bindnetaddr\")",
            "        bindnetaddr_res.append(bindnetaddr)",
            "",
            "        mcastaddr = prompt_for_string(",
            "                'Multicast address',",
            "                default=gen_mcastaddr(),",
            "                valid_func=Validation.valid_mcast_address,",
            "                prev_value=mcastaddr_res)",
            "        if not mcastaddr:",
            "            error(\"No value for mcastaddr\")",
            "        mcastaddr_res.append(mcastaddr)",
            "",
            "        mcastport = prompt_for_string(",
            "                'Multicast port',",
            "                match='[0-9]+',",
            "                default=pick_default_value(default_ports, mcastport_res),",
            "                valid_func=Validation.valid_port,",
            "                prev_value=mcastport_res)",
            "        if not mcastport:",
            "            error(\"No value for mcastport\")",
            "        mcastport_res.append(mcastport)",
            "",
            "        if i == 1 or \\",
            "           not _context.second_heartbeat or \\",
            "           not confirm(\"\\nConfigure a second multicast ring?\"):",
            "            break",
            "        two_rings = True",
            "",
            "    nodeid = None",
            "    if _context.ipv6:",
            "        nodeid = utils.gen_nodeid_from_ipv6(_context.default_ip_list[0])",
            "",
            "    corosync.create_configuration(",
            "        clustername=_context.cluster_name,",
            "        bindnetaddr=bindnetaddr_res,",
            "        mcastaddr=mcastaddr_res,",
            "        mcastport=mcastport_res,",
            "        ipv6=_context.ipv6,",
            "        nodeid=nodeid,",
            "        two_rings=two_rings)",
            "    csync2_update(corosync.conf())",
            "",
            "",
            "def init_corosync():",
            "    \"\"\"",
            "    Configure corosync (unicast or multicast, encrypted?)",
            "    \"\"\"",
            "    def requires_unicast():",
            "        host = utils.detect_cloud()",
            "        if host is not None:",
            "            status(\"Detected cloud platform: {}\".format(host))",
            "        return host is not None",
            "",
            "    init_corosync_auth()",
            "",
            "    if os.path.exists(corosync.conf()):",
            "        if not confirm(\"%s already exists - overwrite?\" % (corosync.conf())):",
            "            return",
            "",
            "    if _context.unicast or requires_unicast():",
            "        init_corosync_unicast()",
            "    else:",
            "        init_corosync_multicast()",
            "",
            "",
            "def is_block_device(dev):",
            "    from stat import S_ISBLK",
            "    try:",
            "        rc = S_ISBLK(os.stat(dev).st_mode)",
            "    except OSError:",
            "        return False",
            "    return rc",
            "",
            "",
            "def list_partitions(dev):",
            "    rc, outp, errp = utils.get_stdout_stderr(\"parted -s %s print\" % (dev))",
            "    partitions = []",
            "    for line in outp.splitlines():",
            "        m = re.match(r\"^\\s*([0-9]+)\\s*\", line)",
            "        if m:",
            "            partitions.append(m.group(1))",
            "    if rc != 0:",
            "        # ignore \"Error: /dev/vdb: unrecognised disk label\"",
            "        if errp.count('\\n') > 1 or \"unrecognised disk label\" not in errp.strip():",
            "            error(\"Failed to list partitions in {}: {}\".format(dev, errp))",
            "    return partitions",
            "",
            "",
            "def list_devices(dev):",
            "    \"TODO: THIS IS *WRONG* FOR MULTIPATH! (but possibly nothing we can do about it)\"",
            "    _rc, outp = utils.get_stdout(\"fdisk -l %s\" % (dev))",
            "    partitions = []",
            "    for line in outp.splitlines():",
            "        m = re.match(r\"^(\\/dev\\S+)\", line)",
            "        if m:",
            "            partitions.append(m.group(1))",
            "    return partitions",
            "",
            "",
            "def init_storage():",
            "    \"\"\"",
            "    Configure SBD and OCFS2 both on the same storage device.",
            "    \"\"\"",
            "    dev = _context.shared_device",
            "    partitions = []",
            "    dev_looks_sane = False",
            "",
            "    if _context.yes_to_all or not dev:",
            "        status(\"Configuring shared storage\")",
            "    else:",
            "        status(\"\"\"",
            "Configure Shared Storage:",
            "  You will need to provide the path to a shared storage device,",
            "  for example a SAN volume or iSCSI target.  The device path must",
            "  be persistent and consistent across all nodes in the cluster,",
            "  so /dev/disk/by-id/* devices are a good choice.  This device",
            "  will be automatically paritioned into two pieces, 1MB for SBD",
            "  fencing, and the remainder for an OCFS2 filesystem.",
            "\"\"\")",
            "",
            "    while not dev_looks_sane:",
            "        dev = prompt_for_string('Path to storage device (e.g. /dev/disk/by-id/...)', r'\\/.*', dev)",
            "        if not dev:",
            "            error(\"No value for shared storage device\")",
            "",
            "        if not is_block_device(dev):",
            "            if _context.yes_to_all:",
            "                error(dev + \" is not a block device\")",
            "            else:",
            "                print(\"    That doesn't look like a block device\", file=sys.stderr)",
            "        else:",
            "            #",
            "            # Got something that looks like a block device, there",
            "            # are four possibilities now:",
            "            #",
            "            #  1) It's completely broken/inaccessible",
            "            #  2) No recognizable partition table",
            "            #  3) Empty partition table",
            "            #  4) Non-empty parition table",
            "            #",
            "            partitions = list_partitions(dev)",
            "            if partitions:",
            "                status(\"WARNING: Partitions exist on %s!\" % (dev))",
            "                if confirm(\"Are you ABSOLUTELY SURE you want to overwrite?\"):",
            "                    dev_looks_sane = True",
            "                else:",
            "                    dev = \"\"",
            "            else:",
            "                # It's either broken, no partition table, or empty partition table",
            "                status(\"%s appears to be empty\" % (dev))",
            "                if confirm(\"Device appears empty (no partition table). Do you want to use {}?\".format(dev)):",
            "                    dev_looks_sane = True",
            "                else:",
            "                    dev = \"\"",
            "",
            "    if partitions:",
            "        if not confirm(\"Really?\"):",
            "            return",
            "        status_long(\"Erasing existing partitions...\")",
            "        for part in partitions:",
            "            if not invokerc(\"parted -s %s rm %s\" % (dev, part)):",
            "                error(\"Failed to remove partition %s from %s\" % (part, dev))",
            "        status_done()",
            "",
            "    status_long(\"Creating partitions...\")",
            "    if not invokerc(\"parted\", \"-s\", dev, \"mklabel\", \"msdos\"):",
            "        error(\"Failed to create partition table\")",
            "",
            "    # This is a bit rough, and probably won't result in great performance,",
            "    # but it's fine for test/demo purposes to carve off 1MB for SBD.  Note",
            "    # we have to specify the size of the first partition in this in bytes",
            "    # rather than MB, or parted's rounding gives us a ~30Kb partition",
            "    # (see rhbz#623268).",
            "    if not invokerc(\"parted -s %s mkpart primary 0 1048576B\" % (dev)):",
            "        error(\"Failed to create first partition on %s\" % (dev))",
            "    if not invokerc(\"parted -s %s mkpart primary 1M 100%%\" % (dev)):",
            "        error(\"Failed to create second partition\")",
            "",
            "    status_done()",
            "",
            "    # TODO: May not be strictly necessary, but...",
            "    probe_partitions()",
            "",
            "    # TODO: THIS IS *WRONG* FOR MULTIPATH! (but possibly nothing we can do about it)",
            "    devices = list_devices(dev)",
            "",
            "    _context.sbd_device = devices[0]",
            "    if not _context.sbd_device:",
            "        error(\"Unable to determine device path for SBD partition\")",
            "",
            "    _context.ocfs2_device = devices[1]",
            "    if not _context.ocfs2_device:",
            "        error(\"Unable to determine device path for OCFS2 partition\")",
            "",
            "    status(\"Created %s for SBD partition\" % (_context.sbd_device))",
            "    status(\"Created %s for OCFS2 partition\" % (_context.ocfs2_device))",
            "",
            "",
            "def init_sbd():",
            "    \"\"\"",
            "    Configure SBD (Storage-based fencing).",
            "",
            "    SBD can also run in diskless mode if no device",
            "    is configured.",
            "    \"\"\"",
            "    _context.sbd_manager.sbd_init()",
            "",
            "",
            "def init_cluster():",
            "    \"\"\"",
            "    Initial cluster configuration.",
            "    \"\"\"",
            "    init_cluster_local()",
            "",
            "    _rc, nnodes = utils.get_stdout(\"crm_node -l\")",
            "    nnodes = len(nnodes.splitlines())",
            "    if nnodes < 1:",
            "        error(\"No nodes found in cluster\")",
            "    if nnodes > 1:",
            "        error(\"Joined existing cluster - will not reconfigure.\")",
            "",
            "    status(\"Loading initial cluster configuration\")",
            "",
            "    crm_configure_load(\"update\", \"\"\"",
            "property cib-bootstrap-options: stonith-enabled=false",
            "op_defaults op-options: timeout=600 record-pending=true",
            "rsc_defaults rsc-options: resource-stickiness=1 migration-threshold=3",
            "\"\"\")",
            "",
            "    _context.sbd_manager.configure_sbd_resource()",
            "",
            "",
            "def init_vgfs():",
            "    \"\"\"",
            "    Configure cluster OCFS2 device.",
            "    \"\"\"",
            "    dev = _context.ocfs2_device",
            "    if not dev:",
            "        error(\"vgfs stage requires -o <dev>\")",
            "    mntpoint = \"/srv/clusterfs\"",
            "",
            "    if not is_block_device(dev):",
            "        error(\"OCFS2 device \\\"{}\\\" does not exist\".format(dev))",
            "",
            "    # TODO: configurable mountpoint and vg name",
            "    crm_configure_load(\"update\", \"\"\"",
            "primitive dlm ocf:pacemaker:controld op start timeout=90 op stop timeout=100 op monitor interval=60 timeout=60",
            "primitive clusterfs Filesystem directory=%(mntpoint)s fstype=ocfs2 device=%(dev)s \\",
            "    op monitor interval=20 timeout=40 op start timeout=60 op stop timeout=60 \\",
            "    meta target-role=Stopped",
            "clone base-clone dlm meta interleave=true",
            "clone c-clusterfs clusterfs meta interleave=true clone-max=8",
            "order base-then-clusterfs inf: base-clone c-clusterfs",
            "colocation clusterfs-with-base inf: c-clusterfs base-clone",
            "    \"\"\" % {\"mntpoint\": utils.doublequote(mntpoint), \"dev\": utils.doublequote(dev)})",
            "",
            "    wait_for_resource(\"Waiting for DLM\", \"dlm:0\")",
            "    wait_for_stop(\"Making sure filesystem is not active\", \"clusterfs:0\")",
            "",
            "    _rc, blkid, _err = utils.get_stdout_stderr(\"blkid %s\" % (dev))",
            "    if \"TYPE\" in blkid:",
            "        if not confirm(\"Exiting filesystem found on \\\"{}\\\" - destroy?\".format(dev)):",
            "            for res in (\"base-clone\", \"c-clusterfs\"):",
            "                invoke(\"crm resource stop %s\" % (res))",
            "                wait_for_stop(\"Waiting for resource %s to stop\" % (res), res)",
            "            invoke(\"crm configure delete dlm clusterfs base-group base-clone c-clusterfs base-then-clusterfs clusterfs-with-base\")",
            "",
            "    status_long(\"Creating OCFS2 filesystem\")",
            "    # TODO: want \"-T vmstore\", but this'll only fly on >2GB partition",
            "    # Note: using undocumented '-x' switch to avoid prompting if overwriting",
            "    # existing partition.  For the commit that introduced this, see:",
            "    # http://oss.oracle.com/git/?p=ocfs2-tools.git;a=commit;h=8345a068479196172190f4fa287052800fa2b66f",
            "    # TODO: if make the cluster name configurable, we need to update it here too",
            "    if not invokerc(\"mkfs.ocfs2 --cluster-stack pcmk --cluster-name %s -N 8 -x %s\" % (_context.cluster_name, dev)):",
            "        error(\"Failed to create OCFS2 filesystem on %s\" % (dev))",
            "    status_done()",
            "",
            "    # TODO: refactor, maybe",
            "    if not invokerc(\"mkdir -p %s\" % (mntpoint)):",
            "        error(\"Can't create mountpoint %s\" % (mntpoint))",
            "    if not invokerc(\"crm resource meta clusterfs delete target-role\"):",
            "        error(\"Can't start cluster filesystem clone\")",
            "    wait_for_resource(\"Waiting for %s to be mounted\" % (mntpoint), \"clusterfs:0\")",
            "",
            "",
            "def init_admin():",
            "    # Skip this section when -y is passed",
            "    # unless $ADMIN_IP is set",
            "    adminaddr = _context.admin_ip",
            "    if _context.yes_to_all and not adminaddr:",
            "        return",
            "",
            "    if not adminaddr:",
            "        status(\"\"\"",
            "Configure Administration IP Address:",
            "  Optionally configure an administration virtual IP",
            "  address. The purpose of this IP address is to",
            "  provide a single IP that can be used to interact",
            "  with the cluster, rather than using the IP address",
            "  of any specific cluster node.",
            "\"\"\")",
            "        if not confirm(\"Do you wish to configure a virtual IP address?\"):",
            "            return",
            "",
            "        adminaddr = prompt_for_string('Virtual IP', valid_func=Validation.valid_admin_ip)",
            "        if not adminaddr:",
            "            error(\"Expected an IP address\")",
            "",
            "    crm_configure_load(\"update\", 'primitive admin-ip IPaddr2 ip=%s op monitor interval=10 timeout=20' % (utils.doublequote(adminaddr)))",
            "    wait_for_resource(\"Configuring virtual IP ({})\".format(adminaddr), \"admin-ip\")",
            "",
            "",
            "def init_qdevice():",
            "    \"\"\"",
            "    Setup qdevice and qnetd service",
            "    \"\"\"",
            "    # If don't want to config qdevice, return",
            "    if not _context.qdevice_inst:",
            "        utils.disable_service(\"corosync-qdevice.service\")",
            "        return",
            "",
            "    status(\"\"\"",
            "Configure Qdevice/Qnetd:\"\"\")",
            "    qdevice_inst = _context.qdevice_inst",
            "    qnetd_addr = qdevice_inst.qnetd_addr",
            "    # Configure ssh passwordless to qnetd if detect password is needed",
            "    if utils.check_ssh_passwd_need(qnetd_addr):",
            "        status(\"Copy ssh key to qnetd node({})\".format(qnetd_addr))",
            "        rc, _, err = invoke(\"ssh-copy-id -i /root/.ssh/id_rsa.pub root@{}\".format(qnetd_addr))",
            "        if not rc:",
            "            error(\"Failed to copy ssh key: {}\".format(err))",
            "    # Start qdevice service if qdevice already configured",
            "    if utils.is_qdevice_configured() and not confirm(\"Qdevice is already configured - overwrite?\"):",
            "        start_qdevice_service()",
            "        return",
            "",
            "    # Validate qnetd node",
            "    qdevice_inst.valid_qnetd()",
            "    # Config qdevice",
            "    config_qdevice()",
            "    # Execute certificate process when tls flag is on",
            "    if utils.is_qdevice_tls_on():",
            "        status_long(\"Qdevice certification process\")",
            "        qdevice_inst.certificate_process_on_init()",
            "        status_done()",
            "",
            "    start_qdevice_service()",
            "",
            "",
            "def start_qdevice_service():",
            "    \"\"\"",
            "    Start qdevice and qnetd service",
            "    \"\"\"",
            "    qdevice_inst = _context.qdevice_inst",
            "    qnetd_addr = qdevice_inst.qnetd_addr",
            "",
            "    status(\"Enable corosync-qdevice.service in cluster\")",
            "    utils.cluster_run_cmd(\"systemctl enable corosync-qdevice\")",
            "    status(\"Starting corosync-qdevice.service in cluster\")",
            "    utils.cluster_run_cmd(\"systemctl start corosync-qdevice\")",
            "",
            "    status(\"Enable corosync-qnetd.service on {}\".format(qnetd_addr))",
            "    qdevice_inst.enable_qnetd()",
            "    status(\"Starting corosync-qnetd.service on {}\".format(qnetd_addr))",
            "    qdevice_inst.start_qnetd()",
            "",
            "",
            "def config_qdevice():",
            "    \"\"\"",
            "    Process of config qdevice",
            "    \"\"\"",
            "    qdevice_inst = _context.qdevice_inst",
            "",
            "    qdevice_inst.remove_qdevice_db()",
            "    qdevice_inst.write_qdevice_config()",
            "    if not corosync.is_unicast():",
            "        corosync.add_nodelist_from_cmaptool()",
            "    status_long(\"Update configuration\")",
            "    update_expected_votes()",
            "    utils.cluster_run_cmd(\"crm corosync reload\")",
            "    status_done()",
            "",
            "",
            "def init():",
            "    \"\"\"",
            "    Basic init",
            "    \"\"\"",
            "    log_start()",
            "    init_network()",
            "",
            "",
            "def join_ssh(seed_host):",
            "    \"\"\"",
            "    SSH configuration for joining node.",
            "    \"\"\"",
            "    if not seed_host:",
            "        error(\"No existing IP/hostname specified (use -c option)\")",
            "",
            "    utils.start_service(\"sshd.service\", enable=True)",
            "    configure_local_ssh_key()",
            "    swap_public_ssh_key(seed_host)",
            "",
            "    # This makes sure the seed host has its own SSH keys in its own",
            "    # authorized_keys file (again, to help with the case where the",
            "    # user has done manual initial setup without the assistance of",
            "    # ha-cluster-init).",
            "    rc, _, err = invoke(\"ssh root@{} crm cluster init -i {} ssh_remote\".format(seed_host, _context.default_nic_list[0]))",
            "    if not rc:",
            "        error(\"Can't invoke crm cluster init -i {} ssh_remote on {}: {}\".format(_context.default_nic_list[0], seed_host, err))",
            "",
            "",
            "def swap_public_ssh_key(remote_node):",
            "    \"\"\"",
            "    Swap public ssh key between remote_node and local",
            "    \"\"\"",
            "    # Detect whether need password to login to remote_node",
            "    if utils.check_ssh_passwd_need(remote_node):",
            "        # If no passwordless configured, paste /root/.ssh/id_rsa.pub to remote_node's /root/.ssh/authorized_keys",
            "        status(\"Configuring SSH passwordless with root@{}\".format(remote_node))",
            "        # After this, login to remote_node is passwordless",
            "        append_to_remote_file(RSA_PUBLIC_KEY, remote_node, AUTHORIZED_KEYS_FILE)",
            "",
            "    try:",
            "        # Fetch public key file from remote_node",
            "        public_key_file_remote = fetch_public_key_from_remote_node(remote_node)",
            "    except ValueError as err:",
            "        warn(err)",
            "        return",
            "    # Append public key file from remote_node to local's /root/.ssh/authorized_keys",
            "    # After this, login from remote_node is passwordless",
            "    # Should do this step even passwordless is True, to make sure we got two-way passwordless",
            "    append_unique(public_key_file_remote, AUTHORIZED_KEYS_FILE)",
            "",
            "",
            "def fetch_public_key_from_remote_node(node):",
            "    \"\"\"",
            "    Fetch public key file from remote node",
            "    Return a temp file contains public key",
            "    Return None if no key exist",
            "    \"\"\"",
            "",
            "    # For dsa, might need to add PubkeyAcceptedKeyTypes=+ssh-dss to config file, see",
            "    # https://superuser.com/questions/1016989/ssh-dsa-keys-no-longer-work-for-password-less-authentication",
            "    for key in (\"id_rsa\", \"id_ecdsa\", \"id_ed25519\", \"id_dsa\"):",
            "        public_key_file = \"/root/.ssh/{}.pub\".format(key)",
            "        cmd = \"ssh -oStrictHostKeyChecking=no root@{} 'test -f {}'\".format(node, public_key_file)",
            "        if not invokerc(cmd):",
            "            continue",
            "        _, temp_public_key_file = tmpfiles.create()",
            "        cmd = \"scp -oStrictHostKeyChecking=no root@{}:{} {}\".format(node, public_key_file, temp_public_key_file)",
            "        rc, _, err = invoke(cmd)",
            "        if not rc:",
            "            error(\"Failed to run \\\"{}\\\": {}\".format(cmd, err))",
            "        return temp_public_key_file",
            "    raise ValueError(\"No ssh key exist on {}\".format(node))",
            "",
            "",
            "def join_csync2(seed_host):",
            "    \"\"\"",
            "    Csync2 configuration for joining node.",
            "    \"\"\"",
            "    if not seed_host:",
            "        error(\"No existing IP/hostname specified (use -c option)\")",
            "    status_long(\"Configuring csync2\")",
            "",
            "    # Necessary if re-running join on a node that's been configured before.",
            "    rmfile(\"/var/lib/csync2/{}.db3\".format(utils.this_node()), ignore_errors=True)",
            "",
            "    # Not automatically updating /etc/hosts - risky in the general case.",
            "    # etc_hosts_add_me",
            "    # local hosts_line=$(etc_hosts_get_me)",
            "    # [ -n \"$hosts_line\" ] || error \"No valid entry for $(hostname) in /etc/hosts - csync2 can't work\"",
            "",
            "    # If we *were* updating /etc/hosts, the next line would have \"\\\"$hosts_line\\\"\" as",
            "    # the last arg (but this requires re-enabling this functionality in ha-cluster-init)",
            "    cmd = \"crm cluster init -i {} csync2_remote {}\".format(_context.default_nic_list[0], utils.this_node())",
            "    rc, _, err = invoke(\"ssh -o StrictHostKeyChecking=no root@{} {}\".format(seed_host, cmd))",
            "    if not rc:",
            "        error(\"Can't invoke \\\"{}\\\" on {}: {}\".format(cmd, seed_host, err))",
            "",
            "    # This is necessary if syncing /etc/hosts (to ensure everyone's got the",
            "    # same list of hosts)",
            "    # local tmp_conf=/etc/hosts.$$",
            "    # invoke scp root@seed_host:/etc/hosts $tmp_conf \\",
            "    #   || error \"Can't retrieve /etc/hosts from seed_host\"",
            "    # install_tmp $tmp_conf /etc/hosts",
            "    rc, _, err = invoke(\"scp root@%s:'/etc/csync2/{csync2.cfg,key_hagroup}' /etc/csync2\" % (seed_host))",
            "    if not rc:",
            "        error(\"Can't retrieve csync2 config from {}: {}\".format(seed_host, err))",
            "",
            "    utils.start_service(\"csync2.socket\", enable=True)",
            "",
            "    # Sync new config out.  This goes to all hosts; csync2.cfg definitely",
            "    # needs to go to all hosts (else hosts other than the seed and the",
            "    # joining host won't have the joining host in their config yet).",
            "    # Strictly, the rest of the files need only go to the new host which",
            "    # could theoretically be effected using `csync2 -xv -P $(hostname)`,",
            "    # but this still leaves all the other files in dirty state (becuase",
            "    # they haven't gone to all nodes in the cluster, which means a",
            "    # subseqent join of another node can fail its sync of corosync.conf",
            "    # when it updates expected_votes.  Grrr...",
            "    if not invokerc('ssh -o StrictHostKeyChecking=no root@{} \"csync2 -rm /; csync2 -rxv || csync2 -rf / && csync2 -rxv\"'.format(seed_host)):",
            "        print(\"\")",
            "        warn(\"csync2 run failed - some files may not be sync'd\")",
            "",
            "    status_done()",
            "",
            "",
            "def join_ssh_merge(_cluster_node):",
            "    status(\"Merging known_hosts\")",
            "",
            "    me = utils.this_node()",
            "    hosts = [m.group(1)",
            "             for m in re.finditer(r\"^\\s*host\\s*([^ ;]+)\\s*;\", open(CSYNC2_CFG).read(), re.M)",
            "             if m.group(1) != me]",
            "    if not hosts:",
            "        hosts = [_cluster_node]",
            "        warn(\"Unable to extract host list from %s\" % (CSYNC2_CFG))",
            "",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        error(\"parallax python library is missing\")",
            "",
            "    opts = parallax.Options()",
            "    opts.ssh_options = ['StrictHostKeyChecking=no']",
            "",
            "    # The act of using pssh to connect to every host (without strict host key",
            "    # checking) ensures that at least *this* host has every other host in its",
            "    # known_hosts",
            "    known_hosts_new = set()",
            "    cat_cmd = \"[ -e /root/.ssh/known_hosts ] && cat /root/.ssh/known_hosts || true\"",
            "    log(\"parallax.call {} : {}\".format(hosts, cat_cmd))",
            "    results = parallax.call(hosts, cat_cmd, opts)",
            "    for host, result in results.items():",
            "        if isinstance(result, parallax.Error):",
            "            warn(\"Failed to get known_hosts from {}: {}\".format(host, str(result)))",
            "        else:",
            "            if result[1]:",
            "                known_hosts_new.update((utils.to_ascii(result[1]) or \"\").splitlines())",
            "    if known_hosts_new:",
            "        hoststxt = \"\\n\".join(sorted(known_hosts_new))",
            "        tmpf = utils.str2tmp(hoststxt)",
            "        log(\"parallax.copy {} : {}\".format(hosts, hoststxt))",
            "        results = parallax.copy(hosts, tmpf, \"/root/.ssh/known_hosts\")",
            "        for host, result in results.items():",
            "            if isinstance(result, parallax.Error):",
            "                warn(\"scp to {} failed ({}), known_hosts update may be incomplete\".format(host, str(result)))",
            "",
            "",
            "def update_expected_votes():",
            "    # get a list of nodes, excluding remote nodes",
            "    nodelist = None",
            "    loop_count = 0",
            "    device_votes = 0",
            "    nodecount = 0",
            "    expected_votes = 0",
            "    while True:",
            "        rc, nodelist_text = utils.get_stdout(\"cibadmin -Ql --xpath '/cib/status/node_state'\")",
            "        if rc == 0:",
            "            try:",
            "                nodelist_xml = etree.fromstring(nodelist_text)",
            "                nodelist = [n.get('uname') for n in nodelist_xml.xpath('//node_state') if n.get('remote_node') != 'true']",
            "                if len(nodelist) >= 2:",
            "                    break",
            "            except Exception:",
            "                break",
            "        # timeout: 10 seconds",
            "        if loop_count == 10:",
            "            break",
            "        loop_count += 1",
            "        sleep(1)",
            "",
            "    # Increase expected_votes",
            "    # TODO: wait to adjust expected_votes until after cluster join,",
            "    # so that we can ask the cluster for the current membership list",
            "    # Have to check if a qnetd device is configured and increase",
            "    # expected_votes in that case",
            "    is_qdevice_configured = utils.is_qdevice_configured()",
            "    if nodelist is None:",
            "        for v in corosync.get_values(\"quorum.expected_votes\"):",
            "            expected_votes = v",
            "",
            "            # For node >= 2, expected_votes = nodecount + device_votes",
            "            # Assume nodecount is N, for ffsplit, qdevice only has one vote",
            "            # which means that device_votes is 1, ie:expected_votes = N + 1;",
            "            # while for lms, qdevice has N - 1 votes, ie: expected_votes = N + (N - 1)",
            "            # and update quorum.device.net.algorithm based on device_votes",
            "",
            "            if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":",
            "                device_votes = int((expected_votes - 1) / 2)",
            "                nodecount = expected_votes - device_votes",
            "                # as nodecount will increase 1, and device_votes is nodecount - 1",
            "                # device_votes also increase 1",
            "                device_votes += 1",
            "            elif corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":",
            "                device_votes = 1",
            "                nodecount = expected_votes - device_votes",
            "            elif is_qdevice_configured:",
            "                device_votes = 0",
            "                nodecount = v",
            "",
            "            nodecount += 1",
            "            expected_votes = nodecount + device_votes",
            "            corosync.set_value(\"quorum.expected_votes\", str(expected_votes))",
            "    else:",
            "        nodecount = len(nodelist)",
            "        expected_votes = 0",
            "        # For node >= 2, expected_votes = nodecount + device_votes",
            "        # Assume nodecount is N, for ffsplit, qdevice only has one vote",
            "        # which means that device_votes is 1, ie:expected_votes = N + 1;",
            "        # while for lms, qdevice has N - 1 votes, ie: expected_votes = N + (N - 1)",
            "        if corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":",
            "            device_votes = 1",
            "        if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":",
            "            device_votes = nodecount - 1",
            "",
            "        if nodecount > 1:",
            "            expected_votes = nodecount + device_votes",
            "",
            "        if corosync.get_value(\"quorum.expected_votes\"):",
            "            corosync.set_value(\"quorum.expected_votes\", str(expected_votes))",
            "    if is_qdevice_configured:",
            "        corosync.set_value(\"quorum.device.votes\", device_votes)",
            "    corosync.set_value(\"quorum.two_node\", 1 if expected_votes == 2 else 0)",
            "",
            "    csync2_update(corosync.conf())",
            "",
            "",
            "def setup_passwordless_with_other_nodes(init_node):",
            "    \"\"\"",
            "    Setup passwordless with other cluster nodes",
            "",
            "    Should fetch the node list from init node, then swap the key",
            "    \"\"\"",
            "    # Fetch cluster nodes list",
            "    cmd = \"ssh -o StrictHostKeyChecking=no root@{} crm_node -l\".format(init_node)",
            "    rc, out, err = utils.get_stdout_stderr(cmd)",
            "    if rc != 0:",
            "        error(\"Can't fetch cluster nodes list from {}: {}\".format(init_node, err))",
            "    cluster_nodes_list = []",
            "    for line in out.splitlines():",
            "        _, node, stat = line.split()",
            "        if stat == \"member\":",
            "            cluster_nodes_list.append(node)",
            "",
            "    # Filter out init node from cluster_nodes_list",
            "    cmd = \"ssh -o StrictHostKeyChecking=no root@{} hostname\".format(init_node)",
            "    rc, out, err = utils.get_stdout_stderr(cmd)",
            "    if rc != 0:",
            "        error(\"Can't fetch hostname of {}: {}\".format(init_node, err))",
            "    if out in cluster_nodes_list:",
            "        cluster_nodes_list.remove(out)",
            "",
            "    # Swap ssh public key between join node and other cluster nodes",
            "    for node in cluster_nodes_list:",
            "        swap_public_ssh_key(node)",
            "",
            "",
            "def join_cluster(seed_host):",
            "    \"\"\"",
            "    Cluster configuration for joining node.",
            "    \"\"\"",
            "    def get_local_nodeid():",
            "        # for IPv6",
            "        return utils.gen_nodeid_from_ipv6(_context.local_ip_list[0])",
            "",
            "    def update_nodeid(nodeid, node=None):",
            "        # for IPv6",
            "        if node and node != utils.this_node():",
            "            cmd = \"crm corosync set totem.nodeid %d\" % nodeid",
            "            invoke(\"crm cluster run '{}' {}\".format(cmd, node))",
            "        else:",
            "            corosync.set_value(\"totem.nodeid\", nodeid)",
            "",
            "    shutil.copy(corosync.conf(), COROSYNC_CONF_ORIG)",
            "",
            "    # check if use IPv6",
            "    ipv6_flag = False",
            "    ipv6 = corosync.get_value(\"totem.ip_version\")",
            "    if ipv6 and ipv6 == \"ipv6\":",
            "        ipv6_flag = True",
            "    _context.ipv6 = ipv6_flag",
            "",
            "    init_network()",
            "",
            "    # check whether have two rings",
            "    rrp_flag = False",
            "    rrp = corosync.get_value(\"totem.rrp_mode\")",
            "    if rrp in ('active', 'passive'):",
            "        rrp_flag = True",
            "",
            "    # Need to do this if second (or subsequent) node happens to be up and",
            "    # connected to storage while it's being repartitioned on the first node.",
            "    probe_partitions()",
            "",
            "    # It would be massively useful at this point if new nodes could come",
            "    # up in standby mode, so we could query the CIB locally to see if",
            "    # there was any further local setup that needed doing, e.g.: creating",
            "    # mountpoints for clustered filesystems.  Unfortunately we don't have",
            "    # that yet, so the following crawling horror takes a punt on the seed",
            "    # node being up, then asks it for a list of mountpoints...",
            "    if _context.cluster_node:",
            "        _rc, outp, _ = utils.get_stdout_stderr(\"ssh -o StrictHostKeyChecking=no root@{} 'cibadmin -Q --xpath \\\"//primitive\\\"'\".format(seed_host))",
            "        if outp:",
            "            xml = etree.fromstring(outp)",
            "            mountpoints = xml.xpath(' and '.join(['//primitive[@class=\"ocf\"',",
            "                                                  '@provider=\"heartbeat\"',",
            "                                                  '@type=\"Filesystem\"]']) +",
            "                                    '/instance_attributes/nvpair[@name=\"directory\"]/@value')",
            "            for m in mountpoints:",
            "                invoke(\"mkdir -p {}\".format(m))",
            "    else:",
            "        status(\"No existing IP/hostname specified - skipping mountpoint detection/creation\")",
            "",
            "    # Bump expected_votes in corosync.conf",
            "    # TODO(must): this is rather fragile (see related code in ha-cluster-remove)",
            "",
            "    # If corosync.conf() doesn't exist or is empty, we will fail here. (bsc#943227)",
            "    if not os.path.exists(corosync.conf()):",
            "        error(\"{} is not readable. Please ensure that hostnames are resolvable.\".format(corosync.conf()))",
            "",
            "    # if unicast, we need to add our node to $corosync.conf()",
            "    is_unicast = corosync.is_unicast()",
            "    if is_unicast:",
            "        ringXaddr_res = []",
            "        for i in 0, 1:",
            "            while True:",
            "                ringXaddr = prompt_for_string(",
            "                        'Address for ring{}'.format(i),",
            "                        default=pick_default_value(_context.default_ip_list, ringXaddr_res),",
            "                        valid_func=Validation.valid_ucast_ip,",
            "                        prev_value=ringXaddr_res)",
            "                if not ringXaddr:",
            "                    error(\"No value for ring{}\".format(i))",
            "                ringXaddr_res.append(ringXaddr)",
            "                break",
            "            if not rrp_flag:",
            "                break",
            "        print(\"\")",
            "        invoke(\"rm -f /var/lib/heartbeat/crm/* /var/lib/pacemaker/cib/*\")",
            "        try:",
            "            corosync.add_node_ucast(ringXaddr_res)",
            "        except corosync.IPAlreadyConfiguredError as e:",
            "            warn(e)",
            "        csync2_update(corosync.conf())",
            "        invoke(\"ssh -o StrictHostKeyChecking=no root@{} corosync-cfgtool -R\".format(seed_host))",
            "",
            "    _context.sbd_manager.join_sbd(seed_host)",
            "",
            "    if ipv6_flag and not is_unicast:",
            "        # for ipv6 mcast",
            "        # using ipv6 need nodeid configured",
            "        local_nodeid = get_local_nodeid()",
            "        update_nodeid(local_nodeid)",
            "",
            "    is_qdevice_configured = utils.is_qdevice_configured()",
            "    if is_qdevice_configured and not is_unicast:",
            "        # expected_votes here maybe is \"0\", set to \"3\" to make sure cluster can start",
            "        corosync.set_value(\"quorum.expected_votes\", \"3\")",
            "",
            "    # Initialize the cluster before adjusting quorum. This is so",
            "    # that we can query the cluster to find out how many nodes",
            "    # there are (so as not to adjust multiple times if a previous",
            "    # attempt to join the cluster failed)",
            "    init_cluster_local()",
            "",
            "    status_long(\"Reloading cluster configuration\")",
            "",
            "    if ipv6_flag and not is_unicast:",
            "        # for ipv6 mcast",
            "        nodeid_dict = {}",
            "        _rc, outp, _ = utils.get_stdout_stderr(\"crm_node -l\")",
            "        if _rc == 0:",
            "            for line in outp.split('\\n'):",
            "                tmp = line.split()",
            "                nodeid_dict[tmp[1]] = tmp[0]",
            "",
            "    # apply nodelist in cluster",
            "    if is_unicast or is_qdevice_configured:",
            "        invoke(\"crm cluster run 'crm corosync reload'\")",
            "",
            "    update_expected_votes()",
            "    # Trigger corosync config reload to ensure expected_votes is propagated",
            "    invoke(\"corosync-cfgtool -R\")",
            "",
            "    # Ditch no-quorum-policy=ignore",
            "    _rc, outp = utils.get_stdout(\"crm configure show\")",
            "    if re.search('no-quorum-policy=.*ignore', outp):",
            "        invoke(\"crm_attribute --attr-name no-quorum-policy --delete-attr\")",
            "",
            "    # if unicast, we need to reload the corosync configuration",
            "    # on the other nodes",
            "    if is_unicast:",
            "        invoke(\"crm cluster run 'crm corosync reload'\")",
            "",
            "    if ipv6_flag and not is_unicast:",
            "        # for ipv6 mcast",
            "        # after csync2_update, all config files are same",
            "        # but nodeid must be uniqe",
            "        for node in list(nodeid_dict.keys()):",
            "            if node == utils.this_node():",
            "                continue",
            "            update_nodeid(int(nodeid_dict[node]), node)",
            "        update_nodeid(local_nodeid)",
            "    status_done()",
            "",
            "    if is_qdevice_configured:",
            "        start_qdevice_on_join_node(seed_host)",
            "    else:",
            "        utils.disable_service(\"corosync-qdevice.service\")",
            "",
            "",
            "def start_qdevice_on_join_node(seed_host):",
            "    \"\"\"",
            "    Doing qdevice certificate process and start qdevice service on join node",
            "    \"\"\"",
            "    status_long(\"Starting corosync-qdevice.service\")",
            "    if not corosync.is_unicast():",
            "        corosync.add_nodelist_from_cmaptool()",
            "        csync2_update(corosync.conf())",
            "        invoke(\"crm corosync reload\")",
            "    if utils.is_qdevice_tls_on():",
            "        qnetd_addr = corosync.get_value(\"quorum.device.net.host\")",
            "        qdevice_inst = corosync.QDevice(qnetd_addr, cluster_node=seed_host)",
            "        qdevice_inst.certificate_process_on_join()",
            "    utils.start_service(\"corosync-qdevice.service\", enable=True)",
            "    status_done()",
            "",
            "",
            "def set_cluster_node_ip():",
            "    \"\"\"",
            "    ringx_addr might be hostname or IP",
            "    _context.cluster_node by now is always hostname",
            "",
            "    If ring0_addr is IP, we should get the configured iplist which belong _context.cluster_node",
            "    Then filter out which one is configured as ring0_addr",
            "    At last assign that ip to _context.cluster_node_ip which will be removed later",
            "    \"\"\"",
            "    node = _context.cluster_node",
            "    addr_list = corosync.get_values('nodelist.node.ring0_addr')",
            "    if node in addr_list:",
            "        return",
            "",
            "    ip_list = utils.get_iplist_from_name(node)",
            "    for ip in ip_list:",
            "        if ip in addr_list:",
            "            _context.cluster_node_ip = ip",
            "            break",
            "",
            "",
            "def stop_services(stop_list, remote_addr=None):",
            "    \"\"\"",
            "    Stop cluster related service",
            "    \"\"\"",
            "    for service in stop_list:",
            "        if utils.service_is_active(service, remote_addr=remote_addr):",
            "            status(\"Stopping the {}\".format(service))",
            "            utils.stop_service(service, disable=True, remote_addr=remote_addr)",
            "",
            "",
            "def remove_node_from_cluster():",
            "    \"\"\"",
            "    Remove node from running cluster and the corosync / pacemaker configuration.",
            "    \"\"\"",
            "    node = _context.cluster_node",
            "    set_cluster_node_ip()",
            "",
            "    stop_services(SERVICES_STOP_LIST, remote_addr=node)",
            "",
            "    # delete configuration files from the node to be removed",
            "    rc, _, err = invoke('ssh -o StrictHostKeyChecking=no root@{} \"bash -c \\\\\\\"rm -f {}\\\\\\\"\"'.format(node, \" \".join(_context.rm_list)))",
            "    if not rc:",
            "        error(\"Deleting the configuration files failed: {}\".format(err))",
            "",
            "    # execute the command : crm node delete $HOSTNAME",
            "    status(\"Removing the node {}\".format(node))",
            "    if not invokerc(\"crm node delete {}\".format(node)):",
            "        error(\"Failed to remove {}\".format(node))",
            "",
            "    if not invokerc(\"sed -i /{}/d {}\".format(node, CSYNC2_CFG)):",
            "        error(\"Removing the node {} from {} failed\".format(node, CSYNC2_CFG))",
            "",
            "    # Remove node from nodelist",
            "    if corosync.get_values(\"nodelist.node.ring0_addr\"):",
            "        del_target = _context.cluster_node_ip or node",
            "        corosync.del_node(del_target)",
            "",
            "    decrease_expected_votes()",
            "",
            "    status(\"Propagating configuration changes across the remaining nodes\")",
            "    csync2_update(CSYNC2_CFG)",
            "    csync2_update(corosync.conf())",
            "",
            "    # Trigger corosync config reload to ensure expected_votes is propagated",
            "    invoke(\"corosync-cfgtool -R\")",
            "",
            "",
            "def decrease_expected_votes():",
            "    '''",
            "    Decrement expected_votes in corosync.conf",
            "    '''",
            "    vote = corosync.get_value(\"quorum.expected_votes\")",
            "    if not vote:",
            "        return",
            "    quorum = int(vote)",
            "    new_quorum = quorum - 1",
            "    if utils.is_qdevice_configured():",
            "        new_nodecount = 0",
            "        device_votes = 0",
            "        nodecount = 0",
            "",
            "        if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":",
            "            nodecount = int((quorum + 1)/2)",
            "            new_nodecount = nodecount - 1",
            "            device_votes = new_nodecount - 1",
            "",
            "        elif corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":",
            "            device_votes = 1",
            "            nodecount = quorum - device_votes",
            "            new_nodecount = nodecount - 1",
            "",
            "        if new_nodecount > 1:",
            "            new_quorum = new_nodecount + device_votes",
            "        else:",
            "            new_quorum = 0",
            "",
            "        corosync.set_value(\"quorum.device.votes\", device_votes)",
            "    else:",
            "        corosync.set_value(\"quorum.two_node\", 1 if new_quorum == 2 else 0)",
            "    corosync.set_value(\"quorum.expected_votes\", str(new_quorum))",
            "",
            "",
            "def bootstrap_init(context):",
            "    \"\"\"",
            "    Init cluster process",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "",
            "    init()",
            "    _context.initialize_qdevice()",
            "    _context.validate_option()",
            "    _context.init_sbd_manager()",
            "",
            "    stage = _context.stage",
            "    if stage is None:",
            "        stage = \"\"",
            "",
            "    # vgfs stage requires running cluster, everything else requires inactive cluster,",
            "    # except ssh and csync2 (which don't care) and csync2_remote (which mustn't care,",
            "    # just in case this breaks ha-cluster-join on another node).",
            "    corosync_active = utils.service_is_active(\"corosync.service\")",
            "    if stage in (\"vgfs\", \"admin\", \"qdevice\"):",
            "        if not corosync_active:",
            "            error(\"Cluster is inactive - can't run %s stage\" % (stage))",
            "    elif stage == \"\":",
            "        if corosync_active:",
            "            error(\"Cluster is currently active - can't run\")",
            "    elif stage not in (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\"):",
            "        if corosync_active:",
            "            error(\"Cluster is currently active - can't run %s stage\" % (stage))",
            "",
            "    # Need hostname resolution to work, want NTP (but don't block ssh_remote or csync2_remote)",
            "    if stage not in ('ssh_remote', 'csync2_remote'):",
            "        check_tty()",
            "        if not check_prereqs(stage):",
            "            return",
            "    elif stage == 'csync2_remote':",
            "        args = _context.args",
            "        log(\"args: {}\".format(args))",
            "        if len(args) != 2:",
            "            error(\"Expected NODE argument to csync2_remote\")",
            "        _context.cluster_node = args[1]",
            "",
            "    if stage != \"\":",
            "        globals()[\"init_\" + stage]()",
            "    else:",
            "        init_ssh()",
            "        init_csync2()",
            "        init_corosync()",
            "        init_remote_auth()",
            "        if _context.template == 'ocfs2':",
            "            if _context.sbd_device is None or _context.ocfs2_device is None:",
            "                init_storage()",
            "        init_sbd()",
            "",
            "        lock_inst = lock.Lock()",
            "        try:",
            "            with lock_inst.lock():",
            "                init_cluster()",
            "                if _context.template == 'ocfs2':",
            "                    init_vgfs()",
            "                init_admin()",
            "                init_qdevice()",
            "        except lock.ClaimLockError as err:",
            "            error(err)",
            "",
            "    status(\"Done (log saved to %s)\" % (LOG_FILE))",
            "",
            "",
            "def bootstrap_join(context):",
            "    \"\"\"",
            "    Join cluster process",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "",
            "    init()",
            "    _context.init_sbd_manager()",
            "    _context.validate_option()",
            "",
            "    check_tty()",
            "",
            "    corosync_active = utils.service_is_active(\"corosync.service\")",
            "    if corosync_active:",
            "        error(\"Abort: Cluster is currently active. Run this command on a node joining the cluster.\")",
            "",
            "    if not check_prereqs(\"join\"):",
            "        return",
            "",
            "    cluster_node = _context.cluster_node",
            "    if _context.stage != \"\":",
            "        globals()[\"join_\" + _context.stage](cluster_node)",
            "    else:",
            "        if not _context.yes_to_all and cluster_node is None:",
            "            status(\"\"\"Join This Node to Cluster:",
            "  You will be asked for the IP address of an existing node, from which",
            "  configuration will be copied.  If you have not already configured",
            "  passwordless ssh between nodes, you will be prompted for the root",
            "  password of the existing node.",
            "\"\"\")",
            "            cluster_node = prompt_for_string(\"IP address or hostname of existing node (e.g.: 192.168.1.1)\", \".+\")",
            "            _context.cluster_node = cluster_node",
            "",
            "        utils.ping_node(cluster_node)",
            "",
            "        join_ssh(cluster_node)",
            "",
            "        if not utils.service_is_active(\"pacemaker.service\", cluster_node):",
            "            error(\"Cluster is inactive on {}\".format(cluster_node))",
            "",
            "        lock_inst = lock.RemoteLock(cluster_node)",
            "        try:",
            "            with lock_inst.lock():",
            "                setup_passwordless_with_other_nodes(cluster_node)",
            "                join_remote_auth(cluster_node)",
            "                join_csync2(cluster_node)",
            "                join_ssh_merge(cluster_node)",
            "                join_cluster(cluster_node)",
            "        except (lock.SSHError, lock.ClaimLockError) as err:",
            "            error(err)",
            "",
            "    status(\"Done (log saved to %s)\" % (LOG_FILE))",
            "",
            "",
            "def join_remote_auth(node):",
            "    if os.path.exists(PCMK_REMOTE_AUTH):",
            "        rmfile(PCMK_REMOTE_AUTH)",
            "    pcmk_remote_dir = os.path.dirname(PCMK_REMOTE_AUTH)",
            "    mkdirs_owned(pcmk_remote_dir, mode=0o750, gid=\"haclient\")",
            "    invoke(\"touch {}\".format(PCMK_REMOTE_AUTH))",
            "",
            "",
            "def remove_qdevice():",
            "    \"\"\"",
            "    Remove qdevice service and configuration from cluster",
            "    \"\"\"",
            "    if not utils.is_qdevice_configured():",
            "        error(\"No QDevice configuration in this cluster\")",
            "    if not confirm(\"Removing QDevice service and configuration from cluster: Are you sure?\"):",
            "        return",
            "",
            "    status(\"Disable corosync-qdevice.service\")",
            "    invoke(\"crm cluster run 'systemctl disable corosync-qdevice'\")",
            "    status(\"Stopping corosync-qdevice.service\")",
            "    invoke(\"crm cluster run 'systemctl stop corosync-qdevice'\")",
            "",
            "    status_long(\"Removing QDevice configuration from cluster\")",
            "    qnetd_host = corosync.get_value('quorum.device.net.host')",
            "    qdevice_inst = corosync.QDevice(qnetd_host)",
            "    qdevice_inst.remove_qdevice_config()",
            "    qdevice_inst.remove_qdevice_db()",
            "    update_expected_votes()",
            "    invoke(\"crm cluster run 'crm corosync reload'\")",
            "    status_done()",
            "",
            "",
            "def bootstrap_remove(context):",
            "    \"\"\"",
            "    Remove node from cluster, or remove qdevice configuration",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "    force_flag = config.core.force or _context.force",
            "",
            "    init()",
            "",
            "    if not utils.service_is_active(\"corosync.service\"):",
            "        error(\"Cluster is not active - can't execute removing action\")",
            "",
            "    if _context.qdevice_rm_flag and _context.cluster_node:",
            "        error(\"Either remove node or qdevice\")",
            "",
            "    if _context.qdevice_rm_flag:",
            "        remove_qdevice()",
            "        return",
            "",
            "    if not _context.yes_to_all and _context.cluster_node is None:",
            "        status(\"\"\"Remove This Node from Cluster:",
            "  You will be asked for the IP address or name of an existing node,",
            "  which will be removed from the cluster. This command must be",
            "  executed from a different node in the cluster.",
            "\"\"\")",
            "        _context.cluster_node = prompt_for_string(\"IP address or hostname of cluster node (e.g.: 192.168.1.1)\", \".+\")",
            "",
            "    if not _context.cluster_node:",
            "        error(\"No existing IP/hostname specified (use -c option)\")",
            "",
            "    _context.cluster_node = get_cluster_node_hostname()",
            "",
            "    if not force_flag and not confirm(\"Removing node \\\"{}\\\" from the cluster: Are you sure?\".format(_context.cluster_node)):",
            "        return",
            "",
            "    if _context.cluster_node == utils.this_node():",
            "        if not force_flag:",
            "            error(\"Removing self requires --force\")",
            "        remove_self()",
            "        return",
            "",
            "    if _context.cluster_node in xmlutil.listnodes():",
            "        remove_node_from_cluster()",
            "    else:",
            "        error(\"Specified node {} is not configured in cluster! Unable to remove.\".format(_context.cluster_node))",
            "",
            "",
            "def remove_self():",
            "    me = _context.cluster_node",
            "    yes_to_all = _context.yes_to_all",
            "    nodes = xmlutil.listnodes(include_remote_nodes=False)",
            "    othernode = next((x for x in nodes if x != me), None)",
            "    if othernode is not None:",
            "        # remove from other node",
            "        cmd = \"crm cluster remove{} -c {}\".format(\" -y\" if yes_to_all else \"\", me)",
            "        rc = utils.ext_cmd_nosudo(\"ssh{} -o StrictHostKeyChecking=no {} '{}'\".format(\"\" if yes_to_all else \" -t\", othernode, cmd))",
            "        if rc != 0:",
            "            error(\"Failed to remove this node from {}\".format(othernode))",
            "    else:",
            "        # disable and stop cluster",
            "        stop_services(SERVICES_STOP_LIST)",
            "        # remove all trace of cluster from this node",
            "        # delete configuration files from the node to be removed",
            "        if not invokerc('bash -c \"rm -f {}\"'.format(\" \".join(_context.rm_list))):",
            "            error(\"Deleting the configuration files failed\")",
            "",
            "",
            "def init_common_geo():",
            "    \"\"\"",
            "    Tasks to do both on first and other geo nodes.",
            "    \"\"\"",
            "    if not utils.package_is_installed(\"booth\"):",
            "        error(\"Booth not installed - Not configurable as a geo cluster node.\")",
            "",
            "",
            "BOOTH_CFG = \"/etc/booth/booth.conf\"",
            "BOOTH_AUTH = \"/etc/booth/authkey\"",
            "",
            "",
            "def init_csync2_geo():",
            "    \"\"\"",
            "    TODO: Configure csync2 for geo cluster",
            "    That is, create a second sync group which",
            "    syncs the geo configuration across the whole",
            "    geo cluster.",
            "    \"\"\"",
            "",
            "",
            "def create_booth_authkey():",
            "    status(\"Create authentication key for booth\")",
            "    if os.path.exists(BOOTH_AUTH):",
            "        rmfile(BOOTH_AUTH)",
            "    rc, _, err = invoke(\"booth-keygen {}\".format(BOOTH_AUTH))",
            "    if not rc:",
            "        error(\"Failed to generate booth authkey: {}\".format(err))",
            "",
            "",
            "def create_booth_config(arbitrator, clusters, tickets):",
            "    status(\"Configure booth\")",
            "",
            "    config_template = \"\"\"# The booth configuration file is \"/etc/booth/booth.conf\". You need to",
            "# prepare the same booth configuration file on each arbitrator and",
            "# each node in the cluster sites where the booth daemon can be launched.",
            "",
            "# \"transport\" means which transport layer booth daemon will use.",
            "# Currently only \"UDP\" is supported.",
            "transport=\"UDP\"",
            "port=\"9929\"",
            "\"\"\"",
            "    cfg = [config_template]",
            "    if arbitrator is not None:",
            "        cfg.append(\"arbitrator=\\\"{}\\\"\".format(arbitrator))",
            "    for s in clusters.values():",
            "        cfg.append(\"site=\\\"{}\\\"\".format(s))",
            "    cfg.append(\"authfile=\\\"{}\\\"\".format(BOOTH_AUTH))",
            "    for t in tickets:",
            "        cfg.append(\"ticket=\\\"{}\\\"\\nexpire=\\\"600\\\"\".format(t))",
            "    cfg = \"\\n\".join(cfg) + \"\\n\"",
            "",
            "    if os.path.exists(BOOTH_CFG):",
            "        rmfile(BOOTH_CFG)",
            "    utils.str2file(cfg, BOOTH_CFG)",
            "    utils.chown(BOOTH_CFG, \"hacluster\", \"haclient\")",
            "    os.chmod(BOOTH_CFG, 0o644)",
            "",
            "",
            "def bootstrap_init_geo(context):",
            "    \"\"\"",
            "    Configure as a geo cluster member.",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "",
            "    if os.path.exists(BOOTH_CFG) and not confirm(\"This will overwrite {} - continue?\".format(BOOTH_CFG)):",
            "        return",
            "    if os.path.exists(BOOTH_AUTH) and not confirm(\"This will overwrite {} - continue?\".format(BOOTH_AUTH)):",
            "        return",
            "",
            "    init_common_geo()",
            "",
            "    # TODO:",
            "    # in /etc/drbd.conf or /etc/drbd.d/global_common.conf",
            "    # set common.startup.wfc-timeout 100",
            "    # set common.startup.degr-wfc-timeout 120",
            "",
            "    create_booth_authkey()",
            "    create_booth_config(_context.arbitrator, _context.clusters, _context.tickets)",
            "    status(\"Sync booth configuration across cluster\")",
            "    csync2_update(\"/etc/booth\")",
            "    init_csync2_geo()",
            "    geo_cib_config(_context.clusters)",
            "",
            "",
            "def geo_fetch_config(node):",
            "    # TODO: clean this up",
            "    status(\"Retrieving configuration - This may prompt for root@%s:\" % (node))",
            "    tmpdir = tmpfiles.create_dir()",
            "    rc, _, err = invoke(\"scp -oStrictHostKeyChecking=no root@%s:'/etc/booth/*' %s/\" % (node, tmpdir))",
            "    if not rc:",
            "        error(\"Failed to retrieve configuration: {}\".format(err))",
            "    try:",
            "        if os.path.isfile(\"%s/authkey\" % (tmpdir)):",
            "            invoke(\"mv %s/authkey %s\" % (tmpdir, BOOTH_AUTH))",
            "            os.chmod(BOOTH_AUTH, 0o600)",
            "        if os.path.isfile(\"%s/booth.conf\" % (tmpdir)):",
            "            invoke(\"mv %s/booth.conf %s\" % (tmpdir, BOOTH_CFG))",
            "            os.chmod(BOOTH_CFG, 0o644)",
            "    except OSError as err:",
            "        raise ValueError(\"Problem encountered with booth configuration from {}: {}\".format(node, err))",
            "",
            "",
            "def geo_cib_config(clusters):",
            "    cluster_name = corosync.get_values('totem.cluster_name')[0]",
            "    if cluster_name not in list(clusters.keys()):",
            "        error(\"Local cluster name is {}, expected {}\".format(cluster_name, \"|\".join(list(clusters.keys()))))",
            "",
            "    status(\"Configure cluster resources for booth\")",
            "    crm_template = Template(\"\"\"",
            "primitive booth-ip ocf:heartbeat:IPaddr2 $iprules",
            "primitive booth-site ocf:pacemaker:booth-site \\",
            "  meta resource-stickiness=\"INFINITY\" \\",
            "  params config=booth op monitor interval=\"10s\"",
            "group g-booth booth-ip booth-site meta target-role=Stopped",
            "\"\"\")",
            "    iprule = 'params rule #cluster-name eq {} ip=\"{}\"'",
            "",
            "    crm_configure_load(\"update\", crm_template.substitute(iprules=\" \".join(iprule.format(k, v) for k, v in clusters.items())))",
            "",
            "",
            "def bootstrap_join_geo(context):",
            "    \"\"\"",
            "    Run on second cluster to add to a geo configuration.",
            "    It fetches its booth configuration from the other node (cluster node or arbitrator).",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "    init_common_geo()",
            "    check_tty()",
            "    geo_fetch_config(_context.cluster_node)",
            "    status(\"Sync booth configuration across cluster\")",
            "    csync2_update(\"/etc/booth\")",
            "    geo_cib_config(_context.clusters)",
            "",
            "",
            "def bootstrap_arbitrator(context):",
            "    \"\"\"",
            "    Configure this machine as an arbitrator.",
            "    It fetches its booth configuration from a cluster node already in the cluster.",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "    node = _context.cluster_node",
            "",
            "    init_common_geo()",
            "    check_tty()",
            "    geo_fetch_config(node)",
            "    if not os.path.isfile(BOOTH_CFG):",
            "        error(\"Failed to copy {} from {}\".format(BOOTH_CFG, node))",
            "    # TODO: verify that the arbitrator IP in the configuration is us?",
            "    status(\"Enabling and starting the booth arbitrator service\")",
            "    utils.start_service(\"booth@booth\", enable=True)",
            "",
            "# EOF"
        ],
        "afterPatchFile": [
            "# Copyright (C) 2016 Kristoffer Gronlund <kgronlund@suse.com>",
            "# See COPYING for license information.",
            "#",
            "# Bootstrap:",
            "#",
            "# Supersedes and replaces both the init/add/remove cluster scripts,",
            "# and the ha-cluster-bootstrap scripts.",
            "#",
            "# Implemented as a straight-forward set of python functions for",
            "# simplicity and flexibility.",
            "#",
            "# TODO: Make csync2 usage optional",
            "# TODO: Configuration file for bootstrap?",
            "",
            "import os",
            "import sys",
            "import random",
            "import re",
            "import time",
            "import readline",
            "import shutil",
            "from string import Template",
            "from lxml import etree",
            "from pathlib import Path",
            "from . import config",
            "from . import utils",
            "from . import xmlutil",
            "from .cibconfig import mkset_obj, cib_factory",
            "from . import corosync",
            "from . import tmpfiles",
            "from . import clidisplay",
            "from . import term",
            "from . import lock",
            "from . import userdir",
            "",
            "",
            "LOG_FILE = \"/var/log/crmsh/ha-cluster-bootstrap.log\"",
            "CSYNC2_KEY = \"/etc/csync2/key_hagroup\"",
            "CSYNC2_CFG = \"/etc/csync2/csync2.cfg\"",
            "COROSYNC_AUTH = \"/etc/corosync/authkey\"",
            "SYSCONFIG_SBD = \"/etc/sysconfig/sbd\"",
            "SYSCONFIG_FW = \"/etc/sysconfig/SuSEfirewall2\"",
            "SYSCONFIG_FW_CLUSTER = \"/etc/sysconfig/SuSEfirewall2.d/services/cluster\"",
            "PCMK_REMOTE_AUTH = \"/etc/pacemaker/authkey\"",
            "COROSYNC_CONF_ORIG = tmpfiles.create()[1]",
            "SERVICES_STOP_LIST = [\"corosync-qdevice.service\", \"corosync.service\", \"hawk.service\"]",
            "USER_LIST = [\"root\", \"hacluster\"]",
            "",
            "INIT_STAGES = (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\", \"corosync\", \"storage\", \"sbd\", \"cluster\", \"vgfs\", \"admin\", \"qdevice\")",
            "",
            "",
            "class Context(object):",
            "    \"\"\"",
            "    Context object used to avoid having to pass these variables",
            "    to every bootstrap method.",
            "    \"\"\"",
            "    def __init__(self):",
            "        '''",
            "        Initialize attributes",
            "        '''",
            "        self.type = None # init or join",
            "        self.quiet = None",
            "        self.yes_to_all = None",
            "        self.template = None",
            "        self.cluster_name = None",
            "        self.watchdog = None",
            "        self.no_overwrite_sshkey = None",
            "        self.nic_list = None",
            "        self.unicast = None",
            "        self.admin_ip = None",
            "        self.second_heartbeat = None",
            "        self.ipv6 = None",
            "        self.qdevice_inst = None",
            "        self.qnetd_addr = None",
            "        self.qdevice_port = None",
            "        self.qdevice_algo = None",
            "        self.qdevice_tie_breaker = None",
            "        self.qdevice_tls = None",
            "        self.qdevice_heuristics = None",
            "        self.qdevice_heuristics_mode = None",
            "        self.qdevice_rm_flag = None",
            "        self.shared_device = None",
            "        self.ocfs2_device = None",
            "        self.cluster_node = None",
            "        self.cluster_node_ip = None",
            "        self.force = None",
            "        self.arbitrator = None",
            "        self.clusters = None",
            "        self.tickets = None",
            "        self.sbd_manager = None",
            "        self.sbd_devices = None",
            "        self.diskless_sbd = None",
            "        self.stage = None",
            "        self.args = None",
            "        self.ui_context = None",
            "        self.interfaces_inst = None",
            "        self.with_other_user = True",
            "        self.default_nic_list = []",
            "        self.default_ip_list = []",
            "        self.local_ip_list = []",
            "        self.local_network_list = []",
            "        self.rm_list = [SYSCONFIG_SBD, CSYNC2_CFG, corosync.conf(), CSYNC2_KEY,",
            "                COROSYNC_AUTH, \"/var/lib/heartbeat/crm/*\", \"/var/lib/pacemaker/cib/*\"]",
            "",
            "    @classmethod",
            "    def set_context(cls, options):",
            "        ctx = cls()",
            "        for opt in vars(options):",
            "            setattr(ctx, opt, getattr(options, opt))",
            "        return ctx",
            "",
            "    def initialize_qdevice(self):",
            "        \"\"\"",
            "        Initialize qdevice instance",
            "        \"\"\"",
            "        if not self.qnetd_addr:",
            "            return",
            "        self.qdevice_inst = corosync.QDevice(",
            "                self.qnetd_addr,",
            "                port=self.qdevice_port,",
            "                algo=self.qdevice_algo,",
            "                tie_breaker=self.qdevice_tie_breaker,",
            "                tls=self.qdevice_tls,",
            "                cmds=self.qdevice_heuristics,",
            "                mode=self.qdevice_heuristics_mode)",
            "",
            "    def validate_option(self):",
            "        \"\"\"",
            "        Validate options",
            "        \"\"\"",
            "        if self.admin_ip:",
            "            try:",
            "                Validation.valid_admin_ip(self.admin_ip)",
            "            except ValueError as err:",
            "                error(err)",
            "        if self.qdevice_inst:",
            "            try:",
            "                self.qdevice_inst.valid_attr()",
            "            except ValueError as err:",
            "                error(err)",
            "        if self.nic_list:",
            "            if len(self.nic_list) > 2:",
            "                error(\"Maximum number of interface is 2\")",
            "            if len(self.nic_list) != len(set(self.nic_list)):",
            "                error(\"Duplicated input\")",
            "        if self.no_overwrite_sshkey:",
            "            warn(\"--no-overwrite-sshkey option is deprecated since crmsh does not overwrite ssh keys by default anymore and will be removed in future versions\")",
            "        if self.type == \"join\" and self.watchdog:",
            "            warn(\"-w option is deprecated and will be removed in future versions\")",
            "",
            "    def init_sbd_manager(self):",
            "        self.sbd_manager = SBDManager(self.sbd_devices, self.diskless_sbd)",
            "",
            "",
            "class Watchdog(object):",
            "    \"\"\"",
            "    Class to find valid watchdog device name",
            "    \"\"\"",
            "    QUERY_CMD = \"sbd query-watchdog\"",
            "    DEVICE_FIND_REGREX = \"\\[[0-9]+\\] (/dev/.*)\\n.*\\nDriver: (.*)\"",
            "",
            "    def __init__(self, _input=None, peer_host=None):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self._input = _input",
            "        self._peer_host = peer_host",
            "        self._watchdog_info_dict = {}",
            "        self._watchdog_device_name = None",
            "    ",
            "    @property",
            "    def watchdog_device_name(self):",
            "        return self._watchdog_device_name",
            "",
            "    @staticmethod",
            "    def _verify_watchdog_device(dev, ignore_error=False):",
            "        \"\"\"",
            "        Use wdctl to verify watchdog device",
            "        \"\"\"",
            "        rc, _, err = utils.get_stdout_stderr(\"wdctl {}\".format(dev))",
            "        if rc != 0:",
            "            if ignore_error:",
            "                return False",
            "            else:",
            "                error(\"Invalid watchdog device {}: {}\".format(dev, err))",
            "        return True",
            "",
            "    @staticmethod",
            "    def _load_watchdog_driver(driver):",
            "        \"\"\"",
            "        Load specific watchdog driver",
            "        \"\"\"",
            "        invoke(\"echo {} > /etc/modules-load.d/watchdog.conf\".format(driver))",
            "        invoke(\"systemctl restart systemd-modules-load\")",
            "",
            "    @staticmethod",
            "    def _get_watchdog_device_from_sbd_config():",
            "        \"\"\"",
            "        Try to get watchdog device name from sbd config file",
            "        \"\"\"",
            "        conf = utils.parse_sysconfig(SYSCONFIG_SBD)",
            "        return conf.get(\"SBD_WATCHDOG_DEV\")",
            "",
            "    @staticmethod",
            "    def _driver_is_loaded(driver):",
            "        \"\"\"",
            "        Check if driver was already loaded",
            "        \"\"\"",
            "        _, out, _ = utils.get_stdout_stderr(\"lsmod\")",
            "        return re.search(\"\\n{}\\s+\".format(driver), out)",
            "",
            "    def _set_watchdog_info(self):",
            "        \"\"\"",
            "        Set watchdog info through sbd query-watchdog command",
            "        Content in self._watchdog_info_dict: {device_name: driver_name}",
            "        \"\"\"",
            "        rc, out, err = utils.get_stdout_stderr(self.QUERY_CMD)",
            "        if rc == 0 and out:",
            "            # output format might like:",
            "            #   [1] /dev/watchdog\\nIdentity: Software Watchdog\\nDriver: softdog\\n",
            "            self._watchdog_info_dict = dict(re.findall(self.DEVICE_FIND_REGREX, out))",
            "        else:",
            "            error(\"Failed to run {}: {}\".format(self.QUERY_CMD, err))",
            "",
            "    def _get_device_through_driver(self, driver_name):",
            "        \"\"\"",
            "        Get watchdog device name which has driver_name",
            "        \"\"\"",
            "        for device, driver in self._watchdog_info_dict.items():",
            "            if driver == driver_name and self._verify_watchdog_device(device):",
            "                return device",
            "        return None",
            "",
            "    def _get_driver_through_device_remotely(self, dev_name):",
            "        \"\"\"",
            "        Given watchdog device name, get driver name on remote node",
            "        \"\"\"",
            "        cmd = \"ssh -o StrictHostKeyChecking=no root@{} {}\".format(self._peer_host, self.QUERY_CMD)",
            "        rc, out, err = utils.get_stdout_stderr(cmd)",
            "        if rc == 0 and out:",
            "            # output format might like:",
            "            #   [1] /dev/watchdog\\nIdentity: Software Watchdog\\nDriver: softdog\\n",
            "            device_driver_dict = dict(re.findall(self.DEVICE_FIND_REGREX, out))",
            "            if device_driver_dict and dev_name in device_driver_dict:",
            "                return device_driver_dict[dev_name]",
            "            else:",
            "                return None",
            "        else:",
            "            error(\"Failed to run {} remotely: {}\".format(self.QUERY_CMD, err))",
            "",
            "    def _get_first_unused_device(self):",
            "        \"\"\"",
            "        Get first unused watchdog device name",
            "        \"\"\"",
            "        for dev in self._watchdog_info_dict:",
            "            if self._verify_watchdog_device(dev, ignore_error=True):",
            "                return dev",
            "        return None",
            "",
            "    def _set_input(self):",
            "        \"\"\"",
            "        If self._input was not provided by option:",
            "          1. Try to get it from sbd config file",
            "          2. Try to get the first valid device from result of sbd query-watchdog",
            "          3. Set the self._input as softdog",
            "        \"\"\"",
            "        if not self._input:",
            "            dev = self._get_watchdog_device_from_sbd_config()",
            "            if dev and self._verify_watchdog_device(dev, ignore_error=True):",
            "                self._input = dev",
            "                return",
            "            first_unused = self._get_first_unused_device()",
            "            self._input = first_unused if first_unused else \"softdog\"",
            "",
            "    def _valid_device(self, dev):",
            "        \"\"\"",
            "        Is an unused watchdog device",
            "        \"\"\"",
            "        if dev in self._watchdog_info_dict and self._verify_watchdog_device(dev):",
            "            return True",
            "        return False",
            "",
            "    def join_watchdog(self):",
            "        \"\"\"",
            "        In join proces, get watchdog device from config",
            "        If that device not exist, get driver name from init node, and load that driver",
            "        \"\"\"",
            "        self._set_watchdog_info()",
            "",
            "        res = self._get_watchdog_device_from_sbd_config()",
            "        if not res:",
            "            error(\"Failed to get watchdog device from {}\".format(SYSCONFIG_SBD))",
            "        self._input = res",
            "",
            "        if not self._valid_device(self._input):",
            "            driver = self._get_driver_through_device_remotely(self._input)",
            "            self._load_watchdog_driver(driver)",
            "",
            "    def init_watchdog(self):",
            "        \"\"\"",
            "        In init process, find valid watchdog device",
            "        \"\"\"",
            "        self._set_watchdog_info()",
            "        self._set_input()",
            "",
            "        # self._input is a device name",
            "        if self._valid_device(self._input):",
            "            self._watchdog_device_name = self._input",
            "            return",
            "",
            "        # self._input is invalid, exit",
            "        if not invokerc(\"modinfo {}\".format(self._input)):",
            "            error(\"Should provide valid watchdog device or driver name by -w option\")",
            "",
            "        # self._input is a driver name, load it if it was unloaded",
            "        if not self._driver_is_loaded(self._input):",
            "            self._load_watchdog_driver(self._input)",
            "            self._set_watchdog_info()",
            "",
            "        # self._input is a loaded driver name, find corresponding device name",
            "        res = self._get_device_through_driver(self._input)",
            "        if res:",
            "            self._watchdog_device_name = res",
            "            return",
            "",
            "",
            "class SBDManager(object):",
            "    \"\"\"",
            "    Class to manage sbd configuration and services",
            "    \"\"\"",
            "    SYSCONFIG_SBD_TEMPLATE = \"/usr/share/fillup-templates/sysconfig.sbd\"",
            "    SBD_STATUS_DESCRIPTION = \"\"\"",
            "Configure SBD:",
            "  If you have shared storage, for example a SAN or iSCSI target,",
            "  you can use it avoid split-brain scenarios by configuring SBD.",
            "  This requires a 1 MB partition, accessible to all nodes in the",
            "  cluster.  The device path must be persistent and consistent",
            "  across all nodes in the cluster, so /dev/disk/by-id/* devices",
            "  are a good choice.  Note that all data on the partition you",
            "  specify here will be destroyed.",
            "\"\"\"",
            "",
            "    def __init__(self, sbd_devices=None, diskless_sbd=False):",
            "        \"\"\"",
            "        Init function",
            "",
            "        sbd_devices is provided by '-s' option on init process",
            "        diskless_sbd is provided by '-S' option on init process",
            "        \"\"\"",
            "        self.sbd_devices_input = sbd_devices",
            "        self.diskless_sbd = diskless_sbd",
            "        self._sbd_devices = None",
            "        self._watchdog_inst = None",
            "",
            "    def _parse_sbd_device(self):",
            "        \"\"\"",
            "        Parse sbd devices, possible command line is like:",
            "          -s \"/dev/sdb1;/dev/sdb2\"",
            "          -s /dev/sdb1 -s /dev/sbd2",
            "        \"\"\"",
            "        result_list = []",
            "        for dev in self.sbd_devices_input:",
            "            if ';' in dev:",
            "                result_list.extend(dev.strip(';').split(';'))",
            "            else:",
            "                result_list.append(dev)",
            "        return result_list",
            "",
            "    @staticmethod",
            "    def _get_device_uuid(dev, node=None):",
            "        \"\"\"",
            "        Get UUID for specific device and node",
            "        \"\"\"",
            "        cmd = \"sbd -d {} dump\".format(dev)",
            "        if node:",
            "            cmd = \"ssh -o StrictHostKeyChecking=no root@{} '{}'\".format(node, cmd)",
            "",
            "        rc, out, err = utils.get_stdout_stderr(cmd)",
            "        if rc != 0 and err:",
            "            raise ValueError(\"Cannot dump sbd meta-data: {}\".format(err))",
            "        if rc == 0 and out:",
            "            res = re.search(\"UUID\\s*:\\s*(.*)\\n\", out)",
            "            if not res:",
            "                raise ValueError(\"Cannot find sbd device UUID for {}\".format(dev))",
            "            return res.group(1)",
            "",
            "    def _compare_device_uuid(self, dev, node_list):",
            "        \"\"\"",
            "        Compare local sbd device UUID with other node's sbd device UUID",
            "        \"\"\"",
            "        if not node_list:",
            "            return",
            "        local_uuid = self._get_device_uuid(dev)",
            "        for node in node_list:",
            "            remote_uuid = self._get_device_uuid(dev, node)",
            "            if local_uuid != remote_uuid:",
            "                raise ValueError(\"Device {} doesn't have the same UUID with {}\".format(dev, node))",
            "",
            "    def _verify_sbd_device(self, dev_list, compare_node_list=[]):",
            "        \"\"\"",
            "        Verify sbd device",
            "        \"\"\"",
            "        if len(dev_list) > 3:",
            "            raise ValueError(\"Maximum number of SBD device is 3\")",
            "        for dev in dev_list:",
            "            if not is_block_device(dev):",
            "                raise ValueError(\"{} doesn't look like a block device\".format(dev))",
            "            self._compare_device_uuid(dev, compare_node_list)",
            "",
            "    def _get_sbd_device_interactive(self):",
            "        \"\"\"",
            "        Get sbd device on interactive mode",
            "        \"\"\"",
            "        if _context.yes_to_all:",
            "            warn(\"Not configuring SBD ({} left untouched).\".format(SYSCONFIG_SBD))",
            "            return",
            "",
            "        status(self.SBD_STATUS_DESCRIPTION)",
            "",
            "        if not confirm(\"Do you wish to use SBD?\"):",
            "            warn(\"Not configuring SBD - STONITH will be disabled.\")",
            "            return",
            "",
            "        configured_dev_list = self._get_sbd_device_from_config()",
            "        if configured_dev_list and not confirm(\"SBD is already configured to use {} - overwrite?\".format(';'.join(configured_dev_list))):",
            "            return configured_dev_list",
            "",
            "        dev_list = []",
            "        dev_looks_sane = False",
            "        while not dev_looks_sane:",
            "            dev = prompt_for_string('Path to storage device (e.g. /dev/disk/by-id/...), or \"none\" for diskless sbd, use \";\" as separator for multi path', r'none|\\/.*')",
            "            if not dev:",
            "                continue",
            "            if dev == \"none\":",
            "                self.diskless_sbd = True",
            "                return",
            "            dev_list = dev.strip(';').split(';')",
            "            try:",
            "                self._verify_sbd_device(dev_list)",
            "            except ValueError as err_msg:",
            "                print_error_msg(str(err_msg))",
            "                continue",
            "            for dev_item in dev_list:",
            "                warn(\"All data on {} will be destroyed!\".format(dev_item))",
            "                if confirm('Are you sure you wish to use this device?'):",
            "                    dev_looks_sane = True",
            "                else:",
            "                    dev_looks_sane = False",
            "                    break",
            "",
            "        return dev_list",
            "",
            "    def _get_sbd_device(self):",
            "        \"\"\"",
            "        Get sbd device from options or interactive mode",
            "        \"\"\"",
            "        dev_list = []",
            "        if self.sbd_devices_input:",
            "            dev_list = self._parse_sbd_device()",
            "            self._verify_sbd_device(dev_list)",
            "        elif not self.diskless_sbd:",
            "            dev_list = self._get_sbd_device_interactive()",
            "        self._sbd_devices = dev_list",
            "",
            "    def _initialize_sbd(self):",
            "        \"\"\"",
            "        Initialize SBD device",
            "        \"\"\"",
            "        if self.diskless_sbd:",
            "            return",
            "        for dev in self._sbd_devices:",
            "            rc, _, err = invoke(\"sbd -d {} create\".format(dev))",
            "            if not rc:",
            "                error(\"Failed to initialize SBD device {}: {}\".format(dev, err))",
            "",
            "    def _update_configuration(self):",
            "        \"\"\"",
            "        Update /etc/sysconfig/sbd",
            "        \"\"\"",
            "        shutil.copyfile(self.SYSCONFIG_SBD_TEMPLATE, SYSCONFIG_SBD)",
            "        sbd_config_dict = {",
            "                \"SBD_PACEMAKER\": \"yes\",",
            "                \"SBD_STARTMODE\": \"always\",",
            "                \"SBD_DELAY_START\": \"no\",",
            "                \"SBD_WATCHDOG_DEV\": self._watchdog_inst.watchdog_device_name",
            "                }",
            "        if self._sbd_devices:",
            "            sbd_config_dict[\"SBD_DEVICE\"] = ';'.join(self._sbd_devices)",
            "        utils.sysconfig_set(SYSCONFIG_SBD, **sbd_config_dict)",
            "        csync2_update(SYSCONFIG_SBD)",
            "",
            "    @staticmethod",
            "    def _get_sbd_device_from_config():",
            "        \"\"\"",
            "        Gets currently configured SBD device, i.e. what's in /etc/sysconfig/sbd",
            "        \"\"\"",
            "        conf = utils.parse_sysconfig(SYSCONFIG_SBD)",
            "        res = conf.get(\"SBD_DEVICE\")",
            "        if res:",
            "            return res.strip(';').split(';')",
            "        else:",
            "            return None",
            "",
            "    def sbd_init(self):",
            "        \"\"\"",
            "        Function sbd_init includes these steps:",
            "        1. Get sbd device from options or interactive mode",
            "        2. Initialize sbd device",
            "        3. Write config file /etc/sysconfig/sbd",
            "        \"\"\"",
            "        if not utils.package_is_installed(\"sbd\"):",
            "            return",
            "        self._watchdog_inst = Watchdog(_input=_context.watchdog)",
            "        self._watchdog_inst.init_watchdog()",
            "        self._get_sbd_device()",
            "        if not self._sbd_devices and not self.diskless_sbd:",
            "            invoke(\"systemctl disable sbd.service\")",
            "            return",
            "        status_long(\"Initializing {}SBD...\".format(\"diskless \" if self.diskless_sbd else \"\"))",
            "        self._initialize_sbd()",
            "        self._update_configuration()",
            "        invoke(\"systemctl enable sbd.service\")",
            "        status_done()",
            "",
            "    def configure_sbd_resource(self):",
            "        \"\"\"",
            "        Configure stonith-sbd resource and stonith-enabled property",
            "        \"\"\"",
            "        if not utils.package_is_installed(\"sbd\"):",
            "            return",
            "        if utils.service_is_enabled(\"sbd.service\"):",
            "            if self._get_sbd_device_from_config():",
            "                if not invokerc(\"crm configure primitive stonith-sbd stonith:external/sbd pcmk_delay_max=30s\"):",
            "                    error(\"Can't create stonith-sbd primitive\")",
            "                if not invokerc(\"crm configure property stonith-enabled=true\"):",
            "                    error(\"Can't enable STONITH for SBD\")",
            "            else:",
            "                if not invokerc(\"crm configure property stonith-enabled=true stonith-watchdog-timeout=5s\"):",
            "                    error(\"Can't enable STONITH for diskless SBD\")",
            "",
            "    def join_sbd(self, peer_host):",
            "        \"\"\"",
            "        Function join_sbd running on join process only",
            "        On joining process, check whether peer node has enabled sbd.service",
            "        If so, check prerequisites of SBD and verify sbd device on join node",
            "        \"\"\"",
            "        if not utils.package_is_installed(\"sbd\"):",
            "            return",
            "        if not os.path.exists(SYSCONFIG_SBD) or not utils.service_is_enabled(\"sbd.service\", peer_host):",
            "            invoke(\"systemctl disable sbd.service\")",
            "            return",
            "        self._watchdog_inst = Watchdog(peer_host=peer_host)",
            "        self._watchdog_inst.join_watchdog()",
            "        dev_list = self._get_sbd_device_from_config()",
            "        if dev_list:",
            "            self._verify_sbd_device(dev_list, [peer_host])",
            "        status(\"Got {}SBD configuration\".format(\"\" if dev_list else \"diskless \"))",
            "        invoke(\"systemctl enable sbd.service\")",
            "",
            "    @classmethod",
            "    def verify_sbd_device(cls):",
            "        \"\"\"",
            "        This classmethod is for verifying sbd device on a running cluster",
            "        Raise ValueError for exceptions",
            "        \"\"\"",
            "        inst = cls()",
            "        dev_list = inst._get_sbd_device_from_config()",
            "        if not dev_list:",
            "            raise ValueError(\"No sbd device configured\")",
            "        inst._verify_sbd_device(dev_list, utils.list_cluster_nodes_except_me())",
            "",
            "",
            "_context = None",
            "",
            "",
            "def die(*args):",
            "    \"\"\"",
            "    Broken out as special case for log() failure.  Ordinarily you",
            "    should just use error() to terminate.",
            "    \"\"\"",
            "    raise ValueError(\" \".join([str(arg) for arg in args]))",
            "",
            "",
            "def error(*args):",
            "    \"\"\"",
            "    Log an error message and raise ValueError to bail out of",
            "    bootstrap process.",
            "    \"\"\"",
            "    log(\"ERROR: {}\".format(\" \".join([str(arg) for arg in args])))",
            "    die(*args)",
            "",
            "",
            "def print_error_msg(msg):",
            "    \"\"\"",
            "    Just print error message",
            "    \"\"\"",
            "    print(term.render(clidisplay.error(\"ERROR:\")) + \" {}\".format(msg))",
            "",
            "",
            "def warn(*args):",
            "    \"\"\"",
            "    Log and display a warning message.",
            "    \"\"\"",
            "    log(\"WARNING: {}\".format(\" \".join(str(arg) for arg in args)))",
            "    print(term.render(clidisplay.warn(\"WARNING: {}\".format(\" \".join(str(arg) for arg in args)))))",
            "",
            "",
            "@utils.memoize",
            "def log_file_fallback():",
            "    \"\"\"",
            "    If the standard log location isn't writable,",
            "    just log to the nearest temp dir.",
            "    \"\"\"",
            "    return os.path.join(utils.get_tempdir(), \"ha-cluster-bootstrap.log\")",
            "",
            "",
            "def log(*args):",
            "    global LOG_FILE",
            "    try:",
            "        Path(os.path.dirname(LOG_FILE)).mkdir(parents=True, exist_ok=True)",
            "        with open(LOG_FILE, \"ab\") as logfile:",
            "            text = \" \".join([utils.to_ascii(arg) for arg in args]) + \"\\n\"",
            "            logfile.write(text.encode('ascii', 'backslashreplace'))",
            "    except IOError:",
            "        if LOG_FILE != log_file_fallback():",
            "            LOG_FILE = log_file_fallback()",
            "            log(*args)",
            "        else:",
            "            die(\"Can't append to {} - aborting\".format(LOG_FILE))",
            "",
            "",
            "def drop_last_history():",
            "    hlen = readline.get_current_history_length()",
            "    if hlen > 0:",
            "        readline.remove_history_item(hlen - 1)",
            "",
            "",
            "def prompt_for_string(msg, match=None, default='', valid_func=None, prev_value=[]):",
            "    if _context.yes_to_all:",
            "        return default",
            "",
            "    while True:",
            "        disable_completion()",
            "        val = utils.multi_input('  %s [%s]' % (msg, default))",
            "        enable_completion()",
            "        if not val:",
            "            val = default",
            "        else:",
            "            drop_last_history()",
            "",
            "        if not val:",
            "            return None",
            "        if not match and not valid_func:",
            "            return val",
            "        if match and not re.match(match, val):",
            "            print_error_msg(\"Invalid value entered\")",
            "            continue",
            "        if valid_func:",
            "            try:",
            "                valid_func(val, prev_value)",
            "            except ValueError as err:",
            "                print_error_msg(err)",
            "                continue",
            "",
            "        return val",
            "",
            "",
            "def confirm(msg):",
            "    if _context.yes_to_all:",
            "        return True",
            "    disable_completion()",
            "    rc = utils.ask(msg)",
            "    enable_completion()",
            "    drop_last_history()",
            "    return rc",
            "",
            "",
            "def disable_completion():",
            "    if _context.ui_context:",
            "        _context.ui_context.disable_completion()",
            "",
            "",
            "def enable_completion():",
            "    if _context.ui_context:",
            "        _context.ui_context.setup_readline()",
            "",
            "",
            "def invoke(*args):",
            "    \"\"\"",
            "    Log command execution to log file.",
            "    Log output from command to log file.",
            "    Return (boolean, stdout, stderr)",
            "    \"\"\"",
            "    log(\"+ \" + \" \".join(args))",
            "    rc, stdout, stderr = utils.get_stdout_stderr(\" \".join(args))",
            "    if stdout:",
            "        log(stdout)",
            "    if stderr:",
            "        log(stderr)",
            "    return rc == 0, stdout, stderr",
            "",
            "",
            "def invokerc(*args):",
            "    \"\"\"",
            "    Calling invoke, return True/False",
            "    \"\"\"",
            "    rc, _, _ = invoke(*args)",
            "    return rc",
            "",
            "",
            "def crm_configure_load(action, configuration):",
            "    log(\": loading crm config (%s), content is:\" % (action))",
            "    log(configuration)",
            "    if not cib_factory.initialize():",
            "        error(\"Failed to load cluster configuration\")",
            "    set_obj = mkset_obj()",
            "    if action == 'replace':",
            "        cib_factory.erase()",
            "    if not set_obj.save(configuration, remove=False, method=action):",
            "        error(\"Failed to load cluster configuration\")",
            "    if not cib_factory.commit():",
            "        error(\"Failed to commit cluster configuration\")",
            "",
            "",
            "def wait_for_resource(message, resource, needle=\"running on\"):",
            "    status_long(message)",
            "    while True:",
            "        _rc, out, err = utils.get_stdout_stderr(\"crm_resource --locate --resource \" + resource)",
            "        if needle in out:",
            "            break",
            "        if needle in err:",
            "            break",
            "        status_progress()",
            "        sleep(1)",
            "    status_done()",
            "",
            "",
            "def wait_for_stop(message, resource):",
            "    return wait_for_resource(message, resource, needle=\"NOT running\")",
            "",
            "",
            "def wait_for_cluster():",
            "    status_long(\"Waiting for cluster\")",
            "    while True:",
            "        _rc, out, _err = utils.get_stdout_stderr(\"crm_mon -1\")",
            "        if is_online(out):",
            "            break",
            "        status_progress()",
            "        sleep(2)",
            "    status_done()",
            "",
            "",
            "def get_cluster_node_hostname():",
            "    \"\"\"",
            "    Get the hostname of the cluster node used during the join process if an IP address is used.",
            "    \"\"\"",
            "    peer_node = None",
            "    if _context.cluster_node:",
            "        if utils.IP.is_valid_ip(_context.cluster_node):",
            "            rc, out, err = utils.get_stdout_stderr(\"ssh {} crm_node --name\".format(_context.cluster_node))",
            "            if rc != 0:",
            "                error(err)",
            "            peer_node = out",
            "        else:",
            "            peer_node = _context.cluster_node",
            "    return peer_node",
            "",
            "",
            "def is_online(crm_mon_txt):",
            "    \"\"\"",
            "    Check whether local node is online",
            "    Besides that, in join process, check whether init node is online",
            "    \"\"\"",
            "    if not re.search(\"Online: .* {} \".format(utils.this_node()), crm_mon_txt):",
            "        return False",
            "",
            "    # if peer_node is None, this is in the init process",
            "    peer_node = get_cluster_node_hostname()",
            "    if peer_node is None:",
            "        return True",
            "    # In join process",
            "    # If the joining node is already online but can't find the init node",
            "    # The communication IP maybe mis-configured",
            "    if not re.search(\"Online: .* {} \".format(peer_node), crm_mon_txt):",
            "        shutil.copy(COROSYNC_CONF_ORIG, corosync.conf())",
            "        csync2_update(corosync.conf())",
            "        utils.stop_service(\"corosync\")",
            "        print()",
            "        error(\"Cannot see peer node \\\"{}\\\", please check the communication IP\".format(peer_node))",
            "    return True",
            "",
            "",
            "def pick_default_value(default_list, prev_list):",
            "    \"\"\"",
            "    Provide default value for function 'prompt_for_string'.",
            "    Make sure give different default value in multi-ring mode.",
            "",
            "    Parameters:",
            "    * default_list - default value list for config item",
            "    * prev_list    - previous value for config item in multi-ring mode",
            "    \"\"\"",
            "    for value in default_list:",
            "        if value not in prev_list:",
            "            return value",
            "    return \"\"",
            "",
            "",
            "def sleep(t):",
            "    \"\"\"",
            "    Sleep for t seconds.",
            "    \"\"\"",
            "    t = float(t)",
            "    time.sleep(t)",
            "",
            "",
            "def status(msg):",
            "    log(\"# \" + msg)",
            "    if not _context.quiet:",
            "        print(\"  {}\".format(msg))",
            "",
            "",
            "def status_long(msg):",
            "    log(\"# {}...\".format(msg))",
            "    if not _context.quiet:",
            "        sys.stdout.write(\"  {}...\".format(msg))",
            "        sys.stdout.flush()",
            "",
            "",
            "def status_progress():",
            "    if not _context.quiet:",
            "        sys.stdout.write(\".\")",
            "        sys.stdout.flush()",
            "",
            "",
            "def status_done():",
            "    log(\"# done\")",
            "    if not _context.quiet:",
            "        print(\"done\")",
            "",
            "",
            "def partprobe():",
            "    # This function uses fdisk to create a list of valid devices for probing",
            "    # with partprobe.  This prevents partprobe from failing on read-only mounted",
            "    # devices such as /dev/sr0 (etc) that might cause it to return an error when",
            "    # it exits.  This allows partprobe to run without forcing _die to bail out.",
            "    # -Brandon Heaton",
            "    #  ATT Training Engineer",
            "    #  Data Center Engineer",
            "    #  bheaton@suse.com",
            "    _rc, out, _err = utils.get_stdout_stderr(\"sfdisk -l\")",
            "    disks = re.findall(r'^Disk\\s*(/.+):', out, re.M)",
            "    invoke(\"partprobe\", *disks)",
            "",
            "",
            "def probe_partitions():",
            "    status_long(\"Probing for new partitions\")",
            "    partprobe()",
            "    sleep(5)",
            "    status_done()",
            "",
            "",
            "def check_tty():",
            "    \"\"\"",
            "    Check for pseudo-tty: Cannot display read prompts without a TTY (bnc#892702)",
            "    \"\"\"",
            "    if _context.yes_to_all:",
            "        return",
            "    if not sys.stdin.isatty():",
            "        error(\"No pseudo-tty detected! Use -t option to ssh if calling remotely.\")",
            "",
            "",
            "def my_hostname_resolves():",
            "    import socket",
            "    hostname = utils.this_node()",
            "    try:",
            "        socket.gethostbyname(hostname)",
            "        return True",
            "    except socket.error:",
            "        return False",
            "",
            "",
            "def check_prereqs(stage):",
            "    warned = False",
            "",
            "    if not my_hostname_resolves():",
            "        warn(\"Hostname '{}' is unresolvable. {}\".format(",
            "            utils.this_node(),",
            "            \"Please add an entry to /etc/hosts or configure DNS.\"))",
            "        warned = True",
            "",
            "    timekeepers = ('chronyd.service', 'ntp.service', 'ntpd.service')",
            "    timekeeper = None",
            "    for tk in timekeepers:",
            "        if utils.service_is_available(tk):",
            "            timekeeper = tk",
            "            break",
            "",
            "    if timekeeper is None:",
            "        warn(\"No NTP service found.\")",
            "        warned = True",
            "    elif not utils.service_is_enabled(timekeeper):",
            "        warn(\"{} is not configured to start at system boot.\".format(timekeeper))",
            "        warned = True",
            "",
            "    if warned:",
            "        if not confirm(\"Do you want to continue anyway?\"):",
            "            return False",
            "",
            "    firewall_open_basic_ports()",
            "    return True",
            "",
            "",
            "def log_start():",
            "    \"\"\"",
            "    Convenient side-effect: this will die immediately if the log file",
            "    is not writable (e.g. if not running as root)",
            "    \"\"\"",
            "    # Reload rsyslog to make sure it logs with the correct hostname",
            "    if utils.service_is_active(\"rsyslog.service\"):",
            "        invoke(\"systemctl reload rsyslog.service\")",
            "    datestr = utils.get_stdout(\"date --rfc-3339=seconds\")[1]",
            "    log('================================================================')",
            "    log(\"%s %s\" % (datestr, \" \".join(sys.argv)))",
            "    log('----------------------------------------------------------------')",
            "",
            "",
            "def init_network():",
            "    \"\"\"",
            "    Get all needed network information through utils.InterfacesInfo",
            "    \"\"\"",
            "    interfaces_inst = utils.InterfacesInfo(_context.ipv6, _context.second_heartbeat, _context.nic_list)",
            "    interfaces_inst.get_interfaces_info()",
            "    _context.default_nic_list = interfaces_inst.get_default_nic_list_from_route()",
            "    _context.default_ip_list = interfaces_inst.get_default_ip_list()",
            "",
            "    # local_ip_list and local_network_list are for validation",
            "    _context.local_ip_list = interfaces_inst.ip_list",
            "    _context.local_network_list = interfaces_inst.network_list",
            "    _context.interfaces_inst = interfaces_inst",
            "    # use two \"-i\" options equal to use \"-M\" option",
            "    if len(_context.default_nic_list) == 2 and not _context.second_heartbeat:",
            "        _context.second_heartbeat = True",
            "",
            "",
            "def configure_firewall(tcp=None, udp=None):",
            "    if tcp is None:",
            "        tcp = []",
            "    if udp is None:",
            "        udp = []",
            "",
            "    def init_firewall_suse(tcp, udp):",
            "        if os.path.exists(SYSCONFIG_FW_CLUSTER):",
            "            cluster = utils.parse_sysconfig(SYSCONFIG_FW_CLUSTER)",
            "            tcpcurr = set(cluster.get(\"TCP\", \"\").split())",
            "            tcpcurr.update(tcp)",
            "            tcp = list(tcpcurr)",
            "            udpcurr = set(cluster.get(\"UDP\", \"\").split())",
            "            udpcurr.update(udp)",
            "            udp = list(udpcurr)",
            "",
            "        utils.sysconfig_set(SYSCONFIG_FW_CLUSTER, TCP=\" \".join(tcp), UDP=\" \".join(udp))",
            "",
            "        ext = \"\"",
            "        if os.path.exists(SYSCONFIG_FW):",
            "            fw = utils.parse_sysconfig(SYSCONFIG_FW)",
            "            ext = fw.get(\"FW_CONFIGURATIONS_EXT\", \"\")",
            "            if \"cluster\" not in ext.split():",
            "                ext = ext + \" cluster\"",
            "        utils.sysconfig_set(SYSCONFIG_FW, FW_CONFIGURATIONS_EXT=ext)",
            "",
            "        # No need to do anything else if the firewall is inactive",
            "        if not utils.service_is_active(\"SuSEfirewall2\"):",
            "            return",
            "",
            "        # Firewall is active, either restart or complain if we couldn't tweak it",
            "        status(\"Restarting firewall (tcp={}, udp={})\".format(\" \".join(tcp), \" \".join(udp)))",
            "        if not invokerc(\"rcSuSEfirewall2 restart\"):",
            "            error(\"Failed to restart firewall (SuSEfirewall2)\")",
            "",
            "    def init_firewall_firewalld(tcp, udp):",
            "        has_firewalld = utils.service_is_active(\"firewalld\")",
            "        cmdbase = 'firewall-cmd --zone=public --permanent ' if has_firewalld else 'firewall-offline-cmd --zone=public '",
            "",
            "        def cmd(args):",
            "            if not invokerc(cmdbase + args):",
            "                error(\"Failed to configure firewall.\")",
            "",
            "        for p in tcp:",
            "            cmd(\"--add-port={}/tcp\".format(p))",
            "",
            "        for p in udp:",
            "            cmd(\"--add-port={}/udp\".format(p))",
            "",
            "        if has_firewalld:",
            "            if not invokerc(\"firewall-cmd --reload\"):",
            "                error(\"Failed to reload firewall configuration.\")",
            "",
            "    def init_firewall_ufw(tcp, udp):",
            "        \"\"\"",
            "        try configuring firewall with ufw",
            "        \"\"\"",
            "        for p in tcp:",
            "            if not invokerc(\"ufw allow {}/tcp\".format(p)):",
            "                error(\"Failed to configure firewall (ufw)\")",
            "        for p in udp:",
            "            if not invokerc(\"ufw allow {}/udp\".format(p)):",
            "                error(\"Failed to configure firewall (ufw)\")",
            "",
            "    if utils.package_is_installed(\"firewalld\"):",
            "        init_firewall_firewalld(tcp, udp)",
            "    elif utils.package_is_installed(\"SuSEfirewall2\"):",
            "        init_firewall_suse(tcp, udp)",
            "    elif utils.package_is_installed(\"ufw\"):",
            "        init_firewall_ufw(tcp, udp)",
            "    else:",
            "        warn(\"Failed to detect firewall: Could not open ports tcp={}, udp={}\".format(\"|\".join(tcp), \"|\".join(udp)))",
            "",
            "",
            "def firewall_open_basic_ports():",
            "    \"\"\"",
            "    Open ports for csync2, mgmtd, hawk & dlm respectively",
            "    \"\"\"",
            "    configure_firewall(tcp=[\"30865\", \"5560\", \"7630\", \"21064\"])",
            "",
            "",
            "def firewall_open_corosync_ports():",
            "    \"\"\"",
            "    Have to do this separately, as we need general firewall config early",
            "    so csync2 works, but need corosync config *after* corosync.conf has",
            "    been created/updated.",
            "",
            "    Please note corosync uses two UDP ports mcastport (for mcast",
            "    receives) and mcastport - 1 (for mcast sends).",
            "",
            "    Also open QNetd/QDevice port if configured.",
            "    \"\"\"",
            "    # all mcastports defined in corosync config",
            "    udp = corosync.get_values(\"totem.interface.mcastport\")",
            "    udp.extend([str(int(p) - 1) for p in udp])",
            "",
            "    tcp = corosync.get_values(\"totem.quorum.device.net.port\")",
            "",
            "    configure_firewall(tcp=tcp, udp=udp)",
            "",
            "",
            "def init_cluster_local():",
            "    # Caller should check this, but I'm paranoid...",
            "    if utils.service_is_active(\"corosync.service\"):",
            "        error(\"corosync service is running!\")",
            "",
            "    firewall_open_corosync_ports()",
            "",
            "    # reset password, but only if it's not already set",
            "    _rc, outp = utils.get_stdout(\"passwd -S hacluster\")",
            "    ps = outp.strip().split()[1]",
            "    pass_msg = \"\"",
            "    if ps not in (\"P\", \"PS\"):",
            "        log(': Resetting password of hacluster user')",
            "        rc, outp, errp = utils.get_stdout_stderr(\"passwd hacluster\", input_s=b\"linux\\nlinux\\n\")",
            "        if rc != 0:",
            "            warn(\"Failed to reset password of hacluster user: %s\" % (outp + errp))",
            "        else:",
            "            pass_msg = \", password 'linux'\"",
            "",
            "    # evil, but necessary",
            "    invoke(\"rm -f /var/lib/heartbeat/crm/* /var/lib/pacemaker/cib/*\")",
            "",
            "    # only try to start hawk if hawk is installed",
            "    if utils.service_is_available(\"hawk.service\"):",
            "        utils.start_service(\"hawk.service\", enable=True)",
            "        status(\"Hawk cluster interface is now running. To see cluster status, open:\")",
            "        status(\"  https://{}:7630/\".format(_context.default_ip_list[0]))",
            "        status(\"Log in with username 'hacluster'{}\".format(pass_msg))",
            "    else:",
            "        warn(\"Hawk not installed - not configuring web management interface.\")",
            "",
            "    if pass_msg:",
            "        warn(\"You should change the hacluster password to something more secure!\")",
            "",
            "    utils.start_service(\"pacemaker.service\", enable=True)",
            "    wait_for_cluster()",
            "",
            "",
            "def install_tmp(tmpfile, to):",
            "    with open(tmpfile, \"r\") as src:",
            "        with utils.open_atomic(to, \"w\") as dst:",
            "            for line in src:",
            "                dst.write(line)",
            "",
            "",
            "def append(fromfile, tofile):",
            "    log(\"+ cat %s >> %s\" % (fromfile, tofile))",
            "    with open(tofile, \"a\") as tf:",
            "        with open(fromfile, \"r\") as ff:",
            "            tf.write(ff.read())",
            "",
            "",
            "def append_unique(fromfile, tofile):",
            "    \"\"\"",
            "    Append unique content from fromfile to tofile",
            "    \"\"\"",
            "    if not utils.check_file_content_included(fromfile, tofile):",
            "        append(fromfile, tofile)",
            "",
            "",
            "def rmfile(path, ignore_errors=False):",
            "    \"\"\"",
            "    Try to remove the given file, and",
            "    report an error on failure",
            "    \"\"\"",
            "    try:",
            "        os.remove(path)",
            "    except os.error as err:",
            "        if not ignore_errors:",
            "            error(\"Failed to remove {}: {}\".format(path, err))",
            "",
            "",
            "def mkdirs_owned(dirs, mode=0o777, uid=-1, gid=-1):",
            "    \"\"\"",
            "    Create directory path, setting the mode and",
            "    ownership of the leaf directory to mode/uid/gid.",
            "    \"\"\"",
            "    if not os.path.exists(dirs):",
            "        try:",
            "            os.makedirs(dirs, mode)",
            "        except OSError as err:",
            "            error(\"Failed to create {}: {}\".format(dirs, err))",
            "        if uid != -1 or gid != -1:",
            "            utils.chown(dirs, uid, gid)",
            "",
            "",
            "def init_ssh():",
            "    \"\"\"",
            "    Configure passwordless SSH.",
            "    \"\"\"",
            "    utils.start_service(\"sshd.service\", enable=True)",
            "    for user in USER_LIST:",
            "        configure_local_ssh_key(user)",
            "",
            "",
            "def key_files(user):",
            "    \"\"\"",
            "    Find home directory for user and return key files with abspath",
            "    \"\"\"",
            "    keyfile_dict = {}",
            "    home_dir = userdir.gethomedir(user)",
            "    keyfile_dict['private'] = \"{}/.ssh/id_rsa\".format(home_dir)",
            "    keyfile_dict['public'] = \"{}/.ssh/id_rsa.pub\".format(home_dir)",
            "    keyfile_dict['authorized'] = \"{}/.ssh/authorized_keys\".format(home_dir)",
            "    return keyfile_dict",
            "",
            "",
            "def is_nologin(user):",
            "    \"\"\"",
            "    Check if user's shell is /sbin/nologin",
            "    \"\"\"",
            "    with open(\"/etc/passwd\") as f:",
            "        return re.search(\"{}:.*:/sbin/nologin\".format(user), f.read())",
            "",
            "",
            "def change_user_shell(user):",
            "    \"\"\"",
            "    To change user's login shell",
            "    \"\"\"",
            "    if user != \"root\" and is_nologin(user):",
            "        if not _context.yes_to_all:",
            "            status(\"\"\"",
            "User {} will be changed the login shell as /bin/bash, and",
            "be setted up authorized ssh access among cluster nodes\"\"\".format(user))",
            "            if not confirm(\"Continue?\"):",
            "                _context.with_other_user = False",
            "                return",
            "        invoke(\"usermod -s /bin/bash {}\".format(user))",
            "",
            "",
            "def configure_local_ssh_key(user=\"root\"):",
            "    \"\"\"",
            "    Configure ssh rsa key locally",
            "",
            "    If <home_dir>/.ssh/id_rsa not exist, generate a new one",
            "    Add <home_dir>/.ssh/id_rsa.pub to <home_dir>/.ssh/authorized_keys anyway, make sure itself authorized",
            "    \"\"\"",
            "    change_user_shell(user)",
            "",
            "    private_key, public_key, authorized_file = key_files(user).values()",
            "    if not os.path.exists(private_key):",
            "        status(\"Generating SSH key for {}\".format(user))",
            "        cmd = \"ssh-keygen -q -f {} -C 'Cluster Internal on {}' -N ''\".format(private_key, utils.this_node())",
            "        cmd = utils.add_su(cmd, user)",
            "        rc, _, err = invoke(cmd)",
            "        if not rc:",
            "            error(\"Failed to generate ssh key for {}: {}\".format(user, err))",
            "",
            "    if not os.path.exists(authorized_file):",
            "        open(authorized_file, 'w').close()",
            "    append_unique(public_key, authorized_file)",
            "",
            "",
            "def init_ssh_remote():",
            "    \"\"\"",
            "    Called by ha-cluster-join",
            "    \"\"\"",
            "    authorized_keys_file = \"/root/.ssh/authorized_keys\"",
            "    if not os.path.exists(authorized_keys_file):",
            "        open(authorized_keys_file, 'w').close()",
            "    authkeys = open(authorized_keys_file, \"r+\")",
            "    authkeys_data = authkeys.read()",
            "    for key in (\"id_rsa\", \"id_dsa\", \"id_ecdsa\", \"id_ed25519\"):",
            "        fn = os.path.join(\"/root/.ssh\", key)",
            "        if not os.path.exists(fn):",
            "            continue",
            "        keydata = open(fn + \".pub\").read()",
            "        if keydata not in authkeys_data:",
            "            append(fn + \".pub\", authorized_keys_file)",
            "",
            "",
            "def append_to_remote_file(fromfile, remote_node, tofile):",
            "    \"\"\"",
            "    Append content of fromfile to tofile on remote_node",
            "    \"\"\"",
            "    err_details_string = \"\"\"",
            "    crmsh has no way to help you to setup up passwordless ssh among nodes at this time. ",
            "    As the hint, likely, `PasswordAuthentication` is 'no' in /etc/ssh/sshd_config. ",
            "    Given in this case, users must setup passwordless ssh beforehand, or change it to 'yes' and manage passwords properly",
            "    \"\"\"",
            "    cmd = \"cat {} | ssh -oStrictHostKeyChecking=no root@{} 'cat >> {}'\".format(fromfile, remote_node, tofile)",
            "    rc, _, err = invoke(cmd)",
            "    if not rc:",
            "        error(\"Failed to append contents of {} to {}:\\n\\\"{}\\\"\\n{}\".format(fromfile, remote_node, err, err_details_string))",
            "",
            "",
            "def init_csync2():",
            "    status(\"Configuring csync2\")",
            "    if os.path.exists(CSYNC2_KEY):",
            "        if not confirm(\"csync2 is already configured - overwrite?\"):",
            "            return",
            "",
            "    invoke(\"rm\", \"-f\", CSYNC2_KEY)",
            "    status_long(\"Generating csync2 shared key (this may take a while)\")",
            "    if not invokerc(\"csync2\", \"-k\", CSYNC2_KEY):",
            "        error(\"Can't create csync2 key {}\".format(CSYNC2_KEY))",
            "    status_done()",
            "",
            "    utils.str2file(\"\"\"group ha_group",
            "{",
            "key /etc/csync2/key_hagroup;",
            "host %s;",
            "include /etc/booth;",
            "include /etc/corosync/corosync.conf;",
            "include /etc/corosync/authkey;",
            "include /etc/csync2/csync2.cfg;",
            "include /etc/csync2/key_hagroup;",
            "include /etc/ctdb/nodes;",
            "include /etc/drbd.conf;",
            "include /etc/drbd.d;",
            "include /etc/ha.d/ldirectord.cf;",
            "include /etc/lvm/lvm.conf;",
            "include /etc/multipath.conf;",
            "include /etc/samba/smb.conf;",
            "include /etc/sysconfig/nfs;",
            "include /etc/sysconfig/pacemaker;",
            "include /etc/sysconfig/sbd;",
            "include /etc/pacemaker/authkey;",
            "include /etc/modules-load.d/watchdog.conf;",
            "}",
            "    \"\"\" % (utils.this_node()), CSYNC2_CFG)",
            "",
            "    utils.start_service(\"csync2.socket\", enable=True)",
            "    status_long(\"csync2 checking files\")",
            "    invoke(\"csync2\", \"-cr\", \"/\")",
            "    status_done()",
            "",
            "",
            "def csync2_update(path):",
            "    '''",
            "    Sync path to all peers",
            "",
            "    If there was a conflict, use '-f' to force this side to win",
            "    '''",
            "    invoke(\"csync2 -rm {}\".format(path))",
            "    if invokerc(\"csync2 -rxv {}\".format(path)):",
            "        return",
            "    invoke(\"csync2 -rf {}\".format(path))",
            "    if not invokerc(\"csync2 -rxv {}\".format(path)):",
            "        warn(\"{} was not synced\".format(path))",
            "",
            "",
            "def init_csync2_remote():",
            "    \"\"\"",
            "    It would be nice if we could just have csync2.cfg include a directory,",
            "    which in turn included one file per node which would be referenced via",
            "    something like \"group ha_group { ... config: /etc/csync2/hosts/*; }\"",
            "    That way, adding a new node would just mean adding a single new file",
            "    to that directory.  Unfortunately, the 'config' statement only allows",
            "    inclusion of specific individual files, not multiple files via wildcard.",
            "    So we have this function which is called by ha-cluster-join to add the new",
            "    remote node to csync2 config on some existing node.  It is intentionally",
            "    not documented in ha-cluster-init's user-visible usage information.",
            "    \"\"\"",
            "    newhost = _context.cluster_node",
            "    if not newhost:",
            "        error(\"Hostname not specified\")",
            "",
            "    curr_cfg = open(CSYNC2_CFG).read()",
            "",
            "    was_quiet = _context.quiet",
            "    try:",
            "        _context.quiet = True",
            "        # if host doesn't already exist in csync2 config, add it",
            "        if not re.search(r\"^\\s*host.*\\s+%s\\s*;\" % (newhost), curr_cfg, flags=re.M):",
            "            curr_cfg = re.sub(r\"\\bhost.*\\s+\\S+\\s*;\", r\"\\g<0>\\n\\thost %s;\" % (utils.doublequote(newhost)), curr_cfg, count=1)",
            "            utils.str2file(curr_cfg, CSYNC2_CFG)",
            "            csync2_update(\"/\")",
            "        else:",
            "            log(\": Not updating %s - remote host %s already exists\" % (CSYNC2_CFG, newhost))",
            "    finally:",
            "        _context.quiet = was_quiet",
            "",
            "",
            "def init_corosync_auth():",
            "    \"\"\"",
            "    Generate the corosync authkey",
            "    \"\"\"",
            "    if os.path.exists(COROSYNC_AUTH):",
            "        if not confirm(\"%s already exists - overwrite?\" % (COROSYNC_AUTH)):",
            "            return",
            "        rmfile(COROSYNC_AUTH)",
            "    invoke(\"corosync-keygen -l\")",
            "",
            "",
            "def init_remote_auth():",
            "    \"\"\"",
            "    Generate the pacemaker-remote authkey",
            "    \"\"\"",
            "    if os.path.exists(PCMK_REMOTE_AUTH):",
            "        if not confirm(\"%s already exists - overwrite?\" % (PCMK_REMOTE_AUTH)):",
            "            return",
            "        rmfile(PCMK_REMOTE_AUTH)",
            "",
            "    pcmk_remote_dir = os.path.dirname(PCMK_REMOTE_AUTH)",
            "    mkdirs_owned(pcmk_remote_dir, mode=0o750, gid=\"haclient\")",
            "    if not invokerc(\"dd if=/dev/urandom of={} bs=4096 count=1\".format(PCMK_REMOTE_AUTH)):",
            "        warn(\"Failed to create pacemaker authkey: {}\".format(PCMK_REMOTE_AUTH))",
            "    utils.chown(PCMK_REMOTE_AUTH, \"hacluster\", \"haclient\")",
            "    os.chmod(PCMK_REMOTE_AUTH, 0o640)",
            "",
            "",
            "class Validation(object):",
            "    \"\"\"",
            "    Class to validate values from interactive inputs",
            "    \"\"\"",
            "",
            "    def __init__(self, value, prev_value_list=[]):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self.value = value",
            "        self.prev_value_list = prev_value_list",
            "        if self.value in self.prev_value_list:",
            "            raise ValueError(\"Already in use: {}\".format(self.value))",
            "",
            "    def _is_mcast_addr(self):",
            "        \"\"\"",
            "        Check whether the address is multicast address",
            "        \"\"\"",
            "        if not utils.IP.is_mcast(self.value):",
            "            raise ValueError(\"{} is not multicast address\".format(self.value))",
            "",
            "    def _is_local_addr(self, local_addr_list):",
            "        \"\"\"",
            "        Check whether the address is in local",
            "        \"\"\"",
            "        if self.value not in local_addr_list:",
            "            raise ValueError(\"Address must be a local address (one of {})\".format(local_addr_list))",
            "",
            "    def _is_valid_port(self):",
            "        \"\"\"",
            "        Check whether the port is valid",
            "        \"\"\"",
            "        if self.prev_value_list and abs(int(self.value) - int(self.prev_value_list[0])) <= 1:",
            "            raise ValueError(\"Port {} is already in use by corosync. Leave a gap between multiple rings.\".format(self.value))",
            "        if int(self.value) <= 1024 or int(self.value) > 65535:",
            "            raise ValueError(\"Valid port range should be 1025-65535\")",
            "",
            "    @classmethod",
            "    def valid_mcast_address(cls, addr, prev_value_list=[]):",
            "        \"\"\"",
            "        Check whether the address is already in use and whether the address is for multicast",
            "        \"\"\"",
            "        cls_inst = cls(addr, prev_value_list)",
            "        cls_inst._is_mcast_addr()",
            "",
            "    @classmethod",
            "    def valid_ucast_ip(cls, addr, prev_value_list=[]):",
            "        \"\"\"",
            "        Check whether the address is already in use and whether the address exists on local",
            "        \"\"\"",
            "        cls_inst = cls(addr, prev_value_list)",
            "        cls_inst._is_local_addr(_context.local_ip_list)",
            "",
            "    @classmethod",
            "    def valid_mcast_ip(cls, addr, prev_value_list=[]):",
            "        \"\"\"",
            "        Check whether the address is already in use and whether the address exists on local address and network",
            "        \"\"\"",
            "        cls_inst = cls(addr, prev_value_list)",
            "        cls_inst._is_local_addr(_context.local_ip_list + _context.local_network_list)",
            "",
            "    @classmethod",
            "    def valid_port(cls, port, prev_value_list=[]):",
            "        \"\"\"",
            "        Check whether the port is valid",
            "        \"\"\"",
            "        cls_inst = cls(port, prev_value_list)",
            "        cls_inst._is_valid_port()",
            "",
            "    @staticmethod",
            "    def valid_admin_ip(addr, prev_value_list=[]):",
            "        \"\"\"",
            "        Validate admin IP address",
            "        \"\"\"",
            "        ipv6 = utils.IP.is_ipv6(addr)",
            "",
            "        # Check whether this IP already configured in cluster",
            "        ping_cmd = \"ping6\" if ipv6 else \"ping\"",
            "        if invokerc(\"{} -c 1 {}\".format(ping_cmd, addr)):",
            "            raise ValueError(\"Address already in use: {}\".format(addr))",
            "",
            "",
            "def init_corosync_unicast():",
            "",
            "    if _context.yes_to_all:",
            "        status(\"Configuring corosync (unicast)\")",
            "    else:",
            "        status(\"\"\"",
            "Configure Corosync (unicast):",
            "  This will configure the cluster messaging layer.  You will need",
            "  to specify a network address over which to communicate (default",
            "  is {}'s network, but you can use the network address of any",
            "  active interface).",
            "\"\"\".format(_context.default_nic_list[0]))",
            "",
            "    ringXaddr_res = []",
            "    mcastport_res = []",
            "    default_ports = [\"5405\", \"5407\"]",
            "    two_rings = False",
            "",
            "    for i in range(2):",
            "        ringXaddr = prompt_for_string(",
            "                'Address for ring{}'.format(i),",
            "                default=pick_default_value(_context.default_ip_list, ringXaddr_res),",
            "                valid_func=Validation.valid_ucast_ip,",
            "                prev_value=ringXaddr_res)",
            "        if not ringXaddr:",
            "            error(\"No value for ring{}\".format(i))",
            "        ringXaddr_res.append(ringXaddr)",
            "",
            "        mcastport = prompt_for_string(",
            "                'Port for ring{}'.format(i),",
            "                match='[0-9]+',",
            "                default=pick_default_value(default_ports, mcastport_res),",
            "                valid_func=Validation.valid_port,",
            "                prev_value=mcastport_res)",
            "        if not mcastport:",
            "            error(\"Expected a multicast port for ring{}\".format(i))",
            "        mcastport_res.append(mcastport)",
            "",
            "        if i == 1 or \\",
            "           not _context.second_heartbeat or \\",
            "           not confirm(\"\\nAdd another heartbeat line?\"):",
            "            break",
            "        two_rings = True",
            "",
            "    corosync.create_configuration(",
            "            clustername=_context.cluster_name,",
            "            ringXaddr=ringXaddr_res,",
            "            mcastport=mcastport_res,",
            "            transport=\"udpu\",",
            "            ipv6=_context.ipv6,",
            "            two_rings=two_rings)",
            "    csync2_update(corosync.conf())",
            "",
            "",
            "def init_corosync_multicast():",
            "    def gen_mcastaddr():",
            "        if _context.ipv6:",
            "            return \"ff3e::%s:%d\" % (",
            "                ''.join([random.choice('0123456789abcdef') for _ in range(4)]),",
            "                random.randint(0, 9))",
            "        return \"239.%d.%d.%d\" % (",
            "            random.randint(0, 255),",
            "            random.randint(0, 255),",
            "            random.randint(1, 255))",
            "",
            "    if _context.yes_to_all:",
            "        status(\"Configuring corosync\")",
            "    else:",
            "        status(\"\"\"",
            "Configure Corosync:",
            "  This will configure the cluster messaging layer.  You will need",
            "  to specify a network address over which to communicate (default",
            "  is {}'s network, but you can use the network address of any",
            "  active interface).",
            "\"\"\".format(_context.default_nic_list[0]))",
            "",
            "    bindnetaddr_res = []",
            "    mcastaddr_res = []",
            "    mcastport_res = []",
            "    default_ports = [\"5405\", \"5407\"]",
            "    two_rings = False",
            "",
            "    for i in range(2):",
            "        bindnetaddr = prompt_for_string(",
            "                'IP or network address to bind to',",
            "                default=pick_default_value(_context.default_ip_list, bindnetaddr_res),",
            "                valid_func=Validation.valid_mcast_ip,",
            "                prev_value=bindnetaddr_res)",
            "        if not bindnetaddr:",
            "            error(\"No value for bindnetaddr\")",
            "        bindnetaddr_res.append(bindnetaddr)",
            "",
            "        mcastaddr = prompt_for_string(",
            "                'Multicast address',",
            "                default=gen_mcastaddr(),",
            "                valid_func=Validation.valid_mcast_address,",
            "                prev_value=mcastaddr_res)",
            "        if not mcastaddr:",
            "            error(\"No value for mcastaddr\")",
            "        mcastaddr_res.append(mcastaddr)",
            "",
            "        mcastport = prompt_for_string(",
            "                'Multicast port',",
            "                match='[0-9]+',",
            "                default=pick_default_value(default_ports, mcastport_res),",
            "                valid_func=Validation.valid_port,",
            "                prev_value=mcastport_res)",
            "        if not mcastport:",
            "            error(\"No value for mcastport\")",
            "        mcastport_res.append(mcastport)",
            "",
            "        if i == 1 or \\",
            "           not _context.second_heartbeat or \\",
            "           not confirm(\"\\nConfigure a second multicast ring?\"):",
            "            break",
            "        two_rings = True",
            "",
            "    nodeid = None",
            "    if _context.ipv6:",
            "        nodeid = utils.gen_nodeid_from_ipv6(_context.default_ip_list[0])",
            "",
            "    corosync.create_configuration(",
            "        clustername=_context.cluster_name,",
            "        bindnetaddr=bindnetaddr_res,",
            "        mcastaddr=mcastaddr_res,",
            "        mcastport=mcastport_res,",
            "        ipv6=_context.ipv6,",
            "        nodeid=nodeid,",
            "        two_rings=two_rings)",
            "    csync2_update(corosync.conf())",
            "",
            "",
            "def init_corosync():",
            "    \"\"\"",
            "    Configure corosync (unicast or multicast, encrypted?)",
            "    \"\"\"",
            "    def requires_unicast():",
            "        host = utils.detect_cloud()",
            "        if host is not None:",
            "            status(\"Detected cloud platform: {}\".format(host))",
            "        return host is not None",
            "",
            "    init_corosync_auth()",
            "",
            "    if os.path.exists(corosync.conf()):",
            "        if not confirm(\"%s already exists - overwrite?\" % (corosync.conf())):",
            "            return",
            "",
            "    if _context.unicast or requires_unicast():",
            "        init_corosync_unicast()",
            "    else:",
            "        init_corosync_multicast()",
            "",
            "",
            "def is_block_device(dev):",
            "    from stat import S_ISBLK",
            "    try:",
            "        rc = S_ISBLK(os.stat(dev).st_mode)",
            "    except OSError:",
            "        return False",
            "    return rc",
            "",
            "",
            "def list_partitions(dev):",
            "    rc, outp, errp = utils.get_stdout_stderr(\"parted -s %s print\" % (dev))",
            "    partitions = []",
            "    for line in outp.splitlines():",
            "        m = re.match(r\"^\\s*([0-9]+)\\s*\", line)",
            "        if m:",
            "            partitions.append(m.group(1))",
            "    if rc != 0:",
            "        # ignore \"Error: /dev/vdb: unrecognised disk label\"",
            "        if errp.count('\\n') > 1 or \"unrecognised disk label\" not in errp.strip():",
            "            error(\"Failed to list partitions in {}: {}\".format(dev, errp))",
            "    return partitions",
            "",
            "",
            "def list_devices(dev):",
            "    \"TODO: THIS IS *WRONG* FOR MULTIPATH! (but possibly nothing we can do about it)\"",
            "    _rc, outp = utils.get_stdout(\"fdisk -l %s\" % (dev))",
            "    partitions = []",
            "    for line in outp.splitlines():",
            "        m = re.match(r\"^(\\/dev\\S+)\", line)",
            "        if m:",
            "            partitions.append(m.group(1))",
            "    return partitions",
            "",
            "",
            "def init_storage():",
            "    \"\"\"",
            "    Configure SBD and OCFS2 both on the same storage device.",
            "    \"\"\"",
            "    dev = _context.shared_device",
            "    partitions = []",
            "    dev_looks_sane = False",
            "",
            "    if _context.yes_to_all or not dev:",
            "        status(\"Configuring shared storage\")",
            "    else:",
            "        status(\"\"\"",
            "Configure Shared Storage:",
            "  You will need to provide the path to a shared storage device,",
            "  for example a SAN volume or iSCSI target.  The device path must",
            "  be persistent and consistent across all nodes in the cluster,",
            "  so /dev/disk/by-id/* devices are a good choice.  This device",
            "  will be automatically paritioned into two pieces, 1MB for SBD",
            "  fencing, and the remainder for an OCFS2 filesystem.",
            "\"\"\")",
            "",
            "    while not dev_looks_sane:",
            "        dev = prompt_for_string('Path to storage device (e.g. /dev/disk/by-id/...)', r'\\/.*', dev)",
            "        if not dev:",
            "            error(\"No value for shared storage device\")",
            "",
            "        if not is_block_device(dev):",
            "            if _context.yes_to_all:",
            "                error(dev + \" is not a block device\")",
            "            else:",
            "                print(\"    That doesn't look like a block device\", file=sys.stderr)",
            "        else:",
            "            #",
            "            # Got something that looks like a block device, there",
            "            # are four possibilities now:",
            "            #",
            "            #  1) It's completely broken/inaccessible",
            "            #  2) No recognizable partition table",
            "            #  3) Empty partition table",
            "            #  4) Non-empty parition table",
            "            #",
            "            partitions = list_partitions(dev)",
            "            if partitions:",
            "                status(\"WARNING: Partitions exist on %s!\" % (dev))",
            "                if confirm(\"Are you ABSOLUTELY SURE you want to overwrite?\"):",
            "                    dev_looks_sane = True",
            "                else:",
            "                    dev = \"\"",
            "            else:",
            "                # It's either broken, no partition table, or empty partition table",
            "                status(\"%s appears to be empty\" % (dev))",
            "                if confirm(\"Device appears empty (no partition table). Do you want to use {}?\".format(dev)):",
            "                    dev_looks_sane = True",
            "                else:",
            "                    dev = \"\"",
            "",
            "    if partitions:",
            "        if not confirm(\"Really?\"):",
            "            return",
            "        status_long(\"Erasing existing partitions...\")",
            "        for part in partitions:",
            "            if not invokerc(\"parted -s %s rm %s\" % (dev, part)):",
            "                error(\"Failed to remove partition %s from %s\" % (part, dev))",
            "        status_done()",
            "",
            "    status_long(\"Creating partitions...\")",
            "    if not invokerc(\"parted\", \"-s\", dev, \"mklabel\", \"msdos\"):",
            "        error(\"Failed to create partition table\")",
            "",
            "    # This is a bit rough, and probably won't result in great performance,",
            "    # but it's fine for test/demo purposes to carve off 1MB for SBD.  Note",
            "    # we have to specify the size of the first partition in this in bytes",
            "    # rather than MB, or parted's rounding gives us a ~30Kb partition",
            "    # (see rhbz#623268).",
            "    if not invokerc(\"parted -s %s mkpart primary 0 1048576B\" % (dev)):",
            "        error(\"Failed to create first partition on %s\" % (dev))",
            "    if not invokerc(\"parted -s %s mkpart primary 1M 100%%\" % (dev)):",
            "        error(\"Failed to create second partition\")",
            "",
            "    status_done()",
            "",
            "    # TODO: May not be strictly necessary, but...",
            "    probe_partitions()",
            "",
            "    # TODO: THIS IS *WRONG* FOR MULTIPATH! (but possibly nothing we can do about it)",
            "    devices = list_devices(dev)",
            "",
            "    _context.sbd_device = devices[0]",
            "    if not _context.sbd_device:",
            "        error(\"Unable to determine device path for SBD partition\")",
            "",
            "    _context.ocfs2_device = devices[1]",
            "    if not _context.ocfs2_device:",
            "        error(\"Unable to determine device path for OCFS2 partition\")",
            "",
            "    status(\"Created %s for SBD partition\" % (_context.sbd_device))",
            "    status(\"Created %s for OCFS2 partition\" % (_context.ocfs2_device))",
            "",
            "",
            "def init_sbd():",
            "    \"\"\"",
            "    Configure SBD (Storage-based fencing).",
            "",
            "    SBD can also run in diskless mode if no device",
            "    is configured.",
            "    \"\"\"",
            "    _context.sbd_manager.sbd_init()",
            "",
            "",
            "def init_cluster():",
            "    \"\"\"",
            "    Initial cluster configuration.",
            "    \"\"\"",
            "    init_cluster_local()",
            "",
            "    _rc, nnodes = utils.get_stdout(\"crm_node -l\")",
            "    nnodes = len(nnodes.splitlines())",
            "    if nnodes < 1:",
            "        error(\"No nodes found in cluster\")",
            "    if nnodes > 1:",
            "        error(\"Joined existing cluster - will not reconfigure.\")",
            "",
            "    status(\"Loading initial cluster configuration\")",
            "",
            "    crm_configure_load(\"update\", \"\"\"",
            "property cib-bootstrap-options: stonith-enabled=false",
            "op_defaults op-options: timeout=600 record-pending=true",
            "rsc_defaults rsc-options: resource-stickiness=1 migration-threshold=3",
            "\"\"\")",
            "",
            "    _context.sbd_manager.configure_sbd_resource()",
            "",
            "",
            "def init_vgfs():",
            "    \"\"\"",
            "    Configure cluster OCFS2 device.",
            "    \"\"\"",
            "    dev = _context.ocfs2_device",
            "    if not dev:",
            "        error(\"vgfs stage requires -o <dev>\")",
            "    mntpoint = \"/srv/clusterfs\"",
            "",
            "    if not is_block_device(dev):",
            "        error(\"OCFS2 device \\\"{}\\\" does not exist\".format(dev))",
            "",
            "    # TODO: configurable mountpoint and vg name",
            "    crm_configure_load(\"update\", \"\"\"",
            "primitive dlm ocf:pacemaker:controld op start timeout=90 op stop timeout=100 op monitor interval=60 timeout=60",
            "primitive clusterfs Filesystem directory=%(mntpoint)s fstype=ocfs2 device=%(dev)s \\",
            "    op monitor interval=20 timeout=40 op start timeout=60 op stop timeout=60 \\",
            "    meta target-role=Stopped",
            "clone base-clone dlm meta interleave=true",
            "clone c-clusterfs clusterfs meta interleave=true clone-max=8",
            "order base-then-clusterfs inf: base-clone c-clusterfs",
            "colocation clusterfs-with-base inf: c-clusterfs base-clone",
            "    \"\"\" % {\"mntpoint\": utils.doublequote(mntpoint), \"dev\": utils.doublequote(dev)})",
            "",
            "    wait_for_resource(\"Waiting for DLM\", \"dlm:0\")",
            "    wait_for_stop(\"Making sure filesystem is not active\", \"clusterfs:0\")",
            "",
            "    _rc, blkid, _err = utils.get_stdout_stderr(\"blkid %s\" % (dev))",
            "    if \"TYPE\" in blkid:",
            "        if not confirm(\"Exiting filesystem found on \\\"{}\\\" - destroy?\".format(dev)):",
            "            for res in (\"base-clone\", \"c-clusterfs\"):",
            "                invoke(\"crm resource stop %s\" % (res))",
            "                wait_for_stop(\"Waiting for resource %s to stop\" % (res), res)",
            "            invoke(\"crm configure delete dlm clusterfs base-group base-clone c-clusterfs base-then-clusterfs clusterfs-with-base\")",
            "",
            "    status_long(\"Creating OCFS2 filesystem\")",
            "    # TODO: want \"-T vmstore\", but this'll only fly on >2GB partition",
            "    # Note: using undocumented '-x' switch to avoid prompting if overwriting",
            "    # existing partition.  For the commit that introduced this, see:",
            "    # http://oss.oracle.com/git/?p=ocfs2-tools.git;a=commit;h=8345a068479196172190f4fa287052800fa2b66f",
            "    # TODO: if make the cluster name configurable, we need to update it here too",
            "    if not invokerc(\"mkfs.ocfs2 --cluster-stack pcmk --cluster-name %s -N 8 -x %s\" % (_context.cluster_name, dev)):",
            "        error(\"Failed to create OCFS2 filesystem on %s\" % (dev))",
            "    status_done()",
            "",
            "    # TODO: refactor, maybe",
            "    if not invokerc(\"mkdir -p %s\" % (mntpoint)):",
            "        error(\"Can't create mountpoint %s\" % (mntpoint))",
            "    if not invokerc(\"crm resource meta clusterfs delete target-role\"):",
            "        error(\"Can't start cluster filesystem clone\")",
            "    wait_for_resource(\"Waiting for %s to be mounted\" % (mntpoint), \"clusterfs:0\")",
            "",
            "",
            "def init_admin():",
            "    # Skip this section when -y is passed",
            "    # unless $ADMIN_IP is set",
            "    adminaddr = _context.admin_ip",
            "    if _context.yes_to_all and not adminaddr:",
            "        return",
            "",
            "    if not adminaddr:",
            "        status(\"\"\"",
            "Configure Administration IP Address:",
            "  Optionally configure an administration virtual IP",
            "  address. The purpose of this IP address is to",
            "  provide a single IP that can be used to interact",
            "  with the cluster, rather than using the IP address",
            "  of any specific cluster node.",
            "\"\"\")",
            "        if not confirm(\"Do you wish to configure a virtual IP address?\"):",
            "            return",
            "",
            "        adminaddr = prompt_for_string('Virtual IP', valid_func=Validation.valid_admin_ip)",
            "        if not adminaddr:",
            "            error(\"Expected an IP address\")",
            "",
            "    crm_configure_load(\"update\", 'primitive admin-ip IPaddr2 ip=%s op monitor interval=10 timeout=20' % (utils.doublequote(adminaddr)))",
            "    wait_for_resource(\"Configuring virtual IP ({})\".format(adminaddr), \"admin-ip\")",
            "",
            "",
            "def init_qdevice():",
            "    \"\"\"",
            "    Setup qdevice and qnetd service",
            "    \"\"\"",
            "    # If don't want to config qdevice, return",
            "    if not _context.qdevice_inst:",
            "        utils.disable_service(\"corosync-qdevice.service\")",
            "        return",
            "",
            "    status(\"\"\"",
            "Configure Qdevice/Qnetd:\"\"\")",
            "    qdevice_inst = _context.qdevice_inst",
            "    qnetd_addr = qdevice_inst.qnetd_addr",
            "    # Configure ssh passwordless to qnetd if detect password is needed",
            "    if utils.check_ssh_passwd_need(qnetd_addr):",
            "        status(\"Copy ssh key to qnetd node({})\".format(qnetd_addr))",
            "        rc, _, err = invoke(\"ssh-copy-id -i /root/.ssh/id_rsa.pub root@{}\".format(qnetd_addr))",
            "        if not rc:",
            "            error(\"Failed to copy ssh key: {}\".format(err))",
            "    # Start qdevice service if qdevice already configured",
            "    if utils.is_qdevice_configured() and not confirm(\"Qdevice is already configured - overwrite?\"):",
            "        start_qdevice_service()",
            "        return",
            "",
            "    # Validate qnetd node",
            "    qdevice_inst.valid_qnetd()",
            "    # Config qdevice",
            "    config_qdevice()",
            "    # Execute certificate process when tls flag is on",
            "    if utils.is_qdevice_tls_on():",
            "        status_long(\"Qdevice certification process\")",
            "        qdevice_inst.certificate_process_on_init()",
            "        status_done()",
            "",
            "    start_qdevice_service()",
            "",
            "",
            "def start_qdevice_service():",
            "    \"\"\"",
            "    Start qdevice and qnetd service",
            "    \"\"\"",
            "    qdevice_inst = _context.qdevice_inst",
            "    qnetd_addr = qdevice_inst.qnetd_addr",
            "",
            "    status(\"Enable corosync-qdevice.service in cluster\")",
            "    utils.cluster_run_cmd(\"systemctl enable corosync-qdevice\")",
            "    status(\"Starting corosync-qdevice.service in cluster\")",
            "    utils.cluster_run_cmd(\"systemctl start corosync-qdevice\")",
            "",
            "    status(\"Enable corosync-qnetd.service on {}\".format(qnetd_addr))",
            "    qdevice_inst.enable_qnetd()",
            "    status(\"Starting corosync-qnetd.service on {}\".format(qnetd_addr))",
            "    qdevice_inst.start_qnetd()",
            "",
            "",
            "def config_qdevice():",
            "    \"\"\"",
            "    Process of config qdevice",
            "    \"\"\"",
            "    qdevice_inst = _context.qdevice_inst",
            "",
            "    qdevice_inst.remove_qdevice_db()",
            "    qdevice_inst.write_qdevice_config()",
            "    if not corosync.is_unicast():",
            "        corosync.add_nodelist_from_cmaptool()",
            "    status_long(\"Update configuration\")",
            "    update_expected_votes()",
            "    utils.cluster_run_cmd(\"crm corosync reload\")",
            "    status_done()",
            "",
            "",
            "def init():",
            "    \"\"\"",
            "    Basic init",
            "    \"\"\"",
            "    log_start()",
            "    init_network()",
            "",
            "",
            "def join_ssh(seed_host):",
            "    \"\"\"",
            "    SSH configuration for joining node.",
            "    \"\"\"",
            "    if not seed_host:",
            "        error(\"No existing IP/hostname specified (use -c option)\")",
            "",
            "    utils.start_service(\"sshd.service\", enable=True)",
            "    for user in USER_LIST:",
            "        configure_local_ssh_key(user)",
            "        swap_public_ssh_key(seed_host, user)",
            "",
            "    # This makes sure the seed host has its own SSH keys in its own",
            "    # authorized_keys file (again, to help with the case where the",
            "    # user has done manual initial setup without the assistance of",
            "    # ha-cluster-init).",
            "    rc, _, err = invoke(\"ssh root@{} crm cluster init -i {} ssh_remote\".format(seed_host, _context.default_nic_list[0]))",
            "    if not rc:",
            "        error(\"Can't invoke crm cluster init -i {} ssh_remote on {}: {}\".format(_context.default_nic_list[0], seed_host, err))",
            "",
            "",
            "def swap_public_ssh_key(remote_node, user=\"root\"):",
            "    \"\"\"",
            "    Swap public ssh key between remote_node and local",
            "    \"\"\"",
            "    if user != \"root\" and not _context.with_other_user:",
            "        return",
            "",
            "    _, public_key, authorized_file = key_files(user).values()",
            "    # Detect whether need password to login to remote_node",
            "    if utils.check_ssh_passwd_need(remote_node, user):",
            "        # If no passwordless configured, paste /root/.ssh/id_rsa.pub to remote_node's /root/.ssh/authorized_keys",
            "        status(\"Configuring SSH passwordless with {}@{}\".format(user, remote_node))",
            "        # After this, login to remote_node is passwordless",
            "        append_to_remote_file(public_key, remote_node, authorized_file)",
            "",
            "    try:",
            "        # Fetch public key file from remote_node",
            "        public_key_file_remote = fetch_public_key_from_remote_node(remote_node, user)",
            "    except ValueError as err:",
            "        warn(err)",
            "        return",
            "    # Append public key file from remote_node to local's /root/.ssh/authorized_keys",
            "    # After this, login from remote_node is passwordless",
            "    # Should do this step even passwordless is True, to make sure we got two-way passwordless",
            "    append_unique(public_key_file_remote, authorized_file)",
            "",
            "",
            "def fetch_public_key_from_remote_node(node, user=\"root\"):",
            "    \"\"\"",
            "    Fetch public key file from remote node",
            "    Return a temp file contains public key",
            "    Return None if no key exist",
            "    \"\"\"",
            "",
            "    # For dsa, might need to add PubkeyAcceptedKeyTypes=+ssh-dss to config file, see",
            "    # https://superuser.com/questions/1016989/ssh-dsa-keys-no-longer-work-for-password-less-authentication",
            "    home_dir = userdir.gethomedir(user)",
            "    for key in (\"id_rsa\", \"id_ecdsa\", \"id_ed25519\", \"id_dsa\"):",
            "        public_key_file = \"{}/.ssh/{}.pub\".format(home_dir, key)",
            "        cmd = \"ssh -oStrictHostKeyChecking=no root@{} 'test -f {}'\".format(node, public_key_file)",
            "        if not invokerc(cmd):",
            "            continue",
            "        _, temp_public_key_file = tmpfiles.create()",
            "        cmd = \"scp -oStrictHostKeyChecking=no root@{}:{} {}\".format(node, public_key_file, temp_public_key_file)",
            "        rc, _, err = invoke(cmd)",
            "        if not rc:",
            "            error(\"Failed to run \\\"{}\\\": {}\".format(cmd, err))",
            "        return temp_public_key_file",
            "    raise ValueError(\"No ssh key exist on {}\".format(node))",
            "",
            "",
            "def join_csync2(seed_host):",
            "    \"\"\"",
            "    Csync2 configuration for joining node.",
            "    \"\"\"",
            "    if not seed_host:",
            "        error(\"No existing IP/hostname specified (use -c option)\")",
            "    status_long(\"Configuring csync2\")",
            "",
            "    # Necessary if re-running join on a node that's been configured before.",
            "    rmfile(\"/var/lib/csync2/{}.db3\".format(utils.this_node()), ignore_errors=True)",
            "",
            "    # Not automatically updating /etc/hosts - risky in the general case.",
            "    # etc_hosts_add_me",
            "    # local hosts_line=$(etc_hosts_get_me)",
            "    # [ -n \"$hosts_line\" ] || error \"No valid entry for $(hostname) in /etc/hosts - csync2 can't work\"",
            "",
            "    # If we *were* updating /etc/hosts, the next line would have \"\\\"$hosts_line\\\"\" as",
            "    # the last arg (but this requires re-enabling this functionality in ha-cluster-init)",
            "    cmd = \"crm cluster init -i {} csync2_remote {}\".format(_context.default_nic_list[0], utils.this_node())",
            "    rc, _, err = invoke(\"ssh -o StrictHostKeyChecking=no root@{} {}\".format(seed_host, cmd))",
            "    if not rc:",
            "        error(\"Can't invoke \\\"{}\\\" on {}: {}\".format(cmd, seed_host, err))",
            "",
            "    # This is necessary if syncing /etc/hosts (to ensure everyone's got the",
            "    # same list of hosts)",
            "    # local tmp_conf=/etc/hosts.$$",
            "    # invoke scp root@seed_host:/etc/hosts $tmp_conf \\",
            "    #   || error \"Can't retrieve /etc/hosts from seed_host\"",
            "    # install_tmp $tmp_conf /etc/hosts",
            "    rc, _, err = invoke(\"scp root@%s:'/etc/csync2/{csync2.cfg,key_hagroup}' /etc/csync2\" % (seed_host))",
            "    if not rc:",
            "        error(\"Can't retrieve csync2 config from {}: {}\".format(seed_host, err))",
            "",
            "    utils.start_service(\"csync2.socket\", enable=True)",
            "",
            "    # Sync new config out.  This goes to all hosts; csync2.cfg definitely",
            "    # needs to go to all hosts (else hosts other than the seed and the",
            "    # joining host won't have the joining host in their config yet).",
            "    # Strictly, the rest of the files need only go to the new host which",
            "    # could theoretically be effected using `csync2 -xv -P $(hostname)`,",
            "    # but this still leaves all the other files in dirty state (becuase",
            "    # they haven't gone to all nodes in the cluster, which means a",
            "    # subseqent join of another node can fail its sync of corosync.conf",
            "    # when it updates expected_votes.  Grrr...",
            "    if not invokerc('ssh -o StrictHostKeyChecking=no root@{} \"csync2 -rm /; csync2 -rxv || csync2 -rf / && csync2 -rxv\"'.format(seed_host)):",
            "        print(\"\")",
            "        warn(\"csync2 run failed - some files may not be sync'd\")",
            "",
            "    status_done()",
            "",
            "",
            "def join_ssh_merge(_cluster_node):",
            "    status(\"Merging known_hosts\")",
            "",
            "    me = utils.this_node()",
            "    hosts = [m.group(1)",
            "             for m in re.finditer(r\"^\\s*host\\s*([^ ;]+)\\s*;\", open(CSYNC2_CFG).read(), re.M)",
            "             if m.group(1) != me]",
            "    if not hosts:",
            "        hosts = [_cluster_node]",
            "        warn(\"Unable to extract host list from %s\" % (CSYNC2_CFG))",
            "",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        error(\"parallax python library is missing\")",
            "",
            "    opts = parallax.Options()",
            "    opts.ssh_options = ['StrictHostKeyChecking=no']",
            "",
            "    # The act of using pssh to connect to every host (without strict host key",
            "    # checking) ensures that at least *this* host has every other host in its",
            "    # known_hosts",
            "    known_hosts_new = set()",
            "    cat_cmd = \"[ -e /root/.ssh/known_hosts ] && cat /root/.ssh/known_hosts || true\"",
            "    log(\"parallax.call {} : {}\".format(hosts, cat_cmd))",
            "    results = parallax.call(hosts, cat_cmd, opts)",
            "    for host, result in results.items():",
            "        if isinstance(result, parallax.Error):",
            "            warn(\"Failed to get known_hosts from {}: {}\".format(host, str(result)))",
            "        else:",
            "            if result[1]:",
            "                known_hosts_new.update((utils.to_ascii(result[1]) or \"\").splitlines())",
            "    if known_hosts_new:",
            "        hoststxt = \"\\n\".join(sorted(known_hosts_new))",
            "        tmpf = utils.str2tmp(hoststxt)",
            "        log(\"parallax.copy {} : {}\".format(hosts, hoststxt))",
            "        results = parallax.copy(hosts, tmpf, \"/root/.ssh/known_hosts\")",
            "        for host, result in results.items():",
            "            if isinstance(result, parallax.Error):",
            "                warn(\"scp to {} failed ({}), known_hosts update may be incomplete\".format(host, str(result)))",
            "",
            "",
            "def update_expected_votes():",
            "    # get a list of nodes, excluding remote nodes",
            "    nodelist = None",
            "    loop_count = 0",
            "    device_votes = 0",
            "    nodecount = 0",
            "    expected_votes = 0",
            "    while True:",
            "        rc, nodelist_text = utils.get_stdout(\"cibadmin -Ql --xpath '/cib/status/node_state'\")",
            "        if rc == 0:",
            "            try:",
            "                nodelist_xml = etree.fromstring(nodelist_text)",
            "                nodelist = [n.get('uname') for n in nodelist_xml.xpath('//node_state') if n.get('remote_node') != 'true']",
            "                if len(nodelist) >= 2:",
            "                    break",
            "            except Exception:",
            "                break",
            "        # timeout: 10 seconds",
            "        if loop_count == 10:",
            "            break",
            "        loop_count += 1",
            "        sleep(1)",
            "",
            "    # Increase expected_votes",
            "    # TODO: wait to adjust expected_votes until after cluster join,",
            "    # so that we can ask the cluster for the current membership list",
            "    # Have to check if a qnetd device is configured and increase",
            "    # expected_votes in that case",
            "    is_qdevice_configured = utils.is_qdevice_configured()",
            "    if nodelist is None:",
            "        for v in corosync.get_values(\"quorum.expected_votes\"):",
            "            expected_votes = v",
            "",
            "            # For node >= 2, expected_votes = nodecount + device_votes",
            "            # Assume nodecount is N, for ffsplit, qdevice only has one vote",
            "            # which means that device_votes is 1, ie:expected_votes = N + 1;",
            "            # while for lms, qdevice has N - 1 votes, ie: expected_votes = N + (N - 1)",
            "            # and update quorum.device.net.algorithm based on device_votes",
            "",
            "            if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":",
            "                device_votes = int((expected_votes - 1) / 2)",
            "                nodecount = expected_votes - device_votes",
            "                # as nodecount will increase 1, and device_votes is nodecount - 1",
            "                # device_votes also increase 1",
            "                device_votes += 1",
            "            elif corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":",
            "                device_votes = 1",
            "                nodecount = expected_votes - device_votes",
            "            elif is_qdevice_configured:",
            "                device_votes = 0",
            "                nodecount = v",
            "",
            "            nodecount += 1",
            "            expected_votes = nodecount + device_votes",
            "            corosync.set_value(\"quorum.expected_votes\", str(expected_votes))",
            "    else:",
            "        nodecount = len(nodelist)",
            "        expected_votes = 0",
            "        # For node >= 2, expected_votes = nodecount + device_votes",
            "        # Assume nodecount is N, for ffsplit, qdevice only has one vote",
            "        # which means that device_votes is 1, ie:expected_votes = N + 1;",
            "        # while for lms, qdevice has N - 1 votes, ie: expected_votes = N + (N - 1)",
            "        if corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":",
            "            device_votes = 1",
            "        if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":",
            "            device_votes = nodecount - 1",
            "",
            "        if nodecount > 1:",
            "            expected_votes = nodecount + device_votes",
            "",
            "        if corosync.get_value(\"quorum.expected_votes\"):",
            "            corosync.set_value(\"quorum.expected_votes\", str(expected_votes))",
            "    if is_qdevice_configured:",
            "        corosync.set_value(\"quorum.device.votes\", device_votes)",
            "    corosync.set_value(\"quorum.two_node\", 1 if expected_votes == 2 else 0)",
            "",
            "    csync2_update(corosync.conf())",
            "",
            "",
            "def setup_passwordless_with_other_nodes(init_node):",
            "    \"\"\"",
            "    Setup passwordless with other cluster nodes",
            "",
            "    Should fetch the node list from init node, then swap the key",
            "    \"\"\"",
            "    # Fetch cluster nodes list",
            "    cmd = \"ssh -o StrictHostKeyChecking=no root@{} crm_node -l\".format(init_node)",
            "    rc, out, err = utils.get_stdout_stderr(cmd)",
            "    if rc != 0:",
            "        error(\"Can't fetch cluster nodes list from {}: {}\".format(init_node, err))",
            "    cluster_nodes_list = []",
            "    for line in out.splitlines():",
            "        _, node, stat = line.split()",
            "        if stat == \"member\":",
            "            cluster_nodes_list.append(node)",
            "",
            "    # Filter out init node from cluster_nodes_list",
            "    cmd = \"ssh -o StrictHostKeyChecking=no root@{} hostname\".format(init_node)",
            "    rc, out, err = utils.get_stdout_stderr(cmd)",
            "    if rc != 0:",
            "        error(\"Can't fetch hostname of {}: {}\".format(init_node, err))",
            "    if out in cluster_nodes_list:",
            "        cluster_nodes_list.remove(out)",
            "",
            "    # Swap ssh public key between join node and other cluster nodes",
            "    for node in cluster_nodes_list:",
            "        for user in USER_LIST:",
            "            swap_public_ssh_key(node, user)",
            "",
            "",
            "def join_cluster(seed_host):",
            "    \"\"\"",
            "    Cluster configuration for joining node.",
            "    \"\"\"",
            "    def get_local_nodeid():",
            "        # for IPv6",
            "        return utils.gen_nodeid_from_ipv6(_context.local_ip_list[0])",
            "",
            "    def update_nodeid(nodeid, node=None):",
            "        # for IPv6",
            "        if node and node != utils.this_node():",
            "            cmd = \"crm corosync set totem.nodeid %d\" % nodeid",
            "            invoke(\"crm cluster run '{}' {}\".format(cmd, node))",
            "        else:",
            "            corosync.set_value(\"totem.nodeid\", nodeid)",
            "",
            "    shutil.copy(corosync.conf(), COROSYNC_CONF_ORIG)",
            "",
            "    # check if use IPv6",
            "    ipv6_flag = False",
            "    ipv6 = corosync.get_value(\"totem.ip_version\")",
            "    if ipv6 and ipv6 == \"ipv6\":",
            "        ipv6_flag = True",
            "    _context.ipv6 = ipv6_flag",
            "",
            "    init_network()",
            "",
            "    # check whether have two rings",
            "    rrp_flag = False",
            "    rrp = corosync.get_value(\"totem.rrp_mode\")",
            "    if rrp in ('active', 'passive'):",
            "        rrp_flag = True",
            "",
            "    # Need to do this if second (or subsequent) node happens to be up and",
            "    # connected to storage while it's being repartitioned on the first node.",
            "    probe_partitions()",
            "",
            "    # It would be massively useful at this point if new nodes could come",
            "    # up in standby mode, so we could query the CIB locally to see if",
            "    # there was any further local setup that needed doing, e.g.: creating",
            "    # mountpoints for clustered filesystems.  Unfortunately we don't have",
            "    # that yet, so the following crawling horror takes a punt on the seed",
            "    # node being up, then asks it for a list of mountpoints...",
            "    if _context.cluster_node:",
            "        _rc, outp, _ = utils.get_stdout_stderr(\"ssh -o StrictHostKeyChecking=no root@{} 'cibadmin -Q --xpath \\\"//primitive\\\"'\".format(seed_host))",
            "        if outp:",
            "            xml = etree.fromstring(outp)",
            "            mountpoints = xml.xpath(' and '.join(['//primitive[@class=\"ocf\"',",
            "                                                  '@provider=\"heartbeat\"',",
            "                                                  '@type=\"Filesystem\"]']) +",
            "                                    '/instance_attributes/nvpair[@name=\"directory\"]/@value')",
            "            for m in mountpoints:",
            "                invoke(\"mkdir -p {}\".format(m))",
            "    else:",
            "        status(\"No existing IP/hostname specified - skipping mountpoint detection/creation\")",
            "",
            "    # Bump expected_votes in corosync.conf",
            "    # TODO(must): this is rather fragile (see related code in ha-cluster-remove)",
            "",
            "    # If corosync.conf() doesn't exist or is empty, we will fail here. (bsc#943227)",
            "    if not os.path.exists(corosync.conf()):",
            "        error(\"{} is not readable. Please ensure that hostnames are resolvable.\".format(corosync.conf()))",
            "",
            "    # if unicast, we need to add our node to $corosync.conf()",
            "    is_unicast = corosync.is_unicast()",
            "    if is_unicast:",
            "        ringXaddr_res = []",
            "        for i in 0, 1:",
            "            while True:",
            "                ringXaddr = prompt_for_string(",
            "                        'Address for ring{}'.format(i),",
            "                        default=pick_default_value(_context.default_ip_list, ringXaddr_res),",
            "                        valid_func=Validation.valid_ucast_ip,",
            "                        prev_value=ringXaddr_res)",
            "                if not ringXaddr:",
            "                    error(\"No value for ring{}\".format(i))",
            "                ringXaddr_res.append(ringXaddr)",
            "                break",
            "            if not rrp_flag:",
            "                break",
            "        print(\"\")",
            "        invoke(\"rm -f /var/lib/heartbeat/crm/* /var/lib/pacemaker/cib/*\")",
            "        try:",
            "            corosync.add_node_ucast(ringXaddr_res)",
            "        except corosync.IPAlreadyConfiguredError as e:",
            "            warn(e)",
            "        csync2_update(corosync.conf())",
            "        invoke(\"ssh -o StrictHostKeyChecking=no root@{} corosync-cfgtool -R\".format(seed_host))",
            "",
            "    _context.sbd_manager.join_sbd(seed_host)",
            "",
            "    if ipv6_flag and not is_unicast:",
            "        # for ipv6 mcast",
            "        # using ipv6 need nodeid configured",
            "        local_nodeid = get_local_nodeid()",
            "        update_nodeid(local_nodeid)",
            "",
            "    is_qdevice_configured = utils.is_qdevice_configured()",
            "    if is_qdevice_configured and not is_unicast:",
            "        # expected_votes here maybe is \"0\", set to \"3\" to make sure cluster can start",
            "        corosync.set_value(\"quorum.expected_votes\", \"3\")",
            "",
            "    # Initialize the cluster before adjusting quorum. This is so",
            "    # that we can query the cluster to find out how many nodes",
            "    # there are (so as not to adjust multiple times if a previous",
            "    # attempt to join the cluster failed)",
            "    init_cluster_local()",
            "",
            "    status_long(\"Reloading cluster configuration\")",
            "",
            "    if ipv6_flag and not is_unicast:",
            "        # for ipv6 mcast",
            "        nodeid_dict = {}",
            "        _rc, outp, _ = utils.get_stdout_stderr(\"crm_node -l\")",
            "        if _rc == 0:",
            "            for line in outp.split('\\n'):",
            "                tmp = line.split()",
            "                nodeid_dict[tmp[1]] = tmp[0]",
            "",
            "    # apply nodelist in cluster",
            "    if is_unicast or is_qdevice_configured:",
            "        invoke(\"crm cluster run 'crm corosync reload'\")",
            "",
            "    update_expected_votes()",
            "    # Trigger corosync config reload to ensure expected_votes is propagated",
            "    invoke(\"corosync-cfgtool -R\")",
            "",
            "    # Ditch no-quorum-policy=ignore",
            "    _rc, outp = utils.get_stdout(\"crm configure show\")",
            "    if re.search('no-quorum-policy=.*ignore', outp):",
            "        invoke(\"crm_attribute --attr-name no-quorum-policy --delete-attr\")",
            "",
            "    # if unicast, we need to reload the corosync configuration",
            "    # on the other nodes",
            "    if is_unicast:",
            "        invoke(\"crm cluster run 'crm corosync reload'\")",
            "",
            "    if ipv6_flag and not is_unicast:",
            "        # for ipv6 mcast",
            "        # after csync2_update, all config files are same",
            "        # but nodeid must be uniqe",
            "        for node in list(nodeid_dict.keys()):",
            "            if node == utils.this_node():",
            "                continue",
            "            update_nodeid(int(nodeid_dict[node]), node)",
            "        update_nodeid(local_nodeid)",
            "    status_done()",
            "",
            "    if is_qdevice_configured:",
            "        start_qdevice_on_join_node(seed_host)",
            "    else:",
            "        utils.disable_service(\"corosync-qdevice.service\")",
            "",
            "",
            "def start_qdevice_on_join_node(seed_host):",
            "    \"\"\"",
            "    Doing qdevice certificate process and start qdevice service on join node",
            "    \"\"\"",
            "    status_long(\"Starting corosync-qdevice.service\")",
            "    if not corosync.is_unicast():",
            "        corosync.add_nodelist_from_cmaptool()",
            "        csync2_update(corosync.conf())",
            "        invoke(\"crm corosync reload\")",
            "    if utils.is_qdevice_tls_on():",
            "        qnetd_addr = corosync.get_value(\"quorum.device.net.host\")",
            "        qdevice_inst = corosync.QDevice(qnetd_addr, cluster_node=seed_host)",
            "        qdevice_inst.certificate_process_on_join()",
            "    utils.start_service(\"corosync-qdevice.service\", enable=True)",
            "    status_done()",
            "",
            "",
            "def set_cluster_node_ip():",
            "    \"\"\"",
            "    ringx_addr might be hostname or IP",
            "    _context.cluster_node by now is always hostname",
            "",
            "    If ring0_addr is IP, we should get the configured iplist which belong _context.cluster_node",
            "    Then filter out which one is configured as ring0_addr",
            "    At last assign that ip to _context.cluster_node_ip which will be removed later",
            "    \"\"\"",
            "    node = _context.cluster_node",
            "    addr_list = corosync.get_values('nodelist.node.ring0_addr')",
            "    if node in addr_list:",
            "        return",
            "",
            "    ip_list = utils.get_iplist_from_name(node)",
            "    for ip in ip_list:",
            "        if ip in addr_list:",
            "            _context.cluster_node_ip = ip",
            "            break",
            "",
            "",
            "def stop_services(stop_list, remote_addr=None):",
            "    \"\"\"",
            "    Stop cluster related service",
            "    \"\"\"",
            "    for service in stop_list:",
            "        if utils.service_is_active(service, remote_addr=remote_addr):",
            "            status(\"Stopping the {}\".format(service))",
            "            utils.stop_service(service, disable=True, remote_addr=remote_addr)",
            "",
            "",
            "def remove_node_from_cluster():",
            "    \"\"\"",
            "    Remove node from running cluster and the corosync / pacemaker configuration.",
            "    \"\"\"",
            "    node = _context.cluster_node",
            "    set_cluster_node_ip()",
            "",
            "    stop_services(SERVICES_STOP_LIST, remote_addr=node)",
            "",
            "    # delete configuration files from the node to be removed",
            "    rc, _, err = invoke('ssh -o StrictHostKeyChecking=no root@{} \"bash -c \\\\\\\"rm -f {}\\\\\\\"\"'.format(node, \" \".join(_context.rm_list)))",
            "    if not rc:",
            "        error(\"Deleting the configuration files failed: {}\".format(err))",
            "",
            "    # execute the command : crm node delete $HOSTNAME",
            "    status(\"Removing the node {}\".format(node))",
            "    if not invokerc(\"crm node delete {}\".format(node)):",
            "        error(\"Failed to remove {}\".format(node))",
            "",
            "    if not invokerc(\"sed -i /{}/d {}\".format(node, CSYNC2_CFG)):",
            "        error(\"Removing the node {} from {} failed\".format(node, CSYNC2_CFG))",
            "",
            "    # Remove node from nodelist",
            "    if corosync.get_values(\"nodelist.node.ring0_addr\"):",
            "        del_target = _context.cluster_node_ip or node",
            "        corosync.del_node(del_target)",
            "",
            "    decrease_expected_votes()",
            "",
            "    status(\"Propagating configuration changes across the remaining nodes\")",
            "    csync2_update(CSYNC2_CFG)",
            "    csync2_update(corosync.conf())",
            "",
            "    # Trigger corosync config reload to ensure expected_votes is propagated",
            "    invoke(\"corosync-cfgtool -R\")",
            "",
            "",
            "def decrease_expected_votes():",
            "    '''",
            "    Decrement expected_votes in corosync.conf",
            "    '''",
            "    vote = corosync.get_value(\"quorum.expected_votes\")",
            "    if not vote:",
            "        return",
            "    quorum = int(vote)",
            "    new_quorum = quorum - 1",
            "    if utils.is_qdevice_configured():",
            "        new_nodecount = 0",
            "        device_votes = 0",
            "        nodecount = 0",
            "",
            "        if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":",
            "            nodecount = int((quorum + 1)/2)",
            "            new_nodecount = nodecount - 1",
            "            device_votes = new_nodecount - 1",
            "",
            "        elif corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":",
            "            device_votes = 1",
            "            nodecount = quorum - device_votes",
            "            new_nodecount = nodecount - 1",
            "",
            "        if new_nodecount > 1:",
            "            new_quorum = new_nodecount + device_votes",
            "        else:",
            "            new_quorum = 0",
            "",
            "        corosync.set_value(\"quorum.device.votes\", device_votes)",
            "    else:",
            "        corosync.set_value(\"quorum.two_node\", 1 if new_quorum == 2 else 0)",
            "    corosync.set_value(\"quorum.expected_votes\", str(new_quorum))",
            "",
            "",
            "def bootstrap_init(context):",
            "    \"\"\"",
            "    Init cluster process",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "",
            "    init()",
            "    _context.initialize_qdevice()",
            "    _context.validate_option()",
            "    _context.init_sbd_manager()",
            "",
            "    stage = _context.stage",
            "    if stage is None:",
            "        stage = \"\"",
            "",
            "    # vgfs stage requires running cluster, everything else requires inactive cluster,",
            "    # except ssh and csync2 (which don't care) and csync2_remote (which mustn't care,",
            "    # just in case this breaks ha-cluster-join on another node).",
            "    corosync_active = utils.service_is_active(\"corosync.service\")",
            "    if stage in (\"vgfs\", \"admin\", \"qdevice\"):",
            "        if not corosync_active:",
            "            error(\"Cluster is inactive - can't run %s stage\" % (stage))",
            "    elif stage == \"\":",
            "        if corosync_active:",
            "            error(\"Cluster is currently active - can't run\")",
            "    elif stage not in (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\"):",
            "        if corosync_active:",
            "            error(\"Cluster is currently active - can't run %s stage\" % (stage))",
            "",
            "    # Need hostname resolution to work, want NTP (but don't block ssh_remote or csync2_remote)",
            "    if stage not in ('ssh_remote', 'csync2_remote'):",
            "        check_tty()",
            "        if not check_prereqs(stage):",
            "            return",
            "    elif stage == 'csync2_remote':",
            "        args = _context.args",
            "        log(\"args: {}\".format(args))",
            "        if len(args) != 2:",
            "            error(\"Expected NODE argument to csync2_remote\")",
            "        _context.cluster_node = args[1]",
            "",
            "    if stage != \"\":",
            "        globals()[\"init_\" + stage]()",
            "    else:",
            "        init_ssh()",
            "        init_csync2()",
            "        init_corosync()",
            "        init_remote_auth()",
            "        if _context.template == 'ocfs2':",
            "            if _context.sbd_device is None or _context.ocfs2_device is None:",
            "                init_storage()",
            "        init_sbd()",
            "",
            "        lock_inst = lock.Lock()",
            "        try:",
            "            with lock_inst.lock():",
            "                init_cluster()",
            "                if _context.template == 'ocfs2':",
            "                    init_vgfs()",
            "                init_admin()",
            "                init_qdevice()",
            "        except lock.ClaimLockError as err:",
            "            error(err)",
            "",
            "    status(\"Done (log saved to %s)\" % (LOG_FILE))",
            "",
            "",
            "def bootstrap_join(context):",
            "    \"\"\"",
            "    Join cluster process",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "",
            "    init()",
            "    _context.init_sbd_manager()",
            "    _context.validate_option()",
            "",
            "    check_tty()",
            "",
            "    corosync_active = utils.service_is_active(\"corosync.service\")",
            "    if corosync_active and _context.stage != \"ssh\":",
            "        error(\"Abort: Cluster is currently active. Run this command on a node joining the cluster.\")",
            "",
            "    if not check_prereqs(\"join\"):",
            "        return",
            "",
            "    cluster_node = _context.cluster_node",
            "    if _context.stage != \"\":",
            "        globals()[\"join_\" + _context.stage](cluster_node)",
            "    else:",
            "        if not _context.yes_to_all and cluster_node is None:",
            "            status(\"\"\"Join This Node to Cluster:",
            "  You will be asked for the IP address of an existing node, from which",
            "  configuration will be copied.  If you have not already configured",
            "  passwordless ssh between nodes, you will be prompted for the root",
            "  password of the existing node.",
            "\"\"\")",
            "            cluster_node = prompt_for_string(\"IP address or hostname of existing node (e.g.: 192.168.1.1)\", \".+\")",
            "            _context.cluster_node = cluster_node",
            "",
            "        utils.ping_node(cluster_node)",
            "",
            "        join_ssh(cluster_node)",
            "",
            "        if not utils.service_is_active(\"pacemaker.service\", cluster_node):",
            "            error(\"Cluster is inactive on {}\".format(cluster_node))",
            "",
            "        lock_inst = lock.RemoteLock(cluster_node)",
            "        try:",
            "            with lock_inst.lock():",
            "                setup_passwordless_with_other_nodes(cluster_node)",
            "                join_remote_auth(cluster_node)",
            "                join_csync2(cluster_node)",
            "                join_ssh_merge(cluster_node)",
            "                join_cluster(cluster_node)",
            "        except (lock.SSHError, lock.ClaimLockError) as err:",
            "            error(err)",
            "",
            "    status(\"Done (log saved to %s)\" % (LOG_FILE))",
            "",
            "",
            "def join_remote_auth(node):",
            "    if os.path.exists(PCMK_REMOTE_AUTH):",
            "        rmfile(PCMK_REMOTE_AUTH)",
            "    pcmk_remote_dir = os.path.dirname(PCMK_REMOTE_AUTH)",
            "    mkdirs_owned(pcmk_remote_dir, mode=0o750, gid=\"haclient\")",
            "    invoke(\"touch {}\".format(PCMK_REMOTE_AUTH))",
            "",
            "",
            "def remove_qdevice():",
            "    \"\"\"",
            "    Remove qdevice service and configuration from cluster",
            "    \"\"\"",
            "    if not utils.is_qdevice_configured():",
            "        error(\"No QDevice configuration in this cluster\")",
            "    if not confirm(\"Removing QDevice service and configuration from cluster: Are you sure?\"):",
            "        return",
            "",
            "    status(\"Disable corosync-qdevice.service\")",
            "    invoke(\"crm cluster run 'systemctl disable corosync-qdevice'\")",
            "    status(\"Stopping corosync-qdevice.service\")",
            "    invoke(\"crm cluster run 'systemctl stop corosync-qdevice'\")",
            "",
            "    status_long(\"Removing QDevice configuration from cluster\")",
            "    qnetd_host = corosync.get_value('quorum.device.net.host')",
            "    qdevice_inst = corosync.QDevice(qnetd_host)",
            "    qdevice_inst.remove_qdevice_config()",
            "    qdevice_inst.remove_qdevice_db()",
            "    update_expected_votes()",
            "    invoke(\"crm cluster run 'crm corosync reload'\")",
            "    status_done()",
            "",
            "",
            "def bootstrap_remove(context):",
            "    \"\"\"",
            "    Remove node from cluster, or remove qdevice configuration",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "    force_flag = config.core.force or _context.force",
            "",
            "    init()",
            "",
            "    if not utils.service_is_active(\"corosync.service\"):",
            "        error(\"Cluster is not active - can't execute removing action\")",
            "",
            "    if _context.qdevice_rm_flag and _context.cluster_node:",
            "        error(\"Either remove node or qdevice\")",
            "",
            "    if _context.qdevice_rm_flag:",
            "        remove_qdevice()",
            "        return",
            "",
            "    if not _context.yes_to_all and _context.cluster_node is None:",
            "        status(\"\"\"Remove This Node from Cluster:",
            "  You will be asked for the IP address or name of an existing node,",
            "  which will be removed from the cluster. This command must be",
            "  executed from a different node in the cluster.",
            "\"\"\")",
            "        _context.cluster_node = prompt_for_string(\"IP address or hostname of cluster node (e.g.: 192.168.1.1)\", \".+\")",
            "",
            "    if not _context.cluster_node:",
            "        error(\"No existing IP/hostname specified (use -c option)\")",
            "",
            "    _context.cluster_node = get_cluster_node_hostname()",
            "",
            "    if not force_flag and not confirm(\"Removing node \\\"{}\\\" from the cluster: Are you sure?\".format(_context.cluster_node)):",
            "        return",
            "",
            "    if _context.cluster_node == utils.this_node():",
            "        if not force_flag:",
            "            error(\"Removing self requires --force\")",
            "        remove_self()",
            "        return",
            "",
            "    if _context.cluster_node in xmlutil.listnodes():",
            "        remove_node_from_cluster()",
            "    else:",
            "        error(\"Specified node {} is not configured in cluster! Unable to remove.\".format(_context.cluster_node))",
            "",
            "",
            "def remove_self():",
            "    me = _context.cluster_node",
            "    yes_to_all = _context.yes_to_all",
            "    nodes = xmlutil.listnodes(include_remote_nodes=False)",
            "    othernode = next((x for x in nodes if x != me), None)",
            "    if othernode is not None:",
            "        # remove from other node",
            "        cmd = \"crm cluster remove{} -c {}\".format(\" -y\" if yes_to_all else \"\", me)",
            "        rc = utils.ext_cmd_nosudo(\"ssh{} -o StrictHostKeyChecking=no {} '{}'\".format(\"\" if yes_to_all else \" -t\", othernode, cmd))",
            "        if rc != 0:",
            "            error(\"Failed to remove this node from {}\".format(othernode))",
            "    else:",
            "        # disable and stop cluster",
            "        stop_services(SERVICES_STOP_LIST)",
            "        # remove all trace of cluster from this node",
            "        # delete configuration files from the node to be removed",
            "        if not invokerc('bash -c \"rm -f {}\"'.format(\" \".join(_context.rm_list))):",
            "            error(\"Deleting the configuration files failed\")",
            "",
            "",
            "def init_common_geo():",
            "    \"\"\"",
            "    Tasks to do both on first and other geo nodes.",
            "    \"\"\"",
            "    if not utils.package_is_installed(\"booth\"):",
            "        error(\"Booth not installed - Not configurable as a geo cluster node.\")",
            "",
            "",
            "BOOTH_CFG = \"/etc/booth/booth.conf\"",
            "BOOTH_AUTH = \"/etc/booth/authkey\"",
            "",
            "",
            "def init_csync2_geo():",
            "    \"\"\"",
            "    TODO: Configure csync2 for geo cluster",
            "    That is, create a second sync group which",
            "    syncs the geo configuration across the whole",
            "    geo cluster.",
            "    \"\"\"",
            "",
            "",
            "def create_booth_authkey():",
            "    status(\"Create authentication key for booth\")",
            "    if os.path.exists(BOOTH_AUTH):",
            "        rmfile(BOOTH_AUTH)",
            "    rc, _, err = invoke(\"booth-keygen {}\".format(BOOTH_AUTH))",
            "    if not rc:",
            "        error(\"Failed to generate booth authkey: {}\".format(err))",
            "",
            "",
            "def create_booth_config(arbitrator, clusters, tickets):",
            "    status(\"Configure booth\")",
            "",
            "    config_template = \"\"\"# The booth configuration file is \"/etc/booth/booth.conf\". You need to",
            "# prepare the same booth configuration file on each arbitrator and",
            "# each node in the cluster sites where the booth daemon can be launched.",
            "",
            "# \"transport\" means which transport layer booth daemon will use.",
            "# Currently only \"UDP\" is supported.",
            "transport=\"UDP\"",
            "port=\"9929\"",
            "\"\"\"",
            "    cfg = [config_template]",
            "    if arbitrator is not None:",
            "        cfg.append(\"arbitrator=\\\"{}\\\"\".format(arbitrator))",
            "    for s in clusters.values():",
            "        cfg.append(\"site=\\\"{}\\\"\".format(s))",
            "    cfg.append(\"authfile=\\\"{}\\\"\".format(BOOTH_AUTH))",
            "    for t in tickets:",
            "        cfg.append(\"ticket=\\\"{}\\\"\\nexpire=\\\"600\\\"\".format(t))",
            "    cfg = \"\\n\".join(cfg) + \"\\n\"",
            "",
            "    if os.path.exists(BOOTH_CFG):",
            "        rmfile(BOOTH_CFG)",
            "    utils.str2file(cfg, BOOTH_CFG)",
            "    utils.chown(BOOTH_CFG, \"hacluster\", \"haclient\")",
            "    os.chmod(BOOTH_CFG, 0o644)",
            "",
            "",
            "def bootstrap_init_geo(context):",
            "    \"\"\"",
            "    Configure as a geo cluster member.",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "",
            "    if os.path.exists(BOOTH_CFG) and not confirm(\"This will overwrite {} - continue?\".format(BOOTH_CFG)):",
            "        return",
            "    if os.path.exists(BOOTH_AUTH) and not confirm(\"This will overwrite {} - continue?\".format(BOOTH_AUTH)):",
            "        return",
            "",
            "    init_common_geo()",
            "",
            "    # TODO:",
            "    # in /etc/drbd.conf or /etc/drbd.d/global_common.conf",
            "    # set common.startup.wfc-timeout 100",
            "    # set common.startup.degr-wfc-timeout 120",
            "",
            "    create_booth_authkey()",
            "    create_booth_config(_context.arbitrator, _context.clusters, _context.tickets)",
            "    status(\"Sync booth configuration across cluster\")",
            "    csync2_update(\"/etc/booth\")",
            "    init_csync2_geo()",
            "    geo_cib_config(_context.clusters)",
            "",
            "",
            "def geo_fetch_config(node):",
            "    # TODO: clean this up",
            "    status(\"Retrieving configuration - This may prompt for root@%s:\" % (node))",
            "    tmpdir = tmpfiles.create_dir()",
            "    rc, _, err = invoke(\"scp -oStrictHostKeyChecking=no root@%s:'/etc/booth/*' %s/\" % (node, tmpdir))",
            "    if not rc:",
            "        error(\"Failed to retrieve configuration: {}\".format(err))",
            "    try:",
            "        if os.path.isfile(\"%s/authkey\" % (tmpdir)):",
            "            invoke(\"mv %s/authkey %s\" % (tmpdir, BOOTH_AUTH))",
            "            os.chmod(BOOTH_AUTH, 0o600)",
            "        if os.path.isfile(\"%s/booth.conf\" % (tmpdir)):",
            "            invoke(\"mv %s/booth.conf %s\" % (tmpdir, BOOTH_CFG))",
            "            os.chmod(BOOTH_CFG, 0o644)",
            "    except OSError as err:",
            "        raise ValueError(\"Problem encountered with booth configuration from {}: {}\".format(node, err))",
            "",
            "",
            "def geo_cib_config(clusters):",
            "    cluster_name = corosync.get_values('totem.cluster_name')[0]",
            "    if cluster_name not in list(clusters.keys()):",
            "        error(\"Local cluster name is {}, expected {}\".format(cluster_name, \"|\".join(list(clusters.keys()))))",
            "",
            "    status(\"Configure cluster resources for booth\")",
            "    crm_template = Template(\"\"\"",
            "primitive booth-ip ocf:heartbeat:IPaddr2 $iprules",
            "primitive booth-site ocf:pacemaker:booth-site \\",
            "  meta resource-stickiness=\"INFINITY\" \\",
            "  params config=booth op monitor interval=\"10s\"",
            "group g-booth booth-ip booth-site meta target-role=Stopped",
            "\"\"\")",
            "    iprule = 'params rule #cluster-name eq {} ip=\"{}\"'",
            "",
            "    crm_configure_load(\"update\", crm_template.substitute(iprules=\" \".join(iprule.format(k, v) for k, v in clusters.items())))",
            "",
            "",
            "def bootstrap_join_geo(context):",
            "    \"\"\"",
            "    Run on second cluster to add to a geo configuration.",
            "    It fetches its booth configuration from the other node (cluster node or arbitrator).",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "    init_common_geo()",
            "    check_tty()",
            "    geo_fetch_config(_context.cluster_node)",
            "    status(\"Sync booth configuration across cluster\")",
            "    csync2_update(\"/etc/booth\")",
            "    geo_cib_config(_context.clusters)",
            "",
            "",
            "def bootstrap_arbitrator(context):",
            "    \"\"\"",
            "    Configure this machine as an arbitrator.",
            "    It fetches its booth configuration from a cluster node already in the cluster.",
            "    \"\"\"",
            "    global _context",
            "    _context = context",
            "    node = _context.cluster_node",
            "",
            "    init_common_geo()",
            "    check_tty()",
            "    geo_fetch_config(node)",
            "    if not os.path.isfile(BOOTH_CFG):",
            "        error(\"Failed to copy {} from {}\".format(BOOTH_CFG, node))",
            "    # TODO: verify that the arbitrator IP in the configuration is us?",
            "    status(\"Enabling and starting the booth arbitrator service\")",
            "    utils.start_service(\"booth@booth\", enable=True)",
            "",
            "# EOF"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "45": [
                "RSA_PRIVATE_KEY"
            ],
            "46": [
                "RSA_PUBLIC_KEY"
            ],
            "47": [
                "AUTHORIZED_KEYS_FILE"
            ],
            "1137": [
                "init_ssh"
            ],
            "1140": [
                "configure_local_ssh_key"
            ],
            "1144": [
                "configure_local_ssh_key"
            ],
            "1145": [
                "configure_local_ssh_key"
            ],
            "1147": [
                "configure_local_ssh_key"
            ],
            "1148": [
                "configure_local_ssh_key"
            ],
            "1149": [
                "configure_local_ssh_key"
            ],
            "1150": [
                "configure_local_ssh_key"
            ],
            "1151": [
                "configure_local_ssh_key"
            ],
            "1152": [
                "configure_local_ssh_key"
            ],
            "1874": [
                "join_ssh"
            ],
            "1875": [
                "join_ssh"
            ],
            "1886": [
                "swap_public_ssh_key"
            ],
            "1891": [
                "swap_public_ssh_key"
            ],
            "1893": [
                "swap_public_ssh_key"
            ],
            "1895": [
                "swap_public_ssh_key"
            ],
            "1899": [
                "swap_public_ssh_key"
            ],
            "1906": [
                "swap_public_ssh_key"
            ],
            "1909": [
                "fetch_public_key_from_remote_node"
            ],
            "1919": [
                "fetch_public_key_from_remote_node"
            ],
            "2131": [
                "setup_passwordless_with_other_nodes"
            ],
            "2490": [
                "bootstrap_join"
            ]
        },
        "addLocation": []
    },
    "crmsh/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 344,
                "afterPatchRowNumber": 344,
                "PatchRowcode": "     return cmd"
            },
            "1": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": 345,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": 346,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 347,
                "PatchRowcode": "+def add_su(cmd, user):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 348,
                "PatchRowcode": "+    \"\"\""
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 349,
                "PatchRowcode": "+    Wrapped cmd with su -c \"<cmd>\" <user>"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 350,
                "PatchRowcode": "+    \"\"\""
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 351,
                "PatchRowcode": "+    if user == \"root\":"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 352,
                "PatchRowcode": "+        return cmd"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 353,
                "PatchRowcode": "+    return \"su -c \\\"{}\\\" {}\".format(cmd, user)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 354,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 355,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": 347,
                "afterPatchRowNumber": 356,
                "PatchRowcode": " def chown(path, user, group):"
            },
            "13": {
                "beforePatchRowNumber": 348,
                "afterPatchRowNumber": 357,
                "PatchRowcode": "     if isinstance(user, int):"
            },
            "14": {
                "beforePatchRowNumber": 349,
                "afterPatchRowNumber": 358,
                "PatchRowcode": "         uid = user"
            },
            "15": {
                "beforePatchRowNumber": 2084,
                "afterPatchRowNumber": 2093,
                "PatchRowcode": "     return re.findall(r'id\\s*=\\s*(.*)', out)"
            },
            "16": {
                "beforePatchRowNumber": 2085,
                "afterPatchRowNumber": 2094,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 2086,
                "afterPatchRowNumber": 2095,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 2087,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def check_ssh_passwd_need(host):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2096,
                "PatchRowcode": "+def check_ssh_passwd_need(host, user=\"root\"):"
            },
            "20": {
                "beforePatchRowNumber": 2088,
                "afterPatchRowNumber": 2097,
                "PatchRowcode": "     \"\"\""
            },
            "21": {
                "beforePatchRowNumber": 2089,
                "afterPatchRowNumber": 2098,
                "PatchRowcode": "     Check whether access to host need password"
            },
            "22": {
                "beforePatchRowNumber": 2090,
                "afterPatchRowNumber": 2099,
                "PatchRowcode": "     \"\"\""
            },
            "23": {
                "beforePatchRowNumber": 2091,
                "afterPatchRowNumber": 2100,
                "PatchRowcode": "     ssh_options = \"-o StrictHostKeyChecking=no -o EscapeChar=none -o ConnectTimeout=15\""
            },
            "24": {
                "beforePatchRowNumber": 2092,
                "afterPatchRowNumber": 2101,
                "PatchRowcode": "     ssh_cmd = \"ssh {} -T -o Batchmode=yes {} true\".format(ssh_options, host)"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2102,
                "PatchRowcode": "+    ssh_cmd = add_su(ssh_cmd, user)"
            },
            "26": {
                "beforePatchRowNumber": 2093,
                "afterPatchRowNumber": 2103,
                "PatchRowcode": "     rc, _, _ = get_stdout_stderr(ssh_cmd)"
            },
            "27": {
                "beforePatchRowNumber": 2094,
                "afterPatchRowNumber": 2104,
                "PatchRowcode": "     return rc != 0"
            },
            "28": {
                "beforePatchRowNumber": 2095,
                "afterPatchRowNumber": 2105,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright (C) 2008-2011 Dejan Muhamedagic <dmuhamedagic@suse.de>",
            "# See COPYING for license information.",
            "",
            "import os",
            "import sys",
            "from tempfile import mkstemp",
            "import subprocess",
            "import re",
            "import glob",
            "import time",
            "import datetime",
            "import shutil",
            "import shlex",
            "import bz2",
            "import fnmatch",
            "import gc",
            "import ipaddress",
            "import argparse",
            "from contextlib import contextmanager, closing",
            "from . import config",
            "from . import userdir",
            "from . import constants",
            "from . import options",
            "from . import term",
            "from . import parallax",
            "from .msg import common_warn, common_info, common_debug, common_err, err_buf",
            "",
            "",
            "def to_ascii(input_str):",
            "    \"\"\"Convert the bytes string to a ASCII string",
            "    Usefull to remove accent (diacritics)\"\"\"",
            "    if input_str is None:",
            "        return input_str",
            "    if isinstance(input_str, str):",
            "        return input_str",
            "    try:",
            "        return str(input_str, 'utf-8')",
            "    except UnicodeDecodeError:",
            "        if config.core.debug or options.regression_tests:",
            "            import traceback",
            "            traceback.print_exc()",
            "        return input_str.decode('utf-8', errors='ignore')",
            "",
            "",
            "def filter_keys(key_list, args, sign=\"=\"):",
            "    \"\"\"Return list item which not be completed yet\"\"\"",
            "    return [s+sign for s in key_list if any_startswith(args, s+sign) is None]",
            "",
            "",
            "def any_startswith(iterable, prefix):",
            "    \"\"\"Return first element in iterable which startswith prefix, or None.\"\"\"",
            "    for element in iterable:",
            "        if element.startswith(prefix):",
            "            return element",
            "    return None",
            "",
            "",
            "def rindex(iterable, value):",
            "    return len(iterable) - iterable[::-1].index(value) - 1",
            "",
            "",
            "def memoize(function):",
            "    \"Decorator to invoke a function once only for any argument\"",
            "    memoized = {}",
            "",
            "    def inner(*args):",
            "        if args in memoized:",
            "            return memoized[args]",
            "        r = function(*args)",
            "        memoized[args] = r",
            "        return r",
            "    return inner",
            "",
            "",
            "@contextmanager",
            "def nogc():",
            "    gc.disable()",
            "    try:",
            "        yield",
            "    finally:",
            "        gc.enable()",
            "",
            "",
            "getuser = userdir.getuser",
            "gethomedir = userdir.gethomedir",
            "",
            "",
            "@memoize",
            "def this_node():",
            "    'returns name of this node (hostname)'",
            "    return os.uname()[1]",
            "",
            "",
            "_cib_shadow = 'CIB_shadow'",
            "_cib_in_use = ''",
            "",
            "",
            "def set_cib_in_use(name):",
            "    os.putenv(_cib_shadow, name)",
            "    global _cib_in_use",
            "    _cib_in_use = name",
            "",
            "",
            "def clear_cib_in_use():",
            "    os.unsetenv(_cib_shadow)",
            "    global _cib_in_use",
            "    _cib_in_use = ''",
            "",
            "",
            "def get_cib_in_use():",
            "    return _cib_in_use",
            "",
            "",
            "def get_tempdir():",
            "    return os.getenv(\"TMPDIR\") or \"/tmp\"",
            "",
            "",
            "def is_program(prog):",
            "    \"\"\"Is this program available?\"\"\"",
            "    def isexec(filename):",
            "        return os.path.isfile(filename) and os.access(filename, os.X_OK)",
            "    for p in os.getenv(\"PATH\").split(os.pathsep):",
            "        f = os.path.join(p, prog)",
            "        if isexec(f):",
            "            return f",
            "    return None",
            "",
            "",
            "def pacemaker_20_daemon(new, old):",
            "    \"helper to discover renamed pacemaker daemons\"",
            "    if is_program(new):",
            "        return new",
            "    return old",
            "",
            "",
            "@memoize",
            "def pacemaker_attrd():",
            "    return pacemaker_20_daemon(\"pacemaker-attrd\", \"attrd\")",
            "",
            "",
            "@memoize",
            "def pacemaker_based():",
            "    return pacemaker_20_daemon(\"pacemaker-based\", \"cib\")",
            "",
            "",
            "@memoize",
            "def pacemaker_controld():",
            "    return pacemaker_20_daemon(\"pacemaker-controld\", \"crmd\")",
            "",
            "",
            "@memoize",
            "def pacemaker_execd():",
            "    return pacemaker_20_daemon(\"pacemaker-execd\", \"lrmd\")",
            "",
            "",
            "@memoize",
            "def pacemaker_fenced():",
            "    return pacemaker_20_daemon(\"pacemaker-fenced\", \"stonithd\")",
            "",
            "",
            "@memoize",
            "def pacemaker_remoted():",
            "    return pacemaker_20_daemon(\"pacemaker-remoted\", \"pacemaker_remoted\")",
            "",
            "",
            "@memoize",
            "def pacemaker_schedulerd():",
            "    return pacemaker_20_daemon(\"pacemaker-schedulerd\", \"pengine\")",
            "",
            "",
            "def pacemaker_daemon(name):",
            "    if name == \"attrd\" or name == \"pacemaker-attrd\":",
            "        return pacemaker_attrd()",
            "    if name == \"cib\" or name == \"pacemaker-based\":",
            "        return pacemaker_based()",
            "    if name == \"crmd\" or name == \"pacemaker-controld\":",
            "        return pacemaker_controld()",
            "    if name == \"lrmd\" or name == \"pacemaker-execd\":",
            "        return pacemaker_execd()",
            "    if name == \"stonithd\" or name == \"pacemaker-fenced\":",
            "        return pacemaker_fenced()",
            "    if name == \"pacemaker_remoted\" or name == \"pacemeaker-remoted\":",
            "        return pacemaker_remoted()",
            "    if name == \"pengine\" or name == \"pacemaker-schedulerd\":",
            "        return pacemaker_schedulerd()",
            "    raise ValueError(\"Not a Pacemaker daemon name: {}\".format(name))",
            "",
            "",
            "def can_ask():",
            "    \"\"\"",
            "    Is user-interactivity possible?",
            "    Checks if connected to a TTY.",
            "    \"\"\"",
            "    return (not options.ask_no) and sys.stdin.isatty()",
            "",
            "",
            "def ask(msg):",
            "    \"\"\"",
            "    Ask for user confirmation.",
            "    If core.force is true, always return true.",
            "    If not interactive and core.force is false, always return false.",
            "    \"\"\"",
            "    if config.core.force:",
            "        common_info(\"%s [YES]\" % (msg))",
            "        return True",
            "    if not can_ask():",
            "        return False",
            "",
            "    msg += ' '",
            "    if msg.endswith('? '):",
            "        msg = msg[:-2] + ' (y/n)? '",
            "",
            "    while True:",
            "        try:",
            "            ans = input(msg)",
            "        except EOFError:",
            "            ans = 'n'",
            "        if ans:",
            "            ans = ans[0].lower()",
            "            if ans in 'yn':",
            "                return ans == 'y'",
            "",
            "",
            "# holds part of line before \\ split",
            "# for a multi-line input",
            "_LINE_BUFFER = ''",
            "",
            "",
            "def get_line_buffer():",
            "    return _LINE_BUFFER",
            "",
            "",
            "def multi_input(prompt=''):",
            "    \"\"\"",
            "    Get input from user",
            "    Allow multiple lines using a continuation character",
            "    \"\"\"",
            "    global _LINE_BUFFER",
            "    line = []",
            "    _LINE_BUFFER = ''",
            "    while True:",
            "        try:",
            "            text = input(prompt)",
            "        except EOFError:",
            "            return None",
            "        err_buf.incr_lineno()",
            "        if options.regression_tests:",
            "            print(\".INP:\", text)",
            "            sys.stdout.flush()",
            "            sys.stderr.flush()",
            "        stripped = text.strip()",
            "        if stripped.endswith('\\\\'):",
            "            stripped = stripped.rstrip('\\\\')",
            "            line.append(stripped)",
            "            _LINE_BUFFER += stripped",
            "            if prompt:",
            "                prompt = '   > '",
            "        else:",
            "            line.append(stripped)",
            "            break",
            "    return ''.join(line)",
            "",
            "",
            "def verify_boolean(opt):",
            "    return opt.lower() in (\"yes\", \"true\", \"on\", \"1\") or \\",
            "        opt.lower() in (\"no\", \"false\", \"off\", \"0\")",
            "",
            "",
            "def is_boolean_true(opt):",
            "    if opt in (None, False):",
            "        return False",
            "    if opt is True:",
            "        return True",
            "    return opt.lower() in (\"yes\", \"true\", \"on\", \"1\")",
            "",
            "",
            "def is_boolean_false(opt):",
            "    if opt in (None, False):",
            "        return True",
            "    if opt is True:",
            "        return False",
            "    return opt.lower() in (\"no\", \"false\", \"off\", \"0\")",
            "",
            "",
            "def get_boolean(opt, dflt=False):",
            "    if not opt:",
            "        return dflt",
            "    return is_boolean_true(opt)",
            "",
            "",
            "def canonical_boolean(opt):",
            "    return 'true' if is_boolean_true(opt) else 'false'",
            "",
            "",
            "def keyword_cmp(string1, string2):",
            "    return string1.lower() == string2.lower()",
            "",
            "",
            "class olist(list):",
            "    \"\"\"",
            "    Implements the 'in' operator",
            "    in a case-insensitive manner,",
            "    allowing \"if x in olist(...)\"",
            "    \"\"\"",
            "    def __init__(self, keys):",
            "        super(olist, self).__init__([k.lower() for k in keys])",
            "",
            "    def __contains__(self, key):",
            "        return super(olist, self).__contains__(key.lower())",
            "",
            "    def append(self, key):",
            "        super(olist, self).append(key.lower())",
            "",
            "",
            "def os_types_list(path):",
            "    l = []",
            "    for f in glob.glob(path):",
            "        if os.access(f, os.X_OK) and os.path.isfile(f):",
            "            a = f.split(\"/\")",
            "            l.append(a[-1])",
            "    return l",
            "",
            "",
            "def listtemplates():",
            "    l = []",
            "    templates_dir = os.path.join(config.path.sharedir, 'templates')",
            "    for f in os.listdir(templates_dir):",
            "        if os.path.isfile(\"%s/%s\" % (templates_dir, f)):",
            "            l.append(f)",
            "    return l",
            "",
            "",
            "def listconfigs():",
            "    l = []",
            "    for f in os.listdir(userdir.CRMCONF_DIR):",
            "        if os.path.isfile(\"%s/%s\" % (userdir.CRMCONF_DIR, f)):",
            "            l.append(f)",
            "    return l",
            "",
            "",
            "def add_sudo(cmd):",
            "    if config.core.user:",
            "        return \"sudo -E -u %s %s\" % (config.core.user, cmd)",
            "    return cmd",
            "",
            "",
            "def chown(path, user, group):",
            "    if isinstance(user, int):",
            "        uid = user",
            "    else:",
            "        import pwd",
            "        uid = pwd.getpwnam(user).pw_uid",
            "    if isinstance(group, int):",
            "        gid = group",
            "    else:",
            "        import grp",
            "        gid = grp.getgrnam(group).gr_gid",
            "    os.chown(path, uid, gid)",
            "",
            "",
            "def ensure_sudo_readable(f):",
            "    # make sure the tempfile is readable to crm_diff (bsc#999683)",
            "    if config.core.user:",
            "        from pwd import getpwnam",
            "        uid = getpwnam(config.core.user).pw_uid",
            "        try:",
            "            os.chown(f, uid, -1)",
            "        except os.error as err:",
            "            common_err('Failed setting temporary file permissions: %s' % (err))",
            "            return False",
            "    return True",
            "",
            "",
            "def pipe_string(cmd, s):",
            "    rc = -1  # command failed",
            "    cmd = add_sudo(cmd)",
            "    common_debug(\"piping string to %s\" % cmd)",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE)",
            "    try:",
            "        # communicate() expects encoded bytes",
            "        if isinstance(s, str):",
            "            s = s.encode('utf-8')",
            "        p.communicate(s)",
            "        p.wait()",
            "        rc = p.returncode",
            "    except IOError as msg:",
            "        if \"Broken pipe\" not in str(msg):",
            "            common_err(msg)",
            "    return rc",
            "",
            "",
            "def filter_string(cmd, s, stderr_on=True, shell=True):",
            "    rc = -1  # command failed",
            "    outp = ''",
            "    if stderr_on is True:",
            "        stderr = None",
            "    else:",
            "        stderr = subprocess.PIPE",
            "    cmd = add_sudo(cmd)",
            "    common_debug(\"pipe through %s\" % cmd)",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    p = subprocess.Popen(cmd,",
            "                         shell=shell,",
            "                         stdin=subprocess.PIPE,",
            "                         stdout=subprocess.PIPE,",
            "                         stderr=stderr)",
            "    try:",
            "        # bytes expected here",
            "        if isinstance(s, str):",
            "            s = s.encode('utf-8')",
            "        ret = p.communicate(s)",
            "        if stderr_on == 'stdout':",
            "            outp = b\"\\n\".join(ret)",
            "        else:",
            "            outp = ret[0]",
            "        p.wait()",
            "        rc = p.returncode",
            "    except OSError as err:",
            "        if err.errno != os.errno.EPIPE:",
            "            common_err(err.strerror)",
            "        common_info(\"from: %s\" % cmd)",
            "    except Exception as msg:",
            "        common_err(msg)",
            "        common_info(\"from: %s\" % cmd)",
            "    return rc, to_ascii(outp)",
            "",
            "",
            "def str2tmp(_str, suffix=\".pcmk\"):",
            "    '''",
            "    Write the given string to a temporary file. Return the name",
            "    of the file.",
            "    '''",
            "    s = to_ascii(_str)",
            "    fd, tmp = mkstemp(suffix=suffix)",
            "    try:",
            "        f = os.fdopen(fd, \"w\")",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return",
            "    f.write(s)",
            "    if not s.endswith('\\n'):",
            "        f.write(\"\\n\")",
            "    f.close()",
            "    return tmp",
            "",
            "",
            "@contextmanager",
            "def create_tempfile(suffix='', dir=None):",
            "    \"\"\" Context for temporary file.",
            "",
            "    Will find a free temporary filename upon entering",
            "    and will try to delete the file on leaving, even in case of an exception.",
            "",
            "    Parameters",
            "    ----------",
            "    suffix : string",
            "        optional file suffix",
            "    dir : string",
            "        optional directory to save temporary file in",
            "",
            "    (from http://stackoverflow.com/a/29491523)",
            "    \"\"\"",
            "    import tempfile",
            "    tf = tempfile.NamedTemporaryFile(delete=False, suffix=suffix, dir=dir)",
            "    tf.file.close()",
            "    try:",
            "        yield tf.name",
            "    finally:",
            "        try:",
            "            os.remove(tf.name)",
            "        except OSError as e:",
            "            if e.errno == 2:",
            "                pass",
            "            else:",
            "                raise",
            "",
            "",
            "@contextmanager",
            "def open_atomic(filepath, mode=\"r\", buffering=-1, fsync=False, encoding=None):",
            "    \"\"\" Open temporary file object that atomically moves to destination upon",
            "    exiting.",
            "",
            "    Allows reading and writing to and from the same filename.",
            "",
            "    The file will not be moved to destination in case of an exception.",
            "",
            "    Parameters",
            "    ----------",
            "    filepath : string",
            "        the file path to be opened",
            "    fsync : bool",
            "        whether to force write the file to disk",
            "",
            "    (from http://stackoverflow.com/a/29491523)",
            "    \"\"\"",
            "",
            "    with create_tempfile(dir=os.path.dirname(os.path.abspath(filepath))) as tmppath:",
            "        with open(tmppath, mode, buffering, encoding=encoding) as file:",
            "            try:",
            "                yield file",
            "            finally:",
            "                if fsync:",
            "                    file.flush()",
            "                    os.fsync(file.fileno())",
            "        os.rename(tmppath, filepath)",
            "",
            "",
            "def str2file(s, fname):",
            "    '''",
            "    Write a string to a file.",
            "    '''",
            "    try:",
            "        with open_atomic(fname, 'w', encoding='utf-8') as dst:",
            "            dst.write(to_ascii(s))",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return False",
            "    return True",
            "",
            "",
            "def file2str(fname, noerr=True):",
            "    '''",
            "    Read a one line file into a string, strip whitespace around.",
            "    '''",
            "    try:",
            "        f = open(fname, \"r\")",
            "    except IOError as msg:",
            "        if not noerr:",
            "            common_err(msg)",
            "        return None",
            "    s = f.readline()",
            "    f.close()",
            "    return s.strip()",
            "",
            "",
            "def file2list(fname):",
            "    '''",
            "    Read a file into a list (newlines dropped).",
            "    '''",
            "    try:",
            "        return open(fname).read().split('\\n')",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return None",
            "",
            "",
            "def safe_open_w(fname):",
            "    if fname == \"-\":",
            "        f = sys.stdout",
            "    else:",
            "        if not options.batch and os.access(fname, os.F_OK):",
            "            if not ask(\"File %s exists. Do you want to overwrite it?\" % fname):",
            "                return None",
            "        try:",
            "            f = open(fname, \"w\")",
            "        except IOError as msg:",
            "            common_err(msg)",
            "            return None",
            "    return f",
            "",
            "",
            "def safe_close_w(f):",
            "    if f and f != sys.stdout:",
            "        f.close()",
            "",
            "",
            "def is_path_sane(name):",
            "    if re.search(r\"['`#*?$\\[\\]]\", name):",
            "        common_err(\"%s: bad path\" % name)",
            "        return False",
            "    return True",
            "",
            "",
            "def is_filename_sane(name):",
            "    if re.search(r\"['`/#*?$\\[\\]]\", name):",
            "        common_err(\"%s: bad filename\" % name)",
            "        return False",
            "    return True",
            "",
            "",
            "def is_name_sane(name):",
            "    if re.search(\"[']\", name):",
            "        common_err(\"%s: bad name\" % name)",
            "        return False",
            "    return True",
            "",
            "",
            "def show_dot_graph(dotfile, keep_file=False, desc=\"transition graph\"):",
            "    cmd = \"%s %s\" % (config.core.dotty, dotfile)",
            "    if not keep_file:",
            "        cmd = \"(%s; rm -f %s)\" % (cmd, dotfile)",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    subprocess.Popen(cmd, shell=True, bufsize=0,",
            "                     stdin=None, stdout=None, stderr=None, close_fds=True)",
            "    common_info(\"starting %s to show %s\" % (config.core.dotty, desc))",
            "",
            "",
            "def ext_cmd(cmd, shell=True):",
            "    cmd = add_sudo(cmd)",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    common_debug(\"invoke: %s\" % cmd)",
            "    return subprocess.call(cmd, shell=shell)",
            "",
            "",
            "def ext_cmd_nosudo(cmd, shell=True):",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    return subprocess.call(cmd, shell=shell)",
            "",
            "",
            "def rmdir_r(d):",
            "    # TODO: Make sure we're not deleting something we shouldn't!",
            "    if d and os.path.isdir(d):",
            "        shutil.rmtree(d)",
            "",
            "",
            "def nvpairs2dict(pairs):",
            "    '''",
            "    takes a list of string of form ['a=b', 'c=d']",
            "    and returns {'a':'b', 'c':'d'}",
            "    '''",
            "    data = []",
            "    for var in pairs:",
            "        if '=' in var:",
            "            data.append(var.split('=', 1))",
            "        else:",
            "            data.append([var, None])",
            "    return dict(data)",
            "",
            "",
            "def is_check_always():",
            "    '''",
            "    Even though the frequency may be set to always, it doesn't",
            "    make sense to do that with non-interactive sessions.",
            "    '''",
            "    return options.interactive and config.core.check_frequency == \"always\"",
            "",
            "",
            "def get_check_rc():",
            "    '''",
            "    If the check mode is set to strict, then on errors we",
            "    return 2 which is the code for error. Otherwise, we",
            "    pretend that errors are warnings.",
            "    '''",
            "    return 2 if config.core.check_mode == \"strict\" else 1",
            "",
            "",
            "_LOCKDIR = \".lockdir\"",
            "_PIDF = \"pid\"",
            "",
            "",
            "def check_locker(lockdir):",
            "    if not os.path.isdir(os.path.join(lockdir, _LOCKDIR)):",
            "        return",
            "    s = file2str(os.path.join(lockdir, _LOCKDIR, _PIDF))",
            "    pid = convert2ints(s)",
            "    if not isinstance(pid, int):",
            "        common_warn(\"history: removing malformed lock\")",
            "        rmdir_r(os.path.join(lockdir, _LOCKDIR))",
            "        return",
            "    try:",
            "        os.kill(pid, 0)",
            "    except OSError as err:",
            "        if err.errno == os.errno.ESRCH:",
            "            common_info(\"history: removing stale lock\")",
            "            rmdir_r(os.path.join(lockdir, _LOCKDIR))",
            "        else:",
            "            common_err(\"%s: %s\" % (_LOCKDIR, err.strerror))",
            "",
            "",
            "@contextmanager",
            "def lock(lockdir):",
            "    \"\"\"",
            "    Ensure that the lock is released properly",
            "    even in the face of an exception between",
            "    acquire and release.",
            "    \"\"\"",
            "    def acquire_lock():",
            "        check_locker(lockdir)",
            "        while True:",
            "            try:",
            "                os.makedirs(os.path.join(lockdir, _LOCKDIR))",
            "                str2file(\"%d\" % os.getpid(), os.path.join(lockdir, _LOCKDIR, _PIDF))",
            "                return True",
            "            except OSError as err:",
            "                if err.errno != os.errno.EEXIST:",
            "                    common_err(\"Failed to acquire lock to %s: %s\" % (lockdir, err.strerror))",
            "                    return False",
            "                time.sleep(0.1)",
            "                continue",
            "            else:",
            "                return False",
            "",
            "    has_lock = acquire_lock()",
            "    try:",
            "        yield",
            "    finally:",
            "        if has_lock:",
            "            rmdir_r(os.path.join(lockdir, _LOCKDIR))",
            "",
            "",
            "def mkdirp(d, mode=0o777):",
            "    if os.path.isdir(d):",
            "        return True",
            "    os.makedirs(d, mode=mode)",
            "",
            "",
            "def pipe_cmd_nosudo(cmd):",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    proc = subprocess.Popen(cmd,",
            "                            shell=True,",
            "                            stdout=subprocess.PIPE,",
            "                            stderr=subprocess.PIPE)",
            "    (outp, err_outp) = proc.communicate()",
            "    proc.wait()",
            "    rc = proc.returncode",
            "    if rc != 0:",
            "        print(outp)",
            "        print(err_outp)",
            "    return rc",
            "",
            "",
            "def run_cmd_on_remote(cmd, remote_addr, prompt_msg=None):",
            "    \"\"\"",
            "    Run a cmd on remote node",
            "    return (rc, stdout, err_msg)",
            "    \"\"\"",
            "    rc = 1",
            "    out_data = None",
            "    err_data = None",
            "",
            "    need_pw = check_ssh_passwd_need(remote_addr)",
            "    if need_pw and prompt_msg:",
            "        print(prompt_msg)",
            "    try:",
            "        result = parallax.parallax_call([remote_addr], cmd, need_pw)",
            "        rc, out_data, _ = result[0][1]",
            "    except ValueError as err:",
            "        err_match = re.search(\"Exited with error code ([0-9]+), Error output: (.*)\", str(err))",
            "        if err_match:",
            "            rc, err_data = err_match.groups()",
            "    finally:",
            "        return int(rc), to_ascii(out_data), err_data",
            "",
            "",
            "def get_stdout(cmd, input_s=None, stderr_on=True, shell=True, raw=False):",
            "    '''",
            "    Run a cmd, return stdout output.",
            "    Optional input string \"input_s\".",
            "    stderr_on controls whether to show output which comes on stderr.",
            "    '''",
            "    if stderr_on:",
            "        stderr = None",
            "    else:",
            "        stderr = subprocess.PIPE",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    proc = subprocess.Popen(cmd,",
            "                            shell=shell,",
            "                            stdin=subprocess.PIPE,",
            "                            stdout=subprocess.PIPE,",
            "                            stderr=stderr)",
            "    stdout_data, stderr_data = proc.communicate(input_s)",
            "    if raw:",
            "        return proc.returncode, stdout_data",
            "    return proc.returncode, to_ascii(stdout_data).strip()",
            "",
            "",
            "def get_stdout_stderr(cmd, input_s=None, shell=True, raw=False):",
            "    '''",
            "    Run a cmd, return (rc, stdout, stderr)",
            "    '''",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    proc = subprocess.Popen(cmd,",
            "                            shell=shell,",
            "                            stdin=input_s and subprocess.PIPE or None,",
            "                            stdout=subprocess.PIPE,",
            "                            stderr=subprocess.PIPE)",
            "    stdout_data, stderr_data = proc.communicate(input_s)",
            "    if raw:",
            "        return proc.returncode, stdout_data, stderr_data",
            "    return proc.returncode, to_ascii(stdout_data).strip(), to_ascii(stderr_data).strip()",
            "",
            "",
            "def stdout2list(cmd, stderr_on=True, shell=True):",
            "    '''",
            "    Run a cmd, fetch output, return it as a list of lines.",
            "    stderr_on controls whether to show output which comes on stderr.",
            "    '''",
            "    rc, s = get_stdout(add_sudo(cmd), stderr_on=stderr_on, shell=shell)",
            "    if not s:",
            "        return rc, []",
            "    return rc, s.split('\\n')",
            "",
            "",
            "def append_file(dest, src):",
            "    'Append src to dest'",
            "    try:",
            "        open(dest, \"a\").write(open(src).read())",
            "        return True",
            "    except IOError as msg:",
            "        common_err(\"append %s to %s: %s\" % (src, dest, msg))",
            "        return False",
            "",
            "",
            "def get_dc():",
            "    cmd = \"crmadmin -D\"",
            "    rc, s = get_stdout(add_sudo(cmd))",
            "    if rc != 0:",
            "        return None",
            "    if not s.startswith(\"Designated\"):",
            "        return None",
            "    return s.split()[-1]",
            "",
            "",
            "def wait4dc(what=\"\", show_progress=True):",
            "    '''",
            "    Wait for the DC to get into the S_IDLE state. This should be",
            "    invoked only after a CIB modification which would exercise",
            "    the PE. Parameter \"what\" is whatever the caller wants to be",
            "    printed if showing progress.",
            "",
            "    It is assumed that the DC is already in a different state,",
            "    usually it should be either PENGINE or TRANSITION. This",
            "    assumption may not be true, but there's a high chance that it",
            "    is since crmd should be faster to move through states than",
            "    this shell.",
            "",
            "    Further, it may also be that crmd already calculated the new",
            "    graph, did transition, and went back to the idle state. This",
            "    may in particular be the case if the transition turned out to",
            "    be empty.",
            "",
            "    Tricky. Though in practice it shouldn't be an issue.",
            "",
            "    There's no timeout, as we expect the DC to eventually becomes",
            "    idle.",
            "    '''",
            "    dc = get_dc()",
            "    if not dc:",
            "        common_warn(\"can't find DC\")",
            "        return False",
            "    cmd = \"crm_attribute -Gq -t crm_config -n crmd-transition-delay 2> /dev/null\"",
            "    delay = get_stdout(add_sudo(cmd))[1]",
            "    if delay:",
            "        delaymsec = crm_msec(delay)",
            "        if delaymsec > 0:",
            "            common_info(\"The crmd-transition-delay is configured. Waiting %d msec before check DC status.\" % delaymsec)",
            "            time.sleep(delaymsec // 1000)",
            "    cnt = 0",
            "    output_started = 0",
            "    init_sleep = 0.25",
            "    max_sleep = 1.00",
            "    sleep_time = init_sleep",
            "    while True:",
            "        dc = get_dc()",
            "        if not dc:",
            "            common_warn(\"DC lost during wait\")",
            "            return False",
            "        cmd = \"crmadmin -S %s\" % dc",
            "        rc, s = get_stdout(add_sudo(cmd))",
            "        if not s.startswith(\"Status\"):",
            "            common_warn(\"%s unexpected output: %s (exit code: %d)\" %",
            "                        (cmd, s, rc))",
            "            return False",
            "        try:",
            "            dc_status = s.split()[-2]",
            "        except:",
            "            common_warn(\"%s unexpected output: %s\" % (cmd, s))",
            "            return False",
            "        if dc_status == \"S_IDLE\":",
            "            if output_started:",
            "                sys.stderr.write(\" done\\n\")",
            "            return True",
            "        time.sleep(sleep_time)",
            "        if sleep_time < max_sleep:",
            "            sleep_time *= 2",
            "        if show_progress:",
            "            if not output_started:",
            "                output_started = 1",
            "                sys.stderr.write(\"waiting for %s to finish .\" % what)",
            "            cnt += 1",
            "            if cnt % 5 == 0:",
            "                sys.stderr.write(\".\")",
            "",
            "",
            "def run_ptest(graph_s, nograph, scores, utilization, actions, verbosity):",
            "    '''",
            "    Pipe graph_s thru ptest(8). Show graph using dotty if requested.",
            "    '''",
            "    actions_filter = \"grep LogActions: | grep -vw Leave\"",
            "    ptest = \"2>&1 %s -x -\" % config.core.ptest",
            "    if re.search(\"simulate\", ptest) and \\",
            "            not re.search(\"-[RS]\", ptest):",
            "        ptest = \"%s -S\" % ptest",
            "    if verbosity:",
            "        if actions:",
            "            verbosity = 'v' * max(3, len(verbosity))",
            "        ptest = \"%s -%s\" % (ptest, verbosity.upper())",
            "    if scores:",
            "        ptest = \"%s -s\" % ptest",
            "    if utilization:",
            "        ptest = \"%s -U\" % ptest",
            "    if config.core.dotty and not nograph:",
            "        fd, dotfile = mkstemp()",
            "        ptest = \"%s -D %s\" % (ptest, dotfile)",
            "    else:",
            "        dotfile = None",
            "    # ptest prints to stderr",
            "    if actions:",
            "        ptest = \"%s | %s\" % (ptest, actions_filter)",
            "    if options.regression_tests:",
            "        ptest = \">/dev/null %s\" % ptest",
            "    common_debug(\"invoke: %s\" % ptest)",
            "    rc, s = get_stdout(ptest, input_s=graph_s)",
            "    if rc != 0:",
            "        common_debug(\"'%s' exited with (rc=%d)\" % (ptest, rc))",
            "        if actions and rc == 1:",
            "            common_warn(\"No actions found.\")",
            "        else:",
            "            common_warn(\"Simulation was unsuccessful (RC=%d).\" % (rc))",
            "    if dotfile:",
            "        if os.path.getsize(dotfile) > 0:",
            "            show_dot_graph(dotfile)",
            "        else:",
            "            common_warn(\"ptest produced empty dot file\")",
            "    else:",
            "        if not nograph:",
            "            common_info(\"install graphviz to see a transition graph\")",
            "    if s:",
            "        page_string(s)",
            "    return True",
            "",
            "",
            "def is_id_valid(ident):",
            "    \"\"\"",
            "    Verify that the id follows the definition:",
            "    http://www.w3.org/TR/1999/REC-xml-names-19990114/#ns-qualnames",
            "    \"\"\"",
            "    if not ident:",
            "        return False",
            "    id_re = r\"^[A-Za-z_][\\w._-]*$\"",
            "    return re.match(id_re, ident)",
            "",
            "",
            "def check_range(a):",
            "    \"\"\"",
            "    Verify that the integer range in list a is valid.",
            "    \"\"\"",
            "    if len(a) != 2:",
            "        return False",
            "    if not isinstance(a[0], int) or not isinstance(a[1], int):",
            "        return False",
            "    return int(a[0]) <= int(a[1])",
            "",
            "",
            "def crm_msec(t):",
            "    '''",
            "    See lib/common/utils.c:crm_get_msec().",
            "    '''",
            "    convtab = {",
            "        'ms': (1, 1),",
            "        'msec': (1, 1),",
            "        'us': (1, 1000),",
            "        'usec': (1, 1000),",
            "        '': (1000, 1),",
            "        's': (1000, 1),",
            "        'sec': (1000, 1),",
            "        'm': (60*1000, 1),",
            "        'min': (60*1000, 1),",
            "        'h': (60*60*1000, 1),",
            "        'hr': (60*60*1000, 1),",
            "    }",
            "    if not t:",
            "        return -1",
            "    r = re.match(r\"\\s*(\\d+)\\s*([a-zA-Z]+)?\", t)",
            "    if not r:",
            "        return -1",
            "    if not r.group(2):",
            "        q = ''",
            "    else:",
            "        q = r.group(2).lower()",
            "    try:",
            "        mult, div = convtab[q]",
            "    except KeyError:",
            "        return -1",
            "    return (int(r.group(1))*mult) // div",
            "",
            "",
            "def crm_time_cmp(a, b):",
            "    return crm_msec(a) - crm_msec(b)",
            "",
            "",
            "def shorttime(ts):",
            "    if isinstance(ts, datetime.datetime):",
            "        return ts.strftime(\"%X\")",
            "    if ts is not None:",
            "        return time.strftime(\"%X\", time.localtime(ts))",
            "    return time.strftime(\"%X\", time.localtime(0))",
            "",
            "",
            "def shortdate(ts):",
            "    if isinstance(ts, datetime.datetime):",
            "        return ts.strftime(\"%F\")",
            "    if ts is not None:",
            "        return time.strftime(\"%F\", time.localtime(ts))",
            "    return time.strftime(\"%F\", time.localtime(0))",
            "",
            "",
            "def sort_by_mtime(l):",
            "    'Sort a (small) list of files by time mod.'",
            "    l2 = [(os.stat(x).st_mtime, x) for x in l]",
            "    l2.sort()",
            "    return [x[1] for x in l2]",
            "",
            "",
            "def file_find_by_name(root, filename):",
            "    'Find a file within a tree matching fname'",
            "    assert root",
            "    assert filename",
            "    for root, dirnames, filenames in os.walk(root):",
            "        for filename in fnmatch.filter(filenames, filename):",
            "            return os.path.join(root, filename)",
            "    return None",
            "",
            "",
            "def convert2ints(l):",
            "    \"\"\"",
            "    Convert a list of strings (or a string) to a list of ints.",
            "    All strings must be ints, otherwise conversion fails and None",
            "    is returned!",
            "    \"\"\"",
            "    try:",
            "        if isinstance(l, (tuple, list)):",
            "            return [int(x) for x in l]",
            "        # it's a string then",
            "        return int(l)",
            "    except ValueError:",
            "        return None",
            "",
            "",
            "def is_int(s):",
            "    'Check if the string can be converted to an integer.'",
            "    try:",
            "        int(s)",
            "        return True",
            "    except ValueError:",
            "        return False",
            "",
            "",
            "def is_process(s):",
            "    \"\"\"",
            "    Returns true if argument is the name of a running process.",
            "",
            "    s: process name",
            "    returns Boolean",
            "    \"\"\"",
            "    from os.path import join, basename",
            "    # find pids of running processes",
            "    pids = [pid for pid in os.listdir('/proc') if pid.isdigit()]",
            "    for pid in pids:",
            "        try:",
            "            cmdline = open(join('/proc', pid, 'cmdline'), 'rb').read()",
            "            procname = basename(to_ascii(cmdline).replace('\\x00', ' ').split(' ')[0])",
            "            if procname == s:",
            "                return True",
            "        except EnvironmentError:",
            "            # a process may have died since we got the list of pids",
            "            pass",
            "    return False",
            "",
            "",
            "def print_stacktrace():",
            "    \"\"\"",
            "    Print the stack at the site of call",
            "    \"\"\"",
            "    import traceback",
            "    import inspect",
            "    sf = inspect.currentframe().f_back.f_back",
            "    traceback.print_stack(sf)",
            "",
            "",
            "@memoize",
            "def cluster_stack():",
            "    if is_process(\"heartbeat:.[m]aster\"):",
            "        return \"heartbeat\"",
            "    elif is_process(\"[a]isexec\"):",
            "        return \"openais\"",
            "    elif os.path.exists(\"/etc/corosync/corosync.conf\") or is_program('corosync-cfgtool'):",
            "        return \"corosync\"",
            "    return \"\"",
            "",
            "",
            "def edit_file(fname):",
            "    'Edit a file.'",
            "    if not fname:",
            "        return",
            "    if not config.core.editor:",
            "        return",
            "    return ext_cmd_nosudo(\"%s %s\" % (config.core.editor, fname))",
            "",
            "",
            "def edit_file_ext(fname, template=''):",
            "    '''",
            "    Edit a file via a temporary file.",
            "    Raises IOError on any error.",
            "    '''",
            "    if not os.path.isfile(fname):",
            "        s = template",
            "    else:",
            "        s = open(fname).read()",
            "    filehash = hash(s)",
            "    tmpfile = str2tmp(s)",
            "    try:",
            "        try:",
            "            if edit_file(tmpfile) != 0:",
            "                return",
            "            s = open(tmpfile, 'r').read()",
            "            if hash(s) == filehash:  # file unchanged",
            "                return",
            "            f2 = open(fname, 'w')",
            "            f2.write(s)",
            "            f2.close()",
            "        finally:",
            "            os.unlink(tmpfile)",
            "    except OSError as e:",
            "        raise IOError(e)",
            "",
            "",
            "def need_pager(s, w, h):",
            "    from math import ceil",
            "    cnt = 0",
            "    for l in s.split('\\n'):",
            "        # need to remove color codes",
            "        l = re.sub(r'\\${\\w+}', '', l)",
            "        cnt += int(ceil((len(l) + 0.5) / w))",
            "        if cnt >= h:",
            "            return True",
            "    return False",
            "",
            "",
            "def term_render(s):",
            "    'Render for TERM.'",
            "    try:",
            "        return term.render(s)",
            "    except:",
            "        return s",
            "",
            "",
            "def get_pager_cmd(*extra_opts):",
            "    'returns a commandline which calls the configured pager'",
            "    cmdline = [config.core.pager]",
            "    if os.path.basename(config.core.pager) == \"less\":",
            "        cmdline.append('-R')",
            "    cmdline.extend(extra_opts)",
            "    return ' '.join(cmdline)",
            "",
            "",
            "def page_string(s):",
            "    'Page string rendered for TERM.'",
            "    if not s:",
            "        return",
            "    constants.need_reset = True",
            "    w, h = get_winsize()",
            "    if not need_pager(s, w, h):",
            "        print(term_render(s))",
            "    elif not config.core.pager or not can_ask() or options.batch:",
            "        print(term_render(s))",
            "    else:",
            "        pipe_string(get_pager_cmd(), term_render(s).encode('utf-8'))",
            "    constants.need_reset = False",
            "",
            "",
            "def page_gen(g):",
            "    'Page lines generated by generator g'",
            "    w, h = get_winsize()",
            "    if not config.core.pager or not can_ask() or options.batch:",
            "        for line in g:",
            "            sys.stdout.write(term_render(line))",
            "    else:",
            "        pipe_string(get_pager_cmd(), term_render(\"\".join(g)))",
            "",
            "",
            "def page_file(filename):",
            "    'Open file in pager'",
            "    if not os.path.isfile(filename):",
            "        return",
            "    return ext_cmd_nosudo(get_pager_cmd(filename), shell=True)",
            "",
            "",
            "def get_winsize():",
            "    try:",
            "        import curses",
            "        curses.setupterm()",
            "        w = curses.tigetnum('cols')",
            "        h = curses.tigetnum('lines')",
            "    except:",
            "        try:",
            "            w = os.environ['COLS']",
            "            h = os.environ['LINES']",
            "        except KeyError:",
            "            w = 80",
            "            h = 25",
            "    return w, h",
            "",
            "",
            "def multicolumn(l):",
            "    '''",
            "    A ls-like representation of a list of strings.",
            "    A naive approach.",
            "    '''",
            "    min_gap = 2",
            "    w, _ = get_winsize()",
            "    max_len = 8",
            "    for s in l:",
            "        if len(s) > max_len:",
            "            max_len = len(s)",
            "    cols = w // (max_len + min_gap)  # approx.",
            "    if not cols:",
            "        cols = 1",
            "    col_len = w // cols",
            "    for i in range(len(l) // cols + 1):",
            "        s = ''",
            "        for j in range(i * cols, (i + 1) * cols):",
            "            if not j < len(l):",
            "                break",
            "            if not s:",
            "                s = \"%-*s\" % (col_len, l[j])",
            "            elif (j + 1) % cols == 0:",
            "                s = \"%s%s\" % (s, l[j])",
            "            else:",
            "                s = \"%s%-*s\" % (s, col_len, l[j])",
            "        if s:",
            "            print(s)",
            "",
            "",
            "def find_value(pl, name):",
            "    for n, v in pl:",
            "        if n == name:",
            "            return v",
            "    return None",
            "",
            "",
            "def cli_replace_attr(pl, name, new_val):",
            "    for i, attr in enumerate(pl):",
            "        if attr[0] == name:",
            "            attr[1] = new_val",
            "            return",
            "",
            "",
            "def cli_append_attr(pl, name, val):",
            "    pl.append([name, val])",
            "",
            "",
            "def lines2cli(s):",
            "    '''",
            "    Convert a string into a list of lines. Replace continuation",
            "    characters. Strip white space, left and right. Drop empty lines.",
            "    '''",
            "    cl = []",
            "    l = s.split('\\n')",
            "    cum = []",
            "    for p in l:",
            "        p = p.strip()",
            "        if p.endswith('\\\\'):",
            "            p = p.rstrip('\\\\')",
            "            cum.append(p)",
            "        else:",
            "            cum.append(p)",
            "            cl.append(''.join(cum).strip())",
            "            cum = []",
            "    if cum:  # in case s ends with backslash",
            "        cl.append(''.join(cum))",
            "    return [x for x in cl if x]",
            "",
            "",
            "def datetime_is_aware(dt):",
            "    \"\"\"",
            "    Determines if a given datetime.datetime is aware.",
            "",
            "    The logic is described in Python's docs:",
            "    http://docs.python.org/library/datetime.html#datetime.tzinfo",
            "    \"\"\"",
            "    return dt and dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None",
            "",
            "",
            "def make_datetime_naive(dt):",
            "    \"\"\"",
            "    Ensures that the datetime is not time zone-aware:",
            "",
            "    The returned datetime object is a naive time in UTC.",
            "    \"\"\"",
            "    if dt and datetime_is_aware(dt):",
            "        return dt.replace(tzinfo=None) - dt.utcoffset()",
            "    return dt",
            "",
            "",
            "def total_seconds(td):",
            "    \"\"\"",
            "    Backwards compatible implementation of timedelta.total_seconds()",
            "    \"\"\"",
            "    if hasattr(datetime.timedelta, 'total_seconds'):",
            "        return td.total_seconds()",
            "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) // 10**6",
            "",
            "",
            "def datetime_to_timestamp(dt):",
            "    \"\"\"",
            "    Convert a datetime object into a floating-point second value",
            "    \"\"\"",
            "    try:",
            "        return total_seconds(make_datetime_naive(dt) - datetime.datetime(1970, 1, 1))",
            "    except Exception as e:",
            "        common_err(\"datetime_to_timestamp error: %s\" % (e))",
            "        return None",
            "",
            "",
            "def timestamp_to_datetime(ts):",
            "    \"\"\"",
            "    Convert a timestamp into a naive datetime object",
            "    \"\"\"",
            "    import dateutil",
            "    import dateutil.tz",
            "    return make_datetime_naive(datetime.datetime.fromtimestamp(ts).replace(tzinfo=dateutil.tz.tzlocal()))",
            "",
            "",
            "def parse_time(t):",
            "    '''",
            "    Try to make sense of the user provided time spec.",
            "    Use dateutil if available, otherwise strptime.",
            "    Return the datetime value.",
            "",
            "    Also does time zone elimination by passing the datetime",
            "    through a timestamp conversion if necessary",
            "",
            "    TODO: dateutil is very slow, avoid it if possible",
            "    '''",
            "    try:",
            "        from dateutil import parser, tz",
            "        dt = parser.parse(t)",
            "",
            "        if datetime_is_aware(dt):",
            "            ts = datetime_to_timestamp(dt)",
            "            if ts is None:",
            "                return None",
            "            dt = datetime.datetime.fromtimestamp(ts)",
            "        else:",
            "            # convert to UTC from local time",
            "            dt = dt - tz.tzlocal().utcoffset(dt)",
            "    except ValueError as msg:",
            "        common_err(\"parse_time %s: %s\" % (t, msg))",
            "        return None",
            "    except ImportError as msg:",
            "        try:",
            "            tm = time.strptime(t)",
            "            dt = datetime.datetime(*tm[0:7])",
            "        except ValueError as msg:",
            "            common_err(\"no dateutil, please provide times as printed by date(1)\")",
            "            return None",
            "    return dt",
            "",
            "",
            "def parse_to_timestamp(t):",
            "    '''",
            "    Read a string and convert it into a UNIX timestamp.",
            "    Added as an optimization of parse_time to avoid",
            "    extra conversion steps when result would be converted",
            "    into a timestamp anyway",
            "    '''",
            "    try:",
            "        from dateutil import parser, tz",
            "        dt = parser.parse(t)",
            "",
            "        if datetime_is_aware(dt):",
            "            return datetime_to_timestamp(dt)",
            "        # convert to UTC from local time",
            "        return total_seconds(dt - tz.tzlocal().utcoffset(dt) - datetime.datetime(1970, 1, 1))",
            "    except ValueError as msg:",
            "        common_err(\"parse_time %s: %s\" % (t, msg))",
            "        return None",
            "    except ImportError as msg:",
            "        try:",
            "            tm = time.strptime(t)",
            "            dt = datetime.datetime(*tm[0:7])",
            "            return datetime_to_timestamp(dt)",
            "        except ValueError as msg:",
            "            common_err(\"no dateutil, please provide times as printed by date(1)\")",
            "            return None",
            "",
            "",
            "def save_graphviz_file(ini_f, attr_d):",
            "    '''",
            "    Save graphviz settings to an ini file, if it does not exist.",
            "    '''",
            "    if os.path.isfile(ini_f):",
            "        common_err(\"%s exists, please remove it first\" % ini_f)",
            "        return False",
            "    try:",
            "        f = open(ini_f, \"wb\")",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return False",
            "    import configparser",
            "    p = configparser.ConfigParser()",
            "    for section, sect_d in attr_d.items():",
            "        p.add_section(section)",
            "        for n, v in sect_d.items():",
            "            p.set(section, n, v)",
            "    try:",
            "        p.write(f)",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return False",
            "    f.close()",
            "    common_info(\"graphviz attributes saved to %s\" % ini_f)",
            "    return True",
            "",
            "",
            "def load_graphviz_file(ini_f):",
            "    '''",
            "    Load graphviz ini file, if it exists.",
            "    '''",
            "    if not os.path.isfile(ini_f):",
            "        return True, None",
            "    import configparser",
            "    p = configparser.ConfigParser()",
            "    try:",
            "        p.read(ini_f)",
            "    except Exception as msg:",
            "        common_err(msg)",
            "        return False, None",
            "    _graph_d = {}",
            "    for section in p.sections():",
            "        d = {}",
            "        for n, v in p.items(section):",
            "            d[n] = v",
            "        _graph_d[section] = d",
            "    return True, _graph_d",
            "",
            "",
            "def get_pcmk_version(dflt):",
            "    version = dflt",
            "",
            "    crmd = pacemaker_controld()",
            "    if crmd:",
            "        cmd = crmd",
            "    else:",
            "        return version",
            "",
            "    try:",
            "        rc, s, err = get_stdout_stderr(\"%s version\" % (cmd))",
            "        if rc != 0:",
            "            common_err(\"%s exited with %d [err: %s][out: %s]\" % (cmd, rc, err, s))",
            "        else:",
            "            common_debug(\"pacemaker version: [err: %s][out: %s]\" % (err, s))",
            "            if err.startswith(\"CRM Version:\"):",
            "                version = s.split()[0]",
            "            else:",
            "                version = s.split()[2]",
            "            common_debug(\"found pacemaker version: %s\" % version)",
            "    except Exception as msg:",
            "        common_warn(\"could not get the pacemaker version, bad installation?\")",
            "        common_warn(msg)",
            "    return version",
            "",
            "",
            "def get_cib_property(cib_f, attr, dflt):",
            "    \"\"\"A poor man's get attribute procedure.",
            "    We don't want heavy parsing, this needs to be relatively",
            "    fast.",
            "    \"\"\"",
            "    open_t = \"<cluster_property_set\"",
            "    close_t = \"</cluster_property_set\"",
            "    attr_s = 'name=\"%s\"' % attr",
            "    ver_patt = re.compile('value=\"([^\"]+)\"')",
            "    ver = dflt  # return some version in any case",
            "    try:",
            "        f = open(cib_f, \"r\")",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return ver",
            "    state = 0",
            "    for s in f:",
            "        if state == 0:",
            "            if open_t in s:",
            "                state += 1",
            "        elif state == 1:",
            "            if close_t in s:",
            "                break",
            "            if attr_s in s:",
            "                r = ver_patt.search(s)",
            "                if r:",
            "                    ver = r.group(1)",
            "                break",
            "    f.close()",
            "    return ver",
            "",
            "",
            "def get_cib_attributes(cib_f, tag, attr_l, dflt_l):",
            "    \"\"\"A poor man's get attribute procedure.",
            "    We don't want heavy parsing, this needs to be relatively",
            "    fast.",
            "    \"\"\"",
            "    open_t = \"<%s \" % tag",
            "    val_patt_l = [re.compile('%s=\"([^\"]+)\"' % x) for x in attr_l]",
            "    val_l = []",
            "    try:",
            "        f = open(cib_f, \"rb\").read()",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return dflt_l",
            "    if os.path.splitext(cib_f)[-1] == '.bz2':",
            "        cib_bits = bz2.decompress(f)",
            "    else:",
            "        cib_bits = f",
            "    cib_s = to_ascii(cib_bits)",
            "    for s in cib_s.split('\\n'):",
            "        if s.startswith(open_t):",
            "            i = 0",
            "            for patt in val_patt_l:",
            "                r = patt.search(s)",
            "                val_l.append(r and r.group(1) or dflt_l[i])",
            "                i += 1",
            "            break",
            "    return val_l",
            "",
            "",
            "def is_min_pcmk_ver(min_ver, cib_f=None):",
            "    if not constants.pcmk_version:",
            "        if cib_f:",
            "            constants.pcmk_version = get_cib_property(cib_f, \"dc-version\", \"1.1.11\")",
            "            common_debug(\"found pacemaker version: %s in cib: %s\" %",
            "                         (constants.pcmk_version, cib_f))",
            "        else:",
            "            constants.pcmk_version = get_pcmk_version(\"1.1.11\")",
            "    from distutils.version import LooseVersion",
            "    return LooseVersion(constants.pcmk_version) >= LooseVersion(min_ver)",
            "",
            "",
            "def is_pcmk_118(cib_f=None):",
            "    return is_min_pcmk_ver(\"1.1.8\", cib_f=cib_f)",
            "",
            "",
            "@memoize",
            "def cibadmin_features():",
            "    '''",
            "    # usage example:",
            "    if 'corosync-plugin' in cibadmin_features()",
            "    '''",
            "    rc, outp = get_stdout(['cibadmin', '-!'], shell=False)",
            "    if rc == 0:",
            "        m = re.match(r'Pacemaker\\s(\\S+)\\s\\(Build: ([^\\)]+)\\):\\s(.*)', outp.strip())",
            "        if m and len(m.groups()) > 2:",
            "            return m.group(3).split()",
            "    return []",
            "",
            "",
            "@memoize",
            "def cibadmin_can_patch():",
            "    # cibadmin -P doesn't handle comments in <1.1.11 (unless patched)",
            "    return is_min_pcmk_ver(\"1.1.11\")",
            "",
            "",
            "# quote function from python module shlex.py in python 3.3",
            "",
            "_find_unsafe = re.compile(r'[^\\w@%+=:,./-]').search",
            "",
            "",
            "def quote(s):",
            "    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"",
            "    if not s:",
            "        return \"''\"",
            "    if _find_unsafe(s) is None:",
            "        return s",
            "",
            "    # use single quotes, and put single quotes into double quotes",
            "    # the string $'b is then quoted as '$'\"'\"'b'",
            "    return \"'\" + s.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"",
            "",
            "",
            "def doublequote(s):",
            "    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"",
            "    if not s:",
            "        return '\"\"'",
            "    if _find_unsafe(s) is None:",
            "        return s",
            "",
            "    # use double quotes",
            "    return '\"' + s.replace('\"', \"\\\\\\\"\") + '\"'",
            "",
            "",
            "def fetch_opts(args, opt_l):",
            "    '''",
            "    Get and remove option keywords from args.",
            "    They are always listed last, at the end of the line.",
            "    Return a list of options found. The caller can do",
            "    if keyw in optlist: ...",
            "    '''",
            "    re_opt = None",
            "    if opt_l[0].startswith(\"@\"):",
            "        re_opt = re.compile(\"^%s$\" % opt_l[0][1:])",
            "        del opt_l[0]",
            "    l = []",
            "    for i in reversed(list(range(len(args)))):",
            "        if (args[i] in opt_l) or (re_opt and re_opt.search(args[i])):",
            "            l.append(args.pop())",
            "        else:",
            "            break",
            "    return l",
            "",
            "",
            "_LIFETIME = [\"reboot\", \"forever\"]",
            "_ISO8601_RE = re.compile(\"(PT?[0-9]|[0-9]+.*[:-])\")",
            "",
            "",
            "def fetch_lifetime_opt(args, iso8601=True):",
            "    '''",
            "    Get and remove a lifetime option from args. It can be one of",
            "    lifetime_options or an ISO 8601 formatted period/time. There",
            "    is apparently no good support in python for this format, so",
            "    we cheat a bit.",
            "    '''",
            "    if args:",
            "        opt = args[-1]",
            "        if opt in _LIFETIME or (iso8601 and _ISO8601_RE.match(opt)):",
            "            return args.pop()",
            "    return None",
            "",
            "",
            "def resolve_hostnames(hostnames):",
            "    '''",
            "    Tries to resolve the given list of hostnames.",
            "    returns (ok, failed-hostname)",
            "    ok: True if all hostnames resolved",
            "    failed-hostname: First failed hostname resolution",
            "    '''",
            "    import socket",
            "    for node in hostnames:",
            "        try:",
            "            socket.gethostbyname(node)",
            "        except socket.error:",
            "            return False, node",
            "    return True, None",
            "",
            "",
            "def list_corosync_node_names():",
            "    '''",
            "    Returns list of nodes configured",
            "    in corosync.conf",
            "    '''",
            "    try:",
            "        cfg = os.getenv('COROSYNC_MAIN_CONFIG_FILE', '/etc/corosync/corosync.conf')",
            "        lines = open(cfg).read().split('\\n')",
            "        name_re = re.compile(r'\\s*name:\\s+(.*)')",
            "        names = []",
            "        for line in lines:",
            "            name = name_re.match(line)",
            "            if name:",
            "                names.append(name.group(1))",
            "        return names",
            "    except Exception:",
            "        return []",
            "",
            "",
            "def list_corosync_nodes():",
            "    '''",
            "    Returns list of nodes configured",
            "    in corosync.conf",
            "    '''",
            "    try:",
            "        cfg = os.getenv('COROSYNC_MAIN_CONFIG_FILE', '/etc/corosync/corosync.conf')",
            "        lines = open(cfg).read().split('\\n')",
            "        addr_re = re.compile(r'\\s*ring0_addr:\\s+(.*)')",
            "        nodes = []",
            "        for line in lines:",
            "            addr = addr_re.match(line)",
            "            if addr:",
            "                nodes.append(addr.group(1))",
            "        return nodes",
            "    except Exception:",
            "        return []",
            "",
            "",
            "def print_cluster_nodes():",
            "    \"\"\"",
            "    Print the output of crm_node -l",
            "    \"\"\"",
            "    rc, out, _ = get_stdout_stderr(\"crm_node -l\")",
            "    if rc == 0 and out:",
            "        print(\"{}\\n\".format(out))",
            "",
            "",
            "def list_cluster_nodes():",
            "    '''",
            "    Returns a list of nodes in the cluster.",
            "    '''",
            "    def getname(toks):",
            "        if toks and len(toks) >= 2:",
            "            return toks[1]",
            "        return None",
            "",
            "    try:",
            "        # when pacemaker running",
            "        rc, outp = stdout2list(['crm_node', '-l'], stderr_on=False, shell=False)",
            "        if rc == 0:",
            "            return [x for x in [getname(line.split()) for line in outp] if x and x != '(null)']",
            "",
            "        # when corosync running",
            "        ip_list = get_member_iplist()",
            "        if ip_list:",
            "            return ip_list",
            "",
            "        # static situation",
            "        cib_path = os.getenv('CIB_file', '/var/lib/pacemaker/cib/cib.xml')",
            "        if not os.path.isfile(cib_path):",
            "            return None",
            "        from . import xmlutil",
            "        node_list = []",
            "        cib = xmlutil.file2cib_elem(cib_path)",
            "        if cib is None:",
            "            return None",
            "        for node in cib.xpath('/cib/configuration/nodes/node'):",
            "            name = node.get('uname') or node.get('id')",
            "            if node.get('type') == 'remote':",
            "                srv = cib.xpath(\"//primitive[@id='%s']/instance_attributes/nvpair[@name='server']\" % (name))",
            "                if srv:",
            "                    continue",
            "            node_list.append(name)",
            "        return node_list",
            "    except OSError as msg:",
            "        raise ValueError(\"Error listing cluster nodes: %s\" % (msg))",
            "",
            "",
            "def cluster_run_cmd(cmd):",
            "    \"\"\"",
            "    Run cmd in cluster nodes",
            "    \"\"\"",
            "    node_list = list_cluster_nodes()",
            "    if not node_list:",
            "        raise ValueError(\"Failed to get node list from cluster\")",
            "    parallax.parallax_call(node_list, cmd)",
            "",
            "",
            "def list_cluster_nodes_except_me():",
            "    \"\"\"",
            "    Get cluster node list and filter out self",
            "    \"\"\"",
            "    node_list = list_cluster_nodes()",
            "    if not node_list:",
            "        raise ValueError(\"Failed to get node list from cluster\")",
            "    me = this_node()",
            "    if me in node_list:",
            "        node_list.remove(me)",
            "    return node_list",
            "",
            "",
            "def service_info(name):",
            "    p = is_program('systemctl')",
            "    if p:",
            "        rc, outp = get_stdout([p, 'show',",
            "                               '-p', 'UnitFileState',",
            "                               '-p', 'ActiveState',",
            "                               '-p', 'SubState',",
            "                               name + '.service'], shell=False)",
            "        if rc == 0:",
            "            info = []",
            "            for line in outp.split('\\n'):",
            "                data = line.split('=', 1)",
            "                if len(data) == 2:",
            "                    info.append(data[1].strip())",
            "            return '/'.join(info)",
            "    return None",
            "",
            "",
            "def running_on(resource):",
            "    \"returns list of node names where the given resource is running\"",
            "    rsc_locate = \"crm_resource --resource '%s' --locate\"",
            "    rc, out, err = get_stdout_stderr(rsc_locate % (resource))",
            "    if rc != 0:",
            "        return []",
            "    nodes = []",
            "    head = \"resource %s is running on: \" % (resource)",
            "    for line in out.split('\\n'):",
            "        if line.strip().startswith(head):",
            "            w = line[len(head):].split()",
            "            if w:",
            "                nodes.append(w[0])",
            "    common_debug(\"%s running on: %s\" % (resource, nodes))",
            "    return nodes",
            "",
            "",
            "# This RE matches nvpair values that can",
            "# be left unquoted",
            "_NOQUOTES_RE = re.compile(r'^[\\w\\.-]+$')",
            "",
            "",
            "def noquotes(v):",
            "    return _NOQUOTES_RE.match(v) is not None",
            "",
            "",
            "def unquote(s):",
            "    \"\"\"",
            "    Reverse shell-quoting a string, so the string '\"a b c\"'",
            "    becomes 'a b c'",
            "    \"\"\"",
            "    sp = shlex.split(s)",
            "    if sp:",
            "        return sp[0]",
            "    return \"\"",
            "",
            "",
            "def parse_sysconfig(sysconfig_file):",
            "    \"\"\"",
            "    Reads a sysconfig file into a dict",
            "    \"\"\"",
            "    ret = {}",
            "    if os.path.isfile(sysconfig_file):",
            "        for line in open(sysconfig_file).readlines():",
            "            if line.lstrip().startswith('#'):",
            "                continue",
            "            try:",
            "                key, val = line.split(\"=\", 1)",
            "                ret[key] = unquote(val)",
            "            except ValueError:",
            "                pass",
            "    return ret",
            "",
            "",
            "def sysconfig_set(sysconfig_file, **values):",
            "    \"\"\"",
            "    Set the values in the sysconfig file, updating the variables",
            "    if they exist already, appending them if not.",
            "    \"\"\"",
            "    outp = \"\"",
            "    if os.path.isfile(sysconfig_file):",
            "        for line in open(sysconfig_file).readlines():",
            "            if line.lstrip().startswith('#'):",
            "                outp += line",
            "            else:",
            "                matched = False",
            "                try:",
            "                    key, _ = line.split(\"=\", 1)",
            "                    for k, v in values.items():",
            "                        if k == key:",
            "                            matched = True",
            "                            outp += '%s=%s\\n' % (k, doublequote(v))",
            "                            del values[k]",
            "                            break",
            "                    if not matched:",
            "                        outp += line",
            "                except ValueError:",
            "                    outp += line",
            "",
            "    for k, v in values.items():",
            "        outp += '%s=%s\\n' % (k, doublequote(v))",
            "    str2file(outp, sysconfig_file)",
            "",
            "",
            "def remote_diff_slurp(nodes, filename):",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"Parallax is required to diff\")",
            "    from . import tmpfiles",
            "",
            "    tmpdir = tmpfiles.create_dir()",
            "    opts = parallax.Options()",
            "    opts.localdir = tmpdir",
            "    dst = os.path.basename(filename)",
            "    return list(parallax.slurp(nodes, filename, dst, opts).items())",
            "",
            "",
            "def remote_diff_this(local_path, nodes, this_node):",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"Parallax is required to diff\")",
            "",
            "    by_host = remote_diff_slurp(nodes, local_path)",
            "    for host, result in by_host:",
            "        if isinstance(result, parallax.Error):",
            "            raise ValueError(\"Failed on %s: %s\" % (host, str(result)))",
            "        _, _, _, path = result",
            "        _, s = get_stdout(\"diff -U 0 -d -b --label %s --label %s %s %s\" %",
            "                          (host, this_node, path, local_path))",
            "        page_string(s)",
            "",
            "",
            "def remote_diff(local_path, nodes):",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"parallax is required to diff\")",
            "",
            "    by_host = remote_diff_slurp(nodes, local_path)",
            "    for host, result in by_host:",
            "        if isinstance(result, parallax.Error):",
            "            raise ValueError(\"Failed on %s: %s\" % (host, str(result)))",
            "    h1, r1 = by_host[0]",
            "    h2, r2 = by_host[1]",
            "    _, s = get_stdout(\"diff -U 0 -d -b --label %s --label %s %s %s\" %",
            "                      (h1, h2, r1[3], r2[3]))",
            "    page_string(s)",
            "",
            "",
            "def remote_checksum(local_path, nodes, this_node):",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"Parallax is required to diff\")",
            "    import hashlib",
            "",
            "    by_host = remote_diff_slurp(nodes, local_path)",
            "    for host, result in by_host:",
            "        if isinstance(result, parallax.Error):",
            "            raise ValueError(str(result))",
            "",
            "    print(\"%-16s  SHA1 checksum of %s\" % ('Host', local_path))",
            "    if this_node not in nodes:",
            "        print(\"%-16s: %s\" % (this_node, hashlib.sha1(open(local_path).read()).hexdigest()))",
            "    for host, result in by_host:",
            "        _, _, _, path = result",
            "        print(\"%-16s: %s\" % (host, hashlib.sha1(open(path).read()).hexdigest()))",
            "",
            "",
            "def cluster_copy_file(local_path, nodes=None):",
            "    \"\"\"",
            "    Copies given file to all other cluster nodes.",
            "    \"\"\"",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"parallax is required to copy cluster files\")",
            "    if not nodes:",
            "        nodes = list_cluster_nodes()",
            "        nodes.remove(this_node())",
            "    opts = parallax.Options()",
            "    opts.timeout = 60",
            "    opts.ssh_options += ['ControlPersist=no']",
            "    ok = True",
            "    for host, result in parallax.copy(nodes,",
            "                                      local_path,",
            "                                      local_path, opts).items():",
            "        if isinstance(result, parallax.Error):",
            "            err_buf.error(\"Failed to push %s to %s: %s\" % (local_path, host, result))",
            "            ok = False",
            "        else:",
            "            err_buf.ok(host)",
            "    return ok",
            "",
            "",
            "# a set of fnmatch patterns to match attributes whose values",
            "# should be obscured as a sequence of **** when printed",
            "_obscured_nvpairs = []",
            "",
            "",
            "def obscured(key, value):",
            "    if key is not None and value is not None:",
            "        for o in _obscured_nvpairs:",
            "            if fnmatch.fnmatch(key, o):",
            "                return '*' * 6",
            "    return value",
            "",
            "",
            "@contextmanager",
            "def obscure(obscure_list):",
            "    global _obscured_nvpairs",
            "    prev = _obscured_nvpairs",
            "    _obscured_nvpairs = obscure_list",
            "    try:",
            "        yield",
            "    finally:",
            "        _obscured_nvpairs = prev",
            "",
            "",
            "def gen_nodeid_from_ipv6(addr):",
            "    return int(ipaddress.ip_address(addr)) % 1000000000",
            "",
            "",
            "# Set by detect_cloud()",
            "# to avoid multiple requests",
            "_ip_for_cloud = None",
            "",
            "",
            "def _cloud_metadata_request(uri, headers={}):",
            "    try:",
            "        import urllib2 as urllib",
            "    except ImportError:",
            "        import urllib.request as urllib",
            "    req = urllib.Request(uri)",
            "    for header, value in headers.items():",
            "        req.add_header(header, value)",
            "    try:",
            "        resp = urllib.urlopen(req, timeout=5)",
            "        content = resp.read()",
            "        if type(content) != str:",
            "            return content.decode('utf-8').strip()",
            "        return content.strip()",
            "    except urllib.URLError:",
            "        return None",
            "",
            "",
            "@memoize",
            "def detect_cloud():",
            "    \"\"\"",
            "    Tries to determine which (if any) cloud environment",
            "    the cluster is running on.",
            "",
            "    This is mainly done using dmidecode.",
            "",
            "    If the host cannot be determined, this function",
            "    returns None. Otherwise, it returns a string",
            "    identifying the platform.",
            "",
            "    These are the currently known platforms:",
            "",
            "    * amazon-web-services",
            "    * microsoft-azure",
            "    * google-cloud-platform",
            "",
            "    \"\"\"",
            "    global _ip_for_cloud",
            "",
            "    if not is_program(\"dmidecode\"):",
            "        return None",
            "    rc, system_version = get_stdout(\"dmidecode -s system-version\")",
            "    if re.search(r\".*amazon.*\", system_version) is not None:",
            "        return \"amazon-web-services\"",
            "    if rc != 0:",
            "        return None",
            "    rc, system_manufacturer = get_stdout(\"dmidecode -s system-manufacturer\")",
            "    if rc == 0 and \"microsoft corporation\" in system_manufacturer.lower():",
            "        # To detect azure we also need to make an API request",
            "        result = _cloud_metadata_request(",
            "            \"http://169.254.169.254/metadata/instance/network/interface/0/ipv4/ipAddress/0/privateIpAddress?api-version=2017-08-01&format=text\",",
            "            headers={\"Metadata\": \"true\"})",
            "        if result:",
            "            _ip_for_cloud = result",
            "            return \"microsoft-azure\"",
            "    rc, bios_vendor = get_stdout(\"dmidecode -s bios-vendor\")",
            "    if rc == 0 and \"Google\" in bios_vendor:",
            "        # To detect GCP we also need to make an API request",
            "        result = _cloud_metadata_request(",
            "            \"http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip\",",
            "            headers={\"Metadata-Flavor\": \"Google\"})",
            "        if result:",
            "            _ip_for_cloud = result",
            "            return \"google-cloud-platform\"",
            "    return None",
            "",
            "",
            "def debug_timestamp():",
            "    return datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')",
            "",
            "",
            "def get_member_iplist():",
            "    rc, out, err= get_stdout_stderr(\"corosync-cmapctl -b runtime.totem.pg.mrp.srp.members\")",
            "    if rc != 0:",
            "        common_debug(err)",
            "        return None",
            "",
            "    ip_list = []",
            "    for line in out.split('\\n'):",
            "        match = re.search(r'ip\\((.*?)\\)', line)",
            "        if match:",
            "            ip_list.append(match.group(1))",
            "    return ip_list",
            "",
            "",
            "def get_iplist_corosync_using():",
            "    \"\"\"",
            "    Get ip list used by corosync",
            "    \"\"\"",
            "    rc, out, err = get_stdout_stderr(\"corosync-cfgtool -s\")",
            "    if rc != 0:",
            "        raise ValueError(err)",
            "    return re.findall(r'id\\s*=\\s*(.*)', out)",
            "",
            "",
            "def check_ssh_passwd_need(host):",
            "    \"\"\"",
            "    Check whether access to host need password",
            "    \"\"\"",
            "    ssh_options = \"-o StrictHostKeyChecking=no -o EscapeChar=none -o ConnectTimeout=15\"",
            "    ssh_cmd = \"ssh {} -T -o Batchmode=yes {} true\".format(ssh_options, host)",
            "    rc, _, _ = get_stdout_stderr(ssh_cmd)",
            "    return rc != 0",
            "",
            "",
            "def check_port_open(ip, port):",
            "    import socket",
            "",
            "    family = socket.AF_INET6 if IP.is_ipv6(ip) else socket.AF_INET",
            "    with closing(socket.socket(family, socket.SOCK_STREAM)) as sock:",
            "        if sock.connect_ex((ip, port)) == 0:",
            "            return True",
            "        else:",
            "            return False",
            "",
            "",
            "def valid_port(port):",
            "    return int(port) >= 1024 and int(port) <= 65535",
            "",
            "",
            "def is_qdevice_configured():",
            "    from . import corosync",
            "    return corosync.get_value(\"quorum.device.model\") == \"net\"",
            "",
            "",
            "def is_qdevice_tls_on():",
            "    from . import corosync",
            "    return corosync.get_value(\"quorum.device.net.tls\") == \"on\"",
            "",
            "",
            "def get_nodeinfo_from_cmaptool():",
            "    nodeid_ip_dict = {}",
            "    rc, out = get_stdout(\"corosync-cmapctl -b runtime.totem.pg.mrp.srp.members\")",
            "    if rc != 0:",
            "        return nodeid_ip_dict",
            "",
            "    for line in out.split('\\n'):",
            "        match = re.search(r'members\\.(.*)\\.ip', line)",
            "        if match:",
            "            node_id = match.group(1)",
            "            iplist = re.findall(r'[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}', line)",
            "            nodeid_ip_dict[node_id] = iplist",
            "    return nodeid_ip_dict",
            "",
            "",
            "def get_iplist_from_name(name):",
            "    \"\"\"",
            "    Given node host name, return this host's ip list in corosync cmap",
            "    \"\"\"",
            "    ip_list = []",
            "    nodeid = get_nodeid_from_name(name)",
            "    if not nodeid:",
            "        return ip_list",
            "    nodeinfo = {}",
            "    nodeinfo = get_nodeinfo_from_cmaptool()",
            "    if not nodeinfo:",
            "        return ip_list",
            "    return nodeinfo[nodeid]",
            "",
            "",
            "def valid_nodeid(nodeid):",
            "    from . import bootstrap",
            "    if not service_is_active('corosync.service'):",
            "        return False",
            "",
            "    for _id, _ in get_nodeinfo_from_cmaptool().items():",
            "        if _id == nodeid:",
            "            return True",
            "    return False",
            "",
            "",
            "def get_nodeid_from_name(name):",
            "    rc, out = get_stdout('crm_node -l')",
            "    if rc != 0:",
            "        return None",
            "    res = re.search(r'^([0-9]+) {} '.format(name), out, re.M)",
            "    if res:",
            "        return res.group(1)",
            "    else:",
            "        return None",
            "",
            "",
            "def check_space_option_value(options):",
            "    if not isinstance(options, argparse.Namespace):",
            "        raise ValueError(\"Expected type of \\\"options\\\" is \\\"argparse.Namespace\\\", not \\\"{}\\\"\".format(type(options)))",
            "",
            "    for opt in vars(options):",
            "        value = getattr(options, opt)",
            "        if isinstance(value, str) and len(value.strip()) == 0:",
            "            raise ValueError(\"Space value not allowed for dest \\\"{}\\\"\".format(opt))",
            "",
            "",
            "def interface_choice():",
            "    _, out = get_stdout(\"ip a\")",
            "    # should consider interface format like \"ethx@xxx\"",
            "    interface_list = re.findall(r'(?:[0-9]+:) (.*?)(?=: |@.*?: )', out)",
            "    return [nic for nic in interface_list if nic != \"lo\"]",
            "",
            "",
            "class IP(object):",
            "    \"\"\"",
            "    Class to get some properties of IP address",
            "    \"\"\"",
            "",
            "    def __init__(self, addr):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self.addr = addr",
            "",
            "    @property",
            "    def ip_address(self):",
            "        \"\"\"",
            "        Create ipaddress instance",
            "        \"\"\"",
            "        return ipaddress.ip_address(self.addr)",
            "",
            "    @property",
            "    def version(self):",
            "        \"\"\"",
            "        Get IP address version",
            "        \"\"\"",
            "        return self.ip_address.version",
            "",
            "    @classmethod",
            "    def is_mcast(cls, addr):",
            "        \"\"\"",
            "        Check whether the address is multicast address",
            "        \"\"\"",
            "        cls_inst = cls(addr)",
            "        return cls_inst.ip_address.is_multicast",
            "",
            "    @classmethod",
            "    def is_ipv6(cls, addr):",
            "        \"\"\"",
            "        Check whether the address is IPV6 address",
            "        \"\"\"",
            "        return cls(addr).version == 6",
            "",
            "    @classmethod",
            "    def is_valid_ip(cls, addr):",
            "        \"\"\"",
            "        Check whether the address is valid IP address",
            "        \"\"\"",
            "        cls_inst = cls(addr)",
            "        try:",
            "            cls_inst.ip_address",
            "        except ValueError:",
            "            return False",
            "        else:",
            "            return True",
            "",
            "    @property",
            "    def is_loopback(self):",
            "        \"\"\"",
            "        Check whether the address is loopback address",
            "        \"\"\"",
            "        return self.ip_address.is_loopback",
            "",
            "    @property",
            "    def is_link_local(self):",
            "        \"\"\"",
            "        Check whether the address is link-local address",
            "        \"\"\"",
            "        return self.ip_address.is_link_local",
            "",
            "",
            "class Interface(IP):",
            "    \"\"\"",
            "    Class to get information from one interface",
            "    \"\"\"",
            "",
            "    def __init__(self, ip_with_mask):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self.ip, self.mask = ip_with_mask.split('/')",
            "        super(__class__, self).__init__(self.ip)",
            "",
            "    @property",
            "    def ip_with_mask(self):",
            "        \"\"\"",
            "        Get ip with netmask",
            "        \"\"\"",
            "        return '{}/{}'.format(self.ip, self.mask)",
            "",
            "    @property",
            "    def ip_interface(self):",
            "        \"\"\"",
            "        Create ip_interface instance",
            "        \"\"\"",
            "        return ipaddress.ip_interface(self.ip_with_mask)",
            "",
            "    @property",
            "    def network(self):",
            "        \"\"\"",
            "        Get network address",
            "        \"\"\"",
            "        return str(self.ip_interface.network.network_address)",
            "",
            "    def ip_in_network(self, addr):",
            "        \"\"\"",
            "        Check whether the addr in the network",
            "        \"\"\"",
            "        return IP(addr).ip_address in self.ip_interface.network",
            "",
            "",
            "class InterfacesInfo(object):",
            "    \"\"\"",
            "    Class to collect interfaces information on local node",
            "    \"\"\"",
            "",
            "    def __init__(self, ipv6=False, second_heartbeat=False, custom_nic_list=[]):",
            "        \"\"\"",
            "        Init function",
            "",
            "        On init process,",
            "        \"ipv6\" is provided by -I option",
            "        \"second_heartbeat\" is provided by -M option",
            "        \"custom_nic_list\" is provided by -i option",
            "        \"\"\"",
            "        self.ip_version = 6 if ipv6 else 4",
            "        self.second_heartbeat = second_heartbeat",
            "        self._default_nic_list = custom_nic_list",
            "        self._nic_info_dict = {}",
            "",
            "    def get_interfaces_info(self):",
            "        \"\"\"",
            "        Try to get interfaces info dictionary via \"ip\" command",
            "",
            "        IMPORTANT: This is the method that populates the data, should always be called after initialize",
            "        \"\"\"",
            "        cmd = \"ip -{} -o addr show\".format(self.ip_version)",
            "        rc, out, err = get_stdout_stderr(cmd)",
            "        if rc != 0:",
            "            raise ValueError(err)",
            "",
            "        # format on each line will like:",
            "        # 2: enp1s0    inet 192.168.122.241/24 brd 192.168.122.255 scope global enp1s0\\       valid_lft forever preferred_lft forever",
            "        for line in out.splitlines():",
            "            _, nic, _, ip_with_mask, *_ = line.split()",
            "            # maybe from tun interface",
            "            if not '/' in ip_with_mask:",
            "                continue",
            "            #TODO change this condition when corosync support link-local address",
            "            interface_inst = Interface(ip_with_mask)",
            "            if interface_inst.is_loopback or interface_inst.is_link_local:",
            "                continue",
            "            # one nic might configured multi IP addresses",
            "            if nic not in self._nic_info_dict:",
            "                self._nic_info_dict[nic] = []",
            "            self._nic_info_dict[nic].append(interface_inst)",
            "",
            "        if not self._nic_info_dict:",
            "            raise ValueError(\"No address configured\")",
            "        if self.second_heartbeat and len(self._nic_info_dict) == 1:",
            "            raise ValueError(\"Cannot configure second heartbeat, since only one address is available\")",
            "",
            "    @property",
            "    def nic_list(self):",
            "        \"\"\"",
            "        Get interfaces name list",
            "        \"\"\"",
            "        return list(self._nic_info_dict.keys())",
            "",
            "    @property",
            "    def interface_list(self):",
            "        \"\"\"",
            "        Get instance list of class Interface",
            "        \"\"\"",
            "        _interface_list = []",
            "        for interface in self._nic_info_dict.values():",
            "            _interface_list.extend(interface)",
            "        return _interface_list",
            "",
            "    @property",
            "    def ip_list(self):",
            "        \"\"\"",
            "        Get IP address list",
            "        \"\"\"",
            "        return [interface.ip for interface in self.interface_list]",
            "",
            "    @classmethod",
            "    def get_local_ip_list(cls, is_ipv6):",
            "        \"\"\"",
            "        Get IP address list",
            "        \"\"\"",
            "        cls_inst = cls(is_ipv6)",
            "        cls_inst.get_interfaces_info()",
            "        return cls_inst.ip_list",
            "",
            "    @classmethod",
            "    def ip_in_local(cls, addr):",
            "        \"\"\"",
            "        Check whether given address was in one of local address",
            "        \"\"\"",
            "        cls_inst = cls(IP.is_ipv6(addr))",
            "        cls_inst.get_interfaces_info()",
            "        return addr in cls_inst.ip_list",
            "",
            "    @property",
            "    def network_list(self):",
            "        \"\"\"",
            "        Get network list",
            "        \"\"\"",
            "        return list(set([interface.network for interface in self.interface_list]))",
            "",
            "    def _nic_first_ip(self, nic):",
            "        \"\"\"",
            "        Get the first IP of specific nic",
            "        \"\"\"",
            "        return self._nic_info_dict[nic][0].ip",
            "",
            "    def get_default_nic_list_from_route(self):",
            "        \"\"\"",
            "        Get default nic list from route",
            "        \"\"\"",
            "        if self._default_nic_list:",
            "            return self._default_nic_list",
            "",
            "        #TODO what if user only has ipv6 route?",
            "        cmd = \"ip -o route show\"",
            "        rc, out, err = get_stdout_stderr(cmd)",
            "        if rc != 0:",
            "            raise ValueError(err)",
            "        res = re.search(r'^default via .* dev (.*?) ', out)",
            "        if res:",
            "            self._default_nic_list = [res.group(1)]",
            "        else:",
            "            if not self.nic_list:",
            "                self.get_interfaces_info()",
            "            common_warn(\"No default route configured. Using the first found nic\")",
            "            self._default_nic_list = [self.nic_list[0]]",
            "        return self._default_nic_list",
            "",
            "    def get_default_ip_list(self):",
            "        \"\"\"",
            "        Get default IP list will be used by corosync",
            "        \"\"\"",
            "        if not self._default_nic_list:",
            "            self.get_default_nic_list_from_route()",
            "        if not self.nic_list:",
            "            self.get_interfaces_info()",
            "",
            "        _ip_list = []",
            "        for nic in self._default_nic_list:",
            "            # in case given interface not exist",
            "            if nic not in self.nic_list:",
            "                raise ValueError(\"Failed to detect IP address for {}\".format(nic))",
            "            _ip_list.append(self._nic_first_ip(nic))",
            "        # in case -M specified but given one interface via -i",
            "        if self.second_heartbeat and len(self._default_nic_list) == 1:",
            "            for nic in self.nic_list:",
            "                if nic not in self._default_nic_list:",
            "                    _ip_list.append(self._nic_first_ip(nic))",
            "                    break",
            "        return _ip_list",
            "",
            "    @classmethod",
            "    def ip_in_network(cls, addr):",
            "        \"\"\"",
            "        Check whether given address was in one of local networks",
            "        \"\"\"",
            "        cls_inst = cls(IP.is_ipv6(addr))",
            "        cls_inst.get_interfaces_info()",
            "        for interface_inst in cls_inst.interface_list:",
            "            if interface_inst.ip_in_network(addr):",
            "                return True",
            "        return False",
            "",
            "",
            "def check_file_content_included(source_file, target_file):",
            "    \"\"\"",
            "    Check whether target_file includes contents of source_file",
            "    \"\"\"",
            "    if not os.path.exists(source_file):",
            "        raise ValueError(\"File {} not exist\".format(source_file))",
            "    if not os.path.exists(target_file):",
            "        return False",
            "",
            "    with open(target_file, 'r') as target_fd:",
            "        target_data = target_fd.read()",
            "    with open(source_file, 'r') as source_fd:",
            "        source_data = source_fd.read()",
            "    return source_data in target_data",
            "",
            "",
            "class ServiceManager(object):",
            "    \"\"\"",
            "    Class to manage systemctl services",
            "    \"\"\"",
            "    ACTION_MAP = {",
            "            \"enable\": \"enable\",",
            "            \"disable\": \"disable\",",
            "            \"start\": \"start\",",
            "            \"stop\": \"stop\",",
            "            \"is_enabled\": \"is-enabled\",",
            "            \"is_active\": \"is-active\",",
            "            \"is_available\": \"list-unit-files\"",
            "            }",
            "",
            "    def __init__(self, service_name, remote_addr=None):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self.service_name = service_name",
            "        self.remote_addr = remote_addr",
            "",
            "    def _do_action(self, action_type):",
            "        \"\"\"",
            "        Actual do actions to manage service",
            "        \"\"\"",
            "        if action_type not in self.ACTION_MAP.values():",
            "            raise ValueError(\"status_type should be {}\".format('/'.join(list(self.ACTION_MAP.values()))))",
            "",
            "        cmd = \"systemctl {} {}\".format(action_type, self.service_name)",
            "        if self.remote_addr:",
            "            prompt_msg = \"Run \\\"{}\\\" on {}\".format(cmd, self.remote_addr)",
            "            rc, output, err = run_cmd_on_remote(cmd, self.remote_addr, prompt_msg)",
            "        else:",
            "            rc, output, err = get_stdout_stderr(cmd)",
            "        if rc != 0 and err:",
            "            raise ValueError(\"Run \\\"{}\\\" error: {}\".format(cmd, err))",
            "        return rc == 0, output",
            "",
            "    @property",
            "    def is_available(self):",
            "        return self.service_name in self._do_action(self.ACTION_MAP[\"is_available\"])[1]",
            "",
            "    @property",
            "    def is_enabled(self):",
            "        return self._do_action(self.ACTION_MAP[\"is_enabled\"])[0]",
            "",
            "    @property",
            "    def is_active(self):",
            "        return self._do_action(self.ACTION_MAP[\"is_active\"])[0]",
            "",
            "    def start(self):",
            "        self._do_action(self.ACTION_MAP[\"start\"])",
            "",
            "    def stop(self):",
            "        self._do_action(self.ACTION_MAP[\"stop\"])",
            "",
            "    def enable(self):",
            "        self._do_action(self.ACTION_MAP[\"enable\"])",
            "",
            "    def disable(self):",
            "        self._do_action(self.ACTION_MAP[\"disable\"])",
            "",
            "    @classmethod",
            "    def service_is_available(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Check whether service is available",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        return inst.is_available",
            "",
            "    @classmethod",
            "    def service_is_enabled(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Check whether service is enabled",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        return inst.is_enabled",
            "",
            "    @classmethod",
            "    def service_is_active(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Check whether service is active",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        return inst.is_active",
            "",
            "    @classmethod",
            "    def start_service(cls, name, enable=False, remote_addr=None):",
            "        \"\"\"",
            "        Start service",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        if enable:",
            "            inst.enable()",
            "        inst.start()",
            "",
            "    @classmethod",
            "    def stop_service(cls, name, disable=False, remote_addr=None):",
            "        \"\"\"",
            "        Stop service",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        if disable:",
            "            inst.disable()",
            "        inst.stop()",
            "",
            "    @classmethod",
            "    def enable_service(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Enable service",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        if inst.is_available and not inst.is_enabled:",
            "            inst.enable()",
            "",
            "    @classmethod",
            "    def disable_service(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Disable service",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        if inst.is_available and inst.is_enabled:",
            "            inst.disable()",
            "",
            "",
            "service_is_available = ServiceManager.service_is_available",
            "service_is_enabled = ServiceManager.service_is_enabled",
            "service_is_active = ServiceManager.service_is_active",
            "start_service = ServiceManager.start_service",
            "stop_service = ServiceManager.stop_service",
            "enable_service = ServiceManager.enable_service",
            "disable_service = ServiceManager.disable_service",
            "",
            "",
            "def package_is_installed(pkg, remote_addr=None):",
            "    \"\"\"",
            "    Check if package is installed",
            "    \"\"\"",
            "    cmd = \"rpm -q --quiet {}\".format(pkg)",
            "    if remote_addr:",
            "        # check on remote",
            "        prompt_msg = \"Check whether {} is installed on {}\".format(pkg, remote_addr)",
            "        rc, _, _ = run_cmd_on_remote(cmd, remote_addr, prompt_msg)",
            "    else:",
            "        # check on local",
            "        rc, _ = get_stdout(cmd)",
            "    return rc == 0",
            "",
            "",
            "def ping_node(node):",
            "    \"\"\"",
            "    Check if the remote node is reachable",
            "    \"\"\"",
            "    rc, _, err = get_stdout_stderr(\"ping -c 1 {}\".format(node))",
            "    if rc != 0:",
            "        raise ValueError(\"host \\\"{}\\\" is unreachable: {}\".format(node, err))",
            "# vim:ts=4:sw=4:et:"
        ],
        "afterPatchFile": [
            "# Copyright (C) 2008-2011 Dejan Muhamedagic <dmuhamedagic@suse.de>",
            "# See COPYING for license information.",
            "",
            "import os",
            "import sys",
            "from tempfile import mkstemp",
            "import subprocess",
            "import re",
            "import glob",
            "import time",
            "import datetime",
            "import shutil",
            "import shlex",
            "import bz2",
            "import fnmatch",
            "import gc",
            "import ipaddress",
            "import argparse",
            "from contextlib import contextmanager, closing",
            "from . import config",
            "from . import userdir",
            "from . import constants",
            "from . import options",
            "from . import term",
            "from . import parallax",
            "from .msg import common_warn, common_info, common_debug, common_err, err_buf",
            "",
            "",
            "def to_ascii(input_str):",
            "    \"\"\"Convert the bytes string to a ASCII string",
            "    Usefull to remove accent (diacritics)\"\"\"",
            "    if input_str is None:",
            "        return input_str",
            "    if isinstance(input_str, str):",
            "        return input_str",
            "    try:",
            "        return str(input_str, 'utf-8')",
            "    except UnicodeDecodeError:",
            "        if config.core.debug or options.regression_tests:",
            "            import traceback",
            "            traceback.print_exc()",
            "        return input_str.decode('utf-8', errors='ignore')",
            "",
            "",
            "def filter_keys(key_list, args, sign=\"=\"):",
            "    \"\"\"Return list item which not be completed yet\"\"\"",
            "    return [s+sign for s in key_list if any_startswith(args, s+sign) is None]",
            "",
            "",
            "def any_startswith(iterable, prefix):",
            "    \"\"\"Return first element in iterable which startswith prefix, or None.\"\"\"",
            "    for element in iterable:",
            "        if element.startswith(prefix):",
            "            return element",
            "    return None",
            "",
            "",
            "def rindex(iterable, value):",
            "    return len(iterable) - iterable[::-1].index(value) - 1",
            "",
            "",
            "def memoize(function):",
            "    \"Decorator to invoke a function once only for any argument\"",
            "    memoized = {}",
            "",
            "    def inner(*args):",
            "        if args in memoized:",
            "            return memoized[args]",
            "        r = function(*args)",
            "        memoized[args] = r",
            "        return r",
            "    return inner",
            "",
            "",
            "@contextmanager",
            "def nogc():",
            "    gc.disable()",
            "    try:",
            "        yield",
            "    finally:",
            "        gc.enable()",
            "",
            "",
            "getuser = userdir.getuser",
            "gethomedir = userdir.gethomedir",
            "",
            "",
            "@memoize",
            "def this_node():",
            "    'returns name of this node (hostname)'",
            "    return os.uname()[1]",
            "",
            "",
            "_cib_shadow = 'CIB_shadow'",
            "_cib_in_use = ''",
            "",
            "",
            "def set_cib_in_use(name):",
            "    os.putenv(_cib_shadow, name)",
            "    global _cib_in_use",
            "    _cib_in_use = name",
            "",
            "",
            "def clear_cib_in_use():",
            "    os.unsetenv(_cib_shadow)",
            "    global _cib_in_use",
            "    _cib_in_use = ''",
            "",
            "",
            "def get_cib_in_use():",
            "    return _cib_in_use",
            "",
            "",
            "def get_tempdir():",
            "    return os.getenv(\"TMPDIR\") or \"/tmp\"",
            "",
            "",
            "def is_program(prog):",
            "    \"\"\"Is this program available?\"\"\"",
            "    def isexec(filename):",
            "        return os.path.isfile(filename) and os.access(filename, os.X_OK)",
            "    for p in os.getenv(\"PATH\").split(os.pathsep):",
            "        f = os.path.join(p, prog)",
            "        if isexec(f):",
            "            return f",
            "    return None",
            "",
            "",
            "def pacemaker_20_daemon(new, old):",
            "    \"helper to discover renamed pacemaker daemons\"",
            "    if is_program(new):",
            "        return new",
            "    return old",
            "",
            "",
            "@memoize",
            "def pacemaker_attrd():",
            "    return pacemaker_20_daemon(\"pacemaker-attrd\", \"attrd\")",
            "",
            "",
            "@memoize",
            "def pacemaker_based():",
            "    return pacemaker_20_daemon(\"pacemaker-based\", \"cib\")",
            "",
            "",
            "@memoize",
            "def pacemaker_controld():",
            "    return pacemaker_20_daemon(\"pacemaker-controld\", \"crmd\")",
            "",
            "",
            "@memoize",
            "def pacemaker_execd():",
            "    return pacemaker_20_daemon(\"pacemaker-execd\", \"lrmd\")",
            "",
            "",
            "@memoize",
            "def pacemaker_fenced():",
            "    return pacemaker_20_daemon(\"pacemaker-fenced\", \"stonithd\")",
            "",
            "",
            "@memoize",
            "def pacemaker_remoted():",
            "    return pacemaker_20_daemon(\"pacemaker-remoted\", \"pacemaker_remoted\")",
            "",
            "",
            "@memoize",
            "def pacemaker_schedulerd():",
            "    return pacemaker_20_daemon(\"pacemaker-schedulerd\", \"pengine\")",
            "",
            "",
            "def pacemaker_daemon(name):",
            "    if name == \"attrd\" or name == \"pacemaker-attrd\":",
            "        return pacemaker_attrd()",
            "    if name == \"cib\" or name == \"pacemaker-based\":",
            "        return pacemaker_based()",
            "    if name == \"crmd\" or name == \"pacemaker-controld\":",
            "        return pacemaker_controld()",
            "    if name == \"lrmd\" or name == \"pacemaker-execd\":",
            "        return pacemaker_execd()",
            "    if name == \"stonithd\" or name == \"pacemaker-fenced\":",
            "        return pacemaker_fenced()",
            "    if name == \"pacemaker_remoted\" or name == \"pacemeaker-remoted\":",
            "        return pacemaker_remoted()",
            "    if name == \"pengine\" or name == \"pacemaker-schedulerd\":",
            "        return pacemaker_schedulerd()",
            "    raise ValueError(\"Not a Pacemaker daemon name: {}\".format(name))",
            "",
            "",
            "def can_ask():",
            "    \"\"\"",
            "    Is user-interactivity possible?",
            "    Checks if connected to a TTY.",
            "    \"\"\"",
            "    return (not options.ask_no) and sys.stdin.isatty()",
            "",
            "",
            "def ask(msg):",
            "    \"\"\"",
            "    Ask for user confirmation.",
            "    If core.force is true, always return true.",
            "    If not interactive and core.force is false, always return false.",
            "    \"\"\"",
            "    if config.core.force:",
            "        common_info(\"%s [YES]\" % (msg))",
            "        return True",
            "    if not can_ask():",
            "        return False",
            "",
            "    msg += ' '",
            "    if msg.endswith('? '):",
            "        msg = msg[:-2] + ' (y/n)? '",
            "",
            "    while True:",
            "        try:",
            "            ans = input(msg)",
            "        except EOFError:",
            "            ans = 'n'",
            "        if ans:",
            "            ans = ans[0].lower()",
            "            if ans in 'yn':",
            "                return ans == 'y'",
            "",
            "",
            "# holds part of line before \\ split",
            "# for a multi-line input",
            "_LINE_BUFFER = ''",
            "",
            "",
            "def get_line_buffer():",
            "    return _LINE_BUFFER",
            "",
            "",
            "def multi_input(prompt=''):",
            "    \"\"\"",
            "    Get input from user",
            "    Allow multiple lines using a continuation character",
            "    \"\"\"",
            "    global _LINE_BUFFER",
            "    line = []",
            "    _LINE_BUFFER = ''",
            "    while True:",
            "        try:",
            "            text = input(prompt)",
            "        except EOFError:",
            "            return None",
            "        err_buf.incr_lineno()",
            "        if options.regression_tests:",
            "            print(\".INP:\", text)",
            "            sys.stdout.flush()",
            "            sys.stderr.flush()",
            "        stripped = text.strip()",
            "        if stripped.endswith('\\\\'):",
            "            stripped = stripped.rstrip('\\\\')",
            "            line.append(stripped)",
            "            _LINE_BUFFER += stripped",
            "            if prompt:",
            "                prompt = '   > '",
            "        else:",
            "            line.append(stripped)",
            "            break",
            "    return ''.join(line)",
            "",
            "",
            "def verify_boolean(opt):",
            "    return opt.lower() in (\"yes\", \"true\", \"on\", \"1\") or \\",
            "        opt.lower() in (\"no\", \"false\", \"off\", \"0\")",
            "",
            "",
            "def is_boolean_true(opt):",
            "    if opt in (None, False):",
            "        return False",
            "    if opt is True:",
            "        return True",
            "    return opt.lower() in (\"yes\", \"true\", \"on\", \"1\")",
            "",
            "",
            "def is_boolean_false(opt):",
            "    if opt in (None, False):",
            "        return True",
            "    if opt is True:",
            "        return False",
            "    return opt.lower() in (\"no\", \"false\", \"off\", \"0\")",
            "",
            "",
            "def get_boolean(opt, dflt=False):",
            "    if not opt:",
            "        return dflt",
            "    return is_boolean_true(opt)",
            "",
            "",
            "def canonical_boolean(opt):",
            "    return 'true' if is_boolean_true(opt) else 'false'",
            "",
            "",
            "def keyword_cmp(string1, string2):",
            "    return string1.lower() == string2.lower()",
            "",
            "",
            "class olist(list):",
            "    \"\"\"",
            "    Implements the 'in' operator",
            "    in a case-insensitive manner,",
            "    allowing \"if x in olist(...)\"",
            "    \"\"\"",
            "    def __init__(self, keys):",
            "        super(olist, self).__init__([k.lower() for k in keys])",
            "",
            "    def __contains__(self, key):",
            "        return super(olist, self).__contains__(key.lower())",
            "",
            "    def append(self, key):",
            "        super(olist, self).append(key.lower())",
            "",
            "",
            "def os_types_list(path):",
            "    l = []",
            "    for f in glob.glob(path):",
            "        if os.access(f, os.X_OK) and os.path.isfile(f):",
            "            a = f.split(\"/\")",
            "            l.append(a[-1])",
            "    return l",
            "",
            "",
            "def listtemplates():",
            "    l = []",
            "    templates_dir = os.path.join(config.path.sharedir, 'templates')",
            "    for f in os.listdir(templates_dir):",
            "        if os.path.isfile(\"%s/%s\" % (templates_dir, f)):",
            "            l.append(f)",
            "    return l",
            "",
            "",
            "def listconfigs():",
            "    l = []",
            "    for f in os.listdir(userdir.CRMCONF_DIR):",
            "        if os.path.isfile(\"%s/%s\" % (userdir.CRMCONF_DIR, f)):",
            "            l.append(f)",
            "    return l",
            "",
            "",
            "def add_sudo(cmd):",
            "    if config.core.user:",
            "        return \"sudo -E -u %s %s\" % (config.core.user, cmd)",
            "    return cmd",
            "",
            "",
            "def add_su(cmd, user):",
            "    \"\"\"",
            "    Wrapped cmd with su -c \"<cmd>\" <user>",
            "    \"\"\"",
            "    if user == \"root\":",
            "        return cmd",
            "    return \"su -c \\\"{}\\\" {}\".format(cmd, user)",
            "",
            "",
            "def chown(path, user, group):",
            "    if isinstance(user, int):",
            "        uid = user",
            "    else:",
            "        import pwd",
            "        uid = pwd.getpwnam(user).pw_uid",
            "    if isinstance(group, int):",
            "        gid = group",
            "    else:",
            "        import grp",
            "        gid = grp.getgrnam(group).gr_gid",
            "    os.chown(path, uid, gid)",
            "",
            "",
            "def ensure_sudo_readable(f):",
            "    # make sure the tempfile is readable to crm_diff (bsc#999683)",
            "    if config.core.user:",
            "        from pwd import getpwnam",
            "        uid = getpwnam(config.core.user).pw_uid",
            "        try:",
            "            os.chown(f, uid, -1)",
            "        except os.error as err:",
            "            common_err('Failed setting temporary file permissions: %s' % (err))",
            "            return False",
            "    return True",
            "",
            "",
            "def pipe_string(cmd, s):",
            "    rc = -1  # command failed",
            "    cmd = add_sudo(cmd)",
            "    common_debug(\"piping string to %s\" % cmd)",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE)",
            "    try:",
            "        # communicate() expects encoded bytes",
            "        if isinstance(s, str):",
            "            s = s.encode('utf-8')",
            "        p.communicate(s)",
            "        p.wait()",
            "        rc = p.returncode",
            "    except IOError as msg:",
            "        if \"Broken pipe\" not in str(msg):",
            "            common_err(msg)",
            "    return rc",
            "",
            "",
            "def filter_string(cmd, s, stderr_on=True, shell=True):",
            "    rc = -1  # command failed",
            "    outp = ''",
            "    if stderr_on is True:",
            "        stderr = None",
            "    else:",
            "        stderr = subprocess.PIPE",
            "    cmd = add_sudo(cmd)",
            "    common_debug(\"pipe through %s\" % cmd)",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    p = subprocess.Popen(cmd,",
            "                         shell=shell,",
            "                         stdin=subprocess.PIPE,",
            "                         stdout=subprocess.PIPE,",
            "                         stderr=stderr)",
            "    try:",
            "        # bytes expected here",
            "        if isinstance(s, str):",
            "            s = s.encode('utf-8')",
            "        ret = p.communicate(s)",
            "        if stderr_on == 'stdout':",
            "            outp = b\"\\n\".join(ret)",
            "        else:",
            "            outp = ret[0]",
            "        p.wait()",
            "        rc = p.returncode",
            "    except OSError as err:",
            "        if err.errno != os.errno.EPIPE:",
            "            common_err(err.strerror)",
            "        common_info(\"from: %s\" % cmd)",
            "    except Exception as msg:",
            "        common_err(msg)",
            "        common_info(\"from: %s\" % cmd)",
            "    return rc, to_ascii(outp)",
            "",
            "",
            "def str2tmp(_str, suffix=\".pcmk\"):",
            "    '''",
            "    Write the given string to a temporary file. Return the name",
            "    of the file.",
            "    '''",
            "    s = to_ascii(_str)",
            "    fd, tmp = mkstemp(suffix=suffix)",
            "    try:",
            "        f = os.fdopen(fd, \"w\")",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return",
            "    f.write(s)",
            "    if not s.endswith('\\n'):",
            "        f.write(\"\\n\")",
            "    f.close()",
            "    return tmp",
            "",
            "",
            "@contextmanager",
            "def create_tempfile(suffix='', dir=None):",
            "    \"\"\" Context for temporary file.",
            "",
            "    Will find a free temporary filename upon entering",
            "    and will try to delete the file on leaving, even in case of an exception.",
            "",
            "    Parameters",
            "    ----------",
            "    suffix : string",
            "        optional file suffix",
            "    dir : string",
            "        optional directory to save temporary file in",
            "",
            "    (from http://stackoverflow.com/a/29491523)",
            "    \"\"\"",
            "    import tempfile",
            "    tf = tempfile.NamedTemporaryFile(delete=False, suffix=suffix, dir=dir)",
            "    tf.file.close()",
            "    try:",
            "        yield tf.name",
            "    finally:",
            "        try:",
            "            os.remove(tf.name)",
            "        except OSError as e:",
            "            if e.errno == 2:",
            "                pass",
            "            else:",
            "                raise",
            "",
            "",
            "@contextmanager",
            "def open_atomic(filepath, mode=\"r\", buffering=-1, fsync=False, encoding=None):",
            "    \"\"\" Open temporary file object that atomically moves to destination upon",
            "    exiting.",
            "",
            "    Allows reading and writing to and from the same filename.",
            "",
            "    The file will not be moved to destination in case of an exception.",
            "",
            "    Parameters",
            "    ----------",
            "    filepath : string",
            "        the file path to be opened",
            "    fsync : bool",
            "        whether to force write the file to disk",
            "",
            "    (from http://stackoverflow.com/a/29491523)",
            "    \"\"\"",
            "",
            "    with create_tempfile(dir=os.path.dirname(os.path.abspath(filepath))) as tmppath:",
            "        with open(tmppath, mode, buffering, encoding=encoding) as file:",
            "            try:",
            "                yield file",
            "            finally:",
            "                if fsync:",
            "                    file.flush()",
            "                    os.fsync(file.fileno())",
            "        os.rename(tmppath, filepath)",
            "",
            "",
            "def str2file(s, fname):",
            "    '''",
            "    Write a string to a file.",
            "    '''",
            "    try:",
            "        with open_atomic(fname, 'w', encoding='utf-8') as dst:",
            "            dst.write(to_ascii(s))",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return False",
            "    return True",
            "",
            "",
            "def file2str(fname, noerr=True):",
            "    '''",
            "    Read a one line file into a string, strip whitespace around.",
            "    '''",
            "    try:",
            "        f = open(fname, \"r\")",
            "    except IOError as msg:",
            "        if not noerr:",
            "            common_err(msg)",
            "        return None",
            "    s = f.readline()",
            "    f.close()",
            "    return s.strip()",
            "",
            "",
            "def file2list(fname):",
            "    '''",
            "    Read a file into a list (newlines dropped).",
            "    '''",
            "    try:",
            "        return open(fname).read().split('\\n')",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return None",
            "",
            "",
            "def safe_open_w(fname):",
            "    if fname == \"-\":",
            "        f = sys.stdout",
            "    else:",
            "        if not options.batch and os.access(fname, os.F_OK):",
            "            if not ask(\"File %s exists. Do you want to overwrite it?\" % fname):",
            "                return None",
            "        try:",
            "            f = open(fname, \"w\")",
            "        except IOError as msg:",
            "            common_err(msg)",
            "            return None",
            "    return f",
            "",
            "",
            "def safe_close_w(f):",
            "    if f and f != sys.stdout:",
            "        f.close()",
            "",
            "",
            "def is_path_sane(name):",
            "    if re.search(r\"['`#*?$\\[\\]]\", name):",
            "        common_err(\"%s: bad path\" % name)",
            "        return False",
            "    return True",
            "",
            "",
            "def is_filename_sane(name):",
            "    if re.search(r\"['`/#*?$\\[\\]]\", name):",
            "        common_err(\"%s: bad filename\" % name)",
            "        return False",
            "    return True",
            "",
            "",
            "def is_name_sane(name):",
            "    if re.search(\"[']\", name):",
            "        common_err(\"%s: bad name\" % name)",
            "        return False",
            "    return True",
            "",
            "",
            "def show_dot_graph(dotfile, keep_file=False, desc=\"transition graph\"):",
            "    cmd = \"%s %s\" % (config.core.dotty, dotfile)",
            "    if not keep_file:",
            "        cmd = \"(%s; rm -f %s)\" % (cmd, dotfile)",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    subprocess.Popen(cmd, shell=True, bufsize=0,",
            "                     stdin=None, stdout=None, stderr=None, close_fds=True)",
            "    common_info(\"starting %s to show %s\" % (config.core.dotty, desc))",
            "",
            "",
            "def ext_cmd(cmd, shell=True):",
            "    cmd = add_sudo(cmd)",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    common_debug(\"invoke: %s\" % cmd)",
            "    return subprocess.call(cmd, shell=shell)",
            "",
            "",
            "def ext_cmd_nosudo(cmd, shell=True):",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    return subprocess.call(cmd, shell=shell)",
            "",
            "",
            "def rmdir_r(d):",
            "    # TODO: Make sure we're not deleting something we shouldn't!",
            "    if d and os.path.isdir(d):",
            "        shutil.rmtree(d)",
            "",
            "",
            "def nvpairs2dict(pairs):",
            "    '''",
            "    takes a list of string of form ['a=b', 'c=d']",
            "    and returns {'a':'b', 'c':'d'}",
            "    '''",
            "    data = []",
            "    for var in pairs:",
            "        if '=' in var:",
            "            data.append(var.split('=', 1))",
            "        else:",
            "            data.append([var, None])",
            "    return dict(data)",
            "",
            "",
            "def is_check_always():",
            "    '''",
            "    Even though the frequency may be set to always, it doesn't",
            "    make sense to do that with non-interactive sessions.",
            "    '''",
            "    return options.interactive and config.core.check_frequency == \"always\"",
            "",
            "",
            "def get_check_rc():",
            "    '''",
            "    If the check mode is set to strict, then on errors we",
            "    return 2 which is the code for error. Otherwise, we",
            "    pretend that errors are warnings.",
            "    '''",
            "    return 2 if config.core.check_mode == \"strict\" else 1",
            "",
            "",
            "_LOCKDIR = \".lockdir\"",
            "_PIDF = \"pid\"",
            "",
            "",
            "def check_locker(lockdir):",
            "    if not os.path.isdir(os.path.join(lockdir, _LOCKDIR)):",
            "        return",
            "    s = file2str(os.path.join(lockdir, _LOCKDIR, _PIDF))",
            "    pid = convert2ints(s)",
            "    if not isinstance(pid, int):",
            "        common_warn(\"history: removing malformed lock\")",
            "        rmdir_r(os.path.join(lockdir, _LOCKDIR))",
            "        return",
            "    try:",
            "        os.kill(pid, 0)",
            "    except OSError as err:",
            "        if err.errno == os.errno.ESRCH:",
            "            common_info(\"history: removing stale lock\")",
            "            rmdir_r(os.path.join(lockdir, _LOCKDIR))",
            "        else:",
            "            common_err(\"%s: %s\" % (_LOCKDIR, err.strerror))",
            "",
            "",
            "@contextmanager",
            "def lock(lockdir):",
            "    \"\"\"",
            "    Ensure that the lock is released properly",
            "    even in the face of an exception between",
            "    acquire and release.",
            "    \"\"\"",
            "    def acquire_lock():",
            "        check_locker(lockdir)",
            "        while True:",
            "            try:",
            "                os.makedirs(os.path.join(lockdir, _LOCKDIR))",
            "                str2file(\"%d\" % os.getpid(), os.path.join(lockdir, _LOCKDIR, _PIDF))",
            "                return True",
            "            except OSError as err:",
            "                if err.errno != os.errno.EEXIST:",
            "                    common_err(\"Failed to acquire lock to %s: %s\" % (lockdir, err.strerror))",
            "                    return False",
            "                time.sleep(0.1)",
            "                continue",
            "            else:",
            "                return False",
            "",
            "    has_lock = acquire_lock()",
            "    try:",
            "        yield",
            "    finally:",
            "        if has_lock:",
            "            rmdir_r(os.path.join(lockdir, _LOCKDIR))",
            "",
            "",
            "def mkdirp(d, mode=0o777):",
            "    if os.path.isdir(d):",
            "        return True",
            "    os.makedirs(d, mode=mode)",
            "",
            "",
            "def pipe_cmd_nosudo(cmd):",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    proc = subprocess.Popen(cmd,",
            "                            shell=True,",
            "                            stdout=subprocess.PIPE,",
            "                            stderr=subprocess.PIPE)",
            "    (outp, err_outp) = proc.communicate()",
            "    proc.wait()",
            "    rc = proc.returncode",
            "    if rc != 0:",
            "        print(outp)",
            "        print(err_outp)",
            "    return rc",
            "",
            "",
            "def run_cmd_on_remote(cmd, remote_addr, prompt_msg=None):",
            "    \"\"\"",
            "    Run a cmd on remote node",
            "    return (rc, stdout, err_msg)",
            "    \"\"\"",
            "    rc = 1",
            "    out_data = None",
            "    err_data = None",
            "",
            "    need_pw = check_ssh_passwd_need(remote_addr)",
            "    if need_pw and prompt_msg:",
            "        print(prompt_msg)",
            "    try:",
            "        result = parallax.parallax_call([remote_addr], cmd, need_pw)",
            "        rc, out_data, _ = result[0][1]",
            "    except ValueError as err:",
            "        err_match = re.search(\"Exited with error code ([0-9]+), Error output: (.*)\", str(err))",
            "        if err_match:",
            "            rc, err_data = err_match.groups()",
            "    finally:",
            "        return int(rc), to_ascii(out_data), err_data",
            "",
            "",
            "def get_stdout(cmd, input_s=None, stderr_on=True, shell=True, raw=False):",
            "    '''",
            "    Run a cmd, return stdout output.",
            "    Optional input string \"input_s\".",
            "    stderr_on controls whether to show output which comes on stderr.",
            "    '''",
            "    if stderr_on:",
            "        stderr = None",
            "    else:",
            "        stderr = subprocess.PIPE",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    proc = subprocess.Popen(cmd,",
            "                            shell=shell,",
            "                            stdin=subprocess.PIPE,",
            "                            stdout=subprocess.PIPE,",
            "                            stderr=stderr)",
            "    stdout_data, stderr_data = proc.communicate(input_s)",
            "    if raw:",
            "        return proc.returncode, stdout_data",
            "    return proc.returncode, to_ascii(stdout_data).strip()",
            "",
            "",
            "def get_stdout_stderr(cmd, input_s=None, shell=True, raw=False):",
            "    '''",
            "    Run a cmd, return (rc, stdout, stderr)",
            "    '''",
            "    if options.regression_tests:",
            "        print(\".EXT\", cmd)",
            "    proc = subprocess.Popen(cmd,",
            "                            shell=shell,",
            "                            stdin=input_s and subprocess.PIPE or None,",
            "                            stdout=subprocess.PIPE,",
            "                            stderr=subprocess.PIPE)",
            "    stdout_data, stderr_data = proc.communicate(input_s)",
            "    if raw:",
            "        return proc.returncode, stdout_data, stderr_data",
            "    return proc.returncode, to_ascii(stdout_data).strip(), to_ascii(stderr_data).strip()",
            "",
            "",
            "def stdout2list(cmd, stderr_on=True, shell=True):",
            "    '''",
            "    Run a cmd, fetch output, return it as a list of lines.",
            "    stderr_on controls whether to show output which comes on stderr.",
            "    '''",
            "    rc, s = get_stdout(add_sudo(cmd), stderr_on=stderr_on, shell=shell)",
            "    if not s:",
            "        return rc, []",
            "    return rc, s.split('\\n')",
            "",
            "",
            "def append_file(dest, src):",
            "    'Append src to dest'",
            "    try:",
            "        open(dest, \"a\").write(open(src).read())",
            "        return True",
            "    except IOError as msg:",
            "        common_err(\"append %s to %s: %s\" % (src, dest, msg))",
            "        return False",
            "",
            "",
            "def get_dc():",
            "    cmd = \"crmadmin -D\"",
            "    rc, s = get_stdout(add_sudo(cmd))",
            "    if rc != 0:",
            "        return None",
            "    if not s.startswith(\"Designated\"):",
            "        return None",
            "    return s.split()[-1]",
            "",
            "",
            "def wait4dc(what=\"\", show_progress=True):",
            "    '''",
            "    Wait for the DC to get into the S_IDLE state. This should be",
            "    invoked only after a CIB modification which would exercise",
            "    the PE. Parameter \"what\" is whatever the caller wants to be",
            "    printed if showing progress.",
            "",
            "    It is assumed that the DC is already in a different state,",
            "    usually it should be either PENGINE or TRANSITION. This",
            "    assumption may not be true, but there's a high chance that it",
            "    is since crmd should be faster to move through states than",
            "    this shell.",
            "",
            "    Further, it may also be that crmd already calculated the new",
            "    graph, did transition, and went back to the idle state. This",
            "    may in particular be the case if the transition turned out to",
            "    be empty.",
            "",
            "    Tricky. Though in practice it shouldn't be an issue.",
            "",
            "    There's no timeout, as we expect the DC to eventually becomes",
            "    idle.",
            "    '''",
            "    dc = get_dc()",
            "    if not dc:",
            "        common_warn(\"can't find DC\")",
            "        return False",
            "    cmd = \"crm_attribute -Gq -t crm_config -n crmd-transition-delay 2> /dev/null\"",
            "    delay = get_stdout(add_sudo(cmd))[1]",
            "    if delay:",
            "        delaymsec = crm_msec(delay)",
            "        if delaymsec > 0:",
            "            common_info(\"The crmd-transition-delay is configured. Waiting %d msec before check DC status.\" % delaymsec)",
            "            time.sleep(delaymsec // 1000)",
            "    cnt = 0",
            "    output_started = 0",
            "    init_sleep = 0.25",
            "    max_sleep = 1.00",
            "    sleep_time = init_sleep",
            "    while True:",
            "        dc = get_dc()",
            "        if not dc:",
            "            common_warn(\"DC lost during wait\")",
            "            return False",
            "        cmd = \"crmadmin -S %s\" % dc",
            "        rc, s = get_stdout(add_sudo(cmd))",
            "        if not s.startswith(\"Status\"):",
            "            common_warn(\"%s unexpected output: %s (exit code: %d)\" %",
            "                        (cmd, s, rc))",
            "            return False",
            "        try:",
            "            dc_status = s.split()[-2]",
            "        except:",
            "            common_warn(\"%s unexpected output: %s\" % (cmd, s))",
            "            return False",
            "        if dc_status == \"S_IDLE\":",
            "            if output_started:",
            "                sys.stderr.write(\" done\\n\")",
            "            return True",
            "        time.sleep(sleep_time)",
            "        if sleep_time < max_sleep:",
            "            sleep_time *= 2",
            "        if show_progress:",
            "            if not output_started:",
            "                output_started = 1",
            "                sys.stderr.write(\"waiting for %s to finish .\" % what)",
            "            cnt += 1",
            "            if cnt % 5 == 0:",
            "                sys.stderr.write(\".\")",
            "",
            "",
            "def run_ptest(graph_s, nograph, scores, utilization, actions, verbosity):",
            "    '''",
            "    Pipe graph_s thru ptest(8). Show graph using dotty if requested.",
            "    '''",
            "    actions_filter = \"grep LogActions: | grep -vw Leave\"",
            "    ptest = \"2>&1 %s -x -\" % config.core.ptest",
            "    if re.search(\"simulate\", ptest) and \\",
            "            not re.search(\"-[RS]\", ptest):",
            "        ptest = \"%s -S\" % ptest",
            "    if verbosity:",
            "        if actions:",
            "            verbosity = 'v' * max(3, len(verbosity))",
            "        ptest = \"%s -%s\" % (ptest, verbosity.upper())",
            "    if scores:",
            "        ptest = \"%s -s\" % ptest",
            "    if utilization:",
            "        ptest = \"%s -U\" % ptest",
            "    if config.core.dotty and not nograph:",
            "        fd, dotfile = mkstemp()",
            "        ptest = \"%s -D %s\" % (ptest, dotfile)",
            "    else:",
            "        dotfile = None",
            "    # ptest prints to stderr",
            "    if actions:",
            "        ptest = \"%s | %s\" % (ptest, actions_filter)",
            "    if options.regression_tests:",
            "        ptest = \">/dev/null %s\" % ptest",
            "    common_debug(\"invoke: %s\" % ptest)",
            "    rc, s = get_stdout(ptest, input_s=graph_s)",
            "    if rc != 0:",
            "        common_debug(\"'%s' exited with (rc=%d)\" % (ptest, rc))",
            "        if actions and rc == 1:",
            "            common_warn(\"No actions found.\")",
            "        else:",
            "            common_warn(\"Simulation was unsuccessful (RC=%d).\" % (rc))",
            "    if dotfile:",
            "        if os.path.getsize(dotfile) > 0:",
            "            show_dot_graph(dotfile)",
            "        else:",
            "            common_warn(\"ptest produced empty dot file\")",
            "    else:",
            "        if not nograph:",
            "            common_info(\"install graphviz to see a transition graph\")",
            "    if s:",
            "        page_string(s)",
            "    return True",
            "",
            "",
            "def is_id_valid(ident):",
            "    \"\"\"",
            "    Verify that the id follows the definition:",
            "    http://www.w3.org/TR/1999/REC-xml-names-19990114/#ns-qualnames",
            "    \"\"\"",
            "    if not ident:",
            "        return False",
            "    id_re = r\"^[A-Za-z_][\\w._-]*$\"",
            "    return re.match(id_re, ident)",
            "",
            "",
            "def check_range(a):",
            "    \"\"\"",
            "    Verify that the integer range in list a is valid.",
            "    \"\"\"",
            "    if len(a) != 2:",
            "        return False",
            "    if not isinstance(a[0], int) or not isinstance(a[1], int):",
            "        return False",
            "    return int(a[0]) <= int(a[1])",
            "",
            "",
            "def crm_msec(t):",
            "    '''",
            "    See lib/common/utils.c:crm_get_msec().",
            "    '''",
            "    convtab = {",
            "        'ms': (1, 1),",
            "        'msec': (1, 1),",
            "        'us': (1, 1000),",
            "        'usec': (1, 1000),",
            "        '': (1000, 1),",
            "        's': (1000, 1),",
            "        'sec': (1000, 1),",
            "        'm': (60*1000, 1),",
            "        'min': (60*1000, 1),",
            "        'h': (60*60*1000, 1),",
            "        'hr': (60*60*1000, 1),",
            "    }",
            "    if not t:",
            "        return -1",
            "    r = re.match(r\"\\s*(\\d+)\\s*([a-zA-Z]+)?\", t)",
            "    if not r:",
            "        return -1",
            "    if not r.group(2):",
            "        q = ''",
            "    else:",
            "        q = r.group(2).lower()",
            "    try:",
            "        mult, div = convtab[q]",
            "    except KeyError:",
            "        return -1",
            "    return (int(r.group(1))*mult) // div",
            "",
            "",
            "def crm_time_cmp(a, b):",
            "    return crm_msec(a) - crm_msec(b)",
            "",
            "",
            "def shorttime(ts):",
            "    if isinstance(ts, datetime.datetime):",
            "        return ts.strftime(\"%X\")",
            "    if ts is not None:",
            "        return time.strftime(\"%X\", time.localtime(ts))",
            "    return time.strftime(\"%X\", time.localtime(0))",
            "",
            "",
            "def shortdate(ts):",
            "    if isinstance(ts, datetime.datetime):",
            "        return ts.strftime(\"%F\")",
            "    if ts is not None:",
            "        return time.strftime(\"%F\", time.localtime(ts))",
            "    return time.strftime(\"%F\", time.localtime(0))",
            "",
            "",
            "def sort_by_mtime(l):",
            "    'Sort a (small) list of files by time mod.'",
            "    l2 = [(os.stat(x).st_mtime, x) for x in l]",
            "    l2.sort()",
            "    return [x[1] for x in l2]",
            "",
            "",
            "def file_find_by_name(root, filename):",
            "    'Find a file within a tree matching fname'",
            "    assert root",
            "    assert filename",
            "    for root, dirnames, filenames in os.walk(root):",
            "        for filename in fnmatch.filter(filenames, filename):",
            "            return os.path.join(root, filename)",
            "    return None",
            "",
            "",
            "def convert2ints(l):",
            "    \"\"\"",
            "    Convert a list of strings (or a string) to a list of ints.",
            "    All strings must be ints, otherwise conversion fails and None",
            "    is returned!",
            "    \"\"\"",
            "    try:",
            "        if isinstance(l, (tuple, list)):",
            "            return [int(x) for x in l]",
            "        # it's a string then",
            "        return int(l)",
            "    except ValueError:",
            "        return None",
            "",
            "",
            "def is_int(s):",
            "    'Check if the string can be converted to an integer.'",
            "    try:",
            "        int(s)",
            "        return True",
            "    except ValueError:",
            "        return False",
            "",
            "",
            "def is_process(s):",
            "    \"\"\"",
            "    Returns true if argument is the name of a running process.",
            "",
            "    s: process name",
            "    returns Boolean",
            "    \"\"\"",
            "    from os.path import join, basename",
            "    # find pids of running processes",
            "    pids = [pid for pid in os.listdir('/proc') if pid.isdigit()]",
            "    for pid in pids:",
            "        try:",
            "            cmdline = open(join('/proc', pid, 'cmdline'), 'rb').read()",
            "            procname = basename(to_ascii(cmdline).replace('\\x00', ' ').split(' ')[0])",
            "            if procname == s:",
            "                return True",
            "        except EnvironmentError:",
            "            # a process may have died since we got the list of pids",
            "            pass",
            "    return False",
            "",
            "",
            "def print_stacktrace():",
            "    \"\"\"",
            "    Print the stack at the site of call",
            "    \"\"\"",
            "    import traceback",
            "    import inspect",
            "    sf = inspect.currentframe().f_back.f_back",
            "    traceback.print_stack(sf)",
            "",
            "",
            "@memoize",
            "def cluster_stack():",
            "    if is_process(\"heartbeat:.[m]aster\"):",
            "        return \"heartbeat\"",
            "    elif is_process(\"[a]isexec\"):",
            "        return \"openais\"",
            "    elif os.path.exists(\"/etc/corosync/corosync.conf\") or is_program('corosync-cfgtool'):",
            "        return \"corosync\"",
            "    return \"\"",
            "",
            "",
            "def edit_file(fname):",
            "    'Edit a file.'",
            "    if not fname:",
            "        return",
            "    if not config.core.editor:",
            "        return",
            "    return ext_cmd_nosudo(\"%s %s\" % (config.core.editor, fname))",
            "",
            "",
            "def edit_file_ext(fname, template=''):",
            "    '''",
            "    Edit a file via a temporary file.",
            "    Raises IOError on any error.",
            "    '''",
            "    if not os.path.isfile(fname):",
            "        s = template",
            "    else:",
            "        s = open(fname).read()",
            "    filehash = hash(s)",
            "    tmpfile = str2tmp(s)",
            "    try:",
            "        try:",
            "            if edit_file(tmpfile) != 0:",
            "                return",
            "            s = open(tmpfile, 'r').read()",
            "            if hash(s) == filehash:  # file unchanged",
            "                return",
            "            f2 = open(fname, 'w')",
            "            f2.write(s)",
            "            f2.close()",
            "        finally:",
            "            os.unlink(tmpfile)",
            "    except OSError as e:",
            "        raise IOError(e)",
            "",
            "",
            "def need_pager(s, w, h):",
            "    from math import ceil",
            "    cnt = 0",
            "    for l in s.split('\\n'):",
            "        # need to remove color codes",
            "        l = re.sub(r'\\${\\w+}', '', l)",
            "        cnt += int(ceil((len(l) + 0.5) / w))",
            "        if cnt >= h:",
            "            return True",
            "    return False",
            "",
            "",
            "def term_render(s):",
            "    'Render for TERM.'",
            "    try:",
            "        return term.render(s)",
            "    except:",
            "        return s",
            "",
            "",
            "def get_pager_cmd(*extra_opts):",
            "    'returns a commandline which calls the configured pager'",
            "    cmdline = [config.core.pager]",
            "    if os.path.basename(config.core.pager) == \"less\":",
            "        cmdline.append('-R')",
            "    cmdline.extend(extra_opts)",
            "    return ' '.join(cmdline)",
            "",
            "",
            "def page_string(s):",
            "    'Page string rendered for TERM.'",
            "    if not s:",
            "        return",
            "    constants.need_reset = True",
            "    w, h = get_winsize()",
            "    if not need_pager(s, w, h):",
            "        print(term_render(s))",
            "    elif not config.core.pager or not can_ask() or options.batch:",
            "        print(term_render(s))",
            "    else:",
            "        pipe_string(get_pager_cmd(), term_render(s).encode('utf-8'))",
            "    constants.need_reset = False",
            "",
            "",
            "def page_gen(g):",
            "    'Page lines generated by generator g'",
            "    w, h = get_winsize()",
            "    if not config.core.pager or not can_ask() or options.batch:",
            "        for line in g:",
            "            sys.stdout.write(term_render(line))",
            "    else:",
            "        pipe_string(get_pager_cmd(), term_render(\"\".join(g)))",
            "",
            "",
            "def page_file(filename):",
            "    'Open file in pager'",
            "    if not os.path.isfile(filename):",
            "        return",
            "    return ext_cmd_nosudo(get_pager_cmd(filename), shell=True)",
            "",
            "",
            "def get_winsize():",
            "    try:",
            "        import curses",
            "        curses.setupterm()",
            "        w = curses.tigetnum('cols')",
            "        h = curses.tigetnum('lines')",
            "    except:",
            "        try:",
            "            w = os.environ['COLS']",
            "            h = os.environ['LINES']",
            "        except KeyError:",
            "            w = 80",
            "            h = 25",
            "    return w, h",
            "",
            "",
            "def multicolumn(l):",
            "    '''",
            "    A ls-like representation of a list of strings.",
            "    A naive approach.",
            "    '''",
            "    min_gap = 2",
            "    w, _ = get_winsize()",
            "    max_len = 8",
            "    for s in l:",
            "        if len(s) > max_len:",
            "            max_len = len(s)",
            "    cols = w // (max_len + min_gap)  # approx.",
            "    if not cols:",
            "        cols = 1",
            "    col_len = w // cols",
            "    for i in range(len(l) // cols + 1):",
            "        s = ''",
            "        for j in range(i * cols, (i + 1) * cols):",
            "            if not j < len(l):",
            "                break",
            "            if not s:",
            "                s = \"%-*s\" % (col_len, l[j])",
            "            elif (j + 1) % cols == 0:",
            "                s = \"%s%s\" % (s, l[j])",
            "            else:",
            "                s = \"%s%-*s\" % (s, col_len, l[j])",
            "        if s:",
            "            print(s)",
            "",
            "",
            "def find_value(pl, name):",
            "    for n, v in pl:",
            "        if n == name:",
            "            return v",
            "    return None",
            "",
            "",
            "def cli_replace_attr(pl, name, new_val):",
            "    for i, attr in enumerate(pl):",
            "        if attr[0] == name:",
            "            attr[1] = new_val",
            "            return",
            "",
            "",
            "def cli_append_attr(pl, name, val):",
            "    pl.append([name, val])",
            "",
            "",
            "def lines2cli(s):",
            "    '''",
            "    Convert a string into a list of lines. Replace continuation",
            "    characters. Strip white space, left and right. Drop empty lines.",
            "    '''",
            "    cl = []",
            "    l = s.split('\\n')",
            "    cum = []",
            "    for p in l:",
            "        p = p.strip()",
            "        if p.endswith('\\\\'):",
            "            p = p.rstrip('\\\\')",
            "            cum.append(p)",
            "        else:",
            "            cum.append(p)",
            "            cl.append(''.join(cum).strip())",
            "            cum = []",
            "    if cum:  # in case s ends with backslash",
            "        cl.append(''.join(cum))",
            "    return [x for x in cl if x]",
            "",
            "",
            "def datetime_is_aware(dt):",
            "    \"\"\"",
            "    Determines if a given datetime.datetime is aware.",
            "",
            "    The logic is described in Python's docs:",
            "    http://docs.python.org/library/datetime.html#datetime.tzinfo",
            "    \"\"\"",
            "    return dt and dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None",
            "",
            "",
            "def make_datetime_naive(dt):",
            "    \"\"\"",
            "    Ensures that the datetime is not time zone-aware:",
            "",
            "    The returned datetime object is a naive time in UTC.",
            "    \"\"\"",
            "    if dt and datetime_is_aware(dt):",
            "        return dt.replace(tzinfo=None) - dt.utcoffset()",
            "    return dt",
            "",
            "",
            "def total_seconds(td):",
            "    \"\"\"",
            "    Backwards compatible implementation of timedelta.total_seconds()",
            "    \"\"\"",
            "    if hasattr(datetime.timedelta, 'total_seconds'):",
            "        return td.total_seconds()",
            "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) // 10**6",
            "",
            "",
            "def datetime_to_timestamp(dt):",
            "    \"\"\"",
            "    Convert a datetime object into a floating-point second value",
            "    \"\"\"",
            "    try:",
            "        return total_seconds(make_datetime_naive(dt) - datetime.datetime(1970, 1, 1))",
            "    except Exception as e:",
            "        common_err(\"datetime_to_timestamp error: %s\" % (e))",
            "        return None",
            "",
            "",
            "def timestamp_to_datetime(ts):",
            "    \"\"\"",
            "    Convert a timestamp into a naive datetime object",
            "    \"\"\"",
            "    import dateutil",
            "    import dateutil.tz",
            "    return make_datetime_naive(datetime.datetime.fromtimestamp(ts).replace(tzinfo=dateutil.tz.tzlocal()))",
            "",
            "",
            "def parse_time(t):",
            "    '''",
            "    Try to make sense of the user provided time spec.",
            "    Use dateutil if available, otherwise strptime.",
            "    Return the datetime value.",
            "",
            "    Also does time zone elimination by passing the datetime",
            "    through a timestamp conversion if necessary",
            "",
            "    TODO: dateutil is very slow, avoid it if possible",
            "    '''",
            "    try:",
            "        from dateutil import parser, tz",
            "        dt = parser.parse(t)",
            "",
            "        if datetime_is_aware(dt):",
            "            ts = datetime_to_timestamp(dt)",
            "            if ts is None:",
            "                return None",
            "            dt = datetime.datetime.fromtimestamp(ts)",
            "        else:",
            "            # convert to UTC from local time",
            "            dt = dt - tz.tzlocal().utcoffset(dt)",
            "    except ValueError as msg:",
            "        common_err(\"parse_time %s: %s\" % (t, msg))",
            "        return None",
            "    except ImportError as msg:",
            "        try:",
            "            tm = time.strptime(t)",
            "            dt = datetime.datetime(*tm[0:7])",
            "        except ValueError as msg:",
            "            common_err(\"no dateutil, please provide times as printed by date(1)\")",
            "            return None",
            "    return dt",
            "",
            "",
            "def parse_to_timestamp(t):",
            "    '''",
            "    Read a string and convert it into a UNIX timestamp.",
            "    Added as an optimization of parse_time to avoid",
            "    extra conversion steps when result would be converted",
            "    into a timestamp anyway",
            "    '''",
            "    try:",
            "        from dateutil import parser, tz",
            "        dt = parser.parse(t)",
            "",
            "        if datetime_is_aware(dt):",
            "            return datetime_to_timestamp(dt)",
            "        # convert to UTC from local time",
            "        return total_seconds(dt - tz.tzlocal().utcoffset(dt) - datetime.datetime(1970, 1, 1))",
            "    except ValueError as msg:",
            "        common_err(\"parse_time %s: %s\" % (t, msg))",
            "        return None",
            "    except ImportError as msg:",
            "        try:",
            "            tm = time.strptime(t)",
            "            dt = datetime.datetime(*tm[0:7])",
            "            return datetime_to_timestamp(dt)",
            "        except ValueError as msg:",
            "            common_err(\"no dateutil, please provide times as printed by date(1)\")",
            "            return None",
            "",
            "",
            "def save_graphviz_file(ini_f, attr_d):",
            "    '''",
            "    Save graphviz settings to an ini file, if it does not exist.",
            "    '''",
            "    if os.path.isfile(ini_f):",
            "        common_err(\"%s exists, please remove it first\" % ini_f)",
            "        return False",
            "    try:",
            "        f = open(ini_f, \"wb\")",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return False",
            "    import configparser",
            "    p = configparser.ConfigParser()",
            "    for section, sect_d in attr_d.items():",
            "        p.add_section(section)",
            "        for n, v in sect_d.items():",
            "            p.set(section, n, v)",
            "    try:",
            "        p.write(f)",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return False",
            "    f.close()",
            "    common_info(\"graphviz attributes saved to %s\" % ini_f)",
            "    return True",
            "",
            "",
            "def load_graphviz_file(ini_f):",
            "    '''",
            "    Load graphviz ini file, if it exists.",
            "    '''",
            "    if not os.path.isfile(ini_f):",
            "        return True, None",
            "    import configparser",
            "    p = configparser.ConfigParser()",
            "    try:",
            "        p.read(ini_f)",
            "    except Exception as msg:",
            "        common_err(msg)",
            "        return False, None",
            "    _graph_d = {}",
            "    for section in p.sections():",
            "        d = {}",
            "        for n, v in p.items(section):",
            "            d[n] = v",
            "        _graph_d[section] = d",
            "    return True, _graph_d",
            "",
            "",
            "def get_pcmk_version(dflt):",
            "    version = dflt",
            "",
            "    crmd = pacemaker_controld()",
            "    if crmd:",
            "        cmd = crmd",
            "    else:",
            "        return version",
            "",
            "    try:",
            "        rc, s, err = get_stdout_stderr(\"%s version\" % (cmd))",
            "        if rc != 0:",
            "            common_err(\"%s exited with %d [err: %s][out: %s]\" % (cmd, rc, err, s))",
            "        else:",
            "            common_debug(\"pacemaker version: [err: %s][out: %s]\" % (err, s))",
            "            if err.startswith(\"CRM Version:\"):",
            "                version = s.split()[0]",
            "            else:",
            "                version = s.split()[2]",
            "            common_debug(\"found pacemaker version: %s\" % version)",
            "    except Exception as msg:",
            "        common_warn(\"could not get the pacemaker version, bad installation?\")",
            "        common_warn(msg)",
            "    return version",
            "",
            "",
            "def get_cib_property(cib_f, attr, dflt):",
            "    \"\"\"A poor man's get attribute procedure.",
            "    We don't want heavy parsing, this needs to be relatively",
            "    fast.",
            "    \"\"\"",
            "    open_t = \"<cluster_property_set\"",
            "    close_t = \"</cluster_property_set\"",
            "    attr_s = 'name=\"%s\"' % attr",
            "    ver_patt = re.compile('value=\"([^\"]+)\"')",
            "    ver = dflt  # return some version in any case",
            "    try:",
            "        f = open(cib_f, \"r\")",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return ver",
            "    state = 0",
            "    for s in f:",
            "        if state == 0:",
            "            if open_t in s:",
            "                state += 1",
            "        elif state == 1:",
            "            if close_t in s:",
            "                break",
            "            if attr_s in s:",
            "                r = ver_patt.search(s)",
            "                if r:",
            "                    ver = r.group(1)",
            "                break",
            "    f.close()",
            "    return ver",
            "",
            "",
            "def get_cib_attributes(cib_f, tag, attr_l, dflt_l):",
            "    \"\"\"A poor man's get attribute procedure.",
            "    We don't want heavy parsing, this needs to be relatively",
            "    fast.",
            "    \"\"\"",
            "    open_t = \"<%s \" % tag",
            "    val_patt_l = [re.compile('%s=\"([^\"]+)\"' % x) for x in attr_l]",
            "    val_l = []",
            "    try:",
            "        f = open(cib_f, \"rb\").read()",
            "    except IOError as msg:",
            "        common_err(msg)",
            "        return dflt_l",
            "    if os.path.splitext(cib_f)[-1] == '.bz2':",
            "        cib_bits = bz2.decompress(f)",
            "    else:",
            "        cib_bits = f",
            "    cib_s = to_ascii(cib_bits)",
            "    for s in cib_s.split('\\n'):",
            "        if s.startswith(open_t):",
            "            i = 0",
            "            for patt in val_patt_l:",
            "                r = patt.search(s)",
            "                val_l.append(r and r.group(1) or dflt_l[i])",
            "                i += 1",
            "            break",
            "    return val_l",
            "",
            "",
            "def is_min_pcmk_ver(min_ver, cib_f=None):",
            "    if not constants.pcmk_version:",
            "        if cib_f:",
            "            constants.pcmk_version = get_cib_property(cib_f, \"dc-version\", \"1.1.11\")",
            "            common_debug(\"found pacemaker version: %s in cib: %s\" %",
            "                         (constants.pcmk_version, cib_f))",
            "        else:",
            "            constants.pcmk_version = get_pcmk_version(\"1.1.11\")",
            "    from distutils.version import LooseVersion",
            "    return LooseVersion(constants.pcmk_version) >= LooseVersion(min_ver)",
            "",
            "",
            "def is_pcmk_118(cib_f=None):",
            "    return is_min_pcmk_ver(\"1.1.8\", cib_f=cib_f)",
            "",
            "",
            "@memoize",
            "def cibadmin_features():",
            "    '''",
            "    # usage example:",
            "    if 'corosync-plugin' in cibadmin_features()",
            "    '''",
            "    rc, outp = get_stdout(['cibadmin', '-!'], shell=False)",
            "    if rc == 0:",
            "        m = re.match(r'Pacemaker\\s(\\S+)\\s\\(Build: ([^\\)]+)\\):\\s(.*)', outp.strip())",
            "        if m and len(m.groups()) > 2:",
            "            return m.group(3).split()",
            "    return []",
            "",
            "",
            "@memoize",
            "def cibadmin_can_patch():",
            "    # cibadmin -P doesn't handle comments in <1.1.11 (unless patched)",
            "    return is_min_pcmk_ver(\"1.1.11\")",
            "",
            "",
            "# quote function from python module shlex.py in python 3.3",
            "",
            "_find_unsafe = re.compile(r'[^\\w@%+=:,./-]').search",
            "",
            "",
            "def quote(s):",
            "    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"",
            "    if not s:",
            "        return \"''\"",
            "    if _find_unsafe(s) is None:",
            "        return s",
            "",
            "    # use single quotes, and put single quotes into double quotes",
            "    # the string $'b is then quoted as '$'\"'\"'b'",
            "    return \"'\" + s.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"",
            "",
            "",
            "def doublequote(s):",
            "    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"",
            "    if not s:",
            "        return '\"\"'",
            "    if _find_unsafe(s) is None:",
            "        return s",
            "",
            "    # use double quotes",
            "    return '\"' + s.replace('\"', \"\\\\\\\"\") + '\"'",
            "",
            "",
            "def fetch_opts(args, opt_l):",
            "    '''",
            "    Get and remove option keywords from args.",
            "    They are always listed last, at the end of the line.",
            "    Return a list of options found. The caller can do",
            "    if keyw in optlist: ...",
            "    '''",
            "    re_opt = None",
            "    if opt_l[0].startswith(\"@\"):",
            "        re_opt = re.compile(\"^%s$\" % opt_l[0][1:])",
            "        del opt_l[0]",
            "    l = []",
            "    for i in reversed(list(range(len(args)))):",
            "        if (args[i] in opt_l) or (re_opt and re_opt.search(args[i])):",
            "            l.append(args.pop())",
            "        else:",
            "            break",
            "    return l",
            "",
            "",
            "_LIFETIME = [\"reboot\", \"forever\"]",
            "_ISO8601_RE = re.compile(\"(PT?[0-9]|[0-9]+.*[:-])\")",
            "",
            "",
            "def fetch_lifetime_opt(args, iso8601=True):",
            "    '''",
            "    Get and remove a lifetime option from args. It can be one of",
            "    lifetime_options or an ISO 8601 formatted period/time. There",
            "    is apparently no good support in python for this format, so",
            "    we cheat a bit.",
            "    '''",
            "    if args:",
            "        opt = args[-1]",
            "        if opt in _LIFETIME or (iso8601 and _ISO8601_RE.match(opt)):",
            "            return args.pop()",
            "    return None",
            "",
            "",
            "def resolve_hostnames(hostnames):",
            "    '''",
            "    Tries to resolve the given list of hostnames.",
            "    returns (ok, failed-hostname)",
            "    ok: True if all hostnames resolved",
            "    failed-hostname: First failed hostname resolution",
            "    '''",
            "    import socket",
            "    for node in hostnames:",
            "        try:",
            "            socket.gethostbyname(node)",
            "        except socket.error:",
            "            return False, node",
            "    return True, None",
            "",
            "",
            "def list_corosync_node_names():",
            "    '''",
            "    Returns list of nodes configured",
            "    in corosync.conf",
            "    '''",
            "    try:",
            "        cfg = os.getenv('COROSYNC_MAIN_CONFIG_FILE', '/etc/corosync/corosync.conf')",
            "        lines = open(cfg).read().split('\\n')",
            "        name_re = re.compile(r'\\s*name:\\s+(.*)')",
            "        names = []",
            "        for line in lines:",
            "            name = name_re.match(line)",
            "            if name:",
            "                names.append(name.group(1))",
            "        return names",
            "    except Exception:",
            "        return []",
            "",
            "",
            "def list_corosync_nodes():",
            "    '''",
            "    Returns list of nodes configured",
            "    in corosync.conf",
            "    '''",
            "    try:",
            "        cfg = os.getenv('COROSYNC_MAIN_CONFIG_FILE', '/etc/corosync/corosync.conf')",
            "        lines = open(cfg).read().split('\\n')",
            "        addr_re = re.compile(r'\\s*ring0_addr:\\s+(.*)')",
            "        nodes = []",
            "        for line in lines:",
            "            addr = addr_re.match(line)",
            "            if addr:",
            "                nodes.append(addr.group(1))",
            "        return nodes",
            "    except Exception:",
            "        return []",
            "",
            "",
            "def print_cluster_nodes():",
            "    \"\"\"",
            "    Print the output of crm_node -l",
            "    \"\"\"",
            "    rc, out, _ = get_stdout_stderr(\"crm_node -l\")",
            "    if rc == 0 and out:",
            "        print(\"{}\\n\".format(out))",
            "",
            "",
            "def list_cluster_nodes():",
            "    '''",
            "    Returns a list of nodes in the cluster.",
            "    '''",
            "    def getname(toks):",
            "        if toks and len(toks) >= 2:",
            "            return toks[1]",
            "        return None",
            "",
            "    try:",
            "        # when pacemaker running",
            "        rc, outp = stdout2list(['crm_node', '-l'], stderr_on=False, shell=False)",
            "        if rc == 0:",
            "            return [x for x in [getname(line.split()) for line in outp] if x and x != '(null)']",
            "",
            "        # when corosync running",
            "        ip_list = get_member_iplist()",
            "        if ip_list:",
            "            return ip_list",
            "",
            "        # static situation",
            "        cib_path = os.getenv('CIB_file', '/var/lib/pacemaker/cib/cib.xml')",
            "        if not os.path.isfile(cib_path):",
            "            return None",
            "        from . import xmlutil",
            "        node_list = []",
            "        cib = xmlutil.file2cib_elem(cib_path)",
            "        if cib is None:",
            "            return None",
            "        for node in cib.xpath('/cib/configuration/nodes/node'):",
            "            name = node.get('uname') or node.get('id')",
            "            if node.get('type') == 'remote':",
            "                srv = cib.xpath(\"//primitive[@id='%s']/instance_attributes/nvpair[@name='server']\" % (name))",
            "                if srv:",
            "                    continue",
            "            node_list.append(name)",
            "        return node_list",
            "    except OSError as msg:",
            "        raise ValueError(\"Error listing cluster nodes: %s\" % (msg))",
            "",
            "",
            "def cluster_run_cmd(cmd):",
            "    \"\"\"",
            "    Run cmd in cluster nodes",
            "    \"\"\"",
            "    node_list = list_cluster_nodes()",
            "    if not node_list:",
            "        raise ValueError(\"Failed to get node list from cluster\")",
            "    parallax.parallax_call(node_list, cmd)",
            "",
            "",
            "def list_cluster_nodes_except_me():",
            "    \"\"\"",
            "    Get cluster node list and filter out self",
            "    \"\"\"",
            "    node_list = list_cluster_nodes()",
            "    if not node_list:",
            "        raise ValueError(\"Failed to get node list from cluster\")",
            "    me = this_node()",
            "    if me in node_list:",
            "        node_list.remove(me)",
            "    return node_list",
            "",
            "",
            "def service_info(name):",
            "    p = is_program('systemctl')",
            "    if p:",
            "        rc, outp = get_stdout([p, 'show',",
            "                               '-p', 'UnitFileState',",
            "                               '-p', 'ActiveState',",
            "                               '-p', 'SubState',",
            "                               name + '.service'], shell=False)",
            "        if rc == 0:",
            "            info = []",
            "            for line in outp.split('\\n'):",
            "                data = line.split('=', 1)",
            "                if len(data) == 2:",
            "                    info.append(data[1].strip())",
            "            return '/'.join(info)",
            "    return None",
            "",
            "",
            "def running_on(resource):",
            "    \"returns list of node names where the given resource is running\"",
            "    rsc_locate = \"crm_resource --resource '%s' --locate\"",
            "    rc, out, err = get_stdout_stderr(rsc_locate % (resource))",
            "    if rc != 0:",
            "        return []",
            "    nodes = []",
            "    head = \"resource %s is running on: \" % (resource)",
            "    for line in out.split('\\n'):",
            "        if line.strip().startswith(head):",
            "            w = line[len(head):].split()",
            "            if w:",
            "                nodes.append(w[0])",
            "    common_debug(\"%s running on: %s\" % (resource, nodes))",
            "    return nodes",
            "",
            "",
            "# This RE matches nvpair values that can",
            "# be left unquoted",
            "_NOQUOTES_RE = re.compile(r'^[\\w\\.-]+$')",
            "",
            "",
            "def noquotes(v):",
            "    return _NOQUOTES_RE.match(v) is not None",
            "",
            "",
            "def unquote(s):",
            "    \"\"\"",
            "    Reverse shell-quoting a string, so the string '\"a b c\"'",
            "    becomes 'a b c'",
            "    \"\"\"",
            "    sp = shlex.split(s)",
            "    if sp:",
            "        return sp[0]",
            "    return \"\"",
            "",
            "",
            "def parse_sysconfig(sysconfig_file):",
            "    \"\"\"",
            "    Reads a sysconfig file into a dict",
            "    \"\"\"",
            "    ret = {}",
            "    if os.path.isfile(sysconfig_file):",
            "        for line in open(sysconfig_file).readlines():",
            "            if line.lstrip().startswith('#'):",
            "                continue",
            "            try:",
            "                key, val = line.split(\"=\", 1)",
            "                ret[key] = unquote(val)",
            "            except ValueError:",
            "                pass",
            "    return ret",
            "",
            "",
            "def sysconfig_set(sysconfig_file, **values):",
            "    \"\"\"",
            "    Set the values in the sysconfig file, updating the variables",
            "    if they exist already, appending them if not.",
            "    \"\"\"",
            "    outp = \"\"",
            "    if os.path.isfile(sysconfig_file):",
            "        for line in open(sysconfig_file).readlines():",
            "            if line.lstrip().startswith('#'):",
            "                outp += line",
            "            else:",
            "                matched = False",
            "                try:",
            "                    key, _ = line.split(\"=\", 1)",
            "                    for k, v in values.items():",
            "                        if k == key:",
            "                            matched = True",
            "                            outp += '%s=%s\\n' % (k, doublequote(v))",
            "                            del values[k]",
            "                            break",
            "                    if not matched:",
            "                        outp += line",
            "                except ValueError:",
            "                    outp += line",
            "",
            "    for k, v in values.items():",
            "        outp += '%s=%s\\n' % (k, doublequote(v))",
            "    str2file(outp, sysconfig_file)",
            "",
            "",
            "def remote_diff_slurp(nodes, filename):",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"Parallax is required to diff\")",
            "    from . import tmpfiles",
            "",
            "    tmpdir = tmpfiles.create_dir()",
            "    opts = parallax.Options()",
            "    opts.localdir = tmpdir",
            "    dst = os.path.basename(filename)",
            "    return list(parallax.slurp(nodes, filename, dst, opts).items())",
            "",
            "",
            "def remote_diff_this(local_path, nodes, this_node):",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"Parallax is required to diff\")",
            "",
            "    by_host = remote_diff_slurp(nodes, local_path)",
            "    for host, result in by_host:",
            "        if isinstance(result, parallax.Error):",
            "            raise ValueError(\"Failed on %s: %s\" % (host, str(result)))",
            "        _, _, _, path = result",
            "        _, s = get_stdout(\"diff -U 0 -d -b --label %s --label %s %s %s\" %",
            "                          (host, this_node, path, local_path))",
            "        page_string(s)",
            "",
            "",
            "def remote_diff(local_path, nodes):",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"parallax is required to diff\")",
            "",
            "    by_host = remote_diff_slurp(nodes, local_path)",
            "    for host, result in by_host:",
            "        if isinstance(result, parallax.Error):",
            "            raise ValueError(\"Failed on %s: %s\" % (host, str(result)))",
            "    h1, r1 = by_host[0]",
            "    h2, r2 = by_host[1]",
            "    _, s = get_stdout(\"diff -U 0 -d -b --label %s --label %s %s %s\" %",
            "                      (h1, h2, r1[3], r2[3]))",
            "    page_string(s)",
            "",
            "",
            "def remote_checksum(local_path, nodes, this_node):",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"Parallax is required to diff\")",
            "    import hashlib",
            "",
            "    by_host = remote_diff_slurp(nodes, local_path)",
            "    for host, result in by_host:",
            "        if isinstance(result, parallax.Error):",
            "            raise ValueError(str(result))",
            "",
            "    print(\"%-16s  SHA1 checksum of %s\" % ('Host', local_path))",
            "    if this_node not in nodes:",
            "        print(\"%-16s: %s\" % (this_node, hashlib.sha1(open(local_path).read()).hexdigest()))",
            "    for host, result in by_host:",
            "        _, _, _, path = result",
            "        print(\"%-16s: %s\" % (host, hashlib.sha1(open(path).read()).hexdigest()))",
            "",
            "",
            "def cluster_copy_file(local_path, nodes=None):",
            "    \"\"\"",
            "    Copies given file to all other cluster nodes.",
            "    \"\"\"",
            "    try:",
            "        import parallax",
            "    except ImportError:",
            "        raise ValueError(\"parallax is required to copy cluster files\")",
            "    if not nodes:",
            "        nodes = list_cluster_nodes()",
            "        nodes.remove(this_node())",
            "    opts = parallax.Options()",
            "    opts.timeout = 60",
            "    opts.ssh_options += ['ControlPersist=no']",
            "    ok = True",
            "    for host, result in parallax.copy(nodes,",
            "                                      local_path,",
            "                                      local_path, opts).items():",
            "        if isinstance(result, parallax.Error):",
            "            err_buf.error(\"Failed to push %s to %s: %s\" % (local_path, host, result))",
            "            ok = False",
            "        else:",
            "            err_buf.ok(host)",
            "    return ok",
            "",
            "",
            "# a set of fnmatch patterns to match attributes whose values",
            "# should be obscured as a sequence of **** when printed",
            "_obscured_nvpairs = []",
            "",
            "",
            "def obscured(key, value):",
            "    if key is not None and value is not None:",
            "        for o in _obscured_nvpairs:",
            "            if fnmatch.fnmatch(key, o):",
            "                return '*' * 6",
            "    return value",
            "",
            "",
            "@contextmanager",
            "def obscure(obscure_list):",
            "    global _obscured_nvpairs",
            "    prev = _obscured_nvpairs",
            "    _obscured_nvpairs = obscure_list",
            "    try:",
            "        yield",
            "    finally:",
            "        _obscured_nvpairs = prev",
            "",
            "",
            "def gen_nodeid_from_ipv6(addr):",
            "    return int(ipaddress.ip_address(addr)) % 1000000000",
            "",
            "",
            "# Set by detect_cloud()",
            "# to avoid multiple requests",
            "_ip_for_cloud = None",
            "",
            "",
            "def _cloud_metadata_request(uri, headers={}):",
            "    try:",
            "        import urllib2 as urllib",
            "    except ImportError:",
            "        import urllib.request as urllib",
            "    req = urllib.Request(uri)",
            "    for header, value in headers.items():",
            "        req.add_header(header, value)",
            "    try:",
            "        resp = urllib.urlopen(req, timeout=5)",
            "        content = resp.read()",
            "        if type(content) != str:",
            "            return content.decode('utf-8').strip()",
            "        return content.strip()",
            "    except urllib.URLError:",
            "        return None",
            "",
            "",
            "@memoize",
            "def detect_cloud():",
            "    \"\"\"",
            "    Tries to determine which (if any) cloud environment",
            "    the cluster is running on.",
            "",
            "    This is mainly done using dmidecode.",
            "",
            "    If the host cannot be determined, this function",
            "    returns None. Otherwise, it returns a string",
            "    identifying the platform.",
            "",
            "    These are the currently known platforms:",
            "",
            "    * amazon-web-services",
            "    * microsoft-azure",
            "    * google-cloud-platform",
            "",
            "    \"\"\"",
            "    global _ip_for_cloud",
            "",
            "    if not is_program(\"dmidecode\"):",
            "        return None",
            "    rc, system_version = get_stdout(\"dmidecode -s system-version\")",
            "    if re.search(r\".*amazon.*\", system_version) is not None:",
            "        return \"amazon-web-services\"",
            "    if rc != 0:",
            "        return None",
            "    rc, system_manufacturer = get_stdout(\"dmidecode -s system-manufacturer\")",
            "    if rc == 0 and \"microsoft corporation\" in system_manufacturer.lower():",
            "        # To detect azure we also need to make an API request",
            "        result = _cloud_metadata_request(",
            "            \"http://169.254.169.254/metadata/instance/network/interface/0/ipv4/ipAddress/0/privateIpAddress?api-version=2017-08-01&format=text\",",
            "            headers={\"Metadata\": \"true\"})",
            "        if result:",
            "            _ip_for_cloud = result",
            "            return \"microsoft-azure\"",
            "    rc, bios_vendor = get_stdout(\"dmidecode -s bios-vendor\")",
            "    if rc == 0 and \"Google\" in bios_vendor:",
            "        # To detect GCP we also need to make an API request",
            "        result = _cloud_metadata_request(",
            "            \"http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip\",",
            "            headers={\"Metadata-Flavor\": \"Google\"})",
            "        if result:",
            "            _ip_for_cloud = result",
            "            return \"google-cloud-platform\"",
            "    return None",
            "",
            "",
            "def debug_timestamp():",
            "    return datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')",
            "",
            "",
            "def get_member_iplist():",
            "    rc, out, err= get_stdout_stderr(\"corosync-cmapctl -b runtime.totem.pg.mrp.srp.members\")",
            "    if rc != 0:",
            "        common_debug(err)",
            "        return None",
            "",
            "    ip_list = []",
            "    for line in out.split('\\n'):",
            "        match = re.search(r'ip\\((.*?)\\)', line)",
            "        if match:",
            "            ip_list.append(match.group(1))",
            "    return ip_list",
            "",
            "",
            "def get_iplist_corosync_using():",
            "    \"\"\"",
            "    Get ip list used by corosync",
            "    \"\"\"",
            "    rc, out, err = get_stdout_stderr(\"corosync-cfgtool -s\")",
            "    if rc != 0:",
            "        raise ValueError(err)",
            "    return re.findall(r'id\\s*=\\s*(.*)', out)",
            "",
            "",
            "def check_ssh_passwd_need(host, user=\"root\"):",
            "    \"\"\"",
            "    Check whether access to host need password",
            "    \"\"\"",
            "    ssh_options = \"-o StrictHostKeyChecking=no -o EscapeChar=none -o ConnectTimeout=15\"",
            "    ssh_cmd = \"ssh {} -T -o Batchmode=yes {} true\".format(ssh_options, host)",
            "    ssh_cmd = add_su(ssh_cmd, user)",
            "    rc, _, _ = get_stdout_stderr(ssh_cmd)",
            "    return rc != 0",
            "",
            "",
            "def check_port_open(ip, port):",
            "    import socket",
            "",
            "    family = socket.AF_INET6 if IP.is_ipv6(ip) else socket.AF_INET",
            "    with closing(socket.socket(family, socket.SOCK_STREAM)) as sock:",
            "        if sock.connect_ex((ip, port)) == 0:",
            "            return True",
            "        else:",
            "            return False",
            "",
            "",
            "def valid_port(port):",
            "    return int(port) >= 1024 and int(port) <= 65535",
            "",
            "",
            "def is_qdevice_configured():",
            "    from . import corosync",
            "    return corosync.get_value(\"quorum.device.model\") == \"net\"",
            "",
            "",
            "def is_qdevice_tls_on():",
            "    from . import corosync",
            "    return corosync.get_value(\"quorum.device.net.tls\") == \"on\"",
            "",
            "",
            "def get_nodeinfo_from_cmaptool():",
            "    nodeid_ip_dict = {}",
            "    rc, out = get_stdout(\"corosync-cmapctl -b runtime.totem.pg.mrp.srp.members\")",
            "    if rc != 0:",
            "        return nodeid_ip_dict",
            "",
            "    for line in out.split('\\n'):",
            "        match = re.search(r'members\\.(.*)\\.ip', line)",
            "        if match:",
            "            node_id = match.group(1)",
            "            iplist = re.findall(r'[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}', line)",
            "            nodeid_ip_dict[node_id] = iplist",
            "    return nodeid_ip_dict",
            "",
            "",
            "def get_iplist_from_name(name):",
            "    \"\"\"",
            "    Given node host name, return this host's ip list in corosync cmap",
            "    \"\"\"",
            "    ip_list = []",
            "    nodeid = get_nodeid_from_name(name)",
            "    if not nodeid:",
            "        return ip_list",
            "    nodeinfo = {}",
            "    nodeinfo = get_nodeinfo_from_cmaptool()",
            "    if not nodeinfo:",
            "        return ip_list",
            "    return nodeinfo[nodeid]",
            "",
            "",
            "def valid_nodeid(nodeid):",
            "    from . import bootstrap",
            "    if not service_is_active('corosync.service'):",
            "        return False",
            "",
            "    for _id, _ in get_nodeinfo_from_cmaptool().items():",
            "        if _id == nodeid:",
            "            return True",
            "    return False",
            "",
            "",
            "def get_nodeid_from_name(name):",
            "    rc, out = get_stdout('crm_node -l')",
            "    if rc != 0:",
            "        return None",
            "    res = re.search(r'^([0-9]+) {} '.format(name), out, re.M)",
            "    if res:",
            "        return res.group(1)",
            "    else:",
            "        return None",
            "",
            "",
            "def check_space_option_value(options):",
            "    if not isinstance(options, argparse.Namespace):",
            "        raise ValueError(\"Expected type of \\\"options\\\" is \\\"argparse.Namespace\\\", not \\\"{}\\\"\".format(type(options)))",
            "",
            "    for opt in vars(options):",
            "        value = getattr(options, opt)",
            "        if isinstance(value, str) and len(value.strip()) == 0:",
            "            raise ValueError(\"Space value not allowed for dest \\\"{}\\\"\".format(opt))",
            "",
            "",
            "def interface_choice():",
            "    _, out = get_stdout(\"ip a\")",
            "    # should consider interface format like \"ethx@xxx\"",
            "    interface_list = re.findall(r'(?:[0-9]+:) (.*?)(?=: |@.*?: )', out)",
            "    return [nic for nic in interface_list if nic != \"lo\"]",
            "",
            "",
            "class IP(object):",
            "    \"\"\"",
            "    Class to get some properties of IP address",
            "    \"\"\"",
            "",
            "    def __init__(self, addr):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self.addr = addr",
            "",
            "    @property",
            "    def ip_address(self):",
            "        \"\"\"",
            "        Create ipaddress instance",
            "        \"\"\"",
            "        return ipaddress.ip_address(self.addr)",
            "",
            "    @property",
            "    def version(self):",
            "        \"\"\"",
            "        Get IP address version",
            "        \"\"\"",
            "        return self.ip_address.version",
            "",
            "    @classmethod",
            "    def is_mcast(cls, addr):",
            "        \"\"\"",
            "        Check whether the address is multicast address",
            "        \"\"\"",
            "        cls_inst = cls(addr)",
            "        return cls_inst.ip_address.is_multicast",
            "",
            "    @classmethod",
            "    def is_ipv6(cls, addr):",
            "        \"\"\"",
            "        Check whether the address is IPV6 address",
            "        \"\"\"",
            "        return cls(addr).version == 6",
            "",
            "    @classmethod",
            "    def is_valid_ip(cls, addr):",
            "        \"\"\"",
            "        Check whether the address is valid IP address",
            "        \"\"\"",
            "        cls_inst = cls(addr)",
            "        try:",
            "            cls_inst.ip_address",
            "        except ValueError:",
            "            return False",
            "        else:",
            "            return True",
            "",
            "    @property",
            "    def is_loopback(self):",
            "        \"\"\"",
            "        Check whether the address is loopback address",
            "        \"\"\"",
            "        return self.ip_address.is_loopback",
            "",
            "    @property",
            "    def is_link_local(self):",
            "        \"\"\"",
            "        Check whether the address is link-local address",
            "        \"\"\"",
            "        return self.ip_address.is_link_local",
            "",
            "",
            "class Interface(IP):",
            "    \"\"\"",
            "    Class to get information from one interface",
            "    \"\"\"",
            "",
            "    def __init__(self, ip_with_mask):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self.ip, self.mask = ip_with_mask.split('/')",
            "        super(__class__, self).__init__(self.ip)",
            "",
            "    @property",
            "    def ip_with_mask(self):",
            "        \"\"\"",
            "        Get ip with netmask",
            "        \"\"\"",
            "        return '{}/{}'.format(self.ip, self.mask)",
            "",
            "    @property",
            "    def ip_interface(self):",
            "        \"\"\"",
            "        Create ip_interface instance",
            "        \"\"\"",
            "        return ipaddress.ip_interface(self.ip_with_mask)",
            "",
            "    @property",
            "    def network(self):",
            "        \"\"\"",
            "        Get network address",
            "        \"\"\"",
            "        return str(self.ip_interface.network.network_address)",
            "",
            "    def ip_in_network(self, addr):",
            "        \"\"\"",
            "        Check whether the addr in the network",
            "        \"\"\"",
            "        return IP(addr).ip_address in self.ip_interface.network",
            "",
            "",
            "class InterfacesInfo(object):",
            "    \"\"\"",
            "    Class to collect interfaces information on local node",
            "    \"\"\"",
            "",
            "    def __init__(self, ipv6=False, second_heartbeat=False, custom_nic_list=[]):",
            "        \"\"\"",
            "        Init function",
            "",
            "        On init process,",
            "        \"ipv6\" is provided by -I option",
            "        \"second_heartbeat\" is provided by -M option",
            "        \"custom_nic_list\" is provided by -i option",
            "        \"\"\"",
            "        self.ip_version = 6 if ipv6 else 4",
            "        self.second_heartbeat = second_heartbeat",
            "        self._default_nic_list = custom_nic_list",
            "        self._nic_info_dict = {}",
            "",
            "    def get_interfaces_info(self):",
            "        \"\"\"",
            "        Try to get interfaces info dictionary via \"ip\" command",
            "",
            "        IMPORTANT: This is the method that populates the data, should always be called after initialize",
            "        \"\"\"",
            "        cmd = \"ip -{} -o addr show\".format(self.ip_version)",
            "        rc, out, err = get_stdout_stderr(cmd)",
            "        if rc != 0:",
            "            raise ValueError(err)",
            "",
            "        # format on each line will like:",
            "        # 2: enp1s0    inet 192.168.122.241/24 brd 192.168.122.255 scope global enp1s0\\       valid_lft forever preferred_lft forever",
            "        for line in out.splitlines():",
            "            _, nic, _, ip_with_mask, *_ = line.split()",
            "            # maybe from tun interface",
            "            if not '/' in ip_with_mask:",
            "                continue",
            "            #TODO change this condition when corosync support link-local address",
            "            interface_inst = Interface(ip_with_mask)",
            "            if interface_inst.is_loopback or interface_inst.is_link_local:",
            "                continue",
            "            # one nic might configured multi IP addresses",
            "            if nic not in self._nic_info_dict:",
            "                self._nic_info_dict[nic] = []",
            "            self._nic_info_dict[nic].append(interface_inst)",
            "",
            "        if not self._nic_info_dict:",
            "            raise ValueError(\"No address configured\")",
            "        if self.second_heartbeat and len(self._nic_info_dict) == 1:",
            "            raise ValueError(\"Cannot configure second heartbeat, since only one address is available\")",
            "",
            "    @property",
            "    def nic_list(self):",
            "        \"\"\"",
            "        Get interfaces name list",
            "        \"\"\"",
            "        return list(self._nic_info_dict.keys())",
            "",
            "    @property",
            "    def interface_list(self):",
            "        \"\"\"",
            "        Get instance list of class Interface",
            "        \"\"\"",
            "        _interface_list = []",
            "        for interface in self._nic_info_dict.values():",
            "            _interface_list.extend(interface)",
            "        return _interface_list",
            "",
            "    @property",
            "    def ip_list(self):",
            "        \"\"\"",
            "        Get IP address list",
            "        \"\"\"",
            "        return [interface.ip for interface in self.interface_list]",
            "",
            "    @classmethod",
            "    def get_local_ip_list(cls, is_ipv6):",
            "        \"\"\"",
            "        Get IP address list",
            "        \"\"\"",
            "        cls_inst = cls(is_ipv6)",
            "        cls_inst.get_interfaces_info()",
            "        return cls_inst.ip_list",
            "",
            "    @classmethod",
            "    def ip_in_local(cls, addr):",
            "        \"\"\"",
            "        Check whether given address was in one of local address",
            "        \"\"\"",
            "        cls_inst = cls(IP.is_ipv6(addr))",
            "        cls_inst.get_interfaces_info()",
            "        return addr in cls_inst.ip_list",
            "",
            "    @property",
            "    def network_list(self):",
            "        \"\"\"",
            "        Get network list",
            "        \"\"\"",
            "        return list(set([interface.network for interface in self.interface_list]))",
            "",
            "    def _nic_first_ip(self, nic):",
            "        \"\"\"",
            "        Get the first IP of specific nic",
            "        \"\"\"",
            "        return self._nic_info_dict[nic][0].ip",
            "",
            "    def get_default_nic_list_from_route(self):",
            "        \"\"\"",
            "        Get default nic list from route",
            "        \"\"\"",
            "        if self._default_nic_list:",
            "            return self._default_nic_list",
            "",
            "        #TODO what if user only has ipv6 route?",
            "        cmd = \"ip -o route show\"",
            "        rc, out, err = get_stdout_stderr(cmd)",
            "        if rc != 0:",
            "            raise ValueError(err)",
            "        res = re.search(r'^default via .* dev (.*?) ', out)",
            "        if res:",
            "            self._default_nic_list = [res.group(1)]",
            "        else:",
            "            if not self.nic_list:",
            "                self.get_interfaces_info()",
            "            common_warn(\"No default route configured. Using the first found nic\")",
            "            self._default_nic_list = [self.nic_list[0]]",
            "        return self._default_nic_list",
            "",
            "    def get_default_ip_list(self):",
            "        \"\"\"",
            "        Get default IP list will be used by corosync",
            "        \"\"\"",
            "        if not self._default_nic_list:",
            "            self.get_default_nic_list_from_route()",
            "        if not self.nic_list:",
            "            self.get_interfaces_info()",
            "",
            "        _ip_list = []",
            "        for nic in self._default_nic_list:",
            "            # in case given interface not exist",
            "            if nic not in self.nic_list:",
            "                raise ValueError(\"Failed to detect IP address for {}\".format(nic))",
            "            _ip_list.append(self._nic_first_ip(nic))",
            "        # in case -M specified but given one interface via -i",
            "        if self.second_heartbeat and len(self._default_nic_list) == 1:",
            "            for nic in self.nic_list:",
            "                if nic not in self._default_nic_list:",
            "                    _ip_list.append(self._nic_first_ip(nic))",
            "                    break",
            "        return _ip_list",
            "",
            "    @classmethod",
            "    def ip_in_network(cls, addr):",
            "        \"\"\"",
            "        Check whether given address was in one of local networks",
            "        \"\"\"",
            "        cls_inst = cls(IP.is_ipv6(addr))",
            "        cls_inst.get_interfaces_info()",
            "        for interface_inst in cls_inst.interface_list:",
            "            if interface_inst.ip_in_network(addr):",
            "                return True",
            "        return False",
            "",
            "",
            "def check_file_content_included(source_file, target_file):",
            "    \"\"\"",
            "    Check whether target_file includes contents of source_file",
            "    \"\"\"",
            "    if not os.path.exists(source_file):",
            "        raise ValueError(\"File {} not exist\".format(source_file))",
            "    if not os.path.exists(target_file):",
            "        return False",
            "",
            "    with open(target_file, 'r') as target_fd:",
            "        target_data = target_fd.read()",
            "    with open(source_file, 'r') as source_fd:",
            "        source_data = source_fd.read()",
            "    return source_data in target_data",
            "",
            "",
            "class ServiceManager(object):",
            "    \"\"\"",
            "    Class to manage systemctl services",
            "    \"\"\"",
            "    ACTION_MAP = {",
            "            \"enable\": \"enable\",",
            "            \"disable\": \"disable\",",
            "            \"start\": \"start\",",
            "            \"stop\": \"stop\",",
            "            \"is_enabled\": \"is-enabled\",",
            "            \"is_active\": \"is-active\",",
            "            \"is_available\": \"list-unit-files\"",
            "            }",
            "",
            "    def __init__(self, service_name, remote_addr=None):",
            "        \"\"\"",
            "        Init function",
            "        \"\"\"",
            "        self.service_name = service_name",
            "        self.remote_addr = remote_addr",
            "",
            "    def _do_action(self, action_type):",
            "        \"\"\"",
            "        Actual do actions to manage service",
            "        \"\"\"",
            "        if action_type not in self.ACTION_MAP.values():",
            "            raise ValueError(\"status_type should be {}\".format('/'.join(list(self.ACTION_MAP.values()))))",
            "",
            "        cmd = \"systemctl {} {}\".format(action_type, self.service_name)",
            "        if self.remote_addr:",
            "            prompt_msg = \"Run \\\"{}\\\" on {}\".format(cmd, self.remote_addr)",
            "            rc, output, err = run_cmd_on_remote(cmd, self.remote_addr, prompt_msg)",
            "        else:",
            "            rc, output, err = get_stdout_stderr(cmd)",
            "        if rc != 0 and err:",
            "            raise ValueError(\"Run \\\"{}\\\" error: {}\".format(cmd, err))",
            "        return rc == 0, output",
            "",
            "    @property",
            "    def is_available(self):",
            "        return self.service_name in self._do_action(self.ACTION_MAP[\"is_available\"])[1]",
            "",
            "    @property",
            "    def is_enabled(self):",
            "        return self._do_action(self.ACTION_MAP[\"is_enabled\"])[0]",
            "",
            "    @property",
            "    def is_active(self):",
            "        return self._do_action(self.ACTION_MAP[\"is_active\"])[0]",
            "",
            "    def start(self):",
            "        self._do_action(self.ACTION_MAP[\"start\"])",
            "",
            "    def stop(self):",
            "        self._do_action(self.ACTION_MAP[\"stop\"])",
            "",
            "    def enable(self):",
            "        self._do_action(self.ACTION_MAP[\"enable\"])",
            "",
            "    def disable(self):",
            "        self._do_action(self.ACTION_MAP[\"disable\"])",
            "",
            "    @classmethod",
            "    def service_is_available(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Check whether service is available",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        return inst.is_available",
            "",
            "    @classmethod",
            "    def service_is_enabled(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Check whether service is enabled",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        return inst.is_enabled",
            "",
            "    @classmethod",
            "    def service_is_active(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Check whether service is active",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        return inst.is_active",
            "",
            "    @classmethod",
            "    def start_service(cls, name, enable=False, remote_addr=None):",
            "        \"\"\"",
            "        Start service",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        if enable:",
            "            inst.enable()",
            "        inst.start()",
            "",
            "    @classmethod",
            "    def stop_service(cls, name, disable=False, remote_addr=None):",
            "        \"\"\"",
            "        Stop service",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        if disable:",
            "            inst.disable()",
            "        inst.stop()",
            "",
            "    @classmethod",
            "    def enable_service(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Enable service",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        if inst.is_available and not inst.is_enabled:",
            "            inst.enable()",
            "",
            "    @classmethod",
            "    def disable_service(cls, name, remote_addr=None):",
            "        \"\"\"",
            "        Disable service",
            "        \"\"\"",
            "        inst = cls(name, remote_addr)",
            "        if inst.is_available and inst.is_enabled:",
            "            inst.disable()",
            "",
            "",
            "service_is_available = ServiceManager.service_is_available",
            "service_is_enabled = ServiceManager.service_is_enabled",
            "service_is_active = ServiceManager.service_is_active",
            "start_service = ServiceManager.start_service",
            "stop_service = ServiceManager.stop_service",
            "enable_service = ServiceManager.enable_service",
            "disable_service = ServiceManager.disable_service",
            "",
            "",
            "def package_is_installed(pkg, remote_addr=None):",
            "    \"\"\"",
            "    Check if package is installed",
            "    \"\"\"",
            "    cmd = \"rpm -q --quiet {}\".format(pkg)",
            "    if remote_addr:",
            "        # check on remote",
            "        prompt_msg = \"Check whether {} is installed on {}\".format(pkg, remote_addr)",
            "        rc, _, _ = run_cmd_on_remote(cmd, remote_addr, prompt_msg)",
            "    else:",
            "        # check on local",
            "        rc, _ = get_stdout(cmd)",
            "    return rc == 0",
            "",
            "",
            "def ping_node(node):",
            "    \"\"\"",
            "    Check if the remote node is reachable",
            "    \"\"\"",
            "    rc, _, err = get_stdout_stderr(\"ping -c 1 {}\".format(node))",
            "    if rc != 0:",
            "        raise ValueError(\"host \\\"{}\\\" is unreachable: {}\".format(node, err))",
            "# vim:ts=4:sw=4:et:"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "2087": [
                "check_ssh_passwd_need"
            ]
        },
        "addLocation": []
    }
}