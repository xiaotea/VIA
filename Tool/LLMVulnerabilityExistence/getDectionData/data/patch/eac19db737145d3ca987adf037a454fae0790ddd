{
    "vantage6-algorithm-tools/vantage6/algorithm/tools/decorators.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from vantage6.algorithm.client import AlgorithmClient"
            },
            "2": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " from vantage6.algorithm.tools.mock_client import MockAlgorithmClient"
            },
            "3": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from vantage6.algorithm.tools.util import info, error, warn"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 13,
                "PatchRowcode": "+from vantage6.algorithm.tools.util import info, error, warn, get_env_var"
            },
            "5": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from vantage6.algorithm.tools.wrappers import load_data"
            },
            "6": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from vantage6.algorithm.tools.preprocessing import preprocess_data"
            },
            "7": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "             if mock_client is not None:"
            },
            "9": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "                 return func(mock_client, *args, **kwargs)"
            },
            "10": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "             # read server address from the environment"
            },
            "11": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            host = os.environ[\"HOST\"]"
            },
            "12": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            port = os.environ[\"PORT\"]"
            },
            "13": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            api_path = os.environ[\"API_PATH\"]"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+            host = get_env_var(\"HOST\")"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+            port = get_env_var(\"PORT\")"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+            api_path = get_env_var(\"API_PATH\")"
            },
            "17": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 95,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "             # read token from the environment"
            },
            "19": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            token_file = os.environ[\"TOKEN_FILE\"]"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+            token_file = get_env_var(\"TOKEN_FILE\")"
            },
            "21": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "             info(\"Reading token\")"
            },
            "22": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "             with open(token_file) as fp:"
            },
            "23": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "                 token = fp.read().strip()"
            },
            "24": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 184,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "                 # do any data preprocessing here"
            },
            "26": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "                 info(f\"Applying preprocessing for database '{label}'\")"
            },
            "27": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                env_prepro = os.environ.get(f\"{label.upper()}_PREPROCESSING\")"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+                env_prepro = get_env_var(f\"{label.upper()}_PREPROCESSING\")"
            },
            "29": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "                 if env_prepro is not None:"
            },
            "30": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 189,
                "PatchRowcode": "                     preprocess = json.loads(env_prepro)"
            },
            "31": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "                     data_ = preprocess_data(data_, preprocess)"
            },
            "32": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": 291,
                "PatchRowcode": "         >>> def my_algorithm(metadata: RunMetaData, <other arguments>):"
            },
            "33": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 292,
                "PatchRowcode": "         >>>     pass"
            },
            "34": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 293,
                "PatchRowcode": "         \"\"\""
            },
            "35": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        token_file = os.environ[\"TOKEN_FILE\"]"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 294,
                "PatchRowcode": "+        token_file = get_env_var(\"TOKEN_FILE\")"
            },
            "37": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": 295,
                "PatchRowcode": "         info(\"Reading token\")"
            },
            "38": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": 296,
                "PatchRowcode": "         with open(token_file) as fp:"
            },
            "39": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": 297,
                "PatchRowcode": "             token = fp.read().strip()"
            },
            "40": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 304,
                "PatchRowcode": "             node_id=payload[\"node_id\"],"
            },
            "41": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": 305,
                "PatchRowcode": "             collaboration_id=payload[\"collaboration_id\"],"
            },
            "42": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": 306,
                "PatchRowcode": "             organization_id=payload[\"organization_id\"],"
            },
            "43": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            temporary_directory=Path(os.environ[\"TEMPORARY_FOLDER\"]),"
            },
            "44": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            output_file=Path(os.environ[\"OUTPUT_FILE\"]),"
            },
            "45": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            input_file=Path(os.environ[\"INPUT_FILE\"]),"
            },
            "46": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            token_file=Path(os.environ[\"TOKEN_FILE\"])"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 307,
                "PatchRowcode": "+            temporary_directory=Path(get_env_var(\"TEMPORARY_FOLDER\")),"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 308,
                "PatchRowcode": "+            output_file=Path(get_env_var(\"OUTPUT_FILE\")),"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 309,
                "PatchRowcode": "+            input_file=Path(get_env_var(\"INPUT_FILE\")),"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 310,
                "PatchRowcode": "+            token_file=Path(get_env_var(\"TOKEN_FILE\"))"
            },
            "51": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": 311,
                "PatchRowcode": "         )"
            },
            "52": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 312,
                "PatchRowcode": "         return func(metadata, *args, **kwargs)"
            },
            "53": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 313,
                "PatchRowcode": "     return decorator"
            },
            "54": {
                "beforePatchRowNumber": 336,
                "afterPatchRowNumber": 336,
                "PatchRowcode": "     for var in expected_env_vars:"
            },
            "55": {
                "beforePatchRowNumber": 337,
                "afterPatchRowNumber": 337,
                "PatchRowcode": "         _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')"
            },
            "56": {
                "beforePatchRowNumber": 338,
                "afterPatchRowNumber": 338,
                "PatchRowcode": " "
            },
            "57": {
                "beforePatchRowNumber": 339,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    tmp = Path(os.environ[\"TEMPORARY_FOLDER\"])"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 339,
                "PatchRowcode": "+    tmp = Path(get_env_var(\"TEMPORARY_FOLDER\"))"
            },
            "59": {
                "beforePatchRowNumber": 340,
                "afterPatchRowNumber": 340,
                "PatchRowcode": "     metadata = OHDSIMetaData("
            },
            "60": {
                "beforePatchRowNumber": 341,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        database=os.environ[f\"{label_}_DB_PARAM_CDM_DATABASE\"],"
            },
            "61": {
                "beforePatchRowNumber": 342,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        cdm_schema=os.environ[f\"{label_}_DB_PARAM_CDM_SCHEMA\"],"
            },
            "62": {
                "beforePatchRowNumber": 343,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        results_schema=os.environ[f\"{label_}_DB_PARAM_RESULTS_SCHEMA\"],"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 341,
                "PatchRowcode": "+        database=get_env_var(f\"{label_}_DB_PARAM_CDM_DATABASE\"),"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 342,
                "PatchRowcode": "+        cdm_schema=get_env_var(f\"{label_}_DB_PARAM_CDM_SCHEMA\"),"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+        results_schema=get_env_var(f\"{label_}_DB_PARAM_RESULTS_SCHEMA\"),"
            },
            "66": {
                "beforePatchRowNumber": 344,
                "afterPatchRowNumber": 344,
                "PatchRowcode": "         incremental_folder=tmp / \"incremental\","
            },
            "67": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": 345,
                "PatchRowcode": "         cohort_statistics_folder=tmp / \"cohort_statistics\","
            },
            "68": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": 346,
                "PatchRowcode": "         export_folder=tmp / \"export\""
            },
            "69": {
                "beforePatchRowNumber": 399,
                "afterPatchRowNumber": 399,
                "PatchRowcode": "         _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')"
            },
            "70": {
                "beforePatchRowNumber": 400,
                "afterPatchRowNumber": 400,
                "PatchRowcode": " "
            },
            "71": {
                "beforePatchRowNumber": 401,
                "afterPatchRowNumber": 401,
                "PatchRowcode": "     info(\"Reading OHDSI environment variables\")"
            },
            "72": {
                "beforePatchRowNumber": 402,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    dbms = os.environ[f\"{label_}_DB_PARAM_DBMS\"]"
            },
            "73": {
                "beforePatchRowNumber": 403,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    uri = os.environ[f\"{label_}_DATABASE_URI\"]"
            },
            "74": {
                "beforePatchRowNumber": 404,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    user = os.environ[f\"{label_}_DB_PARAM_USER\"]"
            },
            "75": {
                "beforePatchRowNumber": 405,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    password = os.environ[f\"{label_}_DB_PARAM_PASSWORD\"]"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 402,
                "PatchRowcode": "+    dbms = get_env_var(f\"{label_}_DB_PARAM_DBMS\")"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 403,
                "PatchRowcode": "+    uri = get_env_var(f\"{label_}_DATABASE_URI\")"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 404,
                "PatchRowcode": "+    user = get_env_var(f\"{label_}_DB_PARAM_USER\")"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 405,
                "PatchRowcode": "+    password = get_env_var(f\"{label_}_DB_PARAM_PASSWORD\")"
            },
            "80": {
                "beforePatchRowNumber": 406,
                "afterPatchRowNumber": 406,
                "PatchRowcode": "     info(f' - dbms: {dbms}')"
            },
            "81": {
                "beforePatchRowNumber": 407,
                "afterPatchRowNumber": 407,
                "PatchRowcode": "     info(f' - uri: {uri}')"
            },
            "82": {
                "beforePatchRowNumber": 408,
                "afterPatchRowNumber": 408,
                "PatchRowcode": "     info(f' - user: {user}')"
            },
            "83": {
                "beforePatchRowNumber": 441,
                "afterPatchRowNumber": 441,
                "PatchRowcode": "         Data from the database"
            },
            "84": {
                "beforePatchRowNumber": 442,
                "afterPatchRowNumber": 442,
                "PatchRowcode": "     \"\"\""
            },
            "85": {
                "beforePatchRowNumber": 443,
                "afterPatchRowNumber": 443,
                "PatchRowcode": "     # Load the input data from the input file - this may e.g. include the"
            },
            "86": {
                "beforePatchRowNumber": 444,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    database_uri = os.environ[f\"{label.upper()}_DATABASE_URI\"]"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 444,
                "PatchRowcode": "+    database_uri = get_env_var(f\"{label.upper()}_DATABASE_URI\")"
            },
            "88": {
                "beforePatchRowNumber": 445,
                "afterPatchRowNumber": 445,
                "PatchRowcode": "     info(f\"Using '{database_uri}' with label '{label}' as database\")"
            },
            "89": {
                "beforePatchRowNumber": 446,
                "afterPatchRowNumber": 446,
                "PatchRowcode": " "
            },
            "90": {
                "beforePatchRowNumber": 447,
                "afterPatchRowNumber": 447,
                "PatchRowcode": "     # Get the database type from the environment variable, this variable is"
            },
            "91": {
                "beforePatchRowNumber": 448,
                "afterPatchRowNumber": 448,
                "PatchRowcode": "     # set by the vantage6 node based on its configuration file."
            },
            "92": {
                "beforePatchRowNumber": 449,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    database_type = os.environ.get("
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 449,
                "PatchRowcode": "+    database_type = get_env_var("
            },
            "94": {
                "beforePatchRowNumber": 450,
                "afterPatchRowNumber": 450,
                "PatchRowcode": "         f\"{label.upper()}_DATABASE_TYPE\", \"csv\").lower()"
            },
            "95": {
                "beforePatchRowNumber": 451,
                "afterPatchRowNumber": 451,
                "PatchRowcode": " "
            },
            "96": {
                "beforePatchRowNumber": 452,
                "afterPatchRowNumber": 452,
                "PatchRowcode": "     # Load the data based on the database type. Try to provide environment"
            },
            "97": {
                "beforePatchRowNumber": 453,
                "afterPatchRowNumber": 453,
                "PatchRowcode": "     # variables that should be available for some data types."
            },
            "98": {
                "beforePatchRowNumber": 454,
                "afterPatchRowNumber": 454,
                "PatchRowcode": "     return load_data("
            },
            "99": {
                "beforePatchRowNumber": 455,
                "afterPatchRowNumber": 455,
                "PatchRowcode": "         database_uri,"
            },
            "100": {
                "beforePatchRowNumber": 456,
                "afterPatchRowNumber": 456,
                "PatchRowcode": "         database_type,"
            },
            "101": {
                "beforePatchRowNumber": 457,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        query=os.environ.get(f\"{label.upper()}_QUERY\"),"
            },
            "102": {
                "beforePatchRowNumber": 458,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        sheet_name=os.environ.get(f\"{label.upper()}_SHEET_NAME\")"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 457,
                "PatchRowcode": "+        query=get_env_var(f\"{label.upper()}_QUERY\"),"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 458,
                "PatchRowcode": "+        sheet_name=get_env_var(f\"{label.upper()}_SHEET_NAME\")"
            },
            "105": {
                "beforePatchRowNumber": 459,
                "afterPatchRowNumber": 459,
                "PatchRowcode": "     )"
            },
            "106": {
                "beforePatchRowNumber": 460,
                "afterPatchRowNumber": 460,
                "PatchRowcode": " "
            },
            "107": {
                "beforePatchRowNumber": 461,
                "afterPatchRowNumber": 461,
                "PatchRowcode": " "
            },
            "108": {
                "beforePatchRowNumber": 470,
                "afterPatchRowNumber": 470,
                "PatchRowcode": "     \"\"\""
            },
            "109": {
                "beforePatchRowNumber": 471,
                "afterPatchRowNumber": 471,
                "PatchRowcode": "     # read the labels that the user requested, which is a comma"
            },
            "110": {
                "beforePatchRowNumber": 472,
                "afterPatchRowNumber": 472,
                "PatchRowcode": "     # separated list of labels."
            },
            "111": {
                "beforePatchRowNumber": 473,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    labels = os.environ[\"USER_REQUESTED_DATABASE_LABELS\"]"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 473,
                "PatchRowcode": "+    labels = get_env_var(\"USER_REQUESTED_DATABASE_LABELS\")"
            },
            "113": {
                "beforePatchRowNumber": 474,
                "afterPatchRowNumber": 474,
                "PatchRowcode": "     return labels.split(',')"
            },
            "114": {
                "beforePatchRowNumber": 475,
                "afterPatchRowNumber": 475,
                "PatchRowcode": " "
            },
            "115": {
                "beforePatchRowNumber": 476,
                "afterPatchRowNumber": 476,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "import os",
            "import json",
            "import jwt",
            "",
            "from pathlib import Path",
            "from functools import wraps",
            "from dataclasses import dataclass",
            "",
            "import pandas as pd",
            "",
            "from vantage6.algorithm.client import AlgorithmClient",
            "from vantage6.algorithm.tools.mock_client import MockAlgorithmClient",
            "from vantage6.algorithm.tools.util import info, error, warn",
            "from vantage6.algorithm.tools.wrappers import load_data",
            "from vantage6.algorithm.tools.preprocessing import preprocess_data",
            "",
            "OHDSI_AVAILABLE = True",
            "try:",
            "    from ohdsi.database_connector import connect as connect_to_omop",
            "except ImportError:",
            "    OHDSI_AVAILABLE = False",
            "",
            "",
            "@dataclass",
            "class RunMetaData:",
            "    \"\"\"Dataclass containing metadata of the run.\"\"\"",
            "    task_id: int | None",
            "    node_id: int | None",
            "    collaboration_id: int | None",
            "    organization_id: int | None",
            "    temporary_directory: Path | None",
            "    output_file: Path | None",
            "    input_file: Path | None",
            "    token_file: Path | None",
            "",
            "",
            "@dataclass",
            "class OHDSIMetaData:",
            "    \"\"\"Dataclass containing metadata of the OMOP database.\"\"\"",
            "    database: str | None",
            "    cdm_schema: str | None",
            "    results_schema: str | None",
            "    incremental_folder: Path | None",
            "    cohort_statistics_folder: Path | None",
            "    export_folder: Path | None",
            "",
            "",
            "def _algorithm_client() -> callable:",
            "    \"\"\"",
            "    Decorator that adds an algorithm client object to a function",
            "",
            "    By adding @algorithm_client to a function, the ``algorithm_client``",
            "    argument will be added to the front of the argument list. This client can",
            "    be used to communicate with the server.",
            "",
            "    There is one reserved argument `mock_client` in the function to be",
            "    decorated. If this argument is provided, the decorator will add this",
            "    MockAlgorithmClient to the front of the argument list instead of the",
            "    regular AlgorithmClient.",
            "",
            "    Parameters",
            "    ----------",
            "    func : callable",
            "        Function to decorate",
            "",
            "    Returns",
            "    -------",
            "    callable",
            "        Decorated function",
            "",
            "    Examples",
            "    --------",
            "    >>> @algorithm_client",
            "    >>> def my_algorithm(algorithm_client: AlgorithmClient, <other arguments>):",
            "    >>>     pass",
            "    \"\"\"",
            "    def protection_decorator(func: callable, *args, **kwargs) -> callable:",
            "        @wraps(func)",
            "        def decorator(*args, mock_client: MockAlgorithmClient = None,",
            "                      **kwargs) -> callable:",
            "            \"\"\"",
            "            Wrap the function with the client object",
            "",
            "            Parameters",
            "            ----------",
            "            mock_client : MockAlgorithmClient",
            "                Mock client to use instead of the regular client",
            "            \"\"\"",
            "            if mock_client is not None:",
            "                return func(mock_client, *args, **kwargs)",
            "            # read server address from the environment",
            "            host = os.environ[\"HOST\"]",
            "            port = os.environ[\"PORT\"]",
            "            api_path = os.environ[\"API_PATH\"]",
            "",
            "            # read token from the environment",
            "            token_file = os.environ[\"TOKEN_FILE\"]",
            "            info(\"Reading token\")",
            "            with open(token_file) as fp:",
            "                token = fp.read().strip()",
            "",
            "            client = AlgorithmClient(token=token, host=host, port=port,",
            "                                     path=api_path)",
            "            return func(client, *args, **kwargs)",
            "        # set attribute that this function is wrapped in an algorithm client",
            "        decorator.wrapped_in_algorithm_client_decorator = True",
            "        return decorator",
            "    return protection_decorator",
            "",
            "",
            "# alias for algorithm_client so that algorithm developers can do",
            "# @algorithm_client instead of @algorithm_client()",
            "algorithm_client = _algorithm_client()",
            "",
            "",
            "def data(number_of_databases: int = 1) -> callable:",
            "    \"\"\"",
            "    Decorator that adds algorithm data to a function",
            "",
            "    By adding `@data()` to a function, one or several pandas dataframes will be",
            "    added to the front of the argument list. This data will be read from the",
            "    databases that the user who creates the task provides.",
            "",
            "    Note that the user should provide exactly as many databases as the",
            "    decorated function requires when they create the task.",
            "",
            "    There is one reserved argument `mock_data` in the function to be",
            "    decorated. If this argument is provided, the decorator will add this",
            "    mocked data to the front of the argument list, instead of reading in the",
            "    data from the databases.",
            "",
            "    Parameters",
            "    ----------",
            "    number_of_databases: int",
            "        Number of data sources to load. These will be loaded in order by which",
            "        the user provided them. Default is 1.",
            "",
            "    Returns",
            "    -------",
            "    callable",
            "        Decorated function",
            "",
            "    Examples",
            "    --------",
            "    >>> @data(number_of_databases=2)",
            "    >>> def my_algorithm(first_df: pd.DataFrame, second_df: pd.DataFrame,",
            "    >>>                  <other arguments>):",
            "    >>>     pass",
            "    \"\"\"",
            "    def protection_decorator(func: callable, *args, **kwargs) -> callable:",
            "        @wraps(func)",
            "        def decorator(*args, mock_data: list[pd.DataFrame] = None,",
            "                      **kwargs) -> callable:",
            "            \"\"\"",
            "            Wrap the function with the data",
            "",
            "            Parameters",
            "            ----------",
            "            mock_data : list[pd.DataFrame]",
            "                Mock data to use instead of the regular data",
            "            \"\"\"",
            "            if mock_data is not None:",
            "                return func(*mock_data, *args, **kwargs)",
            "",
            "            # read the labels that the user requested",
            "            labels = _get_user_database_labels()",
            "",
            "            # check if user provided enough databases",
            "            if len(labels) < number_of_databases:",
            "                error(f\"Algorithm requires {number_of_databases} databases \"",
            "                      f\"but only {len(labels)} were provided. \"",
            "                      \"Exiting...\")",
            "                exit(1)",
            "            elif len(labels) > number_of_databases:",
            "                warn(f\"Algorithm requires only {number_of_databases} databases\"",
            "                     f\", but {len(labels)} were provided. Using the \"",
            "                     f\"first {number_of_databases} databases.\")",
            "",
            "            for i in range(number_of_databases):",
            "                label = labels[i]",
            "                # read the data from the database",
            "                info(\"Reading data from database\")",
            "                data_ = _get_data_from_label(label)",
            "",
            "                # do any data preprocessing here",
            "                info(f\"Applying preprocessing for database '{label}'\")",
            "                env_prepro = os.environ.get(f\"{label.upper()}_PREPROCESSING\")",
            "                if env_prepro is not None:",
            "                    preprocess = json.loads(env_prepro)",
            "                    data_ = preprocess_data(data_, preprocess)",
            "",
            "                # add the data to the arguments",
            "                args = (data_, *args)",
            "",
            "            return func(*args, **kwargs)",
            "        # set attribute that this function is wrapped in a data decorator",
            "        decorator.wrapped_in_data_decorator = True",
            "        return decorator",
            "    return protection_decorator",
            "",
            "",
            "def database_connection(types: list[str], include_metadata: bool = True) \\",
            "        -> callable:",
            "    \"\"\"",
            "    Decorator that adds a database connection to a function",
            "",
            "    By adding @database_connection to a function, a database connection will",
            "    be added to the front of the argument list. This connection can be used to",
            "    communicate with the database.",
            "",
            "    Parameters",
            "    ----------",
            "    types : list[str]",
            "        List of types of databases to connect to. Currently only \"OMOP\" is",
            "        supported.",
            "    include_metadata : bool",
            "        Whether to include metadata in the function arguments. This metadata",
            "        contains the database name, CDM schema, and results schema. Default is",
            "        True.",
            "",
            "    Example",
            "    -------",
            "    For a single OMOP data source:",
            "    >>> @database_connection(types=[\"OMOP\"])",
            "    >>> def my_algorithm(connection: Connection, meta: OHDSIMetaData,",
            "    >>>                  <other arguments>):",
            "    >>>     pass",
            "",
            "    In case you have multiple OMOP data sources:",
            "    >>> @database_connection(types=[\"OMOP\", \"OMOP\"])",
            "    >>> def my_algorithm(connection1: Connection, meta1: OHDSIMetaData,",
            "    >>>                  connection2: Connection, meta2: OHDSIMetaData,",
            "    >>>                  <other arguments>):",
            "    >>>     pass",
            "",
            "    In the case you do not want to include the metadata:",
            "    >>> @database_connection(types=[\"OMOP\"], include_metadata=False)",
            "    >>> def my_algorithm(connection: Connection, <other arguments>):",
            "    >>>     pass",
            "    \"\"\"",
            "    def connection_decorator(func: callable, *args, **kwargs) -> callable:",
            "        @wraps(func)",
            "        def decorator(*args, **kwargs) -> callable:",
            "            \"\"\"",
            "            Wrap the function with the database connection",
            "            \"\"\"",
            "            labels = _get_user_database_labels()",
            "            if len(labels) < len(types):",
            "                error(f\"User provided {len(labels)} databases, but algorithm \"",
            "                      f\"requires {len(types)} database connections. Exiting.\")",
            "                exit(1)",
            "            if len(labels) > len(types):",
            "                warn(f\"User provided {len(labels)} databases, but algorithm \"",
            "                     f\"requires {len(types)} database connections. Using the \"",
            "                     f\"first {len(types)} databases.\")",
            "",
            "            db_args = []",
            "            # Note: zip will stop at the shortest iterable, so this is exactly",
            "            # what we want in the len(labels) > len(types) case.",
            "            for type_, label in zip(types, labels):",
            "                match type_.upper():",
            "                    case \"OMOP\":",
            "                        info(\"Creating OMOP database connection\")",
            "                        connection = _create_omop_database_connection(label)",
            "                        db_args.append(connection)",
            "                        if include_metadata:",
            "                            meta = get_ohdsi_metadata(label)",
            "                            db_args.append(meta)",
            "                    # case \"FHIR\":",
            "                    #     pass",
            "",
            "            return func(*db_args, *args, **kwargs)",
            "",
            "        return decorator",
            "    return connection_decorator",
            "",
            "",
            "def metadata(func: callable) -> callable:",
            "    @wraps(func)",
            "    def decorator(*args, **kwargs) -> callable:",
            "        \"\"\"",
            "        Decorator the function with metadata from the run.",
            "",
            "        Decorator that adds metadata from the run to the function. This",
            "        includes the task id, node id, collaboration id, organization id,",
            "        temporary directory, output file, input file, and token file.",
            "",
            "        Example",
            "        -------",
            "        >>> @metadata",
            "        >>> def my_algorithm(metadata: RunMetaData, <other arguments>):",
            "        >>>     pass",
            "        \"\"\"",
            "        token_file = os.environ[\"TOKEN_FILE\"]",
            "        info(\"Reading token\")",
            "        with open(token_file) as fp:",
            "            token = fp.read().strip()",
            "",
            "        info(\"Extracting payload from token\")",
            "        payload = _extract_token_payload(token)",
            "",
            "        metadata = RunMetaData(",
            "            task_id=payload[\"task_id\"],",
            "            node_id=payload[\"node_id\"],",
            "            collaboration_id=payload[\"collaboration_id\"],",
            "            organization_id=payload[\"organization_id\"],",
            "            temporary_directory=Path(os.environ[\"TEMPORARY_FOLDER\"]),",
            "            output_file=Path(os.environ[\"OUTPUT_FILE\"]),",
            "            input_file=Path(os.environ[\"INPUT_FILE\"]),",
            "            token_file=Path(os.environ[\"TOKEN_FILE\"])",
            "        )",
            "        return func(metadata, *args, **kwargs)",
            "    return decorator",
            "",
            "",
            "def get_ohdsi_metadata(label: str) -> OHDSIMetaData:",
            "    \"\"\"",
            "    Retrieve the OHDSI metadata from the environment variables.",
            "",
            "    The following environment variables are expected to be set in the",
            "    node configuration in the `env` key of the `database` section:",
            "",
            "    - `CDM_DATABASE`",
            "    - `CDM_SCHEMA`",
            "    - `RESULTS_SCHEMA`",
            "",
            "    In case these are not set, the algorithm execution is terminated.",
            "",
            "    Example",
            "    -------",
            "    >>> get_ohdsi_metadata(\"my_database\")",
            "    \"\"\"",
            "    # check that all node environment variables are set",
            "    expected_env_vars = [\"CDM_DATABASE\", \"CDM_SCHEMA\", \"RESULTS_SCHEMA\"]",
            "    label_ = label.upper()",
            "    for var in expected_env_vars:",
            "        _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')",
            "",
            "    tmp = Path(os.environ[\"TEMPORARY_FOLDER\"])",
            "    metadata = OHDSIMetaData(",
            "        database=os.environ[f\"{label_}_DB_PARAM_CDM_DATABASE\"],",
            "        cdm_schema=os.environ[f\"{label_}_DB_PARAM_CDM_SCHEMA\"],",
            "        results_schema=os.environ[f\"{label_}_DB_PARAM_RESULTS_SCHEMA\"],",
            "        incremental_folder=tmp / \"incremental\",",
            "        cohort_statistics_folder=tmp / \"cohort_statistics\",",
            "        export_folder=tmp / \"export\"",
            "    )",
            "    return metadata",
            "",
            "",
            "def _create_omop_database_connection(label: str) -> callable:",
            "    \"\"\"",
            "    Create a connection to an OMOP database.",
            "",
            "    It expects that the following environment variables are set:",
            "    - DB_PARAM_DBMS: type of database to connect to",
            "    - DB_PARAM_USER: username to connect to the database",
            "    - DB_PARAM_PASSWORD: password to connect to the database",
            "",
            "    These should be provided in the vantage6 node configuration file in the",
            "    `database` section without the `DB_PARAM_` prefix. For example:",
            "",
            "    ```yaml",
            "    ...",
            "    databases:",
            "      - label: my_database",
            "        type: OMOP",
            "        uri: jdbc:postgresql://host.docker.internal:5454/postgres",
            "        env:",
            "            DBMS: \"postgresql\"",
            "            USER: \"my_user\"",
            "            PASSWORD: \"my_password\"",
            "    ...",
            "    ```",
            "",
            "    Parameters",
            "    ----------",
            "    label : str",
            "        Label of the database to connect to",
            "",
            "    Returns",
            "    -------",
            "    callable",
            "        OHDSI Database Connection object",
            "    \"\"\"",
            "",
            "    # check that the OHDSI package is available in this container",
            "    if not OHDSI_AVAILABLE:",
            "        error(\"OHDSI/DatabaseConnector is not available.\")",
            "        error(\"Did you use 'algorithm-ohdsi-base' image to build this \"",
            "              \"algorithm?\")",
            "        exit(1)",
            "",
            "    # environment vars are always uppercase",
            "    label_ = label.upper()",
            "",
            "    # check that the required environment variables are set",
            "    for var in (\"DBMS\", \"USER\", \"PASSWORD\"):",
            "        _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')",
            "",
            "    info(\"Reading OHDSI environment variables\")",
            "    dbms = os.environ[f\"{label_}_DB_PARAM_DBMS\"]",
            "    uri = os.environ[f\"{label_}_DATABASE_URI\"]",
            "    user = os.environ[f\"{label_}_DB_PARAM_USER\"]",
            "    password = os.environ[f\"{label_}_DB_PARAM_PASSWORD\"]",
            "    info(f' - dbms: {dbms}')",
            "    info(f' - uri: {uri}')",
            "    info(f' - user: {user}')",
            "",
            "    info(\"Creating OHDSI database connection\")",
            "    return connect_to_omop(dbms=dbms, connection_string=uri, password=password,",
            "                           user=user)",
            "",
            "",
            "def _check_environment_var_exists_or_exit(var: str):",
            "    \"\"\"",
            "    Check if the environment variable 'var' exists or print and exit.",
            "",
            "    Parameters",
            "    ----------",
            "    var : str",
            "        Environment variable name to check",
            "    \"\"\"",
            "    if var not in os.environ:",
            "        error(f\"Environment variable '{var}' is not set. Exiting...\")",
            "        exit(1)",
            "",
            "",
            "def _get_data_from_label(label: str) -> pd.DataFrame:",
            "    \"\"\"",
            "    Load data from a database based on the label",
            "",
            "    Parameters",
            "    ----------",
            "    label : str",
            "        Label of the database to load",
            "",
            "    Returns",
            "    -------",
            "    pd.DataFrame",
            "        Data from the database",
            "    \"\"\"",
            "    # Load the input data from the input file - this may e.g. include the",
            "    database_uri = os.environ[f\"{label.upper()}_DATABASE_URI\"]",
            "    info(f\"Using '{database_uri}' with label '{label}' as database\")",
            "",
            "    # Get the database type from the environment variable, this variable is",
            "    # set by the vantage6 node based on its configuration file.",
            "    database_type = os.environ.get(",
            "        f\"{label.upper()}_DATABASE_TYPE\", \"csv\").lower()",
            "",
            "    # Load the data based on the database type. Try to provide environment",
            "    # variables that should be available for some data types.",
            "    return load_data(",
            "        database_uri,",
            "        database_type,",
            "        query=os.environ.get(f\"{label.upper()}_QUERY\"),",
            "        sheet_name=os.environ.get(f\"{label.upper()}_SHEET_NAME\")",
            "    )",
            "",
            "",
            "def _get_user_database_labels() -> list[str]:",
            "    \"\"\"",
            "    Get the database labels from the environment",
            "",
            "    Returns",
            "    -------",
            "    list[str]",
            "        List of database labels",
            "    \"\"\"",
            "    # read the labels that the user requested, which is a comma",
            "    # separated list of labels.",
            "    labels = os.environ[\"USER_REQUESTED_DATABASE_LABELS\"]",
            "    return labels.split(',')",
            "",
            "",
            "def _extract_token_payload(token: str) -> dict:",
            "    \"\"\"",
            "    Extract the payload from the token.",
            "",
            "    Parameters",
            "    ----------",
            "    token: str",
            "        The token as a string.",
            "",
            "    Returns",
            "    -------",
            "    dict",
            "        The payload as a dictionary. It contains the keys: `client_type`,",
            "        `node_id`, `organization_id`, `collaboration_id`, `task_id`, `image`,",
            "        and `databases`.",
            "    \"\"\"",
            "    jwt_payload = jwt.decode(token, options={\"verify_signature\": False})",
            "    return jwt_payload['sub']"
        ],
        "afterPatchFile": [
            "import os",
            "import json",
            "import jwt",
            "",
            "from pathlib import Path",
            "from functools import wraps",
            "from dataclasses import dataclass",
            "",
            "import pandas as pd",
            "",
            "from vantage6.algorithm.client import AlgorithmClient",
            "from vantage6.algorithm.tools.mock_client import MockAlgorithmClient",
            "from vantage6.algorithm.tools.util import info, error, warn, get_env_var",
            "from vantage6.algorithm.tools.wrappers import load_data",
            "from vantage6.algorithm.tools.preprocessing import preprocess_data",
            "",
            "OHDSI_AVAILABLE = True",
            "try:",
            "    from ohdsi.database_connector import connect as connect_to_omop",
            "except ImportError:",
            "    OHDSI_AVAILABLE = False",
            "",
            "",
            "@dataclass",
            "class RunMetaData:",
            "    \"\"\"Dataclass containing metadata of the run.\"\"\"",
            "    task_id: int | None",
            "    node_id: int | None",
            "    collaboration_id: int | None",
            "    organization_id: int | None",
            "    temporary_directory: Path | None",
            "    output_file: Path | None",
            "    input_file: Path | None",
            "    token_file: Path | None",
            "",
            "",
            "@dataclass",
            "class OHDSIMetaData:",
            "    \"\"\"Dataclass containing metadata of the OMOP database.\"\"\"",
            "    database: str | None",
            "    cdm_schema: str | None",
            "    results_schema: str | None",
            "    incremental_folder: Path | None",
            "    cohort_statistics_folder: Path | None",
            "    export_folder: Path | None",
            "",
            "",
            "def _algorithm_client() -> callable:",
            "    \"\"\"",
            "    Decorator that adds an algorithm client object to a function",
            "",
            "    By adding @algorithm_client to a function, the ``algorithm_client``",
            "    argument will be added to the front of the argument list. This client can",
            "    be used to communicate with the server.",
            "",
            "    There is one reserved argument `mock_client` in the function to be",
            "    decorated. If this argument is provided, the decorator will add this",
            "    MockAlgorithmClient to the front of the argument list instead of the",
            "    regular AlgorithmClient.",
            "",
            "    Parameters",
            "    ----------",
            "    func : callable",
            "        Function to decorate",
            "",
            "    Returns",
            "    -------",
            "    callable",
            "        Decorated function",
            "",
            "    Examples",
            "    --------",
            "    >>> @algorithm_client",
            "    >>> def my_algorithm(algorithm_client: AlgorithmClient, <other arguments>):",
            "    >>>     pass",
            "    \"\"\"",
            "    def protection_decorator(func: callable, *args, **kwargs) -> callable:",
            "        @wraps(func)",
            "        def decorator(*args, mock_client: MockAlgorithmClient = None,",
            "                      **kwargs) -> callable:",
            "            \"\"\"",
            "            Wrap the function with the client object",
            "",
            "            Parameters",
            "            ----------",
            "            mock_client : MockAlgorithmClient",
            "                Mock client to use instead of the regular client",
            "            \"\"\"",
            "            if mock_client is not None:",
            "                return func(mock_client, *args, **kwargs)",
            "            # read server address from the environment",
            "            host = get_env_var(\"HOST\")",
            "            port = get_env_var(\"PORT\")",
            "            api_path = get_env_var(\"API_PATH\")",
            "",
            "            # read token from the environment",
            "            token_file = get_env_var(\"TOKEN_FILE\")",
            "            info(\"Reading token\")",
            "            with open(token_file) as fp:",
            "                token = fp.read().strip()",
            "",
            "            client = AlgorithmClient(token=token, host=host, port=port,",
            "                                     path=api_path)",
            "            return func(client, *args, **kwargs)",
            "        # set attribute that this function is wrapped in an algorithm client",
            "        decorator.wrapped_in_algorithm_client_decorator = True",
            "        return decorator",
            "    return protection_decorator",
            "",
            "",
            "# alias for algorithm_client so that algorithm developers can do",
            "# @algorithm_client instead of @algorithm_client()",
            "algorithm_client = _algorithm_client()",
            "",
            "",
            "def data(number_of_databases: int = 1) -> callable:",
            "    \"\"\"",
            "    Decorator that adds algorithm data to a function",
            "",
            "    By adding `@data()` to a function, one or several pandas dataframes will be",
            "    added to the front of the argument list. This data will be read from the",
            "    databases that the user who creates the task provides.",
            "",
            "    Note that the user should provide exactly as many databases as the",
            "    decorated function requires when they create the task.",
            "",
            "    There is one reserved argument `mock_data` in the function to be",
            "    decorated. If this argument is provided, the decorator will add this",
            "    mocked data to the front of the argument list, instead of reading in the",
            "    data from the databases.",
            "",
            "    Parameters",
            "    ----------",
            "    number_of_databases: int",
            "        Number of data sources to load. These will be loaded in order by which",
            "        the user provided them. Default is 1.",
            "",
            "    Returns",
            "    -------",
            "    callable",
            "        Decorated function",
            "",
            "    Examples",
            "    --------",
            "    >>> @data(number_of_databases=2)",
            "    >>> def my_algorithm(first_df: pd.DataFrame, second_df: pd.DataFrame,",
            "    >>>                  <other arguments>):",
            "    >>>     pass",
            "    \"\"\"",
            "    def protection_decorator(func: callable, *args, **kwargs) -> callable:",
            "        @wraps(func)",
            "        def decorator(*args, mock_data: list[pd.DataFrame] = None,",
            "                      **kwargs) -> callable:",
            "            \"\"\"",
            "            Wrap the function with the data",
            "",
            "            Parameters",
            "            ----------",
            "            mock_data : list[pd.DataFrame]",
            "                Mock data to use instead of the regular data",
            "            \"\"\"",
            "            if mock_data is not None:",
            "                return func(*mock_data, *args, **kwargs)",
            "",
            "            # read the labels that the user requested",
            "            labels = _get_user_database_labels()",
            "",
            "            # check if user provided enough databases",
            "            if len(labels) < number_of_databases:",
            "                error(f\"Algorithm requires {number_of_databases} databases \"",
            "                      f\"but only {len(labels)} were provided. \"",
            "                      \"Exiting...\")",
            "                exit(1)",
            "            elif len(labels) > number_of_databases:",
            "                warn(f\"Algorithm requires only {number_of_databases} databases\"",
            "                     f\", but {len(labels)} were provided. Using the \"",
            "                     f\"first {number_of_databases} databases.\")",
            "",
            "            for i in range(number_of_databases):",
            "                label = labels[i]",
            "                # read the data from the database",
            "                info(\"Reading data from database\")",
            "                data_ = _get_data_from_label(label)",
            "",
            "                # do any data preprocessing here",
            "                info(f\"Applying preprocessing for database '{label}'\")",
            "                env_prepro = get_env_var(f\"{label.upper()}_PREPROCESSING\")",
            "                if env_prepro is not None:",
            "                    preprocess = json.loads(env_prepro)",
            "                    data_ = preprocess_data(data_, preprocess)",
            "",
            "                # add the data to the arguments",
            "                args = (data_, *args)",
            "",
            "            return func(*args, **kwargs)",
            "        # set attribute that this function is wrapped in a data decorator",
            "        decorator.wrapped_in_data_decorator = True",
            "        return decorator",
            "    return protection_decorator",
            "",
            "",
            "def database_connection(types: list[str], include_metadata: bool = True) \\",
            "        -> callable:",
            "    \"\"\"",
            "    Decorator that adds a database connection to a function",
            "",
            "    By adding @database_connection to a function, a database connection will",
            "    be added to the front of the argument list. This connection can be used to",
            "    communicate with the database.",
            "",
            "    Parameters",
            "    ----------",
            "    types : list[str]",
            "        List of types of databases to connect to. Currently only \"OMOP\" is",
            "        supported.",
            "    include_metadata : bool",
            "        Whether to include metadata in the function arguments. This metadata",
            "        contains the database name, CDM schema, and results schema. Default is",
            "        True.",
            "",
            "    Example",
            "    -------",
            "    For a single OMOP data source:",
            "    >>> @database_connection(types=[\"OMOP\"])",
            "    >>> def my_algorithm(connection: Connection, meta: OHDSIMetaData,",
            "    >>>                  <other arguments>):",
            "    >>>     pass",
            "",
            "    In case you have multiple OMOP data sources:",
            "    >>> @database_connection(types=[\"OMOP\", \"OMOP\"])",
            "    >>> def my_algorithm(connection1: Connection, meta1: OHDSIMetaData,",
            "    >>>                  connection2: Connection, meta2: OHDSIMetaData,",
            "    >>>                  <other arguments>):",
            "    >>>     pass",
            "",
            "    In the case you do not want to include the metadata:",
            "    >>> @database_connection(types=[\"OMOP\"], include_metadata=False)",
            "    >>> def my_algorithm(connection: Connection, <other arguments>):",
            "    >>>     pass",
            "    \"\"\"",
            "    def connection_decorator(func: callable, *args, **kwargs) -> callable:",
            "        @wraps(func)",
            "        def decorator(*args, **kwargs) -> callable:",
            "            \"\"\"",
            "            Wrap the function with the database connection",
            "            \"\"\"",
            "            labels = _get_user_database_labels()",
            "            if len(labels) < len(types):",
            "                error(f\"User provided {len(labels)} databases, but algorithm \"",
            "                      f\"requires {len(types)} database connections. Exiting.\")",
            "                exit(1)",
            "            if len(labels) > len(types):",
            "                warn(f\"User provided {len(labels)} databases, but algorithm \"",
            "                     f\"requires {len(types)} database connections. Using the \"",
            "                     f\"first {len(types)} databases.\")",
            "",
            "            db_args = []",
            "            # Note: zip will stop at the shortest iterable, so this is exactly",
            "            # what we want in the len(labels) > len(types) case.",
            "            for type_, label in zip(types, labels):",
            "                match type_.upper():",
            "                    case \"OMOP\":",
            "                        info(\"Creating OMOP database connection\")",
            "                        connection = _create_omop_database_connection(label)",
            "                        db_args.append(connection)",
            "                        if include_metadata:",
            "                            meta = get_ohdsi_metadata(label)",
            "                            db_args.append(meta)",
            "                    # case \"FHIR\":",
            "                    #     pass",
            "",
            "            return func(*db_args, *args, **kwargs)",
            "",
            "        return decorator",
            "    return connection_decorator",
            "",
            "",
            "def metadata(func: callable) -> callable:",
            "    @wraps(func)",
            "    def decorator(*args, **kwargs) -> callable:",
            "        \"\"\"",
            "        Decorator the function with metadata from the run.",
            "",
            "        Decorator that adds metadata from the run to the function. This",
            "        includes the task id, node id, collaboration id, organization id,",
            "        temporary directory, output file, input file, and token file.",
            "",
            "        Example",
            "        -------",
            "        >>> @metadata",
            "        >>> def my_algorithm(metadata: RunMetaData, <other arguments>):",
            "        >>>     pass",
            "        \"\"\"",
            "        token_file = get_env_var(\"TOKEN_FILE\")",
            "        info(\"Reading token\")",
            "        with open(token_file) as fp:",
            "            token = fp.read().strip()",
            "",
            "        info(\"Extracting payload from token\")",
            "        payload = _extract_token_payload(token)",
            "",
            "        metadata = RunMetaData(",
            "            task_id=payload[\"task_id\"],",
            "            node_id=payload[\"node_id\"],",
            "            collaboration_id=payload[\"collaboration_id\"],",
            "            organization_id=payload[\"organization_id\"],",
            "            temporary_directory=Path(get_env_var(\"TEMPORARY_FOLDER\")),",
            "            output_file=Path(get_env_var(\"OUTPUT_FILE\")),",
            "            input_file=Path(get_env_var(\"INPUT_FILE\")),",
            "            token_file=Path(get_env_var(\"TOKEN_FILE\"))",
            "        )",
            "        return func(metadata, *args, **kwargs)",
            "    return decorator",
            "",
            "",
            "def get_ohdsi_metadata(label: str) -> OHDSIMetaData:",
            "    \"\"\"",
            "    Retrieve the OHDSI metadata from the environment variables.",
            "",
            "    The following environment variables are expected to be set in the",
            "    node configuration in the `env` key of the `database` section:",
            "",
            "    - `CDM_DATABASE`",
            "    - `CDM_SCHEMA`",
            "    - `RESULTS_SCHEMA`",
            "",
            "    In case these are not set, the algorithm execution is terminated.",
            "",
            "    Example",
            "    -------",
            "    >>> get_ohdsi_metadata(\"my_database\")",
            "    \"\"\"",
            "    # check that all node environment variables are set",
            "    expected_env_vars = [\"CDM_DATABASE\", \"CDM_SCHEMA\", \"RESULTS_SCHEMA\"]",
            "    label_ = label.upper()",
            "    for var in expected_env_vars:",
            "        _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')",
            "",
            "    tmp = Path(get_env_var(\"TEMPORARY_FOLDER\"))",
            "    metadata = OHDSIMetaData(",
            "        database=get_env_var(f\"{label_}_DB_PARAM_CDM_DATABASE\"),",
            "        cdm_schema=get_env_var(f\"{label_}_DB_PARAM_CDM_SCHEMA\"),",
            "        results_schema=get_env_var(f\"{label_}_DB_PARAM_RESULTS_SCHEMA\"),",
            "        incremental_folder=tmp / \"incremental\",",
            "        cohort_statistics_folder=tmp / \"cohort_statistics\",",
            "        export_folder=tmp / \"export\"",
            "    )",
            "    return metadata",
            "",
            "",
            "def _create_omop_database_connection(label: str) -> callable:",
            "    \"\"\"",
            "    Create a connection to an OMOP database.",
            "",
            "    It expects that the following environment variables are set:",
            "    - DB_PARAM_DBMS: type of database to connect to",
            "    - DB_PARAM_USER: username to connect to the database",
            "    - DB_PARAM_PASSWORD: password to connect to the database",
            "",
            "    These should be provided in the vantage6 node configuration file in the",
            "    `database` section without the `DB_PARAM_` prefix. For example:",
            "",
            "    ```yaml",
            "    ...",
            "    databases:",
            "      - label: my_database",
            "        type: OMOP",
            "        uri: jdbc:postgresql://host.docker.internal:5454/postgres",
            "        env:",
            "            DBMS: \"postgresql\"",
            "            USER: \"my_user\"",
            "            PASSWORD: \"my_password\"",
            "    ...",
            "    ```",
            "",
            "    Parameters",
            "    ----------",
            "    label : str",
            "        Label of the database to connect to",
            "",
            "    Returns",
            "    -------",
            "    callable",
            "        OHDSI Database Connection object",
            "    \"\"\"",
            "",
            "    # check that the OHDSI package is available in this container",
            "    if not OHDSI_AVAILABLE:",
            "        error(\"OHDSI/DatabaseConnector is not available.\")",
            "        error(\"Did you use 'algorithm-ohdsi-base' image to build this \"",
            "              \"algorithm?\")",
            "        exit(1)",
            "",
            "    # environment vars are always uppercase",
            "    label_ = label.upper()",
            "",
            "    # check that the required environment variables are set",
            "    for var in (\"DBMS\", \"USER\", \"PASSWORD\"):",
            "        _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')",
            "",
            "    info(\"Reading OHDSI environment variables\")",
            "    dbms = get_env_var(f\"{label_}_DB_PARAM_DBMS\")",
            "    uri = get_env_var(f\"{label_}_DATABASE_URI\")",
            "    user = get_env_var(f\"{label_}_DB_PARAM_USER\")",
            "    password = get_env_var(f\"{label_}_DB_PARAM_PASSWORD\")",
            "    info(f' - dbms: {dbms}')",
            "    info(f' - uri: {uri}')",
            "    info(f' - user: {user}')",
            "",
            "    info(\"Creating OHDSI database connection\")",
            "    return connect_to_omop(dbms=dbms, connection_string=uri, password=password,",
            "                           user=user)",
            "",
            "",
            "def _check_environment_var_exists_or_exit(var: str):",
            "    \"\"\"",
            "    Check if the environment variable 'var' exists or print and exit.",
            "",
            "    Parameters",
            "    ----------",
            "    var : str",
            "        Environment variable name to check",
            "    \"\"\"",
            "    if var not in os.environ:",
            "        error(f\"Environment variable '{var}' is not set. Exiting...\")",
            "        exit(1)",
            "",
            "",
            "def _get_data_from_label(label: str) -> pd.DataFrame:",
            "    \"\"\"",
            "    Load data from a database based on the label",
            "",
            "    Parameters",
            "    ----------",
            "    label : str",
            "        Label of the database to load",
            "",
            "    Returns",
            "    -------",
            "    pd.DataFrame",
            "        Data from the database",
            "    \"\"\"",
            "    # Load the input data from the input file - this may e.g. include the",
            "    database_uri = get_env_var(f\"{label.upper()}_DATABASE_URI\")",
            "    info(f\"Using '{database_uri}' with label '{label}' as database\")",
            "",
            "    # Get the database type from the environment variable, this variable is",
            "    # set by the vantage6 node based on its configuration file.",
            "    database_type = get_env_var(",
            "        f\"{label.upper()}_DATABASE_TYPE\", \"csv\").lower()",
            "",
            "    # Load the data based on the database type. Try to provide environment",
            "    # variables that should be available for some data types.",
            "    return load_data(",
            "        database_uri,",
            "        database_type,",
            "        query=get_env_var(f\"{label.upper()}_QUERY\"),",
            "        sheet_name=get_env_var(f\"{label.upper()}_SHEET_NAME\")",
            "    )",
            "",
            "",
            "def _get_user_database_labels() -> list[str]:",
            "    \"\"\"",
            "    Get the database labels from the environment",
            "",
            "    Returns",
            "    -------",
            "    list[str]",
            "        List of database labels",
            "    \"\"\"",
            "    # read the labels that the user requested, which is a comma",
            "    # separated list of labels.",
            "    labels = get_env_var(\"USER_REQUESTED_DATABASE_LABELS\")",
            "    return labels.split(',')",
            "",
            "",
            "def _extract_token_payload(token: str) -> dict:",
            "    \"\"\"",
            "    Extract the payload from the token.",
            "",
            "    Parameters",
            "    ----------",
            "    token: str",
            "        The token as a string.",
            "",
            "    Returns",
            "    -------",
            "    dict",
            "        The payload as a dictionary. It contains the keys: `client_type`,",
            "        `node_id`, `organization_id`, `collaboration_id`, `task_id`, `image`,",
            "        and `databases`.",
            "    \"\"\"",
            "    jwt_payload = jwt.decode(token, options={\"verify_signature\": False})",
            "    return jwt_payload['sub']"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "13": [],
            "92": [
                "_algorithm_client",
                "protection_decorator",
                "decorator"
            ],
            "93": [
                "_algorithm_client",
                "protection_decorator",
                "decorator"
            ],
            "94": [
                "_algorithm_client",
                "protection_decorator",
                "decorator"
            ],
            "97": [
                "_algorithm_client",
                "protection_decorator",
                "decorator"
            ],
            "187": [
                "data",
                "protection_decorator",
                "decorator"
            ],
            "294": [
                "metadata",
                "decorator"
            ],
            "307": [
                "metadata",
                "decorator"
            ],
            "308": [
                "metadata",
                "decorator"
            ],
            "309": [
                "metadata",
                "decorator"
            ],
            "310": [
                "metadata",
                "decorator"
            ],
            "339": [
                "get_ohdsi_metadata"
            ],
            "341": [
                "get_ohdsi_metadata"
            ],
            "342": [
                "get_ohdsi_metadata"
            ],
            "343": [
                "get_ohdsi_metadata"
            ],
            "402": [
                "_create_omop_database_connection"
            ],
            "403": [
                "_create_omop_database_connection"
            ],
            "404": [
                "_create_omop_database_connection"
            ],
            "405": [
                "_create_omop_database_connection"
            ],
            "444": [
                "_get_data_from_label"
            ],
            "449": [
                "_get_data_from_label"
            ],
            "457": [
                "_get_data_from_label"
            ],
            "458": [
                "_get_data_from_label"
            ],
            "473": [
                "_get_user_database_labels"
            ]
        },
        "addLocation": []
    },
    "vantage6-algorithm-tools/vantage6/algorithm/tools/util.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import sys"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+import os"
            },
            "2": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3,
                "PatchRowcode": "+import base64"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 4,
                "PatchRowcode": "+from vantage6.common.globals import STRING_ENCODING, ENV_VAR_EQUALS_REPLACEMENT"
            },
            "4": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " def info(msg: str) -> None:"
            },
            "7": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "         Error message to be printed"
            },
            "8": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "     \"\"\""
            },
            "9": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "     sys.stdout.write(f\"error > {msg}\\n\")"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+def get_env_var(var_name: str, default: str | None = None) -> str:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+    \"\"\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    Get the value of an environment variable. Environment variables are encoded"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+    by the node so they need to be decoded here."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+    Note that this decoding follows the reverse of the encoding in the node:"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+    first replace '=' back and then decode the base32 string."
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+    Parameters"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+    ----------"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+    var_name : str"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+        Name of the environment variable"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+    Returns"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+    -------"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+    var_value : str | None"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+        Value of the environment variable, or default value if not found"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+    \"\"\""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+    try:"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+        encoded_env_var_value = \\"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+            os.environ[var_name].replace("
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+                ENV_VAR_EQUALS_REPLACEMENT, \"=\""
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+            ).encode(STRING_ENCODING)"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+        return base64.b32decode(encoded_env_var_value).decode(STRING_ENCODING)"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+    except KeyError:"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+        return default"
            }
        },
        "frontPatchFile": [
            "import sys",
            "",
            "",
            "def info(msg: str) -> None:",
            "    \"\"\"",
            "    Print an info message to stdout.",
            "",
            "    Parameters",
            "    ----------",
            "    msg : str",
            "        Message to be printed",
            "    \"\"\"",
            "    sys.stdout.write(f\"info > {msg}\\n\")",
            "",
            "",
            "def warn(msg: str) -> None:",
            "    \"\"\"",
            "    Print a warning message to stdout.",
            "",
            "    Parameters",
            "    ----------",
            "    msg : str",
            "        Warning message to be printed",
            "    \"\"\"",
            "    sys.stdout.write(f\"warn > {msg}\\n\")",
            "",
            "",
            "def error(msg: str) -> None:",
            "    \"\"\"",
            "    Print an error message to stdout.",
            "",
            "    Parameters",
            "    ----------",
            "    msg : str",
            "        Error message to be printed",
            "    \"\"\"",
            "    sys.stdout.write(f\"error > {msg}\\n\")"
        ],
        "afterPatchFile": [
            "import sys",
            "import os",
            "import base64",
            "from vantage6.common.globals import STRING_ENCODING, ENV_VAR_EQUALS_REPLACEMENT",
            "",
            "",
            "def info(msg: str) -> None:",
            "    \"\"\"",
            "    Print an info message to stdout.",
            "",
            "    Parameters",
            "    ----------",
            "    msg : str",
            "        Message to be printed",
            "    \"\"\"",
            "    sys.stdout.write(f\"info > {msg}\\n\")",
            "",
            "",
            "def warn(msg: str) -> None:",
            "    \"\"\"",
            "    Print a warning message to stdout.",
            "",
            "    Parameters",
            "    ----------",
            "    msg : str",
            "        Warning message to be printed",
            "    \"\"\"",
            "    sys.stdout.write(f\"warn > {msg}\\n\")",
            "",
            "",
            "def error(msg: str) -> None:",
            "    \"\"\"",
            "    Print an error message to stdout.",
            "",
            "    Parameters",
            "    ----------",
            "    msg : str",
            "        Error message to be printed",
            "    \"\"\"",
            "    sys.stdout.write(f\"error > {msg}\\n\")",
            "",
            "",
            "def get_env_var(var_name: str, default: str | None = None) -> str:",
            "    \"\"\"",
            "    Get the value of an environment variable. Environment variables are encoded",
            "    by the node so they need to be decoded here.",
            "",
            "    Note that this decoding follows the reverse of the encoding in the node:",
            "    first replace '=' back and then decode the base32 string.",
            "",
            "    Parameters",
            "    ----------",
            "    var_name : str",
            "        Name of the environment variable",
            "",
            "    Returns",
            "    -------",
            "    var_value : str | None",
            "        Value of the environment variable, or default value if not found",
            "    \"\"\"",
            "    try:",
            "        encoded_env_var_value = \\",
            "            os.environ[var_name].replace(",
            "                ENV_VAR_EQUALS_REPLACEMENT, \"=\"",
            "            ).encode(STRING_ENCODING)",
            "        return base64.b32decode(encoded_env_var_value).decode(STRING_ENCODING)",
            "    except KeyError:",
            "        return default"
        ],
        "action": [
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "aiosmtpd.smtp.SMTP"
        ]
    },
    "vantage6-algorithm-tools/vantage6/algorithm/tools/wrap.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from vantage6.common.client import deserialization"
            },
            "2": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from vantage6.common import serialization"
            },
            "3": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from vantage6.algorithm.tools.util import info, error"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+from vantage6.algorithm.tools.util import info, error, get_env_var"
            },
            "5": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from vantage6.algorithm.tools.exceptions import DeserializationException"
            },
            "6": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def wrap_algorithm(module: str, log_traceback: bool = True) -> None:"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 13,
                "PatchRowcode": "+def wrap_algorithm(log_traceback: bool = True) -> None:"
            },
            "10": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": "     \"\"\""
            },
            "11": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": "     Wrap an algorithm module to provide input and output handling for the"
            },
            "12": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": "     vantage6 infrastructure."
            },
            "13": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "         default False. Algorithm developers should set this to False if"
            },
            "14": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "         the error messages may contain sensitive information. By default True."
            },
            "15": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "     \"\"\""
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+    # get the module name from the environment variable. Note that this env var"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    # is set in the Dockerfile and is therefore not encoded."
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+    module = os.environ.get(\"PKG_NAME\")"
            },
            "19": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "     info(f\"wrapper for {module}\")"
            },
            "20": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "     # read input from the mounted input file."
            },
            "22": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    input_file = os.environ[\"INPUT_FILE\"]"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+    input_file = get_env_var(\"INPUT_FILE\")"
            },
            "24": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "     info(f\"Reading input file {input_file}\")"
            },
            "25": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "     input_data = load_input(input_file)"
            },
            "26": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 57,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "     # write output from the method to mounted output file. Which will be"
            },
            "29": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "     # transferred back to the server by the node-instance."
            },
            "30": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    output_file = os.environ[\"OUTPUT_FILE\"]"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+    output_file = get_env_var(\"OUTPUT_FILE\")"
            },
            "32": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "     info(f\"Writing output to {output_file}\")"
            },
            "33": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 62,
                "PatchRowcode": " "
            },
            "34": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "     _write_output(output, output_file)"
            }
        },
        "frontPatchFile": [
            "import os",
            "import importlib",
            "import traceback",
            "",
            "from typing import Any",
            "",
            "from vantage6.common.client import deserialization",
            "from vantage6.common import serialization",
            "from vantage6.algorithm.tools.util import info, error",
            "from vantage6.algorithm.tools.exceptions import DeserializationException",
            "",
            "",
            "def wrap_algorithm(module: str, log_traceback: bool = True) -> None:",
            "    \"\"\"",
            "    Wrap an algorithm module to provide input and output handling for the",
            "    vantage6 infrastructure.",
            "",
            "    Data is received in the form of files, whose location should be",
            "    specified in the following environment variables:",
            "",
            "    - ``INPUT_FILE``: input arguments for the algorithm. This file should be",
            "      encoded in JSON format.",
            "    - ``OUTPUT_FILE``: location where the results of the algorithm should",
            "      be stored",
            "    - ``TOKEN_FILE``: access token for the vantage6 server REST api",
            "    - ``USER_REQUESTED_DATABASE_LABELS``: comma-separated list of database",
            "      labels that the user requested",
            "    - ``<DB_LABEL>_DATABASE_URI``: uri of the each of the databases that",
            "      the user requested, where ``<DB_LABEL>`` is the label of the",
            "      database given in ``USER_REQUESTED_DATABASE_LABELS``.",
            "",
            "    The wrapper expects the input file to be a json file. Any other file",
            "    format will result in an error.",
            "",
            "    Parameters",
            "    ----------",
            "    module : str",
            "        Python module name of the algorithm to wrap.",
            "    log_traceback: bool",
            "        Whether to print the full error message from algorithms or not, by",
            "        default False. Algorithm developers should set this to False if",
            "        the error messages may contain sensitive information. By default True.",
            "    \"\"\"",
            "    info(f\"wrapper for {module}\")",
            "",
            "    # read input from the mounted input file.",
            "    input_file = os.environ[\"INPUT_FILE\"]",
            "    info(f\"Reading input file {input_file}\")",
            "    input_data = load_input(input_file)",
            "",
            "    # make the actual call to the method/function",
            "    info(\"Dispatching ...\")",
            "    output = _run_algorithm_method(input_data, module, log_traceback)",
            "",
            "    # write output from the method to mounted output file. Which will be",
            "    # transferred back to the server by the node-instance.",
            "    output_file = os.environ[\"OUTPUT_FILE\"]",
            "    info(f\"Writing output to {output_file}\")",
            "",
            "    _write_output(output, output_file)",
            "",
            "",
            "def _run_algorithm_method(input_data: dict, module: str,",
            "                          log_traceback: bool = True) -> Any:",
            "    \"\"\"",
            "    Load the algorithm module and call the correct method to run an algorithm.",
            "",
            "    Parameters",
            "    ----------",
            "    input_data : dict",
            "        The input data that is passed to the algorithm. This should at least",
            "        contain the key 'method' which is the name of the method that should be",
            "        called. Other keys depend on the algorithm.",
            "    module : str",
            "        The module that contains the algorithm.",
            "    log_traceback: bool, optional",
            "        Whether to print the full error message from algorithms or not, by",
            "        default False. Algorithm developers should set this to False if",
            "        the error messages may contain sensitive information. By default True.",
            "",
            "    Returns",
            "    -------",
            "    Any",
            "        The result of the algorithm.",
            "    \"\"\"",
            "    # import algorithm module",
            "    try:",
            "        lib = importlib.import_module(module)",
            "        info(f\"Module '{module}' imported!\")",
            "    except ModuleNotFoundError:",
            "        error(f\"Module '{module}' can not be imported! Exiting...\")",
            "        exit(1)",
            "",
            "    # get algorithm method and attempt to load it",
            "    method_name = input_data[\"method\"]",
            "    try:",
            "        method = getattr(lib, method_name)",
            "    except AttributeError:",
            "        error(f\"Method '{method_name}' not found!\\n\")",
            "        exit(1)",
            "",
            "    # get the args and kwargs input for this function.",
            "    args = input_data.get(\"args\", [])",
            "    kwargs = input_data.get(\"kwargs\", {})",
            "",
            "    # try to run the method",
            "    try:",
            "        result = method(*args, **kwargs)",
            "    except Exception as e:",
            "        error(f\"Error encountered while calling {method_name}: {e}\")",
            "        if log_traceback:",
            "            error(traceback.print_exc())",
            "        exit(1)",
            "",
            "    return result",
            "",
            "",
            "def load_input(input_file: str) -> Any:",
            "    \"\"\"",
            "    Load the input from the input file.",
            "",
            "    Parameters",
            "    ----------",
            "    input_file : str",
            "        File containing the input",
            "",
            "    Returns",
            "    -------",
            "    input_data : Any",
            "        Input data for the algorithm",
            "",
            "    Raises",
            "    ------",
            "    DeserializationException",
            "        Failed to deserialize input data",
            "    \"\"\"",
            "    with open(input_file, \"rb\") as fp:",
            "        try:",
            "            input_data = deserialization.deserialize(fp)",
            "        except DeserializationException:",
            "            raise DeserializationException('Could not deserialize input')",
            "    return input_data",
            "",
            "",
            "def _write_output(output: Any, output_file: str) -> None:",
            "    \"\"\"",
            "    Write output to output file using JSON serialization.",
            "",
            "    Parameters",
            "    ----------",
            "    output : Any",
            "        Output of the algorithm",
            "    output_file : str",
            "        Path to the output file",
            "    \"\"\"",
            "    with open(output_file, 'wb') as fp:",
            "        serialized = serialization.serialize(output)",
            "        fp.write(serialized)"
        ],
        "afterPatchFile": [
            "import os",
            "import importlib",
            "import traceback",
            "",
            "from typing import Any",
            "",
            "from vantage6.common.client import deserialization",
            "from vantage6.common import serialization",
            "from vantage6.algorithm.tools.util import info, error, get_env_var",
            "from vantage6.algorithm.tools.exceptions import DeserializationException",
            "",
            "",
            "def wrap_algorithm(log_traceback: bool = True) -> None:",
            "    \"\"\"",
            "    Wrap an algorithm module to provide input and output handling for the",
            "    vantage6 infrastructure.",
            "",
            "    Data is received in the form of files, whose location should be",
            "    specified in the following environment variables:",
            "",
            "    - ``INPUT_FILE``: input arguments for the algorithm. This file should be",
            "      encoded in JSON format.",
            "    - ``OUTPUT_FILE``: location where the results of the algorithm should",
            "      be stored",
            "    - ``TOKEN_FILE``: access token for the vantage6 server REST api",
            "    - ``USER_REQUESTED_DATABASE_LABELS``: comma-separated list of database",
            "      labels that the user requested",
            "    - ``<DB_LABEL>_DATABASE_URI``: uri of the each of the databases that",
            "      the user requested, where ``<DB_LABEL>`` is the label of the",
            "      database given in ``USER_REQUESTED_DATABASE_LABELS``.",
            "",
            "    The wrapper expects the input file to be a json file. Any other file",
            "    format will result in an error.",
            "",
            "    Parameters",
            "    ----------",
            "    module : str",
            "        Python module name of the algorithm to wrap.",
            "    log_traceback: bool",
            "        Whether to print the full error message from algorithms or not, by",
            "        default False. Algorithm developers should set this to False if",
            "        the error messages may contain sensitive information. By default True.",
            "    \"\"\"",
            "    # get the module name from the environment variable. Note that this env var",
            "    # is set in the Dockerfile and is therefore not encoded.",
            "    module = os.environ.get(\"PKG_NAME\")",
            "    info(f\"wrapper for {module}\")",
            "",
            "    # read input from the mounted input file.",
            "    input_file = get_env_var(\"INPUT_FILE\")",
            "    info(f\"Reading input file {input_file}\")",
            "    input_data = load_input(input_file)",
            "",
            "    # make the actual call to the method/function",
            "    info(\"Dispatching ...\")",
            "    output = _run_algorithm_method(input_data, module, log_traceback)",
            "",
            "    # write output from the method to mounted output file. Which will be",
            "    # transferred back to the server by the node-instance.",
            "    output_file = get_env_var(\"OUTPUT_FILE\")",
            "    info(f\"Writing output to {output_file}\")",
            "",
            "    _write_output(output, output_file)",
            "",
            "",
            "def _run_algorithm_method(input_data: dict, module: str,",
            "                          log_traceback: bool = True) -> Any:",
            "    \"\"\"",
            "    Load the algorithm module and call the correct method to run an algorithm.",
            "",
            "    Parameters",
            "    ----------",
            "    input_data : dict",
            "        The input data that is passed to the algorithm. This should at least",
            "        contain the key 'method' which is the name of the method that should be",
            "        called. Other keys depend on the algorithm.",
            "    module : str",
            "        The module that contains the algorithm.",
            "    log_traceback: bool, optional",
            "        Whether to print the full error message from algorithms or not, by",
            "        default False. Algorithm developers should set this to False if",
            "        the error messages may contain sensitive information. By default True.",
            "",
            "    Returns",
            "    -------",
            "    Any",
            "        The result of the algorithm.",
            "    \"\"\"",
            "    # import algorithm module",
            "    try:",
            "        lib = importlib.import_module(module)",
            "        info(f\"Module '{module}' imported!\")",
            "    except ModuleNotFoundError:",
            "        error(f\"Module '{module}' can not be imported! Exiting...\")",
            "        exit(1)",
            "",
            "    # get algorithm method and attempt to load it",
            "    method_name = input_data[\"method\"]",
            "    try:",
            "        method = getattr(lib, method_name)",
            "    except AttributeError:",
            "        error(f\"Method '{method_name}' not found!\\n\")",
            "        exit(1)",
            "",
            "    # get the args and kwargs input for this function.",
            "    args = input_data.get(\"args\", [])",
            "    kwargs = input_data.get(\"kwargs\", {})",
            "",
            "    # try to run the method",
            "    try:",
            "        result = method(*args, **kwargs)",
            "    except Exception as e:",
            "        error(f\"Error encountered while calling {method_name}: {e}\")",
            "        if log_traceback:",
            "            error(traceback.print_exc())",
            "        exit(1)",
            "",
            "    return result",
            "",
            "",
            "def load_input(input_file: str) -> Any:",
            "    \"\"\"",
            "    Load the input from the input file.",
            "",
            "    Parameters",
            "    ----------",
            "    input_file : str",
            "        File containing the input",
            "",
            "    Returns",
            "    -------",
            "    input_data : Any",
            "        Input data for the algorithm",
            "",
            "    Raises",
            "    ------",
            "    DeserializationException",
            "        Failed to deserialize input data",
            "    \"\"\"",
            "    with open(input_file, \"rb\") as fp:",
            "        try:",
            "            input_data = deserialization.deserialize(fp)",
            "        except DeserializationException:",
            "            raise DeserializationException('Could not deserialize input')",
            "    return input_data",
            "",
            "",
            "def _write_output(output: Any, output_file: str) -> None:",
            "    \"\"\"",
            "    Write output to output file using JSON serialization.",
            "",
            "    Parameters",
            "    ----------",
            "    output : Any",
            "        Output of the algorithm",
            "    output_file : str",
            "        Path to the output file",
            "    \"\"\"",
            "    with open(output_file, 'wb') as fp:",
            "        serialized = serialization.serialize(output)",
            "        fp.write(serialized)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "9": [],
            "13": [
                "wrap_algorithm"
            ],
            "47": [
                "wrap_algorithm"
            ],
            "57": [
                "wrap_algorithm"
            ]
        },
        "addLocation": []
    },
    "vantage6-common/vantage6/common/globals.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " # The basics image can be used (mainly by the UI) to collect column names"
            },
            "2": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " BASIC_PROCESSING_IMAGE = 'harbor2.vantage6.ai/algorithms/basics'"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+# Character to replace '=' with in encoded environment variables"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+ENV_VAR_EQUALS_REPLACEMENT = \"!\""
            }
        },
        "frontPatchFile": [
            "from pathlib import Path",
            "",
            "#",
            "#   PACKAGE GLOBALS",
            "#",
            "STRING_ENCODING = \"utf-8\"",
            "",
            "APPNAME = \"vantage6\"",
            "",
            "MAIN_VERSION_NAME = \"cotopaxi\"",
            "",
            "DEFAULT_DOCKER_REGISTRY = \"harbor2.vantage6.ai\"",
            "",
            "DEFAULT_NODE_IMAGE = f\"infrastructure/node:{MAIN_VERSION_NAME}\"",
            "",
            "DEFAULT_NODE_IMAGE_WO_TAG = \"infrastructure/node\"",
            "",
            "DEFAULT_SERVER_IMAGE = f\"infrastructure/server:{MAIN_VERSION_NAME}\"",
            "",
            "DEFAULT_UI_IMAGE = f\"infrastructure/ui:{MAIN_VERSION_NAME}\"",
            "",
            "#",
            "#   COMMON GLOBALS",
            "#",
            "PACKAGE_FOLDER = Path(__file__).parent.parent.parent",
            "",
            "VPN_CONFIG_FILE = 'vpn-config.ovpn.conf'",
            "",
            "DATABASE_TYPES = [\"csv\", \"parquet\", \"sql\", \"sparql\", \"omop\", \"excel\", \"other\"]",
            "",
            "PING_INTERVAL_SECONDS = 60",
            "",
            "# start trying to refresh the JWT token of the node 10 minutes before it",
            "# expires.",
            "NODE_CLIENT_REFRESH_BEFORE_EXPIRES_SECONDS = 600",
            "",
            "# The basics image can be used (mainly by the UI) to collect column names",
            "BASIC_PROCESSING_IMAGE = 'harbor2.vantage6.ai/algorithms/basics'"
        ],
        "afterPatchFile": [
            "from pathlib import Path",
            "",
            "#",
            "#   PACKAGE GLOBALS",
            "#",
            "STRING_ENCODING = \"utf-8\"",
            "",
            "APPNAME = \"vantage6\"",
            "",
            "MAIN_VERSION_NAME = \"cotopaxi\"",
            "",
            "DEFAULT_DOCKER_REGISTRY = \"harbor2.vantage6.ai\"",
            "",
            "DEFAULT_NODE_IMAGE = f\"infrastructure/node:{MAIN_VERSION_NAME}\"",
            "",
            "DEFAULT_NODE_IMAGE_WO_TAG = \"infrastructure/node\"",
            "",
            "DEFAULT_SERVER_IMAGE = f\"infrastructure/server:{MAIN_VERSION_NAME}\"",
            "",
            "DEFAULT_UI_IMAGE = f\"infrastructure/ui:{MAIN_VERSION_NAME}\"",
            "",
            "#",
            "#   COMMON GLOBALS",
            "#",
            "PACKAGE_FOLDER = Path(__file__).parent.parent.parent",
            "",
            "VPN_CONFIG_FILE = 'vpn-config.ovpn.conf'",
            "",
            "DATABASE_TYPES = [\"csv\", \"parquet\", \"sql\", \"sparql\", \"omop\", \"excel\", \"other\"]",
            "",
            "PING_INTERVAL_SECONDS = 60",
            "",
            "# start trying to refresh the JWT token of the node 10 minutes before it",
            "# expires.",
            "NODE_CLIENT_REFRESH_BEFORE_EXPIRES_SECONDS = 600",
            "",
            "# The basics image can be used (mainly by the UI) to collect column names",
            "BASIC_PROCESSING_IMAGE = 'harbor2.vantage6.ai/algorithms/basics'",
            "",
            "# Character to replace '=' with in encoded environment variables",
            "ENV_VAR_EQUALS_REPLACEMENT = \"!\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "vantage6-common/vantage6/common/serialization.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import json"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+from vantage6.common.globals import STRING_ENCODING"
            },
            "2": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " # TODO BvB 2023-02-03: I feel this function could be given a better name. And"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": "     bytes"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": "         A JSON-serialized and then encoded bytes object representing the data"
            },
            "7": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": "     \"\"\""
            },
            "8": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return json.dumps(data).encode()"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+    return json.dumps(data).encode(STRING_ENCODING)"
            }
        },
        "frontPatchFile": [
            "import json",
            "",
            "",
            "# TODO BvB 2023-02-03: I feel this function could be given a better name. And",
            "# it might not have to be in a separate file.",
            "def serialize(data: any) -> bytes:",
            "    \"\"\"",
            "    Serialize data using the specified format",
            "",
            "    Parameters",
            "    ----------",
            "    data: any",
            "        The data to be serialized",
            "",
            "    Returns",
            "    -------",
            "    bytes",
            "        A JSON-serialized and then encoded bytes object representing the data",
            "    \"\"\"",
            "    return json.dumps(data).encode()"
        ],
        "afterPatchFile": [
            "import json",
            "from vantage6.common.globals import STRING_ENCODING",
            "",
            "",
            "# TODO BvB 2023-02-03: I feel this function could be given a better name. And",
            "# it might not have to be in a separate file.",
            "def serialize(data: any) -> bytes:",
            "    \"\"\"",
            "    Serialize data using the specified format",
            "",
            "    Parameters",
            "    ----------",
            "    data: any",
            "        The data to be serialized",
            "",
            "    Returns",
            "    -------",
            "    bytes",
            "        A JSON-serialized and then encoded bytes object representing the data",
            "    \"\"\"",
            "    return json.dumps(data).encode(STRING_ENCODING)"
        ],
        "action": [
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "20": [
                "serialize"
            ]
        },
        "addLocation": []
    },
    "vantage6-node/vantage6/node/docker/docker_manager.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 485,
                "afterPatchRowNumber": 485,
                "PatchRowcode": "                         self.active_tasks.remove(task)"
            },
            "1": {
                "beforePatchRowNumber": 486,
                "afterPatchRowNumber": 486,
                "PatchRowcode": "                         break"
            },
            "2": {
                "beforePatchRowNumber": 487,
                "afterPatchRowNumber": 487,
                "PatchRowcode": "                 except AlgorithmContainerNotFound:"
            },
            "3": {
                "beforePatchRowNumber": 488,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.log.exception(f'Failed to find container for '"
            },
            "4": {
                "beforePatchRowNumber": 489,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                       f'result {task.result_id}')"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 488,
                "PatchRowcode": "+                    self.log.exception('Failed to find container for '"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 489,
                "PatchRowcode": "+                                       'algorithm with run_id %s', task.run_id)"
            },
            "7": {
                "beforePatchRowNumber": 490,
                "afterPatchRowNumber": 490,
                "PatchRowcode": "                     self.failed_tasks.append(task)"
            },
            "8": {
                "beforePatchRowNumber": 491,
                "afterPatchRowNumber": 491,
                "PatchRowcode": "                     self.active_tasks.remove(task)"
            },
            "9": {
                "beforePatchRowNumber": 492,
                "afterPatchRowNumber": 492,
                "PatchRowcode": "                     break"
            },
            "10": {
                "beforePatchRowNumber": 517,
                "afterPatchRowNumber": 517,
                "PatchRowcode": "         else:"
            },
            "11": {
                "beforePatchRowNumber": 518,
                "afterPatchRowNumber": 518,
                "PatchRowcode": "             # at least one task failed to start"
            },
            "12": {
                "beforePatchRowNumber": 519,
                "afterPatchRowNumber": 519,
                "PatchRowcode": "             finished_task = self.failed_tasks.pop()"
            },
            "13": {
                "beforePatchRowNumber": 520,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            logs = 'Container failed'"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 520,
                "PatchRowcode": "+            logs = 'Container failed. Check node logs for details'"
            },
            "15": {
                "beforePatchRowNumber": 521,
                "afterPatchRowNumber": 521,
                "PatchRowcode": "             results = b''"
            },
            "16": {
                "beforePatchRowNumber": 522,
                "afterPatchRowNumber": 522,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 523,
                "afterPatchRowNumber": 523,
                "PatchRowcode": "         return Result("
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Docker manager",
            "",
            "The docker manager is responsible for communicating with the docker-daemon and",
            "is a wrapper around the docker module. It has methods",
            "for creating docker networks, docker volumes, start containers and retrieve",
            "results from finished containers.",
            "\"\"\"",
            "import os",
            "import time",
            "import logging",
            "import docker",
            "import re",
            "import shutil",
            "",
            "from typing import NamedTuple",
            "from pathlib import Path",
            "",
            "from vantage6.common import logger_name",
            "from vantage6.common import get_database_config",
            "from vantage6.common.docker.addons import get_container, running_in_docker",
            "from vantage6.common.globals import APPNAME, BASIC_PROCESSING_IMAGE",
            "from vantage6.common.task_status import TaskStatus, has_task_failed",
            "from vantage6.common.docker.network_manager import NetworkManager",
            "from vantage6.algorithm.tools.wrappers import get_column_names",
            "from vantage6.cli.context import NodeContext",
            "from vantage6.node.context import DockerNodeContext",
            "from vantage6.node.docker.docker_base import DockerBaseManager",
            "from vantage6.node.docker.vpn_manager import VPNManager",
            "from vantage6.node.docker.task_manager import DockerTaskManager",
            "from vantage6.node.docker.squid import Squid",
            "from vantage6.common.client.node_client import NodeClient",
            "from vantage6.node.docker.exceptions import (",
            "    UnknownAlgorithmStartFail,",
            "    PermanentAlgorithmStartFail,",
            "    AlgorithmContainerNotFound",
            ")",
            "",
            "log = logging.getLogger(logger_name(__name__))",
            "",
            "",
            "class Result(NamedTuple):",
            "    \"\"\"",
            "    Data class to store the result of the docker image.",
            "",
            "    Attributes",
            "    ----------",
            "    run_id: int",
            "        ID of the current algorithm run",
            "    logs: str",
            "        Logs attached to current algorithm run",
            "    data: str",
            "        Output data of the algorithm",
            "    status_code: int",
            "        Status code of the algorithm run",
            "    \"\"\"",
            "    run_id: int",
            "    task_id: int",
            "    logs: str",
            "    data: str",
            "    status: str",
            "    parent_id: int | None",
            "",
            "",
            "class ToBeKilled(NamedTuple):",
            "    \"\"\" Data class to store which tasks should be killed \"\"\"",
            "    task_id: int",
            "    run_id: int",
            "    organization_id: int",
            "",
            "",
            "class KilledRun(NamedTuple):",
            "    \"\"\" Data class to store which algorithms have been killed \"\"\"",
            "    run_id: int",
            "    task_id: int",
            "    parent_id: int",
            "",
            "",
            "class DockerManager(DockerBaseManager):",
            "    \"\"\"",
            "    Wrapper for the docker-py module.",
            "",
            "    This class manages tasks related to Docker, such as logging in to",
            "    docker registries, managing input/output files, logs etc. Results",
            "    can be retrieved through `get_result()` which returns the first available",
            "    algorithm result.",
            "    \"\"\"",
            "    log = logging.getLogger(logger_name(__name__))",
            "",
            "    def __init__(self, ctx: DockerNodeContext | NodeContext,",
            "                 isolated_network_mgr: NetworkManager,",
            "                 vpn_manager: VPNManager, tasks_dir: Path, client: NodeClient,",
            "                 proxy: Squid | None = None) -> None:",
            "        \"\"\" Initialization of DockerManager creates docker connection and",
            "            sets some default values.",
            "",
            "            Parameters",
            "            ----------",
            "            ctx: DockerNodeContext | NodeContext",
            "                Context object from which some settings are obtained",
            "            isolated_network_mgr: NetworkManager",
            "                Manager for the isolated network",
            "            vpn_manager: VPNManager",
            "                VPN Manager object",
            "            tasks_dir: Path",
            "                Directory in which this task's data are stored",
            "            client: NodeClient",
            "                Client object to communicate with the server",
            "            proxy: Squid | None",
            "                Squid proxy object",
            "        \"\"\"",
            "        self.log.debug(\"Initializing DockerManager\")",
            "        super().__init__(isolated_network_mgr)",
            "",
            "        self.data_volume_name = ctx.docker_volume_name",
            "        config = ctx.config",
            "        self.algorithm_env = config.get('algorithm_env', {})",
            "        self.vpn_manager = vpn_manager",
            "        self.client = client",
            "        self.__tasks_dir = tasks_dir",
            "        self.alpine_image = config.get('alpine')",
            "        self.proxy = proxy",
            "",
            "        # keep track of the running containers",
            "        self.active_tasks: list[DockerTaskManager] = []",
            "",
            "        # keep track of the containers that have failed to start",
            "        self.failed_tasks: list[DockerTaskManager] = []",
            "",
            "        # before a task is executed it gets exposed to these policies",
            "        self._policies = config.get(\"policies\", {})",
            "",
            "        # node name is used to identify algorithm containers belonging",
            "        # to this node. This is required as multiple nodes may run at",
            "        # a single machine sharing the docker daemon while using a",
            "        # different server. Using a different server means that there",
            "        # could be duplicate result_id's running at the node at the same",
            "        # time.",
            "        self.node_name = ctx.name",
            "",
            "        # name of the container that is running the node",
            "        self.node_container_name = ctx.docker_container_name",
            "",
            "        # login to the registries",
            "        docker_registries = ctx.config.get(\"docker_registries\", [])",
            "        self.login_to_registries(docker_registries)",
            "",
            "        # set database uri and whether or not it is a file",
            "        self._set_database(ctx.databases)",
            "",
            "        # keep track of linked docker services",
            "        self.linked_services: list[str] = []",
            "",
            "        # set algorithm device requests",
            "        self.algorithm_device_requests = []",
            "        if 'algorithm_device_requests' in config:",
            "            self._set_algorithm_device_requests(",
            "                config['algorithm_device_requests']",
            "            )",
            "",
            "    def _set_database(self, databases: dict | list) -> None:",
            "        \"\"\"",
            "        Set database location and whether or not it is a file",
            "",
            "        Parameters",
            "        ----------",
            "        databases: dict | list",
            "            databases as specified in the config file",
            "        \"\"\"",
            "        db_labels = [db['label'] for db in databases]",
            "",
            "        # If we're running in a docker container, database_uri would point",
            "        # to a path on the *host* (since it's been read from the config",
            "        # file). That's no good here. Therefore, we expect the CLI to set",
            "        # the environment variables for us. This has the added bonus that we",
            "        # can override the URI from the command line as well.",
            "        self.databases = {}",
            "        for label in db_labels:",
            "            label_upper = label.upper()",
            "            db_config = get_database_config(databases, label)",
            "            if running_in_docker():",
            "                uri = os.environ[f'{label_upper}_DATABASE_URI']",
            "            else:",
            "                uri = db_config['uri']",
            "",
            "            if running_in_docker():",
            "                db_is_file = Path(f'/mnt/{uri}').exists()",
            "                if db_is_file:",
            "                    uri = f'/mnt/{uri}'",
            "            else:",
            "                db_is_file = Path(uri).exists()",
            "",
            "            if db_is_file:",
            "                # We'll copy the file to the folder `data` in our task_dir.",
            "                self.log.info(f'Copying {uri} to {self.__tasks_dir}')",
            "                shutil.copy(uri, self.__tasks_dir)",
            "                uri = self.__tasks_dir / os.path.basename(uri)",
            "",
            "            self.databases[label] = {'uri': uri, 'is_file': db_is_file,",
            "                                     'type': db_config['type'],",
            "                                     'env': db_config.get('env', {})}",
            "        self.log.debug(f\"Databases: {self.databases}\")",
            "",
            "    def _set_algorithm_device_requests(self, device_requests_config: dict) \\",
            "            -> None:",
            "        \"\"\"",
            "        Configure device access for the algorithm container.",
            "",
            "        Parameters",
            "        ----------",
            "        device_requests_config: dict",
            "           A dictionary containing configuration options for device access.",
            "           Supported keys:",
            "           - 'gpu': A boolean value indicating whether GPU access is required.",
            "        \"\"\"",
            "        device_requests = []",
            "        if device_requests_config.get('gpu', False):",
            "            device = docker.types.DeviceRequest(count=-1,",
            "                                                capabilities=[['gpu']])",
            "            device_requests.append(device)",
            "        self.algorithm_device_requests = device_requests",
            "",
            "    def create_volume(self, volume_name: str) -> None:",
            "        \"\"\"",
            "        Create a temporary volume for a single run.",
            "",
            "        A single run can consist of multiple algorithm containers. It is",
            "        important to note that all algorithm containers having the same job_id",
            "        have access to this container.",
            "",
            "        Parameters",
            "        ----------",
            "        volume_name: str",
            "            Name of the volume to be created",
            "        \"\"\"",
            "        try:",
            "            self.docker.volumes.get(volume_name)",
            "            self.log.debug(f\"Volume {volume_name} already exists.\")",
            "",
            "        except docker.errors.NotFound:",
            "            self.log.debug(f\"Creating volume {volume_name}\")",
            "            self.docker.volumes.create(volume_name)",
            "",
            "    def is_docker_image_allowed(",
            "        self, docker_image_name: str, task_info: dict",
            "    ) -> bool:",
            "        \"\"\"",
            "        Checks the docker image name.",
            "",
            "        Against a list of regular expressions as defined in the configuration",
            "        file. If no expressions are defined, all docker images are accepted.",
            "",
            "        Parameters",
            "        ----------",
            "        docker_image_name: str",
            "            uri to the docker image",
            "        task_info: dict",
            "            Dictionary with information about the task",
            "",
            "        Returns",
            "        -------",
            "        bool",
            "            Whether docker image is allowed or not",
            "        \"\"\"",
            "        # check if algorithm matches any of the regex cases",
            "        allow_basics = self._policies.get('allow_basics_algorithm', True)",
            "        allowed_algorithms = self._policies.get('allowed_algorithms')",
            "        if docker_image_name.startswith(BASIC_PROCESSING_IMAGE):",
            "            if not allow_basics:",
            "                self.log.warn(\"A task was sent with a basics algorithm that \"",
            "                              \"this node does not allow to run.\")",
            "                return False",
            "            # else: basics are allowed, so we don't need to check the regex",
            "        elif allowed_algorithms:",
            "            if isinstance(allowed_algorithms, str):",
            "                allowed_algorithms = [allowed_algorithms]",
            "            found = False",
            "            for regex_expr in allowed_algorithms:",
            "                expr_ = re.compile(regex_expr)",
            "                if expr_.match(docker_image_name):",
            "                    found = True",
            "",
            "            if not found:",
            "                self.log.warn(\"A task was sent with a docker image that this\"",
            "                              \" node does not allow to run.\")",
            "                return False",
            "",
            "        # check if user or their organization is allowed",
            "        allowed_users = self._policies.get('allowed_users', [])",
            "        allowed_orgs = self._policies.get('allowed_organizations', [])",
            "        if allowed_users or allowed_orgs:",
            "            is_allowed = self.client.check_user_allowed_to_send_task(",
            "                allowed_users, allowed_orgs, task_info['init_org']['id'],",
            "                task_info['init_user']['id']",
            "            )",
            "            if not is_allowed:",
            "                self.log.warn(\"A task was sent by a user or organization that \"",
            "                              \"this node does not allow to start tasks.\")",
            "                return False",
            "",
            "        # if no limits are declared, log warning",
            "        if not self._policies:",
            "            self.log.warn(\"All docker images are allowed on this Node!\")",
            "",
            "        return True",
            "",
            "    def is_running(self, run_id: int) -> bool:",
            "        \"\"\"",
            "        Check if a container is already running for <run_id>.",
            "",
            "        Parameters",
            "        ----------",
            "        run_id: int",
            "            run_id of the algorithm container to be found",
            "",
            "        Returns",
            "        -------",
            "        bool",
            "            Whether or not algorithm container is running already",
            "        \"\"\"",
            "        running_containers = self.docker.containers.list(filters={",
            "            \"label\": [",
            "                f\"{APPNAME}-type=algorithm\",",
            "                f\"node={self.node_name}\",",
            "                f\"run_id={run_id}\"",
            "            ]",
            "        })",
            "        return bool(running_containers)",
            "",
            "    def cleanup_tasks(self) -> list[KilledRun]:",
            "        \"\"\"",
            "        Stop all active tasks",
            "",
            "        Returns",
            "        -------",
            "        list[KilledRun]:",
            "            List of information on tasks that have been killed",
            "        \"\"\"",
            "        run_ids_killed = []",
            "        if self.active_tasks:",
            "            self.log.debug(f'Killing {len(self.active_tasks)} active task(s)')",
            "        while self.active_tasks:",
            "            task = self.active_tasks.pop()",
            "            task.cleanup()",
            "            run_ids_killed.append(KilledRun(",
            "                run_id=task.run_id,",
            "                task_id=task.task_id,",
            "                parent_id=task.parent_id",
            "            ))",
            "        return run_ids_killed",
            "",
            "    def cleanup(self) -> None:",
            "        \"\"\"",
            "        Stop all active tasks and delete the isolated network",
            "",
            "        Note: the temporary docker volumes are kept as they may still be used",
            "        by a parent container",
            "        \"\"\"",
            "        # note: the function `cleanup_tasks` returns a list of tasks that were",
            "        # killed, but we don't register them as killed so they will be run",
            "        # again when the node is restarted",
            "        self.cleanup_tasks()",
            "        for service in self.linked_services:",
            "            self.isolated_network_mgr.disconnect(service)",
            "",
            "        # remove the node container from the network, it runs this code.. so",
            "        # it does not make sense to delete it just yet",
            "        self.isolated_network_mgr.disconnect(self.node_container_name)",
            "",
            "        # remove the connected containers and the network",
            "        self.isolated_network_mgr.delete(kill_containers=True)",
            "",
            "    def run(self, run_id: int, task_info: dict, image: str,",
            "            docker_input: bytes, tmp_vol_name: str, token: str,",
            "            databases_to_use: list[str]",
            "            ) -> tuple[TaskStatus, list[dict] | None]:",
            "        \"\"\"",
            "        Checks if docker task is running. If not, creates DockerTaskManager to",
            "        run the task",
            "",
            "        Parameters",
            "        ----------",
            "        run_id: int",
            "            Server run identifier",
            "        task_info: dict",
            "            Dictionary with task information",
            "        image: str",
            "            Docker image name",
            "        docker_input: bytes",
            "            Input that can be read by docker container",
            "        tmp_vol_name: str",
            "            Name of temporary docker volume assigned to the algorithm",
            "        token: str",
            "            Bearer token that the container can use",
            "        databases_to_use: list[str]",
            "            Labels of the databases to use",
            "",
            "        Returns",
            "        -------",
            "        TaskStatus, list[dict] | None",
            "            Returns a tuple with the status of the task and a description of",
            "            each port on the VPN client that forwards traffic to the algorithm",
            "            container (``None`` if VPN is not set up).",
            "        \"\"\"",
            "        # Verify that an allowed image is used",
            "        if not self.is_docker_image_allowed(image, task_info):",
            "            msg = f\"Docker image {image} is not allowed on this Node!\"",
            "            self.log.critical(msg)",
            "            return TaskStatus.NOT_ALLOWED,  None",
            "",
            "        # Check that this task is not already running",
            "        if self.is_running(run_id):",
            "            self.log.warn(\"Task is already being executed, discarding task\")",
            "            self.log.debug(f\"run_id={run_id} is discarded\")",
            "            return TaskStatus.ACTIVE, None",
            "",
            "        task = DockerTaskManager(",
            "            image=image,",
            "            run_id=run_id,",
            "            task_info=task_info,",
            "            vpn_manager=self.vpn_manager,",
            "            node_name=self.node_name,",
            "            tasks_dir=self.__tasks_dir,",
            "            isolated_network_mgr=self.isolated_network_mgr,",
            "            databases=self.databases,",
            "            docker_volume_name=self.data_volume_name,",
            "            alpine_image=self.alpine_image,",
            "            proxy=self.proxy,",
            "            device_requests=self.algorithm_device_requests",
            "        )",
            "",
            "        # attempt to kick of the task. If it fails do to unknown reasons we try",
            "        # again. If it fails permanently we add it to the failed tasks to be",
            "        # handled by the speaking worker of the node",
            "        attempts = 1",
            "        while not (task.status == TaskStatus.ACTIVE) and attempts < 3:",
            "            try:",
            "                vpn_ports = task.run(",
            "                    docker_input=docker_input, tmp_vol_name=tmp_vol_name,",
            "                    token=token, algorithm_env=self.algorithm_env,",
            "                    databases_to_use=databases_to_use",
            "                )",
            "",
            "            except UnknownAlgorithmStartFail:",
            "                self.log.exception(f'Failed to start run {run_id} for an '",
            "                                   'unknown reason. Retrying...')",
            "                time.sleep(1)  # add some time before retrying the next attempt",
            "",
            "            except PermanentAlgorithmStartFail:",
            "                break",
            "",
            "            attempts += 1",
            "",
            "        # keep track of the active container",
            "        if has_task_failed(task.status):",
            "            self.failed_tasks.append(task)",
            "            return task.status, None",
            "        else:",
            "            self.active_tasks.append(task)",
            "            return task.status, vpn_ports",
            "",
            "    def get_result(self) -> Result:",
            "        \"\"\"",
            "        Returns the oldest (FIFO) finished docker container.",
            "",
            "        This is a blocking method until a finished container shows up. Once the",
            "        container is obtained and the results are read, the container is",
            "        removed from the docker environment.",
            "",
            "        Returns",
            "        -------",
            "        Result",
            "            result of the docker image",
            "        \"\"\"",
            "",
            "        # get finished results and get the first one, if no result is available",
            "        # this is blocking",
            "        finished_tasks = []",
            "        while (not finished_tasks) and (not self.failed_tasks):",
            "            for task in self.active_tasks:",
            "",
            "                try:",
            "                    if task.is_finished():",
            "                        finished_tasks.append(task)",
            "                        self.active_tasks.remove(task)",
            "                        break",
            "                except AlgorithmContainerNotFound:",
            "                    self.log.exception(f'Failed to find container for '",
            "                                       f'result {task.result_id}')",
            "                    self.failed_tasks.append(task)",
            "                    self.active_tasks.remove(task)",
            "                    break",
            "",
            "            # sleep for a second before checking again",
            "            time.sleep(1)",
            "",
            "        if finished_tasks:",
            "            # at least one task is finished",
            "",
            "            finished_task = finished_tasks.pop()",
            "            self.log.debug(f\"Run id={finished_task.run_id} is finished\")",
            "",
            "            # Check exit status and report",
            "            logs = finished_task.report_status()",
            "",
            "            # Cleanup containers",
            "            finished_task.cleanup()",
            "",
            "            # Retrieve results from file",
            "            results = finished_task.get_results()",
            "",
            "            # remove the VPN ports of this run from the database",
            "            self.client.request(",
            "                'port', params={'run_id': finished_task.run_id},",
            "                method=\"DELETE\"",
            "            )",
            "        else:",
            "            # at least one task failed to start",
            "            finished_task = self.failed_tasks.pop()",
            "            logs = 'Container failed'",
            "            results = b''",
            "",
            "        return Result(",
            "            run_id=finished_task.run_id,",
            "            task_id=finished_task.task_id,",
            "            logs=logs,",
            "            data=results,",
            "            status=finished_task.status,",
            "            parent_id=finished_task.parent_id,",
            "        )",
            "",
            "    def login_to_registries(self, registries: list = []) -> None:",
            "        \"\"\"",
            "        Login to the docker registries",
            "",
            "        Parameters",
            "        ----------",
            "        registries: list",
            "            list of registries to login to",
            "        \"\"\"",
            "        for registry in registries:",
            "            try:",
            "                self.docker.login(",
            "                    username=registry.get(\"username\"),",
            "                    password=registry.get(\"password\"),",
            "                    registry=registry.get(\"registry\")",
            "                )",
            "                self.log.info(f\"Logged in to {registry.get('registry')}\")",
            "            except docker.errors.APIError as e:",
            "                self.log.warn(f\"Could not login to {registry.get('registry')}\")",
            "                self.log.debug(e)",
            "",
            "    def link_container_to_network(self, container_name: str,",
            "                                  config_alias: str) -> None:",
            "        \"\"\"",
            "        Link a docker container to the isolated docker network",
            "",
            "        Parameters",
            "        ----------",
            "        container_name: str",
            "            Name of the docker container to be linked to the network",
            "        config_alias: str",
            "            Alias of the docker container defined in the config file",
            "        \"\"\"",
            "        container = get_container(",
            "            docker_client=self.docker, name=container_name",
            "        )",
            "        if not container:",
            "            self.log.error(f\"Could not link docker container {container_name} \"",
            "                           \"that was specified in the configuration file to \"",
            "                           \"the isolated docker network.\")",
            "            self.log.error(\"Container not found!\")",
            "            return",
            "        self.isolated_network_mgr.connect(",
            "            container_name=container_name,",
            "            aliases=[config_alias]",
            "        )",
            "        self.linked_services.append(container_name)",
            "",
            "    def kill_selected_tasks(",
            "        self, org_id: int, kill_list: list[ToBeKilled] = None",
            "    ) -> list[KilledRun]:",
            "        \"\"\"",
            "        Kill tasks specified by a kill list, if they are currently running on",
            "        this node",
            "",
            "        Parameters",
            "        ----------",
            "        org_id: int",
            "            The organization id of this node",
            "        kill_list: list[ToBeKilled]",
            "            A list of info about tasks that should be killed.",
            "",
            "        Returns",
            "        -------",
            "        list[KilledRun]",
            "            List with information on killed tasks",
            "        \"\"\"",
            "        killed_list = []",
            "        for container_to_kill in kill_list:",
            "            if container_to_kill['organization_id'] != org_id:",
            "                continue  # this run is on another node",
            "            # find the task",
            "            task = next((",
            "                t for t in self.active_tasks",
            "                if t.run_id == container_to_kill['run_id']",
            "            ), None)",
            "            if task:",
            "                self.log.info(",
            "                    f\"Killing containers for run_id={task.run_id}\")",
            "                self.active_tasks.remove(task)",
            "                task.cleanup()",
            "                killed_list.append(KilledRun(",
            "                    run_id=task.run_id,",
            "                    task_id=task.task_id,",
            "                    parent_id=task.parent_id,",
            "                ))",
            "            else:",
            "                self.log.warn(",
            "                    \"Received instruction to kill run_id=\"",
            "                    f\"{container_to_kill['run_id']}, but it was not \"",
            "                    \"found running on this node.\")",
            "        return killed_list",
            "",
            "    def kill_tasks(self, org_id: int,",
            "                   kill_list: list[ToBeKilled] = None) -> list[KilledRun]:",
            "        \"\"\"",
            "        Kill tasks currently running on this node.",
            "",
            "        Parameters",
            "        ----------",
            "        org_id: int",
            "            The organization id of this node",
            "        kill_list: list[ToBeKilled] (optional)",
            "            A list of info on tasks that should be killed. If the list",
            "            is not specified, all running algorithm containers will be killed.",
            "",
            "        Returns",
            "        -------",
            "        list[KilledRun]",
            "            List of dictionaries with information on killed tasks",
            "        \"\"\"",
            "        if kill_list:",
            "            killed_runs = self.kill_selected_tasks(org_id=org_id,",
            "                                                   kill_list=kill_list)",
            "        else:",
            "            # received instruction to kill all tasks on this node",
            "            self.log.warn(",
            "                \"Received instruction from server to kill all algorithms \"",
            "                \"running on this node. Executing that now...\")",
            "            killed_runs = self.cleanup_tasks()",
            "            if len(killed_runs):",
            "                self.log.warn(",
            "                    \"Killed the following run ids as instructed via socket:\"",
            "                    f\" {', '.join([str(r.run_id) for r in killed_runs])}\"",
            "                )",
            "            else:",
            "                self.log.warn(",
            "                    \"Instructed to kill tasks but none were running\"",
            "                )",
            "        return killed_runs",
            "",
            "    def get_column_names(self, label: str, type_: str) -> list[str]:",
            "        \"\"\"",
            "        Get column names from a node database",
            "",
            "        Parameters",
            "        ----------",
            "        label: str",
            "            Label of the database",
            "        type_: str",
            "            Type of the database",
            "",
            "        Returns",
            "        -------",
            "        list[str]",
            "            List of column names",
            "        \"\"\"",
            "        db = self.databases.get(label)",
            "        if not db:",
            "            self.log.error(\"Database with label %s not found\", label)",
            "            return []",
            "        if not db['is_file']:",
            "            self.log.error(\"Database with label %s is not a file. Cannot\"",
            "                           \" determine columns without query\", label)",
            "            return []",
            "        if db['type'] == 'excel':",
            "            self.log.error(\"Cannot determine columns for excel database \"",
            "                           \" without a worksheet\")",
            "            return []",
            "        if type_ not in ('csv', 'sparql'):",
            "            self.log.error(\"Cannot determine columns for database of type %s.\"",
            "                           \"Only csv and sparql are supported\", type_)",
            "            return []",
            "        return get_column_names(db['uri'], type_)"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Docker manager",
            "",
            "The docker manager is responsible for communicating with the docker-daemon and",
            "is a wrapper around the docker module. It has methods",
            "for creating docker networks, docker volumes, start containers and retrieve",
            "results from finished containers.",
            "\"\"\"",
            "import os",
            "import time",
            "import logging",
            "import docker",
            "import re",
            "import shutil",
            "",
            "from typing import NamedTuple",
            "from pathlib import Path",
            "",
            "from vantage6.common import logger_name",
            "from vantage6.common import get_database_config",
            "from vantage6.common.docker.addons import get_container, running_in_docker",
            "from vantage6.common.globals import APPNAME, BASIC_PROCESSING_IMAGE",
            "from vantage6.common.task_status import TaskStatus, has_task_failed",
            "from vantage6.common.docker.network_manager import NetworkManager",
            "from vantage6.algorithm.tools.wrappers import get_column_names",
            "from vantage6.cli.context import NodeContext",
            "from vantage6.node.context import DockerNodeContext",
            "from vantage6.node.docker.docker_base import DockerBaseManager",
            "from vantage6.node.docker.vpn_manager import VPNManager",
            "from vantage6.node.docker.task_manager import DockerTaskManager",
            "from vantage6.node.docker.squid import Squid",
            "from vantage6.common.client.node_client import NodeClient",
            "from vantage6.node.docker.exceptions import (",
            "    UnknownAlgorithmStartFail,",
            "    PermanentAlgorithmStartFail,",
            "    AlgorithmContainerNotFound",
            ")",
            "",
            "log = logging.getLogger(logger_name(__name__))",
            "",
            "",
            "class Result(NamedTuple):",
            "    \"\"\"",
            "    Data class to store the result of the docker image.",
            "",
            "    Attributes",
            "    ----------",
            "    run_id: int",
            "        ID of the current algorithm run",
            "    logs: str",
            "        Logs attached to current algorithm run",
            "    data: str",
            "        Output data of the algorithm",
            "    status_code: int",
            "        Status code of the algorithm run",
            "    \"\"\"",
            "    run_id: int",
            "    task_id: int",
            "    logs: str",
            "    data: str",
            "    status: str",
            "    parent_id: int | None",
            "",
            "",
            "class ToBeKilled(NamedTuple):",
            "    \"\"\" Data class to store which tasks should be killed \"\"\"",
            "    task_id: int",
            "    run_id: int",
            "    organization_id: int",
            "",
            "",
            "class KilledRun(NamedTuple):",
            "    \"\"\" Data class to store which algorithms have been killed \"\"\"",
            "    run_id: int",
            "    task_id: int",
            "    parent_id: int",
            "",
            "",
            "class DockerManager(DockerBaseManager):",
            "    \"\"\"",
            "    Wrapper for the docker-py module.",
            "",
            "    This class manages tasks related to Docker, such as logging in to",
            "    docker registries, managing input/output files, logs etc. Results",
            "    can be retrieved through `get_result()` which returns the first available",
            "    algorithm result.",
            "    \"\"\"",
            "    log = logging.getLogger(logger_name(__name__))",
            "",
            "    def __init__(self, ctx: DockerNodeContext | NodeContext,",
            "                 isolated_network_mgr: NetworkManager,",
            "                 vpn_manager: VPNManager, tasks_dir: Path, client: NodeClient,",
            "                 proxy: Squid | None = None) -> None:",
            "        \"\"\" Initialization of DockerManager creates docker connection and",
            "            sets some default values.",
            "",
            "            Parameters",
            "            ----------",
            "            ctx: DockerNodeContext | NodeContext",
            "                Context object from which some settings are obtained",
            "            isolated_network_mgr: NetworkManager",
            "                Manager for the isolated network",
            "            vpn_manager: VPNManager",
            "                VPN Manager object",
            "            tasks_dir: Path",
            "                Directory in which this task's data are stored",
            "            client: NodeClient",
            "                Client object to communicate with the server",
            "            proxy: Squid | None",
            "                Squid proxy object",
            "        \"\"\"",
            "        self.log.debug(\"Initializing DockerManager\")",
            "        super().__init__(isolated_network_mgr)",
            "",
            "        self.data_volume_name = ctx.docker_volume_name",
            "        config = ctx.config",
            "        self.algorithm_env = config.get('algorithm_env', {})",
            "        self.vpn_manager = vpn_manager",
            "        self.client = client",
            "        self.__tasks_dir = tasks_dir",
            "        self.alpine_image = config.get('alpine')",
            "        self.proxy = proxy",
            "",
            "        # keep track of the running containers",
            "        self.active_tasks: list[DockerTaskManager] = []",
            "",
            "        # keep track of the containers that have failed to start",
            "        self.failed_tasks: list[DockerTaskManager] = []",
            "",
            "        # before a task is executed it gets exposed to these policies",
            "        self._policies = config.get(\"policies\", {})",
            "",
            "        # node name is used to identify algorithm containers belonging",
            "        # to this node. This is required as multiple nodes may run at",
            "        # a single machine sharing the docker daemon while using a",
            "        # different server. Using a different server means that there",
            "        # could be duplicate result_id's running at the node at the same",
            "        # time.",
            "        self.node_name = ctx.name",
            "",
            "        # name of the container that is running the node",
            "        self.node_container_name = ctx.docker_container_name",
            "",
            "        # login to the registries",
            "        docker_registries = ctx.config.get(\"docker_registries\", [])",
            "        self.login_to_registries(docker_registries)",
            "",
            "        # set database uri and whether or not it is a file",
            "        self._set_database(ctx.databases)",
            "",
            "        # keep track of linked docker services",
            "        self.linked_services: list[str] = []",
            "",
            "        # set algorithm device requests",
            "        self.algorithm_device_requests = []",
            "        if 'algorithm_device_requests' in config:",
            "            self._set_algorithm_device_requests(",
            "                config['algorithm_device_requests']",
            "            )",
            "",
            "    def _set_database(self, databases: dict | list) -> None:",
            "        \"\"\"",
            "        Set database location and whether or not it is a file",
            "",
            "        Parameters",
            "        ----------",
            "        databases: dict | list",
            "            databases as specified in the config file",
            "        \"\"\"",
            "        db_labels = [db['label'] for db in databases]",
            "",
            "        # If we're running in a docker container, database_uri would point",
            "        # to a path on the *host* (since it's been read from the config",
            "        # file). That's no good here. Therefore, we expect the CLI to set",
            "        # the environment variables for us. This has the added bonus that we",
            "        # can override the URI from the command line as well.",
            "        self.databases = {}",
            "        for label in db_labels:",
            "            label_upper = label.upper()",
            "            db_config = get_database_config(databases, label)",
            "            if running_in_docker():",
            "                uri = os.environ[f'{label_upper}_DATABASE_URI']",
            "            else:",
            "                uri = db_config['uri']",
            "",
            "            if running_in_docker():",
            "                db_is_file = Path(f'/mnt/{uri}').exists()",
            "                if db_is_file:",
            "                    uri = f'/mnt/{uri}'",
            "            else:",
            "                db_is_file = Path(uri).exists()",
            "",
            "            if db_is_file:",
            "                # We'll copy the file to the folder `data` in our task_dir.",
            "                self.log.info(f'Copying {uri} to {self.__tasks_dir}')",
            "                shutil.copy(uri, self.__tasks_dir)",
            "                uri = self.__tasks_dir / os.path.basename(uri)",
            "",
            "            self.databases[label] = {'uri': uri, 'is_file': db_is_file,",
            "                                     'type': db_config['type'],",
            "                                     'env': db_config.get('env', {})}",
            "        self.log.debug(f\"Databases: {self.databases}\")",
            "",
            "    def _set_algorithm_device_requests(self, device_requests_config: dict) \\",
            "            -> None:",
            "        \"\"\"",
            "        Configure device access for the algorithm container.",
            "",
            "        Parameters",
            "        ----------",
            "        device_requests_config: dict",
            "           A dictionary containing configuration options for device access.",
            "           Supported keys:",
            "           - 'gpu': A boolean value indicating whether GPU access is required.",
            "        \"\"\"",
            "        device_requests = []",
            "        if device_requests_config.get('gpu', False):",
            "            device = docker.types.DeviceRequest(count=-1,",
            "                                                capabilities=[['gpu']])",
            "            device_requests.append(device)",
            "        self.algorithm_device_requests = device_requests",
            "",
            "    def create_volume(self, volume_name: str) -> None:",
            "        \"\"\"",
            "        Create a temporary volume for a single run.",
            "",
            "        A single run can consist of multiple algorithm containers. It is",
            "        important to note that all algorithm containers having the same job_id",
            "        have access to this container.",
            "",
            "        Parameters",
            "        ----------",
            "        volume_name: str",
            "            Name of the volume to be created",
            "        \"\"\"",
            "        try:",
            "            self.docker.volumes.get(volume_name)",
            "            self.log.debug(f\"Volume {volume_name} already exists.\")",
            "",
            "        except docker.errors.NotFound:",
            "            self.log.debug(f\"Creating volume {volume_name}\")",
            "            self.docker.volumes.create(volume_name)",
            "",
            "    def is_docker_image_allowed(",
            "        self, docker_image_name: str, task_info: dict",
            "    ) -> bool:",
            "        \"\"\"",
            "        Checks the docker image name.",
            "",
            "        Against a list of regular expressions as defined in the configuration",
            "        file. If no expressions are defined, all docker images are accepted.",
            "",
            "        Parameters",
            "        ----------",
            "        docker_image_name: str",
            "            uri to the docker image",
            "        task_info: dict",
            "            Dictionary with information about the task",
            "",
            "        Returns",
            "        -------",
            "        bool",
            "            Whether docker image is allowed or not",
            "        \"\"\"",
            "        # check if algorithm matches any of the regex cases",
            "        allow_basics = self._policies.get('allow_basics_algorithm', True)",
            "        allowed_algorithms = self._policies.get('allowed_algorithms')",
            "        if docker_image_name.startswith(BASIC_PROCESSING_IMAGE):",
            "            if not allow_basics:",
            "                self.log.warn(\"A task was sent with a basics algorithm that \"",
            "                              \"this node does not allow to run.\")",
            "                return False",
            "            # else: basics are allowed, so we don't need to check the regex",
            "        elif allowed_algorithms:",
            "            if isinstance(allowed_algorithms, str):",
            "                allowed_algorithms = [allowed_algorithms]",
            "            found = False",
            "            for regex_expr in allowed_algorithms:",
            "                expr_ = re.compile(regex_expr)",
            "                if expr_.match(docker_image_name):",
            "                    found = True",
            "",
            "            if not found:",
            "                self.log.warn(\"A task was sent with a docker image that this\"",
            "                              \" node does not allow to run.\")",
            "                return False",
            "",
            "        # check if user or their organization is allowed",
            "        allowed_users = self._policies.get('allowed_users', [])",
            "        allowed_orgs = self._policies.get('allowed_organizations', [])",
            "        if allowed_users or allowed_orgs:",
            "            is_allowed = self.client.check_user_allowed_to_send_task(",
            "                allowed_users, allowed_orgs, task_info['init_org']['id'],",
            "                task_info['init_user']['id']",
            "            )",
            "            if not is_allowed:",
            "                self.log.warn(\"A task was sent by a user or organization that \"",
            "                              \"this node does not allow to start tasks.\")",
            "                return False",
            "",
            "        # if no limits are declared, log warning",
            "        if not self._policies:",
            "            self.log.warn(\"All docker images are allowed on this Node!\")",
            "",
            "        return True",
            "",
            "    def is_running(self, run_id: int) -> bool:",
            "        \"\"\"",
            "        Check if a container is already running for <run_id>.",
            "",
            "        Parameters",
            "        ----------",
            "        run_id: int",
            "            run_id of the algorithm container to be found",
            "",
            "        Returns",
            "        -------",
            "        bool",
            "            Whether or not algorithm container is running already",
            "        \"\"\"",
            "        running_containers = self.docker.containers.list(filters={",
            "            \"label\": [",
            "                f\"{APPNAME}-type=algorithm\",",
            "                f\"node={self.node_name}\",",
            "                f\"run_id={run_id}\"",
            "            ]",
            "        })",
            "        return bool(running_containers)",
            "",
            "    def cleanup_tasks(self) -> list[KilledRun]:",
            "        \"\"\"",
            "        Stop all active tasks",
            "",
            "        Returns",
            "        -------",
            "        list[KilledRun]:",
            "            List of information on tasks that have been killed",
            "        \"\"\"",
            "        run_ids_killed = []",
            "        if self.active_tasks:",
            "            self.log.debug(f'Killing {len(self.active_tasks)} active task(s)')",
            "        while self.active_tasks:",
            "            task = self.active_tasks.pop()",
            "            task.cleanup()",
            "            run_ids_killed.append(KilledRun(",
            "                run_id=task.run_id,",
            "                task_id=task.task_id,",
            "                parent_id=task.parent_id",
            "            ))",
            "        return run_ids_killed",
            "",
            "    def cleanup(self) -> None:",
            "        \"\"\"",
            "        Stop all active tasks and delete the isolated network",
            "",
            "        Note: the temporary docker volumes are kept as they may still be used",
            "        by a parent container",
            "        \"\"\"",
            "        # note: the function `cleanup_tasks` returns a list of tasks that were",
            "        # killed, but we don't register them as killed so they will be run",
            "        # again when the node is restarted",
            "        self.cleanup_tasks()",
            "        for service in self.linked_services:",
            "            self.isolated_network_mgr.disconnect(service)",
            "",
            "        # remove the node container from the network, it runs this code.. so",
            "        # it does not make sense to delete it just yet",
            "        self.isolated_network_mgr.disconnect(self.node_container_name)",
            "",
            "        # remove the connected containers and the network",
            "        self.isolated_network_mgr.delete(kill_containers=True)",
            "",
            "    def run(self, run_id: int, task_info: dict, image: str,",
            "            docker_input: bytes, tmp_vol_name: str, token: str,",
            "            databases_to_use: list[str]",
            "            ) -> tuple[TaskStatus, list[dict] | None]:",
            "        \"\"\"",
            "        Checks if docker task is running. If not, creates DockerTaskManager to",
            "        run the task",
            "",
            "        Parameters",
            "        ----------",
            "        run_id: int",
            "            Server run identifier",
            "        task_info: dict",
            "            Dictionary with task information",
            "        image: str",
            "            Docker image name",
            "        docker_input: bytes",
            "            Input that can be read by docker container",
            "        tmp_vol_name: str",
            "            Name of temporary docker volume assigned to the algorithm",
            "        token: str",
            "            Bearer token that the container can use",
            "        databases_to_use: list[str]",
            "            Labels of the databases to use",
            "",
            "        Returns",
            "        -------",
            "        TaskStatus, list[dict] | None",
            "            Returns a tuple with the status of the task and a description of",
            "            each port on the VPN client that forwards traffic to the algorithm",
            "            container (``None`` if VPN is not set up).",
            "        \"\"\"",
            "        # Verify that an allowed image is used",
            "        if not self.is_docker_image_allowed(image, task_info):",
            "            msg = f\"Docker image {image} is not allowed on this Node!\"",
            "            self.log.critical(msg)",
            "            return TaskStatus.NOT_ALLOWED,  None",
            "",
            "        # Check that this task is not already running",
            "        if self.is_running(run_id):",
            "            self.log.warn(\"Task is already being executed, discarding task\")",
            "            self.log.debug(f\"run_id={run_id} is discarded\")",
            "            return TaskStatus.ACTIVE, None",
            "",
            "        task = DockerTaskManager(",
            "            image=image,",
            "            run_id=run_id,",
            "            task_info=task_info,",
            "            vpn_manager=self.vpn_manager,",
            "            node_name=self.node_name,",
            "            tasks_dir=self.__tasks_dir,",
            "            isolated_network_mgr=self.isolated_network_mgr,",
            "            databases=self.databases,",
            "            docker_volume_name=self.data_volume_name,",
            "            alpine_image=self.alpine_image,",
            "            proxy=self.proxy,",
            "            device_requests=self.algorithm_device_requests",
            "        )",
            "",
            "        # attempt to kick of the task. If it fails do to unknown reasons we try",
            "        # again. If it fails permanently we add it to the failed tasks to be",
            "        # handled by the speaking worker of the node",
            "        attempts = 1",
            "        while not (task.status == TaskStatus.ACTIVE) and attempts < 3:",
            "            try:",
            "                vpn_ports = task.run(",
            "                    docker_input=docker_input, tmp_vol_name=tmp_vol_name,",
            "                    token=token, algorithm_env=self.algorithm_env,",
            "                    databases_to_use=databases_to_use",
            "                )",
            "",
            "            except UnknownAlgorithmStartFail:",
            "                self.log.exception(f'Failed to start run {run_id} for an '",
            "                                   'unknown reason. Retrying...')",
            "                time.sleep(1)  # add some time before retrying the next attempt",
            "",
            "            except PermanentAlgorithmStartFail:",
            "                break",
            "",
            "            attempts += 1",
            "",
            "        # keep track of the active container",
            "        if has_task_failed(task.status):",
            "            self.failed_tasks.append(task)",
            "            return task.status, None",
            "        else:",
            "            self.active_tasks.append(task)",
            "            return task.status, vpn_ports",
            "",
            "    def get_result(self) -> Result:",
            "        \"\"\"",
            "        Returns the oldest (FIFO) finished docker container.",
            "",
            "        This is a blocking method until a finished container shows up. Once the",
            "        container is obtained and the results are read, the container is",
            "        removed from the docker environment.",
            "",
            "        Returns",
            "        -------",
            "        Result",
            "            result of the docker image",
            "        \"\"\"",
            "",
            "        # get finished results and get the first one, if no result is available",
            "        # this is blocking",
            "        finished_tasks = []",
            "        while (not finished_tasks) and (not self.failed_tasks):",
            "            for task in self.active_tasks:",
            "",
            "                try:",
            "                    if task.is_finished():",
            "                        finished_tasks.append(task)",
            "                        self.active_tasks.remove(task)",
            "                        break",
            "                except AlgorithmContainerNotFound:",
            "                    self.log.exception('Failed to find container for '",
            "                                       'algorithm with run_id %s', task.run_id)",
            "                    self.failed_tasks.append(task)",
            "                    self.active_tasks.remove(task)",
            "                    break",
            "",
            "            # sleep for a second before checking again",
            "            time.sleep(1)",
            "",
            "        if finished_tasks:",
            "            # at least one task is finished",
            "",
            "            finished_task = finished_tasks.pop()",
            "            self.log.debug(f\"Run id={finished_task.run_id} is finished\")",
            "",
            "            # Check exit status and report",
            "            logs = finished_task.report_status()",
            "",
            "            # Cleanup containers",
            "            finished_task.cleanup()",
            "",
            "            # Retrieve results from file",
            "            results = finished_task.get_results()",
            "",
            "            # remove the VPN ports of this run from the database",
            "            self.client.request(",
            "                'port', params={'run_id': finished_task.run_id},",
            "                method=\"DELETE\"",
            "            )",
            "        else:",
            "            # at least one task failed to start",
            "            finished_task = self.failed_tasks.pop()",
            "            logs = 'Container failed. Check node logs for details'",
            "            results = b''",
            "",
            "        return Result(",
            "            run_id=finished_task.run_id,",
            "            task_id=finished_task.task_id,",
            "            logs=logs,",
            "            data=results,",
            "            status=finished_task.status,",
            "            parent_id=finished_task.parent_id,",
            "        )",
            "",
            "    def login_to_registries(self, registries: list = []) -> None:",
            "        \"\"\"",
            "        Login to the docker registries",
            "",
            "        Parameters",
            "        ----------",
            "        registries: list",
            "            list of registries to login to",
            "        \"\"\"",
            "        for registry in registries:",
            "            try:",
            "                self.docker.login(",
            "                    username=registry.get(\"username\"),",
            "                    password=registry.get(\"password\"),",
            "                    registry=registry.get(\"registry\")",
            "                )",
            "                self.log.info(f\"Logged in to {registry.get('registry')}\")",
            "            except docker.errors.APIError as e:",
            "                self.log.warn(f\"Could not login to {registry.get('registry')}\")",
            "                self.log.debug(e)",
            "",
            "    def link_container_to_network(self, container_name: str,",
            "                                  config_alias: str) -> None:",
            "        \"\"\"",
            "        Link a docker container to the isolated docker network",
            "",
            "        Parameters",
            "        ----------",
            "        container_name: str",
            "            Name of the docker container to be linked to the network",
            "        config_alias: str",
            "            Alias of the docker container defined in the config file",
            "        \"\"\"",
            "        container = get_container(",
            "            docker_client=self.docker, name=container_name",
            "        )",
            "        if not container:",
            "            self.log.error(f\"Could not link docker container {container_name} \"",
            "                           \"that was specified in the configuration file to \"",
            "                           \"the isolated docker network.\")",
            "            self.log.error(\"Container not found!\")",
            "            return",
            "        self.isolated_network_mgr.connect(",
            "            container_name=container_name,",
            "            aliases=[config_alias]",
            "        )",
            "        self.linked_services.append(container_name)",
            "",
            "    def kill_selected_tasks(",
            "        self, org_id: int, kill_list: list[ToBeKilled] = None",
            "    ) -> list[KilledRun]:",
            "        \"\"\"",
            "        Kill tasks specified by a kill list, if they are currently running on",
            "        this node",
            "",
            "        Parameters",
            "        ----------",
            "        org_id: int",
            "            The organization id of this node",
            "        kill_list: list[ToBeKilled]",
            "            A list of info about tasks that should be killed.",
            "",
            "        Returns",
            "        -------",
            "        list[KilledRun]",
            "            List with information on killed tasks",
            "        \"\"\"",
            "        killed_list = []",
            "        for container_to_kill in kill_list:",
            "            if container_to_kill['organization_id'] != org_id:",
            "                continue  # this run is on another node",
            "            # find the task",
            "            task = next((",
            "                t for t in self.active_tasks",
            "                if t.run_id == container_to_kill['run_id']",
            "            ), None)",
            "            if task:",
            "                self.log.info(",
            "                    f\"Killing containers for run_id={task.run_id}\")",
            "                self.active_tasks.remove(task)",
            "                task.cleanup()",
            "                killed_list.append(KilledRun(",
            "                    run_id=task.run_id,",
            "                    task_id=task.task_id,",
            "                    parent_id=task.parent_id,",
            "                ))",
            "            else:",
            "                self.log.warn(",
            "                    \"Received instruction to kill run_id=\"",
            "                    f\"{container_to_kill['run_id']}, but it was not \"",
            "                    \"found running on this node.\")",
            "        return killed_list",
            "",
            "    def kill_tasks(self, org_id: int,",
            "                   kill_list: list[ToBeKilled] = None) -> list[KilledRun]:",
            "        \"\"\"",
            "        Kill tasks currently running on this node.",
            "",
            "        Parameters",
            "        ----------",
            "        org_id: int",
            "            The organization id of this node",
            "        kill_list: list[ToBeKilled] (optional)",
            "            A list of info on tasks that should be killed. If the list",
            "            is not specified, all running algorithm containers will be killed.",
            "",
            "        Returns",
            "        -------",
            "        list[KilledRun]",
            "            List of dictionaries with information on killed tasks",
            "        \"\"\"",
            "        if kill_list:",
            "            killed_runs = self.kill_selected_tasks(org_id=org_id,",
            "                                                   kill_list=kill_list)",
            "        else:",
            "            # received instruction to kill all tasks on this node",
            "            self.log.warn(",
            "                \"Received instruction from server to kill all algorithms \"",
            "                \"running on this node. Executing that now...\")",
            "            killed_runs = self.cleanup_tasks()",
            "            if len(killed_runs):",
            "                self.log.warn(",
            "                    \"Killed the following run ids as instructed via socket:\"",
            "                    f\" {', '.join([str(r.run_id) for r in killed_runs])}\"",
            "                )",
            "            else:",
            "                self.log.warn(",
            "                    \"Instructed to kill tasks but none were running\"",
            "                )",
            "        return killed_runs",
            "",
            "    def get_column_names(self, label: str, type_: str) -> list[str]:",
            "        \"\"\"",
            "        Get column names from a node database",
            "",
            "        Parameters",
            "        ----------",
            "        label: str",
            "            Label of the database",
            "        type_: str",
            "            Type of the database",
            "",
            "        Returns",
            "        -------",
            "        list[str]",
            "            List of column names",
            "        \"\"\"",
            "        db = self.databases.get(label)",
            "        if not db:",
            "            self.log.error(\"Database with label %s not found\", label)",
            "            return []",
            "        if not db['is_file']:",
            "            self.log.error(\"Database with label %s is not a file. Cannot\"",
            "                           \" determine columns without query\", label)",
            "            return []",
            "        if db['type'] == 'excel':",
            "            self.log.error(\"Cannot determine columns for excel database \"",
            "                           \" without a worksheet\")",
            "            return []",
            "        if type_ not in ('csv', 'sparql'):",
            "            self.log.error(\"Cannot determine columns for database of type %s.\"",
            "                           \"Only csv and sparql are supported\", type_)",
            "            return []",
            "        return get_column_names(db['uri'], type_)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "488": [
                "DockerManager",
                "get_result"
            ],
            "489": [
                "DockerManager",
                "get_result"
            ],
            "520": [
                "DockerManager",
                "get_result"
            ]
        },
        "addLocation": []
    }
}