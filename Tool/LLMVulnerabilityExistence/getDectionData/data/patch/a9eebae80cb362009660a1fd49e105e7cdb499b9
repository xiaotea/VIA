{
    "geonode/proxy/tests.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": 178,
                "PatchRowcode": "             },"
            },
            "1": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 179,
                "PatchRowcode": "         )"
            },
            "2": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 180,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+    def test_proxy_url_forgery(self):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 182,
                "PatchRowcode": "+        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\""
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+        import geonode.proxy.views"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+        from urllib.parse import urlsplit"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+        class Response:"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+            status_code = 200"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+            content = \"Hello World\""
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+            headers = {"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+                \"Content-Type\": \"text/plain\","
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\","
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+                \"X-Content-Type-Options\": \"nosniff\","
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 193,
                "PatchRowcode": "+                \"X-XSS-Protection\": \"1; mode=block\","
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+                \"Referrer-Policy\": \"same-origin\","
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+                \"X-Frame-Options\": \"SAMEORIGIN\","
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+                \"Content-Language\": \"en-us\","
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+                \"Content-Length\": \"119\","
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+            }"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+        request_mock = MagicMock()"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+        request_mock.return_value = (Response(), None)"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+        # Non-Legit requests attempting SSRF"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+        geonode.proxy.views.http_client.request = request_mock"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+        url = f\"http://example.org\\@%23{urlsplit(settings.SITEURL).hostname}\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+        response = self.client.get(f\"{self.proxy_url}?url={url}\")"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+        self.assertEqual(response.status_code, 403)"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+        url = f\"http://125.126.127.128\\@%23{urlsplit(settings.SITEURL).hostname}\""
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 213,
                "PatchRowcode": "+        response = self.client.get(f\"{self.proxy_url}?url={url}\")"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 214,
                "PatchRowcode": "+        self.assertEqual(response.status_code, 403)"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 215,
                "PatchRowcode": "+"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+        # Legit requests using the local host (SITEURL)"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 217,
                "PatchRowcode": "+        url = f\"/\\@%23{urlsplit(settings.SITEURL).hostname}\""
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 218,
                "PatchRowcode": "+"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 219,
                "PatchRowcode": "+        response = self.client.get(f\"{self.proxy_url}?url={url}\")"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 220,
                "PatchRowcode": "+        self.assertEqual(response.status_code, 200)"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 221,
                "PatchRowcode": "+"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+        url = f\"{settings.SITEURL}\\@%23{urlsplit(settings.SITEURL).hostname}\""
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 223,
                "PatchRowcode": "+"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 224,
                "PatchRowcode": "+        response = self.client.get(f\"{self.proxy_url}?url={url}\")"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+        self.assertEqual(response.status_code, 200)"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+"
            },
            "49": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 227,
                "PatchRowcode": " "
            },
            "50": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 228,
                "PatchRowcode": " class DownloadResourceTestCase(GeoNodeBaseTestSupport):"
            },
            "51": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "     def setUp(self):"
            }
        },
        "frontPatchFile": [
            "#########################################################################",
            "#",
            "# Copyright (C) 2016 OSGeo",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU General Public License as published by",
            "# the Free Software Foundation, either version 3 of the License, or",
            "# (at your option) any later version.",
            "#",
            "# This program is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the",
            "# GNU General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU General Public License",
            "# along with this program. If not, see <http://www.gnu.org/licenses/>.",
            "#",
            "#########################################################################",
            "",
            "\"\"\"",
            "This file demonstrates two different styles of tests (one doctest and one",
            "unittest). These will both pass when you run \"manage.py test\".",
            "",
            "Replace these with more appropriate tests for your application.",
            "\"\"\"",
            "import json",
            "import io",
            "import zipfile",
            "",
            "from urllib.parse import urljoin",
            "",
            "from django.conf import settings",
            "from geonode.proxy.templatetags.proxy_lib_tags import original_link_available",
            "from django.test.client import RequestFactory",
            "from django.core.files.uploadedfile import SimpleUploadedFile",
            "from unittest.mock import patch",
            "",
            "from geonode.upload.models import Upload",
            "",
            "try:",
            "    from unittest.mock import MagicMock",
            "except ImportError:",
            "    from unittest.mock import MagicMock",
            "",
            "from django.urls import reverse",
            "from django.contrib.auth import get_user_model",
            "from django.test.utils import override_settings",
            "",
            "from geonode import geoserver",
            "from geonode.base.models import Link",
            "from geonode.layers.models import Dataset",
            "from geonode.decorators import on_ogc_backend",
            "from geonode.tests.base import GeoNodeBaseTestSupport",
            "from geonode.base.populate_test_data import create_models, create_single_dataset",
            "",
            "TEST_DOMAIN = \".github.com\"",
            "TEST_URL = f\"https://help{TEST_DOMAIN}/\"",
            "",
            "",
            "class ProxyTest(GeoNodeBaseTestSupport):",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.maxDiff = None",
            "        self.admin = get_user_model().objects.get(username=\"admin\")",
            "",
            "        # FIXME(Ariel): These tests do not work when the computer is offline.",
            "        self.proxy_url = \"/proxy/\"",
            "        self.url = TEST_URL",
            "",
            "    @override_settings(DEBUG=True, PROXY_ALLOWED_HOSTS=())",
            "    def test_validate_host_disabled_in_debug(self):",
            "        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is True, all hosts pass the proxy.\"\"\"",
            "        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")",
            "        if response.status_code != 404:  # 404 - NOT FOUND",
            "            self.assertTrue(response.status_code in (200, 301), response.status_code)",
            "",
            "    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=())",
            "    def test_validate_host_disabled_not_in_debug(self):",
            "        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is False requests should return 403.\"\"\"",
            "        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")",
            "        if response.status_code != 404:  # 404 - NOT FOUND",
            "            self.assertEqual(response.status_code, 403, response.status_code)",
            "",
            "    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=(TEST_DOMAIN,))",
            "    def test_proxy_allowed_host(self):",
            "        \"\"\"If PROXY_ALLOWED_HOSTS is not empty and DEBUG is False requests should return no error.\"\"\"",
            "        self.client.login(username=\"admin\", password=\"admin\")",
            "        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")",
            "        if response.status_code != 404:  # 404 - NOT FOUND",
            "            self.assertEqual(response.status_code, 200, response.status_code)",
            "",
            "    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=())",
            "    def test_validate_remote_services_hosts(self):",
            "        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is False requests should return 200",
            "        for Remote Services hosts.\"\"\"",
            "        from geonode.services.models import Service",
            "        from geonode.services.enumerations import WMS, INDEXED",
            "",
            "        Service.objects.get_or_create(",
            "            type=WMS,",
            "            name=\"Bogus\",",
            "            title=\"Pocus\",",
            "            owner=self.admin,",
            "            method=INDEXED,",
            "            base_url=\"http://bogus.pocus.com/ows\",",
            "        )",
            "        response = self.client.get(f\"{self.proxy_url}?url=http://bogus.pocus.com/ows/wms?request=GetCapabilities\")",
            "        # 200 - FOUND",
            "        self.assertTrue(response.status_code in (200, 301))",
            "",
            "    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=(\".example.org\",))",
            "    def test_relative_urls(self):",
            "        \"\"\"Proxying to a URL with a relative path element should normalise the path into",
            "        an absolute path before calling the remote URL.\"\"\"",
            "        import geonode.proxy.views",
            "",
            "        class Response:",
            "            status_code = 200",
            "            content = \"Hello World\"",
            "            headers = {\"Content-Type\": \"text/html\"}",
            "",
            "        request_mock = MagicMock()",
            "        request_mock.return_value = (Response(), None)",
            "",
            "        geonode.proxy.views.http_client.request = request_mock",
            "        url = \"http://example.org/test/test/../../index.html\"",
            "",
            "        self.client.get(f\"{self.proxy_url}?url={url}\")",
            "        assert request_mock.call_args[0][0] == \"http://example.org/index.html\"",
            "",
            "    def test_proxy_preserve_headers(self):",
            "        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\"",
            "        import geonode.proxy.views",
            "",
            "        _test_headers = {",
            "            \"Access-Control-Allow-Credentials\": False,",
            "            \"Access-Control-Allow-Headers\": \"Content-Type, Accept, Authorization, Origin, User-Agent\",",
            "            \"Access-Control-Allow-Methods\": \"GET, POST, PUT, PATCH, OPTIONS\",",
            "            \"Cache-Control\": \"public, must-revalidate, max-age = 30\",",
            "            \"Connection\": \"keep-alive\",",
            "            \"Content-Language\": \"en\",",
            "            \"Content-Length\": 116559,",
            "            \"Content-Type\": \"image/tiff\",",
            "            \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',",
            "            \"Date\": \"Fri, 05 Nov 2021 17: 19: 11 GMT\",",
            "            \"Server\": \"nginx/1.17.2\",",
            "            \"Set-Cookie\": \"sessionid = bogus-pocus; HttpOnly; Path=/; SameSite=Lax\",",
            "            \"Strict-Transport-Security\": \"max-age=3600; includeSubDomains\",",
            "            \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",",
            "            \"X-Content-Type-Options\": \"nosniff\",",
            "            \"X-XSS-Protection\": \"1; mode=block\",",
            "        }",
            "",
            "        class Response:",
            "            status_code = 200",
            "            content = \"Hello World\"",
            "            headers = _test_headers",
            "",
            "        request_mock = MagicMock()",
            "        request_mock.return_value = (Response(), None)",
            "",
            "        geonode.proxy.views.http_client.request = request_mock",
            "        url = \"http://example.org/test/test/../../image.tiff\"",
            "",
            "        response = self.client.get(f\"{self.proxy_url}?url={url}\")",
            "        self.assertDictContainsSubset(",
            "            dict(response.headers.copy()),",
            "            {",
            "                \"Content-Type\": \"text/plain\",",
            "                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",",
            "                \"X-Content-Type-Options\": \"nosniff\",",
            "                \"X-XSS-Protection\": \"1; mode=block\",",
            "                \"Referrer-Policy\": \"same-origin\",",
            "                \"X-Frame-Options\": \"SAMEORIGIN\",",
            "                \"Content-Language\": \"en-us\",",
            "                \"Content-Length\": \"119\",",
            "                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',",
            "            },",
            "        )",
            "",
            "",
            "class DownloadResourceTestCase(GeoNodeBaseTestSupport):",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.maxDiff = None",
            "        create_models(type=\"dataset\")",
            "",
            "    @on_ogc_backend(geoserver.BACKEND_PACKAGE)",
            "    def test_download_url_with_not_existing_file(self):",
            "        dataset = Dataset.objects.all().first()",
            "        self.client.login(username=\"admin\", password=\"admin\")",
            "        # ... all should be good",
            "        response = self.client.get(reverse(\"download\", args=(dataset.id,)))",
            "        # Espected 404 since there are no files available for this layer",
            "        self.assertEqual(response.status_code, 404)",
            "        content = response.content",
            "        if isinstance(content, bytes):",
            "            content = content.decode(\"UTF-8\")",
            "        data = content",
            "        self.assertTrue(\"No files have been found for this resource. Please, contact a system administrator.\" in data)",
            "",
            "    @patch(\"geonode.storage.manager.storage_manager.exists\")",
            "    @patch(\"geonode.storage.manager.storage_manager.open\")",
            "    @on_ogc_backend(geoserver.BACKEND_PACKAGE)",
            "    def test_download_url_with_existing_files(self, fopen, fexists):",
            "        fexists.return_value = True",
            "        fopen.return_value = SimpleUploadedFile(\"foo_file.shp\", b\"scc\")",
            "        dataset = Dataset.objects.all().first()",
            "",
            "        dataset.files = [",
            "            \"/tmpe1exb9e9/foo_file.dbf\",",
            "            \"/tmpe1exb9e9/foo_file.prj\",",
            "            \"/tmpe1exb9e9/foo_file.shp\",",
            "            \"/tmpe1exb9e9/foo_file.shx\",",
            "        ]",
            "",
            "        dataset.save()",
            "",
            "        dataset.refresh_from_db()",
            "",
            "        upload = Upload.objects.create(state=\"RUNNING\", resource=dataset)",
            "",
            "        assert upload",
            "",
            "        self.client.login(username=\"admin\", password=\"admin\")",
            "        # ... all should be good",
            "        response = self.client.get(reverse(\"download\", args=(dataset.id,)))",
            "        # Espected 404 since there are no files available for this layer",
            "        self.assertEqual(response.status_code, 200)",
            "        self.assertEqual(\"application/zip\", response.headers.get(\"Content-Type\"))",
            "        self.assertEqual('attachment; filename=\"CA.zip\"', response.headers.get(\"Content-Disposition\"))",
            "",
            "    @patch(\"geonode.storage.manager.storage_manager.exists\")",
            "    @patch(\"geonode.storage.manager.storage_manager.open\")",
            "    @on_ogc_backend(geoserver.BACKEND_PACKAGE)",
            "    def test_download_files(self, fopen, fexists):",
            "        fexists.return_value = True",
            "        fopen.return_value = SimpleUploadedFile(\"foo_file.shp\", b\"scc\")",
            "        dataset = Dataset.objects.all().first()",
            "",
            "        dataset.files = [",
            "            \"/tmpe1exb9e9/foo_file.dbf\",",
            "            \"/tmpe1exb9e9/foo_file.prj\",",
            "            \"/tmpe1exb9e9/foo_file.shp\",",
            "            \"/tmpe1exb9e9/foo_file.shx\",",
            "        ]",
            "",
            "        dataset.save()",
            "",
            "        dataset.refresh_from_db()",
            "",
            "        Upload.objects.create(state=\"COMPLETE\", resource=dataset)",
            "",
            "        self.client.login(username=\"admin\", password=\"admin\")",
            "        response = self.client.get(reverse(\"download\", args=(dataset.id,)))",
            "        # headers and status assertions",
            "        self.assertEqual(response.status_code, 200)",
            "        self.assertEqual(response.get(\"content-type\"), \"application/zip\")",
            "        self.assertEqual(response.get(\"content-disposition\"), f'attachment; filename=\"{dataset.name}.zip\"')",
            "        # Inspect content",
            "        zip_content = io.BytesIO(b\"\".join(response.streaming_content))",
            "        zip = zipfile.ZipFile(zip_content)",
            "        zip_files = zip.namelist()",
            "        self.assertEqual(len(zip_files), 4)",
            "        self.assertIn(\".shp\", \"\".join(zip_files))",
            "        self.assertIn(\".dbf\", \"\".join(zip_files))",
            "        self.assertIn(\".shx\", \"\".join(zip_files))",
            "        self.assertIn(\".prj\", \"\".join(zip_files))",
            "",
            "",
            "class OWSApiTestCase(GeoNodeBaseTestSupport):",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.maxDiff = None",
            "        create_models(type=\"dataset\")",
            "        # prepare some WMS endpoints",
            "        q = Link.objects.all()",
            "        for lyr in q[:3]:",
            "            lyr.link_type = \"OGC:WMS\"",
            "            lyr.save()",
            "",
            "    def test_ows_api(self):",
            "        url = \"/api/ows_endpoints/\"",
            "        q = Link.objects.filter(link_type__startswith=\"OGC:\")",
            "        resp = self.client.get(url)",
            "        self.assertEqual(resp.status_code, 200)",
            "        content = resp.content",
            "        if isinstance(content, bytes):",
            "            content = content.decode(\"UTF-8\")",
            "        data = json.loads(content)",
            "        self.assertTrue(len(data[\"data\"]), q.count())",
            "",
            "",
            "@override_settings(SITEURL=\"http://localhost:8000\")",
            "class TestProxyTags(GeoNodeBaseTestSupport):",
            "    def setUp(self):",
            "        self.maxDiff = None",
            "        self.resource = create_single_dataset(\"foo_dataset\")",
            "        r = RequestFactory()",
            "        self.url = urljoin(settings.SITEURL, reverse(\"download\", args={self.resource.id}))",
            "        r.get(self.url)",
            "        admin = get_user_model().objects.get(username=\"admin\")",
            "        r.user = admin",
            "        self.context = {\"request\": r}",
            "",
            "    def test_tag_original_link_available_with_different_netlock_should_return_true(self):",
            "        actual = original_link_available(self.context, self.resource.resourcebase_ptr_id, \"http://url.com/\")",
            "        self.assertTrue(actual)",
            "",
            "    def test_should_return_false_if_no_files_are_available(self):",
            "        _ = Upload.objects.create(state=\"RUNNING\", resource=self.resource)",
            "",
            "        actual = original_link_available(self.context, self.resource.resourcebase_ptr_id, self.url)",
            "        self.assertFalse(actual)",
            "",
            "    @patch(\"geonode.storage.manager.storage_manager.exists\", return_value=True)",
            "    def test_should_return_true_if_files_are_available(self, fexists):",
            "        upload = Upload.objects.create(state=\"RUNNING\", resource=self.resource)",
            "",
            "        assert upload",
            "",
            "        self.resource.files = [",
            "            \"/tmpe1exb9e9/foo_file.dbf\",",
            "            \"/tmpe1exb9e9/foo_file.prj\",",
            "            \"/tmpe1exb9e9/foo_file.shp\",",
            "            \"/tmpe1exb9e9/foo_file.shx\",",
            "        ]",
            "",
            "        self.resource.save()",
            "",
            "        self.resource.refresh_from_db()",
            "",
            "        actual = original_link_available(self.context, self.resource.resourcebase_ptr_id, self.url)",
            "        self.assertTrue(actual)"
        ],
        "afterPatchFile": [
            "#########################################################################",
            "#",
            "# Copyright (C) 2016 OSGeo",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU General Public License as published by",
            "# the Free Software Foundation, either version 3 of the License, or",
            "# (at your option) any later version.",
            "#",
            "# This program is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the",
            "# GNU General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU General Public License",
            "# along with this program. If not, see <http://www.gnu.org/licenses/>.",
            "#",
            "#########################################################################",
            "",
            "\"\"\"",
            "This file demonstrates two different styles of tests (one doctest and one",
            "unittest). These will both pass when you run \"manage.py test\".",
            "",
            "Replace these with more appropriate tests for your application.",
            "\"\"\"",
            "import json",
            "import io",
            "import zipfile",
            "",
            "from urllib.parse import urljoin",
            "",
            "from django.conf import settings",
            "from geonode.proxy.templatetags.proxy_lib_tags import original_link_available",
            "from django.test.client import RequestFactory",
            "from django.core.files.uploadedfile import SimpleUploadedFile",
            "from unittest.mock import patch",
            "",
            "from geonode.upload.models import Upload",
            "",
            "try:",
            "    from unittest.mock import MagicMock",
            "except ImportError:",
            "    from unittest.mock import MagicMock",
            "",
            "from django.urls import reverse",
            "from django.contrib.auth import get_user_model",
            "from django.test.utils import override_settings",
            "",
            "from geonode import geoserver",
            "from geonode.base.models import Link",
            "from geonode.layers.models import Dataset",
            "from geonode.decorators import on_ogc_backend",
            "from geonode.tests.base import GeoNodeBaseTestSupport",
            "from geonode.base.populate_test_data import create_models, create_single_dataset",
            "",
            "TEST_DOMAIN = \".github.com\"",
            "TEST_URL = f\"https://help{TEST_DOMAIN}/\"",
            "",
            "",
            "class ProxyTest(GeoNodeBaseTestSupport):",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.maxDiff = None",
            "        self.admin = get_user_model().objects.get(username=\"admin\")",
            "",
            "        # FIXME(Ariel): These tests do not work when the computer is offline.",
            "        self.proxy_url = \"/proxy/\"",
            "        self.url = TEST_URL",
            "",
            "    @override_settings(DEBUG=True, PROXY_ALLOWED_HOSTS=())",
            "    def test_validate_host_disabled_in_debug(self):",
            "        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is True, all hosts pass the proxy.\"\"\"",
            "        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")",
            "        if response.status_code != 404:  # 404 - NOT FOUND",
            "            self.assertTrue(response.status_code in (200, 301), response.status_code)",
            "",
            "    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=())",
            "    def test_validate_host_disabled_not_in_debug(self):",
            "        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is False requests should return 403.\"\"\"",
            "        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")",
            "        if response.status_code != 404:  # 404 - NOT FOUND",
            "            self.assertEqual(response.status_code, 403, response.status_code)",
            "",
            "    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=(TEST_DOMAIN,))",
            "    def test_proxy_allowed_host(self):",
            "        \"\"\"If PROXY_ALLOWED_HOSTS is not empty and DEBUG is False requests should return no error.\"\"\"",
            "        self.client.login(username=\"admin\", password=\"admin\")",
            "        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")",
            "        if response.status_code != 404:  # 404 - NOT FOUND",
            "            self.assertEqual(response.status_code, 200, response.status_code)",
            "",
            "    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=())",
            "    def test_validate_remote_services_hosts(self):",
            "        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is False requests should return 200",
            "        for Remote Services hosts.\"\"\"",
            "        from geonode.services.models import Service",
            "        from geonode.services.enumerations import WMS, INDEXED",
            "",
            "        Service.objects.get_or_create(",
            "            type=WMS,",
            "            name=\"Bogus\",",
            "            title=\"Pocus\",",
            "            owner=self.admin,",
            "            method=INDEXED,",
            "            base_url=\"http://bogus.pocus.com/ows\",",
            "        )",
            "        response = self.client.get(f\"{self.proxy_url}?url=http://bogus.pocus.com/ows/wms?request=GetCapabilities\")",
            "        # 200 - FOUND",
            "        self.assertTrue(response.status_code in (200, 301))",
            "",
            "    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=(\".example.org\",))",
            "    def test_relative_urls(self):",
            "        \"\"\"Proxying to a URL with a relative path element should normalise the path into",
            "        an absolute path before calling the remote URL.\"\"\"",
            "        import geonode.proxy.views",
            "",
            "        class Response:",
            "            status_code = 200",
            "            content = \"Hello World\"",
            "            headers = {\"Content-Type\": \"text/html\"}",
            "",
            "        request_mock = MagicMock()",
            "        request_mock.return_value = (Response(), None)",
            "",
            "        geonode.proxy.views.http_client.request = request_mock",
            "        url = \"http://example.org/test/test/../../index.html\"",
            "",
            "        self.client.get(f\"{self.proxy_url}?url={url}\")",
            "        assert request_mock.call_args[0][0] == \"http://example.org/index.html\"",
            "",
            "    def test_proxy_preserve_headers(self):",
            "        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\"",
            "        import geonode.proxy.views",
            "",
            "        _test_headers = {",
            "            \"Access-Control-Allow-Credentials\": False,",
            "            \"Access-Control-Allow-Headers\": \"Content-Type, Accept, Authorization, Origin, User-Agent\",",
            "            \"Access-Control-Allow-Methods\": \"GET, POST, PUT, PATCH, OPTIONS\",",
            "            \"Cache-Control\": \"public, must-revalidate, max-age = 30\",",
            "            \"Connection\": \"keep-alive\",",
            "            \"Content-Language\": \"en\",",
            "            \"Content-Length\": 116559,",
            "            \"Content-Type\": \"image/tiff\",",
            "            \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',",
            "            \"Date\": \"Fri, 05 Nov 2021 17: 19: 11 GMT\",",
            "            \"Server\": \"nginx/1.17.2\",",
            "            \"Set-Cookie\": \"sessionid = bogus-pocus; HttpOnly; Path=/; SameSite=Lax\",",
            "            \"Strict-Transport-Security\": \"max-age=3600; includeSubDomains\",",
            "            \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",",
            "            \"X-Content-Type-Options\": \"nosniff\",",
            "            \"X-XSS-Protection\": \"1; mode=block\",",
            "        }",
            "",
            "        class Response:",
            "            status_code = 200",
            "            content = \"Hello World\"",
            "            headers = _test_headers",
            "",
            "        request_mock = MagicMock()",
            "        request_mock.return_value = (Response(), None)",
            "",
            "        geonode.proxy.views.http_client.request = request_mock",
            "        url = \"http://example.org/test/test/../../image.tiff\"",
            "",
            "        response = self.client.get(f\"{self.proxy_url}?url={url}\")",
            "        self.assertDictContainsSubset(",
            "            dict(response.headers.copy()),",
            "            {",
            "                \"Content-Type\": \"text/plain\",",
            "                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",",
            "                \"X-Content-Type-Options\": \"nosniff\",",
            "                \"X-XSS-Protection\": \"1; mode=block\",",
            "                \"Referrer-Policy\": \"same-origin\",",
            "                \"X-Frame-Options\": \"SAMEORIGIN\",",
            "                \"Content-Language\": \"en-us\",",
            "                \"Content-Length\": \"119\",",
            "                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',",
            "            },",
            "        )",
            "",
            "    def test_proxy_url_forgery(self):",
            "        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\"",
            "        import geonode.proxy.views",
            "        from urllib.parse import urlsplit",
            "",
            "        class Response:",
            "            status_code = 200",
            "            content = \"Hello World\"",
            "            headers = {",
            "                \"Content-Type\": \"text/plain\",",
            "                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",",
            "                \"X-Content-Type-Options\": \"nosniff\",",
            "                \"X-XSS-Protection\": \"1; mode=block\",",
            "                \"Referrer-Policy\": \"same-origin\",",
            "                \"X-Frame-Options\": \"SAMEORIGIN\",",
            "                \"Content-Language\": \"en-us\",",
            "                \"Content-Length\": \"119\",",
            "                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',",
            "            }",
            "",
            "        request_mock = MagicMock()",
            "        request_mock.return_value = (Response(), None)",
            "",
            "        # Non-Legit requests attempting SSRF",
            "        geonode.proxy.views.http_client.request = request_mock",
            "        url = f\"http://example.org\\@%23{urlsplit(settings.SITEURL).hostname}\"",
            "",
            "        response = self.client.get(f\"{self.proxy_url}?url={url}\")",
            "        self.assertEqual(response.status_code, 403)",
            "",
            "        url = f\"http://125.126.127.128\\@%23{urlsplit(settings.SITEURL).hostname}\"",
            "",
            "        response = self.client.get(f\"{self.proxy_url}?url={url}\")",
            "        self.assertEqual(response.status_code, 403)",
            "",
            "        # Legit requests using the local host (SITEURL)",
            "        url = f\"/\\@%23{urlsplit(settings.SITEURL).hostname}\"",
            "",
            "        response = self.client.get(f\"{self.proxy_url}?url={url}\")",
            "        self.assertEqual(response.status_code, 200)",
            "",
            "        url = f\"{settings.SITEURL}\\@%23{urlsplit(settings.SITEURL).hostname}\"",
            "",
            "        response = self.client.get(f\"{self.proxy_url}?url={url}\")",
            "        self.assertEqual(response.status_code, 200)",
            "",
            "",
            "class DownloadResourceTestCase(GeoNodeBaseTestSupport):",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.maxDiff = None",
            "        create_models(type=\"dataset\")",
            "",
            "    @on_ogc_backend(geoserver.BACKEND_PACKAGE)",
            "    def test_download_url_with_not_existing_file(self):",
            "        dataset = Dataset.objects.all().first()",
            "        self.client.login(username=\"admin\", password=\"admin\")",
            "        # ... all should be good",
            "        response = self.client.get(reverse(\"download\", args=(dataset.id,)))",
            "        # Espected 404 since there are no files available for this layer",
            "        self.assertEqual(response.status_code, 404)",
            "        content = response.content",
            "        if isinstance(content, bytes):",
            "            content = content.decode(\"UTF-8\")",
            "        data = content",
            "        self.assertTrue(\"No files have been found for this resource. Please, contact a system administrator.\" in data)",
            "",
            "    @patch(\"geonode.storage.manager.storage_manager.exists\")",
            "    @patch(\"geonode.storage.manager.storage_manager.open\")",
            "    @on_ogc_backend(geoserver.BACKEND_PACKAGE)",
            "    def test_download_url_with_existing_files(self, fopen, fexists):",
            "        fexists.return_value = True",
            "        fopen.return_value = SimpleUploadedFile(\"foo_file.shp\", b\"scc\")",
            "        dataset = Dataset.objects.all().first()",
            "",
            "        dataset.files = [",
            "            \"/tmpe1exb9e9/foo_file.dbf\",",
            "            \"/tmpe1exb9e9/foo_file.prj\",",
            "            \"/tmpe1exb9e9/foo_file.shp\",",
            "            \"/tmpe1exb9e9/foo_file.shx\",",
            "        ]",
            "",
            "        dataset.save()",
            "",
            "        dataset.refresh_from_db()",
            "",
            "        upload = Upload.objects.create(state=\"RUNNING\", resource=dataset)",
            "",
            "        assert upload",
            "",
            "        self.client.login(username=\"admin\", password=\"admin\")",
            "        # ... all should be good",
            "        response = self.client.get(reverse(\"download\", args=(dataset.id,)))",
            "        # Espected 404 since there are no files available for this layer",
            "        self.assertEqual(response.status_code, 200)",
            "        self.assertEqual(\"application/zip\", response.headers.get(\"Content-Type\"))",
            "        self.assertEqual('attachment; filename=\"CA.zip\"', response.headers.get(\"Content-Disposition\"))",
            "",
            "    @patch(\"geonode.storage.manager.storage_manager.exists\")",
            "    @patch(\"geonode.storage.manager.storage_manager.open\")",
            "    @on_ogc_backend(geoserver.BACKEND_PACKAGE)",
            "    def test_download_files(self, fopen, fexists):",
            "        fexists.return_value = True",
            "        fopen.return_value = SimpleUploadedFile(\"foo_file.shp\", b\"scc\")",
            "        dataset = Dataset.objects.all().first()",
            "",
            "        dataset.files = [",
            "            \"/tmpe1exb9e9/foo_file.dbf\",",
            "            \"/tmpe1exb9e9/foo_file.prj\",",
            "            \"/tmpe1exb9e9/foo_file.shp\",",
            "            \"/tmpe1exb9e9/foo_file.shx\",",
            "        ]",
            "",
            "        dataset.save()",
            "",
            "        dataset.refresh_from_db()",
            "",
            "        Upload.objects.create(state=\"COMPLETE\", resource=dataset)",
            "",
            "        self.client.login(username=\"admin\", password=\"admin\")",
            "        response = self.client.get(reverse(\"download\", args=(dataset.id,)))",
            "        # headers and status assertions",
            "        self.assertEqual(response.status_code, 200)",
            "        self.assertEqual(response.get(\"content-type\"), \"application/zip\")",
            "        self.assertEqual(response.get(\"content-disposition\"), f'attachment; filename=\"{dataset.name}.zip\"')",
            "        # Inspect content",
            "        zip_content = io.BytesIO(b\"\".join(response.streaming_content))",
            "        zip = zipfile.ZipFile(zip_content)",
            "        zip_files = zip.namelist()",
            "        self.assertEqual(len(zip_files), 4)",
            "        self.assertIn(\".shp\", \"\".join(zip_files))",
            "        self.assertIn(\".dbf\", \"\".join(zip_files))",
            "        self.assertIn(\".shx\", \"\".join(zip_files))",
            "        self.assertIn(\".prj\", \"\".join(zip_files))",
            "",
            "",
            "class OWSApiTestCase(GeoNodeBaseTestSupport):",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.maxDiff = None",
            "        create_models(type=\"dataset\")",
            "        # prepare some WMS endpoints",
            "        q = Link.objects.all()",
            "        for lyr in q[:3]:",
            "            lyr.link_type = \"OGC:WMS\"",
            "            lyr.save()",
            "",
            "    def test_ows_api(self):",
            "        url = \"/api/ows_endpoints/\"",
            "        q = Link.objects.filter(link_type__startswith=\"OGC:\")",
            "        resp = self.client.get(url)",
            "        self.assertEqual(resp.status_code, 200)",
            "        content = resp.content",
            "        if isinstance(content, bytes):",
            "            content = content.decode(\"UTF-8\")",
            "        data = json.loads(content)",
            "        self.assertTrue(len(data[\"data\"]), q.count())",
            "",
            "",
            "@override_settings(SITEURL=\"http://localhost:8000\")",
            "class TestProxyTags(GeoNodeBaseTestSupport):",
            "    def setUp(self):",
            "        self.maxDiff = None",
            "        self.resource = create_single_dataset(\"foo_dataset\")",
            "        r = RequestFactory()",
            "        self.url = urljoin(settings.SITEURL, reverse(\"download\", args={self.resource.id}))",
            "        r.get(self.url)",
            "        admin = get_user_model().objects.get(username=\"admin\")",
            "        r.user = admin",
            "        self.context = {\"request\": r}",
            "",
            "    def test_tag_original_link_available_with_different_netlock_should_return_true(self):",
            "        actual = original_link_available(self.context, self.resource.resourcebase_ptr_id, \"http://url.com/\")",
            "        self.assertTrue(actual)",
            "",
            "    def test_should_return_false_if_no_files_are_available(self):",
            "        _ = Upload.objects.create(state=\"RUNNING\", resource=self.resource)",
            "",
            "        actual = original_link_available(self.context, self.resource.resourcebase_ptr_id, self.url)",
            "        self.assertFalse(actual)",
            "",
            "    @patch(\"geonode.storage.manager.storage_manager.exists\", return_value=True)",
            "    def test_should_return_true_if_files_are_available(self, fexists):",
            "        upload = Upload.objects.create(state=\"RUNNING\", resource=self.resource)",
            "",
            "        assert upload",
            "",
            "        self.resource.files = [",
            "            \"/tmpe1exb9e9/foo_file.dbf\",",
            "            \"/tmpe1exb9e9/foo_file.prj\",",
            "            \"/tmpe1exb9e9/foo_file.shp\",",
            "            \"/tmpe1exb9e9/foo_file.shx\",",
            "        ]",
            "",
            "        self.resource.save()",
            "",
            "        self.resource.refresh_from_db()",
            "",
            "        actual = original_link_available(self.context, self.resource.resourcebase_ptr_id, self.url)",
            "        self.assertTrue(actual)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "openstack_dashboard.dashboards.project.stacks.forms.TemplateForm"
        ]
    },
    "geonode/proxy/views.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " from geonode.upload.models import Upload"
            },
            "1": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " from geonode.base.models import ResourceBase"
            },
            "2": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " from geonode.storage.manager import storage_manager"
            },
            "3": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from geonode.utils import resolve_object, check_ogc_backend, get_headers, http_client, json_response"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+from geonode.utils import ("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+    resolve_object,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    check_ogc_backend,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+    get_headers,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+    http_client,"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+    json_response,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+    extract_ip_or_domain,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+)"
            },
            "12": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 51,
                "PatchRowcode": " from geonode.base.enumerations import LINK_TYPES as _LT"
            },
            "13": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 52,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " from geonode import geoserver  # noqa"
            },
            "15": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "             _remote_host = urlsplit(_s.base_url).hostname"
            },
            "16": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "             PROXY_ALLOWED_HOSTS += (_remote_host,)"
            },
            "17": {
                "beforePatchRowNumber": 132,
                "afterPatchRowNumber": 139,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 133,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not validate_host(url.hostname, PROXY_ALLOWED_HOSTS):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+        if not validate_host(extract_ip_or_domain(raw_url), PROXY_ALLOWED_HOSTS):"
            },
            "20": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "             return HttpResponse("
            },
            "21": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": 142,
                "PatchRowcode": "                 \"DEBUG is set to False but the host of the path provided to the proxy service\""
            },
            "22": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 143,
                "PatchRowcode": "                 \" is not in the PROXY_ALLOWED_HOSTS setting.\","
            }
        },
        "frontPatchFile": [
            "#########################################################################",
            "#",
            "# Copyright (C) 2016 OSGeo",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU General Public License as published by",
            "# the Free Software Foundation, either version 3 of the License, or",
            "# (at your option) any later version.",
            "#",
            "# This program is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the",
            "# GNU General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU General Public License",
            "# along with this program. If not, see <http://www.gnu.org/licenses/>.",
            "#",
            "#########################################################################",
            "import io",
            "import os",
            "import re",
            "import gzip",
            "import logging",
            "import traceback",
            "import zipstream",
            "",
            "from hyperlink import URL",
            "from urllib.parse import urlparse, urlsplit, urljoin",
            "",
            "from django.conf import settings",
            "from django.template import loader",
            "from django.http import HttpResponse, StreamingHttpResponse",
            "from django.views.generic import View",
            "from distutils.version import StrictVersion",
            "from django.http.request import validate_host",
            "from django.utils.translation import ugettext as _",
            "from django.views.decorators.csrf import requires_csrf_token",
            "",
            "from geonode.layers.models import Dataset",
            "from geonode.upload.models import Upload",
            "from geonode.base.models import ResourceBase",
            "from geonode.storage.manager import storage_manager",
            "from geonode.utils import resolve_object, check_ogc_backend, get_headers, http_client, json_response",
            "from geonode.base.enumerations import LINK_TYPES as _LT",
            "",
            "from geonode import geoserver  # noqa",
            "from geonode.base import register_event",
            "from geonode.base.auth import get_auth_user, get_token_from_auth_header",
            "",
            "BUFFER_CHUNK_SIZE = 64 * 1024",
            "",
            "TIMEOUT = 30",
            "",
            "LINK_TYPES = [L for L in _LT if L.startswith(\"OGC:\")]",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "ows_regexp = re.compile(r\"^(?i)(version)=(\\d\\.\\d\\.\\d)(?i)&(?i)request=(?i)(GetCapabilities)&(?i)service=(?i)(\\w\\w\\w)$\")",
            "",
            "",
            "@requires_csrf_token",
            "def proxy(",
            "    request,",
            "    url=None,",
            "    response_callback=None,",
            "    sec_chk_hosts=True,",
            "    sec_chk_rules=True,",
            "    timeout=None,",
            "    allowed_hosts=[],",
            "    headers=None,",
            "    access_token=None,",
            "    **kwargs,",
            "):",
            "    # Request default timeout",
            "    from geonode.geoserver.helpers import ogc_server_settings",
            "",
            "    if not timeout:",
            "        timeout = getattr(ogc_server_settings, \"TIMEOUT\", TIMEOUT)",
            "",
            "    # Security rules and settings",
            "    PROXY_ALLOWED_HOSTS = getattr(settings, \"PROXY_ALLOWED_HOSTS\", ())",
            "",
            "    # Sanity url checks",
            "    if \"url\" not in request.GET and not url:",
            "        return HttpResponse(",
            "            \"The proxy service requires a URL-encoded URL as a parameter.\", status=400, content_type=\"text/plain\"",
            "        )",
            "",
            "    raw_url = url or request.GET[\"url\"]",
            "    raw_url = urljoin(settings.SITEURL, raw_url) if raw_url.startswith(\"/\") else raw_url",
            "    url = urlsplit(raw_url)",
            "    scheme = str(url.scheme)",
            "    locator = str(url.path)",
            "    if url.query != \"\":",
            "        locator += f\"?{url.query}\"",
            "    if url.fragment != \"\":",
            "        locator += f\"#{url.fragment}\"",
            "",
            "    # White-Black Listing Hosts",
            "    site_url = urlsplit(settings.SITEURL)",
            "    if sec_chk_hosts and not settings.DEBUG:",
            "        # Attach current SITEURL",
            "        if site_url.hostname not in PROXY_ALLOWED_HOSTS:",
            "            PROXY_ALLOWED_HOSTS += (site_url.hostname,)",
            "",
            "        # Attach current hostname",
            "        hostname = (ogc_server_settings.hostname,) if ogc_server_settings else ()",
            "        if hostname not in PROXY_ALLOWED_HOSTS:",
            "            PROXY_ALLOWED_HOSTS += hostname",
            "",
            "        # Check OWS regexp",
            "        if url.query and ows_regexp.match(url.query):",
            "            ows_tokens = ows_regexp.match(url.query).groups()",
            "            if (",
            "                len(ows_tokens) == 4",
            "                and \"version\" == ows_tokens[0]",
            "                and StrictVersion(ows_tokens[1]) >= StrictVersion(\"1.0.0\")",
            "                and StrictVersion(ows_tokens[1]) <= StrictVersion(\"3.0.0\")",
            "                and ows_tokens[2].lower() in (\"getcapabilities\")",
            "                and ows_tokens[3].upper() in (\"OWS\", \"WCS\", \"WFS\", \"WMS\", \"WPS\", \"CSW\")",
            "            ):",
            "                if url.hostname not in PROXY_ALLOWED_HOSTS:",
            "                    PROXY_ALLOWED_HOSTS += (url.hostname,)",
            "",
            "        # Check Remote Services base_urls",
            "        from geonode.services.models import Service",
            "",
            "        for _s in Service.objects.all():",
            "            _remote_host = urlsplit(_s.base_url).hostname",
            "            PROXY_ALLOWED_HOSTS += (_remote_host,)",
            "",
            "        if not validate_host(url.hostname, PROXY_ALLOWED_HOSTS):",
            "            return HttpResponse(",
            "                \"DEBUG is set to False but the host of the path provided to the proxy service\"",
            "                \" is not in the PROXY_ALLOWED_HOSTS setting.\",",
            "                status=403,",
            "                content_type=\"text/plain\",",
            "            )",
            "",
            "    # Security checks based on rules; allow only specific requests",
            "    if sec_chk_rules:",
            "        # TODO: Not yet implemented",
            "        pass",
            "",
            "    # Collecting headers and cookies",
            "    if not headers:",
            "        headers, access_token = get_headers(request, url, raw_url, allowed_hosts=allowed_hosts)",
            "    if not access_token:",
            "        auth_header = None",
            "        if \"Authorization\" in headers:",
            "            auth_header = headers[\"Authorization\"]",
            "        elif \"HTTP_AUTHORIZATION\" in request.META:",
            "            auth_header = request.META.get(\"HTTP_AUTHORIZATION\", request.META.get(\"HTTP_AUTHORIZATION2\"))",
            "        if auth_header:",
            "            access_token = get_token_from_auth_header(auth_header, create_if_not_exists=True)",
            "    user = get_auth_user(access_token)",
            "",
            "    # Inject access_token if necessary",
            "    parsed = urlparse(raw_url)",
            "    parsed._replace(path=locator.encode(\"utf8\"))",
            "    if parsed.netloc == site_url.netloc and scheme != site_url.scheme:",
            "        parsed = parsed._replace(scheme=site_url.scheme)",
            "",
            "    _url = parsed.geturl()",
            "",
            "    # Some clients / JS libraries generate URLs with relative URL paths, e.g.",
            "    # \"http://host/path/path/../file.css\", which the requests library cannot",
            "    # currently handle (https://github.com/kennethreitz/requests/issues/2982).",
            "    # We parse and normalise such URLs into absolute paths before attempting",
            "    # to proxy the request.",
            "    _url = URL.from_text(_url).normalize().to_text()",
            "",
            "    if request.method == \"GET\" and access_token and \"access_token\" not in _url:",
            "        query_separator = \"&\" if \"?\" in _url else \"?\"",
            "        _url = f\"{_url}{query_separator}access_token={access_token}\"",
            "",
            "    _data = request.body.decode(\"utf-8\")",
            "",
            "    # Avoid translating local geoserver calls into external ones",
            "    if check_ogc_backend(geoserver.BACKEND_PACKAGE):",
            "        from geonode.geoserver.helpers import ogc_server_settings",
            "",
            "        _url = _url.replace(f\"{settings.SITEURL}geoserver\", ogc_server_settings.LOCATION.rstrip(\"/\"))",
            "        _data = _data.replace(f\"{settings.SITEURL}geoserver\", ogc_server_settings.LOCATION.rstrip(\"/\"))",
            "",
            "    response, content = http_client.request(",
            "        _url, method=request.method, data=_data.encode(\"utf-8\"), headers=headers, timeout=timeout, user=user",
            "    )",
            "    if response is None:",
            "        return HttpResponse(content=content, reason=content, status=500)",
            "    content = response.content or response.reason",
            "    status = response.status_code",
            "    response_headers = response.headers",
            "    content_type = response.headers.get(\"Content-Type\")",
            "",
            "    if status >= 400:",
            "        _response = HttpResponse(content=content, reason=content, status=status, content_type=content_type)",
            "        return fetch_response_headers(_response, response_headers)",
            "",
            "    # decompress GZipped responses if not enabled",
            "    # if content and response and response.getheader('Content-Encoding') == 'gzip':",
            "    if content and content_type and content_type == \"gzip\":",
            "        buf = io.BytesIO(content)",
            "        with gzip.GzipFile(fileobj=buf) as f:",
            "            content = f.read()",
            "        buf.close()",
            "",
            "    PLAIN_CONTENT_TYPES = [\"text\", \"plain\", \"html\", \"json\", \"xml\", \"gml\"]",
            "    for _ct in PLAIN_CONTENT_TYPES:",
            "        if content_type and _ct in content_type and not isinstance(content, str):",
            "            try:",
            "                content = content.decode()",
            "                break",
            "            except Exception:",
            "                pass",
            "",
            "    if response and response_callback:",
            "        kwargs = {} if not kwargs else kwargs",
            "        kwargs.update(",
            "            {",
            "                \"response\": response,",
            "                \"content\": content,",
            "                \"status\": status,",
            "                \"response_headers\": response_headers,",
            "                \"content_type\": content_type,",
            "            }",
            "        )",
            "        return response_callback(**kwargs)",
            "    else:",
            "        # If we get a redirect, let's add a useful message.",
            "        if status and status in (301, 302, 303, 307):",
            "            _response = HttpResponse(",
            "                (",
            "                    f\"This proxy does not support redirects. The server in '{url}' \"",
            "                    f\"asked for a redirect to '{response.getheader('Location')}'\"",
            "                ),",
            "                status=status,",
            "                content_type=content_type,",
            "            )",
            "            _response[\"Location\"] = response.getheader(\"Location\")",
            "            return fetch_response_headers(_response, response_headers)",
            "        else:",
            "",
            "            def _get_message(text):",
            "                _s = text",
            "                if isinstance(text, bytes):",
            "                    _s = text.decode(\"utf-8\", \"replace\")",
            "                try:",
            "                    found = re.search(\"<b>Message</b>(.+?)</p>\", _s).group(1).strip()",
            "                except Exception:",
            "                    found = _s",
            "                return found",
            "",
            "            _response = HttpResponse(",
            "                content=content,",
            "                reason=_get_message(content) if status not in (200, 201) else None,",
            "                status=status,",
            "                content_type=content_type,",
            "            )",
            "            return fetch_response_headers(_response, response_headers)",
            "",
            "",
            "def download(request, resourceid, sender=Dataset):",
            "    _not_authorized = _(\"You are not authorized to download this resource.\")",
            "    _not_permitted = _(\"You are not permitted to save or edit this resource.\")",
            "    _no_files_found = _(\"No files have been found for this resource. Please, contact a system administrator.\")",
            "",
            "    instance = resolve_object(",
            "        request, sender, {\"pk\": resourceid}, permission=\"base.download_resourcebase\", permission_msg=_not_permitted",
            "    )",
            "",
            "    if isinstance(instance, ResourceBase):",
            "        dataset_files = []",
            "        file_list = []  # Store file info to be returned",
            "        try:",
            "            files = instance.resourcebase_ptr.files",
            "            # Copy all Dataset related files into a temporary folder",
            "            for file_path in files:",
            "                if storage_manager.exists(file_path):",
            "                    dataset_files.append(file_path)",
            "                    filename = os.path.basename(file_path)",
            "                    file_list.append(",
            "                        {",
            "                            \"name\": filename,",
            "                            \"data_iter\": storage_manager.open(file_path),",
            "                        }",
            "                    )",
            "                else:",
            "                    return HttpResponse(",
            "                        loader.render_to_string(",
            "                            \"401.html\",",
            "                            context={\"error_title\": _(\"No files found.\"), \"error_message\": _no_files_found},",
            "                            request=request,",
            "                        ),",
            "                        status=404,",
            "                    )",
            "",
            "            # Check we can access the original files",
            "            if not dataset_files:",
            "                return HttpResponse(",
            "                    loader.render_to_string(",
            "                        \"401.html\",",
            "                        context={\"error_title\": _(\"No files found.\"), \"error_message\": _no_files_found},",
            "                        request=request,",
            "                    ),",
            "                    status=404,",
            "                )",
            "",
            "            # ZIP everything and return",
            "            target_file_name = \"\".join([instance.name, \".zip\"])",
            "",
            "            target_zip = zipstream.ZipFile(mode=\"w\", compression=zipstream.ZIP_DEFLATED, allowZip64=True)",
            "",
            "            # Iterable: Needed when the file_info has it's data as a stream",
            "            def _iterable(source_iter):",
            "                while True:",
            "                    buf = source_iter.read(BUFFER_CHUNK_SIZE)",
            "                    if not buf:",
            "                        break",
            "                    yield buf",
            "",
            "            # Add files to zip",
            "            for file_info in file_list:",
            "                target_zip.write_iter(arcname=file_info[\"name\"], iterable=_iterable(file_info[\"data_iter\"]))",
            "",
            "            register_event(request, \"download\", instance)",
            "",
            "            # Streaming content response",
            "            response = StreamingHttpResponse(target_zip, content_type=\"application/zip\")",
            "            response[\"Content-Disposition\"] = f'attachment; filename=\"{target_file_name}\"'",
            "            return response",
            "        except (NotImplementedError, Upload.DoesNotExist):",
            "            traceback.print_exc()",
            "            tb = traceback.format_exc()",
            "            logger.debug(tb)",
            "            return HttpResponse(",
            "                loader.render_to_string(",
            "                    \"401.html\",",
            "                    context={\"error_title\": _(\"No files found.\"), \"error_message\": _no_files_found},",
            "                    request=request,",
            "                ),",
            "                status=404,",
            "            )",
            "    return HttpResponse(",
            "        loader.render_to_string(",
            "            \"401.html\", context={\"error_title\": _(\"Not Authorized\"), \"error_message\": _not_authorized}, request=request",
            "        ),",
            "        status=403,",
            "    )",
            "",
            "",
            "class OWSListView(View):",
            "    def get(self, request):",
            "        from geonode.geoserver import ows",
            "",
            "        out = {\"success\": True}",
            "        data = []",
            "        out[\"data\"] = data",
            "        # WMS",
            "        _raw_url = ows._wms_get_capabilities()",
            "        _url = urlsplit(_raw_url)",
            "        headers, access_token = get_headers(request, _url, _raw_url)",
            "        if access_token:",
            "            _j = \"&\" if _url.query else \"?\"",
            "            _raw_url = _j.join([_raw_url, f\"access_token={access_token}\"])",
            "        data.append({\"url\": _raw_url, \"type\": \"OGC:WMS\"})",
            "",
            "        # WCS",
            "        _raw_url = ows._wcs_get_capabilities()",
            "        _url = urlsplit(_raw_url)",
            "        headers, access_token = get_headers(request, _url, _raw_url)",
            "        if access_token:",
            "            _j = \"&\" if _url.query else \"?\"",
            "            _raw_url = _j.join([_raw_url, f\"access_token={access_token}\"])",
            "        data.append({\"url\": _raw_url, \"type\": \"OGC:WCS\"})",
            "",
            "        # WFS",
            "        _raw_url = ows._wfs_get_capabilities()",
            "        _url = urlsplit(_raw_url)",
            "        headers, access_token = get_headers(request, _url, _raw_url)",
            "        if access_token:",
            "            _j = \"&\" if _url.query else \"?\"",
            "            _raw_url = _j.join([_raw_url, f\"access_token={access_token}\"])",
            "        data.append({\"url\": _raw_url, \"type\": \"OGC:WFS\"})",
            "",
            "        # catalogue from configuration",
            "        for catname, catconf in settings.CATALOGUE.items():",
            "            # CSW",
            "            _raw_url = catconf[\"URL\"]",
            "            _url = urlsplit(_raw_url)",
            "            headers, access_token = get_headers(request, _url, _raw_url)",
            "            if access_token:",
            "                _j = \"&\" if _url.query else \"?\"",
            "                _raw_url = _j.join([_raw_url, f\"access_token={access_token}\"])",
            "            data.append({\"url\": _raw_url, \"type\": \"OGC:CSW\"})",
            "",
            "        # main site url",
            "        data.append({\"url\": settings.SITEURL, \"type\": \"WWW:LINK\"})",
            "        return json_response(out)",
            "",
            "",
            "_hoppish = {",
            "    \"connection\",",
            "    \"keep-alive\",",
            "    \"proxy-authenticate\",",
            "    \"proxy-authorization\",",
            "    \"te\",",
            "    \"trailers\",",
            "    \"transfer-encoding\",",
            "    \"upgrade\",",
            "    \"content-length\",",
            "    \"content-encoding\",",
            "}.__contains__",
            "",
            "",
            "def is_hop_by_hop(header_name):",
            "    \"\"\"Return true if 'header_name' is an HTTP/1.1 \"Hop-by-Hop\" header\"\"\"",
            "    return _hoppish(header_name.lower())",
            "",
            "",
            "def fetch_response_headers(response, response_headers):",
            "    if response_headers:",
            "        for _header in response_headers:",
            "            if not is_hop_by_hop(_header):",
            "                if hasattr(response, \"headers\") and _header.lower() not in [",
            "                    _k.lower() for _k in response.headers.keys()",
            "                ]:",
            "                    response.headers[_header] = response_headers.get(_header)",
            "                elif hasattr(response, \"_headers\") and _header.lower() not in [",
            "                    _k.lower() for _k in response._headers.keys()",
            "                ]:",
            "                    response._headers[_header] = (_header, response_headers.get(_header))",
            "    return response"
        ],
        "afterPatchFile": [
            "#########################################################################",
            "#",
            "# Copyright (C) 2016 OSGeo",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU General Public License as published by",
            "# the Free Software Foundation, either version 3 of the License, or",
            "# (at your option) any later version.",
            "#",
            "# This program is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the",
            "# GNU General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU General Public License",
            "# along with this program. If not, see <http://www.gnu.org/licenses/>.",
            "#",
            "#########################################################################",
            "import io",
            "import os",
            "import re",
            "import gzip",
            "import logging",
            "import traceback",
            "import zipstream",
            "",
            "from hyperlink import URL",
            "from urllib.parse import urlparse, urlsplit, urljoin",
            "",
            "from django.conf import settings",
            "from django.template import loader",
            "from django.http import HttpResponse, StreamingHttpResponse",
            "from django.views.generic import View",
            "from distutils.version import StrictVersion",
            "from django.http.request import validate_host",
            "from django.utils.translation import ugettext as _",
            "from django.views.decorators.csrf import requires_csrf_token",
            "",
            "from geonode.layers.models import Dataset",
            "from geonode.upload.models import Upload",
            "from geonode.base.models import ResourceBase",
            "from geonode.storage.manager import storage_manager",
            "from geonode.utils import (",
            "    resolve_object,",
            "    check_ogc_backend,",
            "    get_headers,",
            "    http_client,",
            "    json_response,",
            "    extract_ip_or_domain,",
            ")",
            "from geonode.base.enumerations import LINK_TYPES as _LT",
            "",
            "from geonode import geoserver  # noqa",
            "from geonode.base import register_event",
            "from geonode.base.auth import get_auth_user, get_token_from_auth_header",
            "",
            "BUFFER_CHUNK_SIZE = 64 * 1024",
            "",
            "TIMEOUT = 30",
            "",
            "LINK_TYPES = [L for L in _LT if L.startswith(\"OGC:\")]",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "ows_regexp = re.compile(r\"^(?i)(version)=(\\d\\.\\d\\.\\d)(?i)&(?i)request=(?i)(GetCapabilities)&(?i)service=(?i)(\\w\\w\\w)$\")",
            "",
            "",
            "@requires_csrf_token",
            "def proxy(",
            "    request,",
            "    url=None,",
            "    response_callback=None,",
            "    sec_chk_hosts=True,",
            "    sec_chk_rules=True,",
            "    timeout=None,",
            "    allowed_hosts=[],",
            "    headers=None,",
            "    access_token=None,",
            "    **kwargs,",
            "):",
            "    # Request default timeout",
            "    from geonode.geoserver.helpers import ogc_server_settings",
            "",
            "    if not timeout:",
            "        timeout = getattr(ogc_server_settings, \"TIMEOUT\", TIMEOUT)",
            "",
            "    # Security rules and settings",
            "    PROXY_ALLOWED_HOSTS = getattr(settings, \"PROXY_ALLOWED_HOSTS\", ())",
            "",
            "    # Sanity url checks",
            "    if \"url\" not in request.GET and not url:",
            "        return HttpResponse(",
            "            \"The proxy service requires a URL-encoded URL as a parameter.\", status=400, content_type=\"text/plain\"",
            "        )",
            "",
            "    raw_url = url or request.GET[\"url\"]",
            "    raw_url = urljoin(settings.SITEURL, raw_url) if raw_url.startswith(\"/\") else raw_url",
            "    url = urlsplit(raw_url)",
            "    scheme = str(url.scheme)",
            "    locator = str(url.path)",
            "    if url.query != \"\":",
            "        locator += f\"?{url.query}\"",
            "    if url.fragment != \"\":",
            "        locator += f\"#{url.fragment}\"",
            "",
            "    # White-Black Listing Hosts",
            "    site_url = urlsplit(settings.SITEURL)",
            "    if sec_chk_hosts and not settings.DEBUG:",
            "        # Attach current SITEURL",
            "        if site_url.hostname not in PROXY_ALLOWED_HOSTS:",
            "            PROXY_ALLOWED_HOSTS += (site_url.hostname,)",
            "",
            "        # Attach current hostname",
            "        hostname = (ogc_server_settings.hostname,) if ogc_server_settings else ()",
            "        if hostname not in PROXY_ALLOWED_HOSTS:",
            "            PROXY_ALLOWED_HOSTS += hostname",
            "",
            "        # Check OWS regexp",
            "        if url.query and ows_regexp.match(url.query):",
            "            ows_tokens = ows_regexp.match(url.query).groups()",
            "            if (",
            "                len(ows_tokens) == 4",
            "                and \"version\" == ows_tokens[0]",
            "                and StrictVersion(ows_tokens[1]) >= StrictVersion(\"1.0.0\")",
            "                and StrictVersion(ows_tokens[1]) <= StrictVersion(\"3.0.0\")",
            "                and ows_tokens[2].lower() in (\"getcapabilities\")",
            "                and ows_tokens[3].upper() in (\"OWS\", \"WCS\", \"WFS\", \"WMS\", \"WPS\", \"CSW\")",
            "            ):",
            "                if url.hostname not in PROXY_ALLOWED_HOSTS:",
            "                    PROXY_ALLOWED_HOSTS += (url.hostname,)",
            "",
            "        # Check Remote Services base_urls",
            "        from geonode.services.models import Service",
            "",
            "        for _s in Service.objects.all():",
            "            _remote_host = urlsplit(_s.base_url).hostname",
            "            PROXY_ALLOWED_HOSTS += (_remote_host,)",
            "",
            "        if not validate_host(extract_ip_or_domain(raw_url), PROXY_ALLOWED_HOSTS):",
            "            return HttpResponse(",
            "                \"DEBUG is set to False but the host of the path provided to the proxy service\"",
            "                \" is not in the PROXY_ALLOWED_HOSTS setting.\",",
            "                status=403,",
            "                content_type=\"text/plain\",",
            "            )",
            "",
            "    # Security checks based on rules; allow only specific requests",
            "    if sec_chk_rules:",
            "        # TODO: Not yet implemented",
            "        pass",
            "",
            "    # Collecting headers and cookies",
            "    if not headers:",
            "        headers, access_token = get_headers(request, url, raw_url, allowed_hosts=allowed_hosts)",
            "    if not access_token:",
            "        auth_header = None",
            "        if \"Authorization\" in headers:",
            "            auth_header = headers[\"Authorization\"]",
            "        elif \"HTTP_AUTHORIZATION\" in request.META:",
            "            auth_header = request.META.get(\"HTTP_AUTHORIZATION\", request.META.get(\"HTTP_AUTHORIZATION2\"))",
            "        if auth_header:",
            "            access_token = get_token_from_auth_header(auth_header, create_if_not_exists=True)",
            "    user = get_auth_user(access_token)",
            "",
            "    # Inject access_token if necessary",
            "    parsed = urlparse(raw_url)",
            "    parsed._replace(path=locator.encode(\"utf8\"))",
            "    if parsed.netloc == site_url.netloc and scheme != site_url.scheme:",
            "        parsed = parsed._replace(scheme=site_url.scheme)",
            "",
            "    _url = parsed.geturl()",
            "",
            "    # Some clients / JS libraries generate URLs with relative URL paths, e.g.",
            "    # \"http://host/path/path/../file.css\", which the requests library cannot",
            "    # currently handle (https://github.com/kennethreitz/requests/issues/2982).",
            "    # We parse and normalise such URLs into absolute paths before attempting",
            "    # to proxy the request.",
            "    _url = URL.from_text(_url).normalize().to_text()",
            "",
            "    if request.method == \"GET\" and access_token and \"access_token\" not in _url:",
            "        query_separator = \"&\" if \"?\" in _url else \"?\"",
            "        _url = f\"{_url}{query_separator}access_token={access_token}\"",
            "",
            "    _data = request.body.decode(\"utf-8\")",
            "",
            "    # Avoid translating local geoserver calls into external ones",
            "    if check_ogc_backend(geoserver.BACKEND_PACKAGE):",
            "        from geonode.geoserver.helpers import ogc_server_settings",
            "",
            "        _url = _url.replace(f\"{settings.SITEURL}geoserver\", ogc_server_settings.LOCATION.rstrip(\"/\"))",
            "        _data = _data.replace(f\"{settings.SITEURL}geoserver\", ogc_server_settings.LOCATION.rstrip(\"/\"))",
            "",
            "    response, content = http_client.request(",
            "        _url, method=request.method, data=_data.encode(\"utf-8\"), headers=headers, timeout=timeout, user=user",
            "    )",
            "    if response is None:",
            "        return HttpResponse(content=content, reason=content, status=500)",
            "    content = response.content or response.reason",
            "    status = response.status_code",
            "    response_headers = response.headers",
            "    content_type = response.headers.get(\"Content-Type\")",
            "",
            "    if status >= 400:",
            "        _response = HttpResponse(content=content, reason=content, status=status, content_type=content_type)",
            "        return fetch_response_headers(_response, response_headers)",
            "",
            "    # decompress GZipped responses if not enabled",
            "    # if content and response and response.getheader('Content-Encoding') == 'gzip':",
            "    if content and content_type and content_type == \"gzip\":",
            "        buf = io.BytesIO(content)",
            "        with gzip.GzipFile(fileobj=buf) as f:",
            "            content = f.read()",
            "        buf.close()",
            "",
            "    PLAIN_CONTENT_TYPES = [\"text\", \"plain\", \"html\", \"json\", \"xml\", \"gml\"]",
            "    for _ct in PLAIN_CONTENT_TYPES:",
            "        if content_type and _ct in content_type and not isinstance(content, str):",
            "            try:",
            "                content = content.decode()",
            "                break",
            "            except Exception:",
            "                pass",
            "",
            "    if response and response_callback:",
            "        kwargs = {} if not kwargs else kwargs",
            "        kwargs.update(",
            "            {",
            "                \"response\": response,",
            "                \"content\": content,",
            "                \"status\": status,",
            "                \"response_headers\": response_headers,",
            "                \"content_type\": content_type,",
            "            }",
            "        )",
            "        return response_callback(**kwargs)",
            "    else:",
            "        # If we get a redirect, let's add a useful message.",
            "        if status and status in (301, 302, 303, 307):",
            "            _response = HttpResponse(",
            "                (",
            "                    f\"This proxy does not support redirects. The server in '{url}' \"",
            "                    f\"asked for a redirect to '{response.getheader('Location')}'\"",
            "                ),",
            "                status=status,",
            "                content_type=content_type,",
            "            )",
            "            _response[\"Location\"] = response.getheader(\"Location\")",
            "            return fetch_response_headers(_response, response_headers)",
            "        else:",
            "",
            "            def _get_message(text):",
            "                _s = text",
            "                if isinstance(text, bytes):",
            "                    _s = text.decode(\"utf-8\", \"replace\")",
            "                try:",
            "                    found = re.search(\"<b>Message</b>(.+?)</p>\", _s).group(1).strip()",
            "                except Exception:",
            "                    found = _s",
            "                return found",
            "",
            "            _response = HttpResponse(",
            "                content=content,",
            "                reason=_get_message(content) if status not in (200, 201) else None,",
            "                status=status,",
            "                content_type=content_type,",
            "            )",
            "            return fetch_response_headers(_response, response_headers)",
            "",
            "",
            "def download(request, resourceid, sender=Dataset):",
            "    _not_authorized = _(\"You are not authorized to download this resource.\")",
            "    _not_permitted = _(\"You are not permitted to save or edit this resource.\")",
            "    _no_files_found = _(\"No files have been found for this resource. Please, contact a system administrator.\")",
            "",
            "    instance = resolve_object(",
            "        request, sender, {\"pk\": resourceid}, permission=\"base.download_resourcebase\", permission_msg=_not_permitted",
            "    )",
            "",
            "    if isinstance(instance, ResourceBase):",
            "        dataset_files = []",
            "        file_list = []  # Store file info to be returned",
            "        try:",
            "            files = instance.resourcebase_ptr.files",
            "            # Copy all Dataset related files into a temporary folder",
            "            for file_path in files:",
            "                if storage_manager.exists(file_path):",
            "                    dataset_files.append(file_path)",
            "                    filename = os.path.basename(file_path)",
            "                    file_list.append(",
            "                        {",
            "                            \"name\": filename,",
            "                            \"data_iter\": storage_manager.open(file_path),",
            "                        }",
            "                    )",
            "                else:",
            "                    return HttpResponse(",
            "                        loader.render_to_string(",
            "                            \"401.html\",",
            "                            context={\"error_title\": _(\"No files found.\"), \"error_message\": _no_files_found},",
            "                            request=request,",
            "                        ),",
            "                        status=404,",
            "                    )",
            "",
            "            # Check we can access the original files",
            "            if not dataset_files:",
            "                return HttpResponse(",
            "                    loader.render_to_string(",
            "                        \"401.html\",",
            "                        context={\"error_title\": _(\"No files found.\"), \"error_message\": _no_files_found},",
            "                        request=request,",
            "                    ),",
            "                    status=404,",
            "                )",
            "",
            "            # ZIP everything and return",
            "            target_file_name = \"\".join([instance.name, \".zip\"])",
            "",
            "            target_zip = zipstream.ZipFile(mode=\"w\", compression=zipstream.ZIP_DEFLATED, allowZip64=True)",
            "",
            "            # Iterable: Needed when the file_info has it's data as a stream",
            "            def _iterable(source_iter):",
            "                while True:",
            "                    buf = source_iter.read(BUFFER_CHUNK_SIZE)",
            "                    if not buf:",
            "                        break",
            "                    yield buf",
            "",
            "            # Add files to zip",
            "            for file_info in file_list:",
            "                target_zip.write_iter(arcname=file_info[\"name\"], iterable=_iterable(file_info[\"data_iter\"]))",
            "",
            "            register_event(request, \"download\", instance)",
            "",
            "            # Streaming content response",
            "            response = StreamingHttpResponse(target_zip, content_type=\"application/zip\")",
            "            response[\"Content-Disposition\"] = f'attachment; filename=\"{target_file_name}\"'",
            "            return response",
            "        except (NotImplementedError, Upload.DoesNotExist):",
            "            traceback.print_exc()",
            "            tb = traceback.format_exc()",
            "            logger.debug(tb)",
            "            return HttpResponse(",
            "                loader.render_to_string(",
            "                    \"401.html\",",
            "                    context={\"error_title\": _(\"No files found.\"), \"error_message\": _no_files_found},",
            "                    request=request,",
            "                ),",
            "                status=404,",
            "            )",
            "    return HttpResponse(",
            "        loader.render_to_string(",
            "            \"401.html\", context={\"error_title\": _(\"Not Authorized\"), \"error_message\": _not_authorized}, request=request",
            "        ),",
            "        status=403,",
            "    )",
            "",
            "",
            "class OWSListView(View):",
            "    def get(self, request):",
            "        from geonode.geoserver import ows",
            "",
            "        out = {\"success\": True}",
            "        data = []",
            "        out[\"data\"] = data",
            "        # WMS",
            "        _raw_url = ows._wms_get_capabilities()",
            "        _url = urlsplit(_raw_url)",
            "        headers, access_token = get_headers(request, _url, _raw_url)",
            "        if access_token:",
            "            _j = \"&\" if _url.query else \"?\"",
            "            _raw_url = _j.join([_raw_url, f\"access_token={access_token}\"])",
            "        data.append({\"url\": _raw_url, \"type\": \"OGC:WMS\"})",
            "",
            "        # WCS",
            "        _raw_url = ows._wcs_get_capabilities()",
            "        _url = urlsplit(_raw_url)",
            "        headers, access_token = get_headers(request, _url, _raw_url)",
            "        if access_token:",
            "            _j = \"&\" if _url.query else \"?\"",
            "            _raw_url = _j.join([_raw_url, f\"access_token={access_token}\"])",
            "        data.append({\"url\": _raw_url, \"type\": \"OGC:WCS\"})",
            "",
            "        # WFS",
            "        _raw_url = ows._wfs_get_capabilities()",
            "        _url = urlsplit(_raw_url)",
            "        headers, access_token = get_headers(request, _url, _raw_url)",
            "        if access_token:",
            "            _j = \"&\" if _url.query else \"?\"",
            "            _raw_url = _j.join([_raw_url, f\"access_token={access_token}\"])",
            "        data.append({\"url\": _raw_url, \"type\": \"OGC:WFS\"})",
            "",
            "        # catalogue from configuration",
            "        for catname, catconf in settings.CATALOGUE.items():",
            "            # CSW",
            "            _raw_url = catconf[\"URL\"]",
            "            _url = urlsplit(_raw_url)",
            "            headers, access_token = get_headers(request, _url, _raw_url)",
            "            if access_token:",
            "                _j = \"&\" if _url.query else \"?\"",
            "                _raw_url = _j.join([_raw_url, f\"access_token={access_token}\"])",
            "            data.append({\"url\": _raw_url, \"type\": \"OGC:CSW\"})",
            "",
            "        # main site url",
            "        data.append({\"url\": settings.SITEURL, \"type\": \"WWW:LINK\"})",
            "        return json_response(out)",
            "",
            "",
            "_hoppish = {",
            "    \"connection\",",
            "    \"keep-alive\",",
            "    \"proxy-authenticate\",",
            "    \"proxy-authorization\",",
            "    \"te\",",
            "    \"trailers\",",
            "    \"transfer-encoding\",",
            "    \"upgrade\",",
            "    \"content-length\",",
            "    \"content-encoding\",",
            "}.__contains__",
            "",
            "",
            "def is_hop_by_hop(header_name):",
            "    \"\"\"Return true if 'header_name' is an HTTP/1.1 \"Hop-by-Hop\" header\"\"\"",
            "    return _hoppish(header_name.lower())",
            "",
            "",
            "def fetch_response_headers(response, response_headers):",
            "    if response_headers:",
            "        for _header in response_headers:",
            "            if not is_hop_by_hop(_header):",
            "                if hasattr(response, \"headers\") and _header.lower() not in [",
            "                    _k.lower() for _k in response.headers.keys()",
            "                ]:",
            "                    response.headers[_header] = response_headers.get(_header)",
            "                elif hasattr(response, \"_headers\") and _header.lower() not in [",
            "                    _k.lower() for _k in response._headers.keys()",
            "                ]:",
            "                    response._headers[_header] = (_header, response_headers.get(_header))",
            "    return response"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "43": [],
            "133": [
                "proxy"
            ]
        },
        "addLocation": []
    },
    "geonode/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " import requests"
            },
            "1": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " import tempfile"
            },
            "2": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " import importlib"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+import ipaddress"
            },
            "4": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " import itertools"
            },
            "5": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " import traceback"
            },
            "6": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " import subprocess"
            },
            "7": {
                "beforePatchRowNumber": 1930,
                "afterPatchRowNumber": 1931,
                "PatchRowcode": "     return url"
            },
            "8": {
                "beforePatchRowNumber": 1931,
                "afterPatchRowNumber": 1932,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 1932,
                "afterPatchRowNumber": 1933,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1934,
                "PatchRowcode": "+def extract_ip_or_domain(url):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1935,
                "PatchRowcode": "+    ip_regex = re.compile(\"^(?:http\\:\\/\\/|https\\:\\/\\/)(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\")"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1936,
                "PatchRowcode": "+    domain_regex = re.compile(\"^(?:http\\:\\/\\/|https\\:\\/\\/)([a-zA-Z0-9.-]+)\")"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1937,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1938,
                "PatchRowcode": "+    match = ip_regex.findall(url)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1939,
                "PatchRowcode": "+    if len(match):"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1940,
                "PatchRowcode": "+        ip_address = match[0]"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1941,
                "PatchRowcode": "+        try:"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1942,
                "PatchRowcode": "+            ipaddress.ip_address(ip_address)  # Validate the IP address"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1943,
                "PatchRowcode": "+            return ip_address"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1944,
                "PatchRowcode": "+        except ValueError:"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1945,
                "PatchRowcode": "+            pass"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1946,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1947,
                "PatchRowcode": "+    match = domain_regex.findall(url)"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1948,
                "PatchRowcode": "+    if len(match):"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1949,
                "PatchRowcode": "+        return match[0]"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1950,
                "PatchRowcode": "+"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1951,
                "PatchRowcode": "+    return None"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1952,
                "PatchRowcode": "+"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1953,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": 1933,
                "afterPatchRowNumber": 1954,
                "PatchRowcode": " def get_xpath_value("
            },
            "31": {
                "beforePatchRowNumber": 1934,
                "afterPatchRowNumber": 1955,
                "PatchRowcode": "     element: etree.Element, xpath_expression: str, nsmap: typing.Optional[dict] = None"
            },
            "32": {
                "beforePatchRowNumber": 1935,
                "afterPatchRowNumber": 1956,
                "PatchRowcode": " ) -> typing.Optional[str]:"
            }
        },
        "frontPatchFile": [
            "#########################################################################",
            "#",
            "# Copyright (C) 2016 OSGeo",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU General Public License as published by",
            "# the Free Software Foundation, either version 3 of the License, or",
            "# (at your option) any later version.",
            "#",
            "# This program is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the",
            "# GNU General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU General Public License",
            "# along with this program. If not, see <http://www.gnu.org/licenses/>.",
            "#",
            "#########################################################################",
            "",
            "import os",
            "import gc",
            "import re",
            "import json",
            "import time",
            "import base64",
            "import ntpath",
            "import select",
            "import shutil",
            "import string",
            "import typing",
            "import logging",
            "import tarfile",
            "import datetime",
            "import requests",
            "import tempfile",
            "import importlib",
            "import itertools",
            "import traceback",
            "import subprocess",
            "",
            "from lxml import etree",
            "from osgeo import ogr",
            "from PIL import Image",
            "from urllib3 import Retry",
            "from io import BytesIO, StringIO",
            "from decimal import Decimal",
            "from threading import local",
            "from slugify import slugify",
            "from contextlib import closing",
            "from requests.exceptions import RetryError",
            "from collections import namedtuple, defaultdict",
            "from rest_framework.exceptions import APIException",
            "from math import atan, exp, log, pi, sin, tan, floor",
            "from zipfile import ZipFile, is_zipfile, ZIP_DEFLATED",
            "from pathvalidate import ValidationError, validate_filepath, validate_filename",
            "from geonode.upload.api.exceptions import GeneralUploadException",
            "",
            "from django.conf import settings",
            "from django.db.models import signals",
            "from django.utils.http import is_safe_url",
            "from django.apps import apps as django_apps",
            "from django.middleware.csrf import get_token",
            "from django.http import HttpResponse",
            "from django.forms.models import model_to_dict",
            "from django.contrib.auth import get_user_model",
            "from django.shortcuts import get_object_or_404",
            "from django.core.exceptions import PermissionDenied",
            "from django.core.exceptions import ImproperlyConfigured",
            "from django.core.serializers.json import DjangoJSONEncoder",
            "from django.db import models, connection, transaction",
            "from django.utils.translation import ugettext_lazy as _",
            "",
            "from geonode import geoserver, GeoNodeException  # noqa",
            "from geonode.compat import ensure_string",
            "from geonode.layers.enumerations import GXP_PTYPES",
            "from geonode.storage.manager import storage_manager",
            "from geonode.services.serviceprocessors import get_available_service_types",
            "from geonode.base.auth import (",
            "    extend_token,",
            "    get_or_create_token,",
            "    get_token_from_auth_header,",
            "    get_token_object_from_session,",
            ")",
            "",
            "from urllib.parse import (",
            "    urljoin,",
            "    unquote,",
            "    urlparse,",
            "    urlsplit,",
            "    urlencode,",
            "    parse_qsl,",
            "    ParseResult,",
            ")",
            "",
            "MAX_EXTENT = 20037508.34",
            "FULL_ROTATION_DEG = 360.0",
            "HALF_ROTATION_DEG = 180.0",
            "DEFAULT_TITLE = \"\"",
            "DEFAULT_ABSTRACT = \"\"",
            "",
            "INVALID_PERMISSION_MESSAGE = _(\"Invalid permission level.\")",
            "",
            "ALPHABET = f\"{string.ascii_uppercase + string.ascii_lowercase + string.digits}-_\"",
            "ALPHABET_REVERSE = {c: i for (i, c) in enumerate(ALPHABET)}",
            "BASE = len(ALPHABET)",
            "SIGN_CHARACTER = \"$\"",
            "SQL_PARAMS_RE = re.compile(r\"%\\(([\\w_\\-]+)\\)s\")",
            "",
            "FORWARDED_HEADERS = [\"content-type\", \"content-disposition\"]",
            "",
            "# explicitly disable resolving XML entities in order to prevent malicious attacks",
            "XML_PARSER: typing.Final = etree.XMLParser(resolve_entities=False)",
            "",
            "requests.packages.urllib3.disable_warnings()",
            "",
            "signalnames = [",
            "    \"class_prepared\",",
            "    \"m2m_changed\",",
            "    \"post_delete\",",
            "    \"post_init\",",
            "    \"post_save\",",
            "    \"post_syncdb\",",
            "    \"pre_delete\",",
            "    \"pre_init\",",
            "    \"pre_save\",",
            "]",
            "signals_store = {}",
            "",
            "id_none = id(None)",
            "",
            "logger = logging.getLogger(\"geonode.utils\")",
            "",
            "",
            "class ServerDoesNotExist(Exception):",
            "    pass",
            "",
            "",
            "class OGC_Server(object):  # LGTM: @property will not work in old-style classes",
            "",
            "    \"\"\"",
            "    OGC Server object.",
            "    \"\"\"",
            "",
            "    def __init__(self, ogc_server, alias):",
            "        self.alias = alias",
            "        self.server = ogc_server",
            "",
            "    def __getattr__(self, item):",
            "        return self.server.get(item)",
            "",
            "    @property",
            "    def credentials(self):",
            "        \"\"\"",
            "        Returns a tuple of the server's credentials.",
            "        \"\"\"",
            "        creds = namedtuple(\"OGC_SERVER_CREDENTIALS\", [\"username\", \"password\"])",
            "        return creds(username=self.USER, password=self.PASSWORD)",
            "",
            "    @property",
            "    def datastore_db(self):",
            "        \"\"\"",
            "        Returns the server's datastore dict or None.",
            "        \"\"\"",
            "        if self.DATASTORE and settings.DATABASES.get(self.DATASTORE, None):",
            "            datastore_dict = settings.DATABASES.get(self.DATASTORE, dict())",
            "            return datastore_dict",
            "        else:",
            "            return dict()",
            "",
            "    @property",
            "    def ows(self):",
            "        \"\"\"",
            "        The Open Web Service url for the server.",
            "        \"\"\"",
            "        location = self.PUBLIC_LOCATION if self.PUBLIC_LOCATION else self.LOCATION",
            "        return self.OWS_LOCATION if self.OWS_LOCATION else urljoin(location, \"ows\")",
            "",
            "    @property",
            "    def rest(self):",
            "        \"\"\"",
            "        The REST endpoint for the server.",
            "        \"\"\"",
            "        return urljoin(self.LOCATION, \"rest\") if not self.REST_LOCATION else self.REST_LOCATION",
            "",
            "    @property",
            "    def public_url(self):",
            "        \"\"\"",
            "        The global public endpoint for the server.",
            "        \"\"\"",
            "        return self.LOCATION if not self.PUBLIC_LOCATION else self.PUBLIC_LOCATION",
            "",
            "    @property",
            "    def internal_ows(self):",
            "        \"\"\"",
            "        The Open Web Service url for the server used by GeoNode internally.",
            "        \"\"\"",
            "        location = self.LOCATION",
            "        return urljoin(location, \"ows\")",
            "",
            "    @property",
            "    def hostname(self):",
            "        return urlsplit(self.LOCATION).hostname",
            "",
            "    @property",
            "    def netloc(self):",
            "        return urlsplit(self.LOCATION).netloc",
            "",
            "    def __str__(self):",
            "        return str(self.alias)",
            "",
            "",
            "class OGC_Servers_Handler:",
            "",
            "    \"\"\"",
            "    OGC Server Settings Convenience dict.",
            "    \"\"\"",
            "",
            "    def __init__(self, ogc_server_dict):",
            "        self.servers = ogc_server_dict",
            "        # FIXME(Ariel): Are there better ways to do this without involving",
            "        # local?",
            "        self._servers = local()",
            "",
            "    def ensure_valid_configuration(self, alias):",
            "        \"\"\"",
            "        Ensures the settings are valid.",
            "        \"\"\"",
            "        try:",
            "            server = self.servers[alias]",
            "        except KeyError:",
            "            raise ServerDoesNotExist(f\"The server {alias} doesn't exist\")",
            "",
            "        if \"PRINTNG_ENABLED\" in server:",
            "            raise ImproperlyConfigured(\"The PRINTNG_ENABLED setting has been removed, use 'PRINT_NG_ENABLED' instead.\")",
            "",
            "    def ensure_defaults(self, alias):",
            "        \"\"\"",
            "        Puts the defaults into the settings dictionary for a given connection where no settings is provided.",
            "        \"\"\"",
            "        try:",
            "            server = self.servers[alias]",
            "        except KeyError:",
            "            raise ServerDoesNotExist(f\"The server {alias} doesn't exist\")",
            "",
            "        server.setdefault(\"BACKEND\", \"geonode.geoserver\")",
            "        server.setdefault(\"LOCATION\", \"http://localhost:8080/geoserver/\")",
            "        server.setdefault(\"USER\", \"admin\")",
            "        server.setdefault(\"PASSWORD\", \"geoserver\")",
            "        server.setdefault(\"DATASTORE\", \"\")",
            "",
            "        for option in [",
            "            \"MAPFISH_PRINT_ENABLED\",",
            "            \"PRINT_NG_ENABLED\",",
            "            \"GEONODE_SECURITY_ENABLED\",",
            "            \"GEOFENCE_SECURITY_ENABLED\",",
            "            \"BACKEND_WRITE_ENABLED\",",
            "        ]:",
            "            server.setdefault(option, True)",
            "",
            "        for option in [\"WMST_ENABLED\", \"WPS_ENABLED\"]:",
            "            server.setdefault(option, False)",
            "",
            "        for option in [\"TIMEOUT\", \"GEOFENCE_TIMEOUT\"]:",
            "            server.setdefault(option, 60)",
            "",
            "    def __getitem__(self, alias):",
            "        if hasattr(self._servers, alias):",
            "            return getattr(self._servers, alias)",
            "",
            "        self.ensure_defaults(alias)",
            "        self.ensure_valid_configuration(alias)",
            "        server = self.servers[alias]",
            "        server = OGC_Server(alias=alias, ogc_server=server)",
            "        setattr(self._servers, alias, server)",
            "        return server",
            "",
            "    def __setitem__(self, key, value):",
            "        setattr(self._servers, key, value)",
            "",
            "    def __iter__(self):",
            "        return iter(self.servers)",
            "",
            "    def all(self):",
            "        return [self[alias] for alias in self]",
            "",
            "",
            "def mkdtemp(dir=settings.MEDIA_ROOT):",
            "    if not os.path.exists(dir):",
            "        os.makedirs(dir, exist_ok=True)",
            "    tempdir = None",
            "    while not tempdir:",
            "        try:",
            "            tempdir = tempfile.mkdtemp(dir=dir)",
            "            if os.path.exists(tempdir) and os.path.isdir(tempdir):",
            "                if os.listdir(tempdir):",
            "                    raise Exception(\"Directory is not empty\")",
            "            else:",
            "                raise Exception(\"Directory does not exist or is not accessible\")",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            tempdir = None",
            "    return tempdir",
            "",
            "",
            "def unzip_file(upload_file, extension=\".shp\", tempdir=None):",
            "    \"\"\"",
            "    Unzips a zipfile into a temporary directory and returns the full path of the .shp file inside (if any)",
            "    \"\"\"",
            "    absolute_base_file = None",
            "    if tempdir is None:",
            "        tempdir = mkdtemp()",
            "",
            "    the_zip = ZipFile(upload_file, allowZip64=True)",
            "    the_zip.extractall(tempdir)",
            "    for item in the_zip.namelist():",
            "        if item.endswith(extension):",
            "            absolute_base_file = os.path.join(tempdir, item)",
            "",
            "    return absolute_base_file",
            "",
            "",
            "def extract_tarfile(upload_file, extension=\".shp\", tempdir=None):",
            "    \"\"\"",
            "    Extracts a tarfile into a temporary directory and returns the full path of the .shp file inside (if any)",
            "    \"\"\"",
            "    absolute_base_file = None",
            "    if tempdir is None:",
            "        tempdir = mkdtemp()",
            "",
            "    the_tar = tarfile.open(upload_file)",
            "    the_tar.extractall(tempdir)",
            "    for item in the_tar.getnames():",
            "        if item.endswith(extension):",
            "            absolute_base_file = os.path.join(tempdir, item)",
            "",
            "    return absolute_base_file",
            "",
            "",
            "def get_dataset_name(dataset):",
            "    \"\"\"Get the workspace where the input layer belongs\"\"\"",
            "    _name = dataset.name",
            "    if _name and \":\" in _name:",
            "        _name = _name.split(\":\")[1]",
            "    try:",
            "        if not _name and dataset.alternate:",
            "            if \":\" in dataset.alternate:",
            "                _name = dataset.alternate.split(\":\")[1]",
            "            else:",
            "                _name = dataset.alternate",
            "    except Exception:",
            "        pass",
            "    return _name",
            "",
            "",
            "def get_dataset_workspace(dataset):",
            "    \"\"\"Get the workspace where the input layer belongs\"\"\"",
            "    try:",
            "        alternate = dataset.alternate",
            "    except Exception:",
            "        alternate = dataset.name",
            "    try:",
            "        workspace = dataset.workspace",
            "    except Exception:",
            "        workspace = None",
            "    if not workspace and alternate and \":\" in alternate:",
            "        workspace = alternate.split(\":\")[0]",
            "    if not workspace:",
            "        default_workspace = getattr(settings, \"DEFAULT_WORKSPACE\", \"geonode\")",
            "        try:",
            "            from geonode.services.enumerations import CASCADED",
            "",
            "            if dataset.remote_service.method == CASCADED:",
            "                workspace = getattr(settings, \"CASCADE_WORKSPACE\", default_workspace)",
            "            else:",
            "                raise RuntimeError(\"Dataset is not cascaded\")",
            "        except Exception:  # layer does not have a service",
            "            workspace = default_workspace",
            "    return workspace",
            "",
            "",
            "def get_headers(request, url, raw_url, allowed_hosts=[]):",
            "    cookies = None",
            "    csrftoken = None",
            "    headers = {}",
            "",
            "    for _header_key, _header_value in dict(request.headers.copy()).items():",
            "        if _header_key.lower() in FORWARDED_HEADERS:",
            "            headers[_header_key] = _header_value",
            "    if settings.SESSION_COOKIE_NAME in request.COOKIES and is_safe_url(url=raw_url, allowed_hosts=url.hostname):",
            "        cookies = request.META[\"HTTP_COOKIE\"]",
            "",
            "    for cook in request.COOKIES:",
            "        name = str(cook)",
            "        value = request.COOKIES.get(name)",
            "        if name == \"csrftoken\":",
            "            csrftoken = value",
            "        cook = f\"{name}={value}\"",
            "        cookies = cook if not cookies else (f\"{cookies}; {cook}\")",
            "",
            "    csrftoken = get_token(request) if not csrftoken else csrftoken",
            "",
            "    if csrftoken:",
            "        headers[\"X-Requested-With\"] = \"XMLHttpRequest\"",
            "        headers[\"X-CSRFToken\"] = csrftoken",
            "        cook = f\"csrftoken={csrftoken}\"",
            "        cookies = cook if not cookies else (f\"{cookies}; {cook}\")",
            "",
            "    if cookies and request and hasattr(request, \"session\"):",
            "        if \"JSESSIONID\" in request.session and request.session[\"JSESSIONID\"]:",
            "            cookies = f\"{cookies}; JSESSIONID={request.session['JSESSIONID']}\"",
            "        headers[\"Cookie\"] = cookies",
            "",
            "    if request.method in (\"POST\", \"PUT\") and \"CONTENT_TYPE\" in request.META:",
            "        headers[\"Content-Type\"] = request.META[\"CONTENT_TYPE\"]",
            "",
            "    access_token = None",
            "    site_url = urlsplit(settings.SITEURL)",
            "    # We want to convert HTTP_AUTH into a Beraer Token only when hitting the local GeoServer",
            "    if site_url.hostname in (allowed_hosts + [url.hostname]):",
            "        # we give precedence to obtained from Aithorization headers",
            "        if \"HTTP_AUTHORIZATION\" in request.META:",
            "            auth_header = request.META.get(\"HTTP_AUTHORIZATION\", request.META.get(\"HTTP_AUTHORIZATION2\"))",
            "            if auth_header:",
            "                headers[\"Authorization\"] = auth_header",
            "                access_token = get_token_from_auth_header(auth_header, create_if_not_exists=True)",
            "        # otherwise we check if a session is active",
            "        elif request and request.user.is_authenticated:",
            "            access_token = get_token_object_from_session(request.session)",
            "",
            "            # we extend the token in case the session is active but the token expired",
            "            if access_token and access_token.is_expired():",
            "                extend_token(access_token)",
            "            else:",
            "                access_token = get_or_create_token(request.user)",
            "",
            "    if access_token:",
            "        headers[\"Authorization\"] = f\"Bearer {access_token}\"",
            "",
            "    pragma = \"no-cache\"",
            "    referer = (",
            "        request.META[\"HTTP_REFERER\"] if \"HTTP_REFERER\" in request.META else f\"{site_url.scheme}://{site_url.netloc}/\"",
            "    )",
            "    encoding = request.META[\"HTTP_ACCEPT_ENCODING\"] if \"HTTP_ACCEPT_ENCODING\" in request.META else \"gzip\"",
            "    headers.update(",
            "        {",
            "            \"Pragma\": pragma,",
            "            \"Referer\": referer,",
            "            \"Accept-encoding\": encoding,",
            "        }",
            "    )",
            "",
            "    return (headers, access_token)",
            "",
            "",
            "def _get_basic_auth_info(request):",
            "    \"\"\"",
            "    grab basic auth info",
            "    \"\"\"",
            "    meth, auth = request.META[\"HTTP_AUTHORIZATION\"].split()",
            "    if meth.lower() != \"basic\":",
            "        raise ValueError",
            "    username, password = base64.b64decode(auth.encode()).decode().split(\":\")",
            "    return username, password",
            "",
            "",
            "def batch_delete(request):",
            "    # TODO",
            "    pass",
            "",
            "",
            "def _split_query(query):",
            "    \"\"\"",
            "    split and strip keywords, preserve space",
            "    separated quoted blocks.",
            "    \"\"\"",
            "",
            "    qq = query.split(\" \")",
            "    keywords = []",
            "    accum = None",
            "    for kw in qq:",
            "        if accum is None:",
            "            if kw.startswith('\"'):",
            "                accum = kw[1:]",
            "            elif kw:",
            "                keywords.append(kw)",
            "        else:",
            "            accum += f\" {kw}\"",
            "            if kw.endswith('\"'):",
            "                keywords.append(accum[0:-1])",
            "                accum = None",
            "    if accum is not None:",
            "        keywords.append(accum)",
            "    return [kw.strip() for kw in keywords if kw.strip()]",
            "",
            "",
            "def bbox_to_wkt(x0, x1, y0, y1, srid=\"4326\", include_srid=True):",
            "    if srid and str(srid).startswith(\"EPSG:\"):",
            "        srid = srid[5:]",
            "    if None not in {x0, x1, y0, y1}:",
            "        polys = []",
            "",
            "        # We assume that if x1 is smaller then x0 we're crossing the date line",
            "        crossing_idl = x1 < x0",
            "        if crossing_idl:",
            "            polys.append(",
            "                [",
            "                    (float(x0), float(y0)),",
            "                    (float(x0), float(y1)),",
            "                    (180.0, float(y1)),",
            "                    (180.0, float(y0)),",
            "                    (float(x0), float(y0)),",
            "                ]",
            "            )",
            "            polys.append(",
            "                [",
            "                    (-180.0, float(y0)),",
            "                    (-180.0, float(y1)),",
            "                    (float(x1), float(y1)),",
            "                    (float(x1), float(y0)),",
            "                    (-180.0, float(y0)),",
            "                ]",
            "            )",
            "        else:",
            "            polys.append(",
            "                [",
            "                    (float(x0), float(y0)),",
            "                    (float(x0), float(y1)),",
            "                    (float(x1), float(y1)),",
            "                    (float(x1), float(y0)),",
            "                    (float(x0), float(y0)),",
            "                ]",
            "            )",
            "",
            "        poly_wkts = \",\".join(",
            "            [\"(({}))\".format(\",\".join([\"{:f} {:f}\".format(coords[0], coords[1]) for coords in poly])) for poly in polys]",
            "        )",
            "        wkt = f\"MULTIPOLYGON({poly_wkts})\" if len(polys) > 1 else f\"POLYGON{poly_wkts}\"",
            "        if include_srid:",
            "            wkt = f\"SRID={srid};{wkt}\"",
            "    else:",
            "        wkt = \"POLYGON((-180 -90,-180 90,180 90,180 -90,-180 -90))\"",
            "        if include_srid:",
            "            wkt = f\"SRID=4326;{wkt}\"",
            "    return wkt",
            "",
            "",
            "def _v(coord, x, source_srid=4326, target_srid=3857):",
            "    if source_srid == 4326 and x and abs(coord) != HALF_ROTATION_DEG:",
            "        coord -= round(coord / FULL_ROTATION_DEG) * FULL_ROTATION_DEG",
            "    if source_srid == 4326 and target_srid != 4326:",
            "        if x and float(coord) >= 179.999:",
            "            return 179.999",
            "        elif x and float(coord) <= -179.999:",
            "            return -179.999",
            "",
            "        if not x and float(coord) >= 89.999:",
            "            return 89.999",
            "        elif not x and float(coord) <= -89.999:",
            "            return -89.999",
            "    return coord",
            "",
            "",
            "def bbox_to_projection(native_bbox, target_srid=4326):",
            "    \"\"\"",
            "    native_bbox must be in the form",
            "        ('-81.3962935', '-81.3490249', '13.3202891', '13.3859614', 'EPSG:4326')",
            "    \"\"\"",
            "    box = native_bbox[:4]",
            "    proj = native_bbox[-1]",
            "    minx, maxx, miny, maxy = [float(a) for a in box]",
            "    try:",
            "        source_srid = int(proj.split(\":\")[1]) if proj and \":\" in proj else int(proj)",
            "    except Exception:",
            "        source_srid = target_srid",
            "",
            "    if source_srid != target_srid:",
            "        wkt = bbox_to_wkt(",
            "            _v(minx, x=True, source_srid=source_srid, target_srid=target_srid),",
            "            _v(maxx, x=True, source_srid=source_srid, target_srid=target_srid),",
            "            _v(miny, x=False, source_srid=source_srid, target_srid=target_srid),",
            "            _v(maxy, x=False, source_srid=source_srid, target_srid=target_srid),",
            "            srid=source_srid,",
            "            include_srid=False,",
            "        )",
            "        # AF: This causses error with GDAL 3.0.4 due to a breaking change on GDAL",
            "        #     https://code.djangoproject.com/ticket/30645",
            "        import osgeo.gdal",
            "",
            "        _gdal_ver = osgeo.gdal.__version__.split(\".\", 2)",
            "        from osgeo import ogr",
            "        from osgeo.osr import SpatialReference, CoordinateTransformation",
            "",
            "        g = ogr.Geometry(wkt=wkt)",
            "        source = SpatialReference()",
            "        source.ImportFromEPSG(source_srid)",
            "        dest = SpatialReference()",
            "        dest.ImportFromEPSG(target_srid)",
            "        if int(_gdal_ver[0]) >= 3 and ((int(_gdal_ver[1]) == 0 and int(_gdal_ver[2]) >= 4) or int(_gdal_ver[1]) > 0):",
            "            source.SetAxisMappingStrategy(0)",
            "            dest.SetAxisMappingStrategy(0)",
            "        g.Transform(CoordinateTransformation(source, dest))",
            "        projected_bbox = [str(x) for x in g.GetEnvelope()]",
            "        # Must be in the form : [x0, x1, y0, y1, EPSG:<target_srid>)",
            "        return tuple(",
            "            [float(projected_bbox[0]), float(projected_bbox[1]), float(projected_bbox[2]), float(projected_bbox[3])]",
            "        ) + (f\"EPSG:{target_srid}\",)",
            "",
            "    return native_bbox",
            "",
            "",
            "def bounds_to_zoom_level(bounds, width, height):",
            "    WORLD_DIM = {\"height\": 256.0, \"width\": 256.0}",
            "    ZOOM_MAX = 21",
            "",
            "    def latRad(lat):",
            "        _sin = sin(lat * pi / HALF_ROTATION_DEG)",
            "        if abs(_sin) != 1.0:",
            "            radX2 = log((1.0 + _sin) / (1.0 - _sin)) / 2.0",
            "        else:",
            "            radX2 = log(1.0) / 2.0",
            "        return max(min(radX2, pi), -pi) / 2.0",
            "",
            "    def zoom(mapPx, worldPx, fraction):",
            "        try:",
            "            return floor(log(mapPx / worldPx / fraction) / log(2.0))",
            "        except Exception:",
            "            return 0",
            "",
            "    ne = [float(bounds[2]), float(bounds[3])]",
            "    sw = [float(bounds[0]), float(bounds[1])]",
            "    latFraction = (latRad(ne[1]) - latRad(sw[1])) / pi",
            "    lngDiff = ne[0] - sw[0]",
            "    lngFraction = ((lngDiff + FULL_ROTATION_DEG) if lngDiff < 0 else lngDiff) / FULL_ROTATION_DEG",
            "    latZoom = zoom(float(height), WORLD_DIM[\"height\"], latFraction)",
            "    lngZoom = zoom(float(width), WORLD_DIM[\"width\"], lngFraction)",
            "    # ratio = float(max(width, height)) / float(min(width, height))",
            "    # z_offset = 0 if ratio >= 2 else -1",
            "    z_offset = 0",
            "    zoom = int(max(latZoom, lngZoom) + z_offset)",
            "    zoom = 0 if zoom > ZOOM_MAX else zoom",
            "    return max(zoom, 0)",
            "",
            "",
            "def llbbox_to_mercator(llbbox):",
            "    minlonlat = forward_mercator([llbbox[0], llbbox[2]])",
            "    maxlonlat = forward_mercator([llbbox[1], llbbox[3]])",
            "    return [minlonlat[0], minlonlat[1], maxlonlat[0], maxlonlat[1]]",
            "",
            "",
            "def mercator_to_llbbox(bbox):",
            "    minlonlat = inverse_mercator([bbox[0], bbox[2]])",
            "    maxlonlat = inverse_mercator([bbox[1], bbox[3]])",
            "    return [minlonlat[0], minlonlat[1], maxlonlat[0], maxlonlat[1]]",
            "",
            "",
            "def forward_mercator(lonlat):",
            "    \"\"\"",
            "    Given geographic coordinates, return a x,y tuple in spherical mercator.",
            "",
            "    If the lat value is out of range, -inf will be returned as the y value",
            "    \"\"\"",
            "    x = lonlat[0] * MAX_EXTENT / HALF_ROTATION_DEG",
            "    try:",
            "        # With data sets that only have one point the value of this",
            "        # expression becomes negative infinity. In order to continue,",
            "        # we wrap this in a try catch block.",
            "        n = tan((90 + lonlat[1]) * pi / FULL_ROTATION_DEG)",
            "    except ValueError:",
            "        n = 0",
            "    if n <= 0:",
            "        y = float(\"-inf\")",
            "    else:",
            "        y = log(n) / pi * MAX_EXTENT",
            "    return (x, y)",
            "",
            "",
            "def inverse_mercator(xy):",
            "    \"\"\"",
            "    Given coordinates in spherical mercator, return a lon,lat tuple.",
            "    \"\"\"",
            "    lon = (xy[0] / MAX_EXTENT) * HALF_ROTATION_DEG",
            "    lat = (xy[1] / MAX_EXTENT) * HALF_ROTATION_DEG",
            "    lat = HALF_ROTATION_DEG / pi * (2 * atan(exp(lat * pi / HALF_ROTATION_DEG)) - pi / 2)",
            "    return (lon, lat)",
            "",
            "",
            "def resolve_object(",
            "    request, model, query, permission=\"base.view_resourcebase\", user=None, permission_required=True, permission_msg=None",
            "):",
            "    \"\"\"Resolve an object using the provided query and check the optional",
            "    permission. Model views should wrap this function as a shortcut.",
            "",
            "    query - a dict to use for querying the model",
            "    permission - an optional permission to check",
            "    permission_required - if False, allow get methods to proceed",
            "    permission_msg - optional message to use in 403",
            "    \"\"\"",
            "    user = request.user if request and request.user else user",
            "    obj = get_object_or_404(model, **query)",
            "    obj_to_check = obj.get_self_resource()",
            "",
            "    from guardian.shortcuts import get_groups_with_perms",
            "    from geonode.groups.models import GroupProfile",
            "",
            "    groups = get_groups_with_perms(obj_to_check, attach_perms=True)",
            "",
            "    if obj_to_check.group and obj_to_check.group not in groups:",
            "        groups[obj_to_check.group] = obj_to_check.group",
            "",
            "    obj_group_managers = []",
            "    obj_group_members = []",
            "    if groups:",
            "        for group in groups:",
            "            try:",
            "                group_profile = GroupProfile.objects.get(slug=group.name)",
            "                managers = group_profile.get_managers()",
            "                if managers:",
            "                    for manager in managers:",
            "                        if manager not in obj_group_managers and not manager.is_superuser:",
            "                            obj_group_managers.append(manager)",
            "                if group_profile.user_is_member(user) and user not in obj_group_members:",
            "                    obj_group_members.append(user)",
            "            except GroupProfile.DoesNotExist:",
            "                pass",
            "",
            "    allowed = True",
            "    if permission.split(\".\")[-1] in [\"change_dataset_data\", \"change_dataset_style\"]:",
            "        if obj.__class__.__name__ == \"Dataset\":",
            "            obj_to_check = obj",
            "    if permission:",
            "        if permission_required or request.method != \"GET\":",
            "            if user in obj_group_managers:",
            "                allowed = True",
            "            else:",
            "                allowed = user.has_perm(permission, obj_to_check)",
            "    if not allowed:",
            "        mesg = permission_msg or _(\"Permission Denied\")",
            "        raise PermissionDenied(mesg)",
            "    return obj",
            "",
            "",
            "def json_response(body=None, errors=None, url=None, redirect_to=None, exception=None, content_type=None, status=None):",
            "    \"\"\"Create a proper JSON response. If body is provided, this is the response.",
            "    If errors is not None, the response is a success/errors json object.",
            "    If redirect_to is not None, the response is a success=True, redirect_to object",
            "    If the exception is provided, it will be logged. If body is a string, the",
            "    exception message will be used as a format option to that string and the",
            "    result will be a success=False, errors = body % exception",
            "    \"\"\"",
            "    if isinstance(body, HttpResponse):",
            "        return body",
            "    if content_type is None:",
            "        content_type = \"application/json\"",
            "    if errors:",
            "        if isinstance(errors, str):",
            "            errors = [errors]",
            "        body = {\"success\": False, \"errors\": errors}",
            "    elif redirect_to:",
            "        body = {\"success\": True, \"redirect_to\": redirect_to}",
            "    elif url:",
            "        body = {\"success\": True, \"url\": url}",
            "    elif exception:",
            "        if isinstance(exception, APIException):",
            "            raise exception",
            "        if body is None:",
            "            body = f\"Unexpected exception {exception}\"",
            "        else:",
            "            body = body % exception",
            "        body = {\"success\": False, \"errors\": [body]}",
            "        raise GeneralUploadException(detail=body)",
            "    elif body:",
            "        pass",
            "    else:",
            "        raise Exception(\"must call with body, errors or redirect_to\")",
            "",
            "    if status is None:",
            "        status = 200",
            "",
            "    if not isinstance(body, str):",
            "        try:",
            "            body = json.dumps(body, cls=DjangoJSONEncoder)",
            "        except Exception:",
            "            body = str(body)",
            "    return HttpResponse(body, content_type=content_type, status=status)",
            "",
            "",
            "def num_encode(n):",
            "    if n < 0:",
            "        return SIGN_CHARACTER + num_encode(-n)",
            "    s = []",
            "    while True:",
            "        n, r = divmod(n, BASE)",
            "        s.append(ALPHABET[r])",
            "        if n == 0:",
            "            break",
            "    return \"\".join(reversed(s))",
            "",
            "",
            "def num_decode(s):",
            "    if s[0] == SIGN_CHARACTER:",
            "        return -num_decode(s[1:])",
            "    n = 0",
            "    for c in s:",
            "        n = n * BASE + ALPHABET_REVERSE[c]",
            "    return n",
            "",
            "",
            "def format_urls(a, values):",
            "    b = []",
            "    for i in a:",
            "        j = i.copy()",
            "        try:",
            "            j[\"url\"] = str(j[\"url\"]).format(**values)",
            "        except KeyError:",
            "            j[\"url\"] = None",
            "        b.append(j)",
            "    return b",
            "",
            "",
            "def build_abstract(resourcebase, url=None, includeURL=True):",
            "    if resourcebase.abstract and url and includeURL:",
            "        return f\"{resourcebase.abstract} -- [{url}]({url})\"",
            "    else:",
            "        return resourcebase.abstract",
            "",
            "",
            "def build_caveats(resourcebase):",
            "    caveats = []",
            "    if resourcebase.maintenance_frequency:",
            "        caveats.append(resourcebase.maintenance_frequency_title())",
            "    if resourcebase.license:",
            "        caveats.append(resourcebase.license_verbose)",
            "    if resourcebase.data_quality_statement:",
            "        caveats.append(resourcebase.data_quality_statement)",
            "    if len(caveats) > 0:",
            "        return f\"- {'%0A- '.join(caveats)}\"",
            "    else:",
            "        return \"\"",
            "",
            "",
            "def build_social_links(request, resourcebase):",
            "    netschema = \"https\" if request.is_secure() else \"http\"",
            "    host = request.get_host()",
            "    path = request.get_full_path()",
            "    social_url = f\"{netschema}://{host}{path}\"",
            "    # Don't use datetime strftime() because it requires year >= 1900",
            "    # see",
            "    # https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior",
            "    date = \"{0.month:02d}/{0.day:02d}/{0.year:4d}\".format(resourcebase.date) if resourcebase.date else None",
            "    abstract = build_abstract(resourcebase, url=social_url, includeURL=True)",
            "    caveats = build_caveats(resourcebase)",
            "    hashtags = \",\".join(getattr(settings, \"TWITTER_HASHTAGS\", []))",
            "    return format_urls(",
            "        settings.SOCIAL_ORIGINS,",
            "        {",
            "            \"name\": resourcebase.title,",
            "            \"date\": date,",
            "            \"abstract\": abstract,",
            "            \"caveats\": caveats,",
            "            \"hashtags\": hashtags,",
            "            \"url\": social_url,",
            "        },",
            "    )",
            "",
            "",
            "def check_shp_columnnames(layer):",
            "    \"\"\"Check if shapefile for a given layer has valid column names.",
            "    If not, try to fix column names and warn the user",
            "    \"\"\"",
            "    # TODO we may add in a better location this method",
            "    inShapefile = \"\"",
            "    for f in layer.upload_session.layerfile_set.all():",
            "        if os.path.splitext(f.file.name)[1] == \".shp\":",
            "            inShapefile = f.file.path",
            "    if inShapefile:",
            "        return fixup_shp_columnnames(inShapefile, layer.charset)",
            "",
            "",
            "def clone_shp_field_defn(srcFieldDefn, name):",
            "    \"\"\"",
            "    Clone an existing ogr.FieldDefn with a new name",
            "    \"\"\"",
            "    dstFieldDefn = ogr.FieldDefn(name, srcFieldDefn.GetType())",
            "    dstFieldDefn.SetWidth(srcFieldDefn.GetWidth())",
            "    dstFieldDefn.SetPrecision(srcFieldDefn.GetPrecision())",
            "",
            "    return dstFieldDefn",
            "",
            "",
            "def rename_shp_columnnames(inLayer, fieldnames):",
            "    \"\"\"",
            "    Rename columns in a layer to those specified in the given mapping",
            "    \"\"\"",
            "    inLayerDefn = inLayer.GetLayerDefn()",
            "",
            "    for i in range(inLayerDefn.GetFieldCount()):",
            "        srcFieldDefn = inLayerDefn.GetFieldDefn(i)",
            "        dstFieldName = fieldnames.get(srcFieldDefn.GetName())",
            "",
            "        if dstFieldName is not None:",
            "            dstFieldDefn = clone_shp_field_defn(srcFieldDefn, dstFieldName)",
            "            inLayer.AlterFieldDefn(i, dstFieldDefn, ogr.ALTER_NAME_FLAG)",
            "",
            "",
            "def fixup_shp_columnnames(inShapefile, charset, tempdir=None):",
            "    \"\"\"Try to fix column names and warn the user\"\"\"",
            "    charset = charset if charset and \"undefined\" not in charset else \"UTF-8\"",
            "    if not tempdir:",
            "        tempdir = mkdtemp()",
            "",
            "    if is_zipfile(inShapefile):",
            "        inShapefile = unzip_file(inShapefile, \".shp\", tempdir=tempdir)",
            "",
            "    inDriver = ogr.GetDriverByName(\"ESRI Shapefile\")",
            "    try:",
            "        inDataSource = inDriver.Open(inShapefile, 1)",
            "    except Exception:",
            "        tb = traceback.format_exc()",
            "        logger.debug(tb)",
            "        inDataSource = None",
            "",
            "    if inDataSource is None:",
            "        logger.debug(f\"Could not open {inShapefile}\")",
            "        return False, None, None",
            "    else:",
            "        inLayer = inDataSource.GetLayer()",
            "",
            "    # TODO we may need to improve this regexp",
            "    # first character must be any letter or \"_\"",
            "    # following characters can be any letter, number, \"#\", \":\"",
            "    regex = r\"^[a-zA-Z,_][a-zA-Z,_#:\\d]*$\"",
            "    a = re.compile(regex)",
            "    regex_first_char = r\"[a-zA-Z,_]{1}\"",
            "    b = re.compile(regex_first_char)",
            "    inLayerDefn = inLayer.GetLayerDefn()",
            "",
            "    list_col_original = []",
            "    list_col = {}",
            "",
            "    for i in range(inLayerDefn.GetFieldCount()):",
            "        try:",
            "            field_name = inLayerDefn.GetFieldDefn(i).GetName()",
            "            if a.match(field_name):",
            "                list_col_original.append(field_name)",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            return True, None, None",
            "",
            "    for i in range(inLayerDefn.GetFieldCount()):",
            "        try:",
            "            field_name = inLayerDefn.GetFieldDefn(i).GetName()",
            "            if not a.match(field_name):",
            "                # once the field_name contains Chinese, to use slugify_zh",
            "                if any(\"\\u4e00\" <= ch <= \"\\u9fff\" for ch in field_name):",
            "                    new_field_name = slugify_zh(field_name, separator=\"_\")",
            "                else:",
            "                    new_field_name = slugify(field_name)",
            "                if not b.match(new_field_name):",
            "                    new_field_name = f\"_{new_field_name}\"",
            "                j = 0",
            "                while new_field_name in list_col_original or new_field_name in list_col.values():",
            "                    if j == 0:",
            "                        new_field_name += \"_0\"",
            "                    if new_field_name.endswith(f\"_{str(j)}\"):",
            "                        j += 1",
            "                        new_field_name = f\"{new_field_name[:-2]}_{str(j)}\"",
            "                if field_name != new_field_name:",
            "                    list_col[field_name] = new_field_name",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            return True, None, None",
            "",
            "    if len(list_col) == 0:",
            "        return True, None, None",
            "    else:",
            "        try:",
            "            rename_shp_columnnames(inLayer, list_col)",
            "            inDataSource.SyncToDisk()",
            "            inDataSource.Destroy()",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            raise GeoNodeException(f\"Could not decode SHAPEFILE attributes by using the specified charset '{charset}'.\")",
            "    return True, None, list_col",
            "",
            "",
            "def id_to_obj(id_):",
            "    if id_ == id_none:",
            "        return None",
            "",
            "    for obj in gc.get_objects():",
            "        if id(obj) == id_:",
            "            return obj",
            "    raise Exception(\"Not found\")",
            "",
            "",
            "def printsignals():",
            "    for signalname in signalnames:",
            "        logger.debug(f\"SIGNALNAME: {signalname}\")",
            "        signaltype = getattr(models.signals, signalname)",
            "        signals = signaltype.receivers[:]",
            "        for signal in signals:",
            "            logger.debug(signal)",
            "",
            "",
            "class DisableDjangoSignals:",
            "    \"\"\"",
            "    Python3 class temporarily disabling django signals on model creation.",
            "",
            "    usage:",
            "    with DisableDjangoSignals():",
            "        # do some fancy stuff here",
            "    \"\"\"",
            "",
            "    def __init__(self, disabled_signals=None, skip=False):",
            "        self.skip = skip",
            "        self.stashed_signals = defaultdict(list)",
            "        self.disabled_signals = disabled_signals or [",
            "            signals.pre_init,",
            "            signals.post_init,",
            "            signals.pre_save,",
            "            signals.post_save,",
            "            signals.pre_delete,",
            "            signals.post_delete,",
            "            signals.pre_migrate,",
            "            signals.post_migrate,",
            "            signals.m2m_changed,",
            "        ]",
            "",
            "    def __enter__(self):",
            "        if not self.skip:",
            "            for signal in self.disabled_signals:",
            "                self.disconnect(signal)",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        if not self.skip:",
            "            for signal in list(self.stashed_signals):",
            "                self.reconnect(signal)",
            "",
            "    def disconnect(self, signal):",
            "        self.stashed_signals[signal] = signal.receivers",
            "        signal.receivers = []",
            "",
            "    def reconnect(self, signal):",
            "        signal.receivers = self.stashed_signals.get(signal, [])",
            "        del self.stashed_signals[signal]",
            "",
            "",
            "def run_subprocess(*cmd, **kwargs):",
            "    p = subprocess.Popen(\" \".join(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)",
            "    stdout = StringIO()",
            "    stderr = StringIO()",
            "    buff_size = 1024",
            "    while p.poll() is None:",
            "        inr = [p.stdout.fileno(), p.stderr.fileno()]",
            "        inw = []",
            "        rlist, wlist, xlist = select.select(inr, inw, [])",
            "",
            "        for r in rlist:",
            "            if r == p.stdout.fileno():",
            "                readfrom = p.stdout",
            "                readto = stdout",
            "            else:",
            "                readfrom = p.stderr",
            "                readto = stderr",
            "            readto.write(readfrom.read(buff_size))",
            "",
            "        for w in wlist:",
            "            w.write(\"\")",
            "",
            "    return p.returncode, stdout.getvalue(), stderr.getvalue()",
            "",
            "",
            "def parse_datetime(value):",
            "    for patt in settings.DATETIME_INPUT_FORMATS:",
            "        try:",
            "            if isinstance(value, dict):",
            "                value_obj = value[\"$\"] if \"$\" in value else value[\"content\"]",
            "                return datetime.datetime.strptime(value_obj, patt)",
            "            else:",
            "                return datetime.datetime.strptime(value, patt)",
            "        except Exception:",
            "            tb = traceback.format_exc()",
            "            logger.debug(tb)",
            "    raise ValueError(f\"Invalid datetime input: {value}\")",
            "",
            "",
            "def _convert_sql_params(cur, query):",
            "    # sqlite driver doesn't support %(key)s notation,",
            "    # use :key instead.",
            "    if cur.db.vendor in (",
            "        \"sqlite\",",
            "        \"sqlite3\",",
            "        \"spatialite\",",
            "    ):",
            "        return SQL_PARAMS_RE.sub(r\":\\1\", query)",
            "    return query",
            "",
            "",
            "@transaction.atomic",
            "def raw_sql(query, params=None, ret=True):",
            "    \"\"\"",
            "    Execute raw query",
            "    param ret=True returns data from cursor as iterator",
            "    \"\"\"",
            "    with connection.cursor() as c:",
            "        query = _convert_sql_params(c, query)",
            "        c.execute(query, params)",
            "        if ret:",
            "            desc = [r[0] for r in c.description]",
            "            for row in c:",
            "                yield dict(zip(desc, row))",
            "",
            "",
            "def get_client_ip(request):",
            "    x_forwarded_for = request.META.get(\"HTTP_X_FORWARDED_FOR\")",
            "    if x_forwarded_for:",
            "        ip = x_forwarded_for.split(\",\")[0]",
            "    else:",
            "        ip = request.META.get(\"REMOTE_ADDR\")",
            "    return ip",
            "",
            "",
            "def get_client_host(request):",
            "    hostname = None",
            "    http_host = request.META.get(\"HTTP_HOST\")",
            "    if http_host:",
            "        hostname = http_host.split(\":\")[0]",
            "    return hostname",
            "",
            "",
            "def check_ogc_backend(backend_package):",
            "    \"\"\"Check that geonode use a particular OGC Backend integration",
            "",
            "    :param backend_package: django app of backend to use",
            "    :type backend_package: str",
            "",
            "    :return: bool",
            "    :rtype: bool",
            "    \"\"\"",
            "    ogc_conf = settings.OGC_SERVER[\"default\"]",
            "    is_configured = ogc_conf.get(\"BACKEND\") == backend_package",
            "",
            "    # Check environment variables",
            "    _backend = os.environ.get(\"BACKEND\", None)",
            "    if _backend:",
            "        return backend_package == _backend and is_configured",
            "",
            "    # Check exists in INSTALLED_APPS",
            "    try:",
            "        in_installed_apps = backend_package in settings.INSTALLED_APPS",
            "        return in_installed_apps and is_configured",
            "    except Exception:",
            "        pass",
            "    return False",
            "",
            "",
            "class HttpClient:",
            "    def __init__(self):",
            "        self.timeout = 5",
            "        self.retries = 1",
            "        self.pool_maxsize = 10",
            "        self.backoff_factor = 0.3",
            "        self.pool_connections = 10",
            "        self.status_forcelist = (500, 502, 503, 504)",
            "        self.username = \"admin\"",
            "        self.password = \"admin\"",
            "        if check_ogc_backend(geoserver.BACKEND_PACKAGE):",
            "            ogc_server_settings = settings.OGC_SERVER[\"default\"]",
            "            self.timeout = ogc_server_settings.get(\"TIMEOUT\", 5)",
            "            self.retries = ogc_server_settings.get(\"MAX_RETRIES\", 1)",
            "            self.backoff_factor = ogc_server_settings.get(\"BACKOFF_FACTOR\", 0.3)",
            "            self.pool_maxsize = ogc_server_settings.get(\"POOL_MAXSIZE\", 10)",
            "            self.pool_connections = ogc_server_settings.get(\"POOL_CONNECTIONS\", 10)",
            "            self.username = ogc_server_settings.get(\"USER\", \"admin\")",
            "            self.password = ogc_server_settings.get(\"PASSWORD\", \"geoserver\")",
            "",
            "    def request(",
            "        self,",
            "        url,",
            "        method=\"GET\",",
            "        data=None,",
            "        headers={},",
            "        stream=False,",
            "        timeout=None,",
            "        retries=None,",
            "        user=None,",
            "        verify=False,",
            "    ):",
            "        if (",
            "            (user or self.username != \"admin\")",
            "            and check_ogc_backend(geoserver.BACKEND_PACKAGE)",
            "            and \"Authorization\" not in headers",
            "        ):",
            "            if connection.cursor().db.vendor not in (\"sqlite\", \"sqlite3\", \"spatialite\"):",
            "                try:",
            "                    if user and isinstance(user, str):",
            "                        user = get_user_model().objects.get(username=user)",
            "                    _u = user or get_user_model().objects.get(username=self.username)",
            "                    access_token = get_or_create_token(_u)",
            "                    if access_token and not access_token.is_expired():",
            "                        headers[\"Authorization\"] = f\"Bearer {access_token.token}\"",
            "                except Exception:",
            "                    tb = traceback.format_exc()",
            "                    logger.debug(tb)",
            "            elif user == self.username:",
            "                valid_uname_pw = base64.b64encode(f\"{self.username}:{self.password}\".encode()).decode()",
            "                headers[\"Authorization\"] = f\"Basic {valid_uname_pw}\"",
            "",
            "        headers[\"User-Agent\"] = \"GeoNode\"",
            "        response = None",
            "        content = None",
            "        session = requests.Session()",
            "        retry = Retry(",
            "            total=retries or self.retries,",
            "            read=retries or self.retries,",
            "            connect=retries or self.retries,",
            "            backoff_factor=self.backoff_factor,",
            "            status_forcelist=self.status_forcelist,",
            "        )",
            "        adapter = requests.adapters.HTTPAdapter(",
            "            max_retries=retry, pool_maxsize=self.pool_maxsize, pool_connections=self.pool_connections",
            "        )",
            "        scheme = urlsplit(url).scheme",
            "        session.mount(f\"{scheme}://\", adapter)",
            "        session.verify = False",
            "        action = getattr(session, method.lower(), None)",
            "        if action:",
            "            _req_tout = timeout or self.timeout",
            "            try:",
            "                response = action(url=url, data=data, headers=headers, timeout=_req_tout, stream=stream, verify=verify)",
            "            except (",
            "                requests.exceptions.ConnectTimeout,",
            "                requests.exceptions.RequestException,",
            "                ValueError,",
            "                RetryError,",
            "            ) as e:",
            "                msg = f\"Request exception [{e}] - TOUT [{_req_tout}] to URL: {url} - headers: {headers}\"",
            "                logger.exception(Exception(msg))",
            "                response = None",
            "                content = str(e)",
            "        else:",
            "            response = session.get(url, headers=headers, timeout=self.timeout)",
            "        if response:",
            "            try:",
            "                content = ensure_string(response.content) if not stream else response.raw",
            "            except Exception as e:",
            "                content = str(e)",
            "",
            "        return (response, content)",
            "",
            "    def get(self, url, data=None, headers={}, stream=False, timeout=None, user=None, verify=False):",
            "        return self.request(",
            "            url,",
            "            method=\"GET\",",
            "            data=data,",
            "            headers=headers,",
            "            timeout=timeout or self.timeout,",
            "            stream=stream,",
            "            user=user,",
            "            verify=verify,",
            "        )",
            "",
            "    def post(self, url, data=None, headers={}, stream=False, timeout=None, user=None, verify=False):",
            "        return self.request(",
            "            url,",
            "            method=\"POST\",",
            "            data=data,",
            "            headers=headers,",
            "            timeout=timeout or self.timeout,",
            "            stream=stream,",
            "            user=user,",
            "            verify=verify,",
            "        )",
            "",
            "",
            "http_client = HttpClient()",
            "",
            "",
            "def get_dir_time_suffix():",
            "    \"\"\"Returns the name of a folder with the 'now' time as suffix\"\"\"",
            "    dirfmt = \"%4d-%02d-%02d_%02d%02d%02d\"",
            "    now = time.localtime()[0:6]",
            "    dirname = dirfmt % now",
            "",
            "    return dirname",
            "",
            "",
            "def zip_dir(basedir, archivename):",
            "    assert os.path.isdir(basedir)",
            "    with closing(ZipFile(archivename, \"w\", ZIP_DEFLATED, allowZip64=True)) as z:",
            "        for root, dirs, files in os.walk(basedir):",
            "            # NOTE: ignore empty directories",
            "            for fn in files:",
            "                absfn = os.path.join(root, fn)",
            "                zfn = absfn[len(basedir) + len(os.sep) :]  # XXX: relative path",
            "                z.write(absfn, zfn)",
            "",
            "",
            "def copy_tree(src, dst, symlinks=False, ignore=None):",
            "    try:",
            "        for item in os.listdir(src):",
            "            s = os.path.join(src, item)",
            "            d = os.path.join(dst, item)",
            "            if os.path.isdir(s):",
            "                if os.path.exists(d):",
            "                    try:",
            "                        os.remove(d)",
            "                    except Exception:",
            "                        shutil.rmtree(d, ignore_errors=True)",
            "                try:",
            "                    shutil.copytree(s, d, symlinks=symlinks, ignore=ignore)",
            "                except Exception:",
            "                    pass",
            "            else:",
            "                try:",
            "                    if ignore and s in ignore(dst, [s]):",
            "                        return",
            "                    shutil.copy2(s, d)",
            "                except Exception:",
            "                    pass",
            "    except Exception:",
            "        traceback.print_exc()",
            "",
            "",
            "def extract_archive(zip_file, dst):",
            "    target_folder = os.path.join(dst, os.path.splitext(os.path.basename(zip_file))[0])",
            "    if not os.path.exists(target_folder):",
            "        os.makedirs(target_folder, exist_ok=True)",
            "",
            "    with ZipFile(zip_file, \"r\", allowZip64=True) as z:",
            "        z.extractall(target_folder)",
            "",
            "    return target_folder",
            "",
            "",
            "def chmod_tree(dst, permissions=0o777):",
            "    for dirpath, dirnames, filenames in os.walk(dst):",
            "        for filename in filenames:",
            "            path = os.path.join(dirpath, filename)",
            "            os.chmod(path, permissions)",
            "            status = os.stat(path)",
            "            if oct(status.st_mode & 0o777) != str(oct(permissions)):",
            "                raise Exception(f\"Could not update permissions of {path}\")",
            "",
            "        for dirname in dirnames:",
            "            path = os.path.join(dirpath, dirname)",
            "            os.chmod(path, permissions)",
            "            status = os.stat(path)",
            "            if oct(status.st_mode & 0o777) != str(oct(permissions)):",
            "                raise Exception(f\"Could not update permissions of {path}\")",
            "",
            "",
            "def slugify_zh(text, separator=\"_\"):",
            "    \"\"\"",
            "    Make a slug from the given text, which is simplified from slugify.",
            "    It remove the other args and do not convert Chinese into Pinyin",
            "    :param text (str): initial text",
            "    :param separator (str): separator between words",
            "    :return (str):",
            "    \"\"\"",
            "",
            "    QUOTE_PATTERN = re.compile(r\"[\\']+\")",
            "    ALLOWED_CHARS_PATTERN = re.compile(\"[^\\u4e00-\\u9fa5a-z0-9]+\")",
            "    DUPLICATE_DASH_PATTERN = re.compile(\"-{2,}\")",
            "    NUMBERS_PATTERN = re.compile(r\"(?<=\\d),(?=\\d)\")",
            "    DEFAULT_SEPARATOR = \"-\"",
            "",
            "    # if not isinstance(text, types.UnicodeType):",
            "    #    text = unicode(text, 'utf-8', 'ignore')",
            "    # replace quotes with dashes - pre-process",
            "    text = QUOTE_PATTERN.sub(DEFAULT_SEPARATOR, text)",
            "    # make the text lowercase",
            "    text = text.lower()",
            "    # remove generated quotes -- post-process",
            "    text = QUOTE_PATTERN.sub(\"\", text)",
            "    # cleanup numbers",
            "    text = NUMBERS_PATTERN.sub(\"\", text)",
            "    # replace all other unwanted characters",
            "    text = re.sub(ALLOWED_CHARS_PATTERN, DEFAULT_SEPARATOR, text)",
            "    # remove redundant",
            "    text = re.sub(DUPLICATE_DASH_PATTERN, DEFAULT_SEPARATOR, text).strip(DEFAULT_SEPARATOR)",
            "    if separator != DEFAULT_SEPARATOR:",
            "        text = text.replace(DEFAULT_SEPARATOR, separator)",
            "    return text",
            "",
            "",
            "def get_legend_url(",
            "    instance,",
            "    style_name,",
            "    /,",
            "    service_url=None,",
            "    dataset_name=None,",
            "    version=\"1.3.0\",",
            "    sld_version=\"1.1.0\",",
            "    width=20,",
            "    height=20,",
            "    params=None,",
            "):",
            "    from geonode.geoserver.helpers import ogc_server_settings",
            "",
            "    _service_url = service_url or f\"{ogc_server_settings.PUBLIC_LOCATION}ows\"",
            "    _dataset_name = dataset_name or instance.alternate",
            "    _params = f\"&{params}\" if params else \"\"",
            "    return (",
            "        f\"{_service_url}?\"",
            "        f\"service=WMS&request=GetLegendGraphic&format=image/png&WIDTH={width}&HEIGHT={height}&\"",
            "        f\"LAYER={_dataset_name}&STYLE={style_name}&version={version}&\"",
            "        f\"sld_version={sld_version}&legend_options=fontAntiAliasing:true;fontSize:12;forceLabels:on{_params}\"",
            "    )",
            "",
            "",
            "def set_resource_default_links(instance, layer, prune=False, **kwargs):",
            "    from geonode.base.models import Link",
            "    from django.urls import reverse",
            "    from django.utils.translation import ugettext",
            "    from geonode.layers.models import Dataset",
            "    from geonode.documents.models import Document",
            "",
            "    # Prune old links",
            "    if prune:",
            "        logger.debug(\" -- Resource Links[Prune old links]...\")",
            "        _def_link_types = (\"data\", \"image\", \"original\", \"html\", \"OGC:WMS\", \"OGC:WFS\", \"OGC:WCS\")",
            "        Link.objects.filter(resource=instance.resourcebase_ptr, link_type__in=_def_link_types).delete()",
            "        logger.debug(\" -- Resource Links[Prune old links]...done!\")",
            "",
            "    if check_ogc_backend(geoserver.BACKEND_PACKAGE):",
            "        from geonode.geoserver.ows import wcs_links, wfs_links, wms_links",
            "        from geonode.geoserver.helpers import ogc_server_settings, gs_catalog",
            "",
            "        # Compute parameters for the new links",
            "        logger.debug(\" -- Resource Links[Compute parameters for the new links]...\")",
            "        height = 550",
            "        width = 550",
            "",
            "        # Parse Dataset BBOX and SRID",
            "        bbox = None",
            "        srid = instance.srid if instance.srid else getattr(settings, \"DEFAULT_MAP_CRS\", \"EPSG:4326\")",
            "        if not prune and instance.srid and instance.bbox_polygon:",
            "            bbox = instance.bbox_string",
            "        else:",
            "            try:",
            "                gs_resource = gs_catalog.get_resource(",
            "                    name=instance.name, store=instance.store, workspace=instance.workspace",
            "                )",
            "                if not gs_resource:",
            "                    gs_resource = gs_catalog.get_resource(name=instance.name, workspace=instance.workspace)",
            "                if not gs_resource:",
            "                    gs_resource = gs_catalog.get_resource(name=instance.name)",
            "",
            "                if gs_resource:",
            "                    srid = gs_resource.projection",
            "                    bbox = gs_resource.native_bbox",
            "                    ll_bbox = gs_resource.latlon_bbox",
            "                    try:",
            "                        instance.set_bbox_polygon([bbox[0], bbox[2], bbox[1], bbox[3]], srid)",
            "                    except GeoNodeException as e:",
            "                        if not ll_bbox:",
            "                            raise",
            "                        else:",
            "                            logger.exception(e)",
            "                            instance.srid = \"EPSG:4326\"",
            "                    instance.set_ll_bbox_polygon([ll_bbox[0], ll_bbox[2], ll_bbox[1], ll_bbox[3]])",
            "                    if instance.srid:",
            "                        instance.srid_url = (",
            "                            f\"http://www.spatialreference.org/ref/{instance.srid.replace(':', '/').lower()}/\"",
            "                        )",
            "                    elif instance.bbox_polygon is not None:",
            "                        # Guessing 'EPSG:4326' by default",
            "                        instance.srid = \"EPSG:4326\"",
            "                    else:",
            "                        raise GeoNodeException(_(\"Invalid Projection. Dataset is missing CRS!\"))",
            "                    dx = float(bbox[1]) - float(bbox[0])",
            "                    dy = float(bbox[3]) - float(bbox[2])",
            "                    dataAspect = 1 if dy == 0 else dx / dy",
            "                    width = int(height * dataAspect)",
            "                    # Rewriting BBOX as a plain string",
            "                    bbox = \",\".join(str(x) for x in [bbox[0], bbox[2], bbox[1], bbox[3]])",
            "                else:",
            "                    bbox = instance.bbox_string",
            "            except Exception as e:",
            "                logger.exception(e)",
            "                bbox = instance.bbox_string",
            "",
            "        # Create Raw Data download link",
            "        if settings.DISPLAY_ORIGINAL_DATASET_LINK:",
            "            logger.debug(\" -- Resource Links[Create Raw Data download link]...\")",
            "            if isinstance(instance, Dataset):",
            "                download_url = build_absolute_uri(reverse(\"dataset_download\", args=(instance.alternate,)))",
            "            elif isinstance(instance, Document):",
            "                download_url = build_absolute_uri(reverse(\"document_download\", args=(instance.id,)))",
            "            else:",
            "                download_url = None",
            "",
            "            while Link.objects.filter(resource=instance.resourcebase_ptr, link_type=\"original\").exists():",
            "                Link.objects.filter(resource=instance.resourcebase_ptr, link_type=\"original\").delete()",
            "            Link.objects.update_or_create(",
            "                resource=instance.resourcebase_ptr,",
            "                url=download_url,",
            "                defaults=dict(",
            "                    extension=\"zip\",",
            "                    name=\"Original Dataset\",",
            "                    mime=\"application/octet-stream\",",
            "                    link_type=\"original\",",
            "                ),",
            "            )",
            "            logger.debug(\" -- Resource Links[Create Raw Data download link]...done!\")",
            "        else:",
            "            Link.objects.filter(resource=instance.resourcebase_ptr, name=\"Original Dataset\").delete()",
            "",
            "        # Set download links for WMS, WCS or WFS and KML",
            "        logger.debug(\" -- Resource Links[Set download links for WMS, WCS or WFS and KML]...\")",
            "        instance_ows_url = f\"{instance.ows_url}?\" if instance.ows_url else f\"{ogc_server_settings.public_url}ows?\"",
            "        links = wms_links(instance_ows_url, instance.alternate, bbox, srid, height, width)",
            "",
            "        for ext, name, mime, wms_url in links:",
            "            try:",
            "                Link.objects.update_or_create(",
            "                    resource=instance.resourcebase_ptr,",
            "                    name=ugettext(name),",
            "                    defaults=dict(",
            "                        extension=ext,",
            "                        url=wms_url,",
            "                        mime=mime,",
            "                        link_type=\"image\",",
            "                    ),",
            "                )",
            "            except Link.MultipleObjectsReturned:",
            "                _d = dict(extension=ext, url=wms_url, mime=mime, link_type=\"image\")",
            "                Link.objects.filter(resource=instance.resourcebase_ptr, name=ugettext(name), link_type=\"image\").update(",
            "                    **_d",
            "                )",
            "",
            "        if instance.subtype == \"vector\":",
            "            links = wfs_links(",
            "                instance_ows_url,",
            "                instance.alternate,",
            "                bbox=None,  # bbox filter should be set at runtime otherwise conflicting with CQL",
            "                srid=srid,",
            "            )",
            "            for ext, name, mime, wfs_url in links:",
            "                if mime == \"SHAPE-ZIP\":",
            "                    name = \"Zipped Shapefile\"",
            "                if (",
            "                    Link.objects.filter(",
            "                        resource=instance.resourcebase_ptr, url=wfs_url, name=name, link_type=\"data\"",
            "                    ).count()",
            "                    < 2",
            "                ):",
            "                    Link.objects.update_or_create(",
            "                        resource=instance.resourcebase_ptr,",
            "                        url=wfs_url,",
            "                        name=name,",
            "                        link_type=\"data\",",
            "                        defaults=dict(",
            "                            extension=ext,",
            "                            mime=mime,",
            "                        ),",
            "                    )",
            "",
            "        elif instance.subtype == \"raster\":",
            "            \"\"\"",
            "            Going to create the WCS GetCoverage Default download links.",
            "            By providing 'None' bbox and srid, we are going to ask to the WCS to",
            "            skip subsetting, i.e. output the whole coverage in the netive SRS.",
            "",
            "            Notice that the \"wcs_links\" method also generates 1 default \"outputFormat\":",
            "             - \"geotiff\"; GeoTIFF which will be compressed and tiled by passing to the WCS the default query params compression='DEFLATE' and tile_size=512",
            "            \"\"\"",
            "            links = wcs_links(instance_ows_url, instance.alternate)",
            "            for ext, name, mime, wcs_url in links:",
            "                if (",
            "                    Link.objects.filter(",
            "                        resource=instance.resourcebase_ptr, url=wcs_url, name=name, link_type=\"data\"",
            "                    ).count()",
            "                    < 2",
            "                ):",
            "                    Link.objects.update_or_create(",
            "                        resource=instance.resourcebase_ptr,",
            "                        url=wcs_url,",
            "                        name=name,",
            "                        link_type=\"data\",",
            "                        defaults=dict(",
            "                            extension=ext,",
            "                            mime=mime,",
            "                        ),",
            "                    )",
            "",
            "        site_url = settings.SITEURL.rstrip(\"/\") if settings.SITEURL.startswith(\"http\") else settings.SITEURL",
            "        html_link_url = f\"{site_url}{instance.get_absolute_url()}\"",
            "",
            "        if (",
            "            Link.objects.filter(",
            "                resource=instance.resourcebase_ptr, url=html_link_url, name=instance.alternate, link_type=\"html\"",
            "            ).count()",
            "            < 2",
            "        ):",
            "            Link.objects.update_or_create(",
            "                resource=instance.resourcebase_ptr,",
            "                url=html_link_url,",
            "                name=instance.alternate or instance.name,",
            "                link_type=\"html\",",
            "                defaults=dict(",
            "                    extension=\"html\",",
            "                    mime=\"text/html\",",
            "                ),",
            "            )",
            "        logger.debug(\" -- Resource Links[Set download links for WMS, WCS or WFS and KML]...done!\")",
            "",
            "        # Legend link",
            "        logger.debug(\" -- Resource Links[Legend link]...\")",
            "        try:",
            "            if instance.subtype not in [\"tileStore\", \"remote\"]:",
            "                for style in set(",
            "                    list(instance.styles.all())",
            "                    + [",
            "                        instance.default_style,",
            "                    ]",
            "                ):",
            "                    if style:",
            "                        style_name = os.path.basename(urlparse(style.sld_url).path).split(\".\")[0]",
            "                        legend_url = get_legend_url(instance, style_name)",
            "                        if Link.objects.filter(resource=instance.resourcebase_ptr, url=legend_url).count() < 2:",
            "                            Link.objects.update_or_create(",
            "                                resource=instance.resourcebase_ptr,",
            "                                name=\"Legend\",",
            "                                url=legend_url,",
            "                                defaults=dict(",
            "                                    extension=\"png\",",
            "                                    url=legend_url,",
            "                                    mime=\"image/png\",",
            "                                    link_type=\"image\",",
            "                                ),",
            "                            )",
            "            else:",
            "                from geonode.services.serviceprocessors import get_service_handler",
            "",
            "                handler = get_service_handler(",
            "                    instance.remote_service.service_url, service_type=instance.remote_service.type",
            "                )",
            "                if handler and hasattr(handler, \"_create_dataset_legend_link\"):",
            "                    handler._create_dataset_legend_link(instance)",
            "",
            "            logger.debug(\" -- Resource Links[Legend link]...done!\")",
            "        except Exception as e:",
            "            logger.debug(f\" -- Resource Links[Legend link]...error: {e}\")",
            "",
            "        # Thumbnail link",
            "        if instance.get_thumbnail_url():",
            "            logger.debug(\" -- Resource Links[Thumbnail link]...\")",
            "            if (",
            "                Link.objects.filter(",
            "                    resource=instance.resourcebase_ptr, url=instance.get_thumbnail_url(), name=\"Thumbnail\"",
            "                ).count()",
            "                < 2",
            "            ):",
            "                Link.objects.update_or_create(",
            "                    resource=instance.resourcebase_ptr,",
            "                    url=instance.get_thumbnail_url(),",
            "                    name=\"Thumbnail\",",
            "                    defaults=dict(",
            "                        extension=\"png\",",
            "                        mime=\"image/png\",",
            "                        link_type=\"image\",",
            "                    ),",
            "                )",
            "            logger.debug(\" -- Resource Links[Thumbnail link]...done!\")",
            "",
            "        logger.debug(\" -- Resource Links[OWS Links]...\")",
            "        try:",
            "            if (",
            "                not hasattr(instance.get_real_instance(), \"ptype\")",
            "                or instance.get_real_instance().ptype == GXP_PTYPES[\"WMS\"]",
            "            ):",
            "                ogc_wms_url = instance.ows_url or urljoin(ogc_server_settings.public_url, \"ows\")",
            "                ogc_wms_name = f\"OGC WMS: {instance.workspace} Service\"",
            "                if (",
            "                    Link.objects.filter(resource=instance.resourcebase_ptr, name=ogc_wms_name, url=ogc_wms_url).count()",
            "                    < 2",
            "                ):",
            "                    Link.objects.get_or_create(",
            "                        resource=instance.resourcebase_ptr,",
            "                        url=ogc_wms_url,",
            "                        name=ogc_wms_name,",
            "                        defaults=dict(",
            "                            extension=\"html\",",
            "                            url=ogc_wms_url,",
            "                            mime=\"text/html\",",
            "                            link_type=\"OGC:WMS\",",
            "                        ),",
            "                    )",
            "",
            "                if instance.subtype == \"vector\":",
            "                    ogc_wfs_url = instance.ows_url or urljoin(ogc_server_settings.public_url, \"ows\")",
            "                    ogc_wfs_name = f\"OGC WFS: {instance.workspace} Service\"",
            "                    if (",
            "                        Link.objects.filter(",
            "                            resource=instance.resourcebase_ptr, name=ogc_wfs_name, url=ogc_wfs_url",
            "                        ).count()",
            "                        < 2",
            "                    ):",
            "                        Link.objects.get_or_create(",
            "                            resource=instance.resourcebase_ptr,",
            "                            url=ogc_wfs_url,",
            "                            name=ogc_wfs_name,",
            "                            defaults=dict(",
            "                                extension=\"html\",",
            "                                url=ogc_wfs_url,",
            "                                mime=\"text/html\",",
            "                                link_type=\"OGC:WFS\",",
            "                            ),",
            "                        )",
            "",
            "                if instance.subtype == \"raster\":",
            "                    ogc_wcs_url = instance.ows_url or urljoin(ogc_server_settings.public_url, \"ows\")",
            "                    ogc_wcs_name = f\"OGC WCS: {instance.workspace} Service\"",
            "                    if (",
            "                        Link.objects.filter(",
            "                            resource=instance.resourcebase_ptr, name=ogc_wcs_name, url=ogc_wcs_url",
            "                        ).count()",
            "                        < 2",
            "                    ):",
            "                        Link.objects.get_or_create(",
            "                            resource=instance.resourcebase_ptr,",
            "                            url=ogc_wcs_url,",
            "                            name=ogc_wcs_name,",
            "                            defaults=dict(",
            "                                extension=\"html\",",
            "                                url=ogc_wcs_url,",
            "                                mime=\"text/html\",",
            "                                link_type=\"OGC:WCS\",",
            "                            ),",
            "                        )",
            "",
            "            elif hasattr(instance.get_real_instance(), \"ptype\") and instance.get_real_instance().ptype:",
            "                ptype_link = dict((v, k) for k, v in GXP_PTYPES.items()).get(instance.get_real_instance().ptype)",
            "                ptype_link_name = get_available_service_types().get(ptype_link)",
            "                ptype_link_url = instance.ows_url",
            "                if (",
            "                    Link.objects.filter(",
            "                        resource=instance.resourcebase_ptr, name=ptype_link_name, url=ptype_link_url",
            "                    ).count()",
            "                    < 2",
            "                ):",
            "                    Link.objects.get_or_create(",
            "                        resource=instance.resourcebase_ptr,",
            "                        url=ptype_link_url,",
            "                        name=ptype_link_name,",
            "                        defaults=dict(",
            "                            extension=\"html\",",
            "                            url=ptype_link_url,",
            "                            mime=\"text/html\",",
            "                            link_type=\"image\",",
            "                        ),",
            "                    )",
            "            logger.debug(\" -- Resource Links[OWS Links]...done!\")",
            "        except Exception as e:",
            "            logger.error(\" -- Resource Links[OWS Links]...error!\")",
            "            logger.exception(e)",
            "",
            "",
            "def add_url_params(url, params):",
            "    \"\"\"Add GET params to provided URL being aware of existing.",
            "",
            "    :param url: string of target URL",
            "    :param params: dict containing requested params to be added",
            "    :return: string with updated URL",
            "",
            "    >> url = 'http://stackoverflow.com/test?answers=true'",
            "    >> new_params = {'answers': False, 'data': ['some','values']}",
            "    >> add_url_params(url, new_params)",
            "    'http://stackoverflow.com/test?data=some&data=values&answers=false'",
            "    \"\"\"",
            "    # Unquoting URL first so we don't loose existing args",
            "    url = unquote(url)",
            "    # Extracting url info",
            "    parsed_url = urlparse(url)",
            "    # Extracting URL arguments from parsed URL",
            "    get_args = parsed_url.query",
            "    # Converting URL arguments to dict",
            "    parsed_get_args = dict(parse_qsl(get_args))",
            "    # Merging URL arguments dict with new params",
            "    parsed_get_args.update(params)",
            "",
            "    # Bool and Dict values should be converted to json-friendly values",
            "    # you may throw this part away if you don't like it :)",
            "    parsed_get_args.update({k: json.dumps(v) for k, v in parsed_get_args.items() if isinstance(v, (bool, dict))})",
            "",
            "    # Converting URL argument to proper query string",
            "    encoded_get_args = urlencode(parsed_get_args, doseq=True)",
            "    # Creating new parsed result object based on provided with new",
            "    # URL arguments. Same thing happens inside of urlparse.",
            "    new_url = ParseResult(",
            "        parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, encoded_get_args, parsed_url.fragment",
            "    ).geturl()",
            "",
            "    return new_url",
            "",
            "",
            "json_serializer_k_map = {",
            "    \"user\": settings.AUTH_USER_MODEL,",
            "    \"owner\": settings.AUTH_USER_MODEL,",
            "    \"restriction_code_type\": \"base.RestrictionCodeType\",",
            "    \"license\": \"base.License\",",
            "    \"category\": \"base.TopicCategory\",",
            "    \"spatial_representation_type\": \"base.SpatialRepresentationType\",",
            "    \"group\": \"auth.Group\",",
            "    \"default_style\": \"datasets.Style\",",
            "}",
            "",
            "",
            "def json_serializer_producer(dictionary):",
            "    \"\"\"",
            "    - usage:",
            "           serialized_obj =",
            "               json_serializer_producer(model_to_dict(instance))",
            "",
            "    - dump to file:",
            "       with open('data.json', 'w') as outfile:",
            "           json.dump(serialized_obj, outfile)",
            "",
            "    - read from file:",
            "       with open('data.json', 'r') as infile:",
            "           serialized_obj = json.load(infile)",
            "    \"\"\"",
            "",
            "    def to_json(keys):",
            "        if isinstance(keys, datetime.datetime):",
            "            return str(keys)",
            "        elif isinstance(keys, str) or isinstance(keys, int):",
            "            return keys",
            "        elif isinstance(keys, dict):",
            "            return json_serializer_producer(keys)",
            "        elif isinstance(keys, list):",
            "            return [json_serializer_producer(model_to_dict(k)) for k in keys]",
            "        elif isinstance(keys, models.Model):",
            "            return json_serializer_producer(model_to_dict(keys))",
            "        elif isinstance(keys, Decimal):",
            "            return float(keys)",
            "        else:",
            "            return str(keys)",
            "",
            "    output = {}",
            "",
            "    _keys_to_skip = [",
            "        \"email\",",
            "        \"password\",",
            "        \"last_login\",",
            "        \"date_joined\",",
            "        \"is_staff\",",
            "        \"is_active\",",
            "        \"is_superuser\",",
            "        \"permissions\",",
            "        \"user_permissions\",",
            "    ]",
            "",
            "    for x, y in dictionary.items():",
            "        if x not in _keys_to_skip:",
            "            if x in json_serializer_k_map.keys():",
            "                instance = django_apps.get_model(json_serializer_k_map[x], require_ready=False)",
            "                if instance.objects.filter(id=y):",
            "                    _obj = instance.objects.get(id=y)",
            "                    y = model_to_dict(_obj)",
            "            output[x] = to_json(y)",
            "    return output",
            "",
            "",
            "def is_monochromatic_image(image_url, image_data=None):",
            "    def is_local_static(url):",
            "        if url.startswith(settings.STATIC_URL) or (url.startswith(settings.SITEURL) and settings.STATIC_URL in url):",
            "            return True",
            "        return False",
            "",
            "    def is_absolute(url):",
            "        return bool(urlparse(url).netloc)",
            "",
            "    def get_thumb_handler(url):",
            "        _index = url.find(settings.STATIC_URL)",
            "        _thumb_path = urlparse(url[_index + len(settings.STATIC_URL) :]).path",
            "        if storage_manager.exists(_thumb_path):",
            "            return storage_manager.open(_thumb_path)",
            "        return None",
            "",
            "    def verify_image(stream):",
            "        with Image.open(stream) as _stream:",
            "            img = _stream.convert(\"L\")",
            "            img.verify()  # verify that it is, in fact an image",
            "            extr = img.getextrema()",
            "            a = 0",
            "            for i in extr:",
            "                if isinstance(i, tuple):",
            "                    a += abs(i[0] - i[1])",
            "                else:",
            "                    a = abs(extr[0] - extr[1])",
            "                    break",
            "            return a == 0",
            "",
            "    try:",
            "        if image_data:",
            "            logger.debug(\"...Checking if image is a blank image\")",
            "            with BytesIO(image_data) as stream:",
            "                return verify_image(stream)",
            "        elif image_url:",
            "            logger.debug(f\"...Checking if '{image_url}' is a blank image\")",
            "            url = image_url if is_absolute(image_url) else urljoin(settings.SITEURL, image_url)",
            "            if not is_local_static(url):",
            "                req, stream_content = http_client.get(url, timeout=5)",
            "                with BytesIO(stream_content) as stream:",
            "                    return verify_image(stream)",
            "            else:",
            "                with get_thumb_handler(url) as stream:",
            "                    return verify_image(stream)",
            "        return True",
            "    except Exception as e:",
            "        logger.debug(e)",
            "        return False",
            "",
            "",
            "def find_by_attr(lst, val, attr=\"id\"):",
            "    \"\"\"Returns an object if the id matches in any list of objects\"\"\"",
            "    for item in lst:",
            "        if attr in item and item[attr] == val:",
            "            return item",
            "",
            "    return None",
            "",
            "",
            "def build_absolute_uri(url):",
            "    if url and \"http\" not in url:",
            "        url = urljoin(settings.SITEURL, url)",
            "    return url",
            "",
            "",
            "def get_xpath_value(",
            "    element: etree.Element, xpath_expression: str, nsmap: typing.Optional[dict] = None",
            ") -> typing.Optional[str]:",
            "    if not nsmap:",
            "        nsmap = element.nsmap",
            "    values = element.xpath(f\"{xpath_expression}//text()\", namespaces=nsmap)",
            "    return \"\".join(values).strip() or None",
            "",
            "",
            "def get_geonode_app_types():",
            "    from geonode.geoapps.models import GeoApp",
            "",
            "    return list(set(GeoApp.objects.values_list(\"resource_type\", flat=True)))",
            "",
            "",
            "def get_supported_datasets_file_types():",
            "    from django.conf import settings as gn_settings",
            "",
            "    \"\"\"",
            "    Return a list of all supported file type in geonode",
            "    If one of the type provided in the custom type exists in the default",
            "    is going to override it",
            "    \"\"\"",
            "    default_types = settings.SUPPORTED_DATASET_FILE_TYPES",
            "    types_module = (",
            "        gn_settings.ADDITIONAL_DATASET_FILE_TYPES if hasattr(gn_settings, \"ADDITIONAL_DATASET_FILE_TYPES\") else []",
            "    )",
            "    supported_types = default_types.copy()",
            "    default_types_id = [t.get(\"id\") for t in default_types]",
            "    for _type in types_module:",
            "        if _type.get(\"id\") in default_types_id:",
            "            supported_types[default_types_id.index(_type.get(\"id\"))] = _type",
            "        else:",
            "            supported_types.extend([_type])",
            "",
            "    # Order the formats (to support their visualization)",
            "    formats_order = [(\"vector\", 0), (\"raster\", 1), (\"archive\", 2)]",
            "    ordered_payload = (",
            "        (weight[1], resource_type)",
            "        for resource_type in supported_types",
            "        for weight in formats_order",
            "        if resource_type.get(\"format\") in weight[0]",
            "    )",
            "",
            "    # Flatten the list",
            "    ordered_resource_types = [x[1] for x in sorted(ordered_payload, key=lambda x: x[0])]",
            "    other_resource_types = [",
            "        resource_type",
            "        for resource_type in supported_types",
            "        if resource_type.get(\"format\") is None or resource_type.get(\"format\") not in [f[0] for f in formats_order]",
            "    ]",
            "    return ordered_resource_types + other_resource_types",
            "",
            "",
            "def get_allowed_extensions():",
            "    return list(itertools.chain.from_iterable([_type[\"ext\"] for _type in get_supported_datasets_file_types()]))",
            "",
            "",
            "def safe_path_leaf(path):",
            "    \"\"\"A view that is not vulnerable to malicious file access.\"\"\"",
            "    base_path = settings.MEDIA_ROOT",
            "    try:",
            "        validate_filepath(path, platform=\"auto\")",
            "        head, tail = ntpath.split(path)",
            "        filename = tail or ntpath.basename(head)",
            "        validate_filename(filename, platform=\"auto\")",
            "    except ValidationError as e:",
            "        logger.error(f\"{e}\")",
            "        raise e",
            "    # GOOD -- Verify with normalised version of path",
            "    fullpath = os.path.normpath(os.path.join(head, filename))",
            "    if not fullpath.startswith(base_path) or path != fullpath:",
            "        raise GeoNodeException(",
            "            f\"The provided path '{path}' is not safe. The file is outside the MEDIA_ROOT '{base_path}' base path!\"",
            "        )",
            "    return fullpath",
            "",
            "",
            "def import_class_module(full_class_string):",
            "    \"\"\"",
            "    Dynamically load a class from a string",
            "",
            "    >>> klass = import_class_module(\"module.submodule.ClassName\")",
            "    >>> klass2 = import_class_module(\"myfile.Class2\")",
            "    \"\"\"",
            "    try:",
            "        module_path, class_name = full_class_string.rsplit(\".\", 1)",
            "        module = importlib.import_module(module_path)",
            "        class_obj = getattr(module, class_name)",
            "        return class_obj",
            "    except Exception:",
            "        return None"
        ],
        "afterPatchFile": [
            "#########################################################################",
            "#",
            "# Copyright (C) 2016 OSGeo",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU General Public License as published by",
            "# the Free Software Foundation, either version 3 of the License, or",
            "# (at your option) any later version.",
            "#",
            "# This program is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the",
            "# GNU General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU General Public License",
            "# along with this program. If not, see <http://www.gnu.org/licenses/>.",
            "#",
            "#########################################################################",
            "",
            "import os",
            "import gc",
            "import re",
            "import json",
            "import time",
            "import base64",
            "import ntpath",
            "import select",
            "import shutil",
            "import string",
            "import typing",
            "import logging",
            "import tarfile",
            "import datetime",
            "import requests",
            "import tempfile",
            "import importlib",
            "import ipaddress",
            "import itertools",
            "import traceback",
            "import subprocess",
            "",
            "from lxml import etree",
            "from osgeo import ogr",
            "from PIL import Image",
            "from urllib3 import Retry",
            "from io import BytesIO, StringIO",
            "from decimal import Decimal",
            "from threading import local",
            "from slugify import slugify",
            "from contextlib import closing",
            "from requests.exceptions import RetryError",
            "from collections import namedtuple, defaultdict",
            "from rest_framework.exceptions import APIException",
            "from math import atan, exp, log, pi, sin, tan, floor",
            "from zipfile import ZipFile, is_zipfile, ZIP_DEFLATED",
            "from pathvalidate import ValidationError, validate_filepath, validate_filename",
            "from geonode.upload.api.exceptions import GeneralUploadException",
            "",
            "from django.conf import settings",
            "from django.db.models import signals",
            "from django.utils.http import is_safe_url",
            "from django.apps import apps as django_apps",
            "from django.middleware.csrf import get_token",
            "from django.http import HttpResponse",
            "from django.forms.models import model_to_dict",
            "from django.contrib.auth import get_user_model",
            "from django.shortcuts import get_object_or_404",
            "from django.core.exceptions import PermissionDenied",
            "from django.core.exceptions import ImproperlyConfigured",
            "from django.core.serializers.json import DjangoJSONEncoder",
            "from django.db import models, connection, transaction",
            "from django.utils.translation import ugettext_lazy as _",
            "",
            "from geonode import geoserver, GeoNodeException  # noqa",
            "from geonode.compat import ensure_string",
            "from geonode.layers.enumerations import GXP_PTYPES",
            "from geonode.storage.manager import storage_manager",
            "from geonode.services.serviceprocessors import get_available_service_types",
            "from geonode.base.auth import (",
            "    extend_token,",
            "    get_or_create_token,",
            "    get_token_from_auth_header,",
            "    get_token_object_from_session,",
            ")",
            "",
            "from urllib.parse import (",
            "    urljoin,",
            "    unquote,",
            "    urlparse,",
            "    urlsplit,",
            "    urlencode,",
            "    parse_qsl,",
            "    ParseResult,",
            ")",
            "",
            "MAX_EXTENT = 20037508.34",
            "FULL_ROTATION_DEG = 360.0",
            "HALF_ROTATION_DEG = 180.0",
            "DEFAULT_TITLE = \"\"",
            "DEFAULT_ABSTRACT = \"\"",
            "",
            "INVALID_PERMISSION_MESSAGE = _(\"Invalid permission level.\")",
            "",
            "ALPHABET = f\"{string.ascii_uppercase + string.ascii_lowercase + string.digits}-_\"",
            "ALPHABET_REVERSE = {c: i for (i, c) in enumerate(ALPHABET)}",
            "BASE = len(ALPHABET)",
            "SIGN_CHARACTER = \"$\"",
            "SQL_PARAMS_RE = re.compile(r\"%\\(([\\w_\\-]+)\\)s\")",
            "",
            "FORWARDED_HEADERS = [\"content-type\", \"content-disposition\"]",
            "",
            "# explicitly disable resolving XML entities in order to prevent malicious attacks",
            "XML_PARSER: typing.Final = etree.XMLParser(resolve_entities=False)",
            "",
            "requests.packages.urllib3.disable_warnings()",
            "",
            "signalnames = [",
            "    \"class_prepared\",",
            "    \"m2m_changed\",",
            "    \"post_delete\",",
            "    \"post_init\",",
            "    \"post_save\",",
            "    \"post_syncdb\",",
            "    \"pre_delete\",",
            "    \"pre_init\",",
            "    \"pre_save\",",
            "]",
            "signals_store = {}",
            "",
            "id_none = id(None)",
            "",
            "logger = logging.getLogger(\"geonode.utils\")",
            "",
            "",
            "class ServerDoesNotExist(Exception):",
            "    pass",
            "",
            "",
            "class OGC_Server(object):  # LGTM: @property will not work in old-style classes",
            "",
            "    \"\"\"",
            "    OGC Server object.",
            "    \"\"\"",
            "",
            "    def __init__(self, ogc_server, alias):",
            "        self.alias = alias",
            "        self.server = ogc_server",
            "",
            "    def __getattr__(self, item):",
            "        return self.server.get(item)",
            "",
            "    @property",
            "    def credentials(self):",
            "        \"\"\"",
            "        Returns a tuple of the server's credentials.",
            "        \"\"\"",
            "        creds = namedtuple(\"OGC_SERVER_CREDENTIALS\", [\"username\", \"password\"])",
            "        return creds(username=self.USER, password=self.PASSWORD)",
            "",
            "    @property",
            "    def datastore_db(self):",
            "        \"\"\"",
            "        Returns the server's datastore dict or None.",
            "        \"\"\"",
            "        if self.DATASTORE and settings.DATABASES.get(self.DATASTORE, None):",
            "            datastore_dict = settings.DATABASES.get(self.DATASTORE, dict())",
            "            return datastore_dict",
            "        else:",
            "            return dict()",
            "",
            "    @property",
            "    def ows(self):",
            "        \"\"\"",
            "        The Open Web Service url for the server.",
            "        \"\"\"",
            "        location = self.PUBLIC_LOCATION if self.PUBLIC_LOCATION else self.LOCATION",
            "        return self.OWS_LOCATION if self.OWS_LOCATION else urljoin(location, \"ows\")",
            "",
            "    @property",
            "    def rest(self):",
            "        \"\"\"",
            "        The REST endpoint for the server.",
            "        \"\"\"",
            "        return urljoin(self.LOCATION, \"rest\") if not self.REST_LOCATION else self.REST_LOCATION",
            "",
            "    @property",
            "    def public_url(self):",
            "        \"\"\"",
            "        The global public endpoint for the server.",
            "        \"\"\"",
            "        return self.LOCATION if not self.PUBLIC_LOCATION else self.PUBLIC_LOCATION",
            "",
            "    @property",
            "    def internal_ows(self):",
            "        \"\"\"",
            "        The Open Web Service url for the server used by GeoNode internally.",
            "        \"\"\"",
            "        location = self.LOCATION",
            "        return urljoin(location, \"ows\")",
            "",
            "    @property",
            "    def hostname(self):",
            "        return urlsplit(self.LOCATION).hostname",
            "",
            "    @property",
            "    def netloc(self):",
            "        return urlsplit(self.LOCATION).netloc",
            "",
            "    def __str__(self):",
            "        return str(self.alias)",
            "",
            "",
            "class OGC_Servers_Handler:",
            "",
            "    \"\"\"",
            "    OGC Server Settings Convenience dict.",
            "    \"\"\"",
            "",
            "    def __init__(self, ogc_server_dict):",
            "        self.servers = ogc_server_dict",
            "        # FIXME(Ariel): Are there better ways to do this without involving",
            "        # local?",
            "        self._servers = local()",
            "",
            "    def ensure_valid_configuration(self, alias):",
            "        \"\"\"",
            "        Ensures the settings are valid.",
            "        \"\"\"",
            "        try:",
            "            server = self.servers[alias]",
            "        except KeyError:",
            "            raise ServerDoesNotExist(f\"The server {alias} doesn't exist\")",
            "",
            "        if \"PRINTNG_ENABLED\" in server:",
            "            raise ImproperlyConfigured(\"The PRINTNG_ENABLED setting has been removed, use 'PRINT_NG_ENABLED' instead.\")",
            "",
            "    def ensure_defaults(self, alias):",
            "        \"\"\"",
            "        Puts the defaults into the settings dictionary for a given connection where no settings is provided.",
            "        \"\"\"",
            "        try:",
            "            server = self.servers[alias]",
            "        except KeyError:",
            "            raise ServerDoesNotExist(f\"The server {alias} doesn't exist\")",
            "",
            "        server.setdefault(\"BACKEND\", \"geonode.geoserver\")",
            "        server.setdefault(\"LOCATION\", \"http://localhost:8080/geoserver/\")",
            "        server.setdefault(\"USER\", \"admin\")",
            "        server.setdefault(\"PASSWORD\", \"geoserver\")",
            "        server.setdefault(\"DATASTORE\", \"\")",
            "",
            "        for option in [",
            "            \"MAPFISH_PRINT_ENABLED\",",
            "            \"PRINT_NG_ENABLED\",",
            "            \"GEONODE_SECURITY_ENABLED\",",
            "            \"GEOFENCE_SECURITY_ENABLED\",",
            "            \"BACKEND_WRITE_ENABLED\",",
            "        ]:",
            "            server.setdefault(option, True)",
            "",
            "        for option in [\"WMST_ENABLED\", \"WPS_ENABLED\"]:",
            "            server.setdefault(option, False)",
            "",
            "        for option in [\"TIMEOUT\", \"GEOFENCE_TIMEOUT\"]:",
            "            server.setdefault(option, 60)",
            "",
            "    def __getitem__(self, alias):",
            "        if hasattr(self._servers, alias):",
            "            return getattr(self._servers, alias)",
            "",
            "        self.ensure_defaults(alias)",
            "        self.ensure_valid_configuration(alias)",
            "        server = self.servers[alias]",
            "        server = OGC_Server(alias=alias, ogc_server=server)",
            "        setattr(self._servers, alias, server)",
            "        return server",
            "",
            "    def __setitem__(self, key, value):",
            "        setattr(self._servers, key, value)",
            "",
            "    def __iter__(self):",
            "        return iter(self.servers)",
            "",
            "    def all(self):",
            "        return [self[alias] for alias in self]",
            "",
            "",
            "def mkdtemp(dir=settings.MEDIA_ROOT):",
            "    if not os.path.exists(dir):",
            "        os.makedirs(dir, exist_ok=True)",
            "    tempdir = None",
            "    while not tempdir:",
            "        try:",
            "            tempdir = tempfile.mkdtemp(dir=dir)",
            "            if os.path.exists(tempdir) and os.path.isdir(tempdir):",
            "                if os.listdir(tempdir):",
            "                    raise Exception(\"Directory is not empty\")",
            "            else:",
            "                raise Exception(\"Directory does not exist or is not accessible\")",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            tempdir = None",
            "    return tempdir",
            "",
            "",
            "def unzip_file(upload_file, extension=\".shp\", tempdir=None):",
            "    \"\"\"",
            "    Unzips a zipfile into a temporary directory and returns the full path of the .shp file inside (if any)",
            "    \"\"\"",
            "    absolute_base_file = None",
            "    if tempdir is None:",
            "        tempdir = mkdtemp()",
            "",
            "    the_zip = ZipFile(upload_file, allowZip64=True)",
            "    the_zip.extractall(tempdir)",
            "    for item in the_zip.namelist():",
            "        if item.endswith(extension):",
            "            absolute_base_file = os.path.join(tempdir, item)",
            "",
            "    return absolute_base_file",
            "",
            "",
            "def extract_tarfile(upload_file, extension=\".shp\", tempdir=None):",
            "    \"\"\"",
            "    Extracts a tarfile into a temporary directory and returns the full path of the .shp file inside (if any)",
            "    \"\"\"",
            "    absolute_base_file = None",
            "    if tempdir is None:",
            "        tempdir = mkdtemp()",
            "",
            "    the_tar = tarfile.open(upload_file)",
            "    the_tar.extractall(tempdir)",
            "    for item in the_tar.getnames():",
            "        if item.endswith(extension):",
            "            absolute_base_file = os.path.join(tempdir, item)",
            "",
            "    return absolute_base_file",
            "",
            "",
            "def get_dataset_name(dataset):",
            "    \"\"\"Get the workspace where the input layer belongs\"\"\"",
            "    _name = dataset.name",
            "    if _name and \":\" in _name:",
            "        _name = _name.split(\":\")[1]",
            "    try:",
            "        if not _name and dataset.alternate:",
            "            if \":\" in dataset.alternate:",
            "                _name = dataset.alternate.split(\":\")[1]",
            "            else:",
            "                _name = dataset.alternate",
            "    except Exception:",
            "        pass",
            "    return _name",
            "",
            "",
            "def get_dataset_workspace(dataset):",
            "    \"\"\"Get the workspace where the input layer belongs\"\"\"",
            "    try:",
            "        alternate = dataset.alternate",
            "    except Exception:",
            "        alternate = dataset.name",
            "    try:",
            "        workspace = dataset.workspace",
            "    except Exception:",
            "        workspace = None",
            "    if not workspace and alternate and \":\" in alternate:",
            "        workspace = alternate.split(\":\")[0]",
            "    if not workspace:",
            "        default_workspace = getattr(settings, \"DEFAULT_WORKSPACE\", \"geonode\")",
            "        try:",
            "            from geonode.services.enumerations import CASCADED",
            "",
            "            if dataset.remote_service.method == CASCADED:",
            "                workspace = getattr(settings, \"CASCADE_WORKSPACE\", default_workspace)",
            "            else:",
            "                raise RuntimeError(\"Dataset is not cascaded\")",
            "        except Exception:  # layer does not have a service",
            "            workspace = default_workspace",
            "    return workspace",
            "",
            "",
            "def get_headers(request, url, raw_url, allowed_hosts=[]):",
            "    cookies = None",
            "    csrftoken = None",
            "    headers = {}",
            "",
            "    for _header_key, _header_value in dict(request.headers.copy()).items():",
            "        if _header_key.lower() in FORWARDED_HEADERS:",
            "            headers[_header_key] = _header_value",
            "    if settings.SESSION_COOKIE_NAME in request.COOKIES and is_safe_url(url=raw_url, allowed_hosts=url.hostname):",
            "        cookies = request.META[\"HTTP_COOKIE\"]",
            "",
            "    for cook in request.COOKIES:",
            "        name = str(cook)",
            "        value = request.COOKIES.get(name)",
            "        if name == \"csrftoken\":",
            "            csrftoken = value",
            "        cook = f\"{name}={value}\"",
            "        cookies = cook if not cookies else (f\"{cookies}; {cook}\")",
            "",
            "    csrftoken = get_token(request) if not csrftoken else csrftoken",
            "",
            "    if csrftoken:",
            "        headers[\"X-Requested-With\"] = \"XMLHttpRequest\"",
            "        headers[\"X-CSRFToken\"] = csrftoken",
            "        cook = f\"csrftoken={csrftoken}\"",
            "        cookies = cook if not cookies else (f\"{cookies}; {cook}\")",
            "",
            "    if cookies and request and hasattr(request, \"session\"):",
            "        if \"JSESSIONID\" in request.session and request.session[\"JSESSIONID\"]:",
            "            cookies = f\"{cookies}; JSESSIONID={request.session['JSESSIONID']}\"",
            "        headers[\"Cookie\"] = cookies",
            "",
            "    if request.method in (\"POST\", \"PUT\") and \"CONTENT_TYPE\" in request.META:",
            "        headers[\"Content-Type\"] = request.META[\"CONTENT_TYPE\"]",
            "",
            "    access_token = None",
            "    site_url = urlsplit(settings.SITEURL)",
            "    # We want to convert HTTP_AUTH into a Beraer Token only when hitting the local GeoServer",
            "    if site_url.hostname in (allowed_hosts + [url.hostname]):",
            "        # we give precedence to obtained from Aithorization headers",
            "        if \"HTTP_AUTHORIZATION\" in request.META:",
            "            auth_header = request.META.get(\"HTTP_AUTHORIZATION\", request.META.get(\"HTTP_AUTHORIZATION2\"))",
            "            if auth_header:",
            "                headers[\"Authorization\"] = auth_header",
            "                access_token = get_token_from_auth_header(auth_header, create_if_not_exists=True)",
            "        # otherwise we check if a session is active",
            "        elif request and request.user.is_authenticated:",
            "            access_token = get_token_object_from_session(request.session)",
            "",
            "            # we extend the token in case the session is active but the token expired",
            "            if access_token and access_token.is_expired():",
            "                extend_token(access_token)",
            "            else:",
            "                access_token = get_or_create_token(request.user)",
            "",
            "    if access_token:",
            "        headers[\"Authorization\"] = f\"Bearer {access_token}\"",
            "",
            "    pragma = \"no-cache\"",
            "    referer = (",
            "        request.META[\"HTTP_REFERER\"] if \"HTTP_REFERER\" in request.META else f\"{site_url.scheme}://{site_url.netloc}/\"",
            "    )",
            "    encoding = request.META[\"HTTP_ACCEPT_ENCODING\"] if \"HTTP_ACCEPT_ENCODING\" in request.META else \"gzip\"",
            "    headers.update(",
            "        {",
            "            \"Pragma\": pragma,",
            "            \"Referer\": referer,",
            "            \"Accept-encoding\": encoding,",
            "        }",
            "    )",
            "",
            "    return (headers, access_token)",
            "",
            "",
            "def _get_basic_auth_info(request):",
            "    \"\"\"",
            "    grab basic auth info",
            "    \"\"\"",
            "    meth, auth = request.META[\"HTTP_AUTHORIZATION\"].split()",
            "    if meth.lower() != \"basic\":",
            "        raise ValueError",
            "    username, password = base64.b64decode(auth.encode()).decode().split(\":\")",
            "    return username, password",
            "",
            "",
            "def batch_delete(request):",
            "    # TODO",
            "    pass",
            "",
            "",
            "def _split_query(query):",
            "    \"\"\"",
            "    split and strip keywords, preserve space",
            "    separated quoted blocks.",
            "    \"\"\"",
            "",
            "    qq = query.split(\" \")",
            "    keywords = []",
            "    accum = None",
            "    for kw in qq:",
            "        if accum is None:",
            "            if kw.startswith('\"'):",
            "                accum = kw[1:]",
            "            elif kw:",
            "                keywords.append(kw)",
            "        else:",
            "            accum += f\" {kw}\"",
            "            if kw.endswith('\"'):",
            "                keywords.append(accum[0:-1])",
            "                accum = None",
            "    if accum is not None:",
            "        keywords.append(accum)",
            "    return [kw.strip() for kw in keywords if kw.strip()]",
            "",
            "",
            "def bbox_to_wkt(x0, x1, y0, y1, srid=\"4326\", include_srid=True):",
            "    if srid and str(srid).startswith(\"EPSG:\"):",
            "        srid = srid[5:]",
            "    if None not in {x0, x1, y0, y1}:",
            "        polys = []",
            "",
            "        # We assume that if x1 is smaller then x0 we're crossing the date line",
            "        crossing_idl = x1 < x0",
            "        if crossing_idl:",
            "            polys.append(",
            "                [",
            "                    (float(x0), float(y0)),",
            "                    (float(x0), float(y1)),",
            "                    (180.0, float(y1)),",
            "                    (180.0, float(y0)),",
            "                    (float(x0), float(y0)),",
            "                ]",
            "            )",
            "            polys.append(",
            "                [",
            "                    (-180.0, float(y0)),",
            "                    (-180.0, float(y1)),",
            "                    (float(x1), float(y1)),",
            "                    (float(x1), float(y0)),",
            "                    (-180.0, float(y0)),",
            "                ]",
            "            )",
            "        else:",
            "            polys.append(",
            "                [",
            "                    (float(x0), float(y0)),",
            "                    (float(x0), float(y1)),",
            "                    (float(x1), float(y1)),",
            "                    (float(x1), float(y0)),",
            "                    (float(x0), float(y0)),",
            "                ]",
            "            )",
            "",
            "        poly_wkts = \",\".join(",
            "            [\"(({}))\".format(\",\".join([\"{:f} {:f}\".format(coords[0], coords[1]) for coords in poly])) for poly in polys]",
            "        )",
            "        wkt = f\"MULTIPOLYGON({poly_wkts})\" if len(polys) > 1 else f\"POLYGON{poly_wkts}\"",
            "        if include_srid:",
            "            wkt = f\"SRID={srid};{wkt}\"",
            "    else:",
            "        wkt = \"POLYGON((-180 -90,-180 90,180 90,180 -90,-180 -90))\"",
            "        if include_srid:",
            "            wkt = f\"SRID=4326;{wkt}\"",
            "    return wkt",
            "",
            "",
            "def _v(coord, x, source_srid=4326, target_srid=3857):",
            "    if source_srid == 4326 and x and abs(coord) != HALF_ROTATION_DEG:",
            "        coord -= round(coord / FULL_ROTATION_DEG) * FULL_ROTATION_DEG",
            "    if source_srid == 4326 and target_srid != 4326:",
            "        if x and float(coord) >= 179.999:",
            "            return 179.999",
            "        elif x and float(coord) <= -179.999:",
            "            return -179.999",
            "",
            "        if not x and float(coord) >= 89.999:",
            "            return 89.999",
            "        elif not x and float(coord) <= -89.999:",
            "            return -89.999",
            "    return coord",
            "",
            "",
            "def bbox_to_projection(native_bbox, target_srid=4326):",
            "    \"\"\"",
            "    native_bbox must be in the form",
            "        ('-81.3962935', '-81.3490249', '13.3202891', '13.3859614', 'EPSG:4326')",
            "    \"\"\"",
            "    box = native_bbox[:4]",
            "    proj = native_bbox[-1]",
            "    minx, maxx, miny, maxy = [float(a) for a in box]",
            "    try:",
            "        source_srid = int(proj.split(\":\")[1]) if proj and \":\" in proj else int(proj)",
            "    except Exception:",
            "        source_srid = target_srid",
            "",
            "    if source_srid != target_srid:",
            "        wkt = bbox_to_wkt(",
            "            _v(minx, x=True, source_srid=source_srid, target_srid=target_srid),",
            "            _v(maxx, x=True, source_srid=source_srid, target_srid=target_srid),",
            "            _v(miny, x=False, source_srid=source_srid, target_srid=target_srid),",
            "            _v(maxy, x=False, source_srid=source_srid, target_srid=target_srid),",
            "            srid=source_srid,",
            "            include_srid=False,",
            "        )",
            "        # AF: This causses error with GDAL 3.0.4 due to a breaking change on GDAL",
            "        #     https://code.djangoproject.com/ticket/30645",
            "        import osgeo.gdal",
            "",
            "        _gdal_ver = osgeo.gdal.__version__.split(\".\", 2)",
            "        from osgeo import ogr",
            "        from osgeo.osr import SpatialReference, CoordinateTransformation",
            "",
            "        g = ogr.Geometry(wkt=wkt)",
            "        source = SpatialReference()",
            "        source.ImportFromEPSG(source_srid)",
            "        dest = SpatialReference()",
            "        dest.ImportFromEPSG(target_srid)",
            "        if int(_gdal_ver[0]) >= 3 and ((int(_gdal_ver[1]) == 0 and int(_gdal_ver[2]) >= 4) or int(_gdal_ver[1]) > 0):",
            "            source.SetAxisMappingStrategy(0)",
            "            dest.SetAxisMappingStrategy(0)",
            "        g.Transform(CoordinateTransformation(source, dest))",
            "        projected_bbox = [str(x) for x in g.GetEnvelope()]",
            "        # Must be in the form : [x0, x1, y0, y1, EPSG:<target_srid>)",
            "        return tuple(",
            "            [float(projected_bbox[0]), float(projected_bbox[1]), float(projected_bbox[2]), float(projected_bbox[3])]",
            "        ) + (f\"EPSG:{target_srid}\",)",
            "",
            "    return native_bbox",
            "",
            "",
            "def bounds_to_zoom_level(bounds, width, height):",
            "    WORLD_DIM = {\"height\": 256.0, \"width\": 256.0}",
            "    ZOOM_MAX = 21",
            "",
            "    def latRad(lat):",
            "        _sin = sin(lat * pi / HALF_ROTATION_DEG)",
            "        if abs(_sin) != 1.0:",
            "            radX2 = log((1.0 + _sin) / (1.0 - _sin)) / 2.0",
            "        else:",
            "            radX2 = log(1.0) / 2.0",
            "        return max(min(radX2, pi), -pi) / 2.0",
            "",
            "    def zoom(mapPx, worldPx, fraction):",
            "        try:",
            "            return floor(log(mapPx / worldPx / fraction) / log(2.0))",
            "        except Exception:",
            "            return 0",
            "",
            "    ne = [float(bounds[2]), float(bounds[3])]",
            "    sw = [float(bounds[0]), float(bounds[1])]",
            "    latFraction = (latRad(ne[1]) - latRad(sw[1])) / pi",
            "    lngDiff = ne[0] - sw[0]",
            "    lngFraction = ((lngDiff + FULL_ROTATION_DEG) if lngDiff < 0 else lngDiff) / FULL_ROTATION_DEG",
            "    latZoom = zoom(float(height), WORLD_DIM[\"height\"], latFraction)",
            "    lngZoom = zoom(float(width), WORLD_DIM[\"width\"], lngFraction)",
            "    # ratio = float(max(width, height)) / float(min(width, height))",
            "    # z_offset = 0 if ratio >= 2 else -1",
            "    z_offset = 0",
            "    zoom = int(max(latZoom, lngZoom) + z_offset)",
            "    zoom = 0 if zoom > ZOOM_MAX else zoom",
            "    return max(zoom, 0)",
            "",
            "",
            "def llbbox_to_mercator(llbbox):",
            "    minlonlat = forward_mercator([llbbox[0], llbbox[2]])",
            "    maxlonlat = forward_mercator([llbbox[1], llbbox[3]])",
            "    return [minlonlat[0], minlonlat[1], maxlonlat[0], maxlonlat[1]]",
            "",
            "",
            "def mercator_to_llbbox(bbox):",
            "    minlonlat = inverse_mercator([bbox[0], bbox[2]])",
            "    maxlonlat = inverse_mercator([bbox[1], bbox[3]])",
            "    return [minlonlat[0], minlonlat[1], maxlonlat[0], maxlonlat[1]]",
            "",
            "",
            "def forward_mercator(lonlat):",
            "    \"\"\"",
            "    Given geographic coordinates, return a x,y tuple in spherical mercator.",
            "",
            "    If the lat value is out of range, -inf will be returned as the y value",
            "    \"\"\"",
            "    x = lonlat[0] * MAX_EXTENT / HALF_ROTATION_DEG",
            "    try:",
            "        # With data sets that only have one point the value of this",
            "        # expression becomes negative infinity. In order to continue,",
            "        # we wrap this in a try catch block.",
            "        n = tan((90 + lonlat[1]) * pi / FULL_ROTATION_DEG)",
            "    except ValueError:",
            "        n = 0",
            "    if n <= 0:",
            "        y = float(\"-inf\")",
            "    else:",
            "        y = log(n) / pi * MAX_EXTENT",
            "    return (x, y)",
            "",
            "",
            "def inverse_mercator(xy):",
            "    \"\"\"",
            "    Given coordinates in spherical mercator, return a lon,lat tuple.",
            "    \"\"\"",
            "    lon = (xy[0] / MAX_EXTENT) * HALF_ROTATION_DEG",
            "    lat = (xy[1] / MAX_EXTENT) * HALF_ROTATION_DEG",
            "    lat = HALF_ROTATION_DEG / pi * (2 * atan(exp(lat * pi / HALF_ROTATION_DEG)) - pi / 2)",
            "    return (lon, lat)",
            "",
            "",
            "def resolve_object(",
            "    request, model, query, permission=\"base.view_resourcebase\", user=None, permission_required=True, permission_msg=None",
            "):",
            "    \"\"\"Resolve an object using the provided query and check the optional",
            "    permission. Model views should wrap this function as a shortcut.",
            "",
            "    query - a dict to use for querying the model",
            "    permission - an optional permission to check",
            "    permission_required - if False, allow get methods to proceed",
            "    permission_msg - optional message to use in 403",
            "    \"\"\"",
            "    user = request.user if request and request.user else user",
            "    obj = get_object_or_404(model, **query)",
            "    obj_to_check = obj.get_self_resource()",
            "",
            "    from guardian.shortcuts import get_groups_with_perms",
            "    from geonode.groups.models import GroupProfile",
            "",
            "    groups = get_groups_with_perms(obj_to_check, attach_perms=True)",
            "",
            "    if obj_to_check.group and obj_to_check.group not in groups:",
            "        groups[obj_to_check.group] = obj_to_check.group",
            "",
            "    obj_group_managers = []",
            "    obj_group_members = []",
            "    if groups:",
            "        for group in groups:",
            "            try:",
            "                group_profile = GroupProfile.objects.get(slug=group.name)",
            "                managers = group_profile.get_managers()",
            "                if managers:",
            "                    for manager in managers:",
            "                        if manager not in obj_group_managers and not manager.is_superuser:",
            "                            obj_group_managers.append(manager)",
            "                if group_profile.user_is_member(user) and user not in obj_group_members:",
            "                    obj_group_members.append(user)",
            "            except GroupProfile.DoesNotExist:",
            "                pass",
            "",
            "    allowed = True",
            "    if permission.split(\".\")[-1] in [\"change_dataset_data\", \"change_dataset_style\"]:",
            "        if obj.__class__.__name__ == \"Dataset\":",
            "            obj_to_check = obj",
            "    if permission:",
            "        if permission_required or request.method != \"GET\":",
            "            if user in obj_group_managers:",
            "                allowed = True",
            "            else:",
            "                allowed = user.has_perm(permission, obj_to_check)",
            "    if not allowed:",
            "        mesg = permission_msg or _(\"Permission Denied\")",
            "        raise PermissionDenied(mesg)",
            "    return obj",
            "",
            "",
            "def json_response(body=None, errors=None, url=None, redirect_to=None, exception=None, content_type=None, status=None):",
            "    \"\"\"Create a proper JSON response. If body is provided, this is the response.",
            "    If errors is not None, the response is a success/errors json object.",
            "    If redirect_to is not None, the response is a success=True, redirect_to object",
            "    If the exception is provided, it will be logged. If body is a string, the",
            "    exception message will be used as a format option to that string and the",
            "    result will be a success=False, errors = body % exception",
            "    \"\"\"",
            "    if isinstance(body, HttpResponse):",
            "        return body",
            "    if content_type is None:",
            "        content_type = \"application/json\"",
            "    if errors:",
            "        if isinstance(errors, str):",
            "            errors = [errors]",
            "        body = {\"success\": False, \"errors\": errors}",
            "    elif redirect_to:",
            "        body = {\"success\": True, \"redirect_to\": redirect_to}",
            "    elif url:",
            "        body = {\"success\": True, \"url\": url}",
            "    elif exception:",
            "        if isinstance(exception, APIException):",
            "            raise exception",
            "        if body is None:",
            "            body = f\"Unexpected exception {exception}\"",
            "        else:",
            "            body = body % exception",
            "        body = {\"success\": False, \"errors\": [body]}",
            "        raise GeneralUploadException(detail=body)",
            "    elif body:",
            "        pass",
            "    else:",
            "        raise Exception(\"must call with body, errors or redirect_to\")",
            "",
            "    if status is None:",
            "        status = 200",
            "",
            "    if not isinstance(body, str):",
            "        try:",
            "            body = json.dumps(body, cls=DjangoJSONEncoder)",
            "        except Exception:",
            "            body = str(body)",
            "    return HttpResponse(body, content_type=content_type, status=status)",
            "",
            "",
            "def num_encode(n):",
            "    if n < 0:",
            "        return SIGN_CHARACTER + num_encode(-n)",
            "    s = []",
            "    while True:",
            "        n, r = divmod(n, BASE)",
            "        s.append(ALPHABET[r])",
            "        if n == 0:",
            "            break",
            "    return \"\".join(reversed(s))",
            "",
            "",
            "def num_decode(s):",
            "    if s[0] == SIGN_CHARACTER:",
            "        return -num_decode(s[1:])",
            "    n = 0",
            "    for c in s:",
            "        n = n * BASE + ALPHABET_REVERSE[c]",
            "    return n",
            "",
            "",
            "def format_urls(a, values):",
            "    b = []",
            "    for i in a:",
            "        j = i.copy()",
            "        try:",
            "            j[\"url\"] = str(j[\"url\"]).format(**values)",
            "        except KeyError:",
            "            j[\"url\"] = None",
            "        b.append(j)",
            "    return b",
            "",
            "",
            "def build_abstract(resourcebase, url=None, includeURL=True):",
            "    if resourcebase.abstract and url and includeURL:",
            "        return f\"{resourcebase.abstract} -- [{url}]({url})\"",
            "    else:",
            "        return resourcebase.abstract",
            "",
            "",
            "def build_caveats(resourcebase):",
            "    caveats = []",
            "    if resourcebase.maintenance_frequency:",
            "        caveats.append(resourcebase.maintenance_frequency_title())",
            "    if resourcebase.license:",
            "        caveats.append(resourcebase.license_verbose)",
            "    if resourcebase.data_quality_statement:",
            "        caveats.append(resourcebase.data_quality_statement)",
            "    if len(caveats) > 0:",
            "        return f\"- {'%0A- '.join(caveats)}\"",
            "    else:",
            "        return \"\"",
            "",
            "",
            "def build_social_links(request, resourcebase):",
            "    netschema = \"https\" if request.is_secure() else \"http\"",
            "    host = request.get_host()",
            "    path = request.get_full_path()",
            "    social_url = f\"{netschema}://{host}{path}\"",
            "    # Don't use datetime strftime() because it requires year >= 1900",
            "    # see",
            "    # https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior",
            "    date = \"{0.month:02d}/{0.day:02d}/{0.year:4d}\".format(resourcebase.date) if resourcebase.date else None",
            "    abstract = build_abstract(resourcebase, url=social_url, includeURL=True)",
            "    caveats = build_caveats(resourcebase)",
            "    hashtags = \",\".join(getattr(settings, \"TWITTER_HASHTAGS\", []))",
            "    return format_urls(",
            "        settings.SOCIAL_ORIGINS,",
            "        {",
            "            \"name\": resourcebase.title,",
            "            \"date\": date,",
            "            \"abstract\": abstract,",
            "            \"caveats\": caveats,",
            "            \"hashtags\": hashtags,",
            "            \"url\": social_url,",
            "        },",
            "    )",
            "",
            "",
            "def check_shp_columnnames(layer):",
            "    \"\"\"Check if shapefile for a given layer has valid column names.",
            "    If not, try to fix column names and warn the user",
            "    \"\"\"",
            "    # TODO we may add in a better location this method",
            "    inShapefile = \"\"",
            "    for f in layer.upload_session.layerfile_set.all():",
            "        if os.path.splitext(f.file.name)[1] == \".shp\":",
            "            inShapefile = f.file.path",
            "    if inShapefile:",
            "        return fixup_shp_columnnames(inShapefile, layer.charset)",
            "",
            "",
            "def clone_shp_field_defn(srcFieldDefn, name):",
            "    \"\"\"",
            "    Clone an existing ogr.FieldDefn with a new name",
            "    \"\"\"",
            "    dstFieldDefn = ogr.FieldDefn(name, srcFieldDefn.GetType())",
            "    dstFieldDefn.SetWidth(srcFieldDefn.GetWidth())",
            "    dstFieldDefn.SetPrecision(srcFieldDefn.GetPrecision())",
            "",
            "    return dstFieldDefn",
            "",
            "",
            "def rename_shp_columnnames(inLayer, fieldnames):",
            "    \"\"\"",
            "    Rename columns in a layer to those specified in the given mapping",
            "    \"\"\"",
            "    inLayerDefn = inLayer.GetLayerDefn()",
            "",
            "    for i in range(inLayerDefn.GetFieldCount()):",
            "        srcFieldDefn = inLayerDefn.GetFieldDefn(i)",
            "        dstFieldName = fieldnames.get(srcFieldDefn.GetName())",
            "",
            "        if dstFieldName is not None:",
            "            dstFieldDefn = clone_shp_field_defn(srcFieldDefn, dstFieldName)",
            "            inLayer.AlterFieldDefn(i, dstFieldDefn, ogr.ALTER_NAME_FLAG)",
            "",
            "",
            "def fixup_shp_columnnames(inShapefile, charset, tempdir=None):",
            "    \"\"\"Try to fix column names and warn the user\"\"\"",
            "    charset = charset if charset and \"undefined\" not in charset else \"UTF-8\"",
            "    if not tempdir:",
            "        tempdir = mkdtemp()",
            "",
            "    if is_zipfile(inShapefile):",
            "        inShapefile = unzip_file(inShapefile, \".shp\", tempdir=tempdir)",
            "",
            "    inDriver = ogr.GetDriverByName(\"ESRI Shapefile\")",
            "    try:",
            "        inDataSource = inDriver.Open(inShapefile, 1)",
            "    except Exception:",
            "        tb = traceback.format_exc()",
            "        logger.debug(tb)",
            "        inDataSource = None",
            "",
            "    if inDataSource is None:",
            "        logger.debug(f\"Could not open {inShapefile}\")",
            "        return False, None, None",
            "    else:",
            "        inLayer = inDataSource.GetLayer()",
            "",
            "    # TODO we may need to improve this regexp",
            "    # first character must be any letter or \"_\"",
            "    # following characters can be any letter, number, \"#\", \":\"",
            "    regex = r\"^[a-zA-Z,_][a-zA-Z,_#:\\d]*$\"",
            "    a = re.compile(regex)",
            "    regex_first_char = r\"[a-zA-Z,_]{1}\"",
            "    b = re.compile(regex_first_char)",
            "    inLayerDefn = inLayer.GetLayerDefn()",
            "",
            "    list_col_original = []",
            "    list_col = {}",
            "",
            "    for i in range(inLayerDefn.GetFieldCount()):",
            "        try:",
            "            field_name = inLayerDefn.GetFieldDefn(i).GetName()",
            "            if a.match(field_name):",
            "                list_col_original.append(field_name)",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            return True, None, None",
            "",
            "    for i in range(inLayerDefn.GetFieldCount()):",
            "        try:",
            "            field_name = inLayerDefn.GetFieldDefn(i).GetName()",
            "            if not a.match(field_name):",
            "                # once the field_name contains Chinese, to use slugify_zh",
            "                if any(\"\\u4e00\" <= ch <= \"\\u9fff\" for ch in field_name):",
            "                    new_field_name = slugify_zh(field_name, separator=\"_\")",
            "                else:",
            "                    new_field_name = slugify(field_name)",
            "                if not b.match(new_field_name):",
            "                    new_field_name = f\"_{new_field_name}\"",
            "                j = 0",
            "                while new_field_name in list_col_original or new_field_name in list_col.values():",
            "                    if j == 0:",
            "                        new_field_name += \"_0\"",
            "                    if new_field_name.endswith(f\"_{str(j)}\"):",
            "                        j += 1",
            "                        new_field_name = f\"{new_field_name[:-2]}_{str(j)}\"",
            "                if field_name != new_field_name:",
            "                    list_col[field_name] = new_field_name",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            return True, None, None",
            "",
            "    if len(list_col) == 0:",
            "        return True, None, None",
            "    else:",
            "        try:",
            "            rename_shp_columnnames(inLayer, list_col)",
            "            inDataSource.SyncToDisk()",
            "            inDataSource.Destroy()",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            raise GeoNodeException(f\"Could not decode SHAPEFILE attributes by using the specified charset '{charset}'.\")",
            "    return True, None, list_col",
            "",
            "",
            "def id_to_obj(id_):",
            "    if id_ == id_none:",
            "        return None",
            "",
            "    for obj in gc.get_objects():",
            "        if id(obj) == id_:",
            "            return obj",
            "    raise Exception(\"Not found\")",
            "",
            "",
            "def printsignals():",
            "    for signalname in signalnames:",
            "        logger.debug(f\"SIGNALNAME: {signalname}\")",
            "        signaltype = getattr(models.signals, signalname)",
            "        signals = signaltype.receivers[:]",
            "        for signal in signals:",
            "            logger.debug(signal)",
            "",
            "",
            "class DisableDjangoSignals:",
            "    \"\"\"",
            "    Python3 class temporarily disabling django signals on model creation.",
            "",
            "    usage:",
            "    with DisableDjangoSignals():",
            "        # do some fancy stuff here",
            "    \"\"\"",
            "",
            "    def __init__(self, disabled_signals=None, skip=False):",
            "        self.skip = skip",
            "        self.stashed_signals = defaultdict(list)",
            "        self.disabled_signals = disabled_signals or [",
            "            signals.pre_init,",
            "            signals.post_init,",
            "            signals.pre_save,",
            "            signals.post_save,",
            "            signals.pre_delete,",
            "            signals.post_delete,",
            "            signals.pre_migrate,",
            "            signals.post_migrate,",
            "            signals.m2m_changed,",
            "        ]",
            "",
            "    def __enter__(self):",
            "        if not self.skip:",
            "            for signal in self.disabled_signals:",
            "                self.disconnect(signal)",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        if not self.skip:",
            "            for signal in list(self.stashed_signals):",
            "                self.reconnect(signal)",
            "",
            "    def disconnect(self, signal):",
            "        self.stashed_signals[signal] = signal.receivers",
            "        signal.receivers = []",
            "",
            "    def reconnect(self, signal):",
            "        signal.receivers = self.stashed_signals.get(signal, [])",
            "        del self.stashed_signals[signal]",
            "",
            "",
            "def run_subprocess(*cmd, **kwargs):",
            "    p = subprocess.Popen(\" \".join(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)",
            "    stdout = StringIO()",
            "    stderr = StringIO()",
            "    buff_size = 1024",
            "    while p.poll() is None:",
            "        inr = [p.stdout.fileno(), p.stderr.fileno()]",
            "        inw = []",
            "        rlist, wlist, xlist = select.select(inr, inw, [])",
            "",
            "        for r in rlist:",
            "            if r == p.stdout.fileno():",
            "                readfrom = p.stdout",
            "                readto = stdout",
            "            else:",
            "                readfrom = p.stderr",
            "                readto = stderr",
            "            readto.write(readfrom.read(buff_size))",
            "",
            "        for w in wlist:",
            "            w.write(\"\")",
            "",
            "    return p.returncode, stdout.getvalue(), stderr.getvalue()",
            "",
            "",
            "def parse_datetime(value):",
            "    for patt in settings.DATETIME_INPUT_FORMATS:",
            "        try:",
            "            if isinstance(value, dict):",
            "                value_obj = value[\"$\"] if \"$\" in value else value[\"content\"]",
            "                return datetime.datetime.strptime(value_obj, patt)",
            "            else:",
            "                return datetime.datetime.strptime(value, patt)",
            "        except Exception:",
            "            tb = traceback.format_exc()",
            "            logger.debug(tb)",
            "    raise ValueError(f\"Invalid datetime input: {value}\")",
            "",
            "",
            "def _convert_sql_params(cur, query):",
            "    # sqlite driver doesn't support %(key)s notation,",
            "    # use :key instead.",
            "    if cur.db.vendor in (",
            "        \"sqlite\",",
            "        \"sqlite3\",",
            "        \"spatialite\",",
            "    ):",
            "        return SQL_PARAMS_RE.sub(r\":\\1\", query)",
            "    return query",
            "",
            "",
            "@transaction.atomic",
            "def raw_sql(query, params=None, ret=True):",
            "    \"\"\"",
            "    Execute raw query",
            "    param ret=True returns data from cursor as iterator",
            "    \"\"\"",
            "    with connection.cursor() as c:",
            "        query = _convert_sql_params(c, query)",
            "        c.execute(query, params)",
            "        if ret:",
            "            desc = [r[0] for r in c.description]",
            "            for row in c:",
            "                yield dict(zip(desc, row))",
            "",
            "",
            "def get_client_ip(request):",
            "    x_forwarded_for = request.META.get(\"HTTP_X_FORWARDED_FOR\")",
            "    if x_forwarded_for:",
            "        ip = x_forwarded_for.split(\",\")[0]",
            "    else:",
            "        ip = request.META.get(\"REMOTE_ADDR\")",
            "    return ip",
            "",
            "",
            "def get_client_host(request):",
            "    hostname = None",
            "    http_host = request.META.get(\"HTTP_HOST\")",
            "    if http_host:",
            "        hostname = http_host.split(\":\")[0]",
            "    return hostname",
            "",
            "",
            "def check_ogc_backend(backend_package):",
            "    \"\"\"Check that geonode use a particular OGC Backend integration",
            "",
            "    :param backend_package: django app of backend to use",
            "    :type backend_package: str",
            "",
            "    :return: bool",
            "    :rtype: bool",
            "    \"\"\"",
            "    ogc_conf = settings.OGC_SERVER[\"default\"]",
            "    is_configured = ogc_conf.get(\"BACKEND\") == backend_package",
            "",
            "    # Check environment variables",
            "    _backend = os.environ.get(\"BACKEND\", None)",
            "    if _backend:",
            "        return backend_package == _backend and is_configured",
            "",
            "    # Check exists in INSTALLED_APPS",
            "    try:",
            "        in_installed_apps = backend_package in settings.INSTALLED_APPS",
            "        return in_installed_apps and is_configured",
            "    except Exception:",
            "        pass",
            "    return False",
            "",
            "",
            "class HttpClient:",
            "    def __init__(self):",
            "        self.timeout = 5",
            "        self.retries = 1",
            "        self.pool_maxsize = 10",
            "        self.backoff_factor = 0.3",
            "        self.pool_connections = 10",
            "        self.status_forcelist = (500, 502, 503, 504)",
            "        self.username = \"admin\"",
            "        self.password = \"admin\"",
            "        if check_ogc_backend(geoserver.BACKEND_PACKAGE):",
            "            ogc_server_settings = settings.OGC_SERVER[\"default\"]",
            "            self.timeout = ogc_server_settings.get(\"TIMEOUT\", 5)",
            "            self.retries = ogc_server_settings.get(\"MAX_RETRIES\", 1)",
            "            self.backoff_factor = ogc_server_settings.get(\"BACKOFF_FACTOR\", 0.3)",
            "            self.pool_maxsize = ogc_server_settings.get(\"POOL_MAXSIZE\", 10)",
            "            self.pool_connections = ogc_server_settings.get(\"POOL_CONNECTIONS\", 10)",
            "            self.username = ogc_server_settings.get(\"USER\", \"admin\")",
            "            self.password = ogc_server_settings.get(\"PASSWORD\", \"geoserver\")",
            "",
            "    def request(",
            "        self,",
            "        url,",
            "        method=\"GET\",",
            "        data=None,",
            "        headers={},",
            "        stream=False,",
            "        timeout=None,",
            "        retries=None,",
            "        user=None,",
            "        verify=False,",
            "    ):",
            "        if (",
            "            (user or self.username != \"admin\")",
            "            and check_ogc_backend(geoserver.BACKEND_PACKAGE)",
            "            and \"Authorization\" not in headers",
            "        ):",
            "            if connection.cursor().db.vendor not in (\"sqlite\", \"sqlite3\", \"spatialite\"):",
            "                try:",
            "                    if user and isinstance(user, str):",
            "                        user = get_user_model().objects.get(username=user)",
            "                    _u = user or get_user_model().objects.get(username=self.username)",
            "                    access_token = get_or_create_token(_u)",
            "                    if access_token and not access_token.is_expired():",
            "                        headers[\"Authorization\"] = f\"Bearer {access_token.token}\"",
            "                except Exception:",
            "                    tb = traceback.format_exc()",
            "                    logger.debug(tb)",
            "            elif user == self.username:",
            "                valid_uname_pw = base64.b64encode(f\"{self.username}:{self.password}\".encode()).decode()",
            "                headers[\"Authorization\"] = f\"Basic {valid_uname_pw}\"",
            "",
            "        headers[\"User-Agent\"] = \"GeoNode\"",
            "        response = None",
            "        content = None",
            "        session = requests.Session()",
            "        retry = Retry(",
            "            total=retries or self.retries,",
            "            read=retries or self.retries,",
            "            connect=retries or self.retries,",
            "            backoff_factor=self.backoff_factor,",
            "            status_forcelist=self.status_forcelist,",
            "        )",
            "        adapter = requests.adapters.HTTPAdapter(",
            "            max_retries=retry, pool_maxsize=self.pool_maxsize, pool_connections=self.pool_connections",
            "        )",
            "        scheme = urlsplit(url).scheme",
            "        session.mount(f\"{scheme}://\", adapter)",
            "        session.verify = False",
            "        action = getattr(session, method.lower(), None)",
            "        if action:",
            "            _req_tout = timeout or self.timeout",
            "            try:",
            "                response = action(url=url, data=data, headers=headers, timeout=_req_tout, stream=stream, verify=verify)",
            "            except (",
            "                requests.exceptions.ConnectTimeout,",
            "                requests.exceptions.RequestException,",
            "                ValueError,",
            "                RetryError,",
            "            ) as e:",
            "                msg = f\"Request exception [{e}] - TOUT [{_req_tout}] to URL: {url} - headers: {headers}\"",
            "                logger.exception(Exception(msg))",
            "                response = None",
            "                content = str(e)",
            "        else:",
            "            response = session.get(url, headers=headers, timeout=self.timeout)",
            "        if response:",
            "            try:",
            "                content = ensure_string(response.content) if not stream else response.raw",
            "            except Exception as e:",
            "                content = str(e)",
            "",
            "        return (response, content)",
            "",
            "    def get(self, url, data=None, headers={}, stream=False, timeout=None, user=None, verify=False):",
            "        return self.request(",
            "            url,",
            "            method=\"GET\",",
            "            data=data,",
            "            headers=headers,",
            "            timeout=timeout or self.timeout,",
            "            stream=stream,",
            "            user=user,",
            "            verify=verify,",
            "        )",
            "",
            "    def post(self, url, data=None, headers={}, stream=False, timeout=None, user=None, verify=False):",
            "        return self.request(",
            "            url,",
            "            method=\"POST\",",
            "            data=data,",
            "            headers=headers,",
            "            timeout=timeout or self.timeout,",
            "            stream=stream,",
            "            user=user,",
            "            verify=verify,",
            "        )",
            "",
            "",
            "http_client = HttpClient()",
            "",
            "",
            "def get_dir_time_suffix():",
            "    \"\"\"Returns the name of a folder with the 'now' time as suffix\"\"\"",
            "    dirfmt = \"%4d-%02d-%02d_%02d%02d%02d\"",
            "    now = time.localtime()[0:6]",
            "    dirname = dirfmt % now",
            "",
            "    return dirname",
            "",
            "",
            "def zip_dir(basedir, archivename):",
            "    assert os.path.isdir(basedir)",
            "    with closing(ZipFile(archivename, \"w\", ZIP_DEFLATED, allowZip64=True)) as z:",
            "        for root, dirs, files in os.walk(basedir):",
            "            # NOTE: ignore empty directories",
            "            for fn in files:",
            "                absfn = os.path.join(root, fn)",
            "                zfn = absfn[len(basedir) + len(os.sep) :]  # XXX: relative path",
            "                z.write(absfn, zfn)",
            "",
            "",
            "def copy_tree(src, dst, symlinks=False, ignore=None):",
            "    try:",
            "        for item in os.listdir(src):",
            "            s = os.path.join(src, item)",
            "            d = os.path.join(dst, item)",
            "            if os.path.isdir(s):",
            "                if os.path.exists(d):",
            "                    try:",
            "                        os.remove(d)",
            "                    except Exception:",
            "                        shutil.rmtree(d, ignore_errors=True)",
            "                try:",
            "                    shutil.copytree(s, d, symlinks=symlinks, ignore=ignore)",
            "                except Exception:",
            "                    pass",
            "            else:",
            "                try:",
            "                    if ignore and s in ignore(dst, [s]):",
            "                        return",
            "                    shutil.copy2(s, d)",
            "                except Exception:",
            "                    pass",
            "    except Exception:",
            "        traceback.print_exc()",
            "",
            "",
            "def extract_archive(zip_file, dst):",
            "    target_folder = os.path.join(dst, os.path.splitext(os.path.basename(zip_file))[0])",
            "    if not os.path.exists(target_folder):",
            "        os.makedirs(target_folder, exist_ok=True)",
            "",
            "    with ZipFile(zip_file, \"r\", allowZip64=True) as z:",
            "        z.extractall(target_folder)",
            "",
            "    return target_folder",
            "",
            "",
            "def chmod_tree(dst, permissions=0o777):",
            "    for dirpath, dirnames, filenames in os.walk(dst):",
            "        for filename in filenames:",
            "            path = os.path.join(dirpath, filename)",
            "            os.chmod(path, permissions)",
            "            status = os.stat(path)",
            "            if oct(status.st_mode & 0o777) != str(oct(permissions)):",
            "                raise Exception(f\"Could not update permissions of {path}\")",
            "",
            "        for dirname in dirnames:",
            "            path = os.path.join(dirpath, dirname)",
            "            os.chmod(path, permissions)",
            "            status = os.stat(path)",
            "            if oct(status.st_mode & 0o777) != str(oct(permissions)):",
            "                raise Exception(f\"Could not update permissions of {path}\")",
            "",
            "",
            "def slugify_zh(text, separator=\"_\"):",
            "    \"\"\"",
            "    Make a slug from the given text, which is simplified from slugify.",
            "    It remove the other args and do not convert Chinese into Pinyin",
            "    :param text (str): initial text",
            "    :param separator (str): separator between words",
            "    :return (str):",
            "    \"\"\"",
            "",
            "    QUOTE_PATTERN = re.compile(r\"[\\']+\")",
            "    ALLOWED_CHARS_PATTERN = re.compile(\"[^\\u4e00-\\u9fa5a-z0-9]+\")",
            "    DUPLICATE_DASH_PATTERN = re.compile(\"-{2,}\")",
            "    NUMBERS_PATTERN = re.compile(r\"(?<=\\d),(?=\\d)\")",
            "    DEFAULT_SEPARATOR = \"-\"",
            "",
            "    # if not isinstance(text, types.UnicodeType):",
            "    #    text = unicode(text, 'utf-8', 'ignore')",
            "    # replace quotes with dashes - pre-process",
            "    text = QUOTE_PATTERN.sub(DEFAULT_SEPARATOR, text)",
            "    # make the text lowercase",
            "    text = text.lower()",
            "    # remove generated quotes -- post-process",
            "    text = QUOTE_PATTERN.sub(\"\", text)",
            "    # cleanup numbers",
            "    text = NUMBERS_PATTERN.sub(\"\", text)",
            "    # replace all other unwanted characters",
            "    text = re.sub(ALLOWED_CHARS_PATTERN, DEFAULT_SEPARATOR, text)",
            "    # remove redundant",
            "    text = re.sub(DUPLICATE_DASH_PATTERN, DEFAULT_SEPARATOR, text).strip(DEFAULT_SEPARATOR)",
            "    if separator != DEFAULT_SEPARATOR:",
            "        text = text.replace(DEFAULT_SEPARATOR, separator)",
            "    return text",
            "",
            "",
            "def get_legend_url(",
            "    instance,",
            "    style_name,",
            "    /,",
            "    service_url=None,",
            "    dataset_name=None,",
            "    version=\"1.3.0\",",
            "    sld_version=\"1.1.0\",",
            "    width=20,",
            "    height=20,",
            "    params=None,",
            "):",
            "    from geonode.geoserver.helpers import ogc_server_settings",
            "",
            "    _service_url = service_url or f\"{ogc_server_settings.PUBLIC_LOCATION}ows\"",
            "    _dataset_name = dataset_name or instance.alternate",
            "    _params = f\"&{params}\" if params else \"\"",
            "    return (",
            "        f\"{_service_url}?\"",
            "        f\"service=WMS&request=GetLegendGraphic&format=image/png&WIDTH={width}&HEIGHT={height}&\"",
            "        f\"LAYER={_dataset_name}&STYLE={style_name}&version={version}&\"",
            "        f\"sld_version={sld_version}&legend_options=fontAntiAliasing:true;fontSize:12;forceLabels:on{_params}\"",
            "    )",
            "",
            "",
            "def set_resource_default_links(instance, layer, prune=False, **kwargs):",
            "    from geonode.base.models import Link",
            "    from django.urls import reverse",
            "    from django.utils.translation import ugettext",
            "    from geonode.layers.models import Dataset",
            "    from geonode.documents.models import Document",
            "",
            "    # Prune old links",
            "    if prune:",
            "        logger.debug(\" -- Resource Links[Prune old links]...\")",
            "        _def_link_types = (\"data\", \"image\", \"original\", \"html\", \"OGC:WMS\", \"OGC:WFS\", \"OGC:WCS\")",
            "        Link.objects.filter(resource=instance.resourcebase_ptr, link_type__in=_def_link_types).delete()",
            "        logger.debug(\" -- Resource Links[Prune old links]...done!\")",
            "",
            "    if check_ogc_backend(geoserver.BACKEND_PACKAGE):",
            "        from geonode.geoserver.ows import wcs_links, wfs_links, wms_links",
            "        from geonode.geoserver.helpers import ogc_server_settings, gs_catalog",
            "",
            "        # Compute parameters for the new links",
            "        logger.debug(\" -- Resource Links[Compute parameters for the new links]...\")",
            "        height = 550",
            "        width = 550",
            "",
            "        # Parse Dataset BBOX and SRID",
            "        bbox = None",
            "        srid = instance.srid if instance.srid else getattr(settings, \"DEFAULT_MAP_CRS\", \"EPSG:4326\")",
            "        if not prune and instance.srid and instance.bbox_polygon:",
            "            bbox = instance.bbox_string",
            "        else:",
            "            try:",
            "                gs_resource = gs_catalog.get_resource(",
            "                    name=instance.name, store=instance.store, workspace=instance.workspace",
            "                )",
            "                if not gs_resource:",
            "                    gs_resource = gs_catalog.get_resource(name=instance.name, workspace=instance.workspace)",
            "                if not gs_resource:",
            "                    gs_resource = gs_catalog.get_resource(name=instance.name)",
            "",
            "                if gs_resource:",
            "                    srid = gs_resource.projection",
            "                    bbox = gs_resource.native_bbox",
            "                    ll_bbox = gs_resource.latlon_bbox",
            "                    try:",
            "                        instance.set_bbox_polygon([bbox[0], bbox[2], bbox[1], bbox[3]], srid)",
            "                    except GeoNodeException as e:",
            "                        if not ll_bbox:",
            "                            raise",
            "                        else:",
            "                            logger.exception(e)",
            "                            instance.srid = \"EPSG:4326\"",
            "                    instance.set_ll_bbox_polygon([ll_bbox[0], ll_bbox[2], ll_bbox[1], ll_bbox[3]])",
            "                    if instance.srid:",
            "                        instance.srid_url = (",
            "                            f\"http://www.spatialreference.org/ref/{instance.srid.replace(':', '/').lower()}/\"",
            "                        )",
            "                    elif instance.bbox_polygon is not None:",
            "                        # Guessing 'EPSG:4326' by default",
            "                        instance.srid = \"EPSG:4326\"",
            "                    else:",
            "                        raise GeoNodeException(_(\"Invalid Projection. Dataset is missing CRS!\"))",
            "                    dx = float(bbox[1]) - float(bbox[0])",
            "                    dy = float(bbox[3]) - float(bbox[2])",
            "                    dataAspect = 1 if dy == 0 else dx / dy",
            "                    width = int(height * dataAspect)",
            "                    # Rewriting BBOX as a plain string",
            "                    bbox = \",\".join(str(x) for x in [bbox[0], bbox[2], bbox[1], bbox[3]])",
            "                else:",
            "                    bbox = instance.bbox_string",
            "            except Exception as e:",
            "                logger.exception(e)",
            "                bbox = instance.bbox_string",
            "",
            "        # Create Raw Data download link",
            "        if settings.DISPLAY_ORIGINAL_DATASET_LINK:",
            "            logger.debug(\" -- Resource Links[Create Raw Data download link]...\")",
            "            if isinstance(instance, Dataset):",
            "                download_url = build_absolute_uri(reverse(\"dataset_download\", args=(instance.alternate,)))",
            "            elif isinstance(instance, Document):",
            "                download_url = build_absolute_uri(reverse(\"document_download\", args=(instance.id,)))",
            "            else:",
            "                download_url = None",
            "",
            "            while Link.objects.filter(resource=instance.resourcebase_ptr, link_type=\"original\").exists():",
            "                Link.objects.filter(resource=instance.resourcebase_ptr, link_type=\"original\").delete()",
            "            Link.objects.update_or_create(",
            "                resource=instance.resourcebase_ptr,",
            "                url=download_url,",
            "                defaults=dict(",
            "                    extension=\"zip\",",
            "                    name=\"Original Dataset\",",
            "                    mime=\"application/octet-stream\",",
            "                    link_type=\"original\",",
            "                ),",
            "            )",
            "            logger.debug(\" -- Resource Links[Create Raw Data download link]...done!\")",
            "        else:",
            "            Link.objects.filter(resource=instance.resourcebase_ptr, name=\"Original Dataset\").delete()",
            "",
            "        # Set download links for WMS, WCS or WFS and KML",
            "        logger.debug(\" -- Resource Links[Set download links for WMS, WCS or WFS and KML]...\")",
            "        instance_ows_url = f\"{instance.ows_url}?\" if instance.ows_url else f\"{ogc_server_settings.public_url}ows?\"",
            "        links = wms_links(instance_ows_url, instance.alternate, bbox, srid, height, width)",
            "",
            "        for ext, name, mime, wms_url in links:",
            "            try:",
            "                Link.objects.update_or_create(",
            "                    resource=instance.resourcebase_ptr,",
            "                    name=ugettext(name),",
            "                    defaults=dict(",
            "                        extension=ext,",
            "                        url=wms_url,",
            "                        mime=mime,",
            "                        link_type=\"image\",",
            "                    ),",
            "                )",
            "            except Link.MultipleObjectsReturned:",
            "                _d = dict(extension=ext, url=wms_url, mime=mime, link_type=\"image\")",
            "                Link.objects.filter(resource=instance.resourcebase_ptr, name=ugettext(name), link_type=\"image\").update(",
            "                    **_d",
            "                )",
            "",
            "        if instance.subtype == \"vector\":",
            "            links = wfs_links(",
            "                instance_ows_url,",
            "                instance.alternate,",
            "                bbox=None,  # bbox filter should be set at runtime otherwise conflicting with CQL",
            "                srid=srid,",
            "            )",
            "            for ext, name, mime, wfs_url in links:",
            "                if mime == \"SHAPE-ZIP\":",
            "                    name = \"Zipped Shapefile\"",
            "                if (",
            "                    Link.objects.filter(",
            "                        resource=instance.resourcebase_ptr, url=wfs_url, name=name, link_type=\"data\"",
            "                    ).count()",
            "                    < 2",
            "                ):",
            "                    Link.objects.update_or_create(",
            "                        resource=instance.resourcebase_ptr,",
            "                        url=wfs_url,",
            "                        name=name,",
            "                        link_type=\"data\",",
            "                        defaults=dict(",
            "                            extension=ext,",
            "                            mime=mime,",
            "                        ),",
            "                    )",
            "",
            "        elif instance.subtype == \"raster\":",
            "            \"\"\"",
            "            Going to create the WCS GetCoverage Default download links.",
            "            By providing 'None' bbox and srid, we are going to ask to the WCS to",
            "            skip subsetting, i.e. output the whole coverage in the netive SRS.",
            "",
            "            Notice that the \"wcs_links\" method also generates 1 default \"outputFormat\":",
            "             - \"geotiff\"; GeoTIFF which will be compressed and tiled by passing to the WCS the default query params compression='DEFLATE' and tile_size=512",
            "            \"\"\"",
            "            links = wcs_links(instance_ows_url, instance.alternate)",
            "            for ext, name, mime, wcs_url in links:",
            "                if (",
            "                    Link.objects.filter(",
            "                        resource=instance.resourcebase_ptr, url=wcs_url, name=name, link_type=\"data\"",
            "                    ).count()",
            "                    < 2",
            "                ):",
            "                    Link.objects.update_or_create(",
            "                        resource=instance.resourcebase_ptr,",
            "                        url=wcs_url,",
            "                        name=name,",
            "                        link_type=\"data\",",
            "                        defaults=dict(",
            "                            extension=ext,",
            "                            mime=mime,",
            "                        ),",
            "                    )",
            "",
            "        site_url = settings.SITEURL.rstrip(\"/\") if settings.SITEURL.startswith(\"http\") else settings.SITEURL",
            "        html_link_url = f\"{site_url}{instance.get_absolute_url()}\"",
            "",
            "        if (",
            "            Link.objects.filter(",
            "                resource=instance.resourcebase_ptr, url=html_link_url, name=instance.alternate, link_type=\"html\"",
            "            ).count()",
            "            < 2",
            "        ):",
            "            Link.objects.update_or_create(",
            "                resource=instance.resourcebase_ptr,",
            "                url=html_link_url,",
            "                name=instance.alternate or instance.name,",
            "                link_type=\"html\",",
            "                defaults=dict(",
            "                    extension=\"html\",",
            "                    mime=\"text/html\",",
            "                ),",
            "            )",
            "        logger.debug(\" -- Resource Links[Set download links for WMS, WCS or WFS and KML]...done!\")",
            "",
            "        # Legend link",
            "        logger.debug(\" -- Resource Links[Legend link]...\")",
            "        try:",
            "            if instance.subtype not in [\"tileStore\", \"remote\"]:",
            "                for style in set(",
            "                    list(instance.styles.all())",
            "                    + [",
            "                        instance.default_style,",
            "                    ]",
            "                ):",
            "                    if style:",
            "                        style_name = os.path.basename(urlparse(style.sld_url).path).split(\".\")[0]",
            "                        legend_url = get_legend_url(instance, style_name)",
            "                        if Link.objects.filter(resource=instance.resourcebase_ptr, url=legend_url).count() < 2:",
            "                            Link.objects.update_or_create(",
            "                                resource=instance.resourcebase_ptr,",
            "                                name=\"Legend\",",
            "                                url=legend_url,",
            "                                defaults=dict(",
            "                                    extension=\"png\",",
            "                                    url=legend_url,",
            "                                    mime=\"image/png\",",
            "                                    link_type=\"image\",",
            "                                ),",
            "                            )",
            "            else:",
            "                from geonode.services.serviceprocessors import get_service_handler",
            "",
            "                handler = get_service_handler(",
            "                    instance.remote_service.service_url, service_type=instance.remote_service.type",
            "                )",
            "                if handler and hasattr(handler, \"_create_dataset_legend_link\"):",
            "                    handler._create_dataset_legend_link(instance)",
            "",
            "            logger.debug(\" -- Resource Links[Legend link]...done!\")",
            "        except Exception as e:",
            "            logger.debug(f\" -- Resource Links[Legend link]...error: {e}\")",
            "",
            "        # Thumbnail link",
            "        if instance.get_thumbnail_url():",
            "            logger.debug(\" -- Resource Links[Thumbnail link]...\")",
            "            if (",
            "                Link.objects.filter(",
            "                    resource=instance.resourcebase_ptr, url=instance.get_thumbnail_url(), name=\"Thumbnail\"",
            "                ).count()",
            "                < 2",
            "            ):",
            "                Link.objects.update_or_create(",
            "                    resource=instance.resourcebase_ptr,",
            "                    url=instance.get_thumbnail_url(),",
            "                    name=\"Thumbnail\",",
            "                    defaults=dict(",
            "                        extension=\"png\",",
            "                        mime=\"image/png\",",
            "                        link_type=\"image\",",
            "                    ),",
            "                )",
            "            logger.debug(\" -- Resource Links[Thumbnail link]...done!\")",
            "",
            "        logger.debug(\" -- Resource Links[OWS Links]...\")",
            "        try:",
            "            if (",
            "                not hasattr(instance.get_real_instance(), \"ptype\")",
            "                or instance.get_real_instance().ptype == GXP_PTYPES[\"WMS\"]",
            "            ):",
            "                ogc_wms_url = instance.ows_url or urljoin(ogc_server_settings.public_url, \"ows\")",
            "                ogc_wms_name = f\"OGC WMS: {instance.workspace} Service\"",
            "                if (",
            "                    Link.objects.filter(resource=instance.resourcebase_ptr, name=ogc_wms_name, url=ogc_wms_url).count()",
            "                    < 2",
            "                ):",
            "                    Link.objects.get_or_create(",
            "                        resource=instance.resourcebase_ptr,",
            "                        url=ogc_wms_url,",
            "                        name=ogc_wms_name,",
            "                        defaults=dict(",
            "                            extension=\"html\",",
            "                            url=ogc_wms_url,",
            "                            mime=\"text/html\",",
            "                            link_type=\"OGC:WMS\",",
            "                        ),",
            "                    )",
            "",
            "                if instance.subtype == \"vector\":",
            "                    ogc_wfs_url = instance.ows_url or urljoin(ogc_server_settings.public_url, \"ows\")",
            "                    ogc_wfs_name = f\"OGC WFS: {instance.workspace} Service\"",
            "                    if (",
            "                        Link.objects.filter(",
            "                            resource=instance.resourcebase_ptr, name=ogc_wfs_name, url=ogc_wfs_url",
            "                        ).count()",
            "                        < 2",
            "                    ):",
            "                        Link.objects.get_or_create(",
            "                            resource=instance.resourcebase_ptr,",
            "                            url=ogc_wfs_url,",
            "                            name=ogc_wfs_name,",
            "                            defaults=dict(",
            "                                extension=\"html\",",
            "                                url=ogc_wfs_url,",
            "                                mime=\"text/html\",",
            "                                link_type=\"OGC:WFS\",",
            "                            ),",
            "                        )",
            "",
            "                if instance.subtype == \"raster\":",
            "                    ogc_wcs_url = instance.ows_url or urljoin(ogc_server_settings.public_url, \"ows\")",
            "                    ogc_wcs_name = f\"OGC WCS: {instance.workspace} Service\"",
            "                    if (",
            "                        Link.objects.filter(",
            "                            resource=instance.resourcebase_ptr, name=ogc_wcs_name, url=ogc_wcs_url",
            "                        ).count()",
            "                        < 2",
            "                    ):",
            "                        Link.objects.get_or_create(",
            "                            resource=instance.resourcebase_ptr,",
            "                            url=ogc_wcs_url,",
            "                            name=ogc_wcs_name,",
            "                            defaults=dict(",
            "                                extension=\"html\",",
            "                                url=ogc_wcs_url,",
            "                                mime=\"text/html\",",
            "                                link_type=\"OGC:WCS\",",
            "                            ),",
            "                        )",
            "",
            "            elif hasattr(instance.get_real_instance(), \"ptype\") and instance.get_real_instance().ptype:",
            "                ptype_link = dict((v, k) for k, v in GXP_PTYPES.items()).get(instance.get_real_instance().ptype)",
            "                ptype_link_name = get_available_service_types().get(ptype_link)",
            "                ptype_link_url = instance.ows_url",
            "                if (",
            "                    Link.objects.filter(",
            "                        resource=instance.resourcebase_ptr, name=ptype_link_name, url=ptype_link_url",
            "                    ).count()",
            "                    < 2",
            "                ):",
            "                    Link.objects.get_or_create(",
            "                        resource=instance.resourcebase_ptr,",
            "                        url=ptype_link_url,",
            "                        name=ptype_link_name,",
            "                        defaults=dict(",
            "                            extension=\"html\",",
            "                            url=ptype_link_url,",
            "                            mime=\"text/html\",",
            "                            link_type=\"image\",",
            "                        ),",
            "                    )",
            "            logger.debug(\" -- Resource Links[OWS Links]...done!\")",
            "        except Exception as e:",
            "            logger.error(\" -- Resource Links[OWS Links]...error!\")",
            "            logger.exception(e)",
            "",
            "",
            "def add_url_params(url, params):",
            "    \"\"\"Add GET params to provided URL being aware of existing.",
            "",
            "    :param url: string of target URL",
            "    :param params: dict containing requested params to be added",
            "    :return: string with updated URL",
            "",
            "    >> url = 'http://stackoverflow.com/test?answers=true'",
            "    >> new_params = {'answers': False, 'data': ['some','values']}",
            "    >> add_url_params(url, new_params)",
            "    'http://stackoverflow.com/test?data=some&data=values&answers=false'",
            "    \"\"\"",
            "    # Unquoting URL first so we don't loose existing args",
            "    url = unquote(url)",
            "    # Extracting url info",
            "    parsed_url = urlparse(url)",
            "    # Extracting URL arguments from parsed URL",
            "    get_args = parsed_url.query",
            "    # Converting URL arguments to dict",
            "    parsed_get_args = dict(parse_qsl(get_args))",
            "    # Merging URL arguments dict with new params",
            "    parsed_get_args.update(params)",
            "",
            "    # Bool and Dict values should be converted to json-friendly values",
            "    # you may throw this part away if you don't like it :)",
            "    parsed_get_args.update({k: json.dumps(v) for k, v in parsed_get_args.items() if isinstance(v, (bool, dict))})",
            "",
            "    # Converting URL argument to proper query string",
            "    encoded_get_args = urlencode(parsed_get_args, doseq=True)",
            "    # Creating new parsed result object based on provided with new",
            "    # URL arguments. Same thing happens inside of urlparse.",
            "    new_url = ParseResult(",
            "        parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, encoded_get_args, parsed_url.fragment",
            "    ).geturl()",
            "",
            "    return new_url",
            "",
            "",
            "json_serializer_k_map = {",
            "    \"user\": settings.AUTH_USER_MODEL,",
            "    \"owner\": settings.AUTH_USER_MODEL,",
            "    \"restriction_code_type\": \"base.RestrictionCodeType\",",
            "    \"license\": \"base.License\",",
            "    \"category\": \"base.TopicCategory\",",
            "    \"spatial_representation_type\": \"base.SpatialRepresentationType\",",
            "    \"group\": \"auth.Group\",",
            "    \"default_style\": \"datasets.Style\",",
            "}",
            "",
            "",
            "def json_serializer_producer(dictionary):",
            "    \"\"\"",
            "    - usage:",
            "           serialized_obj =",
            "               json_serializer_producer(model_to_dict(instance))",
            "",
            "    - dump to file:",
            "       with open('data.json', 'w') as outfile:",
            "           json.dump(serialized_obj, outfile)",
            "",
            "    - read from file:",
            "       with open('data.json', 'r') as infile:",
            "           serialized_obj = json.load(infile)",
            "    \"\"\"",
            "",
            "    def to_json(keys):",
            "        if isinstance(keys, datetime.datetime):",
            "            return str(keys)",
            "        elif isinstance(keys, str) or isinstance(keys, int):",
            "            return keys",
            "        elif isinstance(keys, dict):",
            "            return json_serializer_producer(keys)",
            "        elif isinstance(keys, list):",
            "            return [json_serializer_producer(model_to_dict(k)) for k in keys]",
            "        elif isinstance(keys, models.Model):",
            "            return json_serializer_producer(model_to_dict(keys))",
            "        elif isinstance(keys, Decimal):",
            "            return float(keys)",
            "        else:",
            "            return str(keys)",
            "",
            "    output = {}",
            "",
            "    _keys_to_skip = [",
            "        \"email\",",
            "        \"password\",",
            "        \"last_login\",",
            "        \"date_joined\",",
            "        \"is_staff\",",
            "        \"is_active\",",
            "        \"is_superuser\",",
            "        \"permissions\",",
            "        \"user_permissions\",",
            "    ]",
            "",
            "    for x, y in dictionary.items():",
            "        if x not in _keys_to_skip:",
            "            if x in json_serializer_k_map.keys():",
            "                instance = django_apps.get_model(json_serializer_k_map[x], require_ready=False)",
            "                if instance.objects.filter(id=y):",
            "                    _obj = instance.objects.get(id=y)",
            "                    y = model_to_dict(_obj)",
            "            output[x] = to_json(y)",
            "    return output",
            "",
            "",
            "def is_monochromatic_image(image_url, image_data=None):",
            "    def is_local_static(url):",
            "        if url.startswith(settings.STATIC_URL) or (url.startswith(settings.SITEURL) and settings.STATIC_URL in url):",
            "            return True",
            "        return False",
            "",
            "    def is_absolute(url):",
            "        return bool(urlparse(url).netloc)",
            "",
            "    def get_thumb_handler(url):",
            "        _index = url.find(settings.STATIC_URL)",
            "        _thumb_path = urlparse(url[_index + len(settings.STATIC_URL) :]).path",
            "        if storage_manager.exists(_thumb_path):",
            "            return storage_manager.open(_thumb_path)",
            "        return None",
            "",
            "    def verify_image(stream):",
            "        with Image.open(stream) as _stream:",
            "            img = _stream.convert(\"L\")",
            "            img.verify()  # verify that it is, in fact an image",
            "            extr = img.getextrema()",
            "            a = 0",
            "            for i in extr:",
            "                if isinstance(i, tuple):",
            "                    a += abs(i[0] - i[1])",
            "                else:",
            "                    a = abs(extr[0] - extr[1])",
            "                    break",
            "            return a == 0",
            "",
            "    try:",
            "        if image_data:",
            "            logger.debug(\"...Checking if image is a blank image\")",
            "            with BytesIO(image_data) as stream:",
            "                return verify_image(stream)",
            "        elif image_url:",
            "            logger.debug(f\"...Checking if '{image_url}' is a blank image\")",
            "            url = image_url if is_absolute(image_url) else urljoin(settings.SITEURL, image_url)",
            "            if not is_local_static(url):",
            "                req, stream_content = http_client.get(url, timeout=5)",
            "                with BytesIO(stream_content) as stream:",
            "                    return verify_image(stream)",
            "            else:",
            "                with get_thumb_handler(url) as stream:",
            "                    return verify_image(stream)",
            "        return True",
            "    except Exception as e:",
            "        logger.debug(e)",
            "        return False",
            "",
            "",
            "def find_by_attr(lst, val, attr=\"id\"):",
            "    \"\"\"Returns an object if the id matches in any list of objects\"\"\"",
            "    for item in lst:",
            "        if attr in item and item[attr] == val:",
            "            return item",
            "",
            "    return None",
            "",
            "",
            "def build_absolute_uri(url):",
            "    if url and \"http\" not in url:",
            "        url = urljoin(settings.SITEURL, url)",
            "    return url",
            "",
            "",
            "def extract_ip_or_domain(url):",
            "    ip_regex = re.compile(\"^(?:http\\:\\/\\/|https\\:\\/\\/)(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\")",
            "    domain_regex = re.compile(\"^(?:http\\:\\/\\/|https\\:\\/\\/)([a-zA-Z0-9.-]+)\")",
            "",
            "    match = ip_regex.findall(url)",
            "    if len(match):",
            "        ip_address = match[0]",
            "        try:",
            "            ipaddress.ip_address(ip_address)  # Validate the IP address",
            "            return ip_address",
            "        except ValueError:",
            "            pass",
            "",
            "    match = domain_regex.findall(url)",
            "    if len(match):",
            "        return match[0]",
            "",
            "    return None",
            "",
            "",
            "def get_xpath_value(",
            "    element: etree.Element, xpath_expression: str, nsmap: typing.Optional[dict] = None",
            ") -> typing.Optional[str]:",
            "    if not nsmap:",
            "        nsmap = element.nsmap",
            "    values = element.xpath(f\"{xpath_expression}//text()\", namespaces=nsmap)",
            "    return \"\".join(values).strip() or None",
            "",
            "",
            "def get_geonode_app_types():",
            "    from geonode.geoapps.models import GeoApp",
            "",
            "    return list(set(GeoApp.objects.values_list(\"resource_type\", flat=True)))",
            "",
            "",
            "def get_supported_datasets_file_types():",
            "    from django.conf import settings as gn_settings",
            "",
            "    \"\"\"",
            "    Return a list of all supported file type in geonode",
            "    If one of the type provided in the custom type exists in the default",
            "    is going to override it",
            "    \"\"\"",
            "    default_types = settings.SUPPORTED_DATASET_FILE_TYPES",
            "    types_module = (",
            "        gn_settings.ADDITIONAL_DATASET_FILE_TYPES if hasattr(gn_settings, \"ADDITIONAL_DATASET_FILE_TYPES\") else []",
            "    )",
            "    supported_types = default_types.copy()",
            "    default_types_id = [t.get(\"id\") for t in default_types]",
            "    for _type in types_module:",
            "        if _type.get(\"id\") in default_types_id:",
            "            supported_types[default_types_id.index(_type.get(\"id\"))] = _type",
            "        else:",
            "            supported_types.extend([_type])",
            "",
            "    # Order the formats (to support their visualization)",
            "    formats_order = [(\"vector\", 0), (\"raster\", 1), (\"archive\", 2)]",
            "    ordered_payload = (",
            "        (weight[1], resource_type)",
            "        for resource_type in supported_types",
            "        for weight in formats_order",
            "        if resource_type.get(\"format\") in weight[0]",
            "    )",
            "",
            "    # Flatten the list",
            "    ordered_resource_types = [x[1] for x in sorted(ordered_payload, key=lambda x: x[0])]",
            "    other_resource_types = [",
            "        resource_type",
            "        for resource_type in supported_types",
            "        if resource_type.get(\"format\") is None or resource_type.get(\"format\") not in [f[0] for f in formats_order]",
            "    ]",
            "    return ordered_resource_types + other_resource_types",
            "",
            "",
            "def get_allowed_extensions():",
            "    return list(itertools.chain.from_iterable([_type[\"ext\"] for _type in get_supported_datasets_file_types()]))",
            "",
            "",
            "def safe_path_leaf(path):",
            "    \"\"\"A view that is not vulnerable to malicious file access.\"\"\"",
            "    base_path = settings.MEDIA_ROOT",
            "    try:",
            "        validate_filepath(path, platform=\"auto\")",
            "        head, tail = ntpath.split(path)",
            "        filename = tail or ntpath.basename(head)",
            "        validate_filename(filename, platform=\"auto\")",
            "    except ValidationError as e:",
            "        logger.error(f\"{e}\")",
            "        raise e",
            "    # GOOD -- Verify with normalised version of path",
            "    fullpath = os.path.normpath(os.path.join(head, filename))",
            "    if not fullpath.startswith(base_path) or path != fullpath:",
            "        raise GeoNodeException(",
            "            f\"The provided path '{path}' is not safe. The file is outside the MEDIA_ROOT '{base_path}' base path!\"",
            "        )",
            "    return fullpath",
            "",
            "",
            "def import_class_module(full_class_string):",
            "    \"\"\"",
            "    Dynamically load a class from a string",
            "",
            "    >>> klass = import_class_module(\"module.submodule.ClassName\")",
            "    >>> klass2 = import_class_module(\"myfile.Class2\")",
            "    \"\"\"",
            "    try:",
            "        module_path, class_name = full_class_string.rsplit(\".\", 1)",
            "        module = importlib.import_module(module_path)",
            "        class_obj = getattr(module, class_name)",
            "        return class_obj",
            "    except Exception:",
            "        return None"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "openstack_dashboard.dashboards.project.stacks.forms.TemplateForm"
        ]
    }
}