{
    "yt_dlp/YoutubeDL.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from .cache import Cache"
            },
            "2": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from .compat import functools, urllib  # isort: split"
            },
            "3": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from .compat import compat_os_name, compat_shlex_quote, urllib_req_to_req"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+from .compat import compat_os_name, urllib_req_to_req"
            },
            "5": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " from .cookies import LenientSimpleCookie, load_cookies"
            },
            "6": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from .downloader import FFmpegFD, get_suitable_downloader, shorten_protocol_name"
            },
            "7": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from .downloader.rtmp import rtmpdump_version"
            },
            "8": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "     UserNotLive,"
            },
            "9": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "     YoutubeDLError,"
            },
            "10": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "     age_restricted,"
            },
            "11": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    args_to_str,"
            },
            "12": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "     bug_reports_message,"
            },
            "13": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "     date_from_str,"
            },
            "14": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "     deprecation_warning,"
            },
            "15": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": 140,
                "PatchRowcode": "     sanitize_filename,"
            },
            "16": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "     sanitize_path,"
            },
            "17": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": 142,
                "PatchRowcode": "     sanitize_url,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 143,
                "PatchRowcode": "+    shell_quote,"
            },
            "19": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 144,
                "PatchRowcode": "     str_or_none,"
            },
            "20": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": 145,
                "PatchRowcode": "     strftime_or_none,"
            },
            "21": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 146,
                "PatchRowcode": "     subtitles_filename,"
            },
            "22": {
                "beforePatchRowNumber": 823,
                "afterPatchRowNumber": 823,
                "PatchRowcode": "             self.report_warning("
            },
            "23": {
                "beforePatchRowNumber": 824,
                "afterPatchRowNumber": 824,
                "PatchRowcode": "                 'Long argument string detected. '"
            },
            "24": {
                "beforePatchRowNumber": 825,
                "afterPatchRowNumber": 825,
                "PatchRowcode": "                 'Use -- to separate parameters and URLs, like this:\\n%s' %"
            },
            "25": {
                "beforePatchRowNumber": 826,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                args_to_str(correct_argv))"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 826,
                "PatchRowcode": "+                shell_quote(correct_argv))"
            },
            "27": {
                "beforePatchRowNumber": 827,
                "afterPatchRowNumber": 827,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": 828,
                "afterPatchRowNumber": 828,
                "PatchRowcode": "     def add_info_extractor(self, ie):"
            },
            "29": {
                "beforePatchRowNumber": 829,
                "afterPatchRowNumber": 829,
                "PatchRowcode": "         \"\"\"Add an InfoExtractor object to the end of the list.\"\"\""
            },
            "30": {
                "beforePatchRowNumber": 1355,
                "afterPatchRowNumber": 1355,
                "PatchRowcode": "                 value, fmt = escapeHTML(str(value)), str_fmt"
            },
            "31": {
                "beforePatchRowNumber": 1356,
                "afterPatchRowNumber": 1356,
                "PatchRowcode": "             elif fmt[-1] == 'q':  # quoted"
            },
            "32": {
                "beforePatchRowNumber": 1357,
                "afterPatchRowNumber": 1357,
                "PatchRowcode": "                 value = map(str, variadic(value) if '#' in flags else [value])"
            },
            "33": {
                "beforePatchRowNumber": 1358,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                value, fmt = ' '.join(map(compat_shlex_quote, value)), str_fmt"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1358,
                "PatchRowcode": "+                value, fmt = shell_quote(value, shell=True), str_fmt"
            },
            "35": {
                "beforePatchRowNumber": 1359,
                "afterPatchRowNumber": 1359,
                "PatchRowcode": "             elif fmt[-1] == 'B':  # bytes"
            },
            "36": {
                "beforePatchRowNumber": 1360,
                "afterPatchRowNumber": 1360,
                "PatchRowcode": "                 value = f'%{str_fmt}'.encode() % str(value).encode()"
            },
            "37": {
                "beforePatchRowNumber": 1361,
                "afterPatchRowNumber": 1361,
                "PatchRowcode": "                 value, fmt = value.decode('utf-8', 'ignore'), 's'"
            }
        },
        "frontPatchFile": [
            "import collections",
            "import contextlib",
            "import copy",
            "import datetime as dt",
            "import errno",
            "import fileinput",
            "import http.cookiejar",
            "import io",
            "import itertools",
            "import json",
            "import locale",
            "import operator",
            "import os",
            "import random",
            "import re",
            "import shutil",
            "import string",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import tokenize",
            "import traceback",
            "import unicodedata",
            "",
            "from .cache import Cache",
            "from .compat import functools, urllib  # isort: split",
            "from .compat import compat_os_name, compat_shlex_quote, urllib_req_to_req",
            "from .cookies import LenientSimpleCookie, load_cookies",
            "from .downloader import FFmpegFD, get_suitable_downloader, shorten_protocol_name",
            "from .downloader.rtmp import rtmpdump_version",
            "from .extractor import gen_extractor_classes, get_info_extractor",
            "from .extractor.common import UnsupportedURLIE",
            "from .extractor.openload import PhantomJSwrapper",
            "from .minicurses import format_text",
            "from .networking import HEADRequest, Request, RequestDirector",
            "from .networking.common import _REQUEST_HANDLERS, _RH_PREFERENCES",
            "from .networking.exceptions import (",
            "    HTTPError,",
            "    NoSupportingHandlers,",
            "    RequestError,",
            "    SSLError,",
            "    network_exceptions,",
            ")",
            "from .networking.impersonate import ImpersonateRequestHandler",
            "from .plugins import directories as plugin_directories",
            "from .postprocessor import _PLUGIN_CLASSES as plugin_pps",
            "from .postprocessor import (",
            "    EmbedThumbnailPP,",
            "    FFmpegFixupDuplicateMoovPP,",
            "    FFmpegFixupDurationPP,",
            "    FFmpegFixupM3u8PP,",
            "    FFmpegFixupM4aPP,",
            "    FFmpegFixupStretchedPP,",
            "    FFmpegFixupTimestampPP,",
            "    FFmpegMergerPP,",
            "    FFmpegPostProcessor,",
            "    FFmpegVideoConvertorPP,",
            "    MoveFilesAfterDownloadPP,",
            "    get_postprocessor,",
            ")",
            "from .postprocessor.ffmpeg import resolve_mapping as resolve_recode_mapping",
            "from .update import (",
            "    REPOSITORY,",
            "    _get_system_deprecation,",
            "    _make_label,",
            "    current_git_head,",
            "    detect_variant,",
            ")",
            "from .utils import (",
            "    DEFAULT_OUTTMPL,",
            "    IDENTITY,",
            "    LINK_TEMPLATES,",
            "    MEDIA_EXTENSIONS,",
            "    NO_DEFAULT,",
            "    NUMBER_RE,",
            "    OUTTMPL_TYPES,",
            "    POSTPROCESS_WHEN,",
            "    STR_FORMAT_RE_TMPL,",
            "    STR_FORMAT_TYPES,",
            "    ContentTooShortError,",
            "    DateRange,",
            "    DownloadCancelled,",
            "    DownloadError,",
            "    EntryNotInPlaylist,",
            "    ExistingVideoReached,",
            "    ExtractorError,",
            "    FormatSorter,",
            "    GeoRestrictedError,",
            "    ISO3166Utils,",
            "    LazyList,",
            "    MaxDownloadsReached,",
            "    Namespace,",
            "    PagedList,",
            "    PlaylistEntries,",
            "    Popen,",
            "    PostProcessingError,",
            "    ReExtractInfo,",
            "    RejectedVideoReached,",
            "    SameFileError,",
            "    UnavailableVideoError,",
            "    UserNotLive,",
            "    YoutubeDLError,",
            "    age_restricted,",
            "    args_to_str,",
            "    bug_reports_message,",
            "    date_from_str,",
            "    deprecation_warning,",
            "    determine_ext,",
            "    determine_protocol,",
            "    encode_compat_str,",
            "    encodeFilename,",
            "    error_to_compat_str,",
            "    escapeHTML,",
            "    expand_path,",
            "    extract_basic_auth,",
            "    filter_dict,",
            "    float_or_none,",
            "    format_bytes,",
            "    format_decimal_suffix,",
            "    format_field,",
            "    formatSeconds,",
            "    get_compatible_ext,",
            "    get_domain,",
            "    int_or_none,",
            "    iri_to_uri,",
            "    is_path_like,",
            "    join_nonempty,",
            "    locked_file,",
            "    make_archive_id,",
            "    make_dir,",
            "    number_of_digits,",
            "    orderedSet,",
            "    orderedSet_from_options,",
            "    parse_filesize,",
            "    preferredencoding,",
            "    prepend_extension,",
            "    remove_terminal_sequences,",
            "    render_table,",
            "    replace_extension,",
            "    sanitize_filename,",
            "    sanitize_path,",
            "    sanitize_url,",
            "    str_or_none,",
            "    strftime_or_none,",
            "    subtitles_filename,",
            "    supports_terminal_sequences,",
            "    system_identifier,",
            "    filesize_from_tbr,",
            "    timetuple_from_msec,",
            "    to_high_limit_path,",
            "    traverse_obj,",
            "    try_call,",
            "    try_get,",
            "    url_basename,",
            "    variadic,",
            "    version_tuple,",
            "    windows_enable_vt_mode,",
            "    write_json_file,",
            "    write_string,",
            ")",
            "from .utils._utils import _YDLLogger",
            "from .utils.networking import (",
            "    HTTPHeaderDict,",
            "    clean_headers,",
            "    clean_proxies,",
            "    std_headers,",
            ")",
            "from .version import CHANNEL, ORIGIN, RELEASE_GIT_HEAD, VARIANT, __version__",
            "",
            "if compat_os_name == 'nt':",
            "    import ctypes",
            "",
            "",
            "class YoutubeDL:",
            "    \"\"\"YoutubeDL class.",
            "",
            "    YoutubeDL objects are the ones responsible of downloading the",
            "    actual video file and writing it to disk if the user has requested",
            "    it, among some other tasks. In most cases there should be one per",
            "    program. As, given a video URL, the downloader doesn't know how to",
            "    extract all the needed information, task that InfoExtractors do, it",
            "    has to pass the URL to one of them.",
            "",
            "    For this, YoutubeDL objects have a method that allows",
            "    InfoExtractors to be registered in a given order. When it is passed",
            "    a URL, the YoutubeDL object handles it to the first InfoExtractor it",
            "    finds that reports being able to handle it. The InfoExtractor extracts",
            "    all the information about the video or videos the URL refers to, and",
            "    YoutubeDL process the extracted information, possibly using a File",
            "    Downloader to download the video.",
            "",
            "    YoutubeDL objects accept a lot of parameters. In order not to saturate",
            "    the object constructor with arguments, it receives a dictionary of",
            "    options instead. These options are available through the params",
            "    attribute for the InfoExtractors to use. The YoutubeDL also",
            "    registers itself as the downloader in charge for the InfoExtractors",
            "    that are added to it, so this is a \"mutual registration\".",
            "",
            "    Available options:",
            "",
            "    username:          Username for authentication purposes.",
            "    password:          Password for authentication purposes.",
            "    videopassword:     Password for accessing a video.",
            "    ap_mso:            Adobe Pass multiple-system operator identifier.",
            "    ap_username:       Multiple-system operator account username.",
            "    ap_password:       Multiple-system operator account password.",
            "    usenetrc:          Use netrc for authentication instead.",
            "    netrc_location:    Location of the netrc file. Defaults to ~/.netrc.",
            "    netrc_cmd:         Use a shell command to get credentials",
            "    verbose:           Print additional info to stdout.",
            "    quiet:             Do not print messages to stdout.",
            "    no_warnings:       Do not print out anything for warnings.",
            "    forceprint:        A dict with keys WHEN mapped to a list of templates to",
            "                       print to stdout. The allowed keys are video or any of the",
            "                       items in utils.POSTPROCESS_WHEN.",
            "                       For compatibility, a single list is also accepted",
            "    print_to_file:     A dict with keys WHEN (same as forceprint) mapped to",
            "                       a list of tuples with (template, filename)",
            "    forcejson:         Force printing info_dict as JSON.",
            "    dump_single_json:  Force printing the info_dict of the whole playlist",
            "                       (or video) as a single JSON line.",
            "    force_write_download_archive: Force writing download archive regardless",
            "                       of 'skip_download' or 'simulate'.",
            "    simulate:          Do not download the video files. If unset (or None),",
            "                       simulate only if listsubtitles, listformats or list_thumbnails is used",
            "    format:            Video format code. see \"FORMAT SELECTION\" for more details.",
            "                       You can also pass a function. The function takes 'ctx' as",
            "                       argument and returns the formats to download.",
            "                       See \"build_format_selector\" for an implementation",
            "    allow_unplayable_formats:   Allow unplayable formats to be extracted and downloaded.",
            "    ignore_no_formats_error: Ignore \"No video formats\" error. Usefull for",
            "                       extracting metadata even if the video is not actually",
            "                       available for download (experimental)",
            "    format_sort:       A list of fields by which to sort the video formats.",
            "                       See \"Sorting Formats\" for more details.",
            "    format_sort_force: Force the given format_sort. see \"Sorting Formats\"",
            "                       for more details.",
            "    prefer_free_formats: Whether to prefer video formats with free containers",
            "                       over non-free ones of same quality.",
            "    allow_multiple_video_streams:   Allow multiple video streams to be merged",
            "                       into a single file",
            "    allow_multiple_audio_streams:   Allow multiple audio streams to be merged",
            "                       into a single file",
            "    check_formats      Whether to test if the formats are downloadable.",
            "                       Can be True (check all), False (check none),",
            "                       'selected' (check selected formats),",
            "                       or None (check only if requested by extractor)",
            "    paths:             Dictionary of output paths. The allowed keys are 'home'",
            "                       'temp' and the keys of OUTTMPL_TYPES (in utils/_utils.py)",
            "    outtmpl:           Dictionary of templates for output names. Allowed keys",
            "                       are 'default' and the keys of OUTTMPL_TYPES (in utils/_utils.py).",
            "                       For compatibility with youtube-dl, a single string can also be used",
            "    outtmpl_na_placeholder: Placeholder for unavailable meta fields.",
            "    restrictfilenames: Do not allow \"&\" and spaces in file names",
            "    trim_file_name:    Limit length of filename (extension excluded)",
            "    windowsfilenames:  Force the filenames to be windows compatible",
            "    ignoreerrors:      Do not stop on download/postprocessing errors.",
            "                       Can be 'only_download' to ignore only download errors.",
            "                       Default is 'only_download' for CLI, but False for API",
            "    skip_playlist_after_errors: Number of allowed failures until the rest of",
            "                       the playlist is skipped",
            "    allowed_extractors:  List of regexes to match against extractor names that are allowed",
            "    overwrites:        Overwrite all video and metadata files if True,",
            "                       overwrite only non-video files if None",
            "                       and don't overwrite any file if False",
            "    playlist_items:    Specific indices of playlist to download.",
            "    playlistrandom:    Download playlist items in random order.",
            "    lazy_playlist:     Process playlist entries as they are received.",
            "    matchtitle:        Download only matching titles.",
            "    rejecttitle:       Reject downloads for matching titles.",
            "    logger:            Log messages to a logging.Logger instance.",
            "    logtostderr:       Print everything to stderr instead of stdout.",
            "    consoletitle:      Display progress in console window's titlebar.",
            "    writedescription:  Write the video description to a .description file",
            "    writeinfojson:     Write the video description to a .info.json file",
            "    clean_infojson:    Remove internal metadata from the infojson",
            "    getcomments:       Extract video comments. This will not be written to disk",
            "                       unless writeinfojson is also given",
            "    writeannotations:  Write the video annotations to a .annotations.xml file",
            "    writethumbnail:    Write the thumbnail image to a file",
            "    allow_playlist_files: Whether to write playlists' description, infojson etc",
            "                       also to disk when using the 'write*' options",
            "    write_all_thumbnails:  Write all thumbnail formats to files",
            "    writelink:         Write an internet shortcut file, depending on the",
            "                       current platform (.url/.webloc/.desktop)",
            "    writeurllink:      Write a Windows internet shortcut file (.url)",
            "    writewebloclink:   Write a macOS internet shortcut file (.webloc)",
            "    writedesktoplink:  Write a Linux internet shortcut file (.desktop)",
            "    writesubtitles:    Write the video subtitles to a file",
            "    writeautomaticsub: Write the automatically generated subtitles to a file",
            "    listsubtitles:     Lists all available subtitles for the video",
            "    subtitlesformat:   The format code for subtitles",
            "    subtitleslangs:    List of languages of the subtitles to download (can be regex).",
            "                       The list may contain \"all\" to refer to all the available",
            "                       subtitles. The language can be prefixed with a \"-\" to",
            "                       exclude it from the requested languages, e.g. ['all', '-live_chat']",
            "    keepvideo:         Keep the video file after post-processing",
            "    daterange:         A utils.DateRange object, download only if the upload_date is in the range.",
            "    skip_download:     Skip the actual download of the video file",
            "    cachedir:          Location of the cache files in the filesystem.",
            "                       False to disable filesystem cache.",
            "    noplaylist:        Download single video instead of a playlist if in doubt.",
            "    age_limit:         An integer representing the user's age in years.",
            "                       Unsuitable videos for the given age are skipped.",
            "    min_views:         An integer representing the minimum view count the video",
            "                       must have in order to not be skipped.",
            "                       Videos without view count information are always",
            "                       downloaded. None for no limit.",
            "    max_views:         An integer representing the maximum view count.",
            "                       Videos that are more popular than that are not",
            "                       downloaded.",
            "                       Videos without view count information are always",
            "                       downloaded. None for no limit.",
            "    download_archive:  A set, or the name of a file where all downloads are recorded.",
            "                       Videos already present in the file are not downloaded again.",
            "    break_on_existing: Stop the download process after attempting to download a",
            "                       file that is in the archive.",
            "    break_per_url:     Whether break_on_reject and break_on_existing",
            "                       should act on each input URL as opposed to for the entire queue",
            "    cookiefile:        File name or text stream from where cookies should be read and dumped to",
            "    cookiesfrombrowser:  A tuple containing the name of the browser, the profile",
            "                       name/path from where cookies are loaded, the name of the keyring,",
            "                       and the container name, e.g. ('chrome', ) or",
            "                       ('vivaldi', 'default', 'BASICTEXT') or ('firefox', 'default', None, 'Meta')",
            "    legacyserverconnect: Explicitly allow HTTPS connection to servers that do not",
            "                       support RFC 5746 secure renegotiation",
            "    nocheckcertificate:  Do not verify SSL certificates",
            "    client_certificate:  Path to client certificate file in PEM format. May include the private key",
            "    client_certificate_key:  Path to private key file for client certificate",
            "    client_certificate_password:  Password for client certificate private key, if encrypted.",
            "                        If not provided and the key is encrypted, yt-dlp will ask interactively",
            "    prefer_insecure:   Use HTTP instead of HTTPS to retrieve information.",
            "                       (Only supported by some extractors)",
            "    enable_file_urls:  Enable file:// URLs. This is disabled by default for security reasons.",
            "    http_headers:      A dictionary of custom headers to be used for all requests",
            "    proxy:             URL of the proxy server to use",
            "    geo_verification_proxy:  URL of the proxy to use for IP address verification",
            "                       on geo-restricted sites.",
            "    socket_timeout:    Time to wait for unresponsive hosts, in seconds",
            "    bidi_workaround:   Work around buggy terminals without bidirectional text",
            "                       support, using fridibi",
            "    debug_printtraffic:Print out sent and received HTTP traffic",
            "    default_search:    Prepend this string if an input url is not valid.",
            "                       'auto' for elaborate guessing",
            "    encoding:          Use this encoding instead of the system-specified.",
            "    extract_flat:      Whether to resolve and process url_results further",
            "                       * False:     Always process. Default for API",
            "                       * True:      Never process",
            "                       * 'in_playlist': Do not process inside playlist/multi_video",
            "                       * 'discard': Always process, but don't return the result",
            "                                    from inside playlist/multi_video",
            "                       * 'discard_in_playlist': Same as \"discard\", but only for",
            "                                    playlists (not multi_video). Default for CLI",
            "    wait_for_video:    If given, wait for scheduled streams to become available.",
            "                       The value should be a tuple containing the range",
            "                       (min_secs, max_secs) to wait between retries",
            "    postprocessors:    A list of dictionaries, each with an entry",
            "                       * key:  The name of the postprocessor. See",
            "                               yt_dlp/postprocessor/__init__.py for a list.",
            "                       * when: When to run the postprocessor. Allowed values are",
            "                               the entries of utils.POSTPROCESS_WHEN",
            "                               Assumed to be 'post_process' if not given",
            "    progress_hooks:    A list of functions that get called on download",
            "                       progress, with a dictionary with the entries",
            "                       * status: One of \"downloading\", \"error\", or \"finished\".",
            "                                 Check this first and ignore unknown values.",
            "                       * info_dict: The extracted info_dict",
            "",
            "                       If status is one of \"downloading\", or \"finished\", the",
            "                       following properties may also be present:",
            "                       * filename: The final filename (always present)",
            "                       * tmpfilename: The filename we're currently writing to",
            "                       * downloaded_bytes: Bytes on disk",
            "                       * total_bytes: Size of the whole file, None if unknown",
            "                       * total_bytes_estimate: Guess of the eventual file size,",
            "                                               None if unavailable.",
            "                       * elapsed: The number of seconds since download started.",
            "                       * eta: The estimated time in seconds, None if unknown",
            "                       * speed: The download speed in bytes/second, None if",
            "                                unknown",
            "                       * fragment_index: The counter of the currently",
            "                                         downloaded video fragment.",
            "                       * fragment_count: The number of fragments (= individual",
            "                                         files that will be merged)",
            "",
            "                       Progress hooks are guaranteed to be called at least once",
            "                       (with status \"finished\") if the download is successful.",
            "    postprocessor_hooks:  A list of functions that get called on postprocessing",
            "                       progress, with a dictionary with the entries",
            "                       * status: One of \"started\", \"processing\", or \"finished\".",
            "                                 Check this first and ignore unknown values.",
            "                       * postprocessor: Name of the postprocessor",
            "                       * info_dict: The extracted info_dict",
            "",
            "                       Progress hooks are guaranteed to be called at least twice",
            "                       (with status \"started\" and \"finished\") if the processing is successful.",
            "    merge_output_format: \"/\" separated list of extensions to use when merging formats.",
            "    final_ext:         Expected final extension; used to detect when the file was",
            "                       already downloaded and converted",
            "    fixup:             Automatically correct known faults of the file.",
            "                       One of:",
            "                       - \"never\": do nothing",
            "                       - \"warn\": only emit a warning",
            "                       - \"detect_or_warn\": check whether we can do anything",
            "                                           about it, warn otherwise (default)",
            "    source_address:    Client-side IP address to bind to.",
            "    impersonate:       Client to impersonate for requests.",
            "                       An ImpersonateTarget (from yt_dlp.networking.impersonate)",
            "    sleep_interval_requests: Number of seconds to sleep between requests",
            "                       during extraction",
            "    sleep_interval:    Number of seconds to sleep before each download when",
            "                       used alone or a lower bound of a range for randomized",
            "                       sleep before each download (minimum possible number",
            "                       of seconds to sleep) when used along with",
            "                       max_sleep_interval.",
            "    max_sleep_interval:Upper bound of a range for randomized sleep before each",
            "                       download (maximum possible number of seconds to sleep).",
            "                       Must only be used along with sleep_interval.",
            "                       Actual sleep time will be a random float from range",
            "                       [sleep_interval; max_sleep_interval].",
            "    sleep_interval_subtitles: Number of seconds to sleep before each subtitle download",
            "    listformats:       Print an overview of available video formats and exit.",
            "    list_thumbnails:   Print a table of all thumbnails and exit.",
            "    match_filter:      A function that gets called for every video with the signature",
            "                       (info_dict, *, incomplete: bool) -> Optional[str]",
            "                       For backward compatibility with youtube-dl, the signature",
            "                       (info_dict) -> Optional[str] is also allowed.",
            "                       - If it returns a message, the video is ignored.",
            "                       - If it returns None, the video is downloaded.",
            "                       - If it returns utils.NO_DEFAULT, the user is interactively",
            "                         asked whether to download the video.",
            "                       - Raise utils.DownloadCancelled(msg) to abort remaining",
            "                         downloads when a video is rejected.",
            "                       match_filter_func in utils/_utils.py is one example for this.",
            "    color:             A Dictionary with output stream names as keys",
            "                       and their respective color policy as values.",
            "                       Can also just be a single color policy,",
            "                       in which case it applies to all outputs.",
            "                       Valid stream names are 'stdout' and 'stderr'.",
            "                       Valid color policies are one of 'always', 'auto', 'no_color' or 'never'.",
            "    geo_bypass:        Bypass geographic restriction via faking X-Forwarded-For",
            "                       HTTP header",
            "    geo_bypass_country:",
            "                       Two-letter ISO 3166-2 country code that will be used for",
            "                       explicit geographic restriction bypassing via faking",
            "                       X-Forwarded-For HTTP header",
            "    geo_bypass_ip_block:",
            "                       IP range in CIDR notation that will be used similarly to",
            "                       geo_bypass_country",
            "    external_downloader: A dictionary of protocol keys and the executable of the",
            "                       external downloader to use for it. The allowed protocols",
            "                       are default|http|ftp|m3u8|dash|rtsp|rtmp|mms.",
            "                       Set the value to 'native' to use the native downloader",
            "    compat_opts:       Compatibility options. See \"Differences in default behavior\".",
            "                       The following options do not work when used through the API:",
            "                       filename, abort-on-error, multistreams, no-live-chat, format-sort",
            "                       no-clean-infojson, no-playlist-metafiles, no-keep-subs, no-attach-info-json.",
            "                       Refer __init__.py for their implementation",
            "    progress_template: Dictionary of templates for progress outputs.",
            "                       Allowed keys are 'download', 'postprocess',",
            "                       'download-title' (console title) and 'postprocess-title'.",
            "                       The template is mapped on a dictionary with keys 'progress' and 'info'",
            "    retry_sleep_functions: Dictionary of functions that takes the number of attempts",
            "                       as argument and returns the time to sleep in seconds.",
            "                       Allowed keys are 'http', 'fragment', 'file_access'",
            "    download_ranges:   A callback function that gets called for every video with",
            "                       the signature (info_dict, ydl) -> Iterable[Section].",
            "                       Only the returned sections will be downloaded.",
            "                       Each Section is a dict with the following keys:",
            "                       * start_time: Start time of the section in seconds",
            "                       * end_time: End time of the section in seconds",
            "                       * title: Section title (Optional)",
            "                       * index: Section number (Optional)",
            "    force_keyframes_at_cuts: Re-encode the video when downloading ranges to get precise cuts",
            "    noprogress:        Do not print the progress bar",
            "    live_from_start:   Whether to download livestreams videos from the start",
            "",
            "    The following parameters are not used by YoutubeDL itself, they are used by",
            "    the downloader (see yt_dlp/downloader/common.py):",
            "    nopart, updatetime, buffersize, ratelimit, throttledratelimit, min_filesize,",
            "    max_filesize, test, noresizebuffer, retries, file_access_retries, fragment_retries,",
            "    continuedl, xattr_set_filesize, hls_use_mpegts, http_chunk_size,",
            "    external_downloader_args, concurrent_fragment_downloads, progress_delta.",
            "",
            "    The following options are used by the post processors:",
            "    ffmpeg_location:   Location of the ffmpeg/avconv binary; either the path",
            "                       to the binary or its containing directory.",
            "    postprocessor_args: A dictionary of postprocessor/executable keys (in lower case)",
            "                       and a list of additional command-line arguments for the",
            "                       postprocessor/executable. The dict can also have \"PP+EXE\" keys",
            "                       which are used when the given exe is used by the given PP.",
            "                       Use 'default' as the name for arguments to passed to all PP",
            "                       For compatibility with youtube-dl, a single list of args",
            "                       can also be used",
            "",
            "    The following options are used by the extractors:",
            "    extractor_retries: Number of times to retry for known errors (default: 3)",
            "    dynamic_mpd:       Whether to process dynamic DASH manifests (default: True)",
            "    hls_split_discontinuity: Split HLS playlists to different formats at",
            "                       discontinuities such as ad breaks (default: False)",
            "    extractor_args:    A dictionary of arguments to be passed to the extractors.",
            "                       See \"EXTRACTOR ARGUMENTS\" for details.",
            "                       E.g. {'youtube': {'skip': ['dash', 'hls']}}",
            "    mark_watched:      Mark videos watched (even with --simulate). Only for YouTube",
            "",
            "    The following options are deprecated and may be removed in the future:",
            "",
            "    break_on_reject:   Stop the download process when encountering a video that",
            "                       has been filtered out.",
            "                       - `raise DownloadCancelled(msg)` in match_filter instead",
            "    force_generic_extractor: Force downloader to use the generic extractor",
            "                       - Use allowed_extractors = ['generic', 'default']",
            "    playliststart:     - Use playlist_items",
            "                       Playlist item to start at.",
            "    playlistend:       - Use playlist_items",
            "                       Playlist item to end at.",
            "    playlistreverse:   - Use playlist_items",
            "                       Download playlist items in reverse order.",
            "    forceurl:          - Use forceprint",
            "                       Force printing final URL.",
            "    forcetitle:        - Use forceprint",
            "                       Force printing title.",
            "    forceid:           - Use forceprint",
            "                       Force printing ID.",
            "    forcethumbnail:    - Use forceprint",
            "                       Force printing thumbnail URL.",
            "    forcedescription:  - Use forceprint",
            "                       Force printing description.",
            "    forcefilename:     - Use forceprint",
            "                       Force printing final filename.",
            "    forceduration:     - Use forceprint",
            "                       Force printing duration.",
            "    allsubtitles:      - Use subtitleslangs = ['all']",
            "                       Downloads all the subtitles of the video",
            "                       (requires writesubtitles or writeautomaticsub)",
            "    include_ads:       - Doesn't work",
            "                       Download ads as well",
            "    call_home:         - Not implemented",
            "                       Boolean, true iff we are allowed to contact the",
            "                       yt-dlp servers for debugging.",
            "    post_hooks:        - Register a custom postprocessor",
            "                       A list of functions that get called as the final step",
            "                       for each video file, after all postprocessors have been",
            "                       called. The filename will be passed as the only argument.",
            "    hls_prefer_native: - Use external_downloader = {'m3u8': 'native'} or {'m3u8': 'ffmpeg'}.",
            "                       Use the native HLS downloader instead of ffmpeg/avconv",
            "                       if True, otherwise use ffmpeg/avconv if False, otherwise",
            "                       use downloader suggested by extractor if None.",
            "    prefer_ffmpeg:     - avconv support is deprecated",
            "                       If False, use avconv instead of ffmpeg if both are available,",
            "                       otherwise prefer ffmpeg.",
            "    youtube_include_dash_manifest: - Use extractor_args",
            "                       If True (default), DASH manifests and related",
            "                       data will be downloaded and processed by extractor.",
            "                       You can reduce network I/O by disabling it if you don't",
            "                       care about DASH. (only for youtube)",
            "    youtube_include_hls_manifest: - Use extractor_args",
            "                       If True (default), HLS manifests and related",
            "                       data will be downloaded and processed by extractor.",
            "                       You can reduce network I/O by disabling it if you don't",
            "                       care about HLS. (only for youtube)",
            "    no_color:          Same as `color='no_color'`",
            "    no_overwrites:     Same as `overwrites=False`",
            "    \"\"\"",
            "",
            "    _NUMERIC_FIELDS = {",
            "        'width', 'height', 'asr', 'audio_channels', 'fps',",
            "        'tbr', 'abr', 'vbr', 'filesize', 'filesize_approx',",
            "        'timestamp', 'release_timestamp',",
            "        'duration', 'view_count', 'like_count', 'dislike_count', 'repost_count',",
            "        'average_rating', 'comment_count', 'age_limit',",
            "        'start_time', 'end_time',",
            "        'chapter_number', 'season_number', 'episode_number',",
            "        'track_number', 'disc_number', 'release_year',",
            "    }",
            "",
            "    _format_fields = {",
            "        # NB: Keep in sync with the docstring of extractor/common.py",
            "        'url', 'manifest_url', 'manifest_stream_number', 'ext', 'format', 'format_id', 'format_note',",
            "        'width', 'height', 'aspect_ratio', 'resolution', 'dynamic_range', 'tbr', 'abr', 'acodec', 'asr', 'audio_channels',",
            "        'vbr', 'fps', 'vcodec', 'container', 'filesize', 'filesize_approx', 'rows', 'columns',",
            "        'player_url', 'protocol', 'fragment_base_url', 'fragments', 'is_from_start', 'is_dash_periods', 'request_data',",
            "        'preference', 'language', 'language_preference', 'quality', 'source_preference', 'cookies',",
            "        'http_headers', 'stretched_ratio', 'no_resume', 'has_drm', 'extra_param_to_segment_url', 'hls_aes', 'downloader_options',",
            "        'page_url', 'app', 'play_path', 'tc_url', 'flash_version', 'rtmp_live', 'rtmp_conn', 'rtmp_protocol', 'rtmp_real_time'",
            "    }",
            "    _deprecated_multivalue_fields = {",
            "        'album_artist': 'album_artists',",
            "        'artist': 'artists',",
            "        'composer': 'composers',",
            "        'creator': 'creators',",
            "        'genre': 'genres',",
            "    }",
            "    _format_selection_exts = {",
            "        'audio': set(MEDIA_EXTENSIONS.common_audio),",
            "        'video': set(MEDIA_EXTENSIONS.common_video + ('3gp', )),",
            "        'storyboards': set(MEDIA_EXTENSIONS.storyboards),",
            "    }",
            "",
            "    def __init__(self, params=None, auto_init=True):",
            "        \"\"\"Create a FileDownloader object with the given options.",
            "        @param auto_init    Whether to load the default extractors and print header (if verbose).",
            "                            Set to 'no_verbose_header' to not print the header",
            "        \"\"\"",
            "        if params is None:",
            "            params = {}",
            "        self.params = params",
            "        self._ies = {}",
            "        self._ies_instances = {}",
            "        self._pps = {k: [] for k in POSTPROCESS_WHEN}",
            "        self._printed_messages = set()",
            "        self._first_webpage_request = True",
            "        self._post_hooks = []",
            "        self._progress_hooks = []",
            "        self._postprocessor_hooks = []",
            "        self._download_retcode = 0",
            "        self._num_downloads = 0",
            "        self._num_videos = 0",
            "        self._playlist_level = 0",
            "        self._playlist_urls = set()",
            "        self.cache = Cache(self)",
            "        self.__header_cookies = []",
            "",
            "        stdout = sys.stderr if self.params.get('logtostderr') else sys.stdout",
            "        self._out_files = Namespace(",
            "            out=stdout,",
            "            error=sys.stderr,",
            "            screen=sys.stderr if self.params.get('quiet') else stdout,",
            "            console=None if compat_os_name == 'nt' else next(",
            "                filter(supports_terminal_sequences, (sys.stderr, sys.stdout)), None)",
            "        )",
            "",
            "        try:",
            "            windows_enable_vt_mode()",
            "        except Exception as e:",
            "            self.write_debug(f'Failed to enable VT mode: {e}')",
            "",
            "        if self.params.get('no_color'):",
            "            if self.params.get('color') is not None:",
            "                self.params.setdefault('_warnings', []).append(",
            "                    'Overwriting params from \"color\" with \"no_color\"')",
            "            self.params['color'] = 'no_color'",
            "",
            "        term_allow_color = os.getenv('TERM', '').lower() != 'dumb'",
            "        no_color = bool(os.getenv('NO_COLOR'))",
            "",
            "        def process_color_policy(stream):",
            "            stream_name = {sys.stdout: 'stdout', sys.stderr: 'stderr'}[stream]",
            "            policy = traverse_obj(self.params, ('color', (stream_name, None), {str}), get_all=False)",
            "            if policy in ('auto', None):",
            "                if term_allow_color and supports_terminal_sequences(stream):",
            "                    return 'no_color' if no_color else True",
            "                return False",
            "            assert policy in ('always', 'never', 'no_color'), policy",
            "            return {'always': True, 'never': False}.get(policy, policy)",
            "",
            "        self._allow_colors = Namespace(**{",
            "            name: process_color_policy(stream)",
            "            for name, stream in self._out_files.items_ if name != 'console'",
            "        })",
            "",
            "        system_deprecation = _get_system_deprecation()",
            "        if system_deprecation:",
            "            self.deprecated_feature(system_deprecation.replace('\\n', '\\n                    '))",
            "",
            "        if self.params.get('allow_unplayable_formats'):",
            "            self.report_warning(",
            "                f'You have asked for {self._format_err(\"UNPLAYABLE\", self.Styles.EMPHASIS)} formats to be listed/downloaded. '",
            "                'This is a developer option intended for debugging. \\n'",
            "                '         If you experience any issues while using this option, '",
            "                f'{self._format_err(\"DO NOT\", self.Styles.ERROR)} open a bug report')",
            "",
            "        if self.params.get('bidi_workaround', False):",
            "            try:",
            "                import pty",
            "                master, slave = pty.openpty()",
            "                width = shutil.get_terminal_size().columns",
            "                width_args = [] if width is None else ['-w', str(width)]",
            "                sp_kwargs = {'stdin': subprocess.PIPE, 'stdout': slave, 'stderr': self._out_files.error}",
            "                try:",
            "                    self._output_process = Popen(['bidiv'] + width_args, **sp_kwargs)",
            "                except OSError:",
            "                    self._output_process = Popen(['fribidi', '-c', 'UTF-8'] + width_args, **sp_kwargs)",
            "                self._output_channel = os.fdopen(master, 'rb')",
            "            except OSError as ose:",
            "                if ose.errno == errno.ENOENT:",
            "                    self.report_warning(",
            "                        'Could not find fribidi executable, ignoring --bidi-workaround. '",
            "                        'Make sure that  fribidi  is an executable file in one of the directories in your $PATH.')",
            "                else:",
            "                    raise",
            "",
            "        self.params['compat_opts'] = set(self.params.get('compat_opts', ()))",
            "        self.params['http_headers'] = HTTPHeaderDict(std_headers, self.params.get('http_headers'))",
            "        self._load_cookies(self.params['http_headers'].get('Cookie'))  # compat",
            "        self.params['http_headers'].pop('Cookie', None)",
            "",
            "        if auto_init and auto_init != 'no_verbose_header':",
            "            self.print_debug_header()",
            "",
            "        def check_deprecated(param, option, suggestion):",
            "            if self.params.get(param) is not None:",
            "                self.report_warning(f'{option} is deprecated. Use {suggestion} instead')",
            "                return True",
            "            return False",
            "",
            "        if check_deprecated('cn_verification_proxy', '--cn-verification-proxy', '--geo-verification-proxy'):",
            "            if self.params.get('geo_verification_proxy') is None:",
            "                self.params['geo_verification_proxy'] = self.params['cn_verification_proxy']",
            "",
            "        check_deprecated('autonumber', '--auto-number', '-o \"%(autonumber)s-%(title)s.%(ext)s\"')",
            "        check_deprecated('usetitle', '--title', '-o \"%(title)s-%(id)s.%(ext)s\"')",
            "        check_deprecated('useid', '--id', '-o \"%(id)s.%(ext)s\"')",
            "",
            "        for msg in self.params.get('_warnings', []):",
            "            self.report_warning(msg)",
            "        for msg in self.params.get('_deprecation_warnings', []):",
            "            self.deprecated_feature(msg)",
            "",
            "        if impersonate_target := self.params.get('impersonate'):",
            "            if not self._impersonate_target_available(impersonate_target):",
            "                raise YoutubeDLError(",
            "                    f'Impersonate target \"{impersonate_target}\" is not available. '",
            "                    f'Use --list-impersonate-targets to see available targets. '",
            "                    f'You may be missing dependencies required to support this target.')",
            "",
            "        if 'list-formats' in self.params['compat_opts']:",
            "            self.params['listformats_table'] = False",
            "",
            "        if 'overwrites' not in self.params and self.params.get('nooverwrites') is not None:",
            "            # nooverwrites was unnecessarily changed to overwrites",
            "            # in 0c3d0f51778b153f65c21906031c2e091fcfb641",
            "            # This ensures compatibility with both keys",
            "            self.params['overwrites'] = not self.params['nooverwrites']",
            "        elif self.params.get('overwrites') is None:",
            "            self.params.pop('overwrites', None)",
            "        else:",
            "            self.params['nooverwrites'] = not self.params['overwrites']",
            "",
            "        if self.params.get('simulate') is None and any((",
            "            self.params.get('list_thumbnails'),",
            "            self.params.get('listformats'),",
            "            self.params.get('listsubtitles'),",
            "        )):",
            "            self.params['simulate'] = 'list_only'",
            "",
            "        self.params.setdefault('forceprint', {})",
            "        self.params.setdefault('print_to_file', {})",
            "",
            "        # Compatibility with older syntax",
            "        if not isinstance(params['forceprint'], dict):",
            "            self.params['forceprint'] = {'video': params['forceprint']}",
            "",
            "        if auto_init:",
            "            self.add_default_info_extractors()",
            "",
            "        if (sys.platform != 'win32'",
            "                and sys.getfilesystemencoding() in ['ascii', 'ANSI_X3.4-1968']",
            "                and not self.params.get('restrictfilenames', False)):",
            "            # Unicode filesystem API will throw errors (#1474, #13027)",
            "            self.report_warning(",
            "                'Assuming --restrict-filenames since file system encoding '",
            "                'cannot encode all characters. '",
            "                'Set the LC_ALL environment variable to fix this.')",
            "            self.params['restrictfilenames'] = True",
            "",
            "        self._parse_outtmpl()",
            "",
            "        # Creating format selector here allows us to catch syntax errors before the extraction",
            "        self.format_selector = (",
            "            self.params.get('format') if self.params.get('format') in (None, '-')",
            "            else self.params['format'] if callable(self.params['format'])",
            "            else self.build_format_selector(self.params['format']))",
            "",
            "        hooks = {",
            "            'post_hooks': self.add_post_hook,",
            "            'progress_hooks': self.add_progress_hook,",
            "            'postprocessor_hooks': self.add_postprocessor_hook,",
            "        }",
            "        for opt, fn in hooks.items():",
            "            for ph in self.params.get(opt, []):",
            "                fn(ph)",
            "",
            "        for pp_def_raw in self.params.get('postprocessors', []):",
            "            pp_def = dict(pp_def_raw)",
            "            when = pp_def.pop('when', 'post_process')",
            "            self.add_post_processor(",
            "                get_postprocessor(pp_def.pop('key'))(self, **pp_def),",
            "                when=when)",
            "",
            "        def preload_download_archive(fn):",
            "            \"\"\"Preload the archive, if any is specified\"\"\"",
            "            archive = set()",
            "            if fn is None:",
            "                return archive",
            "            elif not is_path_like(fn):",
            "                return fn",
            "",
            "            self.write_debug(f'Loading archive file {fn!r}')",
            "            try:",
            "                with locked_file(fn, 'r', encoding='utf-8') as archive_file:",
            "                    for line in archive_file:",
            "                        archive.add(line.strip())",
            "            except OSError as ioe:",
            "                if ioe.errno != errno.ENOENT:",
            "                    raise",
            "            return archive",
            "",
            "        self.archive = preload_download_archive(self.params.get('download_archive'))",
            "",
            "    def warn_if_short_id(self, argv):",
            "        # short YouTube ID starting with dash?",
            "        idxs = [",
            "            i for i, a in enumerate(argv)",
            "            if re.match(r'^-[0-9A-Za-z_-]{10}$', a)]",
            "        if idxs:",
            "            correct_argv = (",
            "                ['yt-dlp']",
            "                + [a for i, a in enumerate(argv) if i not in idxs]",
            "                + ['--'] + [argv[i] for i in idxs]",
            "            )",
            "            self.report_warning(",
            "                'Long argument string detected. '",
            "                'Use -- to separate parameters and URLs, like this:\\n%s' %",
            "                args_to_str(correct_argv))",
            "",
            "    def add_info_extractor(self, ie):",
            "        \"\"\"Add an InfoExtractor object to the end of the list.\"\"\"",
            "        ie_key = ie.ie_key()",
            "        self._ies[ie_key] = ie",
            "        if not isinstance(ie, type):",
            "            self._ies_instances[ie_key] = ie",
            "            ie.set_downloader(self)",
            "",
            "    def get_info_extractor(self, ie_key):",
            "        \"\"\"",
            "        Get an instance of an IE with name ie_key, it will try to get one from",
            "        the _ies list, if there's no instance it will create a new one and add",
            "        it to the extractor list.",
            "        \"\"\"",
            "        ie = self._ies_instances.get(ie_key)",
            "        if ie is None:",
            "            ie = get_info_extractor(ie_key)()",
            "            self.add_info_extractor(ie)",
            "        return ie",
            "",
            "    def add_default_info_extractors(self):",
            "        \"\"\"",
            "        Add the InfoExtractors returned by gen_extractors to the end of the list",
            "        \"\"\"",
            "        all_ies = {ie.IE_NAME.lower(): ie for ie in gen_extractor_classes()}",
            "        all_ies['end'] = UnsupportedURLIE()",
            "        try:",
            "            ie_names = orderedSet_from_options(",
            "                self.params.get('allowed_extractors', ['default']), {",
            "                    'all': list(all_ies),",
            "                    'default': [name for name, ie in all_ies.items() if ie._ENABLED],",
            "                }, use_regex=True)",
            "        except re.error as e:",
            "            raise ValueError(f'Wrong regex for allowed_extractors: {e.pattern}')",
            "        for name in ie_names:",
            "            self.add_info_extractor(all_ies[name])",
            "        self.write_debug(f'Loaded {len(ie_names)} extractors')",
            "",
            "    def add_post_processor(self, pp, when='post_process'):",
            "        \"\"\"Add a PostProcessor object to the end of the chain.\"\"\"",
            "        assert when in POSTPROCESS_WHEN, f'Invalid when={when}'",
            "        self._pps[when].append(pp)",
            "        pp.set_downloader(self)",
            "",
            "    def add_post_hook(self, ph):",
            "        \"\"\"Add the post hook\"\"\"",
            "        self._post_hooks.append(ph)",
            "",
            "    def add_progress_hook(self, ph):",
            "        \"\"\"Add the download progress hook\"\"\"",
            "        self._progress_hooks.append(ph)",
            "",
            "    def add_postprocessor_hook(self, ph):",
            "        \"\"\"Add the postprocessing progress hook\"\"\"",
            "        self._postprocessor_hooks.append(ph)",
            "        for pps in self._pps.values():",
            "            for pp in pps:",
            "                pp.add_progress_hook(ph)",
            "",
            "    def _bidi_workaround(self, message):",
            "        if not hasattr(self, '_output_channel'):",
            "            return message",
            "",
            "        assert hasattr(self, '_output_process')",
            "        assert isinstance(message, str)",
            "        line_count = message.count('\\n') + 1",
            "        self._output_process.stdin.write((message + '\\n').encode())",
            "        self._output_process.stdin.flush()",
            "        res = ''.join(self._output_channel.readline().decode()",
            "                      for _ in range(line_count))",
            "        return res[:-len('\\n')]",
            "",
            "    def _write_string(self, message, out=None, only_once=False):",
            "        if only_once:",
            "            if message in self._printed_messages:",
            "                return",
            "            self._printed_messages.add(message)",
            "        write_string(message, out=out, encoding=self.params.get('encoding'))",
            "",
            "    def to_stdout(self, message, skip_eol=False, quiet=None):",
            "        \"\"\"Print message to stdout\"\"\"",
            "        if quiet is not None:",
            "            self.deprecation_warning('\"YoutubeDL.to_stdout\" no longer accepts the argument quiet. '",
            "                                     'Use \"YoutubeDL.to_screen\" instead')",
            "        if skip_eol is not False:",
            "            self.deprecation_warning('\"YoutubeDL.to_stdout\" no longer accepts the argument skip_eol. '",
            "                                     'Use \"YoutubeDL.to_screen\" instead')",
            "        self._write_string(f'{self._bidi_workaround(message)}\\n', self._out_files.out)",
            "",
            "    def to_screen(self, message, skip_eol=False, quiet=None, only_once=False):",
            "        \"\"\"Print message to screen if not in quiet mode\"\"\"",
            "        if self.params.get('logger'):",
            "            self.params['logger'].debug(message)",
            "            return",
            "        if (self.params.get('quiet') if quiet is None else quiet) and not self.params.get('verbose'):",
            "            return",
            "        self._write_string(",
            "            '%s%s' % (self._bidi_workaround(message), ('' if skip_eol else '\\n')),",
            "            self._out_files.screen, only_once=only_once)",
            "",
            "    def to_stderr(self, message, only_once=False):",
            "        \"\"\"Print message to stderr\"\"\"",
            "        assert isinstance(message, str)",
            "        if self.params.get('logger'):",
            "            self.params['logger'].error(message)",
            "        else:",
            "            self._write_string(f'{self._bidi_workaround(message)}\\n', self._out_files.error, only_once=only_once)",
            "",
            "    def _send_console_code(self, code):",
            "        if compat_os_name == 'nt' or not self._out_files.console:",
            "            return",
            "        self._write_string(code, self._out_files.console)",
            "",
            "    def to_console_title(self, message):",
            "        if not self.params.get('consoletitle', False):",
            "            return",
            "        message = remove_terminal_sequences(message)",
            "        if compat_os_name == 'nt':",
            "            if ctypes.windll.kernel32.GetConsoleWindow():",
            "                # c_wchar_p() might not be necessary if `message` is",
            "                # already of type unicode()",
            "                ctypes.windll.kernel32.SetConsoleTitleW(ctypes.c_wchar_p(message))",
            "        else:",
            "            self._send_console_code(f'\\033]0;{message}\\007')",
            "",
            "    def save_console_title(self):",
            "        if not self.params.get('consoletitle') or self.params.get('simulate'):",
            "            return",
            "        self._send_console_code('\\033[22;0t')  # Save the title on stack",
            "",
            "    def restore_console_title(self):",
            "        if not self.params.get('consoletitle') or self.params.get('simulate'):",
            "            return",
            "        self._send_console_code('\\033[23;0t')  # Restore the title from stack",
            "",
            "    def __enter__(self):",
            "        self.save_console_title()",
            "        return self",
            "",
            "    def save_cookies(self):",
            "        if self.params.get('cookiefile') is not None:",
            "            self.cookiejar.save()",
            "",
            "    def __exit__(self, *args):",
            "        self.restore_console_title()",
            "        self.close()",
            "",
            "    def close(self):",
            "        self.save_cookies()",
            "        if '_request_director' in self.__dict__:",
            "            self._request_director.close()",
            "            del self._request_director",
            "",
            "    def trouble(self, message=None, tb=None, is_error=True):",
            "        \"\"\"Determine action to take when a download problem appears.",
            "",
            "        Depending on if the downloader has been configured to ignore",
            "        download errors or not, this method may throw an exception or",
            "        not when errors are found, after printing the message.",
            "",
            "        @param tb          If given, is additional traceback information",
            "        @param is_error    Whether to raise error according to ignorerrors",
            "        \"\"\"",
            "        if message is not None:",
            "            self.to_stderr(message)",
            "        if self.params.get('verbose'):",
            "            if tb is None:",
            "                if sys.exc_info()[0]:  # if .trouble has been called from an except block",
            "                    tb = ''",
            "                    if hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:",
            "                        tb += ''.join(traceback.format_exception(*sys.exc_info()[1].exc_info))",
            "                    tb += encode_compat_str(traceback.format_exc())",
            "                else:",
            "                    tb_data = traceback.format_list(traceback.extract_stack())",
            "                    tb = ''.join(tb_data)",
            "            if tb:",
            "                self.to_stderr(tb)",
            "        if not is_error:",
            "            return",
            "        if not self.params.get('ignoreerrors'):",
            "            if sys.exc_info()[0] and hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:",
            "                exc_info = sys.exc_info()[1].exc_info",
            "            else:",
            "                exc_info = sys.exc_info()",
            "            raise DownloadError(message, exc_info)",
            "        self._download_retcode = 1",
            "",
            "    Styles = Namespace(",
            "        HEADERS='yellow',",
            "        EMPHASIS='light blue',",
            "        FILENAME='green',",
            "        ID='green',",
            "        DELIM='blue',",
            "        ERROR='red',",
            "        BAD_FORMAT='light red',",
            "        WARNING='yellow',",
            "        SUPPRESS='light black',",
            "    )",
            "",
            "    def _format_text(self, handle, allow_colors, text, f, fallback=None, *, test_encoding=False):",
            "        text = str(text)",
            "        if test_encoding:",
            "            original_text = text",
            "            # handle.encoding can be None. See https://github.com/yt-dlp/yt-dlp/issues/2711",
            "            encoding = self.params.get('encoding') or getattr(handle, 'encoding', None) or 'ascii'",
            "            text = text.encode(encoding, 'ignore').decode(encoding)",
            "            if fallback is not None and text != original_text:",
            "                text = fallback",
            "        return format_text(text, f) if allow_colors is True else text if fallback is None else fallback",
            "",
            "    def _format_out(self, *args, **kwargs):",
            "        return self._format_text(self._out_files.out, self._allow_colors.out, *args, **kwargs)",
            "",
            "    def _format_screen(self, *args, **kwargs):",
            "        return self._format_text(self._out_files.screen, self._allow_colors.screen, *args, **kwargs)",
            "",
            "    def _format_err(self, *args, **kwargs):",
            "        return self._format_text(self._out_files.error, self._allow_colors.error, *args, **kwargs)",
            "",
            "    def report_warning(self, message, only_once=False):",
            "        '''",
            "        Print the message to stderr, it will be prefixed with 'WARNING:'",
            "        If stderr is a tty file the 'WARNING:' will be colored",
            "        '''",
            "        if self.params.get('logger') is not None:",
            "            self.params['logger'].warning(message)",
            "        else:",
            "            if self.params.get('no_warnings'):",
            "                return",
            "            self.to_stderr(f'{self._format_err(\"WARNING:\", self.Styles.WARNING)} {message}', only_once)",
            "",
            "    def deprecation_warning(self, message, *, stacklevel=0):",
            "        deprecation_warning(",
            "            message, stacklevel=stacklevel + 1, printer=self.report_error, is_error=False)",
            "",
            "    def deprecated_feature(self, message):",
            "        if self.params.get('logger') is not None:",
            "            self.params['logger'].warning(f'Deprecated Feature: {message}')",
            "        self.to_stderr(f'{self._format_err(\"Deprecated Feature:\", self.Styles.ERROR)} {message}', True)",
            "",
            "    def report_error(self, message, *args, **kwargs):",
            "        '''",
            "        Do the same as trouble, but prefixes the message with 'ERROR:', colored",
            "        in red if stderr is a tty file.",
            "        '''",
            "        self.trouble(f'{self._format_err(\"ERROR:\", self.Styles.ERROR)} {message}', *args, **kwargs)",
            "",
            "    def write_debug(self, message, only_once=False):",
            "        '''Log debug message or Print message to stderr'''",
            "        if not self.params.get('verbose', False):",
            "            return",
            "        message = f'[debug] {message}'",
            "        if self.params.get('logger'):",
            "            self.params['logger'].debug(message)",
            "        else:",
            "            self.to_stderr(message, only_once)",
            "",
            "    def report_file_already_downloaded(self, file_name):",
            "        \"\"\"Report file has already been fully downloaded.\"\"\"",
            "        try:",
            "            self.to_screen('[download] %s has already been downloaded' % file_name)",
            "        except UnicodeEncodeError:",
            "            self.to_screen('[download] The file has already been downloaded')",
            "",
            "    def report_file_delete(self, file_name):",
            "        \"\"\"Report that existing file will be deleted.\"\"\"",
            "        try:",
            "            self.to_screen('Deleting existing file %s' % file_name)",
            "        except UnicodeEncodeError:",
            "            self.to_screen('Deleting existing file')",
            "",
            "    def raise_no_formats(self, info, forced=False, *, msg=None):",
            "        has_drm = info.get('_has_drm')",
            "        ignored, expected = self.params.get('ignore_no_formats_error'), bool(msg)",
            "        msg = msg or has_drm and 'This video is DRM protected' or 'No video formats found!'",
            "        if forced or not ignored:",
            "            raise ExtractorError(msg, video_id=info['id'], ie=info['extractor'],",
            "                                 expected=has_drm or ignored or expected)",
            "        else:",
            "            self.report_warning(msg)",
            "",
            "    def parse_outtmpl(self):",
            "        self.deprecation_warning('\"YoutubeDL.parse_outtmpl\" is deprecated and may be removed in a future version')",
            "        self._parse_outtmpl()",
            "        return self.params['outtmpl']",
            "",
            "    def _parse_outtmpl(self):",
            "        sanitize = IDENTITY",
            "        if self.params.get('restrictfilenames'):  # Remove spaces in the default template",
            "            sanitize = lambda x: x.replace(' - ', ' ').replace(' ', '-')",
            "",
            "        outtmpl = self.params.setdefault('outtmpl', {})",
            "        if not isinstance(outtmpl, dict):",
            "            self.params['outtmpl'] = outtmpl = {'default': outtmpl}",
            "        outtmpl.update({k: sanitize(v) for k, v in DEFAULT_OUTTMPL.items() if outtmpl.get(k) is None})",
            "",
            "    def get_output_path(self, dir_type='', filename=None):",
            "        paths = self.params.get('paths', {})",
            "        assert isinstance(paths, dict), '\"paths\" parameter must be a dictionary'",
            "        path = os.path.join(",
            "            expand_path(paths.get('home', '').strip()),",
            "            expand_path(paths.get(dir_type, '').strip()) if dir_type else '',",
            "            filename or '')",
            "        return sanitize_path(path, force=self.params.get('windowsfilenames'))",
            "",
            "    @staticmethod",
            "    def _outtmpl_expandpath(outtmpl):",
            "        # expand_path translates '%%' into '%' and '$$' into '$'",
            "        # correspondingly that is not what we want since we need to keep",
            "        # '%%' intact for template dict substitution step. Working around",
            "        # with boundary-alike separator hack.",
            "        sep = ''.join(random.choices(string.ascii_letters, k=32))",
            "        outtmpl = outtmpl.replace('%%', f'%{sep}%').replace('$$', f'${sep}$')",
            "",
            "        # outtmpl should be expand_path'ed before template dict substitution",
            "        # because meta fields may contain env variables we don't want to",
            "        # be expanded. E.g. for outtmpl \"%(title)s.%(ext)s\" and",
            "        # title \"Hello $PATH\", we don't want `$PATH` to be expanded.",
            "        return expand_path(outtmpl).replace(sep, '')",
            "",
            "    @staticmethod",
            "    def escape_outtmpl(outtmpl):",
            "        ''' Escape any remaining strings like %s, %abc% etc. '''",
            "        return re.sub(",
            "            STR_FORMAT_RE_TMPL.format('', '(?![%(\\0])'),",
            "            lambda mobj: ('' if mobj.group('has_key') else '%') + mobj.group(0),",
            "            outtmpl)",
            "",
            "    @classmethod",
            "    def validate_outtmpl(cls, outtmpl):",
            "        ''' @return None or Exception object '''",
            "        outtmpl = re.sub(",
            "            STR_FORMAT_RE_TMPL.format('[^)]*', '[ljhqBUDS]'),",
            "            lambda mobj: f'{mobj.group(0)[:-1]}s',",
            "            cls._outtmpl_expandpath(outtmpl))",
            "        try:",
            "            cls.escape_outtmpl(outtmpl) % collections.defaultdict(int)",
            "            return None",
            "        except ValueError as err:",
            "            return err",
            "",
            "    @staticmethod",
            "    def _copy_infodict(info_dict):",
            "        info_dict = dict(info_dict)",
            "        info_dict.pop('__postprocessors', None)",
            "        info_dict.pop('__pending_error', None)",
            "        return info_dict",
            "",
            "    def prepare_outtmpl(self, outtmpl, info_dict, sanitize=False):",
            "        \"\"\" Make the outtmpl and info_dict suitable for substitution: ydl.escape_outtmpl(outtmpl) % info_dict",
            "        @param sanitize    Whether to sanitize the output as a filename.",
            "                           For backward compatibility, a function can also be passed",
            "        \"\"\"",
            "",
            "        info_dict.setdefault('epoch', int(time.time()))  # keep epoch consistent once set",
            "",
            "        info_dict = self._copy_infodict(info_dict)",
            "        info_dict['duration_string'] = (  # %(duration>%H-%M-%S)s is wrong if duration > 24hrs",
            "            formatSeconds(info_dict['duration'], '-' if sanitize else ':')",
            "            if info_dict.get('duration', None) is not None",
            "            else None)",
            "        info_dict['autonumber'] = int(self.params.get('autonumber_start', 1) - 1 + self._num_downloads)",
            "        info_dict['video_autonumber'] = self._num_videos",
            "        if info_dict.get('resolution') is None:",
            "            info_dict['resolution'] = self.format_resolution(info_dict, default=None)",
            "",
            "        # For fields playlist_index, playlist_autonumber and autonumber convert all occurrences",
            "        # of %(field)s to %(field)0Nd for backward compatibility",
            "        field_size_compat_map = {",
            "            'playlist_index': number_of_digits(info_dict.get('__last_playlist_index') or 0),",
            "            'playlist_autonumber': number_of_digits(info_dict.get('n_entries') or 0),",
            "            'autonumber': self.params.get('autonumber_size') or 5,",
            "        }",
            "",
            "        TMPL_DICT = {}",
            "        EXTERNAL_FORMAT_RE = re.compile(STR_FORMAT_RE_TMPL.format('[^)]*', f'[{STR_FORMAT_TYPES}ljhqBUDS]'))",
            "        MATH_FUNCTIONS = {",
            "            '+': float.__add__,",
            "            '-': float.__sub__,",
            "            '*': float.__mul__,",
            "        }",
            "        # Field is of the form key1.key2...",
            "        # where keys (except first) can be string, int, slice or \"{field, ...}\"",
            "        FIELD_INNER_RE = r'(?:\\w+|%(num)s|%(num)s?(?::%(num)s?){1,2})' % {'num': r'(?:-?\\d+)'}",
            "        FIELD_RE = r'\\w*(?:\\.(?:%(inner)s|{%(field)s(?:,%(field)s)*}))*' % {",
            "            'inner': FIELD_INNER_RE,",
            "            'field': rf'\\w*(?:\\.{FIELD_INNER_RE})*'",
            "        }",
            "        MATH_FIELD_RE = rf'(?:{FIELD_RE}|-?{NUMBER_RE})'",
            "        MATH_OPERATORS_RE = r'(?:%s)' % '|'.join(map(re.escape, MATH_FUNCTIONS.keys()))",
            "        INTERNAL_FORMAT_RE = re.compile(rf'''(?xs)",
            "            (?P<negate>-)?",
            "            (?P<fields>{FIELD_RE})",
            "            (?P<maths>(?:{MATH_OPERATORS_RE}{MATH_FIELD_RE})*)",
            "            (?:>(?P<strf_format>.+?))?",
            "            (?P<remaining>",
            "                (?P<alternate>(?<!\\\\),[^|&)]+)?",
            "                (?:&(?P<replacement>.*?))?",
            "                (?:\\|(?P<default>.*?))?",
            "            )$''')",
            "",
            "        def _from_user_input(field):",
            "            if field == ':':",
            "                return ...",
            "            elif ':' in field:",
            "                return slice(*map(int_or_none, field.split(':')))",
            "            elif int_or_none(field) is not None:",
            "                return int(field)",
            "            return field",
            "",
            "        def _traverse_infodict(fields):",
            "            fields = [f for x in re.split(r'\\.({.+?})\\.?', fields)",
            "                      for f in ([x] if x.startswith('{') else x.split('.'))]",
            "            for i in (0, -1):",
            "                if fields and not fields[i]:",
            "                    fields.pop(i)",
            "",
            "            for i, f in enumerate(fields):",
            "                if not f.startswith('{'):",
            "                    fields[i] = _from_user_input(f)",
            "                    continue",
            "                assert f.endswith('}'), f'No closing brace for {f} in {fields}'",
            "                fields[i] = {k: list(map(_from_user_input, k.split('.'))) for k in f[1:-1].split(',')}",
            "",
            "            return traverse_obj(info_dict, fields, traverse_string=True)",
            "",
            "        def get_value(mdict):",
            "            # Object traversal",
            "            value = _traverse_infodict(mdict['fields'])",
            "            # Negative",
            "            if mdict['negate']:",
            "                value = float_or_none(value)",
            "                if value is not None:",
            "                    value *= -1",
            "            # Do maths",
            "            offset_key = mdict['maths']",
            "            if offset_key:",
            "                value = float_or_none(value)",
            "                operator = None",
            "                while offset_key:",
            "                    item = re.match(",
            "                        MATH_FIELD_RE if operator else MATH_OPERATORS_RE,",
            "                        offset_key).group(0)",
            "                    offset_key = offset_key[len(item):]",
            "                    if operator is None:",
            "                        operator = MATH_FUNCTIONS[item]",
            "                        continue",
            "                    item, multiplier = (item[1:], -1) if item[0] == '-' else (item, 1)",
            "                    offset = float_or_none(item)",
            "                    if offset is None:",
            "                        offset = float_or_none(_traverse_infodict(item))",
            "                    try:",
            "                        value = operator(value, multiplier * offset)",
            "                    except (TypeError, ZeroDivisionError):",
            "                        return None",
            "                    operator = None",
            "            # Datetime formatting",
            "            if mdict['strf_format']:",
            "                value = strftime_or_none(value, mdict['strf_format'].replace('\\\\,', ','))",
            "",
            "            # XXX: Workaround for https://github.com/yt-dlp/yt-dlp/issues/4485",
            "            if sanitize and value == '':",
            "                value = None",
            "            return value",
            "",
            "        na = self.params.get('outtmpl_na_placeholder', 'NA')",
            "",
            "        def filename_sanitizer(key, value, restricted=self.params.get('restrictfilenames')):",
            "            return sanitize_filename(str(value), restricted=restricted, is_id=(",
            "                bool(re.search(r'(^|[_.])id(\\.|$)', key))",
            "                if 'filename-sanitization' in self.params['compat_opts']",
            "                else NO_DEFAULT))",
            "",
            "        sanitizer = sanitize if callable(sanitize) else filename_sanitizer",
            "        sanitize = bool(sanitize)",
            "",
            "        def _dumpjson_default(obj):",
            "            if isinstance(obj, (set, LazyList)):",
            "                return list(obj)",
            "            return repr(obj)",
            "",
            "        class _ReplacementFormatter(string.Formatter):",
            "            def get_field(self, field_name, args, kwargs):",
            "                if field_name.isdigit():",
            "                    return args[0], -1",
            "                raise ValueError('Unsupported field')",
            "",
            "        replacement_formatter = _ReplacementFormatter()",
            "",
            "        def create_key(outer_mobj):",
            "            if not outer_mobj.group('has_key'):",
            "                return outer_mobj.group(0)",
            "            key = outer_mobj.group('key')",
            "            mobj = re.match(INTERNAL_FORMAT_RE, key)",
            "            value, replacement, default, last_field = None, None, na, ''",
            "            while mobj:",
            "                mobj = mobj.groupdict()",
            "                default = mobj['default'] if mobj['default'] is not None else default",
            "                value = get_value(mobj)",
            "                last_field, replacement = mobj['fields'], mobj['replacement']",
            "                if value is None and mobj['alternate']:",
            "                    mobj = re.match(INTERNAL_FORMAT_RE, mobj['remaining'][1:])",
            "                else:",
            "                    break",
            "",
            "            if None not in (value, replacement):",
            "                try:",
            "                    value = replacement_formatter.format(replacement, value)",
            "                except ValueError:",
            "                    value, default = None, na",
            "",
            "            fmt = outer_mobj.group('format')",
            "            if fmt == 's' and last_field in field_size_compat_map.keys() and isinstance(value, int):",
            "                fmt = f'0{field_size_compat_map[last_field]:d}d'",
            "",
            "            flags = outer_mobj.group('conversion') or ''",
            "            str_fmt = f'{fmt[:-1]}s'",
            "            if value is None:",
            "                value, fmt = default, 's'",
            "            elif fmt[-1] == 'l':  # list",
            "                delim = '\\n' if '#' in flags else ', '",
            "                value, fmt = delim.join(map(str, variadic(value, allowed_types=(str, bytes)))), str_fmt",
            "            elif fmt[-1] == 'j':  # json",
            "                value, fmt = json.dumps(",
            "                    value, default=_dumpjson_default,",
            "                    indent=4 if '#' in flags else None, ensure_ascii='+' not in flags), str_fmt",
            "            elif fmt[-1] == 'h':  # html",
            "                value, fmt = escapeHTML(str(value)), str_fmt",
            "            elif fmt[-1] == 'q':  # quoted",
            "                value = map(str, variadic(value) if '#' in flags else [value])",
            "                value, fmt = ' '.join(map(compat_shlex_quote, value)), str_fmt",
            "            elif fmt[-1] == 'B':  # bytes",
            "                value = f'%{str_fmt}'.encode() % str(value).encode()",
            "                value, fmt = value.decode('utf-8', 'ignore'), 's'",
            "            elif fmt[-1] == 'U':  # unicode normalized",
            "                value, fmt = unicodedata.normalize(",
            "                    # \"+\" = compatibility equivalence, \"#\" = NFD",
            "                    'NF%s%s' % ('K' if '+' in flags else '', 'D' if '#' in flags else 'C'),",
            "                    value), str_fmt",
            "            elif fmt[-1] == 'D':  # decimal suffix",
            "                num_fmt, fmt = fmt[:-1].replace('#', ''), 's'",
            "                value = format_decimal_suffix(value, f'%{num_fmt}f%s' if num_fmt else '%d%s',",
            "                                              factor=1024 if '#' in flags else 1000)",
            "            elif fmt[-1] == 'S':  # filename sanitization",
            "                value, fmt = filename_sanitizer(last_field, value, restricted='#' in flags), str_fmt",
            "            elif fmt[-1] == 'c':",
            "                if value:",
            "                    value = str(value)[0]",
            "                else:",
            "                    fmt = str_fmt",
            "            elif fmt[-1] not in 'rsa':  # numeric",
            "                value = float_or_none(value)",
            "                if value is None:",
            "                    value, fmt = default, 's'",
            "",
            "            if sanitize:",
            "                # If value is an object, sanitize might convert it to a string",
            "                # So we convert it to repr first",
            "                if fmt[-1] == 'r':",
            "                    value, fmt = repr(value), str_fmt",
            "                elif fmt[-1] == 'a':",
            "                    value, fmt = ascii(value), str_fmt",
            "                if fmt[-1] in 'csra':",
            "                    value = sanitizer(last_field, value)",
            "",
            "            key = '%s\\0%s' % (key.replace('%', '%\\0'), outer_mobj.group('format'))",
            "            TMPL_DICT[key] = value",
            "            return '{prefix}%({key}){fmt}'.format(key=key, fmt=fmt, prefix=outer_mobj.group('prefix'))",
            "",
            "        return EXTERNAL_FORMAT_RE.sub(create_key, outtmpl), TMPL_DICT",
            "",
            "    def evaluate_outtmpl(self, outtmpl, info_dict, *args, **kwargs):",
            "        outtmpl, info_dict = self.prepare_outtmpl(outtmpl, info_dict, *args, **kwargs)",
            "        return self.escape_outtmpl(outtmpl) % info_dict",
            "",
            "    def _prepare_filename(self, info_dict, *, outtmpl=None, tmpl_type=None):",
            "        assert None in (outtmpl, tmpl_type), 'outtmpl and tmpl_type are mutually exclusive'",
            "        if outtmpl is None:",
            "            outtmpl = self.params['outtmpl'].get(tmpl_type or 'default', self.params['outtmpl']['default'])",
            "        try:",
            "            outtmpl = self._outtmpl_expandpath(outtmpl)",
            "            filename = self.evaluate_outtmpl(outtmpl, info_dict, True)",
            "            if not filename:",
            "                return None",
            "",
            "            if tmpl_type in ('', 'temp'):",
            "                final_ext, ext = self.params.get('final_ext'), info_dict.get('ext')",
            "                if final_ext and ext and final_ext != ext and filename.endswith(f'.{final_ext}'):",
            "                    filename = replace_extension(filename, ext, final_ext)",
            "            elif tmpl_type:",
            "                force_ext = OUTTMPL_TYPES[tmpl_type]",
            "                if force_ext:",
            "                    filename = replace_extension(filename, force_ext, info_dict.get('ext'))",
            "",
            "            # https://github.com/blackjack4494/youtube-dlc/issues/85",
            "            trim_file_name = self.params.get('trim_file_name', False)",
            "            if trim_file_name:",
            "                no_ext, *ext = filename.rsplit('.', 2)",
            "                filename = join_nonempty(no_ext[:trim_file_name], *ext, delim='.')",
            "",
            "            return filename",
            "        except ValueError as err:",
            "            self.report_error('Error in output template: ' + str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')",
            "            return None",
            "",
            "    def prepare_filename(self, info_dict, dir_type='', *, outtmpl=None, warn=False):",
            "        \"\"\"Generate the output filename\"\"\"",
            "        if outtmpl:",
            "            assert not dir_type, 'outtmpl and dir_type are mutually exclusive'",
            "            dir_type = None",
            "        filename = self._prepare_filename(info_dict, tmpl_type=dir_type, outtmpl=outtmpl)",
            "        if not filename and dir_type not in ('', 'temp'):",
            "            return ''",
            "",
            "        if warn:",
            "            if not self.params.get('paths'):",
            "                pass",
            "            elif filename == '-':",
            "                self.report_warning('--paths is ignored when an outputting to stdout', only_once=True)",
            "            elif os.path.isabs(filename):",
            "                self.report_warning('--paths is ignored since an absolute path is given in output template', only_once=True)",
            "        if filename == '-' or not filename:",
            "            return filename",
            "",
            "        return self.get_output_path(dir_type, filename)",
            "",
            "    def _match_entry(self, info_dict, incomplete=False, silent=False):",
            "        \"\"\"Returns None if the file should be downloaded\"\"\"",
            "        _type = 'video' if 'playlist-match-filter' in self.params['compat_opts'] else info_dict.get('_type', 'video')",
            "        assert incomplete or _type == 'video', 'Only video result can be considered complete'",
            "",
            "        video_title = info_dict.get('title', info_dict.get('id', 'entry'))",
            "",
            "        def check_filter():",
            "            if _type in ('playlist', 'multi_video'):",
            "                return",
            "            elif _type in ('url', 'url_transparent') and not try_call(",
            "                    lambda: self.get_info_extractor(info_dict['ie_key']).is_single_video(info_dict['url'])):",
            "                return",
            "",
            "            if 'title' in info_dict:",
            "                # This can happen when we're just evaluating the playlist",
            "                title = info_dict['title']",
            "                matchtitle = self.params.get('matchtitle', False)",
            "                if matchtitle:",
            "                    if not re.search(matchtitle, title, re.IGNORECASE):",
            "                        return '\"' + title + '\" title did not match pattern \"' + matchtitle + '\"'",
            "                rejecttitle = self.params.get('rejecttitle', False)",
            "                if rejecttitle:",
            "                    if re.search(rejecttitle, title, re.IGNORECASE):",
            "                        return '\"' + title + '\" title matched reject pattern \"' + rejecttitle + '\"'",
            "",
            "            date = info_dict.get('upload_date')",
            "            if date is not None:",
            "                dateRange = self.params.get('daterange', DateRange())",
            "                if date not in dateRange:",
            "                    return f'{date_from_str(date).isoformat()} upload date is not in range {dateRange}'",
            "            view_count = info_dict.get('view_count')",
            "            if view_count is not None:",
            "                min_views = self.params.get('min_views')",
            "                if min_views is not None and view_count < min_views:",
            "                    return 'Skipping %s, because it has not reached minimum view count (%d/%d)' % (video_title, view_count, min_views)",
            "                max_views = self.params.get('max_views')",
            "                if max_views is not None and view_count > max_views:",
            "                    return 'Skipping %s, because it has exceeded the maximum view count (%d/%d)' % (video_title, view_count, max_views)",
            "            if age_restricted(info_dict.get('age_limit'), self.params.get('age_limit')):",
            "                return 'Skipping \"%s\" because it is age restricted' % video_title",
            "",
            "            match_filter = self.params.get('match_filter')",
            "            if match_filter is None:",
            "                return None",
            "",
            "            cancelled = None",
            "            try:",
            "                try:",
            "                    ret = match_filter(info_dict, incomplete=incomplete)",
            "                except TypeError:",
            "                    # For backward compatibility",
            "                    ret = None if incomplete else match_filter(info_dict)",
            "            except DownloadCancelled as err:",
            "                if err.msg is not NO_DEFAULT:",
            "                    raise",
            "                ret, cancelled = err.msg, err",
            "",
            "            if ret is NO_DEFAULT:",
            "                while True:",
            "                    filename = self._format_screen(self.prepare_filename(info_dict), self.Styles.FILENAME)",
            "                    reply = input(self._format_screen(",
            "                        f'Download \"{filename}\"? (Y/n): ', self.Styles.EMPHASIS)).lower().strip()",
            "                    if reply in {'y', ''}:",
            "                        return None",
            "                    elif reply == 'n':",
            "                        if cancelled:",
            "                            raise type(cancelled)(f'Skipping {video_title}')",
            "                        return f'Skipping {video_title}'",
            "            return ret",
            "",
            "        if self.in_download_archive(info_dict):",
            "            reason = ''.join((",
            "                format_field(info_dict, 'id', f'{self._format_screen(\"%s\", self.Styles.ID)}: '),",
            "                format_field(info_dict, 'title', f'{self._format_screen(\"%s\", self.Styles.EMPHASIS)} '),",
            "                'has already been recorded in the archive'))",
            "            break_opt, break_err = 'break_on_existing', ExistingVideoReached",
            "        else:",
            "            try:",
            "                reason = check_filter()",
            "            except DownloadCancelled as e:",
            "                reason, break_opt, break_err = e.msg, 'match_filter', type(e)",
            "            else:",
            "                break_opt, break_err = 'break_on_reject', RejectedVideoReached",
            "        if reason is not None:",
            "            if not silent:",
            "                self.to_screen('[download] ' + reason)",
            "            if self.params.get(break_opt, False):",
            "                raise break_err()",
            "        return reason",
            "",
            "    @staticmethod",
            "    def add_extra_info(info_dict, extra_info):",
            "        '''Set the keys from extra_info in info dict if they are missing'''",
            "        for key, value in extra_info.items():",
            "            info_dict.setdefault(key, value)",
            "",
            "    def extract_info(self, url, download=True, ie_key=None, extra_info=None,",
            "                     process=True, force_generic_extractor=False):",
            "        \"\"\"",
            "        Extract and return the information dictionary of the URL",
            "",
            "        Arguments:",
            "        @param url          URL to extract",
            "",
            "        Keyword arguments:",
            "        @param download     Whether to download videos",
            "        @param process      Whether to resolve all unresolved references (URLs, playlist items).",
            "                            Must be True for download to work",
            "        @param ie_key       Use only the extractor with this key",
            "",
            "        @param extra_info   Dictionary containing the extra values to add to the info (For internal use only)",
            "        @force_generic_extractor  Force using the generic extractor (Deprecated; use ie_key='Generic')",
            "        \"\"\"",
            "",
            "        if extra_info is None:",
            "            extra_info = {}",
            "",
            "        if not ie_key and force_generic_extractor:",
            "            ie_key = 'Generic'",
            "",
            "        if ie_key:",
            "            ies = {ie_key: self._ies[ie_key]} if ie_key in self._ies else {}",
            "        else:",
            "            ies = self._ies",
            "",
            "        for key, ie in ies.items():",
            "            if not ie.suitable(url):",
            "                continue",
            "",
            "            if not ie.working():",
            "                self.report_warning('The program functionality for this site has been marked as broken, '",
            "                                    'and will probably not work.')",
            "",
            "            temp_id = ie.get_temp_id(url)",
            "            if temp_id is not None and self.in_download_archive({'id': temp_id, 'ie_key': key}):",
            "                self.to_screen(f'[download] {self._format_screen(temp_id, self.Styles.ID)}: '",
            "                               'has already been recorded in the archive')",
            "                if self.params.get('break_on_existing', False):",
            "                    raise ExistingVideoReached()",
            "                break",
            "            return self.__extract_info(url, self.get_info_extractor(key), download, extra_info, process)",
            "        else:",
            "            extractors_restricted = self.params.get('allowed_extractors') not in (None, ['default'])",
            "            self.report_error(f'No suitable extractor{format_field(ie_key, None, \" (%s)\")} found for URL {url}',",
            "                              tb=False if extractors_restricted else None)",
            "",
            "    def _handle_extraction_exceptions(func):",
            "        @functools.wraps(func)",
            "        def wrapper(self, *args, **kwargs):",
            "            while True:",
            "                try:",
            "                    return func(self, *args, **kwargs)",
            "                except (DownloadCancelled, LazyList.IndexError, PagedList.IndexError):",
            "                    raise",
            "                except ReExtractInfo as e:",
            "                    if e.expected:",
            "                        self.to_screen(f'{e}; Re-extracting data')",
            "                    else:",
            "                        self.to_stderr('\\r')",
            "                        self.report_warning(f'{e}; Re-extracting data')",
            "                    continue",
            "                except GeoRestrictedError as e:",
            "                    msg = e.msg",
            "                    if e.countries:",
            "                        msg += '\\nThis video is available in %s.' % ', '.join(",
            "                            map(ISO3166Utils.short2full, e.countries))",
            "                    msg += '\\nYou might want to use a VPN or a proxy server (with --proxy) to workaround.'",
            "                    self.report_error(msg)",
            "                except ExtractorError as e:  # An error we somewhat expected",
            "                    self.report_error(str(e), e.format_traceback())",
            "                except Exception as e:",
            "                    if self.params.get('ignoreerrors'):",
            "                        self.report_error(str(e), tb=encode_compat_str(traceback.format_exc()))",
            "                    else:",
            "                        raise",
            "                break",
            "        return wrapper",
            "",
            "    def _wait_for_video(self, ie_result={}):",
            "        if (not self.params.get('wait_for_video')",
            "                or ie_result.get('_type', 'video') != 'video'",
            "                or ie_result.get('formats') or ie_result.get('url')):",
            "            return",
            "",
            "        format_dur = lambda dur: '%02d:%02d:%02d' % timetuple_from_msec(dur * 1000)[:-1]",
            "        last_msg = ''",
            "",
            "        def progress(msg):",
            "            nonlocal last_msg",
            "            full_msg = f'{msg}\\n'",
            "            if not self.params.get('noprogress'):",
            "                full_msg = msg + ' ' * (len(last_msg) - len(msg)) + '\\r'",
            "            elif last_msg:",
            "                return",
            "            self.to_screen(full_msg, skip_eol=True)",
            "            last_msg = msg",
            "",
            "        min_wait, max_wait = self.params.get('wait_for_video')",
            "        diff = try_get(ie_result, lambda x: x['release_timestamp'] - time.time())",
            "        if diff is None and ie_result.get('live_status') == 'is_upcoming':",
            "            diff = round(random.uniform(min_wait, max_wait) if (max_wait and min_wait) else (max_wait or min_wait), 0)",
            "            self.report_warning('Release time of video is not known')",
            "        elif ie_result and (diff or 0) <= 0:",
            "            self.report_warning('Video should already be available according to extracted info')",
            "        diff = min(max(diff or 0, min_wait or 0), max_wait or float('inf'))",
            "        self.to_screen(f'[wait] Waiting for {format_dur(diff)} - Press Ctrl+C to try now')",
            "",
            "        wait_till = time.time() + diff",
            "        try:",
            "            while True:",
            "                diff = wait_till - time.time()",
            "                if diff <= 0:",
            "                    progress('')",
            "                    raise ReExtractInfo('[wait] Wait period ended', expected=True)",
            "                progress(f'[wait] Remaining time until next attempt: {self._format_screen(format_dur(diff), self.Styles.EMPHASIS)}')",
            "                time.sleep(1)",
            "        except KeyboardInterrupt:",
            "            progress('')",
            "            raise ReExtractInfo('[wait] Interrupted by user', expected=True)",
            "        except BaseException as e:",
            "            if not isinstance(e, ReExtractInfo):",
            "                self.to_screen('')",
            "            raise",
            "",
            "    def _load_cookies(self, data, *, autoscope=True):",
            "        \"\"\"Loads cookies from a `Cookie` header",
            "",
            "        This tries to work around the security vulnerability of passing cookies to every domain.",
            "        See: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj",
            "",
            "        @param data         The Cookie header as string to load the cookies from",
            "        @param autoscope    If `False`, scope cookies using Set-Cookie syntax and error for cookie without domains",
            "                            If `True`, save cookies for later to be stored in the jar with a limited scope",
            "                            If a URL, save cookies in the jar with the domain of the URL",
            "        \"\"\"",
            "        for cookie in LenientSimpleCookie(data).values():",
            "            if autoscope and any(cookie.values()):",
            "                raise ValueError('Invalid syntax in Cookie Header')",
            "",
            "            domain = cookie.get('domain') or ''",
            "            expiry = cookie.get('expires')",
            "            if expiry == '':  # 0 is valid",
            "                expiry = None",
            "            prepared_cookie = http.cookiejar.Cookie(",
            "                cookie.get('version') or 0, cookie.key, cookie.value, None, False,",
            "                domain, True, True, cookie.get('path') or '', bool(cookie.get('path')),",
            "                cookie.get('secure') or False, expiry, False, None, None, {})",
            "",
            "            if domain:",
            "                self.cookiejar.set_cookie(prepared_cookie)",
            "            elif autoscope is True:",
            "                self.deprecated_feature(",
            "                    'Passing cookies as a header is a potential security risk; '",
            "                    'they will be scoped to the domain of the downloaded urls. '",
            "                    'Please consider loading cookies from a file or browser instead.')",
            "                self.__header_cookies.append(prepared_cookie)",
            "            elif autoscope:",
            "                self.report_warning(",
            "                    'The extractor result contains an unscoped cookie as an HTTP header. '",
            "                    f'If you are using yt-dlp with an input URL{bug_reports_message(before=\",\")}',",
            "                    only_once=True)",
            "                self._apply_header_cookies(autoscope, [prepared_cookie])",
            "            else:",
            "                self.report_error('Unscoped cookies are not allowed; please specify some sort of scoping',",
            "                                  tb=False, is_error=False)",
            "",
            "    def _apply_header_cookies(self, url, cookies=None):",
            "        \"\"\"Applies stray header cookies to the provided url",
            "",
            "        This loads header cookies and scopes them to the domain provided in `url`.",
            "        While this is not ideal, it helps reduce the risk of them being sent",
            "        to an unintended destination while mostly maintaining compatibility.",
            "        \"\"\"",
            "        parsed = urllib.parse.urlparse(url)",
            "        if not parsed.hostname:",
            "            return",
            "",
            "        for cookie in map(copy.copy, cookies or self.__header_cookies):",
            "            cookie.domain = f'.{parsed.hostname}'",
            "            self.cookiejar.set_cookie(cookie)",
            "",
            "    @_handle_extraction_exceptions",
            "    def __extract_info(self, url, ie, download, extra_info, process):",
            "        self._apply_header_cookies(url)",
            "",
            "        try:",
            "            ie_result = ie.extract(url)",
            "        except UserNotLive as e:",
            "            if process:",
            "                if self.params.get('wait_for_video'):",
            "                    self.report_warning(e)",
            "                self._wait_for_video()",
            "            raise",
            "        if ie_result is None:  # Finished already (backwards compatibility; listformats and friends should be moved here)",
            "            self.report_warning(f'Extractor {ie.IE_NAME} returned nothing{bug_reports_message()}')",
            "            return",
            "        if isinstance(ie_result, list):",
            "            # Backwards compatibility: old IE result format",
            "            ie_result = {",
            "                '_type': 'compat_list',",
            "                'entries': ie_result,",
            "            }",
            "        if extra_info.get('original_url'):",
            "            ie_result.setdefault('original_url', extra_info['original_url'])",
            "        self.add_default_extra_info(ie_result, ie, url)",
            "        if process:",
            "            self._wait_for_video(ie_result)",
            "            return self.process_ie_result(ie_result, download, extra_info)",
            "        else:",
            "            return ie_result",
            "",
            "    def add_default_extra_info(self, ie_result, ie, url):",
            "        if url is not None:",
            "            self.add_extra_info(ie_result, {",
            "                'webpage_url': url,",
            "                'original_url': url,",
            "            })",
            "        webpage_url = ie_result.get('webpage_url')",
            "        if webpage_url:",
            "            self.add_extra_info(ie_result, {",
            "                'webpage_url_basename': url_basename(webpage_url),",
            "                'webpage_url_domain': get_domain(webpage_url),",
            "            })",
            "        if ie is not None:",
            "            self.add_extra_info(ie_result, {",
            "                'extractor': ie.IE_NAME,",
            "                'extractor_key': ie.ie_key(),",
            "            })",
            "",
            "    def process_ie_result(self, ie_result, download=True, extra_info=None):",
            "        \"\"\"",
            "        Take the result of the ie(may be modified) and resolve all unresolved",
            "        references (URLs, playlist items).",
            "",
            "        It will also download the videos if 'download'.",
            "        Returns the resolved ie_result.",
            "        \"\"\"",
            "        if extra_info is None:",
            "            extra_info = {}",
            "        result_type = ie_result.get('_type', 'video')",
            "",
            "        if result_type in ('url', 'url_transparent'):",
            "            ie_result['url'] = sanitize_url(",
            "                ie_result['url'], scheme='http' if self.params.get('prefer_insecure') else 'https')",
            "            if ie_result.get('original_url') and not extra_info.get('original_url'):",
            "                extra_info = {'original_url': ie_result['original_url'], **extra_info}",
            "",
            "            extract_flat = self.params.get('extract_flat', False)",
            "            if ((extract_flat == 'in_playlist' and 'playlist' in extra_info)",
            "                    or extract_flat is True):",
            "                info_copy = ie_result.copy()",
            "                ie = try_get(ie_result.get('ie_key'), self.get_info_extractor)",
            "                if ie and not ie_result.get('id'):",
            "                    info_copy['id'] = ie.get_temp_id(ie_result['url'])",
            "                self.add_default_extra_info(info_copy, ie, ie_result['url'])",
            "                self.add_extra_info(info_copy, extra_info)",
            "                info_copy, _ = self.pre_process(info_copy)",
            "                self._fill_common_fields(info_copy, False)",
            "                self.__forced_printings(info_copy)",
            "                self._raise_pending_errors(info_copy)",
            "                if self.params.get('force_write_download_archive', False):",
            "                    self.record_download_archive(info_copy)",
            "                return ie_result",
            "",
            "        if result_type == 'video':",
            "            self.add_extra_info(ie_result, extra_info)",
            "            ie_result = self.process_video_result(ie_result, download=download)",
            "            self._raise_pending_errors(ie_result)",
            "            additional_urls = (ie_result or {}).get('additional_urls')",
            "            if additional_urls:",
            "                # TODO: Improve MetadataParserPP to allow setting a list",
            "                if isinstance(additional_urls, str):",
            "                    additional_urls = [additional_urls]",
            "                self.to_screen(",
            "                    '[info] %s: %d additional URL(s) requested' % (ie_result['id'], len(additional_urls)))",
            "                self.write_debug('Additional URLs: \"%s\"' % '\", \"'.join(additional_urls))",
            "                ie_result['additional_entries'] = [",
            "                    self.extract_info(",
            "                        url, download, extra_info=extra_info,",
            "                        force_generic_extractor=self.params.get('force_generic_extractor'))",
            "                    for url in additional_urls",
            "                ]",
            "            return ie_result",
            "        elif result_type == 'url':",
            "            # We have to add extra_info to the results because it may be",
            "            # contained in a playlist",
            "            return self.extract_info(",
            "                ie_result['url'], download,",
            "                ie_key=ie_result.get('ie_key'),",
            "                extra_info=extra_info)",
            "        elif result_type == 'url_transparent':",
            "            # Use the information from the embedding page",
            "            info = self.extract_info(",
            "                ie_result['url'], ie_key=ie_result.get('ie_key'),",
            "                extra_info=extra_info, download=False, process=False)",
            "",
            "            # extract_info may return None when ignoreerrors is enabled and",
            "            # extraction failed with an error, don't crash and return early",
            "            # in this case",
            "            if not info:",
            "                return info",
            "",
            "            exempted_fields = {'_type', 'url', 'ie_key'}",
            "            if not ie_result.get('section_end') and ie_result.get('section_start') is None:",
            "                # For video clips, the id etc of the clip extractor should be used",
            "                exempted_fields |= {'id', 'extractor', 'extractor_key'}",
            "",
            "            new_result = info.copy()",
            "            new_result.update(filter_dict(ie_result, lambda k, v: v is not None and k not in exempted_fields))",
            "",
            "            # Extracted info may not be a video result (i.e.",
            "            # info.get('_type', 'video') != video) but rather an url or",
            "            # url_transparent. In such cases outer metadata (from ie_result)",
            "            # should be propagated to inner one (info). For this to happen",
            "            # _type of info should be overridden with url_transparent. This",
            "            # fixes issue from https://github.com/ytdl-org/youtube-dl/pull/11163.",
            "            if new_result.get('_type') == 'url':",
            "                new_result['_type'] = 'url_transparent'",
            "",
            "            return self.process_ie_result(",
            "                new_result, download=download, extra_info=extra_info)",
            "        elif result_type in ('playlist', 'multi_video'):",
            "            # Protect from infinite recursion due to recursively nested playlists",
            "            # (see https://github.com/ytdl-org/youtube-dl/issues/27833)",
            "            webpage_url = ie_result.get('webpage_url')  # Playlists maynot have webpage_url",
            "            if webpage_url and webpage_url in self._playlist_urls:",
            "                self.to_screen(",
            "                    '[download] Skipping already downloaded playlist: %s'",
            "                    % ie_result.get('title') or ie_result.get('id'))",
            "                return",
            "",
            "            self._playlist_level += 1",
            "            self._playlist_urls.add(webpage_url)",
            "            self._fill_common_fields(ie_result, False)",
            "            self._sanitize_thumbnails(ie_result)",
            "            try:",
            "                return self.__process_playlist(ie_result, download)",
            "            finally:",
            "                self._playlist_level -= 1",
            "                if not self._playlist_level:",
            "                    self._playlist_urls.clear()",
            "        elif result_type == 'compat_list':",
            "            self.report_warning(",
            "                'Extractor %s returned a compat_list result. '",
            "                'It needs to be updated.' % ie_result.get('extractor'))",
            "",
            "            def _fixup(r):",
            "                self.add_extra_info(r, {",
            "                    'extractor': ie_result['extractor'],",
            "                    'webpage_url': ie_result['webpage_url'],",
            "                    'webpage_url_basename': url_basename(ie_result['webpage_url']),",
            "                    'webpage_url_domain': get_domain(ie_result['webpage_url']),",
            "                    'extractor_key': ie_result['extractor_key'],",
            "                })",
            "                return r",
            "            ie_result['entries'] = [",
            "                self.process_ie_result(_fixup(r), download, extra_info)",
            "                for r in ie_result['entries']",
            "            ]",
            "            return ie_result",
            "        else:",
            "            raise Exception('Invalid result type: %s' % result_type)",
            "",
            "    def _ensure_dir_exists(self, path):",
            "        return make_dir(path, self.report_error)",
            "",
            "    @staticmethod",
            "    def _playlist_infodict(ie_result, strict=False, **kwargs):",
            "        info = {",
            "            'playlist_count': ie_result.get('playlist_count'),",
            "            'playlist': ie_result.get('title') or ie_result.get('id'),",
            "            'playlist_id': ie_result.get('id'),",
            "            'playlist_title': ie_result.get('title'),",
            "            'playlist_uploader': ie_result.get('uploader'),",
            "            'playlist_uploader_id': ie_result.get('uploader_id'),",
            "            **kwargs,",
            "        }",
            "        if strict:",
            "            return info",
            "        if ie_result.get('webpage_url'):",
            "            info.update({",
            "                'webpage_url': ie_result['webpage_url'],",
            "                'webpage_url_basename': url_basename(ie_result['webpage_url']),",
            "                'webpage_url_domain': get_domain(ie_result['webpage_url']),",
            "            })",
            "        return {",
            "            **info,",
            "            'playlist_index': 0,",
            "            '__last_playlist_index': max(ie_result.get('requested_entries') or (0, 0)),",
            "            'extractor': ie_result['extractor'],",
            "            'extractor_key': ie_result['extractor_key'],",
            "        }",
            "",
            "    def __process_playlist(self, ie_result, download):",
            "        \"\"\"Process each entry in the playlist\"\"\"",
            "        assert ie_result['_type'] in ('playlist', 'multi_video')",
            "",
            "        common_info = self._playlist_infodict(ie_result, strict=True)",
            "        title = common_info.get('playlist') or '<Untitled>'",
            "        if self._match_entry(common_info, incomplete=True) is not None:",
            "            return",
            "        self.to_screen(f'[download] Downloading {ie_result[\"_type\"]}: {title}')",
            "",
            "        all_entries = PlaylistEntries(self, ie_result)",
            "        entries = orderedSet(all_entries.get_requested_items(), lazy=True)",
            "",
            "        lazy = self.params.get('lazy_playlist')",
            "        if lazy:",
            "            resolved_entries, n_entries = [], 'N/A'",
            "            ie_result['requested_entries'], ie_result['entries'] = None, None",
            "        else:",
            "            entries = resolved_entries = list(entries)",
            "            n_entries = len(resolved_entries)",
            "            ie_result['requested_entries'], ie_result['entries'] = tuple(zip(*resolved_entries)) or ([], [])",
            "        if not ie_result.get('playlist_count'):",
            "            # Better to do this after potentially exhausting entries",
            "            ie_result['playlist_count'] = all_entries.get_full_count()",
            "",
            "        extra = self._playlist_infodict(ie_result, n_entries=int_or_none(n_entries))",
            "        ie_copy = collections.ChainMap(ie_result, extra)",
            "",
            "        _infojson_written = False",
            "        write_playlist_files = self.params.get('allow_playlist_files', True)",
            "        if write_playlist_files and self.params.get('list_thumbnails'):",
            "            self.list_thumbnails(ie_result)",
            "        if write_playlist_files and not self.params.get('simulate'):",
            "            _infojson_written = self._write_info_json(",
            "                'playlist', ie_result, self.prepare_filename(ie_copy, 'pl_infojson'))",
            "            if _infojson_written is None:",
            "                return",
            "            if self._write_description('playlist', ie_result,",
            "                                       self.prepare_filename(ie_copy, 'pl_description')) is None:",
            "                return",
            "            # TODO: This should be passed to ThumbnailsConvertor if necessary",
            "            self._write_thumbnails('playlist', ie_result, self.prepare_filename(ie_copy, 'pl_thumbnail'))",
            "",
            "        if lazy:",
            "            if self.params.get('playlistreverse') or self.params.get('playlistrandom'):",
            "                self.report_warning('playlistreverse and playlistrandom are not supported with lazy_playlist', only_once=True)",
            "        elif self.params.get('playlistreverse'):",
            "            entries.reverse()",
            "        elif self.params.get('playlistrandom'):",
            "            random.shuffle(entries)",
            "",
            "        self.to_screen(f'[{ie_result[\"extractor\"]}] Playlist {title}: Downloading {n_entries} items'",
            "                       f'{format_field(ie_result, \"playlist_count\", \" of %s\")}')",
            "",
            "        keep_resolved_entries = self.params.get('extract_flat') != 'discard'",
            "        if self.params.get('extract_flat') == 'discard_in_playlist':",
            "            keep_resolved_entries = ie_result['_type'] != 'playlist'",
            "        if keep_resolved_entries:",
            "            self.write_debug('The information of all playlist entries will be held in memory')",
            "",
            "        failures = 0",
            "        max_failures = self.params.get('skip_playlist_after_errors') or float('inf')",
            "        for i, (playlist_index, entry) in enumerate(entries):",
            "            if lazy:",
            "                resolved_entries.append((playlist_index, entry))",
            "            if not entry:",
            "                continue",
            "",
            "            entry['__x_forwarded_for_ip'] = ie_result.get('__x_forwarded_for_ip')",
            "            if not lazy and 'playlist-index' in self.params['compat_opts']:",
            "                playlist_index = ie_result['requested_entries'][i]",
            "",
            "            entry_copy = collections.ChainMap(entry, {",
            "                **common_info,",
            "                'n_entries': int_or_none(n_entries),",
            "                'playlist_index': playlist_index,",
            "                'playlist_autonumber': i + 1,",
            "            })",
            "",
            "            if self._match_entry(entry_copy, incomplete=True) is not None:",
            "                # For compatabilty with youtube-dl. See https://github.com/yt-dlp/yt-dlp/issues/4369",
            "                resolved_entries[i] = (playlist_index, NO_DEFAULT)",
            "                continue",
            "",
            "            self.to_screen('[download] Downloading item %s of %s' % (",
            "                self._format_screen(i + 1, self.Styles.ID), self._format_screen(n_entries, self.Styles.EMPHASIS)))",
            "",
            "            entry_result = self.__process_iterable_entry(entry, download, collections.ChainMap({",
            "                'playlist_index': playlist_index,",
            "                'playlist_autonumber': i + 1,",
            "            }, extra))",
            "            if not entry_result:",
            "                failures += 1",
            "            if failures >= max_failures:",
            "                self.report_error(",
            "                    f'Skipping the remaining entries in playlist \"{title}\" since {failures} items failed extraction')",
            "                break",
            "            if keep_resolved_entries:",
            "                resolved_entries[i] = (playlist_index, entry_result)",
            "",
            "        # Update with processed data",
            "        ie_result['entries'] = [e for _, e in resolved_entries if e is not NO_DEFAULT]",
            "        ie_result['requested_entries'] = [i for i, e in resolved_entries if e is not NO_DEFAULT]",
            "        if ie_result['requested_entries'] == try_call(lambda: list(range(1, ie_result['playlist_count'] + 1))):",
            "            # Do not set for full playlist",
            "            ie_result.pop('requested_entries')",
            "",
            "        # Write the updated info to json",
            "        if _infojson_written is True and self._write_info_json(",
            "                'updated playlist', ie_result,",
            "                self.prepare_filename(ie_copy, 'pl_infojson'), overwrite=True) is None:",
            "            return",
            "",
            "        ie_result = self.run_all_pps('playlist', ie_result)",
            "        self.to_screen(f'[download] Finished downloading playlist: {title}')",
            "        return ie_result",
            "",
            "    @_handle_extraction_exceptions",
            "    def __process_iterable_entry(self, entry, download, extra_info):",
            "        return self.process_ie_result(",
            "            entry, download=download, extra_info=extra_info)",
            "",
            "    def _build_format_filter(self, filter_spec):",
            "        \" Returns a function to filter the formats according to the filter_spec \"",
            "",
            "        OPERATORS = {",
            "            '<': operator.lt,",
            "            '<=': operator.le,",
            "            '>': operator.gt,",
            "            '>=': operator.ge,",
            "            '=': operator.eq,",
            "            '!=': operator.ne,",
            "        }",
            "        operator_rex = re.compile(r'''(?x)\\s*",
            "            (?P<key>[\\w.-]+)\\s*",
            "            (?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*",
            "            (?P<value>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)\\s*",
            "            ''' % '|'.join(map(re.escape, OPERATORS.keys())))",
            "        m = operator_rex.fullmatch(filter_spec)",
            "        if m:",
            "            try:",
            "                comparison_value = int(m.group('value'))",
            "            except ValueError:",
            "                comparison_value = parse_filesize(m.group('value'))",
            "                if comparison_value is None:",
            "                    comparison_value = parse_filesize(m.group('value') + 'B')",
            "                if comparison_value is None:",
            "                    raise ValueError(",
            "                        'Invalid value %r in format specification %r' % (",
            "                            m.group('value'), filter_spec))",
            "            op = OPERATORS[m.group('op')]",
            "",
            "        if not m:",
            "            STR_OPERATORS = {",
            "                '=': operator.eq,",
            "                '^=': lambda attr, value: attr.startswith(value),",
            "                '$=': lambda attr, value: attr.endswith(value),",
            "                '*=': lambda attr, value: value in attr,",
            "                '~=': lambda attr, value: value.search(attr) is not None",
            "            }",
            "            str_operator_rex = re.compile(r'''(?x)\\s*",
            "                (?P<key>[a-zA-Z0-9._-]+)\\s*",
            "                (?P<negation>!\\s*)?(?P<op>%s)\\s*(?P<none_inclusive>\\?\\s*)?",
            "                (?P<quote>[\"'])?",
            "                (?P<value>(?(quote)(?:(?!(?P=quote))[^\\\\]|\\\\.)+|[\\w.-]+))",
            "                (?(quote)(?P=quote))\\s*",
            "                ''' % '|'.join(map(re.escape, STR_OPERATORS.keys())))",
            "            m = str_operator_rex.fullmatch(filter_spec)",
            "            if m:",
            "                if m.group('op') == '~=':",
            "                    comparison_value = re.compile(m.group('value'))",
            "                else:",
            "                    comparison_value = re.sub(r'''\\\\([\\\\\"'])''', r'\\1', m.group('value'))",
            "                str_op = STR_OPERATORS[m.group('op')]",
            "                if m.group('negation'):",
            "                    op = lambda attr, value: not str_op(attr, value)",
            "                else:",
            "                    op = str_op",
            "",
            "        if not m:",
            "            raise SyntaxError('Invalid filter specification %r' % filter_spec)",
            "",
            "        def _filter(f):",
            "            actual_value = f.get(m.group('key'))",
            "            if actual_value is None:",
            "                return m.group('none_inclusive')",
            "            return op(actual_value, comparison_value)",
            "        return _filter",
            "",
            "    def _check_formats(self, formats):",
            "        for f in formats:",
            "            self.to_screen('[info] Testing format %s' % f['format_id'])",
            "            path = self.get_output_path('temp')",
            "            if not self._ensure_dir_exists(f'{path}/'):",
            "                continue",
            "            temp_file = tempfile.NamedTemporaryFile(suffix='.tmp', delete=False, dir=path or None)",
            "            temp_file.close()",
            "            try:",
            "                success, _ = self.dl(temp_file.name, f, test=True)",
            "            except (DownloadError, OSError, ValueError) + network_exceptions:",
            "                success = False",
            "            finally:",
            "                if os.path.exists(temp_file.name):",
            "                    try:",
            "                        os.remove(temp_file.name)",
            "                    except OSError:",
            "                        self.report_warning('Unable to delete temporary file \"%s\"' % temp_file.name)",
            "            if success:",
            "                yield f",
            "            else:",
            "                self.to_screen('[info] Unable to download format %s. Skipping...' % f['format_id'])",
            "",
            "    def _default_format_spec(self, info_dict, download=True):",
            "",
            "        def can_merge():",
            "            merger = FFmpegMergerPP(self)",
            "            return merger.available and merger.can_merge()",
            "",
            "        prefer_best = (",
            "            not self.params.get('simulate')",
            "            and download",
            "            and (",
            "                not can_merge()",
            "                or info_dict.get('is_live') and not self.params.get('live_from_start')",
            "                or self.params['outtmpl']['default'] == '-'))",
            "        compat = (",
            "            prefer_best",
            "            or self.params.get('allow_multiple_audio_streams', False)",
            "            or 'format-spec' in self.params['compat_opts'])",
            "",
            "        return (",
            "            'best/bestvideo+bestaudio' if prefer_best",
            "            else 'bestvideo*+bestaudio/best' if not compat",
            "            else 'bestvideo+bestaudio/best')",
            "",
            "    def build_format_selector(self, format_spec):",
            "        def syntax_error(note, start):",
            "            message = (",
            "                'Invalid format specification: '",
            "                '{}\\n\\t{}\\n\\t{}^'.format(note, format_spec, ' ' * start[1]))",
            "            return SyntaxError(message)",
            "",
            "        PICKFIRST = 'PICKFIRST'",
            "        MERGE = 'MERGE'",
            "        SINGLE = 'SINGLE'",
            "        GROUP = 'GROUP'",
            "        FormatSelector = collections.namedtuple('FormatSelector', ['type', 'selector', 'filters'])",
            "",
            "        allow_multiple_streams = {'audio': self.params.get('allow_multiple_audio_streams', False),",
            "                                  'video': self.params.get('allow_multiple_video_streams', False)}",
            "",
            "        def _parse_filter(tokens):",
            "            filter_parts = []",
            "            for type, string_, start, _, _ in tokens:",
            "                if type == tokenize.OP and string_ == ']':",
            "                    return ''.join(filter_parts)",
            "                else:",
            "                    filter_parts.append(string_)",
            "",
            "        def _remove_unused_ops(tokens):",
            "            # Remove operators that we don't use and join them with the surrounding strings.",
            "            # E.g. 'mp4' '-' 'baseline' '-' '16x9' is converted to 'mp4-baseline-16x9'",
            "            ALLOWED_OPS = ('/', '+', ',', '(', ')')",
            "            last_string, last_start, last_end, last_line = None, None, None, None",
            "            for type, string_, start, end, line in tokens:",
            "                if type == tokenize.OP and string_ == '[':",
            "                    if last_string:",
            "                        yield tokenize.NAME, last_string, last_start, last_end, last_line",
            "                        last_string = None",
            "                    yield type, string_, start, end, line",
            "                    # everything inside brackets will be handled by _parse_filter",
            "                    for type, string_, start, end, line in tokens:",
            "                        yield type, string_, start, end, line",
            "                        if type == tokenize.OP and string_ == ']':",
            "                            break",
            "                elif type == tokenize.OP and string_ in ALLOWED_OPS:",
            "                    if last_string:",
            "                        yield tokenize.NAME, last_string, last_start, last_end, last_line",
            "                        last_string = None",
            "                    yield type, string_, start, end, line",
            "                elif type in [tokenize.NAME, tokenize.NUMBER, tokenize.OP]:",
            "                    if not last_string:",
            "                        last_string = string_",
            "                        last_start = start",
            "                        last_end = end",
            "                    else:",
            "                        last_string += string_",
            "            if last_string:",
            "                yield tokenize.NAME, last_string, last_start, last_end, last_line",
            "",
            "        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):",
            "            selectors = []",
            "            current_selector = None",
            "            for type, string_, start, _, _ in tokens:",
            "                # ENCODING is only defined in Python 3.x",
            "                if type == getattr(tokenize, 'ENCODING', None):",
            "                    continue",
            "                elif type in [tokenize.NAME, tokenize.NUMBER]:",
            "                    current_selector = FormatSelector(SINGLE, string_, [])",
            "                elif type == tokenize.OP:",
            "                    if string_ == ')':",
            "                        if not inside_group:",
            "                            # ')' will be handled by the parentheses group",
            "                            tokens.restore_last_token()",
            "                        break",
            "                    elif inside_merge and string_ in ['/', ',']:",
            "                        tokens.restore_last_token()",
            "                        break",
            "                    elif inside_choice and string_ == ',':",
            "                        tokens.restore_last_token()",
            "                        break",
            "                    elif string_ == ',':",
            "                        if not current_selector:",
            "                            raise syntax_error('\",\" must follow a format selector', start)",
            "                        selectors.append(current_selector)",
            "                        current_selector = None",
            "                    elif string_ == '/':",
            "                        if not current_selector:",
            "                            raise syntax_error('\"/\" must follow a format selector', start)",
            "                        first_choice = current_selector",
            "                        second_choice = _parse_format_selection(tokens, inside_choice=True)",
            "                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])",
            "                    elif string_ == '[':",
            "                        if not current_selector:",
            "                            current_selector = FormatSelector(SINGLE, 'best', [])",
            "                        format_filter = _parse_filter(tokens)",
            "                        current_selector.filters.append(format_filter)",
            "                    elif string_ == '(':",
            "                        if current_selector:",
            "                            raise syntax_error('Unexpected \"(\"', start)",
            "                        group = _parse_format_selection(tokens, inside_group=True)",
            "                        current_selector = FormatSelector(GROUP, group, [])",
            "                    elif string_ == '+':",
            "                        if not current_selector:",
            "                            raise syntax_error('Unexpected \"+\"', start)",
            "                        selector_1 = current_selector",
            "                        selector_2 = _parse_format_selection(tokens, inside_merge=True)",
            "                        if not selector_2:",
            "                            raise syntax_error('Expected a selector', start)",
            "                        current_selector = FormatSelector(MERGE, (selector_1, selector_2), [])",
            "                    else:",
            "                        raise syntax_error(f'Operator not recognized: \"{string_}\"', start)",
            "                elif type == tokenize.ENDMARKER:",
            "                    break",
            "            if current_selector:",
            "                selectors.append(current_selector)",
            "            return selectors",
            "",
            "        def _merge(formats_pair):",
            "            format_1, format_2 = formats_pair",
            "",
            "            formats_info = []",
            "            formats_info.extend(format_1.get('requested_formats', (format_1,)))",
            "            formats_info.extend(format_2.get('requested_formats', (format_2,)))",
            "",
            "            if not allow_multiple_streams['video'] or not allow_multiple_streams['audio']:",
            "                get_no_more = {'video': False, 'audio': False}",
            "                for (i, fmt_info) in enumerate(formats_info):",
            "                    if fmt_info.get('acodec') == fmt_info.get('vcodec') == 'none':",
            "                        formats_info.pop(i)",
            "                        continue",
            "                    for aud_vid in ['audio', 'video']:",
            "                        if not allow_multiple_streams[aud_vid] and fmt_info.get(aud_vid[0] + 'codec') != 'none':",
            "                            if get_no_more[aud_vid]:",
            "                                formats_info.pop(i)",
            "                                break",
            "                            get_no_more[aud_vid] = True",
            "",
            "            if len(formats_info) == 1:",
            "                return formats_info[0]",
            "",
            "            video_fmts = [fmt_info for fmt_info in formats_info if fmt_info.get('vcodec') != 'none']",
            "            audio_fmts = [fmt_info for fmt_info in formats_info if fmt_info.get('acodec') != 'none']",
            "",
            "            the_only_video = video_fmts[0] if len(video_fmts) == 1 else None",
            "            the_only_audio = audio_fmts[0] if len(audio_fmts) == 1 else None",
            "",
            "            output_ext = get_compatible_ext(",
            "                vcodecs=[f.get('vcodec') for f in video_fmts],",
            "                acodecs=[f.get('acodec') for f in audio_fmts],",
            "                vexts=[f['ext'] for f in video_fmts],",
            "                aexts=[f['ext'] for f in audio_fmts],",
            "                preferences=(try_call(lambda: self.params['merge_output_format'].split('/'))",
            "                             or self.params.get('prefer_free_formats') and ('webm', 'mkv')))",
            "",
            "            filtered = lambda *keys: filter(None, (traverse_obj(fmt, *keys) for fmt in formats_info))",
            "",
            "            new_dict = {",
            "                'requested_formats': formats_info,",
            "                'format': '+'.join(filtered('format')),",
            "                'format_id': '+'.join(filtered('format_id')),",
            "                'ext': output_ext,",
            "                'protocol': '+'.join(map(determine_protocol, formats_info)),",
            "                'language': '+'.join(orderedSet(filtered('language'))) or None,",
            "                'format_note': '+'.join(orderedSet(filtered('format_note'))) or None,",
            "                'filesize_approx': sum(filtered('filesize', 'filesize_approx')) or None,",
            "                'tbr': sum(filtered('tbr', 'vbr', 'abr')),",
            "            }",
            "",
            "            if the_only_video:",
            "                new_dict.update({",
            "                    'width': the_only_video.get('width'),",
            "                    'height': the_only_video.get('height'),",
            "                    'resolution': the_only_video.get('resolution') or self.format_resolution(the_only_video),",
            "                    'fps': the_only_video.get('fps'),",
            "                    'dynamic_range': the_only_video.get('dynamic_range'),",
            "                    'vcodec': the_only_video.get('vcodec'),",
            "                    'vbr': the_only_video.get('vbr'),",
            "                    'stretched_ratio': the_only_video.get('stretched_ratio'),",
            "                    'aspect_ratio': the_only_video.get('aspect_ratio'),",
            "                })",
            "",
            "            if the_only_audio:",
            "                new_dict.update({",
            "                    'acodec': the_only_audio.get('acodec'),",
            "                    'abr': the_only_audio.get('abr'),",
            "                    'asr': the_only_audio.get('asr'),",
            "                    'audio_channels': the_only_audio.get('audio_channels')",
            "                })",
            "",
            "            return new_dict",
            "",
            "        def _check_formats(formats):",
            "            if self.params.get('check_formats') == 'selected':",
            "                yield from self._check_formats(formats)",
            "                return",
            "            elif (self.params.get('check_formats') is not None",
            "                    or self.params.get('allow_unplayable_formats')):",
            "                yield from formats",
            "                return",
            "",
            "            for f in formats:",
            "                if f.get('has_drm') or f.get('__needs_testing'):",
            "                    yield from self._check_formats([f])",
            "                else:",
            "                    yield f",
            "",
            "        def _build_selector_function(selector):",
            "            if isinstance(selector, list):  # ,",
            "                fs = [_build_selector_function(s) for s in selector]",
            "",
            "                def selector_function(ctx):",
            "                    for f in fs:",
            "                        yield from f(ctx)",
            "                return selector_function",
            "",
            "            elif selector.type == GROUP:  # ()",
            "                selector_function = _build_selector_function(selector.selector)",
            "",
            "            elif selector.type == PICKFIRST:  # /",
            "                fs = [_build_selector_function(s) for s in selector.selector]",
            "",
            "                def selector_function(ctx):",
            "                    for f in fs:",
            "                        picked_formats = list(f(ctx))",
            "                        if picked_formats:",
            "                            return picked_formats",
            "                    return []",
            "",
            "            elif selector.type == MERGE:  # +",
            "                selector_1, selector_2 = map(_build_selector_function, selector.selector)",
            "",
            "                def selector_function(ctx):",
            "                    for pair in itertools.product(selector_1(ctx), selector_2(ctx)):",
            "                        yield _merge(pair)",
            "",
            "            elif selector.type == SINGLE:  # atom",
            "                format_spec = selector.selector or 'best'",
            "",
            "                # TODO: Add allvideo, allaudio etc by generalizing the code with best/worst selector",
            "                if format_spec == 'all':",
            "                    def selector_function(ctx):",
            "                        yield from _check_formats(ctx['formats'][::-1])",
            "                elif format_spec == 'mergeall':",
            "                    def selector_function(ctx):",
            "                        formats = list(_check_formats(",
            "                            f for f in ctx['formats'] if f.get('vcodec') != 'none' or f.get('acodec') != 'none'))",
            "                        if not formats:",
            "                            return",
            "                        merged_format = formats[-1]",
            "                        for f in formats[-2::-1]:",
            "                            merged_format = _merge((merged_format, f))",
            "                        yield merged_format",
            "",
            "                else:",
            "                    format_fallback, seperate_fallback, format_reverse, format_idx = False, None, True, 1",
            "                    mobj = re.match(",
            "                        r'(?P<bw>best|worst|b|w)(?P<type>video|audio|v|a)?(?P<mod>\\*)?(?:\\.(?P<n>[1-9]\\d*))?$',",
            "                        format_spec)",
            "                    if mobj is not None:",
            "                        format_idx = int_or_none(mobj.group('n'), default=1)",
            "                        format_reverse = mobj.group('bw')[0] == 'b'",
            "                        format_type = (mobj.group('type') or [None])[0]",
            "                        not_format_type = {'v': 'a', 'a': 'v'}.get(format_type)",
            "                        format_modified = mobj.group('mod') is not None",
            "",
            "                        format_fallback = not format_type and not format_modified  # for b, w",
            "                        _filter_f = (",
            "                            (lambda f: f.get('%scodec' % format_type) != 'none')",
            "                            if format_type and format_modified  # bv*, ba*, wv*, wa*",
            "                            else (lambda f: f.get('%scodec' % not_format_type) == 'none')",
            "                            if format_type  # bv, ba, wv, wa",
            "                            else (lambda f: f.get('vcodec') != 'none' and f.get('acodec') != 'none')",
            "                            if not format_modified  # b, w",
            "                            else lambda f: True)  # b*, w*",
            "                        filter_f = lambda f: _filter_f(f) and (",
            "                            f.get('vcodec') != 'none' or f.get('acodec') != 'none')",
            "                    else:",
            "                        if format_spec in self._format_selection_exts['audio']:",
            "                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') != 'none'",
            "                        elif format_spec in self._format_selection_exts['video']:",
            "                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') != 'none' and f.get('vcodec') != 'none'",
            "                            seperate_fallback = lambda f: f.get('ext') == format_spec and f.get('vcodec') != 'none'",
            "                        elif format_spec in self._format_selection_exts['storyboards']:",
            "                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') == 'none' and f.get('vcodec') == 'none'",
            "                        else:",
            "                            filter_f = lambda f: f.get('format_id') == format_spec  # id",
            "",
            "                    def selector_function(ctx):",
            "                        formats = list(ctx['formats'])",
            "                        matches = list(filter(filter_f, formats)) if filter_f is not None else formats",
            "                        if not matches:",
            "                            if format_fallback and ctx['incomplete_formats']:",
            "                                # for extractors with incomplete formats (audio only (soundcloud)",
            "                                # or video only (imgur)) best/worst will fallback to",
            "                                # best/worst {video,audio}-only format",
            "                                matches = list(filter(lambda f: f.get('vcodec') != 'none' or f.get('acodec') != 'none', formats))",
            "                            elif seperate_fallback and not ctx['has_merged_format']:",
            "                                # for compatibility with youtube-dl when there is no pre-merged format",
            "                                matches = list(filter(seperate_fallback, formats))",
            "                        matches = LazyList(_check_formats(matches[::-1 if format_reverse else 1]))",
            "                        try:",
            "                            yield matches[format_idx - 1]",
            "                        except LazyList.IndexError:",
            "                            return",
            "",
            "            filters = [self._build_format_filter(f) for f in selector.filters]",
            "",
            "            def final_selector(ctx):",
            "                ctx_copy = dict(ctx)",
            "                for _filter in filters:",
            "                    ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))",
            "                return selector_function(ctx_copy)",
            "            return final_selector",
            "",
            "        # HACK: Python 3.12 changed the underlying parser, rendering '7_a' invalid",
            "        #       Prefix numbers with random letters to avoid it being classified as a number",
            "        #       See: https://github.com/yt-dlp/yt-dlp/pulls/8797",
            "        # TODO: Implement parser not reliant on tokenize.tokenize",
            "        prefix = ''.join(random.choices(string.ascii_letters, k=32))",
            "        stream = io.BytesIO(re.sub(r'\\d[_\\d]*', rf'{prefix}\\g<0>', format_spec).encode())",
            "        try:",
            "            tokens = list(_remove_unused_ops(",
            "                token._replace(string=token.string.replace(prefix, ''))",
            "                for token in tokenize.tokenize(stream.readline)))",
            "        except tokenize.TokenError:",
            "            raise syntax_error('Missing closing/opening brackets or parenthesis', (0, len(format_spec)))",
            "",
            "        class TokenIterator:",
            "            def __init__(self, tokens):",
            "                self.tokens = tokens",
            "                self.counter = 0",
            "",
            "            def __iter__(self):",
            "                return self",
            "",
            "            def __next__(self):",
            "                if self.counter >= len(self.tokens):",
            "                    raise StopIteration()",
            "                value = self.tokens[self.counter]",
            "                self.counter += 1",
            "                return value",
            "",
            "            next = __next__",
            "",
            "            def restore_last_token(self):",
            "                self.counter -= 1",
            "",
            "        parsed_selector = _parse_format_selection(iter(TokenIterator(tokens)))",
            "        return _build_selector_function(parsed_selector)",
            "",
            "    def _calc_headers(self, info_dict, load_cookies=False):",
            "        res = HTTPHeaderDict(self.params['http_headers'], info_dict.get('http_headers'))",
            "        clean_headers(res)",
            "",
            "        if load_cookies:  # For --load-info-json",
            "            self._load_cookies(res.get('Cookie'), autoscope=info_dict['url'])  # compat",
            "            self._load_cookies(info_dict.get('cookies'), autoscope=False)",
            "        # The `Cookie` header is removed to prevent leaks and unscoped cookies.",
            "        # See: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj",
            "        res.pop('Cookie', None)",
            "        cookies = self.cookiejar.get_cookies_for_url(info_dict['url'])",
            "        if cookies:",
            "            encoder = LenientSimpleCookie()",
            "            values = []",
            "            for cookie in cookies:",
            "                _, value = encoder.value_encode(cookie.value)",
            "                values.append(f'{cookie.name}={value}')",
            "                if cookie.domain:",
            "                    values.append(f'Domain={cookie.domain}')",
            "                if cookie.path:",
            "                    values.append(f'Path={cookie.path}')",
            "                if cookie.secure:",
            "                    values.append('Secure')",
            "                if cookie.expires:",
            "                    values.append(f'Expires={cookie.expires}')",
            "                if cookie.version:",
            "                    values.append(f'Version={cookie.version}')",
            "            info_dict['cookies'] = '; '.join(values)",
            "",
            "        if 'X-Forwarded-For' not in res:",
            "            x_forwarded_for_ip = info_dict.get('__x_forwarded_for_ip')",
            "            if x_forwarded_for_ip:",
            "                res['X-Forwarded-For'] = x_forwarded_for_ip",
            "",
            "        return res",
            "",
            "    def _calc_cookies(self, url):",
            "        self.deprecation_warning('\"YoutubeDL._calc_cookies\" is deprecated and may be removed in a future version')",
            "        return self.cookiejar.get_cookie_header(url)",
            "",
            "    def _sort_thumbnails(self, thumbnails):",
            "        thumbnails.sort(key=lambda t: (",
            "            t.get('preference') if t.get('preference') is not None else -1,",
            "            t.get('width') if t.get('width') is not None else -1,",
            "            t.get('height') if t.get('height') is not None else -1,",
            "            t.get('id') if t.get('id') is not None else '',",
            "            t.get('url')))",
            "",
            "    def _sanitize_thumbnails(self, info_dict):",
            "        thumbnails = info_dict.get('thumbnails')",
            "        if thumbnails is None:",
            "            thumbnail = info_dict.get('thumbnail')",
            "            if thumbnail:",
            "                info_dict['thumbnails'] = thumbnails = [{'url': thumbnail}]",
            "        if not thumbnails:",
            "            return",
            "",
            "        def check_thumbnails(thumbnails):",
            "            for t in thumbnails:",
            "                self.to_screen(f'[info] Testing thumbnail {t[\"id\"]}')",
            "                try:",
            "                    self.urlopen(HEADRequest(t['url']))",
            "                except network_exceptions as err:",
            "                    self.to_screen(f'[info] Unable to connect to thumbnail {t[\"id\"]} URL {t[\"url\"]!r} - {err}. Skipping...')",
            "                    continue",
            "                yield t",
            "",
            "        self._sort_thumbnails(thumbnails)",
            "        for i, t in enumerate(thumbnails):",
            "            if t.get('id') is None:",
            "                t['id'] = '%d' % i",
            "            if t.get('width') and t.get('height'):",
            "                t['resolution'] = '%dx%d' % (t['width'], t['height'])",
            "            t['url'] = sanitize_url(t['url'])",
            "",
            "        if self.params.get('check_formats') is True:",
            "            info_dict['thumbnails'] = LazyList(check_thumbnails(thumbnails[::-1]), reverse=True)",
            "        else:",
            "            info_dict['thumbnails'] = thumbnails",
            "",
            "    def _fill_common_fields(self, info_dict, final=True):",
            "        # TODO: move sanitization here",
            "        if final:",
            "            title = info_dict['fulltitle'] = info_dict.get('title')",
            "            if not title:",
            "                if title == '':",
            "                    self.write_debug('Extractor gave empty title. Creating a generic title')",
            "                else:",
            "                    self.report_warning('Extractor failed to obtain \"title\". Creating a generic title instead')",
            "                info_dict['title'] = f'{info_dict[\"extractor\"].replace(\":\", \"-\")} video #{info_dict[\"id\"]}'",
            "",
            "        if info_dict.get('duration') is not None:",
            "            info_dict['duration_string'] = formatSeconds(info_dict['duration'])",
            "",
            "        for ts_key, date_key in (",
            "                ('timestamp', 'upload_date'),",
            "                ('release_timestamp', 'release_date'),",
            "                ('modified_timestamp', 'modified_date'),",
            "        ):",
            "            if info_dict.get(date_key) is None and info_dict.get(ts_key) is not None:",
            "                # Working around out-of-range timestamp values (e.g. negative ones on Windows,",
            "                # see http://bugs.python.org/issue1646728)",
            "                with contextlib.suppress(ValueError, OverflowError, OSError):",
            "                    upload_date = dt.datetime.fromtimestamp(info_dict[ts_key], dt.timezone.utc)",
            "                    info_dict[date_key] = upload_date.strftime('%Y%m%d')",
            "",
            "        if not info_dict.get('release_year'):",
            "            info_dict['release_year'] = traverse_obj(info_dict, ('release_date', {lambda x: int(x[:4])}))",
            "",
            "        live_keys = ('is_live', 'was_live')",
            "        live_status = info_dict.get('live_status')",
            "        if live_status is None:",
            "            for key in live_keys:",
            "                if info_dict.get(key) is False:",
            "                    continue",
            "                if info_dict.get(key):",
            "                    live_status = key",
            "                break",
            "            if all(info_dict.get(key) is False for key in live_keys):",
            "                live_status = 'not_live'",
            "        if live_status:",
            "            info_dict['live_status'] = live_status",
            "            for key in live_keys:",
            "                if info_dict.get(key) is None:",
            "                    info_dict[key] = (live_status == key)",
            "        if live_status == 'post_live':",
            "            info_dict['was_live'] = True",
            "",
            "        # Auto generate title fields corresponding to the *_number fields when missing",
            "        # in order to always have clean titles. This is very common for TV series.",
            "        for field in ('chapter', 'season', 'episode'):",
            "            if final and info_dict.get('%s_number' % field) is not None and not info_dict.get(field):",
            "                info_dict[field] = '%s %d' % (field.capitalize(), info_dict['%s_number' % field])",
            "",
            "        for old_key, new_key in self._deprecated_multivalue_fields.items():",
            "            if new_key in info_dict and old_key in info_dict:",
            "                if '_version' not in info_dict:  # HACK: Do not warn when using --load-info-json",
            "                    self.deprecation_warning(f'Do not return {old_key!r} when {new_key!r} is present')",
            "            elif old_value := info_dict.get(old_key):",
            "                info_dict[new_key] = old_value.split(', ')",
            "            elif new_value := info_dict.get(new_key):",
            "                info_dict[old_key] = ', '.join(v.replace(',', '\\N{FULLWIDTH COMMA}') for v in new_value)",
            "",
            "    def _raise_pending_errors(self, info):",
            "        err = info.pop('__pending_error', None)",
            "        if err:",
            "            self.report_error(err, tb=False)",
            "",
            "    def sort_formats(self, info_dict):",
            "        formats = self._get_formats(info_dict)",
            "        formats.sort(key=FormatSorter(",
            "            self, info_dict.get('_format_sort_fields') or []).calculate_preference)",
            "",
            "    def process_video_result(self, info_dict, download=True):",
            "        assert info_dict.get('_type', 'video') == 'video'",
            "        self._num_videos += 1",
            "",
            "        if 'id' not in info_dict:",
            "            raise ExtractorError('Missing \"id\" field in extractor result', ie=info_dict['extractor'])",
            "        elif not info_dict.get('id'):",
            "            raise ExtractorError('Extractor failed to obtain \"id\"', ie=info_dict['extractor'])",
            "",
            "        def report_force_conversion(field, field_not, conversion):",
            "            self.report_warning(",
            "                '\"%s\" field is not %s - forcing %s conversion, there is an error in extractor'",
            "                % (field, field_not, conversion))",
            "",
            "        def sanitize_string_field(info, string_field):",
            "            field = info.get(string_field)",
            "            if field is None or isinstance(field, str):",
            "                return",
            "            report_force_conversion(string_field, 'a string', 'string')",
            "            info[string_field] = str(field)",
            "",
            "        def sanitize_numeric_fields(info):",
            "            for numeric_field in self._NUMERIC_FIELDS:",
            "                field = info.get(numeric_field)",
            "                if field is None or isinstance(field, (int, float)):",
            "                    continue",
            "                report_force_conversion(numeric_field, 'numeric', 'int')",
            "                info[numeric_field] = int_or_none(field)",
            "",
            "        sanitize_string_field(info_dict, 'id')",
            "        sanitize_numeric_fields(info_dict)",
            "        if info_dict.get('section_end') and info_dict.get('section_start') is not None:",
            "            info_dict['duration'] = round(info_dict['section_end'] - info_dict['section_start'], 3)",
            "        if (info_dict.get('duration') or 0) <= 0 and info_dict.pop('duration', None):",
            "            self.report_warning('\"duration\" field is negative, there is an error in extractor')",
            "",
            "        chapters = info_dict.get('chapters') or []",
            "        if chapters and chapters[0].get('start_time'):",
            "            chapters.insert(0, {'start_time': 0})",
            "",
            "        dummy_chapter = {'end_time': 0, 'start_time': info_dict.get('duration')}",
            "        for idx, (prev, current, next_) in enumerate(zip(",
            "                (dummy_chapter, *chapters), chapters, (*chapters[1:], dummy_chapter)), 1):",
            "            if current.get('start_time') is None:",
            "                current['start_time'] = prev.get('end_time')",
            "            if not current.get('end_time'):",
            "                current['end_time'] = next_.get('start_time')",
            "            if not current.get('title'):",
            "                current['title'] = f'<Untitled Chapter {idx}>'",
            "",
            "        if 'playlist' not in info_dict:",
            "            # It isn't part of a playlist",
            "            info_dict['playlist'] = None",
            "            info_dict['playlist_index'] = None",
            "",
            "        self._sanitize_thumbnails(info_dict)",
            "",
            "        thumbnail = info_dict.get('thumbnail')",
            "        thumbnails = info_dict.get('thumbnails')",
            "        if thumbnail:",
            "            info_dict['thumbnail'] = sanitize_url(thumbnail)",
            "        elif thumbnails:",
            "            info_dict['thumbnail'] = thumbnails[-1]['url']",
            "",
            "        if info_dict.get('display_id') is None and 'id' in info_dict:",
            "            info_dict['display_id'] = info_dict['id']",
            "",
            "        self._fill_common_fields(info_dict)",
            "",
            "        for cc_kind in ('subtitles', 'automatic_captions'):",
            "            cc = info_dict.get(cc_kind)",
            "            if cc:",
            "                for _, subtitle in cc.items():",
            "                    for subtitle_format in subtitle:",
            "                        if subtitle_format.get('url'):",
            "                            subtitle_format['url'] = sanitize_url(subtitle_format['url'])",
            "                        if subtitle_format.get('ext') is None:",
            "                            subtitle_format['ext'] = determine_ext(subtitle_format['url']).lower()",
            "",
            "        automatic_captions = info_dict.get('automatic_captions')",
            "        subtitles = info_dict.get('subtitles')",
            "",
            "        info_dict['requested_subtitles'] = self.process_subtitles(",
            "            info_dict['id'], subtitles, automatic_captions)",
            "",
            "        formats = self._get_formats(info_dict)",
            "",
            "        # Backward compatibility with InfoExtractor._sort_formats",
            "        field_preference = (formats or [{}])[0].pop('__sort_fields', None)",
            "        if field_preference:",
            "            info_dict['_format_sort_fields'] = field_preference",
            "",
            "        info_dict['_has_drm'] = any(  # or None ensures --clean-infojson removes it",
            "            f.get('has_drm') and f['has_drm'] != 'maybe' for f in formats) or None",
            "        if not self.params.get('allow_unplayable_formats'):",
            "            formats = [f for f in formats if not f.get('has_drm') or f['has_drm'] == 'maybe']",
            "",
            "        if formats and all(f.get('acodec') == f.get('vcodec') == 'none' for f in formats):",
            "            self.report_warning(",
            "                f'{\"This video is DRM protected and \" if info_dict[\"_has_drm\"] else \"\"}'",
            "                'only images are available for download. Use --list-formats to see them'.capitalize())",
            "",
            "        get_from_start = not info_dict.get('is_live') or bool(self.params.get('live_from_start'))",
            "        if not get_from_start:",
            "            info_dict['title'] += ' ' + dt.datetime.now().strftime('%Y-%m-%d %H:%M')",
            "        if info_dict.get('is_live') and formats:",
            "            formats = [f for f in formats if bool(f.get('is_from_start')) == get_from_start]",
            "            if get_from_start and not formats:",
            "                self.raise_no_formats(info_dict, msg=(",
            "                    '--live-from-start is passed, but there are no formats that can be downloaded from the start. '",
            "                    'If you want to download from the current time, use --no-live-from-start'))",
            "",
            "        def is_wellformed(f):",
            "            url = f.get('url')",
            "            if not url:",
            "                self.report_warning(",
            "                    '\"url\" field is missing or empty - skipping format, '",
            "                    'there is an error in extractor')",
            "                return False",
            "            if isinstance(url, bytes):",
            "                sanitize_string_field(f, 'url')",
            "            return True",
            "",
            "        # Filter out malformed formats for better extraction robustness",
            "        formats = list(filter(is_wellformed, formats or []))",
            "",
            "        if not formats:",
            "            self.raise_no_formats(info_dict)",
            "",
            "        for format in formats:",
            "            sanitize_string_field(format, 'format_id')",
            "            sanitize_numeric_fields(format)",
            "            format['url'] = sanitize_url(format['url'])",
            "            if format.get('ext') is None:",
            "                format['ext'] = determine_ext(format['url']).lower()",
            "            if format['ext'] in ('aac', 'opus', 'mp3', 'flac', 'vorbis'):",
            "                if format.get('acodec') is None:",
            "                    format['acodec'] = format['ext']",
            "            if format.get('protocol') is None:",
            "                format['protocol'] = determine_protocol(format)",
            "            if format.get('resolution') is None:",
            "                format['resolution'] = self.format_resolution(format, default=None)",
            "            if format.get('dynamic_range') is None and format.get('vcodec') != 'none':",
            "                format['dynamic_range'] = 'SDR'",
            "            if format.get('aspect_ratio') is None:",
            "                format['aspect_ratio'] = try_call(lambda: round(format['width'] / format['height'], 2))",
            "            # For fragmented formats, \"tbr\" is often max bitrate and not average",
            "            if (('manifest-filesize-approx' in self.params['compat_opts'] or not format.get('manifest_url'))",
            "                    and not format.get('filesize') and not format.get('filesize_approx')):",
            "                format['filesize_approx'] = filesize_from_tbr(format.get('tbr'), info_dict.get('duration'))",
            "            format['http_headers'] = self._calc_headers(collections.ChainMap(format, info_dict), load_cookies=True)",
            "",
            "        # Safeguard against old/insecure infojson when using --load-info-json",
            "        if info_dict.get('http_headers'):",
            "            info_dict['http_headers'] = HTTPHeaderDict(info_dict['http_headers'])",
            "            info_dict['http_headers'].pop('Cookie', None)",
            "",
            "        # This is copied to http_headers by the above _calc_headers and can now be removed",
            "        if '__x_forwarded_for_ip' in info_dict:",
            "            del info_dict['__x_forwarded_for_ip']",
            "",
            "        self.sort_formats({",
            "            'formats': formats,",
            "            '_format_sort_fields': info_dict.get('_format_sort_fields')",
            "        })",
            "",
            "        # Sanitize and group by format_id",
            "        formats_dict = {}",
            "        for i, format in enumerate(formats):",
            "            if not format.get('format_id'):",
            "                format['format_id'] = str(i)",
            "            else:",
            "                # Sanitize format_id from characters used in format selector expression",
            "                format['format_id'] = re.sub(r'[\\s,/+\\[\\]()]', '_', format['format_id'])",
            "            formats_dict.setdefault(format['format_id'], []).append(format)",
            "",
            "        # Make sure all formats have unique format_id",
            "        common_exts = set(itertools.chain(*self._format_selection_exts.values()))",
            "        for format_id, ambiguous_formats in formats_dict.items():",
            "            ambigious_id = len(ambiguous_formats) > 1",
            "            for i, format in enumerate(ambiguous_formats):",
            "                if ambigious_id:",
            "                    format['format_id'] = '%s-%d' % (format_id, i)",
            "                # Ensure there is no conflict between id and ext in format selection",
            "                # See https://github.com/yt-dlp/yt-dlp/issues/1282",
            "                if format['format_id'] != format['ext'] and format['format_id'] in common_exts:",
            "                    format['format_id'] = 'f%s' % format['format_id']",
            "",
            "                if format.get('format') is None:",
            "                    format['format'] = '{id} - {res}{note}'.format(",
            "                        id=format['format_id'],",
            "                        res=self.format_resolution(format),",
            "                        note=format_field(format, 'format_note', ' (%s)'),",
            "                    )",
            "",
            "        if self.params.get('check_formats') is True:",
            "            formats = LazyList(self._check_formats(formats[::-1]), reverse=True)",
            "",
            "        if not formats or formats[0] is not info_dict:",
            "            # only set the 'formats' fields if the original info_dict list them",
            "            # otherwise we end up with a circular reference, the first (and unique)",
            "            # element in the 'formats' field in info_dict is info_dict itself,",
            "            # which can't be exported to json",
            "            info_dict['formats'] = formats",
            "",
            "        info_dict, _ = self.pre_process(info_dict)",
            "",
            "        if self._match_entry(info_dict, incomplete=self._format_fields) is not None:",
            "            return info_dict",
            "",
            "        self.post_extract(info_dict)",
            "        info_dict, _ = self.pre_process(info_dict, 'after_filter')",
            "",
            "        # The pre-processors may have modified the formats",
            "        formats = self._get_formats(info_dict)",
            "",
            "        list_only = self.params.get('simulate') == 'list_only'",
            "        interactive_format_selection = not list_only and self.format_selector == '-'",
            "        if self.params.get('list_thumbnails'):",
            "            self.list_thumbnails(info_dict)",
            "        if self.params.get('listsubtitles'):",
            "            if 'automatic_captions' in info_dict:",
            "                self.list_subtitles(",
            "                    info_dict['id'], automatic_captions, 'automatic captions')",
            "            self.list_subtitles(info_dict['id'], subtitles, 'subtitles')",
            "        if self.params.get('listformats') or interactive_format_selection:",
            "            self.list_formats(info_dict)",
            "        if list_only:",
            "            # Without this printing, -F --print-json will not work",
            "            self.__forced_printings(info_dict)",
            "            return info_dict",
            "",
            "        format_selector = self.format_selector",
            "        while True:",
            "            if interactive_format_selection:",
            "                req_format = input(self._format_screen('\\nEnter format selector ', self.Styles.EMPHASIS)",
            "                                   + '(Press ENTER for default, or Ctrl+C to quit)'",
            "                                   + self._format_screen(': ', self.Styles.EMPHASIS))",
            "                try:",
            "                    format_selector = self.build_format_selector(req_format) if req_format else None",
            "                except SyntaxError as err:",
            "                    self.report_error(err, tb=False, is_error=False)",
            "                    continue",
            "",
            "            if format_selector is None:",
            "                req_format = self._default_format_spec(info_dict, download=download)",
            "                self.write_debug(f'Default format spec: {req_format}')",
            "                format_selector = self.build_format_selector(req_format)",
            "",
            "            formats_to_download = list(format_selector({",
            "                'formats': formats,",
            "                'has_merged_format': any('none' not in (f.get('acodec'), f.get('vcodec')) for f in formats),",
            "                'incomplete_formats': (all(f.get('vcodec') == 'none' for f in formats)  # No formats with video",
            "                                       or all(f.get('acodec') == 'none' for f in formats)),  # OR, No formats with audio",
            "            }))",
            "            if interactive_format_selection and not formats_to_download:",
            "                self.report_error('Requested format is not available', tb=False, is_error=False)",
            "                continue",
            "            break",
            "",
            "        if not formats_to_download:",
            "            if not self.params.get('ignore_no_formats_error'):",
            "                raise ExtractorError(",
            "                    'Requested format is not available. Use --list-formats for a list of available formats',",
            "                    expected=True, video_id=info_dict['id'], ie=info_dict['extractor'])",
            "            self.report_warning('Requested format is not available')",
            "            # Process what we can, even without any available formats.",
            "            formats_to_download = [{}]",
            "",
            "        requested_ranges = tuple(self.params.get('download_ranges', lambda *_: [{}])(info_dict, self))",
            "        best_format, downloaded_formats = formats_to_download[-1], []",
            "        if download:",
            "            if best_format and requested_ranges:",
            "                def to_screen(*msg):",
            "                    self.to_screen(f'[info] {info_dict[\"id\"]}: {\" \".join(\", \".join(variadic(m)) for m in msg)}')",
            "",
            "                to_screen(f'Downloading {len(formats_to_download)} format(s):',",
            "                          (f['format_id'] for f in formats_to_download))",
            "                if requested_ranges != ({}, ):",
            "                    to_screen(f'Downloading {len(requested_ranges)} time ranges:',",
            "                              (f'{c[\"start_time\"]:.1f}-{c[\"end_time\"]:.1f}' for c in requested_ranges))",
            "            max_downloads_reached = False",
            "",
            "            for fmt, chapter in itertools.product(formats_to_download, requested_ranges):",
            "                new_info = self._copy_infodict(info_dict)",
            "                new_info.update(fmt)",
            "                offset, duration = info_dict.get('section_start') or 0, info_dict.get('duration') or float('inf')",
            "                end_time = offset + min(chapter.get('end_time', duration), duration)",
            "                # duration may not be accurate. So allow deviations <1sec",
            "                if end_time == float('inf') or end_time > offset + duration + 1:",
            "                    end_time = None",
            "                if chapter or offset:",
            "                    new_info.update({",
            "                        'section_start': offset + chapter.get('start_time', 0),",
            "                        'section_end': end_time,",
            "                        'section_title': chapter.get('title'),",
            "                        'section_number': chapter.get('index'),",
            "                    })",
            "                downloaded_formats.append(new_info)",
            "                try:",
            "                    self.process_info(new_info)",
            "                except MaxDownloadsReached:",
            "                    max_downloads_reached = True",
            "                self._raise_pending_errors(new_info)",
            "                # Remove copied info",
            "                for key, val in tuple(new_info.items()):",
            "                    if info_dict.get(key) == val:",
            "                        new_info.pop(key)",
            "                if max_downloads_reached:",
            "                    break",
            "",
            "            write_archive = {f.get('__write_download_archive', False) for f in downloaded_formats}",
            "            assert write_archive.issubset({True, False, 'ignore'})",
            "            if True in write_archive and False not in write_archive:",
            "                self.record_download_archive(info_dict)",
            "",
            "            info_dict['requested_downloads'] = downloaded_formats",
            "            info_dict = self.run_all_pps('after_video', info_dict)",
            "            if max_downloads_reached:",
            "                raise MaxDownloadsReached()",
            "",
            "        # We update the info dict with the selected best quality format (backwards compatibility)",
            "        info_dict.update(best_format)",
            "        return info_dict",
            "",
            "    def process_subtitles(self, video_id, normal_subtitles, automatic_captions):",
            "        \"\"\"Select the requested subtitles and their format\"\"\"",
            "        available_subs, normal_sub_langs = {}, []",
            "        if normal_subtitles and self.params.get('writesubtitles'):",
            "            available_subs.update(normal_subtitles)",
            "            normal_sub_langs = tuple(normal_subtitles.keys())",
            "        if automatic_captions and self.params.get('writeautomaticsub'):",
            "            for lang, cap_info in automatic_captions.items():",
            "                if lang not in available_subs:",
            "                    available_subs[lang] = cap_info",
            "",
            "        if not available_subs or (",
            "                not self.params.get('writesubtitles')",
            "                and not self.params.get('writeautomaticsub')):",
            "            return None",
            "",
            "        all_sub_langs = tuple(available_subs.keys())",
            "        if self.params.get('allsubtitles', False):",
            "            requested_langs = all_sub_langs",
            "        elif self.params.get('subtitleslangs', False):",
            "            try:",
            "                requested_langs = orderedSet_from_options(",
            "                    self.params.get('subtitleslangs'), {'all': all_sub_langs}, use_regex=True)",
            "            except re.error as e:",
            "                raise ValueError(f'Wrong regex for subtitlelangs: {e.pattern}')",
            "        else:",
            "            requested_langs = LazyList(itertools.chain(",
            "                ['en'] if 'en' in normal_sub_langs else [],",
            "                filter(lambda f: f.startswith('en'), normal_sub_langs),",
            "                ['en'] if 'en' in all_sub_langs else [],",
            "                filter(lambda f: f.startswith('en'), all_sub_langs),",
            "                normal_sub_langs, all_sub_langs,",
            "            ))[:1]",
            "        if requested_langs:",
            "            self.to_screen(f'[info] {video_id}: Downloading subtitles: {\", \".join(requested_langs)}')",
            "",
            "        formats_query = self.params.get('subtitlesformat', 'best')",
            "        formats_preference = formats_query.split('/') if formats_query else []",
            "        subs = {}",
            "        for lang in requested_langs:",
            "            formats = available_subs.get(lang)",
            "            if formats is None:",
            "                self.report_warning(f'{lang} subtitles not available for {video_id}')",
            "                continue",
            "            for ext in formats_preference:",
            "                if ext == 'best':",
            "                    f = formats[-1]",
            "                    break",
            "                matches = list(filter(lambda f: f['ext'] == ext, formats))",
            "                if matches:",
            "                    f = matches[-1]",
            "                    break",
            "            else:",
            "                f = formats[-1]",
            "                self.report_warning(",
            "                    'No subtitle format found matching \"%s\" for language %s, '",
            "                    'using %s' % (formats_query, lang, f['ext']))",
            "            subs[lang] = f",
            "        return subs",
            "",
            "    def _forceprint(self, key, info_dict):",
            "        if info_dict is None:",
            "            return",
            "        info_copy = info_dict.copy()",
            "        info_copy.setdefault('filename', self.prepare_filename(info_dict))",
            "        if info_dict.get('requested_formats') is not None:",
            "            # For RTMP URLs, also include the playpath",
            "            info_copy['urls'] = '\\n'.join(f['url'] + f.get('play_path', '') for f in info_dict['requested_formats'])",
            "        elif info_dict.get('url'):",
            "            info_copy['urls'] = info_dict['url'] + info_dict.get('play_path', '')",
            "        info_copy['formats_table'] = self.render_formats_table(info_dict)",
            "        info_copy['thumbnails_table'] = self.render_thumbnails_table(info_dict)",
            "        info_copy['subtitles_table'] = self.render_subtitles_table(info_dict.get('id'), info_dict.get('subtitles'))",
            "        info_copy['automatic_captions_table'] = self.render_subtitles_table(info_dict.get('id'), info_dict.get('automatic_captions'))",
            "",
            "        def format_tmpl(tmpl):",
            "            mobj = re.fullmatch(r'([\\w.:,]|-\\d|(?P<dict>{([\\w.:,]|-\\d)+}))+=?', tmpl)",
            "            if not mobj:",
            "                return tmpl",
            "",
            "            fmt = '%({})s'",
            "            if tmpl.startswith('{'):",
            "                tmpl, fmt = f'.{tmpl}', '%({})j'",
            "            if tmpl.endswith('='):",
            "                tmpl, fmt = tmpl[:-1], '{0} = %({0})#j'",
            "            return '\\n'.join(map(fmt.format, [tmpl] if mobj.group('dict') else tmpl.split(',')))",
            "",
            "        for tmpl in self.params['forceprint'].get(key, []):",
            "            self.to_stdout(self.evaluate_outtmpl(format_tmpl(tmpl), info_copy))",
            "",
            "        for tmpl, file_tmpl in self.params['print_to_file'].get(key, []):",
            "            filename = self.prepare_filename(info_dict, outtmpl=file_tmpl)",
            "            tmpl = format_tmpl(tmpl)",
            "            self.to_screen(f'[info] Writing {tmpl!r} to: {filename}')",
            "            if self._ensure_dir_exists(filename):",
            "                with open(filename, 'a', encoding='utf-8', newline='') as f:",
            "                    f.write(self.evaluate_outtmpl(tmpl, info_copy) + os.linesep)",
            "",
            "        return info_copy",
            "",
            "    def __forced_printings(self, info_dict, filename=None, incomplete=True):",
            "        if (self.params.get('forcejson')",
            "                or self.params['forceprint'].get('video')",
            "                or self.params['print_to_file'].get('video')):",
            "            self.post_extract(info_dict)",
            "        if filename:",
            "            info_dict['filename'] = filename",
            "        info_copy = self._forceprint('video', info_dict)",
            "",
            "        def print_field(field, actual_field=None, optional=False):",
            "            if actual_field is None:",
            "                actual_field = field",
            "            if self.params.get(f'force{field}') and (",
            "                    info_copy.get(field) is not None or (not optional and not incomplete)):",
            "                self.to_stdout(info_copy[actual_field])",
            "",
            "        print_field('title')",
            "        print_field('id')",
            "        print_field('url', 'urls')",
            "        print_field('thumbnail', optional=True)",
            "        print_field('description', optional=True)",
            "        print_field('filename')",
            "        if self.params.get('forceduration') and info_copy.get('duration') is not None:",
            "            self.to_stdout(formatSeconds(info_copy['duration']))",
            "        print_field('format')",
            "",
            "        if self.params.get('forcejson'):",
            "            self.to_stdout(json.dumps(self.sanitize_info(info_dict)))",
            "",
            "    def dl(self, name, info, subtitle=False, test=False):",
            "        if not info.get('url'):",
            "            self.raise_no_formats(info, True)",
            "",
            "        if test:",
            "            verbose = self.params.get('verbose')",
            "            params = {",
            "                'test': True,",
            "                'quiet': self.params.get('quiet') or not verbose,",
            "                'verbose': verbose,",
            "                'noprogress': not verbose,",
            "                'nopart': True,",
            "                'skip_unavailable_fragments': False,",
            "                'keep_fragments': False,",
            "                'overwrites': True,",
            "                '_no_ytdl_file': True,",
            "            }",
            "        else:",
            "            params = self.params",
            "        fd = get_suitable_downloader(info, params, to_stdout=(name == '-'))(self, params)",
            "        if not test:",
            "            for ph in self._progress_hooks:",
            "                fd.add_progress_hook(ph)",
            "            urls = '\", \"'.join(",
            "                (f['url'].split(',')[0] + ',<data>' if f['url'].startswith('data:') else f['url'])",
            "                for f in info.get('requested_formats', []) or [info])",
            "            self.write_debug(f'Invoking {fd.FD_NAME} downloader on \"{urls}\"')",
            "",
            "        # Note: Ideally info should be a deep-copied so that hooks cannot modify it.",
            "        # But it may contain objects that are not deep-copyable",
            "        new_info = self._copy_infodict(info)",
            "        if new_info.get('http_headers') is None:",
            "            new_info['http_headers'] = self._calc_headers(new_info)",
            "        return fd.download(name, new_info, subtitle)",
            "",
            "    def existing_file(self, filepaths, *, default_overwrite=True):",
            "        existing_files = list(filter(os.path.exists, orderedSet(filepaths)))",
            "        if existing_files and not self.params.get('overwrites', default_overwrite):",
            "            return existing_files[0]",
            "",
            "        for file in existing_files:",
            "            self.report_file_delete(file)",
            "            os.remove(file)",
            "        return None",
            "",
            "    def process_info(self, info_dict):",
            "        \"\"\"Process a single resolved IE result. (Modifies it in-place)\"\"\"",
            "",
            "        assert info_dict.get('_type', 'video') == 'video'",
            "        original_infodict = info_dict",
            "",
            "        if 'format' not in info_dict and 'ext' in info_dict:",
            "            info_dict['format'] = info_dict['ext']",
            "",
            "        if self._match_entry(info_dict) is not None:",
            "            info_dict['__write_download_archive'] = 'ignore'",
            "            return",
            "",
            "        # Does nothing under normal operation - for backward compatibility of process_info",
            "        self.post_extract(info_dict)",
            "",
            "        def replace_info_dict(new_info):",
            "            nonlocal info_dict",
            "            if new_info == info_dict:",
            "                return",
            "            info_dict.clear()",
            "            info_dict.update(new_info)",
            "",
            "        new_info, _ = self.pre_process(info_dict, 'video')",
            "        replace_info_dict(new_info)",
            "        self._num_downloads += 1",
            "",
            "        # info_dict['_filename'] needs to be set for backward compatibility",
            "        info_dict['_filename'] = full_filename = self.prepare_filename(info_dict, warn=True)",
            "        temp_filename = self.prepare_filename(info_dict, 'temp')",
            "        files_to_move = {}",
            "",
            "        # Forced printings",
            "        self.__forced_printings(info_dict, full_filename, incomplete=('format' not in info_dict))",
            "",
            "        def check_max_downloads():",
            "            if self._num_downloads >= float(self.params.get('max_downloads') or 'inf'):",
            "                raise MaxDownloadsReached()",
            "",
            "        if self.params.get('simulate'):",
            "            info_dict['__write_download_archive'] = self.params.get('force_write_download_archive')",
            "            check_max_downloads()",
            "            return",
            "",
            "        if full_filename is None:",
            "            return",
            "        if not self._ensure_dir_exists(encodeFilename(full_filename)):",
            "            return",
            "        if not self._ensure_dir_exists(encodeFilename(temp_filename)):",
            "            return",
            "",
            "        if self._write_description('video', info_dict,",
            "                                   self.prepare_filename(info_dict, 'description')) is None:",
            "            return",
            "",
            "        sub_files = self._write_subtitles(info_dict, temp_filename)",
            "        if sub_files is None:",
            "            return",
            "        files_to_move.update(dict(sub_files))",
            "",
            "        thumb_files = self._write_thumbnails(",
            "            'video', info_dict, temp_filename, self.prepare_filename(info_dict, 'thumbnail'))",
            "        if thumb_files is None:",
            "            return",
            "        files_to_move.update(dict(thumb_files))",
            "",
            "        infofn = self.prepare_filename(info_dict, 'infojson')",
            "        _infojson_written = self._write_info_json('video', info_dict, infofn)",
            "        if _infojson_written:",
            "            info_dict['infojson_filename'] = infofn",
            "            # For backward compatibility, even though it was a private field",
            "            info_dict['__infojson_filename'] = infofn",
            "        elif _infojson_written is None:",
            "            return",
            "",
            "        # Note: Annotations are deprecated",
            "        annofn = None",
            "        if self.params.get('writeannotations', False):",
            "            annofn = self.prepare_filename(info_dict, 'annotation')",
            "        if annofn:",
            "            if not self._ensure_dir_exists(encodeFilename(annofn)):",
            "                return",
            "            if not self.params.get('overwrites', True) and os.path.exists(encodeFilename(annofn)):",
            "                self.to_screen('[info] Video annotations are already present')",
            "            elif not info_dict.get('annotations'):",
            "                self.report_warning('There are no annotations to write.')",
            "            else:",
            "                try:",
            "                    self.to_screen('[info] Writing video annotations to: ' + annofn)",
            "                    with open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:",
            "                        annofile.write(info_dict['annotations'])",
            "                except (KeyError, TypeError):",
            "                    self.report_warning('There are no annotations to write.')",
            "                except OSError:",
            "                    self.report_error('Cannot write annotations file: ' + annofn)",
            "                    return",
            "",
            "        # Write internet shortcut files",
            "        def _write_link_file(link_type):",
            "            url = try_get(info_dict['webpage_url'], iri_to_uri)",
            "            if not url:",
            "                self.report_warning(",
            "                    f'Cannot write internet shortcut file because the actual URL of \"{info_dict[\"webpage_url\"]}\" is unknown')",
            "                return True",
            "            linkfn = replace_extension(self.prepare_filename(info_dict, 'link'), link_type, info_dict.get('ext'))",
            "            if not self._ensure_dir_exists(encodeFilename(linkfn)):",
            "                return False",
            "            if self.params.get('overwrites', True) and os.path.exists(encodeFilename(linkfn)):",
            "                self.to_screen(f'[info] Internet shortcut (.{link_type}) is already present')",
            "                return True",
            "            try:",
            "                self.to_screen(f'[info] Writing internet shortcut (.{link_type}) to: {linkfn}')",
            "                with open(encodeFilename(to_high_limit_path(linkfn)), 'w', encoding='utf-8',",
            "                          newline='\\r\\n' if link_type == 'url' else '\\n') as linkfile:",
            "                    template_vars = {'url': url}",
            "                    if link_type == 'desktop':",
            "                        template_vars['filename'] = linkfn[:-(len(link_type) + 1)]",
            "                    linkfile.write(LINK_TEMPLATES[link_type] % template_vars)",
            "            except OSError:",
            "                self.report_error(f'Cannot write internet shortcut {linkfn}')",
            "                return False",
            "            return True",
            "",
            "        write_links = {",
            "            'url': self.params.get('writeurllink'),",
            "            'webloc': self.params.get('writewebloclink'),",
            "            'desktop': self.params.get('writedesktoplink'),",
            "        }",
            "        if self.params.get('writelink'):",
            "            link_type = ('webloc' if sys.platform == 'darwin'",
            "                         else 'desktop' if sys.platform.startswith('linux')",
            "                         else 'url')",
            "            write_links[link_type] = True",
            "",
            "        if any(should_write and not _write_link_file(link_type)",
            "               for link_type, should_write in write_links.items()):",
            "            return",
            "",
            "        new_info, files_to_move = self.pre_process(info_dict, 'before_dl', files_to_move)",
            "        replace_info_dict(new_info)",
            "",
            "        if self.params.get('skip_download'):",
            "            info_dict['filepath'] = temp_filename",
            "            info_dict['__finaldir'] = os.path.dirname(os.path.abspath(encodeFilename(full_filename)))",
            "            info_dict['__files_to_move'] = files_to_move",
            "            replace_info_dict(self.run_pp(MoveFilesAfterDownloadPP(self, False), info_dict))",
            "            info_dict['__write_download_archive'] = self.params.get('force_write_download_archive')",
            "        else:",
            "            # Download",
            "            info_dict.setdefault('__postprocessors', [])",
            "            try:",
            "",
            "                def existing_video_file(*filepaths):",
            "                    ext = info_dict.get('ext')",
            "                    converted = lambda file: replace_extension(file, self.params.get('final_ext') or ext, ext)",
            "                    file = self.existing_file(itertools.chain(*zip(map(converted, filepaths), filepaths)),",
            "                                              default_overwrite=False)",
            "                    if file:",
            "                        info_dict['ext'] = os.path.splitext(file)[1][1:]",
            "                    return file",
            "",
            "                fd, success = None, True",
            "                if info_dict.get('protocol') or info_dict.get('url'):",
            "                    fd = get_suitable_downloader(info_dict, self.params, to_stdout=temp_filename == '-')",
            "                    if fd != FFmpegFD and 'no-direct-merge' not in self.params['compat_opts'] and (",
            "                            info_dict.get('section_start') or info_dict.get('section_end')):",
            "                        msg = ('This format cannot be partially downloaded' if FFmpegFD.available()",
            "                               else 'You have requested downloading the video partially, but ffmpeg is not installed')",
            "                        self.report_error(f'{msg}. Aborting')",
            "                        return",
            "",
            "                if info_dict.get('requested_formats') is not None:",
            "                    old_ext = info_dict['ext']",
            "                    if self.params.get('merge_output_format') is None:",
            "                        if (info_dict['ext'] == 'webm'",
            "                                and info_dict.get('thumbnails')",
            "                                # check with type instead of pp_key, __name__, or isinstance",
            "                                # since we dont want any custom PPs to trigger this",
            "                                and any(type(pp) == EmbedThumbnailPP for pp in self._pps['post_process'])):  # noqa: E721",
            "                            info_dict['ext'] = 'mkv'",
            "                            self.report_warning(",
            "                                'webm doesn\\'t support embedding a thumbnail, mkv will be used')",
            "                    new_ext = info_dict['ext']",
            "",
            "                    def correct_ext(filename, ext=new_ext):",
            "                        if filename == '-':",
            "                            return filename",
            "                        filename_real_ext = os.path.splitext(filename)[1][1:]",
            "                        filename_wo_ext = (",
            "                            os.path.splitext(filename)[0]",
            "                            if filename_real_ext in (old_ext, new_ext)",
            "                            else filename)",
            "                        return f'{filename_wo_ext}.{ext}'",
            "",
            "                    # Ensure filename always has a correct extension for successful merge",
            "                    full_filename = correct_ext(full_filename)",
            "                    temp_filename = correct_ext(temp_filename)",
            "                    dl_filename = existing_video_file(full_filename, temp_filename)",
            "",
            "                    info_dict['__real_download'] = False",
            "                    # NOTE: Copy so that original format dicts are not modified",
            "                    info_dict['requested_formats'] = list(map(dict, info_dict['requested_formats']))",
            "",
            "                    merger = FFmpegMergerPP(self)",
            "                    downloaded = []",
            "                    if dl_filename is not None:",
            "                        self.report_file_already_downloaded(dl_filename)",
            "                    elif fd:",
            "                        for f in info_dict['requested_formats'] if fd != FFmpegFD else []:",
            "                            f['filepath'] = fname = prepend_extension(",
            "                                correct_ext(temp_filename, info_dict['ext']),",
            "                                'f%s' % f['format_id'], info_dict['ext'])",
            "                            downloaded.append(fname)",
            "                        info_dict['url'] = '\\n'.join(f['url'] for f in info_dict['requested_formats'])",
            "                        success, real_download = self.dl(temp_filename, info_dict)",
            "                        info_dict['__real_download'] = real_download",
            "                    else:",
            "                        if self.params.get('allow_unplayable_formats'):",
            "                            self.report_warning(",
            "                                'You have requested merging of multiple formats '",
            "                                'while also allowing unplayable formats to be downloaded. '",
            "                                'The formats won\\'t be merged to prevent data corruption.')",
            "                        elif not merger.available:",
            "                            msg = 'You have requested merging of multiple formats but ffmpeg is not installed'",
            "                            if not self.params.get('ignoreerrors'):",
            "                                self.report_error(f'{msg}. Aborting due to --abort-on-error')",
            "                                return",
            "                            self.report_warning(f'{msg}. The formats won\\'t be merged')",
            "",
            "                        if temp_filename == '-':",
            "                            reason = ('using a downloader other than ffmpeg' if FFmpegFD.can_merge_formats(info_dict, self.params)",
            "                                      else 'but the formats are incompatible for simultaneous download' if merger.available",
            "                                      else 'but ffmpeg is not installed')",
            "                            self.report_warning(",
            "                                f'You have requested downloading multiple formats to stdout {reason}. '",
            "                                'The formats will be streamed one after the other')",
            "                            fname = temp_filename",
            "                        for f in info_dict['requested_formats']:",
            "                            new_info = dict(info_dict)",
            "                            del new_info['requested_formats']",
            "                            new_info.update(f)",
            "                            if temp_filename != '-':",
            "                                fname = prepend_extension(",
            "                                    correct_ext(temp_filename, new_info['ext']),",
            "                                    'f%s' % f['format_id'], new_info['ext'])",
            "                                if not self._ensure_dir_exists(fname):",
            "                                    return",
            "                                f['filepath'] = fname",
            "                                downloaded.append(fname)",
            "                            partial_success, real_download = self.dl(fname, new_info)",
            "                            info_dict['__real_download'] = info_dict['__real_download'] or real_download",
            "                            success = success and partial_success",
            "",
            "                    if downloaded and merger.available and not self.params.get('allow_unplayable_formats'):",
            "                        info_dict['__postprocessors'].append(merger)",
            "                        info_dict['__files_to_merge'] = downloaded",
            "                        # Even if there were no downloads, it is being merged only now",
            "                        info_dict['__real_download'] = True",
            "                    else:",
            "                        for file in downloaded:",
            "                            files_to_move[file] = None",
            "                else:",
            "                    # Just a single file",
            "                    dl_filename = existing_video_file(full_filename, temp_filename)",
            "                    if dl_filename is None or dl_filename == temp_filename:",
            "                        # dl_filename == temp_filename could mean that the file was partially downloaded with --no-part.",
            "                        # So we should try to resume the download",
            "                        success, real_download = self.dl(temp_filename, info_dict)",
            "                        info_dict['__real_download'] = real_download",
            "                    else:",
            "                        self.report_file_already_downloaded(dl_filename)",
            "",
            "                dl_filename = dl_filename or temp_filename",
            "                info_dict['__finaldir'] = os.path.dirname(os.path.abspath(encodeFilename(full_filename)))",
            "",
            "            except network_exceptions as err:",
            "                self.report_error('unable to download video data: %s' % error_to_compat_str(err))",
            "                return",
            "            except OSError as err:",
            "                raise UnavailableVideoError(err)",
            "            except (ContentTooShortError, ) as err:",
            "                self.report_error(f'content too short (expected {err.expected} bytes and served {err.downloaded})')",
            "                return",
            "",
            "            self._raise_pending_errors(info_dict)",
            "            if success and full_filename != '-':",
            "",
            "                def fixup():",
            "                    do_fixup = True",
            "                    fixup_policy = self.params.get('fixup')",
            "                    vid = info_dict['id']",
            "",
            "                    if fixup_policy in ('ignore', 'never'):",
            "                        return",
            "                    elif fixup_policy == 'warn':",
            "                        do_fixup = 'warn'",
            "                    elif fixup_policy != 'force':",
            "                        assert fixup_policy in ('detect_or_warn', None)",
            "                        if not info_dict.get('__real_download'):",
            "                            do_fixup = False",
            "",
            "                    def ffmpeg_fixup(cndn, msg, cls):",
            "                        if not (do_fixup and cndn):",
            "                            return",
            "                        elif do_fixup == 'warn':",
            "                            self.report_warning(f'{vid}: {msg}')",
            "                            return",
            "                        pp = cls(self)",
            "                        if pp.available:",
            "                            info_dict['__postprocessors'].append(pp)",
            "                        else:",
            "                            self.report_warning(f'{vid}: {msg}. Install ffmpeg to fix this automatically')",
            "",
            "                    stretched_ratio = info_dict.get('stretched_ratio')",
            "                    ffmpeg_fixup(stretched_ratio not in (1, None),",
            "                                 f'Non-uniform pixel ratio {stretched_ratio}',",
            "                                 FFmpegFixupStretchedPP)",
            "",
            "                    downloader = get_suitable_downloader(info_dict, self.params) if 'protocol' in info_dict else None",
            "                    downloader = downloader.FD_NAME if downloader else None",
            "",
            "                    ext = info_dict.get('ext')",
            "                    postprocessed_by_ffmpeg = info_dict.get('requested_formats') or any((",
            "                        isinstance(pp, FFmpegVideoConvertorPP)",
            "                        and resolve_recode_mapping(ext, pp.mapping)[0] not in (ext, None)",
            "                    ) for pp in self._pps['post_process'])",
            "",
            "                    if not postprocessed_by_ffmpeg:",
            "                        ffmpeg_fixup(fd != FFmpegFD and ext == 'm4a'",
            "                                     and info_dict.get('container') == 'm4a_dash',",
            "                                     'writing DASH m4a. Only some players support this container',",
            "                                     FFmpegFixupM4aPP)",
            "                        ffmpeg_fixup(downloader == 'hlsnative' and not self.params.get('hls_use_mpegts')",
            "                                     or info_dict.get('is_live') and self.params.get('hls_use_mpegts') is None,",
            "                                     'Possible MPEG-TS in MP4 container or malformed AAC timestamps',",
            "                                     FFmpegFixupM3u8PP)",
            "                        ffmpeg_fixup(downloader == 'dashsegments'",
            "                                     and (info_dict.get('is_live') or info_dict.get('is_dash_periods')),",
            "                                     'Possible duplicate MOOV atoms', FFmpegFixupDuplicateMoovPP)",
            "",
            "                    ffmpeg_fixup(downloader == 'web_socket_fragment', 'Malformed timestamps detected', FFmpegFixupTimestampPP)",
            "                    ffmpeg_fixup(downloader == 'web_socket_fragment', 'Malformed duration detected', FFmpegFixupDurationPP)",
            "",
            "                fixup()",
            "                try:",
            "                    replace_info_dict(self.post_process(dl_filename, info_dict, files_to_move))",
            "                except PostProcessingError as err:",
            "                    self.report_error('Postprocessing: %s' % str(err))",
            "                    return",
            "                try:",
            "                    for ph in self._post_hooks:",
            "                        ph(info_dict['filepath'])",
            "                except Exception as err:",
            "                    self.report_error('post hooks: %s' % str(err))",
            "                    return",
            "                info_dict['__write_download_archive'] = True",
            "",
            "        assert info_dict is original_infodict  # Make sure the info_dict was modified in-place",
            "        if self.params.get('force_write_download_archive'):",
            "            info_dict['__write_download_archive'] = True",
            "        check_max_downloads()",
            "",
            "    def __download_wrapper(self, func):",
            "        @functools.wraps(func)",
            "        def wrapper(*args, **kwargs):",
            "            try:",
            "                res = func(*args, **kwargs)",
            "            except UnavailableVideoError as e:",
            "                self.report_error(e)",
            "            except DownloadCancelled as e:",
            "                self.to_screen(f'[info] {e}')",
            "                if not self.params.get('break_per_url'):",
            "                    raise",
            "                self._num_downloads = 0",
            "            else:",
            "                if self.params.get('dump_single_json', False):",
            "                    self.post_extract(res)",
            "                    self.to_stdout(json.dumps(self.sanitize_info(res)))",
            "        return wrapper",
            "",
            "    def download(self, url_list):",
            "        \"\"\"Download a given list of URLs.\"\"\"",
            "        url_list = variadic(url_list)  # Passing a single URL is a common mistake",
            "        outtmpl = self.params['outtmpl']['default']",
            "        if (len(url_list) > 1",
            "                and outtmpl != '-'",
            "                and '%' not in outtmpl",
            "                and self.params.get('max_downloads') != 1):",
            "            raise SameFileError(outtmpl)",
            "",
            "        for url in url_list:",
            "            self.__download_wrapper(self.extract_info)(",
            "                url, force_generic_extractor=self.params.get('force_generic_extractor', False))",
            "",
            "        return self._download_retcode",
            "",
            "    def download_with_info_file(self, info_filename):",
            "        with contextlib.closing(fileinput.FileInput(",
            "                [info_filename], mode='r',",
            "                openhook=fileinput.hook_encoded('utf-8'))) as f:",
            "            # FileInput doesn't have a read method, we can't call json.load",
            "            infos = [self.sanitize_info(info, self.params.get('clean_infojson', True))",
            "                     for info in variadic(json.loads('\\n'.join(f)))]",
            "        for info in infos:",
            "            try:",
            "                self.__download_wrapper(self.process_ie_result)(info, download=True)",
            "            except (DownloadError, EntryNotInPlaylist, ReExtractInfo) as e:",
            "                if not isinstance(e, EntryNotInPlaylist):",
            "                    self.to_stderr('\\r')",
            "                webpage_url = info.get('webpage_url')",
            "                if webpage_url is None:",
            "                    raise",
            "                self.report_warning(f'The info failed to download: {e}; trying with URL {webpage_url}')",
            "                self.download([webpage_url])",
            "            except ExtractorError as e:",
            "                self.report_error(e)",
            "        return self._download_retcode",
            "",
            "    @staticmethod",
            "    def sanitize_info(info_dict, remove_private_keys=False):",
            "        ''' Sanitize the infodict for converting to json '''",
            "        if info_dict is None:",
            "            return info_dict",
            "        info_dict.setdefault('epoch', int(time.time()))",
            "        info_dict.setdefault('_type', 'video')",
            "        info_dict.setdefault('_version', {",
            "            'version': __version__,",
            "            'current_git_head': current_git_head(),",
            "            'release_git_head': RELEASE_GIT_HEAD,",
            "            'repository': ORIGIN,",
            "        })",
            "",
            "        if remove_private_keys:",
            "            reject = lambda k, v: v is None or k.startswith('__') or k in {",
            "                'requested_downloads', 'requested_formats', 'requested_subtitles', 'requested_entries',",
            "                'entries', 'filepath', '_filename', 'filename', 'infojson_filename', 'original_url',",
            "                'playlist_autonumber',",
            "            }",
            "        else:",
            "            reject = lambda k, v: False",
            "",
            "        def filter_fn(obj):",
            "            if isinstance(obj, dict):",
            "                return {k: filter_fn(v) for k, v in obj.items() if not reject(k, v)}",
            "            elif isinstance(obj, (list, tuple, set, LazyList)):",
            "                return list(map(filter_fn, obj))",
            "            elif obj is None or isinstance(obj, (str, int, float, bool)):",
            "                return obj",
            "            else:",
            "                return repr(obj)",
            "",
            "        return filter_fn(info_dict)",
            "",
            "    @staticmethod",
            "    def filter_requested_info(info_dict, actually_filter=True):",
            "        ''' Alias of sanitize_info for backward compatibility '''",
            "        return YoutubeDL.sanitize_info(info_dict, actually_filter)",
            "",
            "    def _delete_downloaded_files(self, *files_to_delete, info={}, msg=None):",
            "        for filename in set(filter(None, files_to_delete)):",
            "            if msg:",
            "                self.to_screen(msg % filename)",
            "            try:",
            "                os.remove(filename)",
            "            except OSError:",
            "                self.report_warning(f'Unable to delete file {filename}')",
            "            if filename in info.get('__files_to_move', []):  # NB: Delete even if None",
            "                del info['__files_to_move'][filename]",
            "",
            "    @staticmethod",
            "    def post_extract(info_dict):",
            "        def actual_post_extract(info_dict):",
            "            if info_dict.get('_type') in ('playlist', 'multi_video'):",
            "                for video_dict in info_dict.get('entries', {}):",
            "                    actual_post_extract(video_dict or {})",
            "                return",
            "",
            "            post_extractor = info_dict.pop('__post_extractor', None) or (lambda: {})",
            "            info_dict.update(post_extractor())",
            "",
            "        actual_post_extract(info_dict or {})",
            "",
            "    def run_pp(self, pp, infodict):",
            "        files_to_delete = []",
            "        if '__files_to_move' not in infodict:",
            "            infodict['__files_to_move'] = {}",
            "        try:",
            "            files_to_delete, infodict = pp.run(infodict)",
            "        except PostProcessingError as e:",
            "            # Must be True and not 'only_download'",
            "            if self.params.get('ignoreerrors') is True:",
            "                self.report_error(e)",
            "                return infodict",
            "            raise",
            "",
            "        if not files_to_delete:",
            "            return infodict",
            "        if self.params.get('keepvideo', False):",
            "            for f in files_to_delete:",
            "                infodict['__files_to_move'].setdefault(f, '')",
            "        else:",
            "            self._delete_downloaded_files(",
            "                *files_to_delete, info=infodict, msg='Deleting original file %s (pass -k to keep)')",
            "        return infodict",
            "",
            "    def run_all_pps(self, key, info, *, additional_pps=None):",
            "        if key != 'video':",
            "            self._forceprint(key, info)",
            "        for pp in (additional_pps or []) + self._pps[key]:",
            "            info = self.run_pp(pp, info)",
            "        return info",
            "",
            "    def pre_process(self, ie_info, key='pre_process', files_to_move=None):",
            "        info = dict(ie_info)",
            "        info['__files_to_move'] = files_to_move or {}",
            "        try:",
            "            info = self.run_all_pps(key, info)",
            "        except PostProcessingError as err:",
            "            msg = f'Preprocessing: {err}'",
            "            info.setdefault('__pending_error', msg)",
            "            self.report_error(msg, is_error=False)",
            "        return info, info.pop('__files_to_move', None)",
            "",
            "    def post_process(self, filename, info, files_to_move=None):",
            "        \"\"\"Run all the postprocessors on the given file.\"\"\"",
            "        info['filepath'] = filename",
            "        info['__files_to_move'] = files_to_move or {}",
            "        info = self.run_all_pps('post_process', info, additional_pps=info.get('__postprocessors'))",
            "        info = self.run_pp(MoveFilesAfterDownloadPP(self), info)",
            "        del info['__files_to_move']",
            "        return self.run_all_pps('after_move', info)",
            "",
            "    def _make_archive_id(self, info_dict):",
            "        video_id = info_dict.get('id')",
            "        if not video_id:",
            "            return",
            "        # Future-proof against any change in case",
            "        # and backwards compatibility with prior versions",
            "        extractor = info_dict.get('extractor_key') or info_dict.get('ie_key')  # key in a playlist",
            "        if extractor is None:",
            "            url = str_or_none(info_dict.get('url'))",
            "            if not url:",
            "                return",
            "            # Try to find matching extractor for the URL and take its ie_key",
            "            for ie_key, ie in self._ies.items():",
            "                if ie.suitable(url):",
            "                    extractor = ie_key",
            "                    break",
            "            else:",
            "                return",
            "        return make_archive_id(extractor, video_id)",
            "",
            "    def in_download_archive(self, info_dict):",
            "        if not self.archive:",
            "            return False",
            "",
            "        vid_ids = [self._make_archive_id(info_dict)]",
            "        vid_ids.extend(info_dict.get('_old_archive_ids') or [])",
            "        return any(id_ in self.archive for id_ in vid_ids)",
            "",
            "    def record_download_archive(self, info_dict):",
            "        fn = self.params.get('download_archive')",
            "        if fn is None:",
            "            return",
            "        vid_id = self._make_archive_id(info_dict)",
            "        assert vid_id",
            "",
            "        self.write_debug(f'Adding to archive: {vid_id}')",
            "        if is_path_like(fn):",
            "            with locked_file(fn, 'a', encoding='utf-8') as archive_file:",
            "                archive_file.write(vid_id + '\\n')",
            "        self.archive.add(vid_id)",
            "",
            "    @staticmethod",
            "    def format_resolution(format, default='unknown'):",
            "        if format.get('vcodec') == 'none' and format.get('acodec') != 'none':",
            "            return 'audio only'",
            "        if format.get('resolution') is not None:",
            "            return format['resolution']",
            "        if format.get('width') and format.get('height'):",
            "            return '%dx%d' % (format['width'], format['height'])",
            "        elif format.get('height'):",
            "            return '%sp' % format['height']",
            "        elif format.get('width'):",
            "            return '%dx?' % format['width']",
            "        return default",
            "",
            "    def _list_format_headers(self, *headers):",
            "        if self.params.get('listformats_table', True) is not False:",
            "            return [self._format_out(header, self.Styles.HEADERS) for header in headers]",
            "        return headers",
            "",
            "    def _format_note(self, fdict):",
            "        res = ''",
            "        if fdict.get('ext') in ['f4f', 'f4m']:",
            "            res += '(unsupported)'",
            "        if fdict.get('language'):",
            "            if res:",
            "                res += ' '",
            "            res += '[%s]' % fdict['language']",
            "        if fdict.get('format_note') is not None:",
            "            if res:",
            "                res += ' '",
            "            res += fdict['format_note']",
            "        if fdict.get('tbr') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += '%4dk' % fdict['tbr']",
            "        if fdict.get('container') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += '%s container' % fdict['container']",
            "        if (fdict.get('vcodec') is not None",
            "                and fdict.get('vcodec') != 'none'):",
            "            if res:",
            "                res += ', '",
            "            res += fdict['vcodec']",
            "            if fdict.get('vbr') is not None:",
            "                res += '@'",
            "        elif fdict.get('vbr') is not None and fdict.get('abr') is not None:",
            "            res += 'video@'",
            "        if fdict.get('vbr') is not None:",
            "            res += '%4dk' % fdict['vbr']",
            "        if fdict.get('fps') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += '%sfps' % fdict['fps']",
            "        if fdict.get('acodec') is not None:",
            "            if res:",
            "                res += ', '",
            "            if fdict['acodec'] == 'none':",
            "                res += 'video only'",
            "            else:",
            "                res += '%-5s' % fdict['acodec']",
            "        elif fdict.get('abr') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += 'audio'",
            "        if fdict.get('abr') is not None:",
            "            res += '@%3dk' % fdict['abr']",
            "        if fdict.get('asr') is not None:",
            "            res += ' (%5dHz)' % fdict['asr']",
            "        if fdict.get('filesize') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += format_bytes(fdict['filesize'])",
            "        elif fdict.get('filesize_approx') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += '~' + format_bytes(fdict['filesize_approx'])",
            "        return res",
            "",
            "    def _get_formats(self, info_dict):",
            "        if info_dict.get('formats') is None:",
            "            if info_dict.get('url') and info_dict.get('_type', 'video') == 'video':",
            "                return [info_dict]",
            "            return []",
            "        return info_dict['formats']",
            "",
            "    def render_formats_table(self, info_dict):",
            "        formats = self._get_formats(info_dict)",
            "        if not formats:",
            "            return",
            "        if not self.params.get('listformats_table', True) is not False:",
            "            table = [",
            "                [",
            "                    format_field(f, 'format_id'),",
            "                    format_field(f, 'ext'),",
            "                    self.format_resolution(f),",
            "                    self._format_note(f)",
            "                ] for f in formats if (f.get('preference') or 0) >= -1000]",
            "            return render_table(['format code', 'extension', 'resolution', 'note'], table, extra_gap=1)",
            "",
            "        def simplified_codec(f, field):",
            "            assert field in ('acodec', 'vcodec')",
            "            codec = f.get(field)",
            "            if not codec:",
            "                return 'unknown'",
            "            elif codec != 'none':",
            "                return '.'.join(codec.split('.')[:4])",
            "",
            "            if field == 'vcodec' and f.get('acodec') == 'none':",
            "                return 'images'",
            "            elif field == 'acodec' and f.get('vcodec') == 'none':",
            "                return ''",
            "            return self._format_out('audio only' if field == 'vcodec' else 'video only',",
            "                                    self.Styles.SUPPRESS)",
            "",
            "        delim = self._format_out('\\u2502', self.Styles.DELIM, '|', test_encoding=True)",
            "        table = [",
            "            [",
            "                self._format_out(format_field(f, 'format_id'), self.Styles.ID),",
            "                format_field(f, 'ext'),",
            "                format_field(f, func=self.format_resolution, ignore=('audio only', 'images')),",
            "                format_field(f, 'fps', '\\t%d', func=round),",
            "                format_field(f, 'dynamic_range', '%s', ignore=(None, 'SDR')).replace('HDR', ''),",
            "                format_field(f, 'audio_channels', '\\t%s'),",
            "                delim, (",
            "                    format_field(f, 'filesize', ' \\t%s', func=format_bytes)",
            "                    or format_field(f, 'filesize_approx', '\u2248\\t%s', func=format_bytes)",
            "                    or format_field(filesize_from_tbr(f.get('tbr'), info_dict.get('duration')), None,",
            "                                    self._format_out('~\\t%s', self.Styles.SUPPRESS), func=format_bytes)),",
            "                format_field(f, 'tbr', '\\t%dk', func=round),",
            "                shorten_protocol_name(f.get('protocol', '')),",
            "                delim,",
            "                simplified_codec(f, 'vcodec'),",
            "                format_field(f, 'vbr', '\\t%dk', func=round),",
            "                simplified_codec(f, 'acodec'),",
            "                format_field(f, 'abr', '\\t%dk', func=round),",
            "                format_field(f, 'asr', '\\t%s', func=format_decimal_suffix),",
            "                join_nonempty(format_field(f, 'language', '[%s]'), join_nonempty(",
            "                    self._format_out('UNSUPPORTED', self.Styles.BAD_FORMAT) if f.get('ext') in ('f4f', 'f4m') else None,",
            "                    (self._format_out('Maybe DRM', self.Styles.WARNING) if f.get('has_drm') == 'maybe'",
            "                     else self._format_out('DRM', self.Styles.BAD_FORMAT) if f.get('has_drm') else None),",
            "                    format_field(f, 'format_note'),",
            "                    format_field(f, 'container', ignore=(None, f.get('ext'))),",
            "                    delim=', '), delim=' '),",
            "            ] for f in formats if f.get('preference') is None or f['preference'] >= -1000]",
            "        header_line = self._list_format_headers(",
            "            'ID', 'EXT', 'RESOLUTION', '\\tFPS', 'HDR', 'CH', delim, '\\tFILESIZE', '\\tTBR', 'PROTO',",
            "            delim, 'VCODEC', '\\tVBR', 'ACODEC', '\\tABR', '\\tASR', 'MORE INFO')",
            "",
            "        return render_table(",
            "            header_line, table, hide_empty=True,",
            "            delim=self._format_out('\\u2500', self.Styles.DELIM, '-', test_encoding=True))",
            "",
            "    def render_thumbnails_table(self, info_dict):",
            "        thumbnails = list(info_dict.get('thumbnails') or [])",
            "        if not thumbnails:",
            "            return None",
            "        return render_table(",
            "            self._list_format_headers('ID', 'Width', 'Height', 'URL'),",
            "            [[t.get('id'), t.get('width') or 'unknown', t.get('height') or 'unknown', t['url']] for t in thumbnails])",
            "",
            "    def render_subtitles_table(self, video_id, subtitles):",
            "        def _row(lang, formats):",
            "            exts, names = zip(*((f['ext'], f.get('name') or 'unknown') for f in reversed(formats)))",
            "            if len(set(names)) == 1:",
            "                names = [] if names[0] == 'unknown' else names[:1]",
            "            return [lang, ', '.join(names), ', '.join(exts)]",
            "",
            "        if not subtitles:",
            "            return None",
            "        return render_table(",
            "            self._list_format_headers('Language', 'Name', 'Formats'),",
            "            [_row(lang, formats) for lang, formats in subtitles.items()],",
            "            hide_empty=True)",
            "",
            "    def __list_table(self, video_id, name, func, *args):",
            "        table = func(*args)",
            "        if not table:",
            "            self.to_screen(f'{video_id} has no {name}')",
            "            return",
            "        self.to_screen(f'[info] Available {name} for {video_id}:')",
            "        self.to_stdout(table)",
            "",
            "    def list_formats(self, info_dict):",
            "        self.__list_table(info_dict['id'], 'formats', self.render_formats_table, info_dict)",
            "",
            "    def list_thumbnails(self, info_dict):",
            "        self.__list_table(info_dict['id'], 'thumbnails', self.render_thumbnails_table, info_dict)",
            "",
            "    def list_subtitles(self, video_id, subtitles, name='subtitles'):",
            "        self.__list_table(video_id, name, self.render_subtitles_table, video_id, subtitles)",
            "",
            "    def print_debug_header(self):",
            "        if not self.params.get('verbose'):",
            "            return",
            "",
            "        from . import _IN_CLI  # Must be delayed import",
            "",
            "        # These imports can be slow. So import them only as needed",
            "        from .extractor.extractors import _LAZY_LOADER",
            "        from .extractor.extractors import (",
            "            _PLUGIN_CLASSES as plugin_ies,",
            "            _PLUGIN_OVERRIDES as plugin_ie_overrides",
            "        )",
            "",
            "        def get_encoding(stream):",
            "            ret = str(getattr(stream, 'encoding', 'missing (%s)' % type(stream).__name__))",
            "            additional_info = []",
            "            if os.environ.get('TERM', '').lower() == 'dumb':",
            "                additional_info.append('dumb')",
            "            if not supports_terminal_sequences(stream):",
            "                from .utils import WINDOWS_VT_MODE  # Must be imported locally",
            "                additional_info.append('No VT' if WINDOWS_VT_MODE is False else 'No ANSI')",
            "            if additional_info:",
            "                ret = f'{ret} ({\",\".join(additional_info)})'",
            "            return ret",
            "",
            "        encoding_str = 'Encodings: locale %s, fs %s, pref %s, %s' % (",
            "            locale.getpreferredencoding(),",
            "            sys.getfilesystemencoding(),",
            "            self.get_encoding(),",
            "            ', '.join(",
            "                f'{key} {get_encoding(stream)}' for key, stream in self._out_files.items_",
            "                if stream is not None and key != 'console')",
            "        )",
            "",
            "        logger = self.params.get('logger')",
            "        if logger:",
            "            write_debug = lambda msg: logger.debug(f'[debug] {msg}')",
            "            write_debug(encoding_str)",
            "        else:",
            "            write_string(f'[debug] {encoding_str}\\n', encoding=None)",
            "            write_debug = lambda msg: self._write_string(f'[debug] {msg}\\n')",
            "",
            "        source = detect_variant()",
            "        if VARIANT not in (None, 'pip'):",
            "            source += '*'",
            "        klass = type(self)",
            "        write_debug(join_nonempty(",
            "            f'{REPOSITORY.rpartition(\"/\")[2]} version',",
            "            _make_label(ORIGIN, CHANNEL.partition('@')[2] or __version__, __version__),",
            "            f'[{RELEASE_GIT_HEAD[:9]}]' if RELEASE_GIT_HEAD else '',",
            "            '' if source == 'unknown' else f'({source})',",
            "            '' if _IN_CLI else 'API' if klass == YoutubeDL else f'API:{self.__module__}.{klass.__qualname__}',",
            "            delim=' '))",
            "",
            "        if not _IN_CLI:",
            "            write_debug(f'params: {self.params}')",
            "",
            "        if not _LAZY_LOADER:",
            "            if os.environ.get('YTDLP_NO_LAZY_EXTRACTORS'):",
            "                write_debug('Lazy loading extractors is forcibly disabled')",
            "            else:",
            "                write_debug('Lazy loading extractors is disabled')",
            "        if self.params['compat_opts']:",
            "            write_debug('Compatibility options: %s' % ', '.join(self.params['compat_opts']))",
            "",
            "        if current_git_head():",
            "            write_debug(f'Git HEAD: {current_git_head()}')",
            "        write_debug(system_identifier())",
            "",
            "        exe_versions, ffmpeg_features = FFmpegPostProcessor.get_versions_and_features(self)",
            "        ffmpeg_features = {key for key, val in ffmpeg_features.items() if val}",
            "        if ffmpeg_features:",
            "            exe_versions['ffmpeg'] += ' (%s)' % ','.join(sorted(ffmpeg_features))",
            "",
            "        exe_versions['rtmpdump'] = rtmpdump_version()",
            "        exe_versions['phantomjs'] = PhantomJSwrapper._version()",
            "        exe_str = ', '.join(",
            "            f'{exe} {v}' for exe, v in sorted(exe_versions.items()) if v",
            "        ) or 'none'",
            "        write_debug('exe versions: %s' % exe_str)",
            "",
            "        from .compat.compat_utils import get_package_info",
            "        from .dependencies import available_dependencies",
            "",
            "        write_debug('Optional libraries: %s' % (', '.join(sorted({",
            "            join_nonempty(*get_package_info(m)) for m in available_dependencies.values()",
            "        })) or 'none'))",
            "",
            "        write_debug(f'Proxy map: {self.proxies}')",
            "        write_debug(f'Request Handlers: {\", \".join(rh.RH_NAME for rh in self._request_director.handlers.values())}')",
            "        for plugin_type, plugins in {'Extractor': plugin_ies, 'Post-Processor': plugin_pps}.items():",
            "            display_list = ['%s%s' % (",
            "                klass.__name__, '' if klass.__name__ == name else f' as {name}')",
            "                for name, klass in plugins.items()]",
            "            if plugin_type == 'Extractor':",
            "                display_list.extend(f'{plugins[-1].IE_NAME.partition(\"+\")[2]} ({parent.__name__})'",
            "                                    for parent, plugins in plugin_ie_overrides.items())",
            "            if not display_list:",
            "                continue",
            "            write_debug(f'{plugin_type} Plugins: {\", \".join(sorted(display_list))}')",
            "",
            "        plugin_dirs = plugin_directories()",
            "        if plugin_dirs:",
            "            write_debug(f'Plugin directories: {plugin_dirs}')",
            "",
            "        # Not implemented",
            "        if False and self.params.get('call_home'):",
            "            ipaddr = self.urlopen('https://yt-dl.org/ip').read().decode()",
            "            write_debug('Public IP address: %s' % ipaddr)",
            "            latest_version = self.urlopen(",
            "                'https://yt-dl.org/latest/version').read().decode()",
            "            if version_tuple(latest_version) > version_tuple(__version__):",
            "                self.report_warning(",
            "                    'You are using an outdated version (newest version: %s)! '",
            "                    'See https://yt-dl.org/update if you need help updating.' %",
            "                    latest_version)",
            "",
            "    @functools.cached_property",
            "    def proxies(self):",
            "        \"\"\"Global proxy configuration\"\"\"",
            "        opts_proxy = self.params.get('proxy')",
            "        if opts_proxy is not None:",
            "            if opts_proxy == '':",
            "                opts_proxy = '__noproxy__'",
            "            proxies = {'all': opts_proxy}",
            "        else:",
            "            proxies = urllib.request.getproxies()",
            "            # compat. Set HTTPS_PROXY to __noproxy__ to revert",
            "            if 'http' in proxies and 'https' not in proxies:",
            "                proxies['https'] = proxies['http']",
            "",
            "        return proxies",
            "",
            "    @functools.cached_property",
            "    def cookiejar(self):",
            "        \"\"\"Global cookiejar instance\"\"\"",
            "        return load_cookies(",
            "            self.params.get('cookiefile'), self.params.get('cookiesfrombrowser'), self)",
            "",
            "    @property",
            "    def _opener(self):",
            "        \"\"\"",
            "        Get a urllib OpenerDirector from the Urllib handler (deprecated).",
            "        \"\"\"",
            "        self.deprecation_warning('YoutubeDL._opener is deprecated, use YoutubeDL.urlopen()')",
            "        handler = self._request_director.handlers['Urllib']",
            "        return handler._get_instance(cookiejar=self.cookiejar, proxies=self.proxies)",
            "",
            "    def _get_available_impersonate_targets(self):",
            "        # todo(future): make available as public API",
            "        return [",
            "            (target, rh.RH_NAME)",
            "            for rh in self._request_director.handlers.values()",
            "            if isinstance(rh, ImpersonateRequestHandler)",
            "            for target in rh.supported_targets",
            "        ]",
            "",
            "    def _impersonate_target_available(self, target):",
            "        # todo(future): make available as public API",
            "        return any(",
            "            rh.is_supported_target(target)",
            "            for rh in self._request_director.handlers.values()",
            "            if isinstance(rh, ImpersonateRequestHandler))",
            "",
            "    def urlopen(self, req):",
            "        \"\"\" Start an HTTP download \"\"\"",
            "        if isinstance(req, str):",
            "            req = Request(req)",
            "        elif isinstance(req, urllib.request.Request):",
            "            self.deprecation_warning(",
            "                'Passing a urllib.request.Request object to YoutubeDL.urlopen() is deprecated. '",
            "                'Use yt_dlp.networking.common.Request instead.')",
            "            req = urllib_req_to_req(req)",
            "        assert isinstance(req, Request)",
            "",
            "        # compat: Assume user:pass url params are basic auth",
            "        url, basic_auth_header = extract_basic_auth(req.url)",
            "        if basic_auth_header:",
            "            req.headers['Authorization'] = basic_auth_header",
            "        req.url = sanitize_url(url)",
            "",
            "        clean_proxies(proxies=req.proxies, headers=req.headers)",
            "        clean_headers(req.headers)",
            "",
            "        try:",
            "            return self._request_director.send(req)",
            "        except NoSupportingHandlers as e:",
            "            for ue in e.unsupported_errors:",
            "                # FIXME: This depends on the order of errors.",
            "                if not (ue.handler and ue.msg):",
            "                    continue",
            "                if ue.handler.RH_KEY == 'Urllib' and 'unsupported url scheme: \"file\"' in ue.msg.lower():",
            "                    raise RequestError(",
            "                        'file:// URLs are disabled by default in yt-dlp for security reasons. '",
            "                        'Use --enable-file-urls to enable at your own risk.', cause=ue) from ue",
            "                if (",
            "                    'unsupported proxy type: \"https\"' in ue.msg.lower()",
            "                    and 'requests' not in self._request_director.handlers",
            "                    and 'curl_cffi' not in self._request_director.handlers",
            "                ):",
            "                    raise RequestError(",
            "                        'To use an HTTPS proxy for this request, one of the following dependencies needs to be installed: requests, curl_cffi')",
            "",
            "                elif (",
            "                    re.match(r'unsupported url scheme: \"wss?\"', ue.msg.lower())",
            "                    and 'websockets' not in self._request_director.handlers",
            "                ):",
            "                    raise RequestError(",
            "                        'This request requires WebSocket support. '",
            "                        'Ensure one of the following dependencies are installed: websockets',",
            "                        cause=ue) from ue",
            "",
            "                elif re.match(r'unsupported (?:extensions: impersonate|impersonate target)', ue.msg.lower()):",
            "                    raise RequestError(",
            "                        f'Impersonate target \"{req.extensions[\"impersonate\"]}\" is not available.'",
            "                        f' See --list-impersonate-targets for available targets.'",
            "                        f' This request requires browser impersonation, however you may be missing dependencies'",
            "                        f' required to support this target.')",
            "            raise",
            "        except SSLError as e:",
            "            if 'UNSAFE_LEGACY_RENEGOTIATION_DISABLED' in str(e):",
            "                raise RequestError('UNSAFE_LEGACY_RENEGOTIATION_DISABLED: Try using --legacy-server-connect', cause=e) from e",
            "            elif 'SSLV3_ALERT_HANDSHAKE_FAILURE' in str(e):",
            "                raise RequestError(",
            "                    'SSLV3_ALERT_HANDSHAKE_FAILURE: The server may not support the current cipher list. '",
            "                    'Try using --legacy-server-connect', cause=e) from e",
            "            raise",
            "",
            "    def build_request_director(self, handlers, preferences=None):",
            "        logger = _YDLLogger(self)",
            "        headers = self.params['http_headers'].copy()",
            "        proxies = self.proxies.copy()",
            "        clean_headers(headers)",
            "        clean_proxies(proxies, headers)",
            "",
            "        director = RequestDirector(logger=logger, verbose=self.params.get('debug_printtraffic'))",
            "        for handler in handlers:",
            "            director.add_handler(handler(",
            "                logger=logger,",
            "                headers=headers,",
            "                cookiejar=self.cookiejar,",
            "                proxies=proxies,",
            "                prefer_system_certs='no-certifi' in self.params['compat_opts'],",
            "                verify=not self.params.get('nocheckcertificate'),",
            "                **traverse_obj(self.params, {",
            "                    'verbose': 'debug_printtraffic',",
            "                    'source_address': 'source_address',",
            "                    'timeout': 'socket_timeout',",
            "                    'legacy_ssl_support': 'legacyserverconnect',",
            "                    'enable_file_urls': 'enable_file_urls',",
            "                    'impersonate': 'impersonate',",
            "                    'client_cert': {",
            "                        'client_certificate': 'client_certificate',",
            "                        'client_certificate_key': 'client_certificate_key',",
            "                        'client_certificate_password': 'client_certificate_password',",
            "                    },",
            "                }),",
            "            ))",
            "        director.preferences.update(preferences or [])",
            "        if 'prefer-legacy-http-handler' in self.params['compat_opts']:",
            "            director.preferences.add(lambda rh, _: 500 if rh.RH_KEY == 'Urllib' else 0)",
            "        return director",
            "",
            "    @functools.cached_property",
            "    def _request_director(self):",
            "        return self.build_request_director(_REQUEST_HANDLERS.values(), _RH_PREFERENCES)",
            "",
            "    def encode(self, s):",
            "        if isinstance(s, bytes):",
            "            return s  # Already encoded",
            "",
            "        try:",
            "            return s.encode(self.get_encoding())",
            "        except UnicodeEncodeError as err:",
            "            err.reason = err.reason + '. Check your system encoding configuration or use the --encoding option.'",
            "            raise",
            "",
            "    def get_encoding(self):",
            "        encoding = self.params.get('encoding')",
            "        if encoding is None:",
            "            encoding = preferredencoding()",
            "        return encoding",
            "",
            "    def _write_info_json(self, label, ie_result, infofn, overwrite=None):",
            "        ''' Write infojson and returns True = written, 'exists' = Already exists, False = skip, None = error '''",
            "        if overwrite is None:",
            "            overwrite = self.params.get('overwrites', True)",
            "        if not self.params.get('writeinfojson'):",
            "            return False",
            "        elif not infofn:",
            "            self.write_debug(f'Skipping writing {label} infojson')",
            "            return False",
            "        elif not self._ensure_dir_exists(infofn):",
            "            return None",
            "        elif not overwrite and os.path.exists(infofn):",
            "            self.to_screen(f'[info] {label.title()} metadata is already present')",
            "            return 'exists'",
            "",
            "        self.to_screen(f'[info] Writing {label} metadata as JSON to: {infofn}')",
            "        try:",
            "            write_json_file(self.sanitize_info(ie_result, self.params.get('clean_infojson', True)), infofn)",
            "            return True",
            "        except OSError:",
            "            self.report_error(f'Cannot write {label} metadata to JSON file {infofn}')",
            "            return None",
            "",
            "    def _write_description(self, label, ie_result, descfn):",
            "        ''' Write description and returns True = written, False = skip, None = error '''",
            "        if not self.params.get('writedescription'):",
            "            return False",
            "        elif not descfn:",
            "            self.write_debug(f'Skipping writing {label} description')",
            "            return False",
            "        elif not self._ensure_dir_exists(descfn):",
            "            return None",
            "        elif not self.params.get('overwrites', True) and os.path.exists(descfn):",
            "            self.to_screen(f'[info] {label.title()} description is already present')",
            "        elif ie_result.get('description') is None:",
            "            self.to_screen(f'[info] There\\'s no {label} description to write')",
            "            return False",
            "        else:",
            "            try:",
            "                self.to_screen(f'[info] Writing {label} description to: {descfn}')",
            "                with open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:",
            "                    descfile.write(ie_result['description'])",
            "            except OSError:",
            "                self.report_error(f'Cannot write {label} description file {descfn}')",
            "                return None",
            "        return True",
            "",
            "    def _write_subtitles(self, info_dict, filename):",
            "        ''' Write subtitles to file and return list of (sub_filename, final_sub_filename); or None if error'''",
            "        ret = []",
            "        subtitles = info_dict.get('requested_subtitles')",
            "        if not (self.params.get('writesubtitles') or self.params.get('writeautomaticsub')):",
            "            # subtitles download errors are already managed as troubles in relevant IE",
            "            # that way it will silently go on when used with unsupporting IE",
            "            return ret",
            "        elif not subtitles:",
            "            self.to_screen('[info] There are no subtitles for the requested languages')",
            "            return ret",
            "        sub_filename_base = self.prepare_filename(info_dict, 'subtitle')",
            "        if not sub_filename_base:",
            "            self.to_screen('[info] Skipping writing video subtitles')",
            "            return ret",
            "",
            "        for sub_lang, sub_info in subtitles.items():",
            "            sub_format = sub_info['ext']",
            "            sub_filename = subtitles_filename(filename, sub_lang, sub_format, info_dict.get('ext'))",
            "            sub_filename_final = subtitles_filename(sub_filename_base, sub_lang, sub_format, info_dict.get('ext'))",
            "            existing_sub = self.existing_file((sub_filename_final, sub_filename))",
            "            if existing_sub:",
            "                self.to_screen(f'[info] Video subtitle {sub_lang}.{sub_format} is already present')",
            "                sub_info['filepath'] = existing_sub",
            "                ret.append((existing_sub, sub_filename_final))",
            "                continue",
            "",
            "            self.to_screen(f'[info] Writing video subtitles to: {sub_filename}')",
            "            if sub_info.get('data') is not None:",
            "                try:",
            "                    # Use newline='' to prevent conversion of newline characters",
            "                    # See https://github.com/ytdl-org/youtube-dl/issues/10268",
            "                    with open(sub_filename, 'w', encoding='utf-8', newline='') as subfile:",
            "                        subfile.write(sub_info['data'])",
            "                    sub_info['filepath'] = sub_filename",
            "                    ret.append((sub_filename, sub_filename_final))",
            "                    continue",
            "                except OSError:",
            "                    self.report_error(f'Cannot write video subtitles file {sub_filename}')",
            "                    return None",
            "",
            "            try:",
            "                sub_copy = sub_info.copy()",
            "                sub_copy.setdefault('http_headers', info_dict.get('http_headers'))",
            "                self.dl(sub_filename, sub_copy, subtitle=True)",
            "                sub_info['filepath'] = sub_filename",
            "                ret.append((sub_filename, sub_filename_final))",
            "            except (DownloadError, ExtractorError, IOError, OSError, ValueError) + network_exceptions as err:",
            "                msg = f'Unable to download video subtitles for {sub_lang!r}: {err}'",
            "                if self.params.get('ignoreerrors') is not True:  # False or 'only_download'",
            "                    if not self.params.get('ignoreerrors'):",
            "                        self.report_error(msg)",
            "                    raise DownloadError(msg)",
            "                self.report_warning(msg)",
            "        return ret",
            "",
            "    def _write_thumbnails(self, label, info_dict, filename, thumb_filename_base=None):",
            "        ''' Write thumbnails to file and return list of (thumb_filename, final_thumb_filename); or None if error '''",
            "        write_all = self.params.get('write_all_thumbnails', False)",
            "        thumbnails, ret = [], []",
            "        if write_all or self.params.get('writethumbnail', False):",
            "            thumbnails = info_dict.get('thumbnails') or []",
            "            if not thumbnails:",
            "                self.to_screen(f'[info] There are no {label} thumbnails to download')",
            "                return ret",
            "        multiple = write_all and len(thumbnails) > 1",
            "",
            "        if thumb_filename_base is None:",
            "            thumb_filename_base = filename",
            "        if thumbnails and not thumb_filename_base:",
            "            self.write_debug(f'Skipping writing {label} thumbnail')",
            "            return ret",
            "",
            "        if thumbnails and not self._ensure_dir_exists(filename):",
            "            return None",
            "",
            "        for idx, t in list(enumerate(thumbnails))[::-1]:",
            "            thumb_ext = (f'{t[\"id\"]}.' if multiple else '') + determine_ext(t['url'], 'jpg')",
            "            thumb_display_id = f'{label} thumbnail {t[\"id\"]}'",
            "            thumb_filename = replace_extension(filename, thumb_ext, info_dict.get('ext'))",
            "            thumb_filename_final = replace_extension(thumb_filename_base, thumb_ext, info_dict.get('ext'))",
            "",
            "            existing_thumb = self.existing_file((thumb_filename_final, thumb_filename))",
            "            if existing_thumb:",
            "                self.to_screen('[info] %s is already present' % (",
            "                    thumb_display_id if multiple else f'{label} thumbnail').capitalize())",
            "                t['filepath'] = existing_thumb",
            "                ret.append((existing_thumb, thumb_filename_final))",
            "            else:",
            "                self.to_screen(f'[info] Downloading {thumb_display_id} ...')",
            "                try:",
            "                    uf = self.urlopen(Request(t['url'], headers=t.get('http_headers', {})))",
            "                    self.to_screen(f'[info] Writing {thumb_display_id} to: {thumb_filename}')",
            "                    with open(encodeFilename(thumb_filename), 'wb') as thumbf:",
            "                        shutil.copyfileobj(uf, thumbf)",
            "                    ret.append((thumb_filename, thumb_filename_final))",
            "                    t['filepath'] = thumb_filename",
            "                except network_exceptions as err:",
            "                    if isinstance(err, HTTPError) and err.status == 404:",
            "                        self.to_screen(f'[info] {thumb_display_id.title()} does not exist')",
            "                    else:",
            "                        self.report_warning(f'Unable to download {thumb_display_id}: {err}')",
            "                    thumbnails.pop(idx)",
            "            if ret and not write_all:",
            "                break",
            "        return ret"
        ],
        "afterPatchFile": [
            "import collections",
            "import contextlib",
            "import copy",
            "import datetime as dt",
            "import errno",
            "import fileinput",
            "import http.cookiejar",
            "import io",
            "import itertools",
            "import json",
            "import locale",
            "import operator",
            "import os",
            "import random",
            "import re",
            "import shutil",
            "import string",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import tokenize",
            "import traceback",
            "import unicodedata",
            "",
            "from .cache import Cache",
            "from .compat import functools, urllib  # isort: split",
            "from .compat import compat_os_name, urllib_req_to_req",
            "from .cookies import LenientSimpleCookie, load_cookies",
            "from .downloader import FFmpegFD, get_suitable_downloader, shorten_protocol_name",
            "from .downloader.rtmp import rtmpdump_version",
            "from .extractor import gen_extractor_classes, get_info_extractor",
            "from .extractor.common import UnsupportedURLIE",
            "from .extractor.openload import PhantomJSwrapper",
            "from .minicurses import format_text",
            "from .networking import HEADRequest, Request, RequestDirector",
            "from .networking.common import _REQUEST_HANDLERS, _RH_PREFERENCES",
            "from .networking.exceptions import (",
            "    HTTPError,",
            "    NoSupportingHandlers,",
            "    RequestError,",
            "    SSLError,",
            "    network_exceptions,",
            ")",
            "from .networking.impersonate import ImpersonateRequestHandler",
            "from .plugins import directories as plugin_directories",
            "from .postprocessor import _PLUGIN_CLASSES as plugin_pps",
            "from .postprocessor import (",
            "    EmbedThumbnailPP,",
            "    FFmpegFixupDuplicateMoovPP,",
            "    FFmpegFixupDurationPP,",
            "    FFmpegFixupM3u8PP,",
            "    FFmpegFixupM4aPP,",
            "    FFmpegFixupStretchedPP,",
            "    FFmpegFixupTimestampPP,",
            "    FFmpegMergerPP,",
            "    FFmpegPostProcessor,",
            "    FFmpegVideoConvertorPP,",
            "    MoveFilesAfterDownloadPP,",
            "    get_postprocessor,",
            ")",
            "from .postprocessor.ffmpeg import resolve_mapping as resolve_recode_mapping",
            "from .update import (",
            "    REPOSITORY,",
            "    _get_system_deprecation,",
            "    _make_label,",
            "    current_git_head,",
            "    detect_variant,",
            ")",
            "from .utils import (",
            "    DEFAULT_OUTTMPL,",
            "    IDENTITY,",
            "    LINK_TEMPLATES,",
            "    MEDIA_EXTENSIONS,",
            "    NO_DEFAULT,",
            "    NUMBER_RE,",
            "    OUTTMPL_TYPES,",
            "    POSTPROCESS_WHEN,",
            "    STR_FORMAT_RE_TMPL,",
            "    STR_FORMAT_TYPES,",
            "    ContentTooShortError,",
            "    DateRange,",
            "    DownloadCancelled,",
            "    DownloadError,",
            "    EntryNotInPlaylist,",
            "    ExistingVideoReached,",
            "    ExtractorError,",
            "    FormatSorter,",
            "    GeoRestrictedError,",
            "    ISO3166Utils,",
            "    LazyList,",
            "    MaxDownloadsReached,",
            "    Namespace,",
            "    PagedList,",
            "    PlaylistEntries,",
            "    Popen,",
            "    PostProcessingError,",
            "    ReExtractInfo,",
            "    RejectedVideoReached,",
            "    SameFileError,",
            "    UnavailableVideoError,",
            "    UserNotLive,",
            "    YoutubeDLError,",
            "    age_restricted,",
            "    bug_reports_message,",
            "    date_from_str,",
            "    deprecation_warning,",
            "    determine_ext,",
            "    determine_protocol,",
            "    encode_compat_str,",
            "    encodeFilename,",
            "    error_to_compat_str,",
            "    escapeHTML,",
            "    expand_path,",
            "    extract_basic_auth,",
            "    filter_dict,",
            "    float_or_none,",
            "    format_bytes,",
            "    format_decimal_suffix,",
            "    format_field,",
            "    formatSeconds,",
            "    get_compatible_ext,",
            "    get_domain,",
            "    int_or_none,",
            "    iri_to_uri,",
            "    is_path_like,",
            "    join_nonempty,",
            "    locked_file,",
            "    make_archive_id,",
            "    make_dir,",
            "    number_of_digits,",
            "    orderedSet,",
            "    orderedSet_from_options,",
            "    parse_filesize,",
            "    preferredencoding,",
            "    prepend_extension,",
            "    remove_terminal_sequences,",
            "    render_table,",
            "    replace_extension,",
            "    sanitize_filename,",
            "    sanitize_path,",
            "    sanitize_url,",
            "    shell_quote,",
            "    str_or_none,",
            "    strftime_or_none,",
            "    subtitles_filename,",
            "    supports_terminal_sequences,",
            "    system_identifier,",
            "    filesize_from_tbr,",
            "    timetuple_from_msec,",
            "    to_high_limit_path,",
            "    traverse_obj,",
            "    try_call,",
            "    try_get,",
            "    url_basename,",
            "    variadic,",
            "    version_tuple,",
            "    windows_enable_vt_mode,",
            "    write_json_file,",
            "    write_string,",
            ")",
            "from .utils._utils import _YDLLogger",
            "from .utils.networking import (",
            "    HTTPHeaderDict,",
            "    clean_headers,",
            "    clean_proxies,",
            "    std_headers,",
            ")",
            "from .version import CHANNEL, ORIGIN, RELEASE_GIT_HEAD, VARIANT, __version__",
            "",
            "if compat_os_name == 'nt':",
            "    import ctypes",
            "",
            "",
            "class YoutubeDL:",
            "    \"\"\"YoutubeDL class.",
            "",
            "    YoutubeDL objects are the ones responsible of downloading the",
            "    actual video file and writing it to disk if the user has requested",
            "    it, among some other tasks. In most cases there should be one per",
            "    program. As, given a video URL, the downloader doesn't know how to",
            "    extract all the needed information, task that InfoExtractors do, it",
            "    has to pass the URL to one of them.",
            "",
            "    For this, YoutubeDL objects have a method that allows",
            "    InfoExtractors to be registered in a given order. When it is passed",
            "    a URL, the YoutubeDL object handles it to the first InfoExtractor it",
            "    finds that reports being able to handle it. The InfoExtractor extracts",
            "    all the information about the video or videos the URL refers to, and",
            "    YoutubeDL process the extracted information, possibly using a File",
            "    Downloader to download the video.",
            "",
            "    YoutubeDL objects accept a lot of parameters. In order not to saturate",
            "    the object constructor with arguments, it receives a dictionary of",
            "    options instead. These options are available through the params",
            "    attribute for the InfoExtractors to use. The YoutubeDL also",
            "    registers itself as the downloader in charge for the InfoExtractors",
            "    that are added to it, so this is a \"mutual registration\".",
            "",
            "    Available options:",
            "",
            "    username:          Username for authentication purposes.",
            "    password:          Password for authentication purposes.",
            "    videopassword:     Password for accessing a video.",
            "    ap_mso:            Adobe Pass multiple-system operator identifier.",
            "    ap_username:       Multiple-system operator account username.",
            "    ap_password:       Multiple-system operator account password.",
            "    usenetrc:          Use netrc for authentication instead.",
            "    netrc_location:    Location of the netrc file. Defaults to ~/.netrc.",
            "    netrc_cmd:         Use a shell command to get credentials",
            "    verbose:           Print additional info to stdout.",
            "    quiet:             Do not print messages to stdout.",
            "    no_warnings:       Do not print out anything for warnings.",
            "    forceprint:        A dict with keys WHEN mapped to a list of templates to",
            "                       print to stdout. The allowed keys are video or any of the",
            "                       items in utils.POSTPROCESS_WHEN.",
            "                       For compatibility, a single list is also accepted",
            "    print_to_file:     A dict with keys WHEN (same as forceprint) mapped to",
            "                       a list of tuples with (template, filename)",
            "    forcejson:         Force printing info_dict as JSON.",
            "    dump_single_json:  Force printing the info_dict of the whole playlist",
            "                       (or video) as a single JSON line.",
            "    force_write_download_archive: Force writing download archive regardless",
            "                       of 'skip_download' or 'simulate'.",
            "    simulate:          Do not download the video files. If unset (or None),",
            "                       simulate only if listsubtitles, listformats or list_thumbnails is used",
            "    format:            Video format code. see \"FORMAT SELECTION\" for more details.",
            "                       You can also pass a function. The function takes 'ctx' as",
            "                       argument and returns the formats to download.",
            "                       See \"build_format_selector\" for an implementation",
            "    allow_unplayable_formats:   Allow unplayable formats to be extracted and downloaded.",
            "    ignore_no_formats_error: Ignore \"No video formats\" error. Usefull for",
            "                       extracting metadata even if the video is not actually",
            "                       available for download (experimental)",
            "    format_sort:       A list of fields by which to sort the video formats.",
            "                       See \"Sorting Formats\" for more details.",
            "    format_sort_force: Force the given format_sort. see \"Sorting Formats\"",
            "                       for more details.",
            "    prefer_free_formats: Whether to prefer video formats with free containers",
            "                       over non-free ones of same quality.",
            "    allow_multiple_video_streams:   Allow multiple video streams to be merged",
            "                       into a single file",
            "    allow_multiple_audio_streams:   Allow multiple audio streams to be merged",
            "                       into a single file",
            "    check_formats      Whether to test if the formats are downloadable.",
            "                       Can be True (check all), False (check none),",
            "                       'selected' (check selected formats),",
            "                       or None (check only if requested by extractor)",
            "    paths:             Dictionary of output paths. The allowed keys are 'home'",
            "                       'temp' and the keys of OUTTMPL_TYPES (in utils/_utils.py)",
            "    outtmpl:           Dictionary of templates for output names. Allowed keys",
            "                       are 'default' and the keys of OUTTMPL_TYPES (in utils/_utils.py).",
            "                       For compatibility with youtube-dl, a single string can also be used",
            "    outtmpl_na_placeholder: Placeholder for unavailable meta fields.",
            "    restrictfilenames: Do not allow \"&\" and spaces in file names",
            "    trim_file_name:    Limit length of filename (extension excluded)",
            "    windowsfilenames:  Force the filenames to be windows compatible",
            "    ignoreerrors:      Do not stop on download/postprocessing errors.",
            "                       Can be 'only_download' to ignore only download errors.",
            "                       Default is 'only_download' for CLI, but False for API",
            "    skip_playlist_after_errors: Number of allowed failures until the rest of",
            "                       the playlist is skipped",
            "    allowed_extractors:  List of regexes to match against extractor names that are allowed",
            "    overwrites:        Overwrite all video and metadata files if True,",
            "                       overwrite only non-video files if None",
            "                       and don't overwrite any file if False",
            "    playlist_items:    Specific indices of playlist to download.",
            "    playlistrandom:    Download playlist items in random order.",
            "    lazy_playlist:     Process playlist entries as they are received.",
            "    matchtitle:        Download only matching titles.",
            "    rejecttitle:       Reject downloads for matching titles.",
            "    logger:            Log messages to a logging.Logger instance.",
            "    logtostderr:       Print everything to stderr instead of stdout.",
            "    consoletitle:      Display progress in console window's titlebar.",
            "    writedescription:  Write the video description to a .description file",
            "    writeinfojson:     Write the video description to a .info.json file",
            "    clean_infojson:    Remove internal metadata from the infojson",
            "    getcomments:       Extract video comments. This will not be written to disk",
            "                       unless writeinfojson is also given",
            "    writeannotations:  Write the video annotations to a .annotations.xml file",
            "    writethumbnail:    Write the thumbnail image to a file",
            "    allow_playlist_files: Whether to write playlists' description, infojson etc",
            "                       also to disk when using the 'write*' options",
            "    write_all_thumbnails:  Write all thumbnail formats to files",
            "    writelink:         Write an internet shortcut file, depending on the",
            "                       current platform (.url/.webloc/.desktop)",
            "    writeurllink:      Write a Windows internet shortcut file (.url)",
            "    writewebloclink:   Write a macOS internet shortcut file (.webloc)",
            "    writedesktoplink:  Write a Linux internet shortcut file (.desktop)",
            "    writesubtitles:    Write the video subtitles to a file",
            "    writeautomaticsub: Write the automatically generated subtitles to a file",
            "    listsubtitles:     Lists all available subtitles for the video",
            "    subtitlesformat:   The format code for subtitles",
            "    subtitleslangs:    List of languages of the subtitles to download (can be regex).",
            "                       The list may contain \"all\" to refer to all the available",
            "                       subtitles. The language can be prefixed with a \"-\" to",
            "                       exclude it from the requested languages, e.g. ['all', '-live_chat']",
            "    keepvideo:         Keep the video file after post-processing",
            "    daterange:         A utils.DateRange object, download only if the upload_date is in the range.",
            "    skip_download:     Skip the actual download of the video file",
            "    cachedir:          Location of the cache files in the filesystem.",
            "                       False to disable filesystem cache.",
            "    noplaylist:        Download single video instead of a playlist if in doubt.",
            "    age_limit:         An integer representing the user's age in years.",
            "                       Unsuitable videos for the given age are skipped.",
            "    min_views:         An integer representing the minimum view count the video",
            "                       must have in order to not be skipped.",
            "                       Videos without view count information are always",
            "                       downloaded. None for no limit.",
            "    max_views:         An integer representing the maximum view count.",
            "                       Videos that are more popular than that are not",
            "                       downloaded.",
            "                       Videos without view count information are always",
            "                       downloaded. None for no limit.",
            "    download_archive:  A set, or the name of a file where all downloads are recorded.",
            "                       Videos already present in the file are not downloaded again.",
            "    break_on_existing: Stop the download process after attempting to download a",
            "                       file that is in the archive.",
            "    break_per_url:     Whether break_on_reject and break_on_existing",
            "                       should act on each input URL as opposed to for the entire queue",
            "    cookiefile:        File name or text stream from where cookies should be read and dumped to",
            "    cookiesfrombrowser:  A tuple containing the name of the browser, the profile",
            "                       name/path from where cookies are loaded, the name of the keyring,",
            "                       and the container name, e.g. ('chrome', ) or",
            "                       ('vivaldi', 'default', 'BASICTEXT') or ('firefox', 'default', None, 'Meta')",
            "    legacyserverconnect: Explicitly allow HTTPS connection to servers that do not",
            "                       support RFC 5746 secure renegotiation",
            "    nocheckcertificate:  Do not verify SSL certificates",
            "    client_certificate:  Path to client certificate file in PEM format. May include the private key",
            "    client_certificate_key:  Path to private key file for client certificate",
            "    client_certificate_password:  Password for client certificate private key, if encrypted.",
            "                        If not provided and the key is encrypted, yt-dlp will ask interactively",
            "    prefer_insecure:   Use HTTP instead of HTTPS to retrieve information.",
            "                       (Only supported by some extractors)",
            "    enable_file_urls:  Enable file:// URLs. This is disabled by default for security reasons.",
            "    http_headers:      A dictionary of custom headers to be used for all requests",
            "    proxy:             URL of the proxy server to use",
            "    geo_verification_proxy:  URL of the proxy to use for IP address verification",
            "                       on geo-restricted sites.",
            "    socket_timeout:    Time to wait for unresponsive hosts, in seconds",
            "    bidi_workaround:   Work around buggy terminals without bidirectional text",
            "                       support, using fridibi",
            "    debug_printtraffic:Print out sent and received HTTP traffic",
            "    default_search:    Prepend this string if an input url is not valid.",
            "                       'auto' for elaborate guessing",
            "    encoding:          Use this encoding instead of the system-specified.",
            "    extract_flat:      Whether to resolve and process url_results further",
            "                       * False:     Always process. Default for API",
            "                       * True:      Never process",
            "                       * 'in_playlist': Do not process inside playlist/multi_video",
            "                       * 'discard': Always process, but don't return the result",
            "                                    from inside playlist/multi_video",
            "                       * 'discard_in_playlist': Same as \"discard\", but only for",
            "                                    playlists (not multi_video). Default for CLI",
            "    wait_for_video:    If given, wait for scheduled streams to become available.",
            "                       The value should be a tuple containing the range",
            "                       (min_secs, max_secs) to wait between retries",
            "    postprocessors:    A list of dictionaries, each with an entry",
            "                       * key:  The name of the postprocessor. See",
            "                               yt_dlp/postprocessor/__init__.py for a list.",
            "                       * when: When to run the postprocessor. Allowed values are",
            "                               the entries of utils.POSTPROCESS_WHEN",
            "                               Assumed to be 'post_process' if not given",
            "    progress_hooks:    A list of functions that get called on download",
            "                       progress, with a dictionary with the entries",
            "                       * status: One of \"downloading\", \"error\", or \"finished\".",
            "                                 Check this first and ignore unknown values.",
            "                       * info_dict: The extracted info_dict",
            "",
            "                       If status is one of \"downloading\", or \"finished\", the",
            "                       following properties may also be present:",
            "                       * filename: The final filename (always present)",
            "                       * tmpfilename: The filename we're currently writing to",
            "                       * downloaded_bytes: Bytes on disk",
            "                       * total_bytes: Size of the whole file, None if unknown",
            "                       * total_bytes_estimate: Guess of the eventual file size,",
            "                                               None if unavailable.",
            "                       * elapsed: The number of seconds since download started.",
            "                       * eta: The estimated time in seconds, None if unknown",
            "                       * speed: The download speed in bytes/second, None if",
            "                                unknown",
            "                       * fragment_index: The counter of the currently",
            "                                         downloaded video fragment.",
            "                       * fragment_count: The number of fragments (= individual",
            "                                         files that will be merged)",
            "",
            "                       Progress hooks are guaranteed to be called at least once",
            "                       (with status \"finished\") if the download is successful.",
            "    postprocessor_hooks:  A list of functions that get called on postprocessing",
            "                       progress, with a dictionary with the entries",
            "                       * status: One of \"started\", \"processing\", or \"finished\".",
            "                                 Check this first and ignore unknown values.",
            "                       * postprocessor: Name of the postprocessor",
            "                       * info_dict: The extracted info_dict",
            "",
            "                       Progress hooks are guaranteed to be called at least twice",
            "                       (with status \"started\" and \"finished\") if the processing is successful.",
            "    merge_output_format: \"/\" separated list of extensions to use when merging formats.",
            "    final_ext:         Expected final extension; used to detect when the file was",
            "                       already downloaded and converted",
            "    fixup:             Automatically correct known faults of the file.",
            "                       One of:",
            "                       - \"never\": do nothing",
            "                       - \"warn\": only emit a warning",
            "                       - \"detect_or_warn\": check whether we can do anything",
            "                                           about it, warn otherwise (default)",
            "    source_address:    Client-side IP address to bind to.",
            "    impersonate:       Client to impersonate for requests.",
            "                       An ImpersonateTarget (from yt_dlp.networking.impersonate)",
            "    sleep_interval_requests: Number of seconds to sleep between requests",
            "                       during extraction",
            "    sleep_interval:    Number of seconds to sleep before each download when",
            "                       used alone or a lower bound of a range for randomized",
            "                       sleep before each download (minimum possible number",
            "                       of seconds to sleep) when used along with",
            "                       max_sleep_interval.",
            "    max_sleep_interval:Upper bound of a range for randomized sleep before each",
            "                       download (maximum possible number of seconds to sleep).",
            "                       Must only be used along with sleep_interval.",
            "                       Actual sleep time will be a random float from range",
            "                       [sleep_interval; max_sleep_interval].",
            "    sleep_interval_subtitles: Number of seconds to sleep before each subtitle download",
            "    listformats:       Print an overview of available video formats and exit.",
            "    list_thumbnails:   Print a table of all thumbnails and exit.",
            "    match_filter:      A function that gets called for every video with the signature",
            "                       (info_dict, *, incomplete: bool) -> Optional[str]",
            "                       For backward compatibility with youtube-dl, the signature",
            "                       (info_dict) -> Optional[str] is also allowed.",
            "                       - If it returns a message, the video is ignored.",
            "                       - If it returns None, the video is downloaded.",
            "                       - If it returns utils.NO_DEFAULT, the user is interactively",
            "                         asked whether to download the video.",
            "                       - Raise utils.DownloadCancelled(msg) to abort remaining",
            "                         downloads when a video is rejected.",
            "                       match_filter_func in utils/_utils.py is one example for this.",
            "    color:             A Dictionary with output stream names as keys",
            "                       and their respective color policy as values.",
            "                       Can also just be a single color policy,",
            "                       in which case it applies to all outputs.",
            "                       Valid stream names are 'stdout' and 'stderr'.",
            "                       Valid color policies are one of 'always', 'auto', 'no_color' or 'never'.",
            "    geo_bypass:        Bypass geographic restriction via faking X-Forwarded-For",
            "                       HTTP header",
            "    geo_bypass_country:",
            "                       Two-letter ISO 3166-2 country code that will be used for",
            "                       explicit geographic restriction bypassing via faking",
            "                       X-Forwarded-For HTTP header",
            "    geo_bypass_ip_block:",
            "                       IP range in CIDR notation that will be used similarly to",
            "                       geo_bypass_country",
            "    external_downloader: A dictionary of protocol keys and the executable of the",
            "                       external downloader to use for it. The allowed protocols",
            "                       are default|http|ftp|m3u8|dash|rtsp|rtmp|mms.",
            "                       Set the value to 'native' to use the native downloader",
            "    compat_opts:       Compatibility options. See \"Differences in default behavior\".",
            "                       The following options do not work when used through the API:",
            "                       filename, abort-on-error, multistreams, no-live-chat, format-sort",
            "                       no-clean-infojson, no-playlist-metafiles, no-keep-subs, no-attach-info-json.",
            "                       Refer __init__.py for their implementation",
            "    progress_template: Dictionary of templates for progress outputs.",
            "                       Allowed keys are 'download', 'postprocess',",
            "                       'download-title' (console title) and 'postprocess-title'.",
            "                       The template is mapped on a dictionary with keys 'progress' and 'info'",
            "    retry_sleep_functions: Dictionary of functions that takes the number of attempts",
            "                       as argument and returns the time to sleep in seconds.",
            "                       Allowed keys are 'http', 'fragment', 'file_access'",
            "    download_ranges:   A callback function that gets called for every video with",
            "                       the signature (info_dict, ydl) -> Iterable[Section].",
            "                       Only the returned sections will be downloaded.",
            "                       Each Section is a dict with the following keys:",
            "                       * start_time: Start time of the section in seconds",
            "                       * end_time: End time of the section in seconds",
            "                       * title: Section title (Optional)",
            "                       * index: Section number (Optional)",
            "    force_keyframes_at_cuts: Re-encode the video when downloading ranges to get precise cuts",
            "    noprogress:        Do not print the progress bar",
            "    live_from_start:   Whether to download livestreams videos from the start",
            "",
            "    The following parameters are not used by YoutubeDL itself, they are used by",
            "    the downloader (see yt_dlp/downloader/common.py):",
            "    nopart, updatetime, buffersize, ratelimit, throttledratelimit, min_filesize,",
            "    max_filesize, test, noresizebuffer, retries, file_access_retries, fragment_retries,",
            "    continuedl, xattr_set_filesize, hls_use_mpegts, http_chunk_size,",
            "    external_downloader_args, concurrent_fragment_downloads, progress_delta.",
            "",
            "    The following options are used by the post processors:",
            "    ffmpeg_location:   Location of the ffmpeg/avconv binary; either the path",
            "                       to the binary or its containing directory.",
            "    postprocessor_args: A dictionary of postprocessor/executable keys (in lower case)",
            "                       and a list of additional command-line arguments for the",
            "                       postprocessor/executable. The dict can also have \"PP+EXE\" keys",
            "                       which are used when the given exe is used by the given PP.",
            "                       Use 'default' as the name for arguments to passed to all PP",
            "                       For compatibility with youtube-dl, a single list of args",
            "                       can also be used",
            "",
            "    The following options are used by the extractors:",
            "    extractor_retries: Number of times to retry for known errors (default: 3)",
            "    dynamic_mpd:       Whether to process dynamic DASH manifests (default: True)",
            "    hls_split_discontinuity: Split HLS playlists to different formats at",
            "                       discontinuities such as ad breaks (default: False)",
            "    extractor_args:    A dictionary of arguments to be passed to the extractors.",
            "                       See \"EXTRACTOR ARGUMENTS\" for details.",
            "                       E.g. {'youtube': {'skip': ['dash', 'hls']}}",
            "    mark_watched:      Mark videos watched (even with --simulate). Only for YouTube",
            "",
            "    The following options are deprecated and may be removed in the future:",
            "",
            "    break_on_reject:   Stop the download process when encountering a video that",
            "                       has been filtered out.",
            "                       - `raise DownloadCancelled(msg)` in match_filter instead",
            "    force_generic_extractor: Force downloader to use the generic extractor",
            "                       - Use allowed_extractors = ['generic', 'default']",
            "    playliststart:     - Use playlist_items",
            "                       Playlist item to start at.",
            "    playlistend:       - Use playlist_items",
            "                       Playlist item to end at.",
            "    playlistreverse:   - Use playlist_items",
            "                       Download playlist items in reverse order.",
            "    forceurl:          - Use forceprint",
            "                       Force printing final URL.",
            "    forcetitle:        - Use forceprint",
            "                       Force printing title.",
            "    forceid:           - Use forceprint",
            "                       Force printing ID.",
            "    forcethumbnail:    - Use forceprint",
            "                       Force printing thumbnail URL.",
            "    forcedescription:  - Use forceprint",
            "                       Force printing description.",
            "    forcefilename:     - Use forceprint",
            "                       Force printing final filename.",
            "    forceduration:     - Use forceprint",
            "                       Force printing duration.",
            "    allsubtitles:      - Use subtitleslangs = ['all']",
            "                       Downloads all the subtitles of the video",
            "                       (requires writesubtitles or writeautomaticsub)",
            "    include_ads:       - Doesn't work",
            "                       Download ads as well",
            "    call_home:         - Not implemented",
            "                       Boolean, true iff we are allowed to contact the",
            "                       yt-dlp servers for debugging.",
            "    post_hooks:        - Register a custom postprocessor",
            "                       A list of functions that get called as the final step",
            "                       for each video file, after all postprocessors have been",
            "                       called. The filename will be passed as the only argument.",
            "    hls_prefer_native: - Use external_downloader = {'m3u8': 'native'} or {'m3u8': 'ffmpeg'}.",
            "                       Use the native HLS downloader instead of ffmpeg/avconv",
            "                       if True, otherwise use ffmpeg/avconv if False, otherwise",
            "                       use downloader suggested by extractor if None.",
            "    prefer_ffmpeg:     - avconv support is deprecated",
            "                       If False, use avconv instead of ffmpeg if both are available,",
            "                       otherwise prefer ffmpeg.",
            "    youtube_include_dash_manifest: - Use extractor_args",
            "                       If True (default), DASH manifests and related",
            "                       data will be downloaded and processed by extractor.",
            "                       You can reduce network I/O by disabling it if you don't",
            "                       care about DASH. (only for youtube)",
            "    youtube_include_hls_manifest: - Use extractor_args",
            "                       If True (default), HLS manifests and related",
            "                       data will be downloaded and processed by extractor.",
            "                       You can reduce network I/O by disabling it if you don't",
            "                       care about HLS. (only for youtube)",
            "    no_color:          Same as `color='no_color'`",
            "    no_overwrites:     Same as `overwrites=False`",
            "    \"\"\"",
            "",
            "    _NUMERIC_FIELDS = {",
            "        'width', 'height', 'asr', 'audio_channels', 'fps',",
            "        'tbr', 'abr', 'vbr', 'filesize', 'filesize_approx',",
            "        'timestamp', 'release_timestamp',",
            "        'duration', 'view_count', 'like_count', 'dislike_count', 'repost_count',",
            "        'average_rating', 'comment_count', 'age_limit',",
            "        'start_time', 'end_time',",
            "        'chapter_number', 'season_number', 'episode_number',",
            "        'track_number', 'disc_number', 'release_year',",
            "    }",
            "",
            "    _format_fields = {",
            "        # NB: Keep in sync with the docstring of extractor/common.py",
            "        'url', 'manifest_url', 'manifest_stream_number', 'ext', 'format', 'format_id', 'format_note',",
            "        'width', 'height', 'aspect_ratio', 'resolution', 'dynamic_range', 'tbr', 'abr', 'acodec', 'asr', 'audio_channels',",
            "        'vbr', 'fps', 'vcodec', 'container', 'filesize', 'filesize_approx', 'rows', 'columns',",
            "        'player_url', 'protocol', 'fragment_base_url', 'fragments', 'is_from_start', 'is_dash_periods', 'request_data',",
            "        'preference', 'language', 'language_preference', 'quality', 'source_preference', 'cookies',",
            "        'http_headers', 'stretched_ratio', 'no_resume', 'has_drm', 'extra_param_to_segment_url', 'hls_aes', 'downloader_options',",
            "        'page_url', 'app', 'play_path', 'tc_url', 'flash_version', 'rtmp_live', 'rtmp_conn', 'rtmp_protocol', 'rtmp_real_time'",
            "    }",
            "    _deprecated_multivalue_fields = {",
            "        'album_artist': 'album_artists',",
            "        'artist': 'artists',",
            "        'composer': 'composers',",
            "        'creator': 'creators',",
            "        'genre': 'genres',",
            "    }",
            "    _format_selection_exts = {",
            "        'audio': set(MEDIA_EXTENSIONS.common_audio),",
            "        'video': set(MEDIA_EXTENSIONS.common_video + ('3gp', )),",
            "        'storyboards': set(MEDIA_EXTENSIONS.storyboards),",
            "    }",
            "",
            "    def __init__(self, params=None, auto_init=True):",
            "        \"\"\"Create a FileDownloader object with the given options.",
            "        @param auto_init    Whether to load the default extractors and print header (if verbose).",
            "                            Set to 'no_verbose_header' to not print the header",
            "        \"\"\"",
            "        if params is None:",
            "            params = {}",
            "        self.params = params",
            "        self._ies = {}",
            "        self._ies_instances = {}",
            "        self._pps = {k: [] for k in POSTPROCESS_WHEN}",
            "        self._printed_messages = set()",
            "        self._first_webpage_request = True",
            "        self._post_hooks = []",
            "        self._progress_hooks = []",
            "        self._postprocessor_hooks = []",
            "        self._download_retcode = 0",
            "        self._num_downloads = 0",
            "        self._num_videos = 0",
            "        self._playlist_level = 0",
            "        self._playlist_urls = set()",
            "        self.cache = Cache(self)",
            "        self.__header_cookies = []",
            "",
            "        stdout = sys.stderr if self.params.get('logtostderr') else sys.stdout",
            "        self._out_files = Namespace(",
            "            out=stdout,",
            "            error=sys.stderr,",
            "            screen=sys.stderr if self.params.get('quiet') else stdout,",
            "            console=None if compat_os_name == 'nt' else next(",
            "                filter(supports_terminal_sequences, (sys.stderr, sys.stdout)), None)",
            "        )",
            "",
            "        try:",
            "            windows_enable_vt_mode()",
            "        except Exception as e:",
            "            self.write_debug(f'Failed to enable VT mode: {e}')",
            "",
            "        if self.params.get('no_color'):",
            "            if self.params.get('color') is not None:",
            "                self.params.setdefault('_warnings', []).append(",
            "                    'Overwriting params from \"color\" with \"no_color\"')",
            "            self.params['color'] = 'no_color'",
            "",
            "        term_allow_color = os.getenv('TERM', '').lower() != 'dumb'",
            "        no_color = bool(os.getenv('NO_COLOR'))",
            "",
            "        def process_color_policy(stream):",
            "            stream_name = {sys.stdout: 'stdout', sys.stderr: 'stderr'}[stream]",
            "            policy = traverse_obj(self.params, ('color', (stream_name, None), {str}), get_all=False)",
            "            if policy in ('auto', None):",
            "                if term_allow_color and supports_terminal_sequences(stream):",
            "                    return 'no_color' if no_color else True",
            "                return False",
            "            assert policy in ('always', 'never', 'no_color'), policy",
            "            return {'always': True, 'never': False}.get(policy, policy)",
            "",
            "        self._allow_colors = Namespace(**{",
            "            name: process_color_policy(stream)",
            "            for name, stream in self._out_files.items_ if name != 'console'",
            "        })",
            "",
            "        system_deprecation = _get_system_deprecation()",
            "        if system_deprecation:",
            "            self.deprecated_feature(system_deprecation.replace('\\n', '\\n                    '))",
            "",
            "        if self.params.get('allow_unplayable_formats'):",
            "            self.report_warning(",
            "                f'You have asked for {self._format_err(\"UNPLAYABLE\", self.Styles.EMPHASIS)} formats to be listed/downloaded. '",
            "                'This is a developer option intended for debugging. \\n'",
            "                '         If you experience any issues while using this option, '",
            "                f'{self._format_err(\"DO NOT\", self.Styles.ERROR)} open a bug report')",
            "",
            "        if self.params.get('bidi_workaround', False):",
            "            try:",
            "                import pty",
            "                master, slave = pty.openpty()",
            "                width = shutil.get_terminal_size().columns",
            "                width_args = [] if width is None else ['-w', str(width)]",
            "                sp_kwargs = {'stdin': subprocess.PIPE, 'stdout': slave, 'stderr': self._out_files.error}",
            "                try:",
            "                    self._output_process = Popen(['bidiv'] + width_args, **sp_kwargs)",
            "                except OSError:",
            "                    self._output_process = Popen(['fribidi', '-c', 'UTF-8'] + width_args, **sp_kwargs)",
            "                self._output_channel = os.fdopen(master, 'rb')",
            "            except OSError as ose:",
            "                if ose.errno == errno.ENOENT:",
            "                    self.report_warning(",
            "                        'Could not find fribidi executable, ignoring --bidi-workaround. '",
            "                        'Make sure that  fribidi  is an executable file in one of the directories in your $PATH.')",
            "                else:",
            "                    raise",
            "",
            "        self.params['compat_opts'] = set(self.params.get('compat_opts', ()))",
            "        self.params['http_headers'] = HTTPHeaderDict(std_headers, self.params.get('http_headers'))",
            "        self._load_cookies(self.params['http_headers'].get('Cookie'))  # compat",
            "        self.params['http_headers'].pop('Cookie', None)",
            "",
            "        if auto_init and auto_init != 'no_verbose_header':",
            "            self.print_debug_header()",
            "",
            "        def check_deprecated(param, option, suggestion):",
            "            if self.params.get(param) is not None:",
            "                self.report_warning(f'{option} is deprecated. Use {suggestion} instead')",
            "                return True",
            "            return False",
            "",
            "        if check_deprecated('cn_verification_proxy', '--cn-verification-proxy', '--geo-verification-proxy'):",
            "            if self.params.get('geo_verification_proxy') is None:",
            "                self.params['geo_verification_proxy'] = self.params['cn_verification_proxy']",
            "",
            "        check_deprecated('autonumber', '--auto-number', '-o \"%(autonumber)s-%(title)s.%(ext)s\"')",
            "        check_deprecated('usetitle', '--title', '-o \"%(title)s-%(id)s.%(ext)s\"')",
            "        check_deprecated('useid', '--id', '-o \"%(id)s.%(ext)s\"')",
            "",
            "        for msg in self.params.get('_warnings', []):",
            "            self.report_warning(msg)",
            "        for msg in self.params.get('_deprecation_warnings', []):",
            "            self.deprecated_feature(msg)",
            "",
            "        if impersonate_target := self.params.get('impersonate'):",
            "            if not self._impersonate_target_available(impersonate_target):",
            "                raise YoutubeDLError(",
            "                    f'Impersonate target \"{impersonate_target}\" is not available. '",
            "                    f'Use --list-impersonate-targets to see available targets. '",
            "                    f'You may be missing dependencies required to support this target.')",
            "",
            "        if 'list-formats' in self.params['compat_opts']:",
            "            self.params['listformats_table'] = False",
            "",
            "        if 'overwrites' not in self.params and self.params.get('nooverwrites') is not None:",
            "            # nooverwrites was unnecessarily changed to overwrites",
            "            # in 0c3d0f51778b153f65c21906031c2e091fcfb641",
            "            # This ensures compatibility with both keys",
            "            self.params['overwrites'] = not self.params['nooverwrites']",
            "        elif self.params.get('overwrites') is None:",
            "            self.params.pop('overwrites', None)",
            "        else:",
            "            self.params['nooverwrites'] = not self.params['overwrites']",
            "",
            "        if self.params.get('simulate') is None and any((",
            "            self.params.get('list_thumbnails'),",
            "            self.params.get('listformats'),",
            "            self.params.get('listsubtitles'),",
            "        )):",
            "            self.params['simulate'] = 'list_only'",
            "",
            "        self.params.setdefault('forceprint', {})",
            "        self.params.setdefault('print_to_file', {})",
            "",
            "        # Compatibility with older syntax",
            "        if not isinstance(params['forceprint'], dict):",
            "            self.params['forceprint'] = {'video': params['forceprint']}",
            "",
            "        if auto_init:",
            "            self.add_default_info_extractors()",
            "",
            "        if (sys.platform != 'win32'",
            "                and sys.getfilesystemencoding() in ['ascii', 'ANSI_X3.4-1968']",
            "                and not self.params.get('restrictfilenames', False)):",
            "            # Unicode filesystem API will throw errors (#1474, #13027)",
            "            self.report_warning(",
            "                'Assuming --restrict-filenames since file system encoding '",
            "                'cannot encode all characters. '",
            "                'Set the LC_ALL environment variable to fix this.')",
            "            self.params['restrictfilenames'] = True",
            "",
            "        self._parse_outtmpl()",
            "",
            "        # Creating format selector here allows us to catch syntax errors before the extraction",
            "        self.format_selector = (",
            "            self.params.get('format') if self.params.get('format') in (None, '-')",
            "            else self.params['format'] if callable(self.params['format'])",
            "            else self.build_format_selector(self.params['format']))",
            "",
            "        hooks = {",
            "            'post_hooks': self.add_post_hook,",
            "            'progress_hooks': self.add_progress_hook,",
            "            'postprocessor_hooks': self.add_postprocessor_hook,",
            "        }",
            "        for opt, fn in hooks.items():",
            "            for ph in self.params.get(opt, []):",
            "                fn(ph)",
            "",
            "        for pp_def_raw in self.params.get('postprocessors', []):",
            "            pp_def = dict(pp_def_raw)",
            "            when = pp_def.pop('when', 'post_process')",
            "            self.add_post_processor(",
            "                get_postprocessor(pp_def.pop('key'))(self, **pp_def),",
            "                when=when)",
            "",
            "        def preload_download_archive(fn):",
            "            \"\"\"Preload the archive, if any is specified\"\"\"",
            "            archive = set()",
            "            if fn is None:",
            "                return archive",
            "            elif not is_path_like(fn):",
            "                return fn",
            "",
            "            self.write_debug(f'Loading archive file {fn!r}')",
            "            try:",
            "                with locked_file(fn, 'r', encoding='utf-8') as archive_file:",
            "                    for line in archive_file:",
            "                        archive.add(line.strip())",
            "            except OSError as ioe:",
            "                if ioe.errno != errno.ENOENT:",
            "                    raise",
            "            return archive",
            "",
            "        self.archive = preload_download_archive(self.params.get('download_archive'))",
            "",
            "    def warn_if_short_id(self, argv):",
            "        # short YouTube ID starting with dash?",
            "        idxs = [",
            "            i for i, a in enumerate(argv)",
            "            if re.match(r'^-[0-9A-Za-z_-]{10}$', a)]",
            "        if idxs:",
            "            correct_argv = (",
            "                ['yt-dlp']",
            "                + [a for i, a in enumerate(argv) if i not in idxs]",
            "                + ['--'] + [argv[i] for i in idxs]",
            "            )",
            "            self.report_warning(",
            "                'Long argument string detected. '",
            "                'Use -- to separate parameters and URLs, like this:\\n%s' %",
            "                shell_quote(correct_argv))",
            "",
            "    def add_info_extractor(self, ie):",
            "        \"\"\"Add an InfoExtractor object to the end of the list.\"\"\"",
            "        ie_key = ie.ie_key()",
            "        self._ies[ie_key] = ie",
            "        if not isinstance(ie, type):",
            "            self._ies_instances[ie_key] = ie",
            "            ie.set_downloader(self)",
            "",
            "    def get_info_extractor(self, ie_key):",
            "        \"\"\"",
            "        Get an instance of an IE with name ie_key, it will try to get one from",
            "        the _ies list, if there's no instance it will create a new one and add",
            "        it to the extractor list.",
            "        \"\"\"",
            "        ie = self._ies_instances.get(ie_key)",
            "        if ie is None:",
            "            ie = get_info_extractor(ie_key)()",
            "            self.add_info_extractor(ie)",
            "        return ie",
            "",
            "    def add_default_info_extractors(self):",
            "        \"\"\"",
            "        Add the InfoExtractors returned by gen_extractors to the end of the list",
            "        \"\"\"",
            "        all_ies = {ie.IE_NAME.lower(): ie for ie in gen_extractor_classes()}",
            "        all_ies['end'] = UnsupportedURLIE()",
            "        try:",
            "            ie_names = orderedSet_from_options(",
            "                self.params.get('allowed_extractors', ['default']), {",
            "                    'all': list(all_ies),",
            "                    'default': [name for name, ie in all_ies.items() if ie._ENABLED],",
            "                }, use_regex=True)",
            "        except re.error as e:",
            "            raise ValueError(f'Wrong regex for allowed_extractors: {e.pattern}')",
            "        for name in ie_names:",
            "            self.add_info_extractor(all_ies[name])",
            "        self.write_debug(f'Loaded {len(ie_names)} extractors')",
            "",
            "    def add_post_processor(self, pp, when='post_process'):",
            "        \"\"\"Add a PostProcessor object to the end of the chain.\"\"\"",
            "        assert when in POSTPROCESS_WHEN, f'Invalid when={when}'",
            "        self._pps[when].append(pp)",
            "        pp.set_downloader(self)",
            "",
            "    def add_post_hook(self, ph):",
            "        \"\"\"Add the post hook\"\"\"",
            "        self._post_hooks.append(ph)",
            "",
            "    def add_progress_hook(self, ph):",
            "        \"\"\"Add the download progress hook\"\"\"",
            "        self._progress_hooks.append(ph)",
            "",
            "    def add_postprocessor_hook(self, ph):",
            "        \"\"\"Add the postprocessing progress hook\"\"\"",
            "        self._postprocessor_hooks.append(ph)",
            "        for pps in self._pps.values():",
            "            for pp in pps:",
            "                pp.add_progress_hook(ph)",
            "",
            "    def _bidi_workaround(self, message):",
            "        if not hasattr(self, '_output_channel'):",
            "            return message",
            "",
            "        assert hasattr(self, '_output_process')",
            "        assert isinstance(message, str)",
            "        line_count = message.count('\\n') + 1",
            "        self._output_process.stdin.write((message + '\\n').encode())",
            "        self._output_process.stdin.flush()",
            "        res = ''.join(self._output_channel.readline().decode()",
            "                      for _ in range(line_count))",
            "        return res[:-len('\\n')]",
            "",
            "    def _write_string(self, message, out=None, only_once=False):",
            "        if only_once:",
            "            if message in self._printed_messages:",
            "                return",
            "            self._printed_messages.add(message)",
            "        write_string(message, out=out, encoding=self.params.get('encoding'))",
            "",
            "    def to_stdout(self, message, skip_eol=False, quiet=None):",
            "        \"\"\"Print message to stdout\"\"\"",
            "        if quiet is not None:",
            "            self.deprecation_warning('\"YoutubeDL.to_stdout\" no longer accepts the argument quiet. '",
            "                                     'Use \"YoutubeDL.to_screen\" instead')",
            "        if skip_eol is not False:",
            "            self.deprecation_warning('\"YoutubeDL.to_stdout\" no longer accepts the argument skip_eol. '",
            "                                     'Use \"YoutubeDL.to_screen\" instead')",
            "        self._write_string(f'{self._bidi_workaround(message)}\\n', self._out_files.out)",
            "",
            "    def to_screen(self, message, skip_eol=False, quiet=None, only_once=False):",
            "        \"\"\"Print message to screen if not in quiet mode\"\"\"",
            "        if self.params.get('logger'):",
            "            self.params['logger'].debug(message)",
            "            return",
            "        if (self.params.get('quiet') if quiet is None else quiet) and not self.params.get('verbose'):",
            "            return",
            "        self._write_string(",
            "            '%s%s' % (self._bidi_workaround(message), ('' if skip_eol else '\\n')),",
            "            self._out_files.screen, only_once=only_once)",
            "",
            "    def to_stderr(self, message, only_once=False):",
            "        \"\"\"Print message to stderr\"\"\"",
            "        assert isinstance(message, str)",
            "        if self.params.get('logger'):",
            "            self.params['logger'].error(message)",
            "        else:",
            "            self._write_string(f'{self._bidi_workaround(message)}\\n', self._out_files.error, only_once=only_once)",
            "",
            "    def _send_console_code(self, code):",
            "        if compat_os_name == 'nt' or not self._out_files.console:",
            "            return",
            "        self._write_string(code, self._out_files.console)",
            "",
            "    def to_console_title(self, message):",
            "        if not self.params.get('consoletitle', False):",
            "            return",
            "        message = remove_terminal_sequences(message)",
            "        if compat_os_name == 'nt':",
            "            if ctypes.windll.kernel32.GetConsoleWindow():",
            "                # c_wchar_p() might not be necessary if `message` is",
            "                # already of type unicode()",
            "                ctypes.windll.kernel32.SetConsoleTitleW(ctypes.c_wchar_p(message))",
            "        else:",
            "            self._send_console_code(f'\\033]0;{message}\\007')",
            "",
            "    def save_console_title(self):",
            "        if not self.params.get('consoletitle') or self.params.get('simulate'):",
            "            return",
            "        self._send_console_code('\\033[22;0t')  # Save the title on stack",
            "",
            "    def restore_console_title(self):",
            "        if not self.params.get('consoletitle') or self.params.get('simulate'):",
            "            return",
            "        self._send_console_code('\\033[23;0t')  # Restore the title from stack",
            "",
            "    def __enter__(self):",
            "        self.save_console_title()",
            "        return self",
            "",
            "    def save_cookies(self):",
            "        if self.params.get('cookiefile') is not None:",
            "            self.cookiejar.save()",
            "",
            "    def __exit__(self, *args):",
            "        self.restore_console_title()",
            "        self.close()",
            "",
            "    def close(self):",
            "        self.save_cookies()",
            "        if '_request_director' in self.__dict__:",
            "            self._request_director.close()",
            "            del self._request_director",
            "",
            "    def trouble(self, message=None, tb=None, is_error=True):",
            "        \"\"\"Determine action to take when a download problem appears.",
            "",
            "        Depending on if the downloader has been configured to ignore",
            "        download errors or not, this method may throw an exception or",
            "        not when errors are found, after printing the message.",
            "",
            "        @param tb          If given, is additional traceback information",
            "        @param is_error    Whether to raise error according to ignorerrors",
            "        \"\"\"",
            "        if message is not None:",
            "            self.to_stderr(message)",
            "        if self.params.get('verbose'):",
            "            if tb is None:",
            "                if sys.exc_info()[0]:  # if .trouble has been called from an except block",
            "                    tb = ''",
            "                    if hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:",
            "                        tb += ''.join(traceback.format_exception(*sys.exc_info()[1].exc_info))",
            "                    tb += encode_compat_str(traceback.format_exc())",
            "                else:",
            "                    tb_data = traceback.format_list(traceback.extract_stack())",
            "                    tb = ''.join(tb_data)",
            "            if tb:",
            "                self.to_stderr(tb)",
            "        if not is_error:",
            "            return",
            "        if not self.params.get('ignoreerrors'):",
            "            if sys.exc_info()[0] and hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:",
            "                exc_info = sys.exc_info()[1].exc_info",
            "            else:",
            "                exc_info = sys.exc_info()",
            "            raise DownloadError(message, exc_info)",
            "        self._download_retcode = 1",
            "",
            "    Styles = Namespace(",
            "        HEADERS='yellow',",
            "        EMPHASIS='light blue',",
            "        FILENAME='green',",
            "        ID='green',",
            "        DELIM='blue',",
            "        ERROR='red',",
            "        BAD_FORMAT='light red',",
            "        WARNING='yellow',",
            "        SUPPRESS='light black',",
            "    )",
            "",
            "    def _format_text(self, handle, allow_colors, text, f, fallback=None, *, test_encoding=False):",
            "        text = str(text)",
            "        if test_encoding:",
            "            original_text = text",
            "            # handle.encoding can be None. See https://github.com/yt-dlp/yt-dlp/issues/2711",
            "            encoding = self.params.get('encoding') or getattr(handle, 'encoding', None) or 'ascii'",
            "            text = text.encode(encoding, 'ignore').decode(encoding)",
            "            if fallback is not None and text != original_text:",
            "                text = fallback",
            "        return format_text(text, f) if allow_colors is True else text if fallback is None else fallback",
            "",
            "    def _format_out(self, *args, **kwargs):",
            "        return self._format_text(self._out_files.out, self._allow_colors.out, *args, **kwargs)",
            "",
            "    def _format_screen(self, *args, **kwargs):",
            "        return self._format_text(self._out_files.screen, self._allow_colors.screen, *args, **kwargs)",
            "",
            "    def _format_err(self, *args, **kwargs):",
            "        return self._format_text(self._out_files.error, self._allow_colors.error, *args, **kwargs)",
            "",
            "    def report_warning(self, message, only_once=False):",
            "        '''",
            "        Print the message to stderr, it will be prefixed with 'WARNING:'",
            "        If stderr is a tty file the 'WARNING:' will be colored",
            "        '''",
            "        if self.params.get('logger') is not None:",
            "            self.params['logger'].warning(message)",
            "        else:",
            "            if self.params.get('no_warnings'):",
            "                return",
            "            self.to_stderr(f'{self._format_err(\"WARNING:\", self.Styles.WARNING)} {message}', only_once)",
            "",
            "    def deprecation_warning(self, message, *, stacklevel=0):",
            "        deprecation_warning(",
            "            message, stacklevel=stacklevel + 1, printer=self.report_error, is_error=False)",
            "",
            "    def deprecated_feature(self, message):",
            "        if self.params.get('logger') is not None:",
            "            self.params['logger'].warning(f'Deprecated Feature: {message}')",
            "        self.to_stderr(f'{self._format_err(\"Deprecated Feature:\", self.Styles.ERROR)} {message}', True)",
            "",
            "    def report_error(self, message, *args, **kwargs):",
            "        '''",
            "        Do the same as trouble, but prefixes the message with 'ERROR:', colored",
            "        in red if stderr is a tty file.",
            "        '''",
            "        self.trouble(f'{self._format_err(\"ERROR:\", self.Styles.ERROR)} {message}', *args, **kwargs)",
            "",
            "    def write_debug(self, message, only_once=False):",
            "        '''Log debug message or Print message to stderr'''",
            "        if not self.params.get('verbose', False):",
            "            return",
            "        message = f'[debug] {message}'",
            "        if self.params.get('logger'):",
            "            self.params['logger'].debug(message)",
            "        else:",
            "            self.to_stderr(message, only_once)",
            "",
            "    def report_file_already_downloaded(self, file_name):",
            "        \"\"\"Report file has already been fully downloaded.\"\"\"",
            "        try:",
            "            self.to_screen('[download] %s has already been downloaded' % file_name)",
            "        except UnicodeEncodeError:",
            "            self.to_screen('[download] The file has already been downloaded')",
            "",
            "    def report_file_delete(self, file_name):",
            "        \"\"\"Report that existing file will be deleted.\"\"\"",
            "        try:",
            "            self.to_screen('Deleting existing file %s' % file_name)",
            "        except UnicodeEncodeError:",
            "            self.to_screen('Deleting existing file')",
            "",
            "    def raise_no_formats(self, info, forced=False, *, msg=None):",
            "        has_drm = info.get('_has_drm')",
            "        ignored, expected = self.params.get('ignore_no_formats_error'), bool(msg)",
            "        msg = msg or has_drm and 'This video is DRM protected' or 'No video formats found!'",
            "        if forced or not ignored:",
            "            raise ExtractorError(msg, video_id=info['id'], ie=info['extractor'],",
            "                                 expected=has_drm or ignored or expected)",
            "        else:",
            "            self.report_warning(msg)",
            "",
            "    def parse_outtmpl(self):",
            "        self.deprecation_warning('\"YoutubeDL.parse_outtmpl\" is deprecated and may be removed in a future version')",
            "        self._parse_outtmpl()",
            "        return self.params['outtmpl']",
            "",
            "    def _parse_outtmpl(self):",
            "        sanitize = IDENTITY",
            "        if self.params.get('restrictfilenames'):  # Remove spaces in the default template",
            "            sanitize = lambda x: x.replace(' - ', ' ').replace(' ', '-')",
            "",
            "        outtmpl = self.params.setdefault('outtmpl', {})",
            "        if not isinstance(outtmpl, dict):",
            "            self.params['outtmpl'] = outtmpl = {'default': outtmpl}",
            "        outtmpl.update({k: sanitize(v) for k, v in DEFAULT_OUTTMPL.items() if outtmpl.get(k) is None})",
            "",
            "    def get_output_path(self, dir_type='', filename=None):",
            "        paths = self.params.get('paths', {})",
            "        assert isinstance(paths, dict), '\"paths\" parameter must be a dictionary'",
            "        path = os.path.join(",
            "            expand_path(paths.get('home', '').strip()),",
            "            expand_path(paths.get(dir_type, '').strip()) if dir_type else '',",
            "            filename or '')",
            "        return sanitize_path(path, force=self.params.get('windowsfilenames'))",
            "",
            "    @staticmethod",
            "    def _outtmpl_expandpath(outtmpl):",
            "        # expand_path translates '%%' into '%' and '$$' into '$'",
            "        # correspondingly that is not what we want since we need to keep",
            "        # '%%' intact for template dict substitution step. Working around",
            "        # with boundary-alike separator hack.",
            "        sep = ''.join(random.choices(string.ascii_letters, k=32))",
            "        outtmpl = outtmpl.replace('%%', f'%{sep}%').replace('$$', f'${sep}$')",
            "",
            "        # outtmpl should be expand_path'ed before template dict substitution",
            "        # because meta fields may contain env variables we don't want to",
            "        # be expanded. E.g. for outtmpl \"%(title)s.%(ext)s\" and",
            "        # title \"Hello $PATH\", we don't want `$PATH` to be expanded.",
            "        return expand_path(outtmpl).replace(sep, '')",
            "",
            "    @staticmethod",
            "    def escape_outtmpl(outtmpl):",
            "        ''' Escape any remaining strings like %s, %abc% etc. '''",
            "        return re.sub(",
            "            STR_FORMAT_RE_TMPL.format('', '(?![%(\\0])'),",
            "            lambda mobj: ('' if mobj.group('has_key') else '%') + mobj.group(0),",
            "            outtmpl)",
            "",
            "    @classmethod",
            "    def validate_outtmpl(cls, outtmpl):",
            "        ''' @return None or Exception object '''",
            "        outtmpl = re.sub(",
            "            STR_FORMAT_RE_TMPL.format('[^)]*', '[ljhqBUDS]'),",
            "            lambda mobj: f'{mobj.group(0)[:-1]}s',",
            "            cls._outtmpl_expandpath(outtmpl))",
            "        try:",
            "            cls.escape_outtmpl(outtmpl) % collections.defaultdict(int)",
            "            return None",
            "        except ValueError as err:",
            "            return err",
            "",
            "    @staticmethod",
            "    def _copy_infodict(info_dict):",
            "        info_dict = dict(info_dict)",
            "        info_dict.pop('__postprocessors', None)",
            "        info_dict.pop('__pending_error', None)",
            "        return info_dict",
            "",
            "    def prepare_outtmpl(self, outtmpl, info_dict, sanitize=False):",
            "        \"\"\" Make the outtmpl and info_dict suitable for substitution: ydl.escape_outtmpl(outtmpl) % info_dict",
            "        @param sanitize    Whether to sanitize the output as a filename.",
            "                           For backward compatibility, a function can also be passed",
            "        \"\"\"",
            "",
            "        info_dict.setdefault('epoch', int(time.time()))  # keep epoch consistent once set",
            "",
            "        info_dict = self._copy_infodict(info_dict)",
            "        info_dict['duration_string'] = (  # %(duration>%H-%M-%S)s is wrong if duration > 24hrs",
            "            formatSeconds(info_dict['duration'], '-' if sanitize else ':')",
            "            if info_dict.get('duration', None) is not None",
            "            else None)",
            "        info_dict['autonumber'] = int(self.params.get('autonumber_start', 1) - 1 + self._num_downloads)",
            "        info_dict['video_autonumber'] = self._num_videos",
            "        if info_dict.get('resolution') is None:",
            "            info_dict['resolution'] = self.format_resolution(info_dict, default=None)",
            "",
            "        # For fields playlist_index, playlist_autonumber and autonumber convert all occurrences",
            "        # of %(field)s to %(field)0Nd for backward compatibility",
            "        field_size_compat_map = {",
            "            'playlist_index': number_of_digits(info_dict.get('__last_playlist_index') or 0),",
            "            'playlist_autonumber': number_of_digits(info_dict.get('n_entries') or 0),",
            "            'autonumber': self.params.get('autonumber_size') or 5,",
            "        }",
            "",
            "        TMPL_DICT = {}",
            "        EXTERNAL_FORMAT_RE = re.compile(STR_FORMAT_RE_TMPL.format('[^)]*', f'[{STR_FORMAT_TYPES}ljhqBUDS]'))",
            "        MATH_FUNCTIONS = {",
            "            '+': float.__add__,",
            "            '-': float.__sub__,",
            "            '*': float.__mul__,",
            "        }",
            "        # Field is of the form key1.key2...",
            "        # where keys (except first) can be string, int, slice or \"{field, ...}\"",
            "        FIELD_INNER_RE = r'(?:\\w+|%(num)s|%(num)s?(?::%(num)s?){1,2})' % {'num': r'(?:-?\\d+)'}",
            "        FIELD_RE = r'\\w*(?:\\.(?:%(inner)s|{%(field)s(?:,%(field)s)*}))*' % {",
            "            'inner': FIELD_INNER_RE,",
            "            'field': rf'\\w*(?:\\.{FIELD_INNER_RE})*'",
            "        }",
            "        MATH_FIELD_RE = rf'(?:{FIELD_RE}|-?{NUMBER_RE})'",
            "        MATH_OPERATORS_RE = r'(?:%s)' % '|'.join(map(re.escape, MATH_FUNCTIONS.keys()))",
            "        INTERNAL_FORMAT_RE = re.compile(rf'''(?xs)",
            "            (?P<negate>-)?",
            "            (?P<fields>{FIELD_RE})",
            "            (?P<maths>(?:{MATH_OPERATORS_RE}{MATH_FIELD_RE})*)",
            "            (?:>(?P<strf_format>.+?))?",
            "            (?P<remaining>",
            "                (?P<alternate>(?<!\\\\),[^|&)]+)?",
            "                (?:&(?P<replacement>.*?))?",
            "                (?:\\|(?P<default>.*?))?",
            "            )$''')",
            "",
            "        def _from_user_input(field):",
            "            if field == ':':",
            "                return ...",
            "            elif ':' in field:",
            "                return slice(*map(int_or_none, field.split(':')))",
            "            elif int_or_none(field) is not None:",
            "                return int(field)",
            "            return field",
            "",
            "        def _traverse_infodict(fields):",
            "            fields = [f for x in re.split(r'\\.({.+?})\\.?', fields)",
            "                      for f in ([x] if x.startswith('{') else x.split('.'))]",
            "            for i in (0, -1):",
            "                if fields and not fields[i]:",
            "                    fields.pop(i)",
            "",
            "            for i, f in enumerate(fields):",
            "                if not f.startswith('{'):",
            "                    fields[i] = _from_user_input(f)",
            "                    continue",
            "                assert f.endswith('}'), f'No closing brace for {f} in {fields}'",
            "                fields[i] = {k: list(map(_from_user_input, k.split('.'))) for k in f[1:-1].split(',')}",
            "",
            "            return traverse_obj(info_dict, fields, traverse_string=True)",
            "",
            "        def get_value(mdict):",
            "            # Object traversal",
            "            value = _traverse_infodict(mdict['fields'])",
            "            # Negative",
            "            if mdict['negate']:",
            "                value = float_or_none(value)",
            "                if value is not None:",
            "                    value *= -1",
            "            # Do maths",
            "            offset_key = mdict['maths']",
            "            if offset_key:",
            "                value = float_or_none(value)",
            "                operator = None",
            "                while offset_key:",
            "                    item = re.match(",
            "                        MATH_FIELD_RE if operator else MATH_OPERATORS_RE,",
            "                        offset_key).group(0)",
            "                    offset_key = offset_key[len(item):]",
            "                    if operator is None:",
            "                        operator = MATH_FUNCTIONS[item]",
            "                        continue",
            "                    item, multiplier = (item[1:], -1) if item[0] == '-' else (item, 1)",
            "                    offset = float_or_none(item)",
            "                    if offset is None:",
            "                        offset = float_or_none(_traverse_infodict(item))",
            "                    try:",
            "                        value = operator(value, multiplier * offset)",
            "                    except (TypeError, ZeroDivisionError):",
            "                        return None",
            "                    operator = None",
            "            # Datetime formatting",
            "            if mdict['strf_format']:",
            "                value = strftime_or_none(value, mdict['strf_format'].replace('\\\\,', ','))",
            "",
            "            # XXX: Workaround for https://github.com/yt-dlp/yt-dlp/issues/4485",
            "            if sanitize and value == '':",
            "                value = None",
            "            return value",
            "",
            "        na = self.params.get('outtmpl_na_placeholder', 'NA')",
            "",
            "        def filename_sanitizer(key, value, restricted=self.params.get('restrictfilenames')):",
            "            return sanitize_filename(str(value), restricted=restricted, is_id=(",
            "                bool(re.search(r'(^|[_.])id(\\.|$)', key))",
            "                if 'filename-sanitization' in self.params['compat_opts']",
            "                else NO_DEFAULT))",
            "",
            "        sanitizer = sanitize if callable(sanitize) else filename_sanitizer",
            "        sanitize = bool(sanitize)",
            "",
            "        def _dumpjson_default(obj):",
            "            if isinstance(obj, (set, LazyList)):",
            "                return list(obj)",
            "            return repr(obj)",
            "",
            "        class _ReplacementFormatter(string.Formatter):",
            "            def get_field(self, field_name, args, kwargs):",
            "                if field_name.isdigit():",
            "                    return args[0], -1",
            "                raise ValueError('Unsupported field')",
            "",
            "        replacement_formatter = _ReplacementFormatter()",
            "",
            "        def create_key(outer_mobj):",
            "            if not outer_mobj.group('has_key'):",
            "                return outer_mobj.group(0)",
            "            key = outer_mobj.group('key')",
            "            mobj = re.match(INTERNAL_FORMAT_RE, key)",
            "            value, replacement, default, last_field = None, None, na, ''",
            "            while mobj:",
            "                mobj = mobj.groupdict()",
            "                default = mobj['default'] if mobj['default'] is not None else default",
            "                value = get_value(mobj)",
            "                last_field, replacement = mobj['fields'], mobj['replacement']",
            "                if value is None and mobj['alternate']:",
            "                    mobj = re.match(INTERNAL_FORMAT_RE, mobj['remaining'][1:])",
            "                else:",
            "                    break",
            "",
            "            if None not in (value, replacement):",
            "                try:",
            "                    value = replacement_formatter.format(replacement, value)",
            "                except ValueError:",
            "                    value, default = None, na",
            "",
            "            fmt = outer_mobj.group('format')",
            "            if fmt == 's' and last_field in field_size_compat_map.keys() and isinstance(value, int):",
            "                fmt = f'0{field_size_compat_map[last_field]:d}d'",
            "",
            "            flags = outer_mobj.group('conversion') or ''",
            "            str_fmt = f'{fmt[:-1]}s'",
            "            if value is None:",
            "                value, fmt = default, 's'",
            "            elif fmt[-1] == 'l':  # list",
            "                delim = '\\n' if '#' in flags else ', '",
            "                value, fmt = delim.join(map(str, variadic(value, allowed_types=(str, bytes)))), str_fmt",
            "            elif fmt[-1] == 'j':  # json",
            "                value, fmt = json.dumps(",
            "                    value, default=_dumpjson_default,",
            "                    indent=4 if '#' in flags else None, ensure_ascii='+' not in flags), str_fmt",
            "            elif fmt[-1] == 'h':  # html",
            "                value, fmt = escapeHTML(str(value)), str_fmt",
            "            elif fmt[-1] == 'q':  # quoted",
            "                value = map(str, variadic(value) if '#' in flags else [value])",
            "                value, fmt = shell_quote(value, shell=True), str_fmt",
            "            elif fmt[-1] == 'B':  # bytes",
            "                value = f'%{str_fmt}'.encode() % str(value).encode()",
            "                value, fmt = value.decode('utf-8', 'ignore'), 's'",
            "            elif fmt[-1] == 'U':  # unicode normalized",
            "                value, fmt = unicodedata.normalize(",
            "                    # \"+\" = compatibility equivalence, \"#\" = NFD",
            "                    'NF%s%s' % ('K' if '+' in flags else '', 'D' if '#' in flags else 'C'),",
            "                    value), str_fmt",
            "            elif fmt[-1] == 'D':  # decimal suffix",
            "                num_fmt, fmt = fmt[:-1].replace('#', ''), 's'",
            "                value = format_decimal_suffix(value, f'%{num_fmt}f%s' if num_fmt else '%d%s',",
            "                                              factor=1024 if '#' in flags else 1000)",
            "            elif fmt[-1] == 'S':  # filename sanitization",
            "                value, fmt = filename_sanitizer(last_field, value, restricted='#' in flags), str_fmt",
            "            elif fmt[-1] == 'c':",
            "                if value:",
            "                    value = str(value)[0]",
            "                else:",
            "                    fmt = str_fmt",
            "            elif fmt[-1] not in 'rsa':  # numeric",
            "                value = float_or_none(value)",
            "                if value is None:",
            "                    value, fmt = default, 's'",
            "",
            "            if sanitize:",
            "                # If value is an object, sanitize might convert it to a string",
            "                # So we convert it to repr first",
            "                if fmt[-1] == 'r':",
            "                    value, fmt = repr(value), str_fmt",
            "                elif fmt[-1] == 'a':",
            "                    value, fmt = ascii(value), str_fmt",
            "                if fmt[-1] in 'csra':",
            "                    value = sanitizer(last_field, value)",
            "",
            "            key = '%s\\0%s' % (key.replace('%', '%\\0'), outer_mobj.group('format'))",
            "            TMPL_DICT[key] = value",
            "            return '{prefix}%({key}){fmt}'.format(key=key, fmt=fmt, prefix=outer_mobj.group('prefix'))",
            "",
            "        return EXTERNAL_FORMAT_RE.sub(create_key, outtmpl), TMPL_DICT",
            "",
            "    def evaluate_outtmpl(self, outtmpl, info_dict, *args, **kwargs):",
            "        outtmpl, info_dict = self.prepare_outtmpl(outtmpl, info_dict, *args, **kwargs)",
            "        return self.escape_outtmpl(outtmpl) % info_dict",
            "",
            "    def _prepare_filename(self, info_dict, *, outtmpl=None, tmpl_type=None):",
            "        assert None in (outtmpl, tmpl_type), 'outtmpl and tmpl_type are mutually exclusive'",
            "        if outtmpl is None:",
            "            outtmpl = self.params['outtmpl'].get(tmpl_type or 'default', self.params['outtmpl']['default'])",
            "        try:",
            "            outtmpl = self._outtmpl_expandpath(outtmpl)",
            "            filename = self.evaluate_outtmpl(outtmpl, info_dict, True)",
            "            if not filename:",
            "                return None",
            "",
            "            if tmpl_type in ('', 'temp'):",
            "                final_ext, ext = self.params.get('final_ext'), info_dict.get('ext')",
            "                if final_ext and ext and final_ext != ext and filename.endswith(f'.{final_ext}'):",
            "                    filename = replace_extension(filename, ext, final_ext)",
            "            elif tmpl_type:",
            "                force_ext = OUTTMPL_TYPES[tmpl_type]",
            "                if force_ext:",
            "                    filename = replace_extension(filename, force_ext, info_dict.get('ext'))",
            "",
            "            # https://github.com/blackjack4494/youtube-dlc/issues/85",
            "            trim_file_name = self.params.get('trim_file_name', False)",
            "            if trim_file_name:",
            "                no_ext, *ext = filename.rsplit('.', 2)",
            "                filename = join_nonempty(no_ext[:trim_file_name], *ext, delim='.')",
            "",
            "            return filename",
            "        except ValueError as err:",
            "            self.report_error('Error in output template: ' + str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')",
            "            return None",
            "",
            "    def prepare_filename(self, info_dict, dir_type='', *, outtmpl=None, warn=False):",
            "        \"\"\"Generate the output filename\"\"\"",
            "        if outtmpl:",
            "            assert not dir_type, 'outtmpl and dir_type are mutually exclusive'",
            "            dir_type = None",
            "        filename = self._prepare_filename(info_dict, tmpl_type=dir_type, outtmpl=outtmpl)",
            "        if not filename and dir_type not in ('', 'temp'):",
            "            return ''",
            "",
            "        if warn:",
            "            if not self.params.get('paths'):",
            "                pass",
            "            elif filename == '-':",
            "                self.report_warning('--paths is ignored when an outputting to stdout', only_once=True)",
            "            elif os.path.isabs(filename):",
            "                self.report_warning('--paths is ignored since an absolute path is given in output template', only_once=True)",
            "        if filename == '-' or not filename:",
            "            return filename",
            "",
            "        return self.get_output_path(dir_type, filename)",
            "",
            "    def _match_entry(self, info_dict, incomplete=False, silent=False):",
            "        \"\"\"Returns None if the file should be downloaded\"\"\"",
            "        _type = 'video' if 'playlist-match-filter' in self.params['compat_opts'] else info_dict.get('_type', 'video')",
            "        assert incomplete or _type == 'video', 'Only video result can be considered complete'",
            "",
            "        video_title = info_dict.get('title', info_dict.get('id', 'entry'))",
            "",
            "        def check_filter():",
            "            if _type in ('playlist', 'multi_video'):",
            "                return",
            "            elif _type in ('url', 'url_transparent') and not try_call(",
            "                    lambda: self.get_info_extractor(info_dict['ie_key']).is_single_video(info_dict['url'])):",
            "                return",
            "",
            "            if 'title' in info_dict:",
            "                # This can happen when we're just evaluating the playlist",
            "                title = info_dict['title']",
            "                matchtitle = self.params.get('matchtitle', False)",
            "                if matchtitle:",
            "                    if not re.search(matchtitle, title, re.IGNORECASE):",
            "                        return '\"' + title + '\" title did not match pattern \"' + matchtitle + '\"'",
            "                rejecttitle = self.params.get('rejecttitle', False)",
            "                if rejecttitle:",
            "                    if re.search(rejecttitle, title, re.IGNORECASE):",
            "                        return '\"' + title + '\" title matched reject pattern \"' + rejecttitle + '\"'",
            "",
            "            date = info_dict.get('upload_date')",
            "            if date is not None:",
            "                dateRange = self.params.get('daterange', DateRange())",
            "                if date not in dateRange:",
            "                    return f'{date_from_str(date).isoformat()} upload date is not in range {dateRange}'",
            "            view_count = info_dict.get('view_count')",
            "            if view_count is not None:",
            "                min_views = self.params.get('min_views')",
            "                if min_views is not None and view_count < min_views:",
            "                    return 'Skipping %s, because it has not reached minimum view count (%d/%d)' % (video_title, view_count, min_views)",
            "                max_views = self.params.get('max_views')",
            "                if max_views is not None and view_count > max_views:",
            "                    return 'Skipping %s, because it has exceeded the maximum view count (%d/%d)' % (video_title, view_count, max_views)",
            "            if age_restricted(info_dict.get('age_limit'), self.params.get('age_limit')):",
            "                return 'Skipping \"%s\" because it is age restricted' % video_title",
            "",
            "            match_filter = self.params.get('match_filter')",
            "            if match_filter is None:",
            "                return None",
            "",
            "            cancelled = None",
            "            try:",
            "                try:",
            "                    ret = match_filter(info_dict, incomplete=incomplete)",
            "                except TypeError:",
            "                    # For backward compatibility",
            "                    ret = None if incomplete else match_filter(info_dict)",
            "            except DownloadCancelled as err:",
            "                if err.msg is not NO_DEFAULT:",
            "                    raise",
            "                ret, cancelled = err.msg, err",
            "",
            "            if ret is NO_DEFAULT:",
            "                while True:",
            "                    filename = self._format_screen(self.prepare_filename(info_dict), self.Styles.FILENAME)",
            "                    reply = input(self._format_screen(",
            "                        f'Download \"{filename}\"? (Y/n): ', self.Styles.EMPHASIS)).lower().strip()",
            "                    if reply in {'y', ''}:",
            "                        return None",
            "                    elif reply == 'n':",
            "                        if cancelled:",
            "                            raise type(cancelled)(f'Skipping {video_title}')",
            "                        return f'Skipping {video_title}'",
            "            return ret",
            "",
            "        if self.in_download_archive(info_dict):",
            "            reason = ''.join((",
            "                format_field(info_dict, 'id', f'{self._format_screen(\"%s\", self.Styles.ID)}: '),",
            "                format_field(info_dict, 'title', f'{self._format_screen(\"%s\", self.Styles.EMPHASIS)} '),",
            "                'has already been recorded in the archive'))",
            "            break_opt, break_err = 'break_on_existing', ExistingVideoReached",
            "        else:",
            "            try:",
            "                reason = check_filter()",
            "            except DownloadCancelled as e:",
            "                reason, break_opt, break_err = e.msg, 'match_filter', type(e)",
            "            else:",
            "                break_opt, break_err = 'break_on_reject', RejectedVideoReached",
            "        if reason is not None:",
            "            if not silent:",
            "                self.to_screen('[download] ' + reason)",
            "            if self.params.get(break_opt, False):",
            "                raise break_err()",
            "        return reason",
            "",
            "    @staticmethod",
            "    def add_extra_info(info_dict, extra_info):",
            "        '''Set the keys from extra_info in info dict if they are missing'''",
            "        for key, value in extra_info.items():",
            "            info_dict.setdefault(key, value)",
            "",
            "    def extract_info(self, url, download=True, ie_key=None, extra_info=None,",
            "                     process=True, force_generic_extractor=False):",
            "        \"\"\"",
            "        Extract and return the information dictionary of the URL",
            "",
            "        Arguments:",
            "        @param url          URL to extract",
            "",
            "        Keyword arguments:",
            "        @param download     Whether to download videos",
            "        @param process      Whether to resolve all unresolved references (URLs, playlist items).",
            "                            Must be True for download to work",
            "        @param ie_key       Use only the extractor with this key",
            "",
            "        @param extra_info   Dictionary containing the extra values to add to the info (For internal use only)",
            "        @force_generic_extractor  Force using the generic extractor (Deprecated; use ie_key='Generic')",
            "        \"\"\"",
            "",
            "        if extra_info is None:",
            "            extra_info = {}",
            "",
            "        if not ie_key and force_generic_extractor:",
            "            ie_key = 'Generic'",
            "",
            "        if ie_key:",
            "            ies = {ie_key: self._ies[ie_key]} if ie_key in self._ies else {}",
            "        else:",
            "            ies = self._ies",
            "",
            "        for key, ie in ies.items():",
            "            if not ie.suitable(url):",
            "                continue",
            "",
            "            if not ie.working():",
            "                self.report_warning('The program functionality for this site has been marked as broken, '",
            "                                    'and will probably not work.')",
            "",
            "            temp_id = ie.get_temp_id(url)",
            "            if temp_id is not None and self.in_download_archive({'id': temp_id, 'ie_key': key}):",
            "                self.to_screen(f'[download] {self._format_screen(temp_id, self.Styles.ID)}: '",
            "                               'has already been recorded in the archive')",
            "                if self.params.get('break_on_existing', False):",
            "                    raise ExistingVideoReached()",
            "                break",
            "            return self.__extract_info(url, self.get_info_extractor(key), download, extra_info, process)",
            "        else:",
            "            extractors_restricted = self.params.get('allowed_extractors') not in (None, ['default'])",
            "            self.report_error(f'No suitable extractor{format_field(ie_key, None, \" (%s)\")} found for URL {url}',",
            "                              tb=False if extractors_restricted else None)",
            "",
            "    def _handle_extraction_exceptions(func):",
            "        @functools.wraps(func)",
            "        def wrapper(self, *args, **kwargs):",
            "            while True:",
            "                try:",
            "                    return func(self, *args, **kwargs)",
            "                except (DownloadCancelled, LazyList.IndexError, PagedList.IndexError):",
            "                    raise",
            "                except ReExtractInfo as e:",
            "                    if e.expected:",
            "                        self.to_screen(f'{e}; Re-extracting data')",
            "                    else:",
            "                        self.to_stderr('\\r')",
            "                        self.report_warning(f'{e}; Re-extracting data')",
            "                    continue",
            "                except GeoRestrictedError as e:",
            "                    msg = e.msg",
            "                    if e.countries:",
            "                        msg += '\\nThis video is available in %s.' % ', '.join(",
            "                            map(ISO3166Utils.short2full, e.countries))",
            "                    msg += '\\nYou might want to use a VPN or a proxy server (with --proxy) to workaround.'",
            "                    self.report_error(msg)",
            "                except ExtractorError as e:  # An error we somewhat expected",
            "                    self.report_error(str(e), e.format_traceback())",
            "                except Exception as e:",
            "                    if self.params.get('ignoreerrors'):",
            "                        self.report_error(str(e), tb=encode_compat_str(traceback.format_exc()))",
            "                    else:",
            "                        raise",
            "                break",
            "        return wrapper",
            "",
            "    def _wait_for_video(self, ie_result={}):",
            "        if (not self.params.get('wait_for_video')",
            "                or ie_result.get('_type', 'video') != 'video'",
            "                or ie_result.get('formats') or ie_result.get('url')):",
            "            return",
            "",
            "        format_dur = lambda dur: '%02d:%02d:%02d' % timetuple_from_msec(dur * 1000)[:-1]",
            "        last_msg = ''",
            "",
            "        def progress(msg):",
            "            nonlocal last_msg",
            "            full_msg = f'{msg}\\n'",
            "            if not self.params.get('noprogress'):",
            "                full_msg = msg + ' ' * (len(last_msg) - len(msg)) + '\\r'",
            "            elif last_msg:",
            "                return",
            "            self.to_screen(full_msg, skip_eol=True)",
            "            last_msg = msg",
            "",
            "        min_wait, max_wait = self.params.get('wait_for_video')",
            "        diff = try_get(ie_result, lambda x: x['release_timestamp'] - time.time())",
            "        if diff is None and ie_result.get('live_status') == 'is_upcoming':",
            "            diff = round(random.uniform(min_wait, max_wait) if (max_wait and min_wait) else (max_wait or min_wait), 0)",
            "            self.report_warning('Release time of video is not known')",
            "        elif ie_result and (diff or 0) <= 0:",
            "            self.report_warning('Video should already be available according to extracted info')",
            "        diff = min(max(diff or 0, min_wait or 0), max_wait or float('inf'))",
            "        self.to_screen(f'[wait] Waiting for {format_dur(diff)} - Press Ctrl+C to try now')",
            "",
            "        wait_till = time.time() + diff",
            "        try:",
            "            while True:",
            "                diff = wait_till - time.time()",
            "                if diff <= 0:",
            "                    progress('')",
            "                    raise ReExtractInfo('[wait] Wait period ended', expected=True)",
            "                progress(f'[wait] Remaining time until next attempt: {self._format_screen(format_dur(diff), self.Styles.EMPHASIS)}')",
            "                time.sleep(1)",
            "        except KeyboardInterrupt:",
            "            progress('')",
            "            raise ReExtractInfo('[wait] Interrupted by user', expected=True)",
            "        except BaseException as e:",
            "            if not isinstance(e, ReExtractInfo):",
            "                self.to_screen('')",
            "            raise",
            "",
            "    def _load_cookies(self, data, *, autoscope=True):",
            "        \"\"\"Loads cookies from a `Cookie` header",
            "",
            "        This tries to work around the security vulnerability of passing cookies to every domain.",
            "        See: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj",
            "",
            "        @param data         The Cookie header as string to load the cookies from",
            "        @param autoscope    If `False`, scope cookies using Set-Cookie syntax and error for cookie without domains",
            "                            If `True`, save cookies for later to be stored in the jar with a limited scope",
            "                            If a URL, save cookies in the jar with the domain of the URL",
            "        \"\"\"",
            "        for cookie in LenientSimpleCookie(data).values():",
            "            if autoscope and any(cookie.values()):",
            "                raise ValueError('Invalid syntax in Cookie Header')",
            "",
            "            domain = cookie.get('domain') or ''",
            "            expiry = cookie.get('expires')",
            "            if expiry == '':  # 0 is valid",
            "                expiry = None",
            "            prepared_cookie = http.cookiejar.Cookie(",
            "                cookie.get('version') or 0, cookie.key, cookie.value, None, False,",
            "                domain, True, True, cookie.get('path') or '', bool(cookie.get('path')),",
            "                cookie.get('secure') or False, expiry, False, None, None, {})",
            "",
            "            if domain:",
            "                self.cookiejar.set_cookie(prepared_cookie)",
            "            elif autoscope is True:",
            "                self.deprecated_feature(",
            "                    'Passing cookies as a header is a potential security risk; '",
            "                    'they will be scoped to the domain of the downloaded urls. '",
            "                    'Please consider loading cookies from a file or browser instead.')",
            "                self.__header_cookies.append(prepared_cookie)",
            "            elif autoscope:",
            "                self.report_warning(",
            "                    'The extractor result contains an unscoped cookie as an HTTP header. '",
            "                    f'If you are using yt-dlp with an input URL{bug_reports_message(before=\",\")}',",
            "                    only_once=True)",
            "                self._apply_header_cookies(autoscope, [prepared_cookie])",
            "            else:",
            "                self.report_error('Unscoped cookies are not allowed; please specify some sort of scoping',",
            "                                  tb=False, is_error=False)",
            "",
            "    def _apply_header_cookies(self, url, cookies=None):",
            "        \"\"\"Applies stray header cookies to the provided url",
            "",
            "        This loads header cookies and scopes them to the domain provided in `url`.",
            "        While this is not ideal, it helps reduce the risk of them being sent",
            "        to an unintended destination while mostly maintaining compatibility.",
            "        \"\"\"",
            "        parsed = urllib.parse.urlparse(url)",
            "        if not parsed.hostname:",
            "            return",
            "",
            "        for cookie in map(copy.copy, cookies or self.__header_cookies):",
            "            cookie.domain = f'.{parsed.hostname}'",
            "            self.cookiejar.set_cookie(cookie)",
            "",
            "    @_handle_extraction_exceptions",
            "    def __extract_info(self, url, ie, download, extra_info, process):",
            "        self._apply_header_cookies(url)",
            "",
            "        try:",
            "            ie_result = ie.extract(url)",
            "        except UserNotLive as e:",
            "            if process:",
            "                if self.params.get('wait_for_video'):",
            "                    self.report_warning(e)",
            "                self._wait_for_video()",
            "            raise",
            "        if ie_result is None:  # Finished already (backwards compatibility; listformats and friends should be moved here)",
            "            self.report_warning(f'Extractor {ie.IE_NAME} returned nothing{bug_reports_message()}')",
            "            return",
            "        if isinstance(ie_result, list):",
            "            # Backwards compatibility: old IE result format",
            "            ie_result = {",
            "                '_type': 'compat_list',",
            "                'entries': ie_result,",
            "            }",
            "        if extra_info.get('original_url'):",
            "            ie_result.setdefault('original_url', extra_info['original_url'])",
            "        self.add_default_extra_info(ie_result, ie, url)",
            "        if process:",
            "            self._wait_for_video(ie_result)",
            "            return self.process_ie_result(ie_result, download, extra_info)",
            "        else:",
            "            return ie_result",
            "",
            "    def add_default_extra_info(self, ie_result, ie, url):",
            "        if url is not None:",
            "            self.add_extra_info(ie_result, {",
            "                'webpage_url': url,",
            "                'original_url': url,",
            "            })",
            "        webpage_url = ie_result.get('webpage_url')",
            "        if webpage_url:",
            "            self.add_extra_info(ie_result, {",
            "                'webpage_url_basename': url_basename(webpage_url),",
            "                'webpage_url_domain': get_domain(webpage_url),",
            "            })",
            "        if ie is not None:",
            "            self.add_extra_info(ie_result, {",
            "                'extractor': ie.IE_NAME,",
            "                'extractor_key': ie.ie_key(),",
            "            })",
            "",
            "    def process_ie_result(self, ie_result, download=True, extra_info=None):",
            "        \"\"\"",
            "        Take the result of the ie(may be modified) and resolve all unresolved",
            "        references (URLs, playlist items).",
            "",
            "        It will also download the videos if 'download'.",
            "        Returns the resolved ie_result.",
            "        \"\"\"",
            "        if extra_info is None:",
            "            extra_info = {}",
            "        result_type = ie_result.get('_type', 'video')",
            "",
            "        if result_type in ('url', 'url_transparent'):",
            "            ie_result['url'] = sanitize_url(",
            "                ie_result['url'], scheme='http' if self.params.get('prefer_insecure') else 'https')",
            "            if ie_result.get('original_url') and not extra_info.get('original_url'):",
            "                extra_info = {'original_url': ie_result['original_url'], **extra_info}",
            "",
            "            extract_flat = self.params.get('extract_flat', False)",
            "            if ((extract_flat == 'in_playlist' and 'playlist' in extra_info)",
            "                    or extract_flat is True):",
            "                info_copy = ie_result.copy()",
            "                ie = try_get(ie_result.get('ie_key'), self.get_info_extractor)",
            "                if ie and not ie_result.get('id'):",
            "                    info_copy['id'] = ie.get_temp_id(ie_result['url'])",
            "                self.add_default_extra_info(info_copy, ie, ie_result['url'])",
            "                self.add_extra_info(info_copy, extra_info)",
            "                info_copy, _ = self.pre_process(info_copy)",
            "                self._fill_common_fields(info_copy, False)",
            "                self.__forced_printings(info_copy)",
            "                self._raise_pending_errors(info_copy)",
            "                if self.params.get('force_write_download_archive', False):",
            "                    self.record_download_archive(info_copy)",
            "                return ie_result",
            "",
            "        if result_type == 'video':",
            "            self.add_extra_info(ie_result, extra_info)",
            "            ie_result = self.process_video_result(ie_result, download=download)",
            "            self._raise_pending_errors(ie_result)",
            "            additional_urls = (ie_result or {}).get('additional_urls')",
            "            if additional_urls:",
            "                # TODO: Improve MetadataParserPP to allow setting a list",
            "                if isinstance(additional_urls, str):",
            "                    additional_urls = [additional_urls]",
            "                self.to_screen(",
            "                    '[info] %s: %d additional URL(s) requested' % (ie_result['id'], len(additional_urls)))",
            "                self.write_debug('Additional URLs: \"%s\"' % '\", \"'.join(additional_urls))",
            "                ie_result['additional_entries'] = [",
            "                    self.extract_info(",
            "                        url, download, extra_info=extra_info,",
            "                        force_generic_extractor=self.params.get('force_generic_extractor'))",
            "                    for url in additional_urls",
            "                ]",
            "            return ie_result",
            "        elif result_type == 'url':",
            "            # We have to add extra_info to the results because it may be",
            "            # contained in a playlist",
            "            return self.extract_info(",
            "                ie_result['url'], download,",
            "                ie_key=ie_result.get('ie_key'),",
            "                extra_info=extra_info)",
            "        elif result_type == 'url_transparent':",
            "            # Use the information from the embedding page",
            "            info = self.extract_info(",
            "                ie_result['url'], ie_key=ie_result.get('ie_key'),",
            "                extra_info=extra_info, download=False, process=False)",
            "",
            "            # extract_info may return None when ignoreerrors is enabled and",
            "            # extraction failed with an error, don't crash and return early",
            "            # in this case",
            "            if not info:",
            "                return info",
            "",
            "            exempted_fields = {'_type', 'url', 'ie_key'}",
            "            if not ie_result.get('section_end') and ie_result.get('section_start') is None:",
            "                # For video clips, the id etc of the clip extractor should be used",
            "                exempted_fields |= {'id', 'extractor', 'extractor_key'}",
            "",
            "            new_result = info.copy()",
            "            new_result.update(filter_dict(ie_result, lambda k, v: v is not None and k not in exempted_fields))",
            "",
            "            # Extracted info may not be a video result (i.e.",
            "            # info.get('_type', 'video') != video) but rather an url or",
            "            # url_transparent. In such cases outer metadata (from ie_result)",
            "            # should be propagated to inner one (info). For this to happen",
            "            # _type of info should be overridden with url_transparent. This",
            "            # fixes issue from https://github.com/ytdl-org/youtube-dl/pull/11163.",
            "            if new_result.get('_type') == 'url':",
            "                new_result['_type'] = 'url_transparent'",
            "",
            "            return self.process_ie_result(",
            "                new_result, download=download, extra_info=extra_info)",
            "        elif result_type in ('playlist', 'multi_video'):",
            "            # Protect from infinite recursion due to recursively nested playlists",
            "            # (see https://github.com/ytdl-org/youtube-dl/issues/27833)",
            "            webpage_url = ie_result.get('webpage_url')  # Playlists maynot have webpage_url",
            "            if webpage_url and webpage_url in self._playlist_urls:",
            "                self.to_screen(",
            "                    '[download] Skipping already downloaded playlist: %s'",
            "                    % ie_result.get('title') or ie_result.get('id'))",
            "                return",
            "",
            "            self._playlist_level += 1",
            "            self._playlist_urls.add(webpage_url)",
            "            self._fill_common_fields(ie_result, False)",
            "            self._sanitize_thumbnails(ie_result)",
            "            try:",
            "                return self.__process_playlist(ie_result, download)",
            "            finally:",
            "                self._playlist_level -= 1",
            "                if not self._playlist_level:",
            "                    self._playlist_urls.clear()",
            "        elif result_type == 'compat_list':",
            "            self.report_warning(",
            "                'Extractor %s returned a compat_list result. '",
            "                'It needs to be updated.' % ie_result.get('extractor'))",
            "",
            "            def _fixup(r):",
            "                self.add_extra_info(r, {",
            "                    'extractor': ie_result['extractor'],",
            "                    'webpage_url': ie_result['webpage_url'],",
            "                    'webpage_url_basename': url_basename(ie_result['webpage_url']),",
            "                    'webpage_url_domain': get_domain(ie_result['webpage_url']),",
            "                    'extractor_key': ie_result['extractor_key'],",
            "                })",
            "                return r",
            "            ie_result['entries'] = [",
            "                self.process_ie_result(_fixup(r), download, extra_info)",
            "                for r in ie_result['entries']",
            "            ]",
            "            return ie_result",
            "        else:",
            "            raise Exception('Invalid result type: %s' % result_type)",
            "",
            "    def _ensure_dir_exists(self, path):",
            "        return make_dir(path, self.report_error)",
            "",
            "    @staticmethod",
            "    def _playlist_infodict(ie_result, strict=False, **kwargs):",
            "        info = {",
            "            'playlist_count': ie_result.get('playlist_count'),",
            "            'playlist': ie_result.get('title') or ie_result.get('id'),",
            "            'playlist_id': ie_result.get('id'),",
            "            'playlist_title': ie_result.get('title'),",
            "            'playlist_uploader': ie_result.get('uploader'),",
            "            'playlist_uploader_id': ie_result.get('uploader_id'),",
            "            **kwargs,",
            "        }",
            "        if strict:",
            "            return info",
            "        if ie_result.get('webpage_url'):",
            "            info.update({",
            "                'webpage_url': ie_result['webpage_url'],",
            "                'webpage_url_basename': url_basename(ie_result['webpage_url']),",
            "                'webpage_url_domain': get_domain(ie_result['webpage_url']),",
            "            })",
            "        return {",
            "            **info,",
            "            'playlist_index': 0,",
            "            '__last_playlist_index': max(ie_result.get('requested_entries') or (0, 0)),",
            "            'extractor': ie_result['extractor'],",
            "            'extractor_key': ie_result['extractor_key'],",
            "        }",
            "",
            "    def __process_playlist(self, ie_result, download):",
            "        \"\"\"Process each entry in the playlist\"\"\"",
            "        assert ie_result['_type'] in ('playlist', 'multi_video')",
            "",
            "        common_info = self._playlist_infodict(ie_result, strict=True)",
            "        title = common_info.get('playlist') or '<Untitled>'",
            "        if self._match_entry(common_info, incomplete=True) is not None:",
            "            return",
            "        self.to_screen(f'[download] Downloading {ie_result[\"_type\"]}: {title}')",
            "",
            "        all_entries = PlaylistEntries(self, ie_result)",
            "        entries = orderedSet(all_entries.get_requested_items(), lazy=True)",
            "",
            "        lazy = self.params.get('lazy_playlist')",
            "        if lazy:",
            "            resolved_entries, n_entries = [], 'N/A'",
            "            ie_result['requested_entries'], ie_result['entries'] = None, None",
            "        else:",
            "            entries = resolved_entries = list(entries)",
            "            n_entries = len(resolved_entries)",
            "            ie_result['requested_entries'], ie_result['entries'] = tuple(zip(*resolved_entries)) or ([], [])",
            "        if not ie_result.get('playlist_count'):",
            "            # Better to do this after potentially exhausting entries",
            "            ie_result['playlist_count'] = all_entries.get_full_count()",
            "",
            "        extra = self._playlist_infodict(ie_result, n_entries=int_or_none(n_entries))",
            "        ie_copy = collections.ChainMap(ie_result, extra)",
            "",
            "        _infojson_written = False",
            "        write_playlist_files = self.params.get('allow_playlist_files', True)",
            "        if write_playlist_files and self.params.get('list_thumbnails'):",
            "            self.list_thumbnails(ie_result)",
            "        if write_playlist_files and not self.params.get('simulate'):",
            "            _infojson_written = self._write_info_json(",
            "                'playlist', ie_result, self.prepare_filename(ie_copy, 'pl_infojson'))",
            "            if _infojson_written is None:",
            "                return",
            "            if self._write_description('playlist', ie_result,",
            "                                       self.prepare_filename(ie_copy, 'pl_description')) is None:",
            "                return",
            "            # TODO: This should be passed to ThumbnailsConvertor if necessary",
            "            self._write_thumbnails('playlist', ie_result, self.prepare_filename(ie_copy, 'pl_thumbnail'))",
            "",
            "        if lazy:",
            "            if self.params.get('playlistreverse') or self.params.get('playlistrandom'):",
            "                self.report_warning('playlistreverse and playlistrandom are not supported with lazy_playlist', only_once=True)",
            "        elif self.params.get('playlistreverse'):",
            "            entries.reverse()",
            "        elif self.params.get('playlistrandom'):",
            "            random.shuffle(entries)",
            "",
            "        self.to_screen(f'[{ie_result[\"extractor\"]}] Playlist {title}: Downloading {n_entries} items'",
            "                       f'{format_field(ie_result, \"playlist_count\", \" of %s\")}')",
            "",
            "        keep_resolved_entries = self.params.get('extract_flat') != 'discard'",
            "        if self.params.get('extract_flat') == 'discard_in_playlist':",
            "            keep_resolved_entries = ie_result['_type'] != 'playlist'",
            "        if keep_resolved_entries:",
            "            self.write_debug('The information of all playlist entries will be held in memory')",
            "",
            "        failures = 0",
            "        max_failures = self.params.get('skip_playlist_after_errors') or float('inf')",
            "        for i, (playlist_index, entry) in enumerate(entries):",
            "            if lazy:",
            "                resolved_entries.append((playlist_index, entry))",
            "            if not entry:",
            "                continue",
            "",
            "            entry['__x_forwarded_for_ip'] = ie_result.get('__x_forwarded_for_ip')",
            "            if not lazy and 'playlist-index' in self.params['compat_opts']:",
            "                playlist_index = ie_result['requested_entries'][i]",
            "",
            "            entry_copy = collections.ChainMap(entry, {",
            "                **common_info,",
            "                'n_entries': int_or_none(n_entries),",
            "                'playlist_index': playlist_index,",
            "                'playlist_autonumber': i + 1,",
            "            })",
            "",
            "            if self._match_entry(entry_copy, incomplete=True) is not None:",
            "                # For compatabilty with youtube-dl. See https://github.com/yt-dlp/yt-dlp/issues/4369",
            "                resolved_entries[i] = (playlist_index, NO_DEFAULT)",
            "                continue",
            "",
            "            self.to_screen('[download] Downloading item %s of %s' % (",
            "                self._format_screen(i + 1, self.Styles.ID), self._format_screen(n_entries, self.Styles.EMPHASIS)))",
            "",
            "            entry_result = self.__process_iterable_entry(entry, download, collections.ChainMap({",
            "                'playlist_index': playlist_index,",
            "                'playlist_autonumber': i + 1,",
            "            }, extra))",
            "            if not entry_result:",
            "                failures += 1",
            "            if failures >= max_failures:",
            "                self.report_error(",
            "                    f'Skipping the remaining entries in playlist \"{title}\" since {failures} items failed extraction')",
            "                break",
            "            if keep_resolved_entries:",
            "                resolved_entries[i] = (playlist_index, entry_result)",
            "",
            "        # Update with processed data",
            "        ie_result['entries'] = [e for _, e in resolved_entries if e is not NO_DEFAULT]",
            "        ie_result['requested_entries'] = [i for i, e in resolved_entries if e is not NO_DEFAULT]",
            "        if ie_result['requested_entries'] == try_call(lambda: list(range(1, ie_result['playlist_count'] + 1))):",
            "            # Do not set for full playlist",
            "            ie_result.pop('requested_entries')",
            "",
            "        # Write the updated info to json",
            "        if _infojson_written is True and self._write_info_json(",
            "                'updated playlist', ie_result,",
            "                self.prepare_filename(ie_copy, 'pl_infojson'), overwrite=True) is None:",
            "            return",
            "",
            "        ie_result = self.run_all_pps('playlist', ie_result)",
            "        self.to_screen(f'[download] Finished downloading playlist: {title}')",
            "        return ie_result",
            "",
            "    @_handle_extraction_exceptions",
            "    def __process_iterable_entry(self, entry, download, extra_info):",
            "        return self.process_ie_result(",
            "            entry, download=download, extra_info=extra_info)",
            "",
            "    def _build_format_filter(self, filter_spec):",
            "        \" Returns a function to filter the formats according to the filter_spec \"",
            "",
            "        OPERATORS = {",
            "            '<': operator.lt,",
            "            '<=': operator.le,",
            "            '>': operator.gt,",
            "            '>=': operator.ge,",
            "            '=': operator.eq,",
            "            '!=': operator.ne,",
            "        }",
            "        operator_rex = re.compile(r'''(?x)\\s*",
            "            (?P<key>[\\w.-]+)\\s*",
            "            (?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*",
            "            (?P<value>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)\\s*",
            "            ''' % '|'.join(map(re.escape, OPERATORS.keys())))",
            "        m = operator_rex.fullmatch(filter_spec)",
            "        if m:",
            "            try:",
            "                comparison_value = int(m.group('value'))",
            "            except ValueError:",
            "                comparison_value = parse_filesize(m.group('value'))",
            "                if comparison_value is None:",
            "                    comparison_value = parse_filesize(m.group('value') + 'B')",
            "                if comparison_value is None:",
            "                    raise ValueError(",
            "                        'Invalid value %r in format specification %r' % (",
            "                            m.group('value'), filter_spec))",
            "            op = OPERATORS[m.group('op')]",
            "",
            "        if not m:",
            "            STR_OPERATORS = {",
            "                '=': operator.eq,",
            "                '^=': lambda attr, value: attr.startswith(value),",
            "                '$=': lambda attr, value: attr.endswith(value),",
            "                '*=': lambda attr, value: value in attr,",
            "                '~=': lambda attr, value: value.search(attr) is not None",
            "            }",
            "            str_operator_rex = re.compile(r'''(?x)\\s*",
            "                (?P<key>[a-zA-Z0-9._-]+)\\s*",
            "                (?P<negation>!\\s*)?(?P<op>%s)\\s*(?P<none_inclusive>\\?\\s*)?",
            "                (?P<quote>[\"'])?",
            "                (?P<value>(?(quote)(?:(?!(?P=quote))[^\\\\]|\\\\.)+|[\\w.-]+))",
            "                (?(quote)(?P=quote))\\s*",
            "                ''' % '|'.join(map(re.escape, STR_OPERATORS.keys())))",
            "            m = str_operator_rex.fullmatch(filter_spec)",
            "            if m:",
            "                if m.group('op') == '~=':",
            "                    comparison_value = re.compile(m.group('value'))",
            "                else:",
            "                    comparison_value = re.sub(r'''\\\\([\\\\\"'])''', r'\\1', m.group('value'))",
            "                str_op = STR_OPERATORS[m.group('op')]",
            "                if m.group('negation'):",
            "                    op = lambda attr, value: not str_op(attr, value)",
            "                else:",
            "                    op = str_op",
            "",
            "        if not m:",
            "            raise SyntaxError('Invalid filter specification %r' % filter_spec)",
            "",
            "        def _filter(f):",
            "            actual_value = f.get(m.group('key'))",
            "            if actual_value is None:",
            "                return m.group('none_inclusive')",
            "            return op(actual_value, comparison_value)",
            "        return _filter",
            "",
            "    def _check_formats(self, formats):",
            "        for f in formats:",
            "            self.to_screen('[info] Testing format %s' % f['format_id'])",
            "            path = self.get_output_path('temp')",
            "            if not self._ensure_dir_exists(f'{path}/'):",
            "                continue",
            "            temp_file = tempfile.NamedTemporaryFile(suffix='.tmp', delete=False, dir=path or None)",
            "            temp_file.close()",
            "            try:",
            "                success, _ = self.dl(temp_file.name, f, test=True)",
            "            except (DownloadError, OSError, ValueError) + network_exceptions:",
            "                success = False",
            "            finally:",
            "                if os.path.exists(temp_file.name):",
            "                    try:",
            "                        os.remove(temp_file.name)",
            "                    except OSError:",
            "                        self.report_warning('Unable to delete temporary file \"%s\"' % temp_file.name)",
            "            if success:",
            "                yield f",
            "            else:",
            "                self.to_screen('[info] Unable to download format %s. Skipping...' % f['format_id'])",
            "",
            "    def _default_format_spec(self, info_dict, download=True):",
            "",
            "        def can_merge():",
            "            merger = FFmpegMergerPP(self)",
            "            return merger.available and merger.can_merge()",
            "",
            "        prefer_best = (",
            "            not self.params.get('simulate')",
            "            and download",
            "            and (",
            "                not can_merge()",
            "                or info_dict.get('is_live') and not self.params.get('live_from_start')",
            "                or self.params['outtmpl']['default'] == '-'))",
            "        compat = (",
            "            prefer_best",
            "            or self.params.get('allow_multiple_audio_streams', False)",
            "            or 'format-spec' in self.params['compat_opts'])",
            "",
            "        return (",
            "            'best/bestvideo+bestaudio' if prefer_best",
            "            else 'bestvideo*+bestaudio/best' if not compat",
            "            else 'bestvideo+bestaudio/best')",
            "",
            "    def build_format_selector(self, format_spec):",
            "        def syntax_error(note, start):",
            "            message = (",
            "                'Invalid format specification: '",
            "                '{}\\n\\t{}\\n\\t{}^'.format(note, format_spec, ' ' * start[1]))",
            "            return SyntaxError(message)",
            "",
            "        PICKFIRST = 'PICKFIRST'",
            "        MERGE = 'MERGE'",
            "        SINGLE = 'SINGLE'",
            "        GROUP = 'GROUP'",
            "        FormatSelector = collections.namedtuple('FormatSelector', ['type', 'selector', 'filters'])",
            "",
            "        allow_multiple_streams = {'audio': self.params.get('allow_multiple_audio_streams', False),",
            "                                  'video': self.params.get('allow_multiple_video_streams', False)}",
            "",
            "        def _parse_filter(tokens):",
            "            filter_parts = []",
            "            for type, string_, start, _, _ in tokens:",
            "                if type == tokenize.OP and string_ == ']':",
            "                    return ''.join(filter_parts)",
            "                else:",
            "                    filter_parts.append(string_)",
            "",
            "        def _remove_unused_ops(tokens):",
            "            # Remove operators that we don't use and join them with the surrounding strings.",
            "            # E.g. 'mp4' '-' 'baseline' '-' '16x9' is converted to 'mp4-baseline-16x9'",
            "            ALLOWED_OPS = ('/', '+', ',', '(', ')')",
            "            last_string, last_start, last_end, last_line = None, None, None, None",
            "            for type, string_, start, end, line in tokens:",
            "                if type == tokenize.OP and string_ == '[':",
            "                    if last_string:",
            "                        yield tokenize.NAME, last_string, last_start, last_end, last_line",
            "                        last_string = None",
            "                    yield type, string_, start, end, line",
            "                    # everything inside brackets will be handled by _parse_filter",
            "                    for type, string_, start, end, line in tokens:",
            "                        yield type, string_, start, end, line",
            "                        if type == tokenize.OP and string_ == ']':",
            "                            break",
            "                elif type == tokenize.OP and string_ in ALLOWED_OPS:",
            "                    if last_string:",
            "                        yield tokenize.NAME, last_string, last_start, last_end, last_line",
            "                        last_string = None",
            "                    yield type, string_, start, end, line",
            "                elif type in [tokenize.NAME, tokenize.NUMBER, tokenize.OP]:",
            "                    if not last_string:",
            "                        last_string = string_",
            "                        last_start = start",
            "                        last_end = end",
            "                    else:",
            "                        last_string += string_",
            "            if last_string:",
            "                yield tokenize.NAME, last_string, last_start, last_end, last_line",
            "",
            "        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):",
            "            selectors = []",
            "            current_selector = None",
            "            for type, string_, start, _, _ in tokens:",
            "                # ENCODING is only defined in Python 3.x",
            "                if type == getattr(tokenize, 'ENCODING', None):",
            "                    continue",
            "                elif type in [tokenize.NAME, tokenize.NUMBER]:",
            "                    current_selector = FormatSelector(SINGLE, string_, [])",
            "                elif type == tokenize.OP:",
            "                    if string_ == ')':",
            "                        if not inside_group:",
            "                            # ')' will be handled by the parentheses group",
            "                            tokens.restore_last_token()",
            "                        break",
            "                    elif inside_merge and string_ in ['/', ',']:",
            "                        tokens.restore_last_token()",
            "                        break",
            "                    elif inside_choice and string_ == ',':",
            "                        tokens.restore_last_token()",
            "                        break",
            "                    elif string_ == ',':",
            "                        if not current_selector:",
            "                            raise syntax_error('\",\" must follow a format selector', start)",
            "                        selectors.append(current_selector)",
            "                        current_selector = None",
            "                    elif string_ == '/':",
            "                        if not current_selector:",
            "                            raise syntax_error('\"/\" must follow a format selector', start)",
            "                        first_choice = current_selector",
            "                        second_choice = _parse_format_selection(tokens, inside_choice=True)",
            "                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])",
            "                    elif string_ == '[':",
            "                        if not current_selector:",
            "                            current_selector = FormatSelector(SINGLE, 'best', [])",
            "                        format_filter = _parse_filter(tokens)",
            "                        current_selector.filters.append(format_filter)",
            "                    elif string_ == '(':",
            "                        if current_selector:",
            "                            raise syntax_error('Unexpected \"(\"', start)",
            "                        group = _parse_format_selection(tokens, inside_group=True)",
            "                        current_selector = FormatSelector(GROUP, group, [])",
            "                    elif string_ == '+':",
            "                        if not current_selector:",
            "                            raise syntax_error('Unexpected \"+\"', start)",
            "                        selector_1 = current_selector",
            "                        selector_2 = _parse_format_selection(tokens, inside_merge=True)",
            "                        if not selector_2:",
            "                            raise syntax_error('Expected a selector', start)",
            "                        current_selector = FormatSelector(MERGE, (selector_1, selector_2), [])",
            "                    else:",
            "                        raise syntax_error(f'Operator not recognized: \"{string_}\"', start)",
            "                elif type == tokenize.ENDMARKER:",
            "                    break",
            "            if current_selector:",
            "                selectors.append(current_selector)",
            "            return selectors",
            "",
            "        def _merge(formats_pair):",
            "            format_1, format_2 = formats_pair",
            "",
            "            formats_info = []",
            "            formats_info.extend(format_1.get('requested_formats', (format_1,)))",
            "            formats_info.extend(format_2.get('requested_formats', (format_2,)))",
            "",
            "            if not allow_multiple_streams['video'] or not allow_multiple_streams['audio']:",
            "                get_no_more = {'video': False, 'audio': False}",
            "                for (i, fmt_info) in enumerate(formats_info):",
            "                    if fmt_info.get('acodec') == fmt_info.get('vcodec') == 'none':",
            "                        formats_info.pop(i)",
            "                        continue",
            "                    for aud_vid in ['audio', 'video']:",
            "                        if not allow_multiple_streams[aud_vid] and fmt_info.get(aud_vid[0] + 'codec') != 'none':",
            "                            if get_no_more[aud_vid]:",
            "                                formats_info.pop(i)",
            "                                break",
            "                            get_no_more[aud_vid] = True",
            "",
            "            if len(formats_info) == 1:",
            "                return formats_info[0]",
            "",
            "            video_fmts = [fmt_info for fmt_info in formats_info if fmt_info.get('vcodec') != 'none']",
            "            audio_fmts = [fmt_info for fmt_info in formats_info if fmt_info.get('acodec') != 'none']",
            "",
            "            the_only_video = video_fmts[0] if len(video_fmts) == 1 else None",
            "            the_only_audio = audio_fmts[0] if len(audio_fmts) == 1 else None",
            "",
            "            output_ext = get_compatible_ext(",
            "                vcodecs=[f.get('vcodec') for f in video_fmts],",
            "                acodecs=[f.get('acodec') for f in audio_fmts],",
            "                vexts=[f['ext'] for f in video_fmts],",
            "                aexts=[f['ext'] for f in audio_fmts],",
            "                preferences=(try_call(lambda: self.params['merge_output_format'].split('/'))",
            "                             or self.params.get('prefer_free_formats') and ('webm', 'mkv')))",
            "",
            "            filtered = lambda *keys: filter(None, (traverse_obj(fmt, *keys) for fmt in formats_info))",
            "",
            "            new_dict = {",
            "                'requested_formats': formats_info,",
            "                'format': '+'.join(filtered('format')),",
            "                'format_id': '+'.join(filtered('format_id')),",
            "                'ext': output_ext,",
            "                'protocol': '+'.join(map(determine_protocol, formats_info)),",
            "                'language': '+'.join(orderedSet(filtered('language'))) or None,",
            "                'format_note': '+'.join(orderedSet(filtered('format_note'))) or None,",
            "                'filesize_approx': sum(filtered('filesize', 'filesize_approx')) or None,",
            "                'tbr': sum(filtered('tbr', 'vbr', 'abr')),",
            "            }",
            "",
            "            if the_only_video:",
            "                new_dict.update({",
            "                    'width': the_only_video.get('width'),",
            "                    'height': the_only_video.get('height'),",
            "                    'resolution': the_only_video.get('resolution') or self.format_resolution(the_only_video),",
            "                    'fps': the_only_video.get('fps'),",
            "                    'dynamic_range': the_only_video.get('dynamic_range'),",
            "                    'vcodec': the_only_video.get('vcodec'),",
            "                    'vbr': the_only_video.get('vbr'),",
            "                    'stretched_ratio': the_only_video.get('stretched_ratio'),",
            "                    'aspect_ratio': the_only_video.get('aspect_ratio'),",
            "                })",
            "",
            "            if the_only_audio:",
            "                new_dict.update({",
            "                    'acodec': the_only_audio.get('acodec'),",
            "                    'abr': the_only_audio.get('abr'),",
            "                    'asr': the_only_audio.get('asr'),",
            "                    'audio_channels': the_only_audio.get('audio_channels')",
            "                })",
            "",
            "            return new_dict",
            "",
            "        def _check_formats(formats):",
            "            if self.params.get('check_formats') == 'selected':",
            "                yield from self._check_formats(formats)",
            "                return",
            "            elif (self.params.get('check_formats') is not None",
            "                    or self.params.get('allow_unplayable_formats')):",
            "                yield from formats",
            "                return",
            "",
            "            for f in formats:",
            "                if f.get('has_drm') or f.get('__needs_testing'):",
            "                    yield from self._check_formats([f])",
            "                else:",
            "                    yield f",
            "",
            "        def _build_selector_function(selector):",
            "            if isinstance(selector, list):  # ,",
            "                fs = [_build_selector_function(s) for s in selector]",
            "",
            "                def selector_function(ctx):",
            "                    for f in fs:",
            "                        yield from f(ctx)",
            "                return selector_function",
            "",
            "            elif selector.type == GROUP:  # ()",
            "                selector_function = _build_selector_function(selector.selector)",
            "",
            "            elif selector.type == PICKFIRST:  # /",
            "                fs = [_build_selector_function(s) for s in selector.selector]",
            "",
            "                def selector_function(ctx):",
            "                    for f in fs:",
            "                        picked_formats = list(f(ctx))",
            "                        if picked_formats:",
            "                            return picked_formats",
            "                    return []",
            "",
            "            elif selector.type == MERGE:  # +",
            "                selector_1, selector_2 = map(_build_selector_function, selector.selector)",
            "",
            "                def selector_function(ctx):",
            "                    for pair in itertools.product(selector_1(ctx), selector_2(ctx)):",
            "                        yield _merge(pair)",
            "",
            "            elif selector.type == SINGLE:  # atom",
            "                format_spec = selector.selector or 'best'",
            "",
            "                # TODO: Add allvideo, allaudio etc by generalizing the code with best/worst selector",
            "                if format_spec == 'all':",
            "                    def selector_function(ctx):",
            "                        yield from _check_formats(ctx['formats'][::-1])",
            "                elif format_spec == 'mergeall':",
            "                    def selector_function(ctx):",
            "                        formats = list(_check_formats(",
            "                            f for f in ctx['formats'] if f.get('vcodec') != 'none' or f.get('acodec') != 'none'))",
            "                        if not formats:",
            "                            return",
            "                        merged_format = formats[-1]",
            "                        for f in formats[-2::-1]:",
            "                            merged_format = _merge((merged_format, f))",
            "                        yield merged_format",
            "",
            "                else:",
            "                    format_fallback, seperate_fallback, format_reverse, format_idx = False, None, True, 1",
            "                    mobj = re.match(",
            "                        r'(?P<bw>best|worst|b|w)(?P<type>video|audio|v|a)?(?P<mod>\\*)?(?:\\.(?P<n>[1-9]\\d*))?$',",
            "                        format_spec)",
            "                    if mobj is not None:",
            "                        format_idx = int_or_none(mobj.group('n'), default=1)",
            "                        format_reverse = mobj.group('bw')[0] == 'b'",
            "                        format_type = (mobj.group('type') or [None])[0]",
            "                        not_format_type = {'v': 'a', 'a': 'v'}.get(format_type)",
            "                        format_modified = mobj.group('mod') is not None",
            "",
            "                        format_fallback = not format_type and not format_modified  # for b, w",
            "                        _filter_f = (",
            "                            (lambda f: f.get('%scodec' % format_type) != 'none')",
            "                            if format_type and format_modified  # bv*, ba*, wv*, wa*",
            "                            else (lambda f: f.get('%scodec' % not_format_type) == 'none')",
            "                            if format_type  # bv, ba, wv, wa",
            "                            else (lambda f: f.get('vcodec') != 'none' and f.get('acodec') != 'none')",
            "                            if not format_modified  # b, w",
            "                            else lambda f: True)  # b*, w*",
            "                        filter_f = lambda f: _filter_f(f) and (",
            "                            f.get('vcodec') != 'none' or f.get('acodec') != 'none')",
            "                    else:",
            "                        if format_spec in self._format_selection_exts['audio']:",
            "                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') != 'none'",
            "                        elif format_spec in self._format_selection_exts['video']:",
            "                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') != 'none' and f.get('vcodec') != 'none'",
            "                            seperate_fallback = lambda f: f.get('ext') == format_spec and f.get('vcodec') != 'none'",
            "                        elif format_spec in self._format_selection_exts['storyboards']:",
            "                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') == 'none' and f.get('vcodec') == 'none'",
            "                        else:",
            "                            filter_f = lambda f: f.get('format_id') == format_spec  # id",
            "",
            "                    def selector_function(ctx):",
            "                        formats = list(ctx['formats'])",
            "                        matches = list(filter(filter_f, formats)) if filter_f is not None else formats",
            "                        if not matches:",
            "                            if format_fallback and ctx['incomplete_formats']:",
            "                                # for extractors with incomplete formats (audio only (soundcloud)",
            "                                # or video only (imgur)) best/worst will fallback to",
            "                                # best/worst {video,audio}-only format",
            "                                matches = list(filter(lambda f: f.get('vcodec') != 'none' or f.get('acodec') != 'none', formats))",
            "                            elif seperate_fallback and not ctx['has_merged_format']:",
            "                                # for compatibility with youtube-dl when there is no pre-merged format",
            "                                matches = list(filter(seperate_fallback, formats))",
            "                        matches = LazyList(_check_formats(matches[::-1 if format_reverse else 1]))",
            "                        try:",
            "                            yield matches[format_idx - 1]",
            "                        except LazyList.IndexError:",
            "                            return",
            "",
            "            filters = [self._build_format_filter(f) for f in selector.filters]",
            "",
            "            def final_selector(ctx):",
            "                ctx_copy = dict(ctx)",
            "                for _filter in filters:",
            "                    ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))",
            "                return selector_function(ctx_copy)",
            "            return final_selector",
            "",
            "        # HACK: Python 3.12 changed the underlying parser, rendering '7_a' invalid",
            "        #       Prefix numbers with random letters to avoid it being classified as a number",
            "        #       See: https://github.com/yt-dlp/yt-dlp/pulls/8797",
            "        # TODO: Implement parser not reliant on tokenize.tokenize",
            "        prefix = ''.join(random.choices(string.ascii_letters, k=32))",
            "        stream = io.BytesIO(re.sub(r'\\d[_\\d]*', rf'{prefix}\\g<0>', format_spec).encode())",
            "        try:",
            "            tokens = list(_remove_unused_ops(",
            "                token._replace(string=token.string.replace(prefix, ''))",
            "                for token in tokenize.tokenize(stream.readline)))",
            "        except tokenize.TokenError:",
            "            raise syntax_error('Missing closing/opening brackets or parenthesis', (0, len(format_spec)))",
            "",
            "        class TokenIterator:",
            "            def __init__(self, tokens):",
            "                self.tokens = tokens",
            "                self.counter = 0",
            "",
            "            def __iter__(self):",
            "                return self",
            "",
            "            def __next__(self):",
            "                if self.counter >= len(self.tokens):",
            "                    raise StopIteration()",
            "                value = self.tokens[self.counter]",
            "                self.counter += 1",
            "                return value",
            "",
            "            next = __next__",
            "",
            "            def restore_last_token(self):",
            "                self.counter -= 1",
            "",
            "        parsed_selector = _parse_format_selection(iter(TokenIterator(tokens)))",
            "        return _build_selector_function(parsed_selector)",
            "",
            "    def _calc_headers(self, info_dict, load_cookies=False):",
            "        res = HTTPHeaderDict(self.params['http_headers'], info_dict.get('http_headers'))",
            "        clean_headers(res)",
            "",
            "        if load_cookies:  # For --load-info-json",
            "            self._load_cookies(res.get('Cookie'), autoscope=info_dict['url'])  # compat",
            "            self._load_cookies(info_dict.get('cookies'), autoscope=False)",
            "        # The `Cookie` header is removed to prevent leaks and unscoped cookies.",
            "        # See: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj",
            "        res.pop('Cookie', None)",
            "        cookies = self.cookiejar.get_cookies_for_url(info_dict['url'])",
            "        if cookies:",
            "            encoder = LenientSimpleCookie()",
            "            values = []",
            "            for cookie in cookies:",
            "                _, value = encoder.value_encode(cookie.value)",
            "                values.append(f'{cookie.name}={value}')",
            "                if cookie.domain:",
            "                    values.append(f'Domain={cookie.domain}')",
            "                if cookie.path:",
            "                    values.append(f'Path={cookie.path}')",
            "                if cookie.secure:",
            "                    values.append('Secure')",
            "                if cookie.expires:",
            "                    values.append(f'Expires={cookie.expires}')",
            "                if cookie.version:",
            "                    values.append(f'Version={cookie.version}')",
            "            info_dict['cookies'] = '; '.join(values)",
            "",
            "        if 'X-Forwarded-For' not in res:",
            "            x_forwarded_for_ip = info_dict.get('__x_forwarded_for_ip')",
            "            if x_forwarded_for_ip:",
            "                res['X-Forwarded-For'] = x_forwarded_for_ip",
            "",
            "        return res",
            "",
            "    def _calc_cookies(self, url):",
            "        self.deprecation_warning('\"YoutubeDL._calc_cookies\" is deprecated and may be removed in a future version')",
            "        return self.cookiejar.get_cookie_header(url)",
            "",
            "    def _sort_thumbnails(self, thumbnails):",
            "        thumbnails.sort(key=lambda t: (",
            "            t.get('preference') if t.get('preference') is not None else -1,",
            "            t.get('width') if t.get('width') is not None else -1,",
            "            t.get('height') if t.get('height') is not None else -1,",
            "            t.get('id') if t.get('id') is not None else '',",
            "            t.get('url')))",
            "",
            "    def _sanitize_thumbnails(self, info_dict):",
            "        thumbnails = info_dict.get('thumbnails')",
            "        if thumbnails is None:",
            "            thumbnail = info_dict.get('thumbnail')",
            "            if thumbnail:",
            "                info_dict['thumbnails'] = thumbnails = [{'url': thumbnail}]",
            "        if not thumbnails:",
            "            return",
            "",
            "        def check_thumbnails(thumbnails):",
            "            for t in thumbnails:",
            "                self.to_screen(f'[info] Testing thumbnail {t[\"id\"]}')",
            "                try:",
            "                    self.urlopen(HEADRequest(t['url']))",
            "                except network_exceptions as err:",
            "                    self.to_screen(f'[info] Unable to connect to thumbnail {t[\"id\"]} URL {t[\"url\"]!r} - {err}. Skipping...')",
            "                    continue",
            "                yield t",
            "",
            "        self._sort_thumbnails(thumbnails)",
            "        for i, t in enumerate(thumbnails):",
            "            if t.get('id') is None:",
            "                t['id'] = '%d' % i",
            "            if t.get('width') and t.get('height'):",
            "                t['resolution'] = '%dx%d' % (t['width'], t['height'])",
            "            t['url'] = sanitize_url(t['url'])",
            "",
            "        if self.params.get('check_formats') is True:",
            "            info_dict['thumbnails'] = LazyList(check_thumbnails(thumbnails[::-1]), reverse=True)",
            "        else:",
            "            info_dict['thumbnails'] = thumbnails",
            "",
            "    def _fill_common_fields(self, info_dict, final=True):",
            "        # TODO: move sanitization here",
            "        if final:",
            "            title = info_dict['fulltitle'] = info_dict.get('title')",
            "            if not title:",
            "                if title == '':",
            "                    self.write_debug('Extractor gave empty title. Creating a generic title')",
            "                else:",
            "                    self.report_warning('Extractor failed to obtain \"title\". Creating a generic title instead')",
            "                info_dict['title'] = f'{info_dict[\"extractor\"].replace(\":\", \"-\")} video #{info_dict[\"id\"]}'",
            "",
            "        if info_dict.get('duration') is not None:",
            "            info_dict['duration_string'] = formatSeconds(info_dict['duration'])",
            "",
            "        for ts_key, date_key in (",
            "                ('timestamp', 'upload_date'),",
            "                ('release_timestamp', 'release_date'),",
            "                ('modified_timestamp', 'modified_date'),",
            "        ):",
            "            if info_dict.get(date_key) is None and info_dict.get(ts_key) is not None:",
            "                # Working around out-of-range timestamp values (e.g. negative ones on Windows,",
            "                # see http://bugs.python.org/issue1646728)",
            "                with contextlib.suppress(ValueError, OverflowError, OSError):",
            "                    upload_date = dt.datetime.fromtimestamp(info_dict[ts_key], dt.timezone.utc)",
            "                    info_dict[date_key] = upload_date.strftime('%Y%m%d')",
            "",
            "        if not info_dict.get('release_year'):",
            "            info_dict['release_year'] = traverse_obj(info_dict, ('release_date', {lambda x: int(x[:4])}))",
            "",
            "        live_keys = ('is_live', 'was_live')",
            "        live_status = info_dict.get('live_status')",
            "        if live_status is None:",
            "            for key in live_keys:",
            "                if info_dict.get(key) is False:",
            "                    continue",
            "                if info_dict.get(key):",
            "                    live_status = key",
            "                break",
            "            if all(info_dict.get(key) is False for key in live_keys):",
            "                live_status = 'not_live'",
            "        if live_status:",
            "            info_dict['live_status'] = live_status",
            "            for key in live_keys:",
            "                if info_dict.get(key) is None:",
            "                    info_dict[key] = (live_status == key)",
            "        if live_status == 'post_live':",
            "            info_dict['was_live'] = True",
            "",
            "        # Auto generate title fields corresponding to the *_number fields when missing",
            "        # in order to always have clean titles. This is very common for TV series.",
            "        for field in ('chapter', 'season', 'episode'):",
            "            if final and info_dict.get('%s_number' % field) is not None and not info_dict.get(field):",
            "                info_dict[field] = '%s %d' % (field.capitalize(), info_dict['%s_number' % field])",
            "",
            "        for old_key, new_key in self._deprecated_multivalue_fields.items():",
            "            if new_key in info_dict and old_key in info_dict:",
            "                if '_version' not in info_dict:  # HACK: Do not warn when using --load-info-json",
            "                    self.deprecation_warning(f'Do not return {old_key!r} when {new_key!r} is present')",
            "            elif old_value := info_dict.get(old_key):",
            "                info_dict[new_key] = old_value.split(', ')",
            "            elif new_value := info_dict.get(new_key):",
            "                info_dict[old_key] = ', '.join(v.replace(',', '\\N{FULLWIDTH COMMA}') for v in new_value)",
            "",
            "    def _raise_pending_errors(self, info):",
            "        err = info.pop('__pending_error', None)",
            "        if err:",
            "            self.report_error(err, tb=False)",
            "",
            "    def sort_formats(self, info_dict):",
            "        formats = self._get_formats(info_dict)",
            "        formats.sort(key=FormatSorter(",
            "            self, info_dict.get('_format_sort_fields') or []).calculate_preference)",
            "",
            "    def process_video_result(self, info_dict, download=True):",
            "        assert info_dict.get('_type', 'video') == 'video'",
            "        self._num_videos += 1",
            "",
            "        if 'id' not in info_dict:",
            "            raise ExtractorError('Missing \"id\" field in extractor result', ie=info_dict['extractor'])",
            "        elif not info_dict.get('id'):",
            "            raise ExtractorError('Extractor failed to obtain \"id\"', ie=info_dict['extractor'])",
            "",
            "        def report_force_conversion(field, field_not, conversion):",
            "            self.report_warning(",
            "                '\"%s\" field is not %s - forcing %s conversion, there is an error in extractor'",
            "                % (field, field_not, conversion))",
            "",
            "        def sanitize_string_field(info, string_field):",
            "            field = info.get(string_field)",
            "            if field is None or isinstance(field, str):",
            "                return",
            "            report_force_conversion(string_field, 'a string', 'string')",
            "            info[string_field] = str(field)",
            "",
            "        def sanitize_numeric_fields(info):",
            "            for numeric_field in self._NUMERIC_FIELDS:",
            "                field = info.get(numeric_field)",
            "                if field is None or isinstance(field, (int, float)):",
            "                    continue",
            "                report_force_conversion(numeric_field, 'numeric', 'int')",
            "                info[numeric_field] = int_or_none(field)",
            "",
            "        sanitize_string_field(info_dict, 'id')",
            "        sanitize_numeric_fields(info_dict)",
            "        if info_dict.get('section_end') and info_dict.get('section_start') is not None:",
            "            info_dict['duration'] = round(info_dict['section_end'] - info_dict['section_start'], 3)",
            "        if (info_dict.get('duration') or 0) <= 0 and info_dict.pop('duration', None):",
            "            self.report_warning('\"duration\" field is negative, there is an error in extractor')",
            "",
            "        chapters = info_dict.get('chapters') or []",
            "        if chapters and chapters[0].get('start_time'):",
            "            chapters.insert(0, {'start_time': 0})",
            "",
            "        dummy_chapter = {'end_time': 0, 'start_time': info_dict.get('duration')}",
            "        for idx, (prev, current, next_) in enumerate(zip(",
            "                (dummy_chapter, *chapters), chapters, (*chapters[1:], dummy_chapter)), 1):",
            "            if current.get('start_time') is None:",
            "                current['start_time'] = prev.get('end_time')",
            "            if not current.get('end_time'):",
            "                current['end_time'] = next_.get('start_time')",
            "            if not current.get('title'):",
            "                current['title'] = f'<Untitled Chapter {idx}>'",
            "",
            "        if 'playlist' not in info_dict:",
            "            # It isn't part of a playlist",
            "            info_dict['playlist'] = None",
            "            info_dict['playlist_index'] = None",
            "",
            "        self._sanitize_thumbnails(info_dict)",
            "",
            "        thumbnail = info_dict.get('thumbnail')",
            "        thumbnails = info_dict.get('thumbnails')",
            "        if thumbnail:",
            "            info_dict['thumbnail'] = sanitize_url(thumbnail)",
            "        elif thumbnails:",
            "            info_dict['thumbnail'] = thumbnails[-1]['url']",
            "",
            "        if info_dict.get('display_id') is None and 'id' in info_dict:",
            "            info_dict['display_id'] = info_dict['id']",
            "",
            "        self._fill_common_fields(info_dict)",
            "",
            "        for cc_kind in ('subtitles', 'automatic_captions'):",
            "            cc = info_dict.get(cc_kind)",
            "            if cc:",
            "                for _, subtitle in cc.items():",
            "                    for subtitle_format in subtitle:",
            "                        if subtitle_format.get('url'):",
            "                            subtitle_format['url'] = sanitize_url(subtitle_format['url'])",
            "                        if subtitle_format.get('ext') is None:",
            "                            subtitle_format['ext'] = determine_ext(subtitle_format['url']).lower()",
            "",
            "        automatic_captions = info_dict.get('automatic_captions')",
            "        subtitles = info_dict.get('subtitles')",
            "",
            "        info_dict['requested_subtitles'] = self.process_subtitles(",
            "            info_dict['id'], subtitles, automatic_captions)",
            "",
            "        formats = self._get_formats(info_dict)",
            "",
            "        # Backward compatibility with InfoExtractor._sort_formats",
            "        field_preference = (formats or [{}])[0].pop('__sort_fields', None)",
            "        if field_preference:",
            "            info_dict['_format_sort_fields'] = field_preference",
            "",
            "        info_dict['_has_drm'] = any(  # or None ensures --clean-infojson removes it",
            "            f.get('has_drm') and f['has_drm'] != 'maybe' for f in formats) or None",
            "        if not self.params.get('allow_unplayable_formats'):",
            "            formats = [f for f in formats if not f.get('has_drm') or f['has_drm'] == 'maybe']",
            "",
            "        if formats and all(f.get('acodec') == f.get('vcodec') == 'none' for f in formats):",
            "            self.report_warning(",
            "                f'{\"This video is DRM protected and \" if info_dict[\"_has_drm\"] else \"\"}'",
            "                'only images are available for download. Use --list-formats to see them'.capitalize())",
            "",
            "        get_from_start = not info_dict.get('is_live') or bool(self.params.get('live_from_start'))",
            "        if not get_from_start:",
            "            info_dict['title'] += ' ' + dt.datetime.now().strftime('%Y-%m-%d %H:%M')",
            "        if info_dict.get('is_live') and formats:",
            "            formats = [f for f in formats if bool(f.get('is_from_start')) == get_from_start]",
            "            if get_from_start and not formats:",
            "                self.raise_no_formats(info_dict, msg=(",
            "                    '--live-from-start is passed, but there are no formats that can be downloaded from the start. '",
            "                    'If you want to download from the current time, use --no-live-from-start'))",
            "",
            "        def is_wellformed(f):",
            "            url = f.get('url')",
            "            if not url:",
            "                self.report_warning(",
            "                    '\"url\" field is missing or empty - skipping format, '",
            "                    'there is an error in extractor')",
            "                return False",
            "            if isinstance(url, bytes):",
            "                sanitize_string_field(f, 'url')",
            "            return True",
            "",
            "        # Filter out malformed formats for better extraction robustness",
            "        formats = list(filter(is_wellformed, formats or []))",
            "",
            "        if not formats:",
            "            self.raise_no_formats(info_dict)",
            "",
            "        for format in formats:",
            "            sanitize_string_field(format, 'format_id')",
            "            sanitize_numeric_fields(format)",
            "            format['url'] = sanitize_url(format['url'])",
            "            if format.get('ext') is None:",
            "                format['ext'] = determine_ext(format['url']).lower()",
            "            if format['ext'] in ('aac', 'opus', 'mp3', 'flac', 'vorbis'):",
            "                if format.get('acodec') is None:",
            "                    format['acodec'] = format['ext']",
            "            if format.get('protocol') is None:",
            "                format['protocol'] = determine_protocol(format)",
            "            if format.get('resolution') is None:",
            "                format['resolution'] = self.format_resolution(format, default=None)",
            "            if format.get('dynamic_range') is None and format.get('vcodec') != 'none':",
            "                format['dynamic_range'] = 'SDR'",
            "            if format.get('aspect_ratio') is None:",
            "                format['aspect_ratio'] = try_call(lambda: round(format['width'] / format['height'], 2))",
            "            # For fragmented formats, \"tbr\" is often max bitrate and not average",
            "            if (('manifest-filesize-approx' in self.params['compat_opts'] or not format.get('manifest_url'))",
            "                    and not format.get('filesize') and not format.get('filesize_approx')):",
            "                format['filesize_approx'] = filesize_from_tbr(format.get('tbr'), info_dict.get('duration'))",
            "            format['http_headers'] = self._calc_headers(collections.ChainMap(format, info_dict), load_cookies=True)",
            "",
            "        # Safeguard against old/insecure infojson when using --load-info-json",
            "        if info_dict.get('http_headers'):",
            "            info_dict['http_headers'] = HTTPHeaderDict(info_dict['http_headers'])",
            "            info_dict['http_headers'].pop('Cookie', None)",
            "",
            "        # This is copied to http_headers by the above _calc_headers and can now be removed",
            "        if '__x_forwarded_for_ip' in info_dict:",
            "            del info_dict['__x_forwarded_for_ip']",
            "",
            "        self.sort_formats({",
            "            'formats': formats,",
            "            '_format_sort_fields': info_dict.get('_format_sort_fields')",
            "        })",
            "",
            "        # Sanitize and group by format_id",
            "        formats_dict = {}",
            "        for i, format in enumerate(formats):",
            "            if not format.get('format_id'):",
            "                format['format_id'] = str(i)",
            "            else:",
            "                # Sanitize format_id from characters used in format selector expression",
            "                format['format_id'] = re.sub(r'[\\s,/+\\[\\]()]', '_', format['format_id'])",
            "            formats_dict.setdefault(format['format_id'], []).append(format)",
            "",
            "        # Make sure all formats have unique format_id",
            "        common_exts = set(itertools.chain(*self._format_selection_exts.values()))",
            "        for format_id, ambiguous_formats in formats_dict.items():",
            "            ambigious_id = len(ambiguous_formats) > 1",
            "            for i, format in enumerate(ambiguous_formats):",
            "                if ambigious_id:",
            "                    format['format_id'] = '%s-%d' % (format_id, i)",
            "                # Ensure there is no conflict between id and ext in format selection",
            "                # See https://github.com/yt-dlp/yt-dlp/issues/1282",
            "                if format['format_id'] != format['ext'] and format['format_id'] in common_exts:",
            "                    format['format_id'] = 'f%s' % format['format_id']",
            "",
            "                if format.get('format') is None:",
            "                    format['format'] = '{id} - {res}{note}'.format(",
            "                        id=format['format_id'],",
            "                        res=self.format_resolution(format),",
            "                        note=format_field(format, 'format_note', ' (%s)'),",
            "                    )",
            "",
            "        if self.params.get('check_formats') is True:",
            "            formats = LazyList(self._check_formats(formats[::-1]), reverse=True)",
            "",
            "        if not formats or formats[0] is not info_dict:",
            "            # only set the 'formats' fields if the original info_dict list them",
            "            # otherwise we end up with a circular reference, the first (and unique)",
            "            # element in the 'formats' field in info_dict is info_dict itself,",
            "            # which can't be exported to json",
            "            info_dict['formats'] = formats",
            "",
            "        info_dict, _ = self.pre_process(info_dict)",
            "",
            "        if self._match_entry(info_dict, incomplete=self._format_fields) is not None:",
            "            return info_dict",
            "",
            "        self.post_extract(info_dict)",
            "        info_dict, _ = self.pre_process(info_dict, 'after_filter')",
            "",
            "        # The pre-processors may have modified the formats",
            "        formats = self._get_formats(info_dict)",
            "",
            "        list_only = self.params.get('simulate') == 'list_only'",
            "        interactive_format_selection = not list_only and self.format_selector == '-'",
            "        if self.params.get('list_thumbnails'):",
            "            self.list_thumbnails(info_dict)",
            "        if self.params.get('listsubtitles'):",
            "            if 'automatic_captions' in info_dict:",
            "                self.list_subtitles(",
            "                    info_dict['id'], automatic_captions, 'automatic captions')",
            "            self.list_subtitles(info_dict['id'], subtitles, 'subtitles')",
            "        if self.params.get('listformats') or interactive_format_selection:",
            "            self.list_formats(info_dict)",
            "        if list_only:",
            "            # Without this printing, -F --print-json will not work",
            "            self.__forced_printings(info_dict)",
            "            return info_dict",
            "",
            "        format_selector = self.format_selector",
            "        while True:",
            "            if interactive_format_selection:",
            "                req_format = input(self._format_screen('\\nEnter format selector ', self.Styles.EMPHASIS)",
            "                                   + '(Press ENTER for default, or Ctrl+C to quit)'",
            "                                   + self._format_screen(': ', self.Styles.EMPHASIS))",
            "                try:",
            "                    format_selector = self.build_format_selector(req_format) if req_format else None",
            "                except SyntaxError as err:",
            "                    self.report_error(err, tb=False, is_error=False)",
            "                    continue",
            "",
            "            if format_selector is None:",
            "                req_format = self._default_format_spec(info_dict, download=download)",
            "                self.write_debug(f'Default format spec: {req_format}')",
            "                format_selector = self.build_format_selector(req_format)",
            "",
            "            formats_to_download = list(format_selector({",
            "                'formats': formats,",
            "                'has_merged_format': any('none' not in (f.get('acodec'), f.get('vcodec')) for f in formats),",
            "                'incomplete_formats': (all(f.get('vcodec') == 'none' for f in formats)  # No formats with video",
            "                                       or all(f.get('acodec') == 'none' for f in formats)),  # OR, No formats with audio",
            "            }))",
            "            if interactive_format_selection and not formats_to_download:",
            "                self.report_error('Requested format is not available', tb=False, is_error=False)",
            "                continue",
            "            break",
            "",
            "        if not formats_to_download:",
            "            if not self.params.get('ignore_no_formats_error'):",
            "                raise ExtractorError(",
            "                    'Requested format is not available. Use --list-formats for a list of available formats',",
            "                    expected=True, video_id=info_dict['id'], ie=info_dict['extractor'])",
            "            self.report_warning('Requested format is not available')",
            "            # Process what we can, even without any available formats.",
            "            formats_to_download = [{}]",
            "",
            "        requested_ranges = tuple(self.params.get('download_ranges', lambda *_: [{}])(info_dict, self))",
            "        best_format, downloaded_formats = formats_to_download[-1], []",
            "        if download:",
            "            if best_format and requested_ranges:",
            "                def to_screen(*msg):",
            "                    self.to_screen(f'[info] {info_dict[\"id\"]}: {\" \".join(\", \".join(variadic(m)) for m in msg)}')",
            "",
            "                to_screen(f'Downloading {len(formats_to_download)} format(s):',",
            "                          (f['format_id'] for f in formats_to_download))",
            "                if requested_ranges != ({}, ):",
            "                    to_screen(f'Downloading {len(requested_ranges)} time ranges:',",
            "                              (f'{c[\"start_time\"]:.1f}-{c[\"end_time\"]:.1f}' for c in requested_ranges))",
            "            max_downloads_reached = False",
            "",
            "            for fmt, chapter in itertools.product(formats_to_download, requested_ranges):",
            "                new_info = self._copy_infodict(info_dict)",
            "                new_info.update(fmt)",
            "                offset, duration = info_dict.get('section_start') or 0, info_dict.get('duration') or float('inf')",
            "                end_time = offset + min(chapter.get('end_time', duration), duration)",
            "                # duration may not be accurate. So allow deviations <1sec",
            "                if end_time == float('inf') or end_time > offset + duration + 1:",
            "                    end_time = None",
            "                if chapter or offset:",
            "                    new_info.update({",
            "                        'section_start': offset + chapter.get('start_time', 0),",
            "                        'section_end': end_time,",
            "                        'section_title': chapter.get('title'),",
            "                        'section_number': chapter.get('index'),",
            "                    })",
            "                downloaded_formats.append(new_info)",
            "                try:",
            "                    self.process_info(new_info)",
            "                except MaxDownloadsReached:",
            "                    max_downloads_reached = True",
            "                self._raise_pending_errors(new_info)",
            "                # Remove copied info",
            "                for key, val in tuple(new_info.items()):",
            "                    if info_dict.get(key) == val:",
            "                        new_info.pop(key)",
            "                if max_downloads_reached:",
            "                    break",
            "",
            "            write_archive = {f.get('__write_download_archive', False) for f in downloaded_formats}",
            "            assert write_archive.issubset({True, False, 'ignore'})",
            "            if True in write_archive and False not in write_archive:",
            "                self.record_download_archive(info_dict)",
            "",
            "            info_dict['requested_downloads'] = downloaded_formats",
            "            info_dict = self.run_all_pps('after_video', info_dict)",
            "            if max_downloads_reached:",
            "                raise MaxDownloadsReached()",
            "",
            "        # We update the info dict with the selected best quality format (backwards compatibility)",
            "        info_dict.update(best_format)",
            "        return info_dict",
            "",
            "    def process_subtitles(self, video_id, normal_subtitles, automatic_captions):",
            "        \"\"\"Select the requested subtitles and their format\"\"\"",
            "        available_subs, normal_sub_langs = {}, []",
            "        if normal_subtitles and self.params.get('writesubtitles'):",
            "            available_subs.update(normal_subtitles)",
            "            normal_sub_langs = tuple(normal_subtitles.keys())",
            "        if automatic_captions and self.params.get('writeautomaticsub'):",
            "            for lang, cap_info in automatic_captions.items():",
            "                if lang not in available_subs:",
            "                    available_subs[lang] = cap_info",
            "",
            "        if not available_subs or (",
            "                not self.params.get('writesubtitles')",
            "                and not self.params.get('writeautomaticsub')):",
            "            return None",
            "",
            "        all_sub_langs = tuple(available_subs.keys())",
            "        if self.params.get('allsubtitles', False):",
            "            requested_langs = all_sub_langs",
            "        elif self.params.get('subtitleslangs', False):",
            "            try:",
            "                requested_langs = orderedSet_from_options(",
            "                    self.params.get('subtitleslangs'), {'all': all_sub_langs}, use_regex=True)",
            "            except re.error as e:",
            "                raise ValueError(f'Wrong regex for subtitlelangs: {e.pattern}')",
            "        else:",
            "            requested_langs = LazyList(itertools.chain(",
            "                ['en'] if 'en' in normal_sub_langs else [],",
            "                filter(lambda f: f.startswith('en'), normal_sub_langs),",
            "                ['en'] if 'en' in all_sub_langs else [],",
            "                filter(lambda f: f.startswith('en'), all_sub_langs),",
            "                normal_sub_langs, all_sub_langs,",
            "            ))[:1]",
            "        if requested_langs:",
            "            self.to_screen(f'[info] {video_id}: Downloading subtitles: {\", \".join(requested_langs)}')",
            "",
            "        formats_query = self.params.get('subtitlesformat', 'best')",
            "        formats_preference = formats_query.split('/') if formats_query else []",
            "        subs = {}",
            "        for lang in requested_langs:",
            "            formats = available_subs.get(lang)",
            "            if formats is None:",
            "                self.report_warning(f'{lang} subtitles not available for {video_id}')",
            "                continue",
            "            for ext in formats_preference:",
            "                if ext == 'best':",
            "                    f = formats[-1]",
            "                    break",
            "                matches = list(filter(lambda f: f['ext'] == ext, formats))",
            "                if matches:",
            "                    f = matches[-1]",
            "                    break",
            "            else:",
            "                f = formats[-1]",
            "                self.report_warning(",
            "                    'No subtitle format found matching \"%s\" for language %s, '",
            "                    'using %s' % (formats_query, lang, f['ext']))",
            "            subs[lang] = f",
            "        return subs",
            "",
            "    def _forceprint(self, key, info_dict):",
            "        if info_dict is None:",
            "            return",
            "        info_copy = info_dict.copy()",
            "        info_copy.setdefault('filename', self.prepare_filename(info_dict))",
            "        if info_dict.get('requested_formats') is not None:",
            "            # For RTMP URLs, also include the playpath",
            "            info_copy['urls'] = '\\n'.join(f['url'] + f.get('play_path', '') for f in info_dict['requested_formats'])",
            "        elif info_dict.get('url'):",
            "            info_copy['urls'] = info_dict['url'] + info_dict.get('play_path', '')",
            "        info_copy['formats_table'] = self.render_formats_table(info_dict)",
            "        info_copy['thumbnails_table'] = self.render_thumbnails_table(info_dict)",
            "        info_copy['subtitles_table'] = self.render_subtitles_table(info_dict.get('id'), info_dict.get('subtitles'))",
            "        info_copy['automatic_captions_table'] = self.render_subtitles_table(info_dict.get('id'), info_dict.get('automatic_captions'))",
            "",
            "        def format_tmpl(tmpl):",
            "            mobj = re.fullmatch(r'([\\w.:,]|-\\d|(?P<dict>{([\\w.:,]|-\\d)+}))+=?', tmpl)",
            "            if not mobj:",
            "                return tmpl",
            "",
            "            fmt = '%({})s'",
            "            if tmpl.startswith('{'):",
            "                tmpl, fmt = f'.{tmpl}', '%({})j'",
            "            if tmpl.endswith('='):",
            "                tmpl, fmt = tmpl[:-1], '{0} = %({0})#j'",
            "            return '\\n'.join(map(fmt.format, [tmpl] if mobj.group('dict') else tmpl.split(',')))",
            "",
            "        for tmpl in self.params['forceprint'].get(key, []):",
            "            self.to_stdout(self.evaluate_outtmpl(format_tmpl(tmpl), info_copy))",
            "",
            "        for tmpl, file_tmpl in self.params['print_to_file'].get(key, []):",
            "            filename = self.prepare_filename(info_dict, outtmpl=file_tmpl)",
            "            tmpl = format_tmpl(tmpl)",
            "            self.to_screen(f'[info] Writing {tmpl!r} to: {filename}')",
            "            if self._ensure_dir_exists(filename):",
            "                with open(filename, 'a', encoding='utf-8', newline='') as f:",
            "                    f.write(self.evaluate_outtmpl(tmpl, info_copy) + os.linesep)",
            "",
            "        return info_copy",
            "",
            "    def __forced_printings(self, info_dict, filename=None, incomplete=True):",
            "        if (self.params.get('forcejson')",
            "                or self.params['forceprint'].get('video')",
            "                or self.params['print_to_file'].get('video')):",
            "            self.post_extract(info_dict)",
            "        if filename:",
            "            info_dict['filename'] = filename",
            "        info_copy = self._forceprint('video', info_dict)",
            "",
            "        def print_field(field, actual_field=None, optional=False):",
            "            if actual_field is None:",
            "                actual_field = field",
            "            if self.params.get(f'force{field}') and (",
            "                    info_copy.get(field) is not None or (not optional and not incomplete)):",
            "                self.to_stdout(info_copy[actual_field])",
            "",
            "        print_field('title')",
            "        print_field('id')",
            "        print_field('url', 'urls')",
            "        print_field('thumbnail', optional=True)",
            "        print_field('description', optional=True)",
            "        print_field('filename')",
            "        if self.params.get('forceduration') and info_copy.get('duration') is not None:",
            "            self.to_stdout(formatSeconds(info_copy['duration']))",
            "        print_field('format')",
            "",
            "        if self.params.get('forcejson'):",
            "            self.to_stdout(json.dumps(self.sanitize_info(info_dict)))",
            "",
            "    def dl(self, name, info, subtitle=False, test=False):",
            "        if not info.get('url'):",
            "            self.raise_no_formats(info, True)",
            "",
            "        if test:",
            "            verbose = self.params.get('verbose')",
            "            params = {",
            "                'test': True,",
            "                'quiet': self.params.get('quiet') or not verbose,",
            "                'verbose': verbose,",
            "                'noprogress': not verbose,",
            "                'nopart': True,",
            "                'skip_unavailable_fragments': False,",
            "                'keep_fragments': False,",
            "                'overwrites': True,",
            "                '_no_ytdl_file': True,",
            "            }",
            "        else:",
            "            params = self.params",
            "        fd = get_suitable_downloader(info, params, to_stdout=(name == '-'))(self, params)",
            "        if not test:",
            "            for ph in self._progress_hooks:",
            "                fd.add_progress_hook(ph)",
            "            urls = '\", \"'.join(",
            "                (f['url'].split(',')[0] + ',<data>' if f['url'].startswith('data:') else f['url'])",
            "                for f in info.get('requested_formats', []) or [info])",
            "            self.write_debug(f'Invoking {fd.FD_NAME} downloader on \"{urls}\"')",
            "",
            "        # Note: Ideally info should be a deep-copied so that hooks cannot modify it.",
            "        # But it may contain objects that are not deep-copyable",
            "        new_info = self._copy_infodict(info)",
            "        if new_info.get('http_headers') is None:",
            "            new_info['http_headers'] = self._calc_headers(new_info)",
            "        return fd.download(name, new_info, subtitle)",
            "",
            "    def existing_file(self, filepaths, *, default_overwrite=True):",
            "        existing_files = list(filter(os.path.exists, orderedSet(filepaths)))",
            "        if existing_files and not self.params.get('overwrites', default_overwrite):",
            "            return existing_files[0]",
            "",
            "        for file in existing_files:",
            "            self.report_file_delete(file)",
            "            os.remove(file)",
            "        return None",
            "",
            "    def process_info(self, info_dict):",
            "        \"\"\"Process a single resolved IE result. (Modifies it in-place)\"\"\"",
            "",
            "        assert info_dict.get('_type', 'video') == 'video'",
            "        original_infodict = info_dict",
            "",
            "        if 'format' not in info_dict and 'ext' in info_dict:",
            "            info_dict['format'] = info_dict['ext']",
            "",
            "        if self._match_entry(info_dict) is not None:",
            "            info_dict['__write_download_archive'] = 'ignore'",
            "            return",
            "",
            "        # Does nothing under normal operation - for backward compatibility of process_info",
            "        self.post_extract(info_dict)",
            "",
            "        def replace_info_dict(new_info):",
            "            nonlocal info_dict",
            "            if new_info == info_dict:",
            "                return",
            "            info_dict.clear()",
            "            info_dict.update(new_info)",
            "",
            "        new_info, _ = self.pre_process(info_dict, 'video')",
            "        replace_info_dict(new_info)",
            "        self._num_downloads += 1",
            "",
            "        # info_dict['_filename'] needs to be set for backward compatibility",
            "        info_dict['_filename'] = full_filename = self.prepare_filename(info_dict, warn=True)",
            "        temp_filename = self.prepare_filename(info_dict, 'temp')",
            "        files_to_move = {}",
            "",
            "        # Forced printings",
            "        self.__forced_printings(info_dict, full_filename, incomplete=('format' not in info_dict))",
            "",
            "        def check_max_downloads():",
            "            if self._num_downloads >= float(self.params.get('max_downloads') or 'inf'):",
            "                raise MaxDownloadsReached()",
            "",
            "        if self.params.get('simulate'):",
            "            info_dict['__write_download_archive'] = self.params.get('force_write_download_archive')",
            "            check_max_downloads()",
            "            return",
            "",
            "        if full_filename is None:",
            "            return",
            "        if not self._ensure_dir_exists(encodeFilename(full_filename)):",
            "            return",
            "        if not self._ensure_dir_exists(encodeFilename(temp_filename)):",
            "            return",
            "",
            "        if self._write_description('video', info_dict,",
            "                                   self.prepare_filename(info_dict, 'description')) is None:",
            "            return",
            "",
            "        sub_files = self._write_subtitles(info_dict, temp_filename)",
            "        if sub_files is None:",
            "            return",
            "        files_to_move.update(dict(sub_files))",
            "",
            "        thumb_files = self._write_thumbnails(",
            "            'video', info_dict, temp_filename, self.prepare_filename(info_dict, 'thumbnail'))",
            "        if thumb_files is None:",
            "            return",
            "        files_to_move.update(dict(thumb_files))",
            "",
            "        infofn = self.prepare_filename(info_dict, 'infojson')",
            "        _infojson_written = self._write_info_json('video', info_dict, infofn)",
            "        if _infojson_written:",
            "            info_dict['infojson_filename'] = infofn",
            "            # For backward compatibility, even though it was a private field",
            "            info_dict['__infojson_filename'] = infofn",
            "        elif _infojson_written is None:",
            "            return",
            "",
            "        # Note: Annotations are deprecated",
            "        annofn = None",
            "        if self.params.get('writeannotations', False):",
            "            annofn = self.prepare_filename(info_dict, 'annotation')",
            "        if annofn:",
            "            if not self._ensure_dir_exists(encodeFilename(annofn)):",
            "                return",
            "            if not self.params.get('overwrites', True) and os.path.exists(encodeFilename(annofn)):",
            "                self.to_screen('[info] Video annotations are already present')",
            "            elif not info_dict.get('annotations'):",
            "                self.report_warning('There are no annotations to write.')",
            "            else:",
            "                try:",
            "                    self.to_screen('[info] Writing video annotations to: ' + annofn)",
            "                    with open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:",
            "                        annofile.write(info_dict['annotations'])",
            "                except (KeyError, TypeError):",
            "                    self.report_warning('There are no annotations to write.')",
            "                except OSError:",
            "                    self.report_error('Cannot write annotations file: ' + annofn)",
            "                    return",
            "",
            "        # Write internet shortcut files",
            "        def _write_link_file(link_type):",
            "            url = try_get(info_dict['webpage_url'], iri_to_uri)",
            "            if not url:",
            "                self.report_warning(",
            "                    f'Cannot write internet shortcut file because the actual URL of \"{info_dict[\"webpage_url\"]}\" is unknown')",
            "                return True",
            "            linkfn = replace_extension(self.prepare_filename(info_dict, 'link'), link_type, info_dict.get('ext'))",
            "            if not self._ensure_dir_exists(encodeFilename(linkfn)):",
            "                return False",
            "            if self.params.get('overwrites', True) and os.path.exists(encodeFilename(linkfn)):",
            "                self.to_screen(f'[info] Internet shortcut (.{link_type}) is already present')",
            "                return True",
            "            try:",
            "                self.to_screen(f'[info] Writing internet shortcut (.{link_type}) to: {linkfn}')",
            "                with open(encodeFilename(to_high_limit_path(linkfn)), 'w', encoding='utf-8',",
            "                          newline='\\r\\n' if link_type == 'url' else '\\n') as linkfile:",
            "                    template_vars = {'url': url}",
            "                    if link_type == 'desktop':",
            "                        template_vars['filename'] = linkfn[:-(len(link_type) + 1)]",
            "                    linkfile.write(LINK_TEMPLATES[link_type] % template_vars)",
            "            except OSError:",
            "                self.report_error(f'Cannot write internet shortcut {linkfn}')",
            "                return False",
            "            return True",
            "",
            "        write_links = {",
            "            'url': self.params.get('writeurllink'),",
            "            'webloc': self.params.get('writewebloclink'),",
            "            'desktop': self.params.get('writedesktoplink'),",
            "        }",
            "        if self.params.get('writelink'):",
            "            link_type = ('webloc' if sys.platform == 'darwin'",
            "                         else 'desktop' if sys.platform.startswith('linux')",
            "                         else 'url')",
            "            write_links[link_type] = True",
            "",
            "        if any(should_write and not _write_link_file(link_type)",
            "               for link_type, should_write in write_links.items()):",
            "            return",
            "",
            "        new_info, files_to_move = self.pre_process(info_dict, 'before_dl', files_to_move)",
            "        replace_info_dict(new_info)",
            "",
            "        if self.params.get('skip_download'):",
            "            info_dict['filepath'] = temp_filename",
            "            info_dict['__finaldir'] = os.path.dirname(os.path.abspath(encodeFilename(full_filename)))",
            "            info_dict['__files_to_move'] = files_to_move",
            "            replace_info_dict(self.run_pp(MoveFilesAfterDownloadPP(self, False), info_dict))",
            "            info_dict['__write_download_archive'] = self.params.get('force_write_download_archive')",
            "        else:",
            "            # Download",
            "            info_dict.setdefault('__postprocessors', [])",
            "            try:",
            "",
            "                def existing_video_file(*filepaths):",
            "                    ext = info_dict.get('ext')",
            "                    converted = lambda file: replace_extension(file, self.params.get('final_ext') or ext, ext)",
            "                    file = self.existing_file(itertools.chain(*zip(map(converted, filepaths), filepaths)),",
            "                                              default_overwrite=False)",
            "                    if file:",
            "                        info_dict['ext'] = os.path.splitext(file)[1][1:]",
            "                    return file",
            "",
            "                fd, success = None, True",
            "                if info_dict.get('protocol') or info_dict.get('url'):",
            "                    fd = get_suitable_downloader(info_dict, self.params, to_stdout=temp_filename == '-')",
            "                    if fd != FFmpegFD and 'no-direct-merge' not in self.params['compat_opts'] and (",
            "                            info_dict.get('section_start') or info_dict.get('section_end')):",
            "                        msg = ('This format cannot be partially downloaded' if FFmpegFD.available()",
            "                               else 'You have requested downloading the video partially, but ffmpeg is not installed')",
            "                        self.report_error(f'{msg}. Aborting')",
            "                        return",
            "",
            "                if info_dict.get('requested_formats') is not None:",
            "                    old_ext = info_dict['ext']",
            "                    if self.params.get('merge_output_format') is None:",
            "                        if (info_dict['ext'] == 'webm'",
            "                                and info_dict.get('thumbnails')",
            "                                # check with type instead of pp_key, __name__, or isinstance",
            "                                # since we dont want any custom PPs to trigger this",
            "                                and any(type(pp) == EmbedThumbnailPP for pp in self._pps['post_process'])):  # noqa: E721",
            "                            info_dict['ext'] = 'mkv'",
            "                            self.report_warning(",
            "                                'webm doesn\\'t support embedding a thumbnail, mkv will be used')",
            "                    new_ext = info_dict['ext']",
            "",
            "                    def correct_ext(filename, ext=new_ext):",
            "                        if filename == '-':",
            "                            return filename",
            "                        filename_real_ext = os.path.splitext(filename)[1][1:]",
            "                        filename_wo_ext = (",
            "                            os.path.splitext(filename)[0]",
            "                            if filename_real_ext in (old_ext, new_ext)",
            "                            else filename)",
            "                        return f'{filename_wo_ext}.{ext}'",
            "",
            "                    # Ensure filename always has a correct extension for successful merge",
            "                    full_filename = correct_ext(full_filename)",
            "                    temp_filename = correct_ext(temp_filename)",
            "                    dl_filename = existing_video_file(full_filename, temp_filename)",
            "",
            "                    info_dict['__real_download'] = False",
            "                    # NOTE: Copy so that original format dicts are not modified",
            "                    info_dict['requested_formats'] = list(map(dict, info_dict['requested_formats']))",
            "",
            "                    merger = FFmpegMergerPP(self)",
            "                    downloaded = []",
            "                    if dl_filename is not None:",
            "                        self.report_file_already_downloaded(dl_filename)",
            "                    elif fd:",
            "                        for f in info_dict['requested_formats'] if fd != FFmpegFD else []:",
            "                            f['filepath'] = fname = prepend_extension(",
            "                                correct_ext(temp_filename, info_dict['ext']),",
            "                                'f%s' % f['format_id'], info_dict['ext'])",
            "                            downloaded.append(fname)",
            "                        info_dict['url'] = '\\n'.join(f['url'] for f in info_dict['requested_formats'])",
            "                        success, real_download = self.dl(temp_filename, info_dict)",
            "                        info_dict['__real_download'] = real_download",
            "                    else:",
            "                        if self.params.get('allow_unplayable_formats'):",
            "                            self.report_warning(",
            "                                'You have requested merging of multiple formats '",
            "                                'while also allowing unplayable formats to be downloaded. '",
            "                                'The formats won\\'t be merged to prevent data corruption.')",
            "                        elif not merger.available:",
            "                            msg = 'You have requested merging of multiple formats but ffmpeg is not installed'",
            "                            if not self.params.get('ignoreerrors'):",
            "                                self.report_error(f'{msg}. Aborting due to --abort-on-error')",
            "                                return",
            "                            self.report_warning(f'{msg}. The formats won\\'t be merged')",
            "",
            "                        if temp_filename == '-':",
            "                            reason = ('using a downloader other than ffmpeg' if FFmpegFD.can_merge_formats(info_dict, self.params)",
            "                                      else 'but the formats are incompatible for simultaneous download' if merger.available",
            "                                      else 'but ffmpeg is not installed')",
            "                            self.report_warning(",
            "                                f'You have requested downloading multiple formats to stdout {reason}. '",
            "                                'The formats will be streamed one after the other')",
            "                            fname = temp_filename",
            "                        for f in info_dict['requested_formats']:",
            "                            new_info = dict(info_dict)",
            "                            del new_info['requested_formats']",
            "                            new_info.update(f)",
            "                            if temp_filename != '-':",
            "                                fname = prepend_extension(",
            "                                    correct_ext(temp_filename, new_info['ext']),",
            "                                    'f%s' % f['format_id'], new_info['ext'])",
            "                                if not self._ensure_dir_exists(fname):",
            "                                    return",
            "                                f['filepath'] = fname",
            "                                downloaded.append(fname)",
            "                            partial_success, real_download = self.dl(fname, new_info)",
            "                            info_dict['__real_download'] = info_dict['__real_download'] or real_download",
            "                            success = success and partial_success",
            "",
            "                    if downloaded and merger.available and not self.params.get('allow_unplayable_formats'):",
            "                        info_dict['__postprocessors'].append(merger)",
            "                        info_dict['__files_to_merge'] = downloaded",
            "                        # Even if there were no downloads, it is being merged only now",
            "                        info_dict['__real_download'] = True",
            "                    else:",
            "                        for file in downloaded:",
            "                            files_to_move[file] = None",
            "                else:",
            "                    # Just a single file",
            "                    dl_filename = existing_video_file(full_filename, temp_filename)",
            "                    if dl_filename is None or dl_filename == temp_filename:",
            "                        # dl_filename == temp_filename could mean that the file was partially downloaded with --no-part.",
            "                        # So we should try to resume the download",
            "                        success, real_download = self.dl(temp_filename, info_dict)",
            "                        info_dict['__real_download'] = real_download",
            "                    else:",
            "                        self.report_file_already_downloaded(dl_filename)",
            "",
            "                dl_filename = dl_filename or temp_filename",
            "                info_dict['__finaldir'] = os.path.dirname(os.path.abspath(encodeFilename(full_filename)))",
            "",
            "            except network_exceptions as err:",
            "                self.report_error('unable to download video data: %s' % error_to_compat_str(err))",
            "                return",
            "            except OSError as err:",
            "                raise UnavailableVideoError(err)",
            "            except (ContentTooShortError, ) as err:",
            "                self.report_error(f'content too short (expected {err.expected} bytes and served {err.downloaded})')",
            "                return",
            "",
            "            self._raise_pending_errors(info_dict)",
            "            if success and full_filename != '-':",
            "",
            "                def fixup():",
            "                    do_fixup = True",
            "                    fixup_policy = self.params.get('fixup')",
            "                    vid = info_dict['id']",
            "",
            "                    if fixup_policy in ('ignore', 'never'):",
            "                        return",
            "                    elif fixup_policy == 'warn':",
            "                        do_fixup = 'warn'",
            "                    elif fixup_policy != 'force':",
            "                        assert fixup_policy in ('detect_or_warn', None)",
            "                        if not info_dict.get('__real_download'):",
            "                            do_fixup = False",
            "",
            "                    def ffmpeg_fixup(cndn, msg, cls):",
            "                        if not (do_fixup and cndn):",
            "                            return",
            "                        elif do_fixup == 'warn':",
            "                            self.report_warning(f'{vid}: {msg}')",
            "                            return",
            "                        pp = cls(self)",
            "                        if pp.available:",
            "                            info_dict['__postprocessors'].append(pp)",
            "                        else:",
            "                            self.report_warning(f'{vid}: {msg}. Install ffmpeg to fix this automatically')",
            "",
            "                    stretched_ratio = info_dict.get('stretched_ratio')",
            "                    ffmpeg_fixup(stretched_ratio not in (1, None),",
            "                                 f'Non-uniform pixel ratio {stretched_ratio}',",
            "                                 FFmpegFixupStretchedPP)",
            "",
            "                    downloader = get_suitable_downloader(info_dict, self.params) if 'protocol' in info_dict else None",
            "                    downloader = downloader.FD_NAME if downloader else None",
            "",
            "                    ext = info_dict.get('ext')",
            "                    postprocessed_by_ffmpeg = info_dict.get('requested_formats') or any((",
            "                        isinstance(pp, FFmpegVideoConvertorPP)",
            "                        and resolve_recode_mapping(ext, pp.mapping)[0] not in (ext, None)",
            "                    ) for pp in self._pps['post_process'])",
            "",
            "                    if not postprocessed_by_ffmpeg:",
            "                        ffmpeg_fixup(fd != FFmpegFD and ext == 'm4a'",
            "                                     and info_dict.get('container') == 'm4a_dash',",
            "                                     'writing DASH m4a. Only some players support this container',",
            "                                     FFmpegFixupM4aPP)",
            "                        ffmpeg_fixup(downloader == 'hlsnative' and not self.params.get('hls_use_mpegts')",
            "                                     or info_dict.get('is_live') and self.params.get('hls_use_mpegts') is None,",
            "                                     'Possible MPEG-TS in MP4 container or malformed AAC timestamps',",
            "                                     FFmpegFixupM3u8PP)",
            "                        ffmpeg_fixup(downloader == 'dashsegments'",
            "                                     and (info_dict.get('is_live') or info_dict.get('is_dash_periods')),",
            "                                     'Possible duplicate MOOV atoms', FFmpegFixupDuplicateMoovPP)",
            "",
            "                    ffmpeg_fixup(downloader == 'web_socket_fragment', 'Malformed timestamps detected', FFmpegFixupTimestampPP)",
            "                    ffmpeg_fixup(downloader == 'web_socket_fragment', 'Malformed duration detected', FFmpegFixupDurationPP)",
            "",
            "                fixup()",
            "                try:",
            "                    replace_info_dict(self.post_process(dl_filename, info_dict, files_to_move))",
            "                except PostProcessingError as err:",
            "                    self.report_error('Postprocessing: %s' % str(err))",
            "                    return",
            "                try:",
            "                    for ph in self._post_hooks:",
            "                        ph(info_dict['filepath'])",
            "                except Exception as err:",
            "                    self.report_error('post hooks: %s' % str(err))",
            "                    return",
            "                info_dict['__write_download_archive'] = True",
            "",
            "        assert info_dict is original_infodict  # Make sure the info_dict was modified in-place",
            "        if self.params.get('force_write_download_archive'):",
            "            info_dict['__write_download_archive'] = True",
            "        check_max_downloads()",
            "",
            "    def __download_wrapper(self, func):",
            "        @functools.wraps(func)",
            "        def wrapper(*args, **kwargs):",
            "            try:",
            "                res = func(*args, **kwargs)",
            "            except UnavailableVideoError as e:",
            "                self.report_error(e)",
            "            except DownloadCancelled as e:",
            "                self.to_screen(f'[info] {e}')",
            "                if not self.params.get('break_per_url'):",
            "                    raise",
            "                self._num_downloads = 0",
            "            else:",
            "                if self.params.get('dump_single_json', False):",
            "                    self.post_extract(res)",
            "                    self.to_stdout(json.dumps(self.sanitize_info(res)))",
            "        return wrapper",
            "",
            "    def download(self, url_list):",
            "        \"\"\"Download a given list of URLs.\"\"\"",
            "        url_list = variadic(url_list)  # Passing a single URL is a common mistake",
            "        outtmpl = self.params['outtmpl']['default']",
            "        if (len(url_list) > 1",
            "                and outtmpl != '-'",
            "                and '%' not in outtmpl",
            "                and self.params.get('max_downloads') != 1):",
            "            raise SameFileError(outtmpl)",
            "",
            "        for url in url_list:",
            "            self.__download_wrapper(self.extract_info)(",
            "                url, force_generic_extractor=self.params.get('force_generic_extractor', False))",
            "",
            "        return self._download_retcode",
            "",
            "    def download_with_info_file(self, info_filename):",
            "        with contextlib.closing(fileinput.FileInput(",
            "                [info_filename], mode='r',",
            "                openhook=fileinput.hook_encoded('utf-8'))) as f:",
            "            # FileInput doesn't have a read method, we can't call json.load",
            "            infos = [self.sanitize_info(info, self.params.get('clean_infojson', True))",
            "                     for info in variadic(json.loads('\\n'.join(f)))]",
            "        for info in infos:",
            "            try:",
            "                self.__download_wrapper(self.process_ie_result)(info, download=True)",
            "            except (DownloadError, EntryNotInPlaylist, ReExtractInfo) as e:",
            "                if not isinstance(e, EntryNotInPlaylist):",
            "                    self.to_stderr('\\r')",
            "                webpage_url = info.get('webpage_url')",
            "                if webpage_url is None:",
            "                    raise",
            "                self.report_warning(f'The info failed to download: {e}; trying with URL {webpage_url}')",
            "                self.download([webpage_url])",
            "            except ExtractorError as e:",
            "                self.report_error(e)",
            "        return self._download_retcode",
            "",
            "    @staticmethod",
            "    def sanitize_info(info_dict, remove_private_keys=False):",
            "        ''' Sanitize the infodict for converting to json '''",
            "        if info_dict is None:",
            "            return info_dict",
            "        info_dict.setdefault('epoch', int(time.time()))",
            "        info_dict.setdefault('_type', 'video')",
            "        info_dict.setdefault('_version', {",
            "            'version': __version__,",
            "            'current_git_head': current_git_head(),",
            "            'release_git_head': RELEASE_GIT_HEAD,",
            "            'repository': ORIGIN,",
            "        })",
            "",
            "        if remove_private_keys:",
            "            reject = lambda k, v: v is None or k.startswith('__') or k in {",
            "                'requested_downloads', 'requested_formats', 'requested_subtitles', 'requested_entries',",
            "                'entries', 'filepath', '_filename', 'filename', 'infojson_filename', 'original_url',",
            "                'playlist_autonumber',",
            "            }",
            "        else:",
            "            reject = lambda k, v: False",
            "",
            "        def filter_fn(obj):",
            "            if isinstance(obj, dict):",
            "                return {k: filter_fn(v) for k, v in obj.items() if not reject(k, v)}",
            "            elif isinstance(obj, (list, tuple, set, LazyList)):",
            "                return list(map(filter_fn, obj))",
            "            elif obj is None or isinstance(obj, (str, int, float, bool)):",
            "                return obj",
            "            else:",
            "                return repr(obj)",
            "",
            "        return filter_fn(info_dict)",
            "",
            "    @staticmethod",
            "    def filter_requested_info(info_dict, actually_filter=True):",
            "        ''' Alias of sanitize_info for backward compatibility '''",
            "        return YoutubeDL.sanitize_info(info_dict, actually_filter)",
            "",
            "    def _delete_downloaded_files(self, *files_to_delete, info={}, msg=None):",
            "        for filename in set(filter(None, files_to_delete)):",
            "            if msg:",
            "                self.to_screen(msg % filename)",
            "            try:",
            "                os.remove(filename)",
            "            except OSError:",
            "                self.report_warning(f'Unable to delete file {filename}')",
            "            if filename in info.get('__files_to_move', []):  # NB: Delete even if None",
            "                del info['__files_to_move'][filename]",
            "",
            "    @staticmethod",
            "    def post_extract(info_dict):",
            "        def actual_post_extract(info_dict):",
            "            if info_dict.get('_type') in ('playlist', 'multi_video'):",
            "                for video_dict in info_dict.get('entries', {}):",
            "                    actual_post_extract(video_dict or {})",
            "                return",
            "",
            "            post_extractor = info_dict.pop('__post_extractor', None) or (lambda: {})",
            "            info_dict.update(post_extractor())",
            "",
            "        actual_post_extract(info_dict or {})",
            "",
            "    def run_pp(self, pp, infodict):",
            "        files_to_delete = []",
            "        if '__files_to_move' not in infodict:",
            "            infodict['__files_to_move'] = {}",
            "        try:",
            "            files_to_delete, infodict = pp.run(infodict)",
            "        except PostProcessingError as e:",
            "            # Must be True and not 'only_download'",
            "            if self.params.get('ignoreerrors') is True:",
            "                self.report_error(e)",
            "                return infodict",
            "            raise",
            "",
            "        if not files_to_delete:",
            "            return infodict",
            "        if self.params.get('keepvideo', False):",
            "            for f in files_to_delete:",
            "                infodict['__files_to_move'].setdefault(f, '')",
            "        else:",
            "            self._delete_downloaded_files(",
            "                *files_to_delete, info=infodict, msg='Deleting original file %s (pass -k to keep)')",
            "        return infodict",
            "",
            "    def run_all_pps(self, key, info, *, additional_pps=None):",
            "        if key != 'video':",
            "            self._forceprint(key, info)",
            "        for pp in (additional_pps or []) + self._pps[key]:",
            "            info = self.run_pp(pp, info)",
            "        return info",
            "",
            "    def pre_process(self, ie_info, key='pre_process', files_to_move=None):",
            "        info = dict(ie_info)",
            "        info['__files_to_move'] = files_to_move or {}",
            "        try:",
            "            info = self.run_all_pps(key, info)",
            "        except PostProcessingError as err:",
            "            msg = f'Preprocessing: {err}'",
            "            info.setdefault('__pending_error', msg)",
            "            self.report_error(msg, is_error=False)",
            "        return info, info.pop('__files_to_move', None)",
            "",
            "    def post_process(self, filename, info, files_to_move=None):",
            "        \"\"\"Run all the postprocessors on the given file.\"\"\"",
            "        info['filepath'] = filename",
            "        info['__files_to_move'] = files_to_move or {}",
            "        info = self.run_all_pps('post_process', info, additional_pps=info.get('__postprocessors'))",
            "        info = self.run_pp(MoveFilesAfterDownloadPP(self), info)",
            "        del info['__files_to_move']",
            "        return self.run_all_pps('after_move', info)",
            "",
            "    def _make_archive_id(self, info_dict):",
            "        video_id = info_dict.get('id')",
            "        if not video_id:",
            "            return",
            "        # Future-proof against any change in case",
            "        # and backwards compatibility with prior versions",
            "        extractor = info_dict.get('extractor_key') or info_dict.get('ie_key')  # key in a playlist",
            "        if extractor is None:",
            "            url = str_or_none(info_dict.get('url'))",
            "            if not url:",
            "                return",
            "            # Try to find matching extractor for the URL and take its ie_key",
            "            for ie_key, ie in self._ies.items():",
            "                if ie.suitable(url):",
            "                    extractor = ie_key",
            "                    break",
            "            else:",
            "                return",
            "        return make_archive_id(extractor, video_id)",
            "",
            "    def in_download_archive(self, info_dict):",
            "        if not self.archive:",
            "            return False",
            "",
            "        vid_ids = [self._make_archive_id(info_dict)]",
            "        vid_ids.extend(info_dict.get('_old_archive_ids') or [])",
            "        return any(id_ in self.archive for id_ in vid_ids)",
            "",
            "    def record_download_archive(self, info_dict):",
            "        fn = self.params.get('download_archive')",
            "        if fn is None:",
            "            return",
            "        vid_id = self._make_archive_id(info_dict)",
            "        assert vid_id",
            "",
            "        self.write_debug(f'Adding to archive: {vid_id}')",
            "        if is_path_like(fn):",
            "            with locked_file(fn, 'a', encoding='utf-8') as archive_file:",
            "                archive_file.write(vid_id + '\\n')",
            "        self.archive.add(vid_id)",
            "",
            "    @staticmethod",
            "    def format_resolution(format, default='unknown'):",
            "        if format.get('vcodec') == 'none' and format.get('acodec') != 'none':",
            "            return 'audio only'",
            "        if format.get('resolution') is not None:",
            "            return format['resolution']",
            "        if format.get('width') and format.get('height'):",
            "            return '%dx%d' % (format['width'], format['height'])",
            "        elif format.get('height'):",
            "            return '%sp' % format['height']",
            "        elif format.get('width'):",
            "            return '%dx?' % format['width']",
            "        return default",
            "",
            "    def _list_format_headers(self, *headers):",
            "        if self.params.get('listformats_table', True) is not False:",
            "            return [self._format_out(header, self.Styles.HEADERS) for header in headers]",
            "        return headers",
            "",
            "    def _format_note(self, fdict):",
            "        res = ''",
            "        if fdict.get('ext') in ['f4f', 'f4m']:",
            "            res += '(unsupported)'",
            "        if fdict.get('language'):",
            "            if res:",
            "                res += ' '",
            "            res += '[%s]' % fdict['language']",
            "        if fdict.get('format_note') is not None:",
            "            if res:",
            "                res += ' '",
            "            res += fdict['format_note']",
            "        if fdict.get('tbr') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += '%4dk' % fdict['tbr']",
            "        if fdict.get('container') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += '%s container' % fdict['container']",
            "        if (fdict.get('vcodec') is not None",
            "                and fdict.get('vcodec') != 'none'):",
            "            if res:",
            "                res += ', '",
            "            res += fdict['vcodec']",
            "            if fdict.get('vbr') is not None:",
            "                res += '@'",
            "        elif fdict.get('vbr') is not None and fdict.get('abr') is not None:",
            "            res += 'video@'",
            "        if fdict.get('vbr') is not None:",
            "            res += '%4dk' % fdict['vbr']",
            "        if fdict.get('fps') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += '%sfps' % fdict['fps']",
            "        if fdict.get('acodec') is not None:",
            "            if res:",
            "                res += ', '",
            "            if fdict['acodec'] == 'none':",
            "                res += 'video only'",
            "            else:",
            "                res += '%-5s' % fdict['acodec']",
            "        elif fdict.get('abr') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += 'audio'",
            "        if fdict.get('abr') is not None:",
            "            res += '@%3dk' % fdict['abr']",
            "        if fdict.get('asr') is not None:",
            "            res += ' (%5dHz)' % fdict['asr']",
            "        if fdict.get('filesize') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += format_bytes(fdict['filesize'])",
            "        elif fdict.get('filesize_approx') is not None:",
            "            if res:",
            "                res += ', '",
            "            res += '~' + format_bytes(fdict['filesize_approx'])",
            "        return res",
            "",
            "    def _get_formats(self, info_dict):",
            "        if info_dict.get('formats') is None:",
            "            if info_dict.get('url') and info_dict.get('_type', 'video') == 'video':",
            "                return [info_dict]",
            "            return []",
            "        return info_dict['formats']",
            "",
            "    def render_formats_table(self, info_dict):",
            "        formats = self._get_formats(info_dict)",
            "        if not formats:",
            "            return",
            "        if not self.params.get('listformats_table', True) is not False:",
            "            table = [",
            "                [",
            "                    format_field(f, 'format_id'),",
            "                    format_field(f, 'ext'),",
            "                    self.format_resolution(f),",
            "                    self._format_note(f)",
            "                ] for f in formats if (f.get('preference') or 0) >= -1000]",
            "            return render_table(['format code', 'extension', 'resolution', 'note'], table, extra_gap=1)",
            "",
            "        def simplified_codec(f, field):",
            "            assert field in ('acodec', 'vcodec')",
            "            codec = f.get(field)",
            "            if not codec:",
            "                return 'unknown'",
            "            elif codec != 'none':",
            "                return '.'.join(codec.split('.')[:4])",
            "",
            "            if field == 'vcodec' and f.get('acodec') == 'none':",
            "                return 'images'",
            "            elif field == 'acodec' and f.get('vcodec') == 'none':",
            "                return ''",
            "            return self._format_out('audio only' if field == 'vcodec' else 'video only',",
            "                                    self.Styles.SUPPRESS)",
            "",
            "        delim = self._format_out('\\u2502', self.Styles.DELIM, '|', test_encoding=True)",
            "        table = [",
            "            [",
            "                self._format_out(format_field(f, 'format_id'), self.Styles.ID),",
            "                format_field(f, 'ext'),",
            "                format_field(f, func=self.format_resolution, ignore=('audio only', 'images')),",
            "                format_field(f, 'fps', '\\t%d', func=round),",
            "                format_field(f, 'dynamic_range', '%s', ignore=(None, 'SDR')).replace('HDR', ''),",
            "                format_field(f, 'audio_channels', '\\t%s'),",
            "                delim, (",
            "                    format_field(f, 'filesize', ' \\t%s', func=format_bytes)",
            "                    or format_field(f, 'filesize_approx', '\u2248\\t%s', func=format_bytes)",
            "                    or format_field(filesize_from_tbr(f.get('tbr'), info_dict.get('duration')), None,",
            "                                    self._format_out('~\\t%s', self.Styles.SUPPRESS), func=format_bytes)),",
            "                format_field(f, 'tbr', '\\t%dk', func=round),",
            "                shorten_protocol_name(f.get('protocol', '')),",
            "                delim,",
            "                simplified_codec(f, 'vcodec'),",
            "                format_field(f, 'vbr', '\\t%dk', func=round),",
            "                simplified_codec(f, 'acodec'),",
            "                format_field(f, 'abr', '\\t%dk', func=round),",
            "                format_field(f, 'asr', '\\t%s', func=format_decimal_suffix),",
            "                join_nonempty(format_field(f, 'language', '[%s]'), join_nonempty(",
            "                    self._format_out('UNSUPPORTED', self.Styles.BAD_FORMAT) if f.get('ext') in ('f4f', 'f4m') else None,",
            "                    (self._format_out('Maybe DRM', self.Styles.WARNING) if f.get('has_drm') == 'maybe'",
            "                     else self._format_out('DRM', self.Styles.BAD_FORMAT) if f.get('has_drm') else None),",
            "                    format_field(f, 'format_note'),",
            "                    format_field(f, 'container', ignore=(None, f.get('ext'))),",
            "                    delim=', '), delim=' '),",
            "            ] for f in formats if f.get('preference') is None or f['preference'] >= -1000]",
            "        header_line = self._list_format_headers(",
            "            'ID', 'EXT', 'RESOLUTION', '\\tFPS', 'HDR', 'CH', delim, '\\tFILESIZE', '\\tTBR', 'PROTO',",
            "            delim, 'VCODEC', '\\tVBR', 'ACODEC', '\\tABR', '\\tASR', 'MORE INFO')",
            "",
            "        return render_table(",
            "            header_line, table, hide_empty=True,",
            "            delim=self._format_out('\\u2500', self.Styles.DELIM, '-', test_encoding=True))",
            "",
            "    def render_thumbnails_table(self, info_dict):",
            "        thumbnails = list(info_dict.get('thumbnails') or [])",
            "        if not thumbnails:",
            "            return None",
            "        return render_table(",
            "            self._list_format_headers('ID', 'Width', 'Height', 'URL'),",
            "            [[t.get('id'), t.get('width') or 'unknown', t.get('height') or 'unknown', t['url']] for t in thumbnails])",
            "",
            "    def render_subtitles_table(self, video_id, subtitles):",
            "        def _row(lang, formats):",
            "            exts, names = zip(*((f['ext'], f.get('name') or 'unknown') for f in reversed(formats)))",
            "            if len(set(names)) == 1:",
            "                names = [] if names[0] == 'unknown' else names[:1]",
            "            return [lang, ', '.join(names), ', '.join(exts)]",
            "",
            "        if not subtitles:",
            "            return None",
            "        return render_table(",
            "            self._list_format_headers('Language', 'Name', 'Formats'),",
            "            [_row(lang, formats) for lang, formats in subtitles.items()],",
            "            hide_empty=True)",
            "",
            "    def __list_table(self, video_id, name, func, *args):",
            "        table = func(*args)",
            "        if not table:",
            "            self.to_screen(f'{video_id} has no {name}')",
            "            return",
            "        self.to_screen(f'[info] Available {name} for {video_id}:')",
            "        self.to_stdout(table)",
            "",
            "    def list_formats(self, info_dict):",
            "        self.__list_table(info_dict['id'], 'formats', self.render_formats_table, info_dict)",
            "",
            "    def list_thumbnails(self, info_dict):",
            "        self.__list_table(info_dict['id'], 'thumbnails', self.render_thumbnails_table, info_dict)",
            "",
            "    def list_subtitles(self, video_id, subtitles, name='subtitles'):",
            "        self.__list_table(video_id, name, self.render_subtitles_table, video_id, subtitles)",
            "",
            "    def print_debug_header(self):",
            "        if not self.params.get('verbose'):",
            "            return",
            "",
            "        from . import _IN_CLI  # Must be delayed import",
            "",
            "        # These imports can be slow. So import them only as needed",
            "        from .extractor.extractors import _LAZY_LOADER",
            "        from .extractor.extractors import (",
            "            _PLUGIN_CLASSES as plugin_ies,",
            "            _PLUGIN_OVERRIDES as plugin_ie_overrides",
            "        )",
            "",
            "        def get_encoding(stream):",
            "            ret = str(getattr(stream, 'encoding', 'missing (%s)' % type(stream).__name__))",
            "            additional_info = []",
            "            if os.environ.get('TERM', '').lower() == 'dumb':",
            "                additional_info.append('dumb')",
            "            if not supports_terminal_sequences(stream):",
            "                from .utils import WINDOWS_VT_MODE  # Must be imported locally",
            "                additional_info.append('No VT' if WINDOWS_VT_MODE is False else 'No ANSI')",
            "            if additional_info:",
            "                ret = f'{ret} ({\",\".join(additional_info)})'",
            "            return ret",
            "",
            "        encoding_str = 'Encodings: locale %s, fs %s, pref %s, %s' % (",
            "            locale.getpreferredencoding(),",
            "            sys.getfilesystemencoding(),",
            "            self.get_encoding(),",
            "            ', '.join(",
            "                f'{key} {get_encoding(stream)}' for key, stream in self._out_files.items_",
            "                if stream is not None and key != 'console')",
            "        )",
            "",
            "        logger = self.params.get('logger')",
            "        if logger:",
            "            write_debug = lambda msg: logger.debug(f'[debug] {msg}')",
            "            write_debug(encoding_str)",
            "        else:",
            "            write_string(f'[debug] {encoding_str}\\n', encoding=None)",
            "            write_debug = lambda msg: self._write_string(f'[debug] {msg}\\n')",
            "",
            "        source = detect_variant()",
            "        if VARIANT not in (None, 'pip'):",
            "            source += '*'",
            "        klass = type(self)",
            "        write_debug(join_nonempty(",
            "            f'{REPOSITORY.rpartition(\"/\")[2]} version',",
            "            _make_label(ORIGIN, CHANNEL.partition('@')[2] or __version__, __version__),",
            "            f'[{RELEASE_GIT_HEAD[:9]}]' if RELEASE_GIT_HEAD else '',",
            "            '' if source == 'unknown' else f'({source})',",
            "            '' if _IN_CLI else 'API' if klass == YoutubeDL else f'API:{self.__module__}.{klass.__qualname__}',",
            "            delim=' '))",
            "",
            "        if not _IN_CLI:",
            "            write_debug(f'params: {self.params}')",
            "",
            "        if not _LAZY_LOADER:",
            "            if os.environ.get('YTDLP_NO_LAZY_EXTRACTORS'):",
            "                write_debug('Lazy loading extractors is forcibly disabled')",
            "            else:",
            "                write_debug('Lazy loading extractors is disabled')",
            "        if self.params['compat_opts']:",
            "            write_debug('Compatibility options: %s' % ', '.join(self.params['compat_opts']))",
            "",
            "        if current_git_head():",
            "            write_debug(f'Git HEAD: {current_git_head()}')",
            "        write_debug(system_identifier())",
            "",
            "        exe_versions, ffmpeg_features = FFmpegPostProcessor.get_versions_and_features(self)",
            "        ffmpeg_features = {key for key, val in ffmpeg_features.items() if val}",
            "        if ffmpeg_features:",
            "            exe_versions['ffmpeg'] += ' (%s)' % ','.join(sorted(ffmpeg_features))",
            "",
            "        exe_versions['rtmpdump'] = rtmpdump_version()",
            "        exe_versions['phantomjs'] = PhantomJSwrapper._version()",
            "        exe_str = ', '.join(",
            "            f'{exe} {v}' for exe, v in sorted(exe_versions.items()) if v",
            "        ) or 'none'",
            "        write_debug('exe versions: %s' % exe_str)",
            "",
            "        from .compat.compat_utils import get_package_info",
            "        from .dependencies import available_dependencies",
            "",
            "        write_debug('Optional libraries: %s' % (', '.join(sorted({",
            "            join_nonempty(*get_package_info(m)) for m in available_dependencies.values()",
            "        })) or 'none'))",
            "",
            "        write_debug(f'Proxy map: {self.proxies}')",
            "        write_debug(f'Request Handlers: {\", \".join(rh.RH_NAME for rh in self._request_director.handlers.values())}')",
            "        for plugin_type, plugins in {'Extractor': plugin_ies, 'Post-Processor': plugin_pps}.items():",
            "            display_list = ['%s%s' % (",
            "                klass.__name__, '' if klass.__name__ == name else f' as {name}')",
            "                for name, klass in plugins.items()]",
            "            if plugin_type == 'Extractor':",
            "                display_list.extend(f'{plugins[-1].IE_NAME.partition(\"+\")[2]} ({parent.__name__})'",
            "                                    for parent, plugins in plugin_ie_overrides.items())",
            "            if not display_list:",
            "                continue",
            "            write_debug(f'{plugin_type} Plugins: {\", \".join(sorted(display_list))}')",
            "",
            "        plugin_dirs = plugin_directories()",
            "        if plugin_dirs:",
            "            write_debug(f'Plugin directories: {plugin_dirs}')",
            "",
            "        # Not implemented",
            "        if False and self.params.get('call_home'):",
            "            ipaddr = self.urlopen('https://yt-dl.org/ip').read().decode()",
            "            write_debug('Public IP address: %s' % ipaddr)",
            "            latest_version = self.urlopen(",
            "                'https://yt-dl.org/latest/version').read().decode()",
            "            if version_tuple(latest_version) > version_tuple(__version__):",
            "                self.report_warning(",
            "                    'You are using an outdated version (newest version: %s)! '",
            "                    'See https://yt-dl.org/update if you need help updating.' %",
            "                    latest_version)",
            "",
            "    @functools.cached_property",
            "    def proxies(self):",
            "        \"\"\"Global proxy configuration\"\"\"",
            "        opts_proxy = self.params.get('proxy')",
            "        if opts_proxy is not None:",
            "            if opts_proxy == '':",
            "                opts_proxy = '__noproxy__'",
            "            proxies = {'all': opts_proxy}",
            "        else:",
            "            proxies = urllib.request.getproxies()",
            "            # compat. Set HTTPS_PROXY to __noproxy__ to revert",
            "            if 'http' in proxies and 'https' not in proxies:",
            "                proxies['https'] = proxies['http']",
            "",
            "        return proxies",
            "",
            "    @functools.cached_property",
            "    def cookiejar(self):",
            "        \"\"\"Global cookiejar instance\"\"\"",
            "        return load_cookies(",
            "            self.params.get('cookiefile'), self.params.get('cookiesfrombrowser'), self)",
            "",
            "    @property",
            "    def _opener(self):",
            "        \"\"\"",
            "        Get a urllib OpenerDirector from the Urllib handler (deprecated).",
            "        \"\"\"",
            "        self.deprecation_warning('YoutubeDL._opener is deprecated, use YoutubeDL.urlopen()')",
            "        handler = self._request_director.handlers['Urllib']",
            "        return handler._get_instance(cookiejar=self.cookiejar, proxies=self.proxies)",
            "",
            "    def _get_available_impersonate_targets(self):",
            "        # todo(future): make available as public API",
            "        return [",
            "            (target, rh.RH_NAME)",
            "            for rh in self._request_director.handlers.values()",
            "            if isinstance(rh, ImpersonateRequestHandler)",
            "            for target in rh.supported_targets",
            "        ]",
            "",
            "    def _impersonate_target_available(self, target):",
            "        # todo(future): make available as public API",
            "        return any(",
            "            rh.is_supported_target(target)",
            "            for rh in self._request_director.handlers.values()",
            "            if isinstance(rh, ImpersonateRequestHandler))",
            "",
            "    def urlopen(self, req):",
            "        \"\"\" Start an HTTP download \"\"\"",
            "        if isinstance(req, str):",
            "            req = Request(req)",
            "        elif isinstance(req, urllib.request.Request):",
            "            self.deprecation_warning(",
            "                'Passing a urllib.request.Request object to YoutubeDL.urlopen() is deprecated. '",
            "                'Use yt_dlp.networking.common.Request instead.')",
            "            req = urllib_req_to_req(req)",
            "        assert isinstance(req, Request)",
            "",
            "        # compat: Assume user:pass url params are basic auth",
            "        url, basic_auth_header = extract_basic_auth(req.url)",
            "        if basic_auth_header:",
            "            req.headers['Authorization'] = basic_auth_header",
            "        req.url = sanitize_url(url)",
            "",
            "        clean_proxies(proxies=req.proxies, headers=req.headers)",
            "        clean_headers(req.headers)",
            "",
            "        try:",
            "            return self._request_director.send(req)",
            "        except NoSupportingHandlers as e:",
            "            for ue in e.unsupported_errors:",
            "                # FIXME: This depends on the order of errors.",
            "                if not (ue.handler and ue.msg):",
            "                    continue",
            "                if ue.handler.RH_KEY == 'Urllib' and 'unsupported url scheme: \"file\"' in ue.msg.lower():",
            "                    raise RequestError(",
            "                        'file:// URLs are disabled by default in yt-dlp for security reasons. '",
            "                        'Use --enable-file-urls to enable at your own risk.', cause=ue) from ue",
            "                if (",
            "                    'unsupported proxy type: \"https\"' in ue.msg.lower()",
            "                    and 'requests' not in self._request_director.handlers",
            "                    and 'curl_cffi' not in self._request_director.handlers",
            "                ):",
            "                    raise RequestError(",
            "                        'To use an HTTPS proxy for this request, one of the following dependencies needs to be installed: requests, curl_cffi')",
            "",
            "                elif (",
            "                    re.match(r'unsupported url scheme: \"wss?\"', ue.msg.lower())",
            "                    and 'websockets' not in self._request_director.handlers",
            "                ):",
            "                    raise RequestError(",
            "                        'This request requires WebSocket support. '",
            "                        'Ensure one of the following dependencies are installed: websockets',",
            "                        cause=ue) from ue",
            "",
            "                elif re.match(r'unsupported (?:extensions: impersonate|impersonate target)', ue.msg.lower()):",
            "                    raise RequestError(",
            "                        f'Impersonate target \"{req.extensions[\"impersonate\"]}\" is not available.'",
            "                        f' See --list-impersonate-targets for available targets.'",
            "                        f' This request requires browser impersonation, however you may be missing dependencies'",
            "                        f' required to support this target.')",
            "            raise",
            "        except SSLError as e:",
            "            if 'UNSAFE_LEGACY_RENEGOTIATION_DISABLED' in str(e):",
            "                raise RequestError('UNSAFE_LEGACY_RENEGOTIATION_DISABLED: Try using --legacy-server-connect', cause=e) from e",
            "            elif 'SSLV3_ALERT_HANDSHAKE_FAILURE' in str(e):",
            "                raise RequestError(",
            "                    'SSLV3_ALERT_HANDSHAKE_FAILURE: The server may not support the current cipher list. '",
            "                    'Try using --legacy-server-connect', cause=e) from e",
            "            raise",
            "",
            "    def build_request_director(self, handlers, preferences=None):",
            "        logger = _YDLLogger(self)",
            "        headers = self.params['http_headers'].copy()",
            "        proxies = self.proxies.copy()",
            "        clean_headers(headers)",
            "        clean_proxies(proxies, headers)",
            "",
            "        director = RequestDirector(logger=logger, verbose=self.params.get('debug_printtraffic'))",
            "        for handler in handlers:",
            "            director.add_handler(handler(",
            "                logger=logger,",
            "                headers=headers,",
            "                cookiejar=self.cookiejar,",
            "                proxies=proxies,",
            "                prefer_system_certs='no-certifi' in self.params['compat_opts'],",
            "                verify=not self.params.get('nocheckcertificate'),",
            "                **traverse_obj(self.params, {",
            "                    'verbose': 'debug_printtraffic',",
            "                    'source_address': 'source_address',",
            "                    'timeout': 'socket_timeout',",
            "                    'legacy_ssl_support': 'legacyserverconnect',",
            "                    'enable_file_urls': 'enable_file_urls',",
            "                    'impersonate': 'impersonate',",
            "                    'client_cert': {",
            "                        'client_certificate': 'client_certificate',",
            "                        'client_certificate_key': 'client_certificate_key',",
            "                        'client_certificate_password': 'client_certificate_password',",
            "                    },",
            "                }),",
            "            ))",
            "        director.preferences.update(preferences or [])",
            "        if 'prefer-legacy-http-handler' in self.params['compat_opts']:",
            "            director.preferences.add(lambda rh, _: 500 if rh.RH_KEY == 'Urllib' else 0)",
            "        return director",
            "",
            "    @functools.cached_property",
            "    def _request_director(self):",
            "        return self.build_request_director(_REQUEST_HANDLERS.values(), _RH_PREFERENCES)",
            "",
            "    def encode(self, s):",
            "        if isinstance(s, bytes):",
            "            return s  # Already encoded",
            "",
            "        try:",
            "            return s.encode(self.get_encoding())",
            "        except UnicodeEncodeError as err:",
            "            err.reason = err.reason + '. Check your system encoding configuration or use the --encoding option.'",
            "            raise",
            "",
            "    def get_encoding(self):",
            "        encoding = self.params.get('encoding')",
            "        if encoding is None:",
            "            encoding = preferredencoding()",
            "        return encoding",
            "",
            "    def _write_info_json(self, label, ie_result, infofn, overwrite=None):",
            "        ''' Write infojson and returns True = written, 'exists' = Already exists, False = skip, None = error '''",
            "        if overwrite is None:",
            "            overwrite = self.params.get('overwrites', True)",
            "        if not self.params.get('writeinfojson'):",
            "            return False",
            "        elif not infofn:",
            "            self.write_debug(f'Skipping writing {label} infojson')",
            "            return False",
            "        elif not self._ensure_dir_exists(infofn):",
            "            return None",
            "        elif not overwrite and os.path.exists(infofn):",
            "            self.to_screen(f'[info] {label.title()} metadata is already present')",
            "            return 'exists'",
            "",
            "        self.to_screen(f'[info] Writing {label} metadata as JSON to: {infofn}')",
            "        try:",
            "            write_json_file(self.sanitize_info(ie_result, self.params.get('clean_infojson', True)), infofn)",
            "            return True",
            "        except OSError:",
            "            self.report_error(f'Cannot write {label} metadata to JSON file {infofn}')",
            "            return None",
            "",
            "    def _write_description(self, label, ie_result, descfn):",
            "        ''' Write description and returns True = written, False = skip, None = error '''",
            "        if not self.params.get('writedescription'):",
            "            return False",
            "        elif not descfn:",
            "            self.write_debug(f'Skipping writing {label} description')",
            "            return False",
            "        elif not self._ensure_dir_exists(descfn):",
            "            return None",
            "        elif not self.params.get('overwrites', True) and os.path.exists(descfn):",
            "            self.to_screen(f'[info] {label.title()} description is already present')",
            "        elif ie_result.get('description') is None:",
            "            self.to_screen(f'[info] There\\'s no {label} description to write')",
            "            return False",
            "        else:",
            "            try:",
            "                self.to_screen(f'[info] Writing {label} description to: {descfn}')",
            "                with open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:",
            "                    descfile.write(ie_result['description'])",
            "            except OSError:",
            "                self.report_error(f'Cannot write {label} description file {descfn}')",
            "                return None",
            "        return True",
            "",
            "    def _write_subtitles(self, info_dict, filename):",
            "        ''' Write subtitles to file and return list of (sub_filename, final_sub_filename); or None if error'''",
            "        ret = []",
            "        subtitles = info_dict.get('requested_subtitles')",
            "        if not (self.params.get('writesubtitles') or self.params.get('writeautomaticsub')):",
            "            # subtitles download errors are already managed as troubles in relevant IE",
            "            # that way it will silently go on when used with unsupporting IE",
            "            return ret",
            "        elif not subtitles:",
            "            self.to_screen('[info] There are no subtitles for the requested languages')",
            "            return ret",
            "        sub_filename_base = self.prepare_filename(info_dict, 'subtitle')",
            "        if not sub_filename_base:",
            "            self.to_screen('[info] Skipping writing video subtitles')",
            "            return ret",
            "",
            "        for sub_lang, sub_info in subtitles.items():",
            "            sub_format = sub_info['ext']",
            "            sub_filename = subtitles_filename(filename, sub_lang, sub_format, info_dict.get('ext'))",
            "            sub_filename_final = subtitles_filename(sub_filename_base, sub_lang, sub_format, info_dict.get('ext'))",
            "            existing_sub = self.existing_file((sub_filename_final, sub_filename))",
            "            if existing_sub:",
            "                self.to_screen(f'[info] Video subtitle {sub_lang}.{sub_format} is already present')",
            "                sub_info['filepath'] = existing_sub",
            "                ret.append((existing_sub, sub_filename_final))",
            "                continue",
            "",
            "            self.to_screen(f'[info] Writing video subtitles to: {sub_filename}')",
            "            if sub_info.get('data') is not None:",
            "                try:",
            "                    # Use newline='' to prevent conversion of newline characters",
            "                    # See https://github.com/ytdl-org/youtube-dl/issues/10268",
            "                    with open(sub_filename, 'w', encoding='utf-8', newline='') as subfile:",
            "                        subfile.write(sub_info['data'])",
            "                    sub_info['filepath'] = sub_filename",
            "                    ret.append((sub_filename, sub_filename_final))",
            "                    continue",
            "                except OSError:",
            "                    self.report_error(f'Cannot write video subtitles file {sub_filename}')",
            "                    return None",
            "",
            "            try:",
            "                sub_copy = sub_info.copy()",
            "                sub_copy.setdefault('http_headers', info_dict.get('http_headers'))",
            "                self.dl(sub_filename, sub_copy, subtitle=True)",
            "                sub_info['filepath'] = sub_filename",
            "                ret.append((sub_filename, sub_filename_final))",
            "            except (DownloadError, ExtractorError, IOError, OSError, ValueError) + network_exceptions as err:",
            "                msg = f'Unable to download video subtitles for {sub_lang!r}: {err}'",
            "                if self.params.get('ignoreerrors') is not True:  # False or 'only_download'",
            "                    if not self.params.get('ignoreerrors'):",
            "                        self.report_error(msg)",
            "                    raise DownloadError(msg)",
            "                self.report_warning(msg)",
            "        return ret",
            "",
            "    def _write_thumbnails(self, label, info_dict, filename, thumb_filename_base=None):",
            "        ''' Write thumbnails to file and return list of (thumb_filename, final_thumb_filename); or None if error '''",
            "        write_all = self.params.get('write_all_thumbnails', False)",
            "        thumbnails, ret = [], []",
            "        if write_all or self.params.get('writethumbnail', False):",
            "            thumbnails = info_dict.get('thumbnails') or []",
            "            if not thumbnails:",
            "                self.to_screen(f'[info] There are no {label} thumbnails to download')",
            "                return ret",
            "        multiple = write_all and len(thumbnails) > 1",
            "",
            "        if thumb_filename_base is None:",
            "            thumb_filename_base = filename",
            "        if thumbnails and not thumb_filename_base:",
            "            self.write_debug(f'Skipping writing {label} thumbnail')",
            "            return ret",
            "",
            "        if thumbnails and not self._ensure_dir_exists(filename):",
            "            return None",
            "",
            "        for idx, t in list(enumerate(thumbnails))[::-1]:",
            "            thumb_ext = (f'{t[\"id\"]}.' if multiple else '') + determine_ext(t['url'], 'jpg')",
            "            thumb_display_id = f'{label} thumbnail {t[\"id\"]}'",
            "            thumb_filename = replace_extension(filename, thumb_ext, info_dict.get('ext'))",
            "            thumb_filename_final = replace_extension(thumb_filename_base, thumb_ext, info_dict.get('ext'))",
            "",
            "            existing_thumb = self.existing_file((thumb_filename_final, thumb_filename))",
            "            if existing_thumb:",
            "                self.to_screen('[info] %s is already present' % (",
            "                    thumb_display_id if multiple else f'{label} thumbnail').capitalize())",
            "                t['filepath'] = existing_thumb",
            "                ret.append((existing_thumb, thumb_filename_final))",
            "            else:",
            "                self.to_screen(f'[info] Downloading {thumb_display_id} ...')",
            "                try:",
            "                    uf = self.urlopen(Request(t['url'], headers=t.get('http_headers', {})))",
            "                    self.to_screen(f'[info] Writing {thumb_display_id} to: {thumb_filename}')",
            "                    with open(encodeFilename(thumb_filename), 'wb') as thumbf:",
            "                        shutil.copyfileobj(uf, thumbf)",
            "                    ret.append((thumb_filename, thumb_filename_final))",
            "                    t['filepath'] = thumb_filename",
            "                except network_exceptions as err:",
            "                    if isinstance(err, HTTPError) and err.status == 404:",
            "                        self.to_screen(f'[info] {thumb_display_id.title()} does not exist')",
            "                    else:",
            "                        self.report_warning(f'Unable to download {thumb_display_id}: {err}')",
            "                    thumbnails.pop(idx)",
            "            if ret and not write_all:",
            "                break",
            "        return ret"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "28": [],
            "105": [],
            "826": [
                "YoutubeDL",
                "warn_if_short_id"
            ],
            "1358": [
                "YoutubeDL",
                "prepare_outtmpl",
                "create_key"
            ]
        },
        "addLocation": []
    },
    "yt_dlp/compat/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " compat_os_name = os._name if os.name == 'java' else os.name"
            },
            "1": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-if compat_os_name == 'nt':"
            },
            "4": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def compat_shlex_quote(s):"
            },
            "5": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        import re"
            },
            "6": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return s if re.match(r'^[-_\\w./]+$', s) else s.replace('\"', '\"\"').join('\"\"')"
            },
            "7": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-else:"
            },
            "8": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    from shlex import quote as compat_shlex_quote  # noqa: F401"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+def compat_shlex_quote(s):"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+    from ..utils import shell_quote"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+    return shell_quote(s)"
            },
            "12": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " def compat_ord(c):"
            }
        },
        "frontPatchFile": [
            "import os",
            "import sys",
            "import xml.etree.ElementTree as etree",
            "",
            "from .compat_utils import passthrough_module",
            "",
            "passthrough_module(__name__, '._deprecated')",
            "del passthrough_module",
            "",
            "",
            "# HTMLParseError has been deprecated in Python 3.3 and removed in",
            "# Python 3.5. Introducing dummy exception for Python >3.5 for compatible",
            "# and uniform cross-version exception handling",
            "class compat_HTMLParseError(ValueError):",
            "    pass",
            "",
            "",
            "class _TreeBuilder(etree.TreeBuilder):",
            "    def doctype(self, name, pubid, system):",
            "        pass",
            "",
            "",
            "def compat_etree_fromstring(text):",
            "    return etree.XML(text, parser=etree.XMLParser(target=_TreeBuilder()))",
            "",
            "",
            "compat_os_name = os._name if os.name == 'java' else os.name",
            "",
            "",
            "if compat_os_name == 'nt':",
            "    def compat_shlex_quote(s):",
            "        import re",
            "        return s if re.match(r'^[-_\\w./]+$', s) else s.replace('\"', '\"\"').join('\"\"')",
            "else:",
            "    from shlex import quote as compat_shlex_quote  # noqa: F401",
            "",
            "",
            "def compat_ord(c):",
            "    return c if isinstance(c, int) else ord(c)",
            "",
            "",
            "if compat_os_name == 'nt' and sys.version_info < (3, 8):",
            "    # os.path.realpath on Windows does not follow symbolic links",
            "    # prior to Python 3.8 (see https://bugs.python.org/issue9949)",
            "    def compat_realpath(path):",
            "        while os.path.islink(path):",
            "            path = os.path.abspath(os.readlink(path))",
            "        return os.path.realpath(path)",
            "else:",
            "    compat_realpath = os.path.realpath",
            "",
            "",
            "# Python 3.8+ does not honor %HOME% on windows, but this breaks compatibility with youtube-dl",
            "# See https://github.com/yt-dlp/yt-dlp/issues/792",
            "# https://docs.python.org/3/library/os.path.html#os.path.expanduser",
            "if compat_os_name in ('nt', 'ce'):",
            "    def compat_expanduser(path):",
            "        HOME = os.environ.get('HOME')",
            "        if not HOME:",
            "            return os.path.expanduser(path)",
            "        elif not path.startswith('~'):",
            "            return path",
            "        i = path.replace('\\\\', '/', 1).find('/')  # ~user",
            "        if i < 0:",
            "            i = len(path)",
            "        userhome = os.path.join(os.path.dirname(HOME), path[1:i]) if i > 1 else HOME",
            "        return userhome + path[i:]",
            "else:",
            "    compat_expanduser = os.path.expanduser",
            "",
            "",
            "def urllib_req_to_req(urllib_request):",
            "    \"\"\"Convert urllib Request to a networking Request\"\"\"",
            "    from ..networking import Request",
            "    from ..utils.networking import HTTPHeaderDict",
            "    return Request(",
            "        urllib_request.get_full_url(), data=urllib_request.data, method=urllib_request.get_method(),",
            "        headers=HTTPHeaderDict(urllib_request.headers, urllib_request.unredirected_hdrs),",
            "        extensions={'timeout': urllib_request.timeout} if hasattr(urllib_request, 'timeout') else None)"
        ],
        "afterPatchFile": [
            "import os",
            "import sys",
            "import xml.etree.ElementTree as etree",
            "",
            "from .compat_utils import passthrough_module",
            "",
            "passthrough_module(__name__, '._deprecated')",
            "del passthrough_module",
            "",
            "",
            "# HTMLParseError has been deprecated in Python 3.3 and removed in",
            "# Python 3.5. Introducing dummy exception for Python >3.5 for compatible",
            "# and uniform cross-version exception handling",
            "class compat_HTMLParseError(ValueError):",
            "    pass",
            "",
            "",
            "class _TreeBuilder(etree.TreeBuilder):",
            "    def doctype(self, name, pubid, system):",
            "        pass",
            "",
            "",
            "def compat_etree_fromstring(text):",
            "    return etree.XML(text, parser=etree.XMLParser(target=_TreeBuilder()))",
            "",
            "",
            "compat_os_name = os._name if os.name == 'java' else os.name",
            "",
            "",
            "def compat_shlex_quote(s):",
            "    from ..utils import shell_quote",
            "    return shell_quote(s)",
            "",
            "",
            "def compat_ord(c):",
            "    return c if isinstance(c, int) else ord(c)",
            "",
            "",
            "if compat_os_name == 'nt' and sys.version_info < (3, 8):",
            "    # os.path.realpath on Windows does not follow symbolic links",
            "    # prior to Python 3.8 (see https://bugs.python.org/issue9949)",
            "    def compat_realpath(path):",
            "        while os.path.islink(path):",
            "            path = os.path.abspath(os.readlink(path))",
            "        return os.path.realpath(path)",
            "else:",
            "    compat_realpath = os.path.realpath",
            "",
            "",
            "# Python 3.8+ does not honor %HOME% on windows, but this breaks compatibility with youtube-dl",
            "# See https://github.com/yt-dlp/yt-dlp/issues/792",
            "# https://docs.python.org/3/library/os.path.html#os.path.expanduser",
            "if compat_os_name in ('nt', 'ce'):",
            "    def compat_expanduser(path):",
            "        HOME = os.environ.get('HOME')",
            "        if not HOME:",
            "            return os.path.expanduser(path)",
            "        elif not path.startswith('~'):",
            "            return path",
            "        i = path.replace('\\\\', '/', 1).find('/')  # ~user",
            "        if i < 0:",
            "            i = len(path)",
            "        userhome = os.path.join(os.path.dirname(HOME), path[1:i]) if i > 1 else HOME",
            "        return userhome + path[i:]",
            "else:",
            "    compat_expanduser = os.path.expanduser",
            "",
            "",
            "def urllib_req_to_req(urllib_request):",
            "    \"\"\"Convert urllib Request to a networking Request\"\"\"",
            "    from ..networking import Request",
            "    from ..utils.networking import HTTPHeaderDict",
            "    return Request(",
            "        urllib_request.get_full_url(), data=urllib_request.data, method=urllib_request.get_method(),",
            "        headers=HTTPHeaderDict(urllib_request.headers, urllib_request.unredirected_hdrs),",
            "        extensions={'timeout': urllib_request.timeout} if hasattr(urllib_request, 'timeout') else None)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "30": [],
            "31": [
                "compat_shlex_quote"
            ],
            "32": [
                "compat_shlex_quote"
            ],
            "33": [
                "compat_shlex_quote"
            ],
            "34": [],
            "35": []
        },
        "addLocation": []
    },
    "yt_dlp/utils/_utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "     compat_expanduser,"
            },
            "1": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "     compat_HTMLParseError,"
            },
            "2": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "     compat_os_name,"
            },
            "3": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    compat_shlex_quote,"
            },
            "4": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " )"
            },
            "5": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " from ..dependencies import xattr"
            },
            "6": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 55,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 836,
                "afterPatchRowNumber": 835,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 837,
                "afterPatchRowNumber": 836,
                "PatchRowcode": "         if shell and compat_os_name == 'nt' and kwargs.get('executable') is None:"
            },
            "9": {
                "beforePatchRowNumber": 838,
                "afterPatchRowNumber": 837,
                "PatchRowcode": "             if not isinstance(args, str):"
            },
            "10": {
                "beforePatchRowNumber": 839,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                args = ' '.join(compat_shlex_quote(a) for a in args)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 838,
                "PatchRowcode": "+                args = shell_quote(args, shell=True)"
            },
            "12": {
                "beforePatchRowNumber": 840,
                "afterPatchRowNumber": 839,
                "PatchRowcode": "             shell = False"
            },
            "13": {
                "beforePatchRowNumber": 841,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            args = f'{self.__comspec()} /Q /S /D /V:OFF /C \"{args}\"'"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 840,
                "PatchRowcode": "+            # Set variable for `cmd.exe` newline escaping (see `utils.shell_quote`)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 841,
                "PatchRowcode": "+            env['='] = '\"^\\n\\n\"'"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 842,
                "PatchRowcode": "+            args = f'{self.__comspec()} /Q /S /D /V:OFF /E:ON /C \"{args}\"'"
            },
            "17": {
                "beforePatchRowNumber": 842,
                "afterPatchRowNumber": 843,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 843,
                "afterPatchRowNumber": 844,
                "PatchRowcode": "         super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)"
            },
            "19": {
                "beforePatchRowNumber": 844,
                "afterPatchRowNumber": 845,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 1637,
                "afterPatchRowNumber": 1638,
                "PatchRowcode": "     return encoding if encoding is not None else 'utf-8'"
            },
            "21": {
                "beforePatchRowNumber": 1638,
                "afterPatchRowNumber": 1639,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 1639,
                "afterPatchRowNumber": 1640,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 1640,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def shell_quote(args):"
            },
            "24": {
                "beforePatchRowNumber": 1641,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    quoted_args = []"
            },
            "25": {
                "beforePatchRowNumber": 1642,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    encoding = get_filesystem_encoding()"
            },
            "26": {
                "beforePatchRowNumber": 1643,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for a in args:"
            },
            "27": {
                "beforePatchRowNumber": 1644,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if isinstance(a, bytes):"
            },
            "28": {
                "beforePatchRowNumber": 1645,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # We may get a filename encoded with 'encodeFilename'"
            },
            "29": {
                "beforePatchRowNumber": 1646,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            a = a.decode(encoding)"
            },
            "30": {
                "beforePatchRowNumber": 1647,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        quoted_args.append(compat_shlex_quote(a))"
            },
            "31": {
                "beforePatchRowNumber": 1648,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return ' '.join(quoted_args)"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1641,
                "PatchRowcode": "+_WINDOWS_QUOTE_TRANS = str.maketrans({'\"': '\\\\\"', '\\\\': '\\\\\\\\'})"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1642,
                "PatchRowcode": "+_CMD_QUOTE_TRANS = str.maketrans({"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1643,
                "PatchRowcode": "+    # Keep quotes balanced by replacing them with `\"\"` instead of `\\\\\"`"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1644,
                "PatchRowcode": "+    '\"': '\"\"',"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1645,
                "PatchRowcode": "+    # Requires a variable `=` containing `\"^\\n\\n\"` (set in `utils.Popen`)"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1646,
                "PatchRowcode": "+    # `=` should be unique since variables containing `=` cannot be set using cmd"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1647,
                "PatchRowcode": "+    '\\n': '%=%',"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1648,
                "PatchRowcode": "+    # While we are only required to escape backslashes immediately before quotes,"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1649,
                "PatchRowcode": "+    # we instead escape all of 'em anyways to be consistent"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1650,
                "PatchRowcode": "+    '\\\\': '\\\\\\\\',"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1651,
                "PatchRowcode": "+    # Use zero length variable replacement so `%` doesn't get expanded"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1652,
                "PatchRowcode": "+    # `cd` is always set as long as extensions are enabled (`/E:ON` in `utils.Popen`)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1653,
                "PatchRowcode": "+    '%': '%%cd:~,%',"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1654,
                "PatchRowcode": "+})"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1655,
                "PatchRowcode": "+"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1656,
                "PatchRowcode": "+"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1657,
                "PatchRowcode": "+def shell_quote(args, *, shell=False):"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1658,
                "PatchRowcode": "+    args = list(variadic(args))"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1659,
                "PatchRowcode": "+    if any(isinstance(item, bytes) for item in args):"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1660,
                "PatchRowcode": "+        deprecation_warning('Passing bytes to utils.shell_quote is deprecated')"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1661,
                "PatchRowcode": "+        encoding = get_filesystem_encoding()"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1662,
                "PatchRowcode": "+        for index, item in enumerate(args):"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1663,
                "PatchRowcode": "+            if isinstance(item, bytes):"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1664,
                "PatchRowcode": "+                args[index] = item.decode(encoding)"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1665,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1666,
                "PatchRowcode": "+    if compat_os_name != 'nt':"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1667,
                "PatchRowcode": "+        return shlex.join(args)"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1668,
                "PatchRowcode": "+"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1669,
                "PatchRowcode": "+    trans = _CMD_QUOTE_TRANS if shell else _WINDOWS_QUOTE_TRANS"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1670,
                "PatchRowcode": "+    return ' '.join("
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1671,
                "PatchRowcode": "+        s if re.fullmatch(r'[\\w#$*\\-+./:?@\\\\]+', s, re.ASCII) else s.translate(trans).join('\"\"')"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1672,
                "PatchRowcode": "+        for s in args)"
            },
            "64": {
                "beforePatchRowNumber": 1649,
                "afterPatchRowNumber": 1673,
                "PatchRowcode": " "
            },
            "65": {
                "beforePatchRowNumber": 1650,
                "afterPatchRowNumber": 1674,
                "PatchRowcode": " "
            },
            "66": {
                "beforePatchRowNumber": 1651,
                "afterPatchRowNumber": 1675,
                "PatchRowcode": " def smuggle_url(url, data):"
            },
            "67": {
                "beforePatchRowNumber": 2849,
                "afterPatchRowNumber": 2873,
                "PatchRowcode": " "
            },
            "68": {
                "beforePatchRowNumber": 2850,
                "afterPatchRowNumber": 2874,
                "PatchRowcode": " def args_to_str(args):"
            },
            "69": {
                "beforePatchRowNumber": 2851,
                "afterPatchRowNumber": 2875,
                "PatchRowcode": "     # Get a short string representation for a subprocess command"
            },
            "70": {
                "beforePatchRowNumber": 2852,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return ' '.join(compat_shlex_quote(a) for a in args)"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2876,
                "PatchRowcode": "+    return shell_quote(args)"
            },
            "72": {
                "beforePatchRowNumber": 2853,
                "afterPatchRowNumber": 2877,
                "PatchRowcode": " "
            },
            "73": {
                "beforePatchRowNumber": 2854,
                "afterPatchRowNumber": 2878,
                "PatchRowcode": " "
            },
            "74": {
                "beforePatchRowNumber": 2855,
                "afterPatchRowNumber": 2879,
                "PatchRowcode": " def error_to_str(err):"
            }
        },
        "frontPatchFile": [
            "import base64",
            "import binascii",
            "import calendar",
            "import codecs",
            "import collections",
            "import collections.abc",
            "import contextlib",
            "import datetime as dt",
            "import email.header",
            "import email.utils",
            "import errno",
            "import hashlib",
            "import hmac",
            "import html.entities",
            "import html.parser",
            "import inspect",
            "import io",
            "import itertools",
            "import json",
            "import locale",
            "import math",
            "import mimetypes",
            "import netrc",
            "import operator",
            "import os",
            "import platform",
            "import random",
            "import re",
            "import shlex",
            "import socket",
            "import ssl",
            "import struct",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import traceback",
            "import types",
            "import unicodedata",
            "import urllib.error",
            "import urllib.parse",
            "import urllib.request",
            "import xml.etree.ElementTree",
            "",
            "from . import traversal",
            "",
            "from ..compat import functools  # isort: split",
            "from ..compat import (",
            "    compat_etree_fromstring,",
            "    compat_expanduser,",
            "    compat_HTMLParseError,",
            "    compat_os_name,",
            "    compat_shlex_quote,",
            ")",
            "from ..dependencies import xattr",
            "",
            "__name__ = __name__.rsplit('.', 1)[0]  # Pretend to be the parent module",
            "",
            "# This is not clearly defined otherwise",
            "compiled_regex_type = type(re.compile(''))",
            "",
            "",
            "class NO_DEFAULT:",
            "    pass",
            "",
            "",
            "def IDENTITY(x):",
            "    return x",
            "",
            "",
            "ENGLISH_MONTH_NAMES = [",
            "    'January', 'February', 'March', 'April', 'May', 'June',",
            "    'July', 'August', 'September', 'October', 'November', 'December']",
            "",
            "MONTH_NAMES = {",
            "    'en': ENGLISH_MONTH_NAMES,",
            "    'fr': [",
            "        'janvier', 'f\u00e9vrier', 'mars', 'avril', 'mai', 'juin',",
            "        'juillet', 'ao\u00fbt', 'septembre', 'octobre', 'novembre', 'd\u00e9cembre'],",
            "    # these follow the genitive grammatical case (dope\u0142niacz)",
            "    # some websites might be using nominative, which will require another month list",
            "    # https://en.wikibooks.org/wiki/Polish/Noun_cases",
            "    'pl': ['stycznia', 'lutego', 'marca', 'kwietnia', 'maja', 'czerwca',",
            "           'lipca', 'sierpnia', 'wrze\u015bnia', 'pa\u017adziernika', 'listopada', 'grudnia'],",
            "}",
            "",
            "# From https://github.com/python/cpython/blob/3.11/Lib/email/_parseaddr.py#L36-L42",
            "TIMEZONE_NAMES = {",
            "    'UT': 0, 'UTC': 0, 'GMT': 0, 'Z': 0,",
            "    'AST': -4, 'ADT': -3,  # Atlantic (used in Canada)",
            "    'EST': -5, 'EDT': -4,  # Eastern",
            "    'CST': -6, 'CDT': -5,  # Central",
            "    'MST': -7, 'MDT': -6,  # Mountain",
            "    'PST': -8, 'PDT': -7   # Pacific",
            "}",
            "",
            "# needed for sanitizing filenames in restricted mode",
            "ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',",
            "                        itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],",
            "                                        'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))",
            "",
            "DATE_FORMATS = (",
            "    '%d %B %Y',",
            "    '%d %b %Y',",
            "    '%B %d %Y',",
            "    '%B %dst %Y',",
            "    '%B %dnd %Y',",
            "    '%B %drd %Y',",
            "    '%B %dth %Y',",
            "    '%b %d %Y',",
            "    '%b %dst %Y',",
            "    '%b %dnd %Y',",
            "    '%b %drd %Y',",
            "    '%b %dth %Y',",
            "    '%b %dst %Y %I:%M',",
            "    '%b %dnd %Y %I:%M',",
            "    '%b %drd %Y %I:%M',",
            "    '%b %dth %Y %I:%M',",
            "    '%Y %m %d',",
            "    '%Y-%m-%d',",
            "    '%Y.%m.%d.',",
            "    '%Y/%m/%d',",
            "    '%Y/%m/%d %H:%M',",
            "    '%Y/%m/%d %H:%M:%S',",
            "    '%Y%m%d%H%M',",
            "    '%Y%m%d%H%M%S',",
            "    '%Y%m%d',",
            "    '%Y-%m-%d %H:%M',",
            "    '%Y-%m-%d %H:%M:%S',",
            "    '%Y-%m-%d %H:%M:%S.%f',",
            "    '%Y-%m-%d %H:%M:%S:%f',",
            "    '%d.%m.%Y %H:%M',",
            "    '%d.%m.%Y %H.%M',",
            "    '%Y-%m-%dT%H:%M:%SZ',",
            "    '%Y-%m-%dT%H:%M:%S.%fZ',",
            "    '%Y-%m-%dT%H:%M:%S.%f0Z',",
            "    '%Y-%m-%dT%H:%M:%S',",
            "    '%Y-%m-%dT%H:%M:%S.%f',",
            "    '%Y-%m-%dT%H:%M',",
            "    '%b %d %Y at %H:%M',",
            "    '%b %d %Y at %H:%M:%S',",
            "    '%B %d %Y at %H:%M',",
            "    '%B %d %Y at %H:%M:%S',",
            "    '%H:%M %d-%b-%Y',",
            ")",
            "",
            "DATE_FORMATS_DAY_FIRST = list(DATE_FORMATS)",
            "DATE_FORMATS_DAY_FIRST.extend([",
            "    '%d-%m-%Y',",
            "    '%d.%m.%Y',",
            "    '%d.%m.%y',",
            "    '%d/%m/%Y',",
            "    '%d/%m/%y',",
            "    '%d/%m/%Y %H:%M:%S',",
            "    '%d-%m-%Y %H:%M',",
            "    '%H:%M %d/%m/%Y',",
            "])",
            "",
            "DATE_FORMATS_MONTH_FIRST = list(DATE_FORMATS)",
            "DATE_FORMATS_MONTH_FIRST.extend([",
            "    '%m-%d-%Y',",
            "    '%m.%d.%Y',",
            "    '%m/%d/%Y',",
            "    '%m/%d/%y',",
            "    '%m/%d/%Y %H:%M:%S',",
            "])",
            "",
            "PACKED_CODES_RE = r\"}\\('(.+)',(\\d+),(\\d+),'([^']+)'\\.split\\('\\|'\\)\"",
            "JSON_LD_RE = r'(?is)<script[^>]+type=([\"\\']?)application/ld\\+json\\1[^>]*>\\s*(?P<json_ld>{.+?}|\\[.+?\\])\\s*</script>'",
            "",
            "NUMBER_RE = r'\\d+(?:\\.\\d+)?'",
            "",
            "",
            "@functools.cache",
            "def preferredencoding():",
            "    \"\"\"Get preferred encoding.",
            "",
            "    Returns the best encoding scheme for the system, based on",
            "    locale.getpreferredencoding() and some further tweaks.",
            "    \"\"\"",
            "    try:",
            "        pref = locale.getpreferredencoding()",
            "        'TEST'.encode(pref)",
            "    except Exception:",
            "        pref = 'UTF-8'",
            "",
            "    return pref",
            "",
            "",
            "def write_json_file(obj, fn):",
            "    \"\"\" Encode obj as JSON and write it to fn, atomically if possible \"\"\"",
            "",
            "    tf = tempfile.NamedTemporaryFile(",
            "        prefix=f'{os.path.basename(fn)}.', dir=os.path.dirname(fn),",
            "        suffix='.tmp', delete=False, mode='w', encoding='utf-8')",
            "",
            "    try:",
            "        with tf:",
            "            json.dump(obj, tf, ensure_ascii=False)",
            "        if sys.platform == 'win32':",
            "            # Need to remove existing file on Windows, else os.rename raises",
            "            # WindowsError or FileExistsError.",
            "            with contextlib.suppress(OSError):",
            "                os.unlink(fn)",
            "        with contextlib.suppress(OSError):",
            "            mask = os.umask(0)",
            "            os.umask(mask)",
            "            os.chmod(tf.name, 0o666 & ~mask)",
            "        os.rename(tf.name, fn)",
            "    except Exception:",
            "        with contextlib.suppress(OSError):",
            "            os.remove(tf.name)",
            "        raise",
            "",
            "",
            "def find_xpath_attr(node, xpath, key, val=None):",
            "    \"\"\" Find the xpath xpath[@key=val] \"\"\"",
            "    assert re.match(r'^[a-zA-Z_-]+$', key)",
            "    expr = xpath + ('[@%s]' % key if val is None else f\"[@{key}='{val}']\")",
            "    return node.find(expr)",
            "",
            "# On python2.6 the xml.etree.ElementTree.Element methods don't support",
            "# the namespace parameter",
            "",
            "",
            "def xpath_with_ns(path, ns_map):",
            "    components = [c.split(':') for c in path.split('/')]",
            "    replaced = []",
            "    for c in components:",
            "        if len(c) == 1:",
            "            replaced.append(c[0])",
            "        else:",
            "            ns, tag = c",
            "            replaced.append('{%s}%s' % (ns_map[ns], tag))",
            "    return '/'.join(replaced)",
            "",
            "",
            "def xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):",
            "    def _find_xpath(xpath):",
            "        return node.find(xpath)",
            "",
            "    if isinstance(xpath, str):",
            "        n = _find_xpath(xpath)",
            "    else:",
            "        for xp in xpath:",
            "            n = _find_xpath(xp)",
            "            if n is not None:",
            "                break",
            "",
            "    if n is None:",
            "        if default is not NO_DEFAULT:",
            "            return default",
            "        elif fatal:",
            "            name = xpath if name is None else name",
            "            raise ExtractorError('Could not find XML element %s' % name)",
            "        else:",
            "            return None",
            "    return n",
            "",
            "",
            "def xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):",
            "    n = xpath_element(node, xpath, name, fatal=fatal, default=default)",
            "    if n is None or n == default:",
            "        return n",
            "    if n.text is None:",
            "        if default is not NO_DEFAULT:",
            "            return default",
            "        elif fatal:",
            "            name = xpath if name is None else name",
            "            raise ExtractorError('Could not find XML element\\'s text %s' % name)",
            "        else:",
            "            return None",
            "    return n.text",
            "",
            "",
            "def xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):",
            "    n = find_xpath_attr(node, xpath, key)",
            "    if n is None:",
            "        if default is not NO_DEFAULT:",
            "            return default",
            "        elif fatal:",
            "            name = f'{xpath}[@{key}]' if name is None else name",
            "            raise ExtractorError('Could not find XML attribute %s' % name)",
            "        else:",
            "            return None",
            "    return n.attrib[key]",
            "",
            "",
            "def get_element_by_id(id, html, **kwargs):",
            "    \"\"\"Return the content of the tag with the specified ID in the passed HTML document\"\"\"",
            "    return get_element_by_attribute('id', id, html, **kwargs)",
            "",
            "",
            "def get_element_html_by_id(id, html, **kwargs):",
            "    \"\"\"Return the html of the tag with the specified ID in the passed HTML document\"\"\"",
            "    return get_element_html_by_attribute('id', id, html, **kwargs)",
            "",
            "",
            "def get_element_by_class(class_name, html):",
            "    \"\"\"Return the content of the first tag with the specified class in the passed HTML document\"\"\"",
            "    retval = get_elements_by_class(class_name, html)",
            "    return retval[0] if retval else None",
            "",
            "",
            "def get_element_html_by_class(class_name, html):",
            "    \"\"\"Return the html of the first tag with the specified class in the passed HTML document\"\"\"",
            "    retval = get_elements_html_by_class(class_name, html)",
            "    return retval[0] if retval else None",
            "",
            "",
            "def get_element_by_attribute(attribute, value, html, **kwargs):",
            "    retval = get_elements_by_attribute(attribute, value, html, **kwargs)",
            "    return retval[0] if retval else None",
            "",
            "",
            "def get_element_html_by_attribute(attribute, value, html, **kargs):",
            "    retval = get_elements_html_by_attribute(attribute, value, html, **kargs)",
            "    return retval[0] if retval else None",
            "",
            "",
            "def get_elements_by_class(class_name, html, **kargs):",
            "    \"\"\"Return the content of all tags with the specified class in the passed HTML document as a list\"\"\"",
            "    return get_elements_by_attribute(",
            "        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),",
            "        html, escape_value=False)",
            "",
            "",
            "def get_elements_html_by_class(class_name, html):",
            "    \"\"\"Return the html of all tags with the specified class in the passed HTML document as a list\"\"\"",
            "    return get_elements_html_by_attribute(",
            "        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),",
            "        html, escape_value=False)",
            "",
            "",
            "def get_elements_by_attribute(*args, **kwargs):",
            "    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"",
            "    return [content for content, _ in get_elements_text_and_html_by_attribute(*args, **kwargs)]",
            "",
            "",
            "def get_elements_html_by_attribute(*args, **kwargs):",
            "    \"\"\"Return the html of the tag with the specified attribute in the passed HTML document\"\"\"",
            "    return [whole for _, whole in get_elements_text_and_html_by_attribute(*args, **kwargs)]",
            "",
            "",
            "def get_elements_text_and_html_by_attribute(attribute, value, html, *, tag=r'[\\w:.-]+', escape_value=True):",
            "    \"\"\"",
            "    Return the text (content) and the html (whole) of the tag with the specified",
            "    attribute in the passed HTML document",
            "    \"\"\"",
            "    if not value:",
            "        return",
            "",
            "    quote = '' if re.match(r'''[\\s\"'`=<>]''', value) else '?'",
            "",
            "    value = re.escape(value) if escape_value else value",
            "",
            "    partial_element_re = rf'''(?x)",
            "        <(?P<tag>{tag})",
            "         (?:\\s(?:[^>\"']|\"[^\"]*\"|'[^']*')*)?",
            "         \\s{re.escape(attribute)}\\s*=\\s*(?P<_q>['\"]{quote})(?-x:{value})(?P=_q)",
            "        '''",
            "",
            "    for m in re.finditer(partial_element_re, html):",
            "        content, whole = get_element_text_and_html_by_tag(m.group('tag'), html[m.start():])",
            "",
            "        yield (",
            "            unescapeHTML(re.sub(r'^(?P<q>[\"\\'])(?P<content>.*)(?P=q)$', r'\\g<content>', content, flags=re.DOTALL)),",
            "            whole",
            "        )",
            "",
            "",
            "class HTMLBreakOnClosingTagParser(html.parser.HTMLParser):",
            "    \"\"\"",
            "    HTML parser which raises HTMLBreakOnClosingTagException upon reaching the",
            "    closing tag for the first opening tag it has encountered, and can be used",
            "    as a context manager",
            "    \"\"\"",
            "",
            "    class HTMLBreakOnClosingTagException(Exception):",
            "        pass",
            "",
            "    def __init__(self):",
            "        self.tagstack = collections.deque()",
            "        html.parser.HTMLParser.__init__(self)",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, *_):",
            "        self.close()",
            "",
            "    def close(self):",
            "        # handle_endtag does not return upon raising HTMLBreakOnClosingTagException,",
            "        # so data remains buffered; we no longer have any interest in it, thus",
            "        # override this method to discard it",
            "        pass",
            "",
            "    def handle_starttag(self, tag, _):",
            "        self.tagstack.append(tag)",
            "",
            "    def handle_endtag(self, tag):",
            "        if not self.tagstack:",
            "            raise compat_HTMLParseError('no tags in the stack')",
            "        while self.tagstack:",
            "            inner_tag = self.tagstack.pop()",
            "            if inner_tag == tag:",
            "                break",
            "        else:",
            "            raise compat_HTMLParseError(f'matching opening tag for closing {tag} tag not found')",
            "        if not self.tagstack:",
            "            raise self.HTMLBreakOnClosingTagException()",
            "",
            "",
            "# XXX: This should be far less strict",
            "def get_element_text_and_html_by_tag(tag, html):",
            "    \"\"\"",
            "    For the first element with the specified tag in the passed HTML document",
            "    return its' content (text) and the whole element (html)",
            "    \"\"\"",
            "    def find_or_raise(haystack, needle, exc):",
            "        try:",
            "            return haystack.index(needle)",
            "        except ValueError:",
            "            raise exc",
            "    closing_tag = f'</{tag}>'",
            "    whole_start = find_or_raise(",
            "        html, f'<{tag}', compat_HTMLParseError(f'opening {tag} tag not found'))",
            "    content_start = find_or_raise(",
            "        html[whole_start:], '>', compat_HTMLParseError(f'malformed opening {tag} tag'))",
            "    content_start += whole_start + 1",
            "    with HTMLBreakOnClosingTagParser() as parser:",
            "        parser.feed(html[whole_start:content_start])",
            "        if not parser.tagstack or parser.tagstack[0] != tag:",
            "            raise compat_HTMLParseError(f'parser did not match opening {tag} tag')",
            "        offset = content_start",
            "        while offset < len(html):",
            "            next_closing_tag_start = find_or_raise(",
            "                html[offset:], closing_tag,",
            "                compat_HTMLParseError(f'closing {tag} tag not found'))",
            "            next_closing_tag_end = next_closing_tag_start + len(closing_tag)",
            "            try:",
            "                parser.feed(html[offset:offset + next_closing_tag_end])",
            "                offset += next_closing_tag_end",
            "            except HTMLBreakOnClosingTagParser.HTMLBreakOnClosingTagException:",
            "                return html[content_start:offset + next_closing_tag_start], \\",
            "                    html[whole_start:offset + next_closing_tag_end]",
            "        raise compat_HTMLParseError('unexpected end of html')",
            "",
            "",
            "class HTMLAttributeParser(html.parser.HTMLParser):",
            "    \"\"\"Trivial HTML parser to gather the attributes for a single element\"\"\"",
            "",
            "    def __init__(self):",
            "        self.attrs = {}",
            "        html.parser.HTMLParser.__init__(self)",
            "",
            "    def handle_starttag(self, tag, attrs):",
            "        self.attrs = dict(attrs)",
            "        raise compat_HTMLParseError('done')",
            "",
            "",
            "class HTMLListAttrsParser(html.parser.HTMLParser):",
            "    \"\"\"HTML parser to gather the attributes for the elements of a list\"\"\"",
            "",
            "    def __init__(self):",
            "        html.parser.HTMLParser.__init__(self)",
            "        self.items = []",
            "        self._level = 0",
            "",
            "    def handle_starttag(self, tag, attrs):",
            "        if tag == 'li' and self._level == 0:",
            "            self.items.append(dict(attrs))",
            "        self._level += 1",
            "",
            "    def handle_endtag(self, tag):",
            "        self._level -= 1",
            "",
            "",
            "def extract_attributes(html_element):",
            "    \"\"\"Given a string for an HTML element such as",
            "    <el",
            "         a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz",
            "         empty= noval entity=\"&amp;\"",
            "         sq='\"' dq=\"'\"",
            "    >",
            "    Decode and return a dictionary of attributes.",
            "    {",
            "        'a': 'foo', 'b': 'bar', c: 'baz', d: 'boz',",
            "        'empty': '', 'noval': None, 'entity': '&',",
            "        'sq': '\"', 'dq': '\\''",
            "    }.",
            "    \"\"\"",
            "    parser = HTMLAttributeParser()",
            "    with contextlib.suppress(compat_HTMLParseError):",
            "        parser.feed(html_element)",
            "        parser.close()",
            "    return parser.attrs",
            "",
            "",
            "def parse_list(webpage):",
            "    \"\"\"Given a string for an series of HTML <li> elements,",
            "    return a dictionary of their attributes\"\"\"",
            "    parser = HTMLListAttrsParser()",
            "    parser.feed(webpage)",
            "    parser.close()",
            "    return parser.items",
            "",
            "",
            "def clean_html(html):",
            "    \"\"\"Clean an HTML snippet into a readable string\"\"\"",
            "",
            "    if html is None:  # Convenience for sanitizing descriptions etc.",
            "        return html",
            "",
            "    html = re.sub(r'\\s+', ' ', html)",
            "    html = re.sub(r'(?u)\\s?<\\s?br\\s?/?\\s?>\\s?', '\\n', html)",
            "    html = re.sub(r'(?u)<\\s?/\\s?p\\s?>\\s?<\\s?p[^>]*>', '\\n', html)",
            "    # Strip html tags",
            "    html = re.sub('<.*?>', '', html)",
            "    # Replace html entities",
            "    html = unescapeHTML(html)",
            "    return html.strip()",
            "",
            "",
            "class LenientJSONDecoder(json.JSONDecoder):",
            "    # TODO: Write tests",
            "    def __init__(self, *args, transform_source=None, ignore_extra=False, close_objects=0, **kwargs):",
            "        self.transform_source, self.ignore_extra = transform_source, ignore_extra",
            "        self._close_attempts = 2 * close_objects",
            "        super().__init__(*args, **kwargs)",
            "",
            "    @staticmethod",
            "    def _close_object(err):",
            "        doc = err.doc[:err.pos]",
            "        # We need to add comma first to get the correct error message",
            "        if err.msg.startswith('Expecting \\',\\''):",
            "            return doc + ','",
            "        elif not doc.endswith(','):",
            "            return",
            "",
            "        if err.msg.startswith('Expecting property name'):",
            "            return doc[:-1] + '}'",
            "        elif err.msg.startswith('Expecting value'):",
            "            return doc[:-1] + ']'",
            "",
            "    def decode(self, s):",
            "        if self.transform_source:",
            "            s = self.transform_source(s)",
            "        for attempt in range(self._close_attempts + 1):",
            "            try:",
            "                if self.ignore_extra:",
            "                    return self.raw_decode(s.lstrip())[0]",
            "                return super().decode(s)",
            "            except json.JSONDecodeError as e:",
            "                if e.pos is None:",
            "                    raise",
            "                elif attempt < self._close_attempts:",
            "                    s = self._close_object(e)",
            "                    if s is not None:",
            "                        continue",
            "                raise type(e)(f'{e.msg} in {s[e.pos - 10:e.pos + 10]!r}', s, e.pos)",
            "        assert False, 'Too many attempts to decode JSON'",
            "",
            "",
            "def sanitize_open(filename, open_mode):",
            "    \"\"\"Try to open the given filename, and slightly tweak it if this fails.",
            "",
            "    Attempts to open the given filename. If this fails, it tries to change",
            "    the filename slightly, step by step, until it's either able to open it",
            "    or it fails and raises a final exception, like the standard open()",
            "    function.",
            "",
            "    It returns the tuple (stream, definitive_file_name).",
            "    \"\"\"",
            "    if filename == '-':",
            "        if sys.platform == 'win32':",
            "            import msvcrt",
            "",
            "            # stdout may be any IO stream, e.g. when using contextlib.redirect_stdout",
            "            with contextlib.suppress(io.UnsupportedOperation):",
            "                msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)",
            "        return (sys.stdout.buffer if hasattr(sys.stdout, 'buffer') else sys.stdout, filename)",
            "",
            "    for attempt in range(2):",
            "        try:",
            "            try:",
            "                if sys.platform == 'win32':",
            "                    # FIXME: An exclusive lock also locks the file from being read.",
            "                    # Since windows locks are mandatory, don't lock the file on windows (for now).",
            "                    # Ref: https://github.com/yt-dlp/yt-dlp/issues/3124",
            "                    raise LockingUnsupportedError()",
            "                stream = locked_file(filename, open_mode, block=False).__enter__()",
            "            except OSError:",
            "                stream = open(filename, open_mode)",
            "            return stream, filename",
            "        except OSError as err:",
            "            if attempt or err.errno in (errno.EACCES,):",
            "                raise",
            "            old_filename, filename = filename, sanitize_path(filename)",
            "            if old_filename == filename:",
            "                raise",
            "",
            "",
            "def timeconvert(timestr):",
            "    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"",
            "    timestamp = None",
            "    timetuple = email.utils.parsedate_tz(timestr)",
            "    if timetuple is not None:",
            "        timestamp = email.utils.mktime_tz(timetuple)",
            "    return timestamp",
            "",
            "",
            "def sanitize_filename(s, restricted=False, is_id=NO_DEFAULT):",
            "    \"\"\"Sanitizes a string so it could be used as part of a filename.",
            "    @param restricted   Use a stricter subset of allowed characters",
            "    @param is_id        Whether this is an ID that should be kept unchanged if possible.",
            "                        If unset, yt-dlp's new sanitization rules are in effect",
            "    \"\"\"",
            "    if s == '':",
            "        return ''",
            "",
            "    def replace_insane(char):",
            "        if restricted and char in ACCENT_CHARS:",
            "            return ACCENT_CHARS[char]",
            "        elif not restricted and char == '\\n':",
            "            return '\\0 '",
            "        elif is_id is NO_DEFAULT and not restricted and char in '\"*:<>?|/\\\\':",
            "            # Replace with their full-width unicode counterparts",
            "            return {'/': '\\u29F8', '\\\\': '\\u29f9'}.get(char, chr(ord(char) + 0xfee0))",
            "        elif char == '?' or ord(char) < 32 or ord(char) == 127:",
            "            return ''",
            "        elif char == '\"':",
            "            return '' if restricted else '\\''",
            "        elif char == ':':",
            "            return '\\0_\\0-' if restricted else '\\0 \\0-'",
            "        elif char in '\\\\/|*<>':",
            "            return '\\0_'",
            "        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace() or ord(char) > 127):",
            "            return '' if unicodedata.category(char)[0] in 'CM' else '\\0_'",
            "        return char",
            "",
            "    # Replace look-alike Unicode glyphs",
            "    if restricted and (is_id is NO_DEFAULT or not is_id):",
            "        s = unicodedata.normalize('NFKC', s)",
            "    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)  # Handle timestamps",
            "    result = ''.join(map(replace_insane, s))",
            "    if is_id is NO_DEFAULT:",
            "        result = re.sub(r'(\\0.)(?:(?=\\1)..)+', r'\\1', result)  # Remove repeated substitute chars",
            "        STRIP_RE = r'(?:\\0.|[ _-])*'",
            "        result = re.sub(f'^\\0.{STRIP_RE}|{STRIP_RE}\\0.$', '', result)  # Remove substitute chars from start/end",
            "    result = result.replace('\\0', '') or '_'",
            "",
            "    if not is_id:",
            "        while '__' in result:",
            "            result = result.replace('__', '_')",
            "        result = result.strip('_')",
            "        # Common case of \"Foreign band name - English song title\"",
            "        if restricted and result.startswith('-_'):",
            "            result = result[2:]",
            "        if result.startswith('-'):",
            "            result = '_' + result[len('-'):]",
            "        result = result.lstrip('.')",
            "        if not result:",
            "            result = '_'",
            "    return result",
            "",
            "",
            "def sanitize_path(s, force=False):",
            "    \"\"\"Sanitizes and normalizes path on Windows\"\"\"",
            "    # XXX: this handles drive relative paths (c:sth) incorrectly",
            "    if sys.platform == 'win32':",
            "        force = False",
            "        drive_or_unc, _ = os.path.splitdrive(s)",
            "    elif force:",
            "        drive_or_unc = ''",
            "    else:",
            "        return s",
            "",
            "    norm_path = os.path.normpath(remove_start(s, drive_or_unc)).split(os.path.sep)",
            "    if drive_or_unc:",
            "        norm_path.pop(0)",
            "    sanitized_path = [",
            "        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s.]$)', '#', path_part)",
            "        for path_part in norm_path]",
            "    if drive_or_unc:",
            "        sanitized_path.insert(0, drive_or_unc + os.path.sep)",
            "    elif force and s and s[0] == os.path.sep:",
            "        sanitized_path.insert(0, os.path.sep)",
            "    # TODO: Fix behavioral differences <3.12",
            "    # The workaround using `normpath` only superficially passes tests",
            "    # Ref: https://github.com/python/cpython/pull/100351",
            "    return os.path.normpath(os.path.join(*sanitized_path))",
            "",
            "",
            "def sanitize_url(url, *, scheme='http'):",
            "    # Prepend protocol-less URLs with `http:` scheme in order to mitigate",
            "    # the number of unwanted failures due to missing protocol",
            "    if url is None:",
            "        return",
            "    elif url.startswith('//'):",
            "        return f'{scheme}:{url}'",
            "    # Fix some common typos seen so far",
            "    COMMON_TYPOS = (",
            "        # https://github.com/ytdl-org/youtube-dl/issues/15649",
            "        (r'^httpss://', r'https://'),",
            "        # https://bx1.be/lives/direct-tv/",
            "        (r'^rmtp([es]?)://', r'rtmp\\1://'),",
            "    )",
            "    for mistake, fixup in COMMON_TYPOS:",
            "        if re.match(mistake, url):",
            "            return re.sub(mistake, fixup, url)",
            "    return url",
            "",
            "",
            "def extract_basic_auth(url):",
            "    parts = urllib.parse.urlsplit(url)",
            "    if parts.username is None:",
            "        return url, None",
            "    url = urllib.parse.urlunsplit(parts._replace(netloc=(",
            "        parts.hostname if parts.port is None",
            "        else '%s:%d' % (parts.hostname, parts.port))))",
            "    auth_payload = base64.b64encode(",
            "        ('%s:%s' % (parts.username, parts.password or '')).encode())",
            "    return url, f'Basic {auth_payload.decode()}'",
            "",
            "",
            "def expand_path(s):",
            "    \"\"\"Expand shell variables and ~\"\"\"",
            "    return os.path.expandvars(compat_expanduser(s))",
            "",
            "",
            "def orderedSet(iterable, *, lazy=False):",
            "    \"\"\"Remove all duplicates from the input iterable\"\"\"",
            "    def _iter():",
            "        seen = []  # Do not use set since the items can be unhashable",
            "        for x in iterable:",
            "            if x not in seen:",
            "                seen.append(x)",
            "                yield x",
            "",
            "    return _iter() if lazy else list(_iter())",
            "",
            "",
            "def _htmlentity_transform(entity_with_semicolon):",
            "    \"\"\"Transforms an HTML entity to a character.\"\"\"",
            "    entity = entity_with_semicolon[:-1]",
            "",
            "    # Known non-numeric HTML entity",
            "    if entity in html.entities.name2codepoint:",
            "        return chr(html.entities.name2codepoint[entity])",
            "",
            "    # TODO: HTML5 allows entities without a semicolon.",
            "    # E.g. '&Eacuteric' should be decoded as '\u00c9ric'.",
            "    if entity_with_semicolon in html.entities.html5:",
            "        return html.entities.html5[entity_with_semicolon]",
            "",
            "    mobj = re.match(r'#(x[0-9a-fA-F]+|[0-9]+)', entity)",
            "    if mobj is not None:",
            "        numstr = mobj.group(1)",
            "        if numstr.startswith('x'):",
            "            base = 16",
            "            numstr = '0%s' % numstr",
            "        else:",
            "            base = 10",
            "        # See https://github.com/ytdl-org/youtube-dl/issues/7518",
            "        with contextlib.suppress(ValueError):",
            "            return chr(int(numstr, base))",
            "",
            "    # Unknown entity in name, return its literal representation",
            "    return '&%s;' % entity",
            "",
            "",
            "def unescapeHTML(s):",
            "    if s is None:",
            "        return None",
            "    assert isinstance(s, str)",
            "",
            "    return re.sub(",
            "        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)",
            "",
            "",
            "def escapeHTML(text):",
            "    return (",
            "        text",
            "        .replace('&', '&amp;')",
            "        .replace('<', '&lt;')",
            "        .replace('>', '&gt;')",
            "        .replace('\"', '&quot;')",
            "        .replace(\"'\", '&#39;')",
            "    )",
            "",
            "",
            "class netrc_from_content(netrc.netrc):",
            "    def __init__(self, content):",
            "        self.hosts, self.macros = {}, {}",
            "        with io.StringIO(content) as stream:",
            "            self._parse('-', stream, False)",
            "",
            "",
            "class Popen(subprocess.Popen):",
            "    if sys.platform == 'win32':",
            "        _startupinfo = subprocess.STARTUPINFO()",
            "        _startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW",
            "    else:",
            "        _startupinfo = None",
            "",
            "    @staticmethod",
            "    def _fix_pyinstaller_ld_path(env):",
            "        \"\"\"Restore LD_LIBRARY_PATH when using PyInstaller",
            "            Ref: https://github.com/pyinstaller/pyinstaller/blob/develop/doc/runtime-information.rst#ld_library_path--libpath-considerations",
            "                 https://github.com/yt-dlp/yt-dlp/issues/4573",
            "        \"\"\"",
            "        if not hasattr(sys, '_MEIPASS'):",
            "            return",
            "",
            "        def _fix(key):",
            "            orig = env.get(f'{key}_ORIG')",
            "            if orig is None:",
            "                env.pop(key, None)",
            "            else:",
            "                env[key] = orig",
            "",
            "        _fix('LD_LIBRARY_PATH')  # Linux",
            "        _fix('DYLD_LIBRARY_PATH')  # macOS",
            "",
            "    def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):",
            "        if env is None:",
            "            env = os.environ.copy()",
            "        self._fix_pyinstaller_ld_path(env)",
            "",
            "        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')",
            "        if text is True:",
            "            kwargs['universal_newlines'] = True  # For 3.6 compatibility",
            "            kwargs.setdefault('encoding', 'utf-8')",
            "            kwargs.setdefault('errors', 'replace')",
            "",
            "        if shell and compat_os_name == 'nt' and kwargs.get('executable') is None:",
            "            if not isinstance(args, str):",
            "                args = ' '.join(compat_shlex_quote(a) for a in args)",
            "            shell = False",
            "            args = f'{self.__comspec()} /Q /S /D /V:OFF /C \"{args}\"'",
            "",
            "        super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)",
            "",
            "    def __comspec(self):",
            "        comspec = os.environ.get('ComSpec') or os.path.join(",
            "            os.environ.get('SystemRoot', ''), 'System32', 'cmd.exe')",
            "        if os.path.isabs(comspec):",
            "            return comspec",
            "        raise FileNotFoundError('shell not found: neither %ComSpec% nor %SystemRoot% is set')",
            "",
            "    def communicate_or_kill(self, *args, **kwargs):",
            "        try:",
            "            return self.communicate(*args, **kwargs)",
            "        except BaseException:  # Including KeyboardInterrupt",
            "            self.kill(timeout=None)",
            "            raise",
            "",
            "    def kill(self, *, timeout=0):",
            "        super().kill()",
            "        if timeout != 0:",
            "            self.wait(timeout=timeout)",
            "",
            "    @classmethod",
            "    def run(cls, *args, timeout=None, **kwargs):",
            "        with cls(*args, **kwargs) as proc:",
            "            default = '' if proc.__text_mode else b''",
            "            stdout, stderr = proc.communicate_or_kill(timeout=timeout)",
            "            return stdout or default, stderr or default, proc.returncode",
            "",
            "",
            "def encodeArgument(s):",
            "    # Legacy code that uses byte strings",
            "    # Uncomment the following line after fixing all post processors",
            "    # assert isinstance(s, str), 'Internal error: %r should be of type %r, is %r' % (s, str, type(s))",
            "    return s if isinstance(s, str) else s.decode('ascii')",
            "",
            "",
            "_timetuple = collections.namedtuple('Time', ('hours', 'minutes', 'seconds', 'milliseconds'))",
            "",
            "",
            "def timetuple_from_msec(msec):",
            "    secs, msec = divmod(msec, 1000)",
            "    mins, secs = divmod(secs, 60)",
            "    hrs, mins = divmod(mins, 60)",
            "    return _timetuple(hrs, mins, secs, msec)",
            "",
            "",
            "def formatSeconds(secs, delim=':', msec=False):",
            "    time = timetuple_from_msec(secs * 1000)",
            "    if time.hours:",
            "        ret = '%d%s%02d%s%02d' % (time.hours, delim, time.minutes, delim, time.seconds)",
            "    elif time.minutes:",
            "        ret = '%d%s%02d' % (time.minutes, delim, time.seconds)",
            "    else:",
            "        ret = '%d' % time.seconds",
            "    return '%s.%03d' % (ret, time.milliseconds) if msec else ret",
            "",
            "",
            "def bug_reports_message(before=';'):",
            "    from ..update import REPOSITORY",
            "",
            "    msg = (f'please report this issue on  https://github.com/{REPOSITORY}/issues?q= , '",
            "           'filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U')",
            "",
            "    before = before.rstrip()",
            "    if not before or before.endswith(('.', '!', '?')):",
            "        msg = msg[0].title() + msg[1:]",
            "",
            "    return (before + ' ' if before else '') + msg",
            "",
            "",
            "class YoutubeDLError(Exception):",
            "    \"\"\"Base exception for YoutubeDL errors.\"\"\"",
            "    msg = None",
            "",
            "    def __init__(self, msg=None):",
            "        if msg is not None:",
            "            self.msg = msg",
            "        elif self.msg is None:",
            "            self.msg = type(self).__name__",
            "        super().__init__(self.msg)",
            "",
            "",
            "class ExtractorError(YoutubeDLError):",
            "    \"\"\"Error during info extraction.\"\"\"",
            "",
            "    def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None, ie=None):",
            "        \"\"\" tb, if given, is the original traceback (so that it can be printed out).",
            "        If expected is set, this is a normal error message and most likely not a bug in yt-dlp.",
            "        \"\"\"",
            "        from ..networking.exceptions import network_exceptions",
            "        if sys.exc_info()[0] in network_exceptions:",
            "            expected = True",
            "",
            "        self.orig_msg = str(msg)",
            "        self.traceback = tb",
            "        self.expected = expected",
            "        self.cause = cause",
            "        self.video_id = video_id",
            "        self.ie = ie",
            "        self.exc_info = sys.exc_info()  # preserve original exception",
            "        if isinstance(self.exc_info[1], ExtractorError):",
            "            self.exc_info = self.exc_info[1].exc_info",
            "        super().__init__(self.__msg)",
            "",
            "    @property",
            "    def __msg(self):",
            "        return ''.join((",
            "            format_field(self.ie, None, '[%s] '),",
            "            format_field(self.video_id, None, '%s: '),",
            "            self.orig_msg,",
            "            format_field(self.cause, None, ' (caused by %r)'),",
            "            '' if self.expected else bug_reports_message()))",
            "",
            "    def format_traceback(self):",
            "        return join_nonempty(",
            "            self.traceback and ''.join(traceback.format_tb(self.traceback)),",
            "            self.cause and ''.join(traceback.format_exception(None, self.cause, self.cause.__traceback__)[1:]),",
            "            delim='\\n') or None",
            "",
            "    def __setattr__(self, name, value):",
            "        super().__setattr__(name, value)",
            "        if getattr(self, 'msg', None) and name not in ('msg', 'args'):",
            "            self.msg = self.__msg or type(self).__name__",
            "            self.args = (self.msg, )  # Cannot be property",
            "",
            "",
            "class UnsupportedError(ExtractorError):",
            "    def __init__(self, url):",
            "        super().__init__(",
            "            'Unsupported URL: %s' % url, expected=True)",
            "        self.url = url",
            "",
            "",
            "class RegexNotFoundError(ExtractorError):",
            "    \"\"\"Error when a regex didn't match\"\"\"",
            "    pass",
            "",
            "",
            "class GeoRestrictedError(ExtractorError):",
            "    \"\"\"Geographic restriction Error exception.",
            "",
            "    This exception may be thrown when a video is not available from your",
            "    geographic location due to geographic restrictions imposed by a website.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg, countries=None, **kwargs):",
            "        kwargs['expected'] = True",
            "        super().__init__(msg, **kwargs)",
            "        self.countries = countries",
            "",
            "",
            "class UserNotLive(ExtractorError):",
            "    \"\"\"Error when a channel/user is not live\"\"\"",
            "",
            "    def __init__(self, msg=None, **kwargs):",
            "        kwargs['expected'] = True",
            "        super().__init__(msg or 'The channel is not currently live', **kwargs)",
            "",
            "",
            "class DownloadError(YoutubeDLError):",
            "    \"\"\"Download Error exception.",
            "",
            "    This exception may be thrown by FileDownloader objects if they are not",
            "    configured to continue on errors. They will contain the appropriate",
            "    error message.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg, exc_info=None):",
            "        \"\"\" exc_info, if given, is the original exception that caused the trouble (as returned by sys.exc_info()). \"\"\"",
            "        super().__init__(msg)",
            "        self.exc_info = exc_info",
            "",
            "",
            "class EntryNotInPlaylist(YoutubeDLError):",
            "    \"\"\"Entry not in playlist exception.",
            "",
            "    This exception will be thrown by YoutubeDL when a requested entry",
            "    is not found in the playlist info_dict",
            "    \"\"\"",
            "    msg = 'Entry not found in info'",
            "",
            "",
            "class SameFileError(YoutubeDLError):",
            "    \"\"\"Same File exception.",
            "",
            "    This exception will be thrown by FileDownloader objects if they detect",
            "    multiple files would have to be downloaded to the same file on disk.",
            "    \"\"\"",
            "    msg = 'Fixed output name but more than one file to download'",
            "",
            "    def __init__(self, filename=None):",
            "        if filename is not None:",
            "            self.msg += f': {filename}'",
            "        super().__init__(self.msg)",
            "",
            "",
            "class PostProcessingError(YoutubeDLError):",
            "    \"\"\"Post Processing exception.",
            "",
            "    This exception may be raised by PostProcessor's .run() method to",
            "    indicate an error in the postprocessing task.",
            "    \"\"\"",
            "",
            "",
            "class DownloadCancelled(YoutubeDLError):",
            "    \"\"\" Exception raised when the download queue should be interrupted \"\"\"",
            "    msg = 'The download was cancelled'",
            "",
            "",
            "class ExistingVideoReached(DownloadCancelled):",
            "    \"\"\" --break-on-existing triggered \"\"\"",
            "    msg = 'Encountered a video that is already in the archive, stopping due to --break-on-existing'",
            "",
            "",
            "class RejectedVideoReached(DownloadCancelled):",
            "    \"\"\" --break-match-filter triggered \"\"\"",
            "    msg = 'Encountered a video that did not match filter, stopping due to --break-match-filter'",
            "",
            "",
            "class MaxDownloadsReached(DownloadCancelled):",
            "    \"\"\" --max-downloads limit has been reached. \"\"\"",
            "    msg = 'Maximum number of downloads reached, stopping due to --max-downloads'",
            "",
            "",
            "class ReExtractInfo(YoutubeDLError):",
            "    \"\"\" Video info needs to be re-extracted. \"\"\"",
            "",
            "    def __init__(self, msg, expected=False):",
            "        super().__init__(msg)",
            "        self.expected = expected",
            "",
            "",
            "class ThrottledDownload(ReExtractInfo):",
            "    \"\"\" Download speed below --throttled-rate. \"\"\"",
            "    msg = 'The download speed is below throttle limit'",
            "",
            "    def __init__(self):",
            "        super().__init__(self.msg, expected=False)",
            "",
            "",
            "class UnavailableVideoError(YoutubeDLError):",
            "    \"\"\"Unavailable Format exception.",
            "",
            "    This exception will be thrown when a video is requested",
            "    in a format that is not available for that video.",
            "    \"\"\"",
            "    msg = 'Unable to download video'",
            "",
            "    def __init__(self, err=None):",
            "        if err is not None:",
            "            self.msg += f': {err}'",
            "        super().__init__(self.msg)",
            "",
            "",
            "class ContentTooShortError(YoutubeDLError):",
            "    \"\"\"Content Too Short exception.",
            "",
            "    This exception may be raised by FileDownloader objects when a file they",
            "    download is too small for what the server announced first, indicating",
            "    the connection was probably interrupted.",
            "    \"\"\"",
            "",
            "    def __init__(self, downloaded, expected):",
            "        super().__init__(f'Downloaded {downloaded} bytes, expected {expected} bytes')",
            "        # Both in bytes",
            "        self.downloaded = downloaded",
            "        self.expected = expected",
            "",
            "",
            "class XAttrMetadataError(YoutubeDLError):",
            "    def __init__(self, code=None, msg='Unknown error'):",
            "        super().__init__(msg)",
            "        self.code = code",
            "        self.msg = msg",
            "",
            "        # Parsing code and msg",
            "        if (self.code in (errno.ENOSPC, errno.EDQUOT)",
            "                or 'No space left' in self.msg or 'Disk quota exceeded' in self.msg):",
            "            self.reason = 'NO_SPACE'",
            "        elif self.code == errno.E2BIG or 'Argument list too long' in self.msg:",
            "            self.reason = 'VALUE_TOO_LONG'",
            "        else:",
            "            self.reason = 'NOT_SUPPORTED'",
            "",
            "",
            "class XAttrUnavailableError(YoutubeDLError):",
            "    pass",
            "",
            "",
            "def is_path_like(f):",
            "    return isinstance(f, (str, bytes, os.PathLike))",
            "",
            "",
            "def extract_timezone(date_str):",
            "    m = re.search(",
            "        r'''(?x)",
            "            ^.{8,}?                                              # >=8 char non-TZ prefix, if present",
            "            (?P<tz>Z|                                            # just the UTC Z, or",
            "                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|                   # preceded by 4 digits or hh:mm or",
            "                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))     # not preceded by 3 alpha word or >= 4 alpha or 2 digits",
            "                   [ ]?                                          # optional space",
            "                (?P<sign>\\+|-)                                   # +/-",
            "                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})       # hh[:]mm",
            "            $)",
            "        ''', date_str)",
            "    if not m:",
            "        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)",
            "        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())",
            "        if timezone is not None:",
            "            date_str = date_str[:-len(m.group('tz'))]",
            "        timezone = dt.timedelta(hours=timezone or 0)",
            "    else:",
            "        date_str = date_str[:-len(m.group('tz'))]",
            "        if not m.group('sign'):",
            "            timezone = dt.timedelta()",
            "        else:",
            "            sign = 1 if m.group('sign') == '+' else -1",
            "            timezone = dt.timedelta(",
            "                hours=sign * int(m.group('hours')),",
            "                minutes=sign * int(m.group('minutes')))",
            "    return timezone, date_str",
            "",
            "",
            "def parse_iso8601(date_str, delimiter='T', timezone=None):",
            "    \"\"\" Return a UNIX timestamp from the given date \"\"\"",
            "",
            "    if date_str is None:",
            "        return None",
            "",
            "    date_str = re.sub(r'\\.[0-9]+', '', date_str)",
            "",
            "    if timezone is None:",
            "        timezone, date_str = extract_timezone(date_str)",
            "",
            "    with contextlib.suppress(ValueError):",
            "        date_format = f'%Y-%m-%d{delimiter}%H:%M:%S'",
            "        dt_ = dt.datetime.strptime(date_str, date_format) - timezone",
            "        return calendar.timegm(dt_.timetuple())",
            "",
            "",
            "def date_formats(day_first=True):",
            "    return DATE_FORMATS_DAY_FIRST if day_first else DATE_FORMATS_MONTH_FIRST",
            "",
            "",
            "def unified_strdate(date_str, day_first=True):",
            "    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"",
            "",
            "    if date_str is None:",
            "        return None",
            "    upload_date = None",
            "    # Replace commas",
            "    date_str = date_str.replace(',', ' ')",
            "    # Remove AM/PM + timezone",
            "    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)",
            "    _, date_str = extract_timezone(date_str)",
            "",
            "    for expression in date_formats(day_first):",
            "        with contextlib.suppress(ValueError):",
            "            upload_date = dt.datetime.strptime(date_str, expression).strftime('%Y%m%d')",
            "    if upload_date is None:",
            "        timetuple = email.utils.parsedate_tz(date_str)",
            "        if timetuple:",
            "            with contextlib.suppress(ValueError):",
            "                upload_date = dt.datetime(*timetuple[:6]).strftime('%Y%m%d')",
            "    if upload_date is not None:",
            "        return str(upload_date)",
            "",
            "",
            "def unified_timestamp(date_str, day_first=True):",
            "    if not isinstance(date_str, str):",
            "        return None",
            "",
            "    date_str = re.sub(r'\\s+', ' ', re.sub(",
            "        r'(?i)[,|]|(mon|tues?|wed(nes)?|thu(rs)?|fri|sat(ur)?)(day)?', '', date_str))",
            "",
            "    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0",
            "    timezone, date_str = extract_timezone(date_str)",
            "",
            "    # Remove AM/PM + timezone",
            "    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)",
            "",
            "    # Remove unrecognized timezones from ISO 8601 alike timestamps",
            "    m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)",
            "    if m:",
            "        date_str = date_str[:-len(m.group('tz'))]",
            "",
            "    # Python only supports microseconds, so remove nanoseconds",
            "    m = re.search(r'^([0-9]{4,}-[0-9]{1,2}-[0-9]{1,2}T[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{6})[0-9]+$', date_str)",
            "    if m:",
            "        date_str = m.group(1)",
            "",
            "    for expression in date_formats(day_first):",
            "        with contextlib.suppress(ValueError):",
            "            dt_ = dt.datetime.strptime(date_str, expression) - timezone + dt.timedelta(hours=pm_delta)",
            "            return calendar.timegm(dt_.timetuple())",
            "",
            "    timetuple = email.utils.parsedate_tz(date_str)",
            "    if timetuple:",
            "        return calendar.timegm(timetuple) + pm_delta * 3600 - timezone.total_seconds()",
            "",
            "",
            "def determine_ext(url, default_ext='unknown_video'):",
            "    if url is None or '.' not in url:",
            "        return default_ext",
            "    guess = url.partition('?')[0].rpartition('.')[2]",
            "    if re.match(r'^[A-Za-z0-9]+$', guess):",
            "        return guess",
            "    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download",
            "    elif guess.rstrip('/') in KNOWN_EXTENSIONS:",
            "        return guess.rstrip('/')",
            "    else:",
            "        return default_ext",
            "",
            "",
            "def subtitles_filename(filename, sub_lang, sub_format, expected_real_ext=None):",
            "    return replace_extension(filename, sub_lang + '.' + sub_format, expected_real_ext)",
            "",
            "",
            "def datetime_from_str(date_str, precision='auto', format='%Y%m%d'):",
            "    R\"\"\"",
            "    Return a datetime object from a string.",
            "    Supported format:",
            "        (now|today|yesterday|DATE)([+-]\\d+(microsecond|second|minute|hour|day|week|month|year)s?)?",
            "",
            "    @param format       strftime format of DATE",
            "    @param precision    Round the datetime object: auto|microsecond|second|minute|hour|day",
            "                        auto: round to the unit provided in date_str (if applicable).",
            "    \"\"\"",
            "    auto_precision = False",
            "    if precision == 'auto':",
            "        auto_precision = True",
            "        precision = 'microsecond'",
            "    today = datetime_round(dt.datetime.now(dt.timezone.utc), precision)",
            "    if date_str in ('now', 'today'):",
            "        return today",
            "    if date_str == 'yesterday':",
            "        return today - dt.timedelta(days=1)",
            "    match = re.match(",
            "        r'(?P<start>.+)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>microsecond|second|minute|hour|day|week|month|year)s?',",
            "        date_str)",
            "    if match is not None:",
            "        start_time = datetime_from_str(match.group('start'), precision, format)",
            "        time = int(match.group('time')) * (-1 if match.group('sign') == '-' else 1)",
            "        unit = match.group('unit')",
            "        if unit == 'month' or unit == 'year':",
            "            new_date = datetime_add_months(start_time, time * 12 if unit == 'year' else time)",
            "            unit = 'day'",
            "        else:",
            "            if unit == 'week':",
            "                unit = 'day'",
            "                time *= 7",
            "            delta = dt.timedelta(**{unit + 's': time})",
            "            new_date = start_time + delta",
            "        if auto_precision:",
            "            return datetime_round(new_date, unit)",
            "        return new_date",
            "",
            "    return datetime_round(dt.datetime.strptime(date_str, format), precision)",
            "",
            "",
            "def date_from_str(date_str, format='%Y%m%d', strict=False):",
            "    R\"\"\"",
            "    Return a date object from a string using datetime_from_str",
            "",
            "    @param strict  Restrict allowed patterns to \"YYYYMMDD\" and",
            "                   (now|today|yesterday)(-\\d+(day|week|month|year)s?)?",
            "    \"\"\"",
            "    if strict and not re.fullmatch(r'\\d{8}|(now|today|yesterday)(-\\d+(day|week|month|year)s?)?', date_str):",
            "        raise ValueError(f'Invalid date format \"{date_str}\"')",
            "    return datetime_from_str(date_str, precision='microsecond', format=format).date()",
            "",
            "",
            "def datetime_add_months(dt_, months):",
            "    \"\"\"Increment/Decrement a datetime object by months.\"\"\"",
            "    month = dt_.month + months - 1",
            "    year = dt_.year + month // 12",
            "    month = month % 12 + 1",
            "    day = min(dt_.day, calendar.monthrange(year, month)[1])",
            "    return dt_.replace(year, month, day)",
            "",
            "",
            "def datetime_round(dt_, precision='day'):",
            "    \"\"\"",
            "    Round a datetime object's time to a specific precision",
            "    \"\"\"",
            "    if precision == 'microsecond':",
            "        return dt_",
            "",
            "    unit_seconds = {",
            "        'day': 86400,",
            "        'hour': 3600,",
            "        'minute': 60,",
            "        'second': 1,",
            "    }",
            "    roundto = lambda x, n: ((x + n / 2) // n) * n",
            "    timestamp = roundto(calendar.timegm(dt_.timetuple()), unit_seconds[precision])",
            "    return dt.datetime.fromtimestamp(timestamp, dt.timezone.utc)",
            "",
            "",
            "def hyphenate_date(date_str):",
            "    \"\"\"",
            "    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format\"\"\"",
            "    match = re.match(r'^(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)$', date_str)",
            "    if match is not None:",
            "        return '-'.join(match.groups())",
            "    else:",
            "        return date_str",
            "",
            "",
            "class DateRange:",
            "    \"\"\"Represents a time interval between two dates\"\"\"",
            "",
            "    def __init__(self, start=None, end=None):",
            "        \"\"\"start and end must be strings in the format accepted by date\"\"\"",
            "        if start is not None:",
            "            self.start = date_from_str(start, strict=True)",
            "        else:",
            "            self.start = dt.datetime.min.date()",
            "        if end is not None:",
            "            self.end = date_from_str(end, strict=True)",
            "        else:",
            "            self.end = dt.datetime.max.date()",
            "        if self.start > self.end:",
            "            raise ValueError('Date range: \"%s\" , the start date must be before the end date' % self)",
            "",
            "    @classmethod",
            "    def day(cls, day):",
            "        \"\"\"Returns a range that only contains the given day\"\"\"",
            "        return cls(day, day)",
            "",
            "    def __contains__(self, date):",
            "        \"\"\"Check if the date is in the range\"\"\"",
            "        if not isinstance(date, dt.date):",
            "            date = date_from_str(date)",
            "        return self.start <= date <= self.end",
            "",
            "    def __repr__(self):",
            "        return f'{__name__}.{type(self).__name__}({self.start.isoformat()!r}, {self.end.isoformat()!r})'",
            "",
            "    def __str__(self):",
            "        return f'{self.start} to {self.end}'",
            "",
            "    def __eq__(self, other):",
            "        return (isinstance(other, DateRange)",
            "                and self.start == other.start and self.end == other.end)",
            "",
            "",
            "@functools.cache",
            "def system_identifier():",
            "    python_implementation = platform.python_implementation()",
            "    if python_implementation == 'PyPy' and hasattr(sys, 'pypy_version_info'):",
            "        python_implementation += ' version %d.%d.%d' % sys.pypy_version_info[:3]",
            "    libc_ver = []",
            "    with contextlib.suppress(OSError):  # We may not have access to the executable",
            "        libc_ver = platform.libc_ver()",
            "",
            "    return 'Python %s (%s %s %s) - %s (%s%s)' % (",
            "        platform.python_version(),",
            "        python_implementation,",
            "        platform.machine(),",
            "        platform.architecture()[0],",
            "        platform.platform(),",
            "        ssl.OPENSSL_VERSION,",
            "        format_field(join_nonempty(*libc_ver, delim=' '), None, ', %s'),",
            "    )",
            "",
            "",
            "@functools.cache",
            "def get_windows_version():",
            "    ''' Get Windows version. returns () if it's not running on Windows '''",
            "    if compat_os_name == 'nt':",
            "        return version_tuple(platform.win32_ver()[1])",
            "    else:",
            "        return ()",
            "",
            "",
            "def write_string(s, out=None, encoding=None):",
            "    assert isinstance(s, str)",
            "    out = out or sys.stderr",
            "    # `sys.stderr` might be `None` (Ref: https://github.com/pyinstaller/pyinstaller/pull/7217)",
            "    if not out:",
            "        return",
            "",
            "    if compat_os_name == 'nt' and supports_terminal_sequences(out):",
            "        s = re.sub(r'([\\r\\n]+)', r' \\1', s)",
            "",
            "    enc, buffer = None, out",
            "    # `mode` might be `None` (Ref: https://github.com/yt-dlp/yt-dlp/issues/8816)",
            "    if 'b' in (getattr(out, 'mode', None) or ''):",
            "        enc = encoding or preferredencoding()",
            "    elif hasattr(out, 'buffer'):",
            "        buffer = out.buffer",
            "        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()",
            "",
            "    buffer.write(s.encode(enc, 'ignore') if enc else s)",
            "    out.flush()",
            "",
            "",
            "# TODO: Use global logger",
            "def deprecation_warning(msg, *, printer=None, stacklevel=0, **kwargs):",
            "    from .. import _IN_CLI",
            "    if _IN_CLI:",
            "        if msg in deprecation_warning._cache:",
            "            return",
            "        deprecation_warning._cache.add(msg)",
            "        if printer:",
            "            return printer(f'{msg}{bug_reports_message()}', **kwargs)",
            "        return write_string(f'ERROR: {msg}{bug_reports_message()}\\n', **kwargs)",
            "    else:",
            "        import warnings",
            "        warnings.warn(DeprecationWarning(msg), stacklevel=stacklevel + 3)",
            "",
            "",
            "deprecation_warning._cache = set()",
            "",
            "",
            "def bytes_to_intlist(bs):",
            "    if not bs:",
            "        return []",
            "    if isinstance(bs[0], int):  # Python 3",
            "        return list(bs)",
            "    else:",
            "        return [ord(c) for c in bs]",
            "",
            "",
            "def intlist_to_bytes(xs):",
            "    if not xs:",
            "        return b''",
            "    return struct.pack('%dB' % len(xs), *xs)",
            "",
            "",
            "class LockingUnsupportedError(OSError):",
            "    msg = 'File locking is not supported'",
            "",
            "    def __init__(self):",
            "        super().__init__(self.msg)",
            "",
            "",
            "# Cross-platform file locking",
            "if sys.platform == 'win32':",
            "    import ctypes",
            "    import ctypes.wintypes",
            "    import msvcrt",
            "",
            "    class OVERLAPPED(ctypes.Structure):",
            "        _fields_ = [",
            "            ('Internal', ctypes.wintypes.LPVOID),",
            "            ('InternalHigh', ctypes.wintypes.LPVOID),",
            "            ('Offset', ctypes.wintypes.DWORD),",
            "            ('OffsetHigh', ctypes.wintypes.DWORD),",
            "            ('hEvent', ctypes.wintypes.HANDLE),",
            "        ]",
            "",
            "    kernel32 = ctypes.WinDLL('kernel32')",
            "    LockFileEx = kernel32.LockFileEx",
            "    LockFileEx.argtypes = [",
            "        ctypes.wintypes.HANDLE,     # hFile",
            "        ctypes.wintypes.DWORD,      # dwFlags",
            "        ctypes.wintypes.DWORD,      # dwReserved",
            "        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow",
            "        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh",
            "        ctypes.POINTER(OVERLAPPED)  # Overlapped",
            "    ]",
            "    LockFileEx.restype = ctypes.wintypes.BOOL",
            "    UnlockFileEx = kernel32.UnlockFileEx",
            "    UnlockFileEx.argtypes = [",
            "        ctypes.wintypes.HANDLE,     # hFile",
            "        ctypes.wintypes.DWORD,      # dwReserved",
            "        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow",
            "        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh",
            "        ctypes.POINTER(OVERLAPPED)  # Overlapped",
            "    ]",
            "    UnlockFileEx.restype = ctypes.wintypes.BOOL",
            "    whole_low = 0xffffffff",
            "    whole_high = 0x7fffffff",
            "",
            "    def _lock_file(f, exclusive, block):",
            "        overlapped = OVERLAPPED()",
            "        overlapped.Offset = 0",
            "        overlapped.OffsetHigh = 0",
            "        overlapped.hEvent = 0",
            "        f._lock_file_overlapped_p = ctypes.pointer(overlapped)",
            "",
            "        if not LockFileEx(msvcrt.get_osfhandle(f.fileno()),",
            "                          (0x2 if exclusive else 0x0) | (0x0 if block else 0x1),",
            "                          0, whole_low, whole_high, f._lock_file_overlapped_p):",
            "            # NB: No argument form of \"ctypes.FormatError\" does not work on PyPy",
            "            raise BlockingIOError(f'Locking file failed: {ctypes.FormatError(ctypes.GetLastError())!r}')",
            "",
            "    def _unlock_file(f):",
            "        assert f._lock_file_overlapped_p",
            "        handle = msvcrt.get_osfhandle(f.fileno())",
            "        if not UnlockFileEx(handle, 0, whole_low, whole_high, f._lock_file_overlapped_p):",
            "            raise OSError('Unlocking file failed: %r' % ctypes.FormatError())",
            "",
            "else:",
            "    try:",
            "        import fcntl",
            "",
            "        def _lock_file(f, exclusive, block):",
            "            flags = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH",
            "            if not block:",
            "                flags |= fcntl.LOCK_NB",
            "            try:",
            "                fcntl.flock(f, flags)",
            "            except BlockingIOError:",
            "                raise",
            "            except OSError:  # AOSP does not have flock()",
            "                fcntl.lockf(f, flags)",
            "",
            "        def _unlock_file(f):",
            "            with contextlib.suppress(OSError):",
            "                return fcntl.flock(f, fcntl.LOCK_UN)",
            "            with contextlib.suppress(OSError):",
            "                return fcntl.lockf(f, fcntl.LOCK_UN)  # AOSP does not have flock()",
            "            return fcntl.flock(f, fcntl.LOCK_UN | fcntl.LOCK_NB)  # virtiofs needs LOCK_NB on unlocking",
            "",
            "    except ImportError:",
            "",
            "        def _lock_file(f, exclusive, block):",
            "            raise LockingUnsupportedError()",
            "",
            "        def _unlock_file(f):",
            "            raise LockingUnsupportedError()",
            "",
            "",
            "class locked_file:",
            "    locked = False",
            "",
            "    def __init__(self, filename, mode, block=True, encoding=None):",
            "        if mode not in {'r', 'rb', 'a', 'ab', 'w', 'wb'}:",
            "            raise NotImplementedError(mode)",
            "        self.mode, self.block = mode, block",
            "",
            "        writable = any(f in mode for f in 'wax+')",
            "        readable = any(f in mode for f in 'r+')",
            "        flags = functools.reduce(operator.ior, (",
            "            getattr(os, 'O_CLOEXEC', 0),  # UNIX only",
            "            getattr(os, 'O_BINARY', 0),  # Windows only",
            "            getattr(os, 'O_NOINHERIT', 0),  # Windows only",
            "            os.O_CREAT if writable else 0,  # O_TRUNC only after locking",
            "            os.O_APPEND if 'a' in mode else 0,",
            "            os.O_EXCL if 'x' in mode else 0,",
            "            os.O_RDONLY if not writable else os.O_RDWR if readable else os.O_WRONLY,",
            "        ))",
            "",
            "        self.f = os.fdopen(os.open(filename, flags, 0o666), mode, encoding=encoding)",
            "",
            "    def __enter__(self):",
            "        exclusive = 'r' not in self.mode",
            "        try:",
            "            _lock_file(self.f, exclusive, self.block)",
            "            self.locked = True",
            "        except OSError:",
            "            self.f.close()",
            "            raise",
            "        if 'w' in self.mode:",
            "            try:",
            "                self.f.truncate()",
            "            except OSError as e:",
            "                if e.errno not in (",
            "                    errno.ESPIPE,  # Illegal seek - expected for FIFO",
            "                    errno.EINVAL,  # Invalid argument - expected for /dev/null",
            "                ):",
            "                    raise",
            "        return self",
            "",
            "    def unlock(self):",
            "        if not self.locked:",
            "            return",
            "        try:",
            "            _unlock_file(self.f)",
            "        finally:",
            "            self.locked = False",
            "",
            "    def __exit__(self, *_):",
            "        try:",
            "            self.unlock()",
            "        finally:",
            "            self.f.close()",
            "",
            "    open = __enter__",
            "    close = __exit__",
            "",
            "    def __getattr__(self, attr):",
            "        return getattr(self.f, attr)",
            "",
            "    def __iter__(self):",
            "        return iter(self.f)",
            "",
            "",
            "@functools.cache",
            "def get_filesystem_encoding():",
            "    encoding = sys.getfilesystemencoding()",
            "    return encoding if encoding is not None else 'utf-8'",
            "",
            "",
            "def shell_quote(args):",
            "    quoted_args = []",
            "    encoding = get_filesystem_encoding()",
            "    for a in args:",
            "        if isinstance(a, bytes):",
            "            # We may get a filename encoded with 'encodeFilename'",
            "            a = a.decode(encoding)",
            "        quoted_args.append(compat_shlex_quote(a))",
            "    return ' '.join(quoted_args)",
            "",
            "",
            "def smuggle_url(url, data):",
            "    \"\"\" Pass additional data in a URL for internal use. \"\"\"",
            "",
            "    url, idata = unsmuggle_url(url, {})",
            "    data.update(idata)",
            "    sdata = urllib.parse.urlencode(",
            "        {'__youtubedl_smuggle': json.dumps(data)})",
            "    return url + '#' + sdata",
            "",
            "",
            "def unsmuggle_url(smug_url, default=None):",
            "    if '#__youtubedl_smuggle' not in smug_url:",
            "        return smug_url, default",
            "    url, _, sdata = smug_url.rpartition('#')",
            "    jsond = urllib.parse.parse_qs(sdata)['__youtubedl_smuggle'][0]",
            "    data = json.loads(jsond)",
            "    return url, data",
            "",
            "",
            "def format_decimal_suffix(num, fmt='%d%s', *, factor=1000):",
            "    \"\"\" Formats numbers with decimal sufixes like K, M, etc \"\"\"",
            "    num, factor = float_or_none(num), float(factor)",
            "    if num is None or num < 0:",
            "        return None",
            "    POSSIBLE_SUFFIXES = 'kMGTPEZY'",
            "    exponent = 0 if num == 0 else min(int(math.log(num, factor)), len(POSSIBLE_SUFFIXES))",
            "    suffix = ['', *POSSIBLE_SUFFIXES][exponent]",
            "    if factor == 1024:",
            "        suffix = {'k': 'Ki', '': ''}.get(suffix, f'{suffix}i')",
            "    converted = num / (factor ** exponent)",
            "    return fmt % (converted, suffix)",
            "",
            "",
            "def format_bytes(bytes):",
            "    return format_decimal_suffix(bytes, '%.2f%sB', factor=1024) or 'N/A'",
            "",
            "",
            "def lookup_unit_table(unit_table, s, strict=False):",
            "    num_re = NUMBER_RE if strict else NUMBER_RE.replace(R'\\.', '[,.]')",
            "    units_re = '|'.join(re.escape(u) for u in unit_table)",
            "    m = (re.fullmatch if strict else re.match)(",
            "        rf'(?P<num>{num_re})\\s*(?P<unit>{units_re})\\b', s)",
            "    if not m:",
            "        return None",
            "",
            "    num = float(m.group('num').replace(',', '.'))",
            "    mult = unit_table[m.group('unit')]",
            "    return round(num * mult)",
            "",
            "",
            "def parse_bytes(s):",
            "    \"\"\"Parse a string indicating a byte quantity into an integer\"\"\"",
            "    return lookup_unit_table(",
            "        {u: 1024**i for i, u in enumerate(['', *'KMGTPEZY'])},",
            "        s.upper(), strict=True)",
            "",
            "",
            "def parse_filesize(s):",
            "    if s is None:",
            "        return None",
            "",
            "    # The lower-case forms are of course incorrect and unofficial,",
            "    # but we support those too",
            "    _UNIT_TABLE = {",
            "        'B': 1,",
            "        'b': 1,",
            "        'bytes': 1,",
            "        'KiB': 1024,",
            "        'KB': 1000,",
            "        'kB': 1024,",
            "        'Kb': 1000,",
            "        'kb': 1000,",
            "        'kilobytes': 1000,",
            "        'kibibytes': 1024,",
            "        'MiB': 1024 ** 2,",
            "        'MB': 1000 ** 2,",
            "        'mB': 1024 ** 2,",
            "        'Mb': 1000 ** 2,",
            "        'mb': 1000 ** 2,",
            "        'megabytes': 1000 ** 2,",
            "        'mebibytes': 1024 ** 2,",
            "        'GiB': 1024 ** 3,",
            "        'GB': 1000 ** 3,",
            "        'gB': 1024 ** 3,",
            "        'Gb': 1000 ** 3,",
            "        'gb': 1000 ** 3,",
            "        'gigabytes': 1000 ** 3,",
            "        'gibibytes': 1024 ** 3,",
            "        'TiB': 1024 ** 4,",
            "        'TB': 1000 ** 4,",
            "        'tB': 1024 ** 4,",
            "        'Tb': 1000 ** 4,",
            "        'tb': 1000 ** 4,",
            "        'terabytes': 1000 ** 4,",
            "        'tebibytes': 1024 ** 4,",
            "        'PiB': 1024 ** 5,",
            "        'PB': 1000 ** 5,",
            "        'pB': 1024 ** 5,",
            "        'Pb': 1000 ** 5,",
            "        'pb': 1000 ** 5,",
            "        'petabytes': 1000 ** 5,",
            "        'pebibytes': 1024 ** 5,",
            "        'EiB': 1024 ** 6,",
            "        'EB': 1000 ** 6,",
            "        'eB': 1024 ** 6,",
            "        'Eb': 1000 ** 6,",
            "        'eb': 1000 ** 6,",
            "        'exabytes': 1000 ** 6,",
            "        'exbibytes': 1024 ** 6,",
            "        'ZiB': 1024 ** 7,",
            "        'ZB': 1000 ** 7,",
            "        'zB': 1024 ** 7,",
            "        'Zb': 1000 ** 7,",
            "        'zb': 1000 ** 7,",
            "        'zettabytes': 1000 ** 7,",
            "        'zebibytes': 1024 ** 7,",
            "        'YiB': 1024 ** 8,",
            "        'YB': 1000 ** 8,",
            "        'yB': 1024 ** 8,",
            "        'Yb': 1000 ** 8,",
            "        'yb': 1000 ** 8,",
            "        'yottabytes': 1000 ** 8,",
            "        'yobibytes': 1024 ** 8,",
            "    }",
            "",
            "    return lookup_unit_table(_UNIT_TABLE, s)",
            "",
            "",
            "def parse_count(s):",
            "    if s is None:",
            "        return None",
            "",
            "    s = re.sub(r'^[^\\d]+\\s', '', s).strip()",
            "",
            "    if re.match(r'^[\\d,.]+$', s):",
            "        return str_to_int(s)",
            "",
            "    _UNIT_TABLE = {",
            "        'k': 1000,",
            "        'K': 1000,",
            "        'm': 1000 ** 2,",
            "        'M': 1000 ** 2,",
            "        'kk': 1000 ** 2,",
            "        'KK': 1000 ** 2,",
            "        'b': 1000 ** 3,",
            "        'B': 1000 ** 3,",
            "    }",
            "",
            "    ret = lookup_unit_table(_UNIT_TABLE, s)",
            "    if ret is not None:",
            "        return ret",
            "",
            "    mobj = re.match(r'([\\d,.]+)(?:$|\\s)', s)",
            "    if mobj:",
            "        return str_to_int(mobj.group(1))",
            "",
            "",
            "def parse_resolution(s, *, lenient=False):",
            "    if s is None:",
            "        return {}",
            "",
            "    if lenient:",
            "        mobj = re.search(r'(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)', s)",
            "    else:",
            "        mobj = re.search(r'(?<![a-zA-Z0-9])(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)(?![a-zA-Z0-9])', s)",
            "    if mobj:",
            "        return {",
            "            'width': int(mobj.group('w')),",
            "            'height': int(mobj.group('h')),",
            "        }",
            "",
            "    mobj = re.search(r'(?<![a-zA-Z0-9])(\\d+)[pPiI](?![a-zA-Z0-9])', s)",
            "    if mobj:",
            "        return {'height': int(mobj.group(1))}",
            "",
            "    mobj = re.search(r'\\b([48])[kK]\\b', s)",
            "    if mobj:",
            "        return {'height': int(mobj.group(1)) * 540}",
            "",
            "    return {}",
            "",
            "",
            "def parse_bitrate(s):",
            "    if not isinstance(s, str):",
            "        return",
            "    mobj = re.search(r'\\b(\\d+)\\s*kbps', s)",
            "    if mobj:",
            "        return int(mobj.group(1))",
            "",
            "",
            "def month_by_name(name, lang='en'):",
            "    \"\"\" Return the number of a month by (locale-independently) English name \"\"\"",
            "",
            "    month_names = MONTH_NAMES.get(lang, MONTH_NAMES['en'])",
            "",
            "    try:",
            "        return month_names.index(name) + 1",
            "    except ValueError:",
            "        return None",
            "",
            "",
            "def month_by_abbreviation(abbrev):",
            "    \"\"\" Return the number of a month by (locale-independently) English",
            "        abbreviations \"\"\"",
            "",
            "    try:",
            "        return [s[:3] for s in ENGLISH_MONTH_NAMES].index(abbrev) + 1",
            "    except ValueError:",
            "        return None",
            "",
            "",
            "def fix_xml_ampersands(xml_str):",
            "    \"\"\"Replace all the '&' by '&amp;' in XML\"\"\"",
            "    return re.sub(",
            "        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',",
            "        '&amp;',",
            "        xml_str)",
            "",
            "",
            "def setproctitle(title):",
            "    assert isinstance(title, str)",
            "",
            "    # Workaround for https://github.com/yt-dlp/yt-dlp/issues/4541",
            "    try:",
            "        import ctypes",
            "    except ImportError:",
            "        return",
            "",
            "    try:",
            "        libc = ctypes.cdll.LoadLibrary('libc.so.6')",
            "    except OSError:",
            "        return",
            "    except TypeError:",
            "        # LoadLibrary in Windows Python 2.7.13 only expects",
            "        # a bytestring, but since unicode_literals turns",
            "        # every string into a unicode string, it fails.",
            "        return",
            "    title_bytes = title.encode()",
            "    buf = ctypes.create_string_buffer(len(title_bytes))",
            "    buf.value = title_bytes",
            "    try:",
            "        # PR_SET_NAME = 15      Ref: /usr/include/linux/prctl.h",
            "        libc.prctl(15, buf, 0, 0, 0)",
            "    except AttributeError:",
            "        return  # Strange libc, just skip this",
            "",
            "",
            "def remove_start(s, start):",
            "    return s[len(start):] if s is not None and s.startswith(start) else s",
            "",
            "",
            "def remove_end(s, end):",
            "    return s[:-len(end)] if s is not None and s.endswith(end) else s",
            "",
            "",
            "def remove_quotes(s):",
            "    if s is None or len(s) < 2:",
            "        return s",
            "    for quote in ('\"', \"'\", ):",
            "        if s[0] == quote and s[-1] == quote:",
            "            return s[1:-1]",
            "    return s",
            "",
            "",
            "def get_domain(url):",
            "    \"\"\"",
            "    This implementation is inconsistent, but is kept for compatibility.",
            "    Use this only for \"webpage_url_domain\"",
            "    \"\"\"",
            "    return remove_start(urllib.parse.urlparse(url).netloc, 'www.') or None",
            "",
            "",
            "def url_basename(url):",
            "    path = urllib.parse.urlparse(url).path",
            "    return path.strip('/').split('/')[-1]",
            "",
            "",
            "def base_url(url):",
            "    return re.match(r'https?://[^?#]+/', url).group()",
            "",
            "",
            "def urljoin(base, path):",
            "    if isinstance(path, bytes):",
            "        path = path.decode()",
            "    if not isinstance(path, str) or not path:",
            "        return None",
            "    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):",
            "        return path",
            "    if isinstance(base, bytes):",
            "        base = base.decode()",
            "    if not isinstance(base, str) or not re.match(",
            "            r'^(?:https?:)?//', base):",
            "        return None",
            "    return urllib.parse.urljoin(base, path)",
            "",
            "",
            "def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):",
            "    if get_attr and v is not None:",
            "        v = getattr(v, get_attr, None)",
            "    try:",
            "        return int(v) * invscale // scale",
            "    except (ValueError, TypeError, OverflowError):",
            "        return default",
            "",
            "",
            "def str_or_none(v, default=None):",
            "    return default if v is None else str(v)",
            "",
            "",
            "def str_to_int(int_str):",
            "    \"\"\" A more relaxed version of int_or_none \"\"\"",
            "    if isinstance(int_str, int):",
            "        return int_str",
            "    elif isinstance(int_str, str):",
            "        int_str = re.sub(r'[,\\.\\+]', '', int_str)",
            "        return int_or_none(int_str)",
            "",
            "",
            "def float_or_none(v, scale=1, invscale=1, default=None):",
            "    if v is None:",
            "        return default",
            "    try:",
            "        return float(v) * invscale / scale",
            "    except (ValueError, TypeError):",
            "        return default",
            "",
            "",
            "def bool_or_none(v, default=None):",
            "    return v if isinstance(v, bool) else default",
            "",
            "",
            "def strip_or_none(v, default=None):",
            "    return v.strip() if isinstance(v, str) else default",
            "",
            "",
            "def url_or_none(url):",
            "    if not url or not isinstance(url, str):",
            "        return None",
            "    url = url.strip()",
            "    return url if re.match(r'^(?:(?:https?|rt(?:m(?:pt?[es]?|fp)|sp[su]?)|mms|ftps?):)?//', url) else None",
            "",
            "",
            "def strftime_or_none(timestamp, date_format='%Y%m%d', default=None):",
            "    datetime_object = None",
            "    try:",
            "        if isinstance(timestamp, (int, float)):  # unix timestamp",
            "            # Using naive datetime here can break timestamp() in Windows",
            "            # Ref: https://github.com/yt-dlp/yt-dlp/issues/5185, https://github.com/python/cpython/issues/94414",
            "            # Also, dt.datetime.fromtimestamp breaks for negative timestamps",
            "            # Ref: https://github.com/yt-dlp/yt-dlp/issues/6706#issuecomment-1496842642",
            "            datetime_object = (dt.datetime.fromtimestamp(0, dt.timezone.utc)",
            "                               + dt.timedelta(seconds=timestamp))",
            "        elif isinstance(timestamp, str):  # assume YYYYMMDD",
            "            datetime_object = dt.datetime.strptime(timestamp, '%Y%m%d')",
            "        date_format = re.sub(  # Support %s on windows",
            "            r'(?<!%)(%%)*%s', rf'\\g<1>{int(datetime_object.timestamp())}', date_format)",
            "        return datetime_object.strftime(date_format)",
            "    except (ValueError, TypeError, AttributeError):",
            "        return default",
            "",
            "",
            "def parse_duration(s):",
            "    if not isinstance(s, str):",
            "        return None",
            "    s = s.strip()",
            "    if not s:",
            "        return None",
            "",
            "    days, hours, mins, secs, ms = [None] * 5",
            "    m = re.match(r'''(?x)",
            "            (?P<before_secs>",
            "                (?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?",
            "            (?P<secs>(?(before_secs)[0-9]{1,2}|[0-9]+))",
            "            (?P<ms>[.:][0-9]+)?Z?$",
            "        ''', s)",
            "    if m:",
            "        days, hours, mins, secs, ms = m.group('days', 'hours', 'mins', 'secs', 'ms')",
            "    else:",
            "        m = re.match(",
            "            r'''(?ix)(?:P?",
            "                (?:",
            "                    [0-9]+\\s*y(?:ears?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    [0-9]+\\s*m(?:onths?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    [0-9]+\\s*w(?:eeks?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    (?P<days>[0-9]+)\\s*d(?:ays?)?,?\\s*",
            "                )?",
            "                T)?",
            "                (?:",
            "                    (?P<hours>[0-9]+)\\s*h(?:(?:ou)?rs?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    (?P<mins>[0-9]+)\\s*m(?:in(?:ute)?s?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*s(?:ec(?:ond)?s?)?\\s*",
            "                )?Z?$''', s)",
            "        if m:",
            "            days, hours, mins, secs, ms = m.groups()",
            "        else:",
            "            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\\s*(?:hours?)|(?P<mins>[0-9.]+)\\s*(?:mins?\\.?|minutes?)\\s*)Z?$', s)",
            "            if m:",
            "                hours, mins = m.groups()",
            "            else:",
            "                return None",
            "",
            "    if ms:",
            "        ms = ms.replace(':', '.')",
            "    return sum(float(part or 0) * mult for part, mult in (",
            "        (days, 86400), (hours, 3600), (mins, 60), (secs, 1), (ms, 1)))",
            "",
            "",
            "def prepend_extension(filename, ext, expected_real_ext=None):",
            "    name, real_ext = os.path.splitext(filename)",
            "    return (",
            "        f'{name}.{ext}{real_ext}'",
            "        if not expected_real_ext or real_ext[1:] == expected_real_ext",
            "        else f'{filename}.{ext}')",
            "",
            "",
            "def replace_extension(filename, ext, expected_real_ext=None):",
            "    name, real_ext = os.path.splitext(filename)",
            "    return '{}.{}'.format(",
            "        name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename,",
            "        ext)",
            "",
            "",
            "def check_executable(exe, args=[]):",
            "    \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.",
            "    args can be a list of arguments for a short output (like -version) \"\"\"",
            "    try:",
            "        Popen.run([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)",
            "    except OSError:",
            "        return False",
            "    return exe",
            "",
            "",
            "def _get_exe_version_output(exe, args):",
            "    try:",
            "        # STDIN should be redirected too. On UNIX-like systems, ffmpeg triggers",
            "        # SIGTTOU if yt-dlp is run in the background.",
            "        # See https://github.com/ytdl-org/youtube-dl/issues/955#issuecomment-209789656",
            "        stdout, _, ret = Popen.run([encodeArgument(exe)] + args, text=True,",
            "                                   stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)",
            "        if ret:",
            "            return None",
            "    except OSError:",
            "        return False",
            "    return stdout",
            "",
            "",
            "def detect_exe_version(output, version_re=None, unrecognized='present'):",
            "    assert isinstance(output, str)",
            "    if version_re is None:",
            "        version_re = r'version\\s+([-0-9._a-zA-Z]+)'",
            "    m = re.search(version_re, output)",
            "    if m:",
            "        return m.group(1)",
            "    else:",
            "        return unrecognized",
            "",
            "",
            "def get_exe_version(exe, args=['--version'],",
            "                    version_re=None, unrecognized=('present', 'broken')):",
            "    \"\"\" Returns the version of the specified executable,",
            "    or False if the executable is not present \"\"\"",
            "    unrecognized = variadic(unrecognized)",
            "    assert len(unrecognized) in (1, 2)",
            "    out = _get_exe_version_output(exe, args)",
            "    if out is None:",
            "        return unrecognized[-1]",
            "    return out and detect_exe_version(out, version_re, unrecognized[0])",
            "",
            "",
            "def frange(start=0, stop=None, step=1):",
            "    \"\"\"Float range\"\"\"",
            "    if stop is None:",
            "        start, stop = 0, start",
            "    sign = [-1, 1][step > 0] if step else 0",
            "    while sign * start < sign * stop:",
            "        yield start",
            "        start += step",
            "",
            "",
            "class LazyList(collections.abc.Sequence):",
            "    \"\"\"Lazy immutable list from an iterable",
            "    Note that slices of a LazyList are lists and not LazyList\"\"\"",
            "",
            "    class IndexError(IndexError):",
            "        pass",
            "",
            "    def __init__(self, iterable, *, reverse=False, _cache=None):",
            "        self._iterable = iter(iterable)",
            "        self._cache = [] if _cache is None else _cache",
            "        self._reversed = reverse",
            "",
            "    def __iter__(self):",
            "        if self._reversed:",
            "            # We need to consume the entire iterable to iterate in reverse",
            "            yield from self.exhaust()",
            "            return",
            "        yield from self._cache",
            "        for item in self._iterable:",
            "            self._cache.append(item)",
            "            yield item",
            "",
            "    def _exhaust(self):",
            "        self._cache.extend(self._iterable)",
            "        self._iterable = []  # Discard the emptied iterable to make it pickle-able",
            "        return self._cache",
            "",
            "    def exhaust(self):",
            "        \"\"\"Evaluate the entire iterable\"\"\"",
            "        return self._exhaust()[::-1 if self._reversed else 1]",
            "",
            "    @staticmethod",
            "    def _reverse_index(x):",
            "        return None if x is None else ~x",
            "",
            "    def __getitem__(self, idx):",
            "        if isinstance(idx, slice):",
            "            if self._reversed:",
            "                idx = slice(self._reverse_index(idx.start), self._reverse_index(idx.stop), -(idx.step or 1))",
            "            start, stop, step = idx.start, idx.stop, idx.step or 1",
            "        elif isinstance(idx, int):",
            "            if self._reversed:",
            "                idx = self._reverse_index(idx)",
            "            start, stop, step = idx, idx, 0",
            "        else:",
            "            raise TypeError('indices must be integers or slices')",
            "        if ((start or 0) < 0 or (stop or 0) < 0",
            "                or (start is None and step < 0)",
            "                or (stop is None and step > 0)):",
            "            # We need to consume the entire iterable to be able to slice from the end",
            "            # Obviously, never use this with infinite iterables",
            "            self._exhaust()",
            "            try:",
            "                return self._cache[idx]",
            "            except IndexError as e:",
            "                raise self.IndexError(e) from e",
            "        n = max(start or 0, stop or 0) - len(self._cache) + 1",
            "        if n > 0:",
            "            self._cache.extend(itertools.islice(self._iterable, n))",
            "        try:",
            "            return self._cache[idx]",
            "        except IndexError as e:",
            "            raise self.IndexError(e) from e",
            "",
            "    def __bool__(self):",
            "        try:",
            "            self[-1] if self._reversed else self[0]",
            "        except self.IndexError:",
            "            return False",
            "        return True",
            "",
            "    def __len__(self):",
            "        self._exhaust()",
            "        return len(self._cache)",
            "",
            "    def __reversed__(self):",
            "        return type(self)(self._iterable, reverse=not self._reversed, _cache=self._cache)",
            "",
            "    def __copy__(self):",
            "        return type(self)(self._iterable, reverse=self._reversed, _cache=self._cache)",
            "",
            "    def __repr__(self):",
            "        # repr and str should mimic a list. So we exhaust the iterable",
            "        return repr(self.exhaust())",
            "",
            "    def __str__(self):",
            "        return repr(self.exhaust())",
            "",
            "",
            "class PagedList:",
            "",
            "    class IndexError(IndexError):",
            "        pass",
            "",
            "    def __len__(self):",
            "        # This is only useful for tests",
            "        return len(self.getslice())",
            "",
            "    def __init__(self, pagefunc, pagesize, use_cache=True):",
            "        self._pagefunc = pagefunc",
            "        self._pagesize = pagesize",
            "        self._pagecount = float('inf')",
            "        self._use_cache = use_cache",
            "        self._cache = {}",
            "",
            "    def getpage(self, pagenum):",
            "        page_results = self._cache.get(pagenum)",
            "        if page_results is None:",
            "            page_results = [] if pagenum > self._pagecount else list(self._pagefunc(pagenum))",
            "        if self._use_cache:",
            "            self._cache[pagenum] = page_results",
            "        return page_results",
            "",
            "    def getslice(self, start=0, end=None):",
            "        return list(self._getslice(start, end))",
            "",
            "    def _getslice(self, start, end):",
            "        raise NotImplementedError('This method must be implemented by subclasses')",
            "",
            "    def __getitem__(self, idx):",
            "        assert self._use_cache, 'Indexing PagedList requires cache'",
            "        if not isinstance(idx, int) or idx < 0:",
            "            raise TypeError('indices must be non-negative integers')",
            "        entries = self.getslice(idx, idx + 1)",
            "        if not entries:",
            "            raise self.IndexError()",
            "        return entries[0]",
            "",
            "    def __bool__(self):",
            "        return bool(self.getslice(0, 1))",
            "",
            "",
            "class OnDemandPagedList(PagedList):",
            "    \"\"\"Download pages until a page with less than maximum results\"\"\"",
            "",
            "    def _getslice(self, start, end):",
            "        for pagenum in itertools.count(start // self._pagesize):",
            "            firstid = pagenum * self._pagesize",
            "            nextfirstid = pagenum * self._pagesize + self._pagesize",
            "            if start >= nextfirstid:",
            "                continue",
            "",
            "            startv = (",
            "                start % self._pagesize",
            "                if firstid <= start < nextfirstid",
            "                else 0)",
            "            endv = (",
            "                ((end - 1) % self._pagesize) + 1",
            "                if (end is not None and firstid <= end <= nextfirstid)",
            "                else None)",
            "",
            "            try:",
            "                page_results = self.getpage(pagenum)",
            "            except Exception:",
            "                self._pagecount = pagenum - 1",
            "                raise",
            "            if startv != 0 or endv is not None:",
            "                page_results = page_results[startv:endv]",
            "            yield from page_results",
            "",
            "            # A little optimization - if current page is not \"full\", ie. does",
            "            # not contain page_size videos then we can assume that this page",
            "            # is the last one - there are no more ids on further pages -",
            "            # i.e. no need to query again.",
            "            if len(page_results) + startv < self._pagesize:",
            "                break",
            "",
            "            # If we got the whole page, but the next page is not interesting,",
            "            # break out early as well",
            "            if end == nextfirstid:",
            "                break",
            "",
            "",
            "class InAdvancePagedList(PagedList):",
            "    \"\"\"PagedList with total number of pages known in advance\"\"\"",
            "",
            "    def __init__(self, pagefunc, pagecount, pagesize):",
            "        PagedList.__init__(self, pagefunc, pagesize, True)",
            "        self._pagecount = pagecount",
            "",
            "    def _getslice(self, start, end):",
            "        start_page = start // self._pagesize",
            "        end_page = self._pagecount if end is None else min(self._pagecount, end // self._pagesize + 1)",
            "        skip_elems = start - start_page * self._pagesize",
            "        only_more = None if end is None else end - start",
            "        for pagenum in range(start_page, end_page):",
            "            page_results = self.getpage(pagenum)",
            "            if skip_elems:",
            "                page_results = page_results[skip_elems:]",
            "                skip_elems = None",
            "            if only_more is not None:",
            "                if len(page_results) < only_more:",
            "                    only_more -= len(page_results)",
            "                else:",
            "                    yield from page_results[:only_more]",
            "                    break",
            "            yield from page_results",
            "",
            "",
            "class PlaylistEntries:",
            "    MissingEntry = object()",
            "    is_exhausted = False",
            "",
            "    def __init__(self, ydl, info_dict):",
            "        self.ydl = ydl",
            "",
            "        # _entries must be assigned now since infodict can change during iteration",
            "        entries = info_dict.get('entries')",
            "        if entries is None:",
            "            raise EntryNotInPlaylist('There are no entries')",
            "        elif isinstance(entries, list):",
            "            self.is_exhausted = True",
            "",
            "        requested_entries = info_dict.get('requested_entries')",
            "        self.is_incomplete = requested_entries is not None",
            "        if self.is_incomplete:",
            "            assert self.is_exhausted",
            "            self._entries = [self.MissingEntry] * max(requested_entries or [0])",
            "            for i, entry in zip(requested_entries, entries):",
            "                self._entries[i - 1] = entry",
            "        elif isinstance(entries, (list, PagedList, LazyList)):",
            "            self._entries = entries",
            "        else:",
            "            self._entries = LazyList(entries)",
            "",
            "    PLAYLIST_ITEMS_RE = re.compile(r'''(?x)",
            "        (?P<start>[+-]?\\d+)?",
            "        (?P<range>[:-]",
            "            (?P<end>[+-]?\\d+|inf(?:inite)?)?",
            "            (?::(?P<step>[+-]?\\d+))?",
            "        )?''')",
            "",
            "    @classmethod",
            "    def parse_playlist_items(cls, string):",
            "        for segment in string.split(','):",
            "            if not segment:",
            "                raise ValueError('There is two or more consecutive commas')",
            "            mobj = cls.PLAYLIST_ITEMS_RE.fullmatch(segment)",
            "            if not mobj:",
            "                raise ValueError(f'{segment!r} is not a valid specification')",
            "            start, end, step, has_range = mobj.group('start', 'end', 'step', 'range')",
            "            if int_or_none(step) == 0:",
            "                raise ValueError(f'Step in {segment!r} cannot be zero')",
            "            yield slice(int_or_none(start), float_or_none(end), int_or_none(step)) if has_range else int(start)",
            "",
            "    def get_requested_items(self):",
            "        playlist_items = self.ydl.params.get('playlist_items')",
            "        playlist_start = self.ydl.params.get('playliststart', 1)",
            "        playlist_end = self.ydl.params.get('playlistend')",
            "        # For backwards compatibility, interpret -1 as whole list",
            "        if playlist_end in (-1, None):",
            "            playlist_end = ''",
            "        if not playlist_items:",
            "            playlist_items = f'{playlist_start}:{playlist_end}'",
            "        elif playlist_start != 1 or playlist_end:",
            "            self.ydl.report_warning('Ignoring playliststart and playlistend because playlistitems was given', only_once=True)",
            "",
            "        for index in self.parse_playlist_items(playlist_items):",
            "            for i, entry in self[index]:",
            "                yield i, entry",
            "                if not entry:",
            "                    continue",
            "                try:",
            "                    # The item may have just been added to archive. Don't break due to it",
            "                    if not self.ydl.params.get('lazy_playlist'):",
            "                        # TODO: Add auto-generated fields",
            "                        self.ydl._match_entry(entry, incomplete=True, silent=True)",
            "                except (ExistingVideoReached, RejectedVideoReached):",
            "                    return",
            "",
            "    def get_full_count(self):",
            "        if self.is_exhausted and not self.is_incomplete:",
            "            return len(self)",
            "        elif isinstance(self._entries, InAdvancePagedList):",
            "            if self._entries._pagesize == 1:",
            "                return self._entries._pagecount",
            "",
            "    @functools.cached_property",
            "    def _getter(self):",
            "        if isinstance(self._entries, list):",
            "            def get_entry(i):",
            "                try:",
            "                    entry = self._entries[i]",
            "                except IndexError:",
            "                    entry = self.MissingEntry",
            "                    if not self.is_incomplete:",
            "                        raise self.IndexError()",
            "                if entry is self.MissingEntry:",
            "                    raise EntryNotInPlaylist(f'Entry {i + 1} cannot be found')",
            "                return entry",
            "        else:",
            "            def get_entry(i):",
            "                try:",
            "                    return type(self.ydl)._handle_extraction_exceptions(lambda _, i: self._entries[i])(self.ydl, i)",
            "                except (LazyList.IndexError, PagedList.IndexError):",
            "                    raise self.IndexError()",
            "        return get_entry",
            "",
            "    def __getitem__(self, idx):",
            "        if isinstance(idx, int):",
            "            idx = slice(idx, idx)",
            "",
            "        # NB: PlaylistEntries[1:10] => (0, 1, ... 9)",
            "        step = 1 if idx.step is None else idx.step",
            "        if idx.start is None:",
            "            start = 0 if step > 0 else len(self) - 1",
            "        else:",
            "            start = idx.start - 1 if idx.start >= 0 else len(self) + idx.start",
            "",
            "        # NB: Do not call len(self) when idx == [:]",
            "        if idx.stop is None:",
            "            stop = 0 if step < 0 else float('inf')",
            "        else:",
            "            stop = idx.stop - 1 if idx.stop >= 0 else len(self) + idx.stop",
            "        stop += [-1, 1][step > 0]",
            "",
            "        for i in frange(start, stop, step):",
            "            if i < 0:",
            "                continue",
            "            try:",
            "                entry = self._getter(i)",
            "            except self.IndexError:",
            "                self.is_exhausted = True",
            "                if step > 0:",
            "                    break",
            "                continue",
            "            yield i + 1, entry",
            "",
            "    def __len__(self):",
            "        return len(tuple(self[:]))",
            "",
            "    class IndexError(IndexError):",
            "        pass",
            "",
            "",
            "def uppercase_escape(s):",
            "    unicode_escape = codecs.getdecoder('unicode_escape')",
            "    return re.sub(",
            "        r'\\\\U[0-9a-fA-F]{8}',",
            "        lambda m: unicode_escape(m.group(0))[0],",
            "        s)",
            "",
            "",
            "def lowercase_escape(s):",
            "    unicode_escape = codecs.getdecoder('unicode_escape')",
            "    return re.sub(",
            "        r'\\\\u[0-9a-fA-F]{4}',",
            "        lambda m: unicode_escape(m.group(0))[0],",
            "        s)",
            "",
            "",
            "def parse_qs(url, **kwargs):",
            "    return urllib.parse.parse_qs(urllib.parse.urlparse(url).query, **kwargs)",
            "",
            "",
            "def read_batch_urls(batch_fd):",
            "    def fixup(url):",
            "        if not isinstance(url, str):",
            "            url = url.decode('utf-8', 'replace')",
            "        BOM_UTF8 = ('\\xef\\xbb\\xbf', '\\ufeff')",
            "        for bom in BOM_UTF8:",
            "            if url.startswith(bom):",
            "                url = url[len(bom):]",
            "        url = url.lstrip()",
            "        if not url or url.startswith(('#', ';', ']')):",
            "            return False",
            "        # \"#\" cannot be stripped out since it is part of the URI",
            "        # However, it can be safely stripped out if following a whitespace",
            "        return re.split(r'\\s#', url, 1)[0].rstrip()",
            "",
            "    with contextlib.closing(batch_fd) as fd:",
            "        return [url for url in map(fixup, fd) if url]",
            "",
            "",
            "def urlencode_postdata(*args, **kargs):",
            "    return urllib.parse.urlencode(*args, **kargs).encode('ascii')",
            "",
            "",
            "def update_url(url, *, query_update=None, **kwargs):",
            "    \"\"\"Replace URL components specified by kwargs",
            "       @param url           str or parse url tuple",
            "       @param query_update  update query",
            "       @returns             str",
            "    \"\"\"",
            "    if isinstance(url, str):",
            "        if not kwargs and not query_update:",
            "            return url",
            "        else:",
            "            url = urllib.parse.urlparse(url)",
            "    if query_update:",
            "        assert 'query' not in kwargs, 'query_update and query cannot be specified at the same time'",
            "        kwargs['query'] = urllib.parse.urlencode({",
            "            **urllib.parse.parse_qs(url.query),",
            "            **query_update",
            "        }, True)",
            "    return urllib.parse.urlunparse(url._replace(**kwargs))",
            "",
            "",
            "def update_url_query(url, query):",
            "    return update_url(url, query_update=query)",
            "",
            "",
            "def _multipart_encode_impl(data, boundary):",
            "    content_type = 'multipart/form-data; boundary=%s' % boundary",
            "",
            "    out = b''",
            "    for k, v in data.items():",
            "        out += b'--' + boundary.encode('ascii') + b'\\r\\n'",
            "        if isinstance(k, str):",
            "            k = k.encode()",
            "        if isinstance(v, str):",
            "            v = v.encode()",
            "        # RFC 2047 requires non-ASCII field names to be encoded, while RFC 7578",
            "        # suggests sending UTF-8 directly. Firefox sends UTF-8, too",
            "        content = b'Content-Disposition: form-data; name=\"' + k + b'\"\\r\\n\\r\\n' + v + b'\\r\\n'",
            "        if boundary.encode('ascii') in content:",
            "            raise ValueError('Boundary overlaps with data')",
            "        out += content",
            "",
            "    out += b'--' + boundary.encode('ascii') + b'--\\r\\n'",
            "",
            "    return out, content_type",
            "",
            "",
            "def multipart_encode(data, boundary=None):",
            "    '''",
            "    Encode a dict to RFC 7578-compliant form-data",
            "",
            "    data:",
            "        A dict where keys and values can be either Unicode or bytes-like",
            "        objects.",
            "    boundary:",
            "        If specified a Unicode object, it's used as the boundary. Otherwise",
            "        a random boundary is generated.",
            "",
            "    Reference: https://tools.ietf.org/html/rfc7578",
            "    '''",
            "    has_specified_boundary = boundary is not None",
            "",
            "    while True:",
            "        if boundary is None:",
            "            boundary = '---------------' + str(random.randrange(0x0fffffff, 0xffffffff))",
            "",
            "        try:",
            "            out, content_type = _multipart_encode_impl(data, boundary)",
            "            break",
            "        except ValueError:",
            "            if has_specified_boundary:",
            "                raise",
            "            boundary = None",
            "",
            "    return out, content_type",
            "",
            "",
            "def is_iterable_like(x, allowed_types=collections.abc.Iterable, blocked_types=NO_DEFAULT):",
            "    if blocked_types is NO_DEFAULT:",
            "        blocked_types = (str, bytes, collections.abc.Mapping)",
            "    return isinstance(x, allowed_types) and not isinstance(x, blocked_types)",
            "",
            "",
            "def variadic(x, allowed_types=NO_DEFAULT):",
            "    if not isinstance(allowed_types, (tuple, type)):",
            "        deprecation_warning('allowed_types should be a tuple or a type')",
            "        allowed_types = tuple(allowed_types)",
            "    return x if is_iterable_like(x, blocked_types=allowed_types) else (x, )",
            "",
            "",
            "def try_call(*funcs, expected_type=None, args=[], kwargs={}):",
            "    for f in funcs:",
            "        try:",
            "            val = f(*args, **kwargs)",
            "        except (AttributeError, KeyError, TypeError, IndexError, ValueError, ZeroDivisionError):",
            "            pass",
            "        else:",
            "            if expected_type is None or isinstance(val, expected_type):",
            "                return val",
            "",
            "",
            "def try_get(src, getter, expected_type=None):",
            "    return try_call(*variadic(getter), args=(src,), expected_type=expected_type)",
            "",
            "",
            "def filter_dict(dct, cndn=lambda _, v: v is not None):",
            "    return {k: v for k, v in dct.items() if cndn(k, v)}",
            "",
            "",
            "def merge_dicts(*dicts):",
            "    merged = {}",
            "    for a_dict in dicts:",
            "        for k, v in a_dict.items():",
            "            if (v is not None and k not in merged",
            "                    or isinstance(v, str) and merged[k] == ''):",
            "                merged[k] = v",
            "    return merged",
            "",
            "",
            "def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):",
            "    return string if isinstance(string, str) else str(string, encoding, errors)",
            "",
            "",
            "US_RATINGS = {",
            "    'G': 0,",
            "    'PG': 10,",
            "    'PG-13': 13,",
            "    'R': 16,",
            "    'NC': 18,",
            "}",
            "",
            "",
            "TV_PARENTAL_GUIDELINES = {",
            "    'TV-Y': 0,",
            "    'TV-Y7': 7,",
            "    'TV-G': 0,",
            "    'TV-PG': 0,",
            "    'TV-14': 14,",
            "    'TV-MA': 17,",
            "}",
            "",
            "",
            "def parse_age_limit(s):",
            "    # isinstance(False, int) is True. So type() must be used instead",
            "    if type(s) is int:  # noqa: E721",
            "        return s if 0 <= s <= 21 else None",
            "    elif not isinstance(s, str):",
            "        return None",
            "    m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)",
            "    if m:",
            "        return int(m.group('age'))",
            "    s = s.upper()",
            "    if s in US_RATINGS:",
            "        return US_RATINGS[s]",
            "    m = re.match(r'^TV[_-]?(%s)$' % '|'.join(k[3:] for k in TV_PARENTAL_GUIDELINES), s)",
            "    if m:",
            "        return TV_PARENTAL_GUIDELINES['TV-' + m.group(1)]",
            "    return None",
            "",
            "",
            "def strip_jsonp(code):",
            "    return re.sub(",
            "        r'''(?sx)^",
            "            (?:window\\.)?(?P<func_name>[a-zA-Z0-9_.$]*)",
            "            (?:\\s*&&\\s*(?P=func_name))?",
            "            \\s*\\(\\s*(?P<callback_data>.*)\\);?",
            "            \\s*?(?://[^\\n]*)*$''',",
            "        r'\\g<callback_data>', code)",
            "",
            "",
            "def js_to_json(code, vars={}, *, strict=False):",
            "    # vars is a dict of var, val pairs to substitute",
            "    STRING_QUOTES = '\\'\"`'",
            "    STRING_RE = '|'.join(rf'{q}(?:\\\\.|[^\\\\{q}])*{q}' for q in STRING_QUOTES)",
            "    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'",
            "    SKIP_RE = fr'\\s*(?:{COMMENT_RE})?\\s*'",
            "    INTEGER_TABLE = (",
            "        (fr'(?s)^(0[xX][0-9a-fA-F]+){SKIP_RE}:?$', 16),",
            "        (fr'(?s)^(0+[0-7]+){SKIP_RE}:?$', 8),",
            "    )",
            "",
            "    def process_escape(match):",
            "        JSON_PASSTHROUGH_ESCAPES = R'\"\\bfnrtu'",
            "        escape = match.group(1) or match.group(2)",
            "",
            "        return (Rf'\\{escape}' if escape in JSON_PASSTHROUGH_ESCAPES",
            "                else R'\\u00' if escape == 'x'",
            "                else '' if escape == '\\n'",
            "                else escape)",
            "",
            "    def template_substitute(match):",
            "        evaluated = js_to_json(match.group(1), vars, strict=strict)",
            "        if evaluated[0] == '\"':",
            "            return json.loads(evaluated)",
            "        return evaluated",
            "",
            "    def fix_kv(m):",
            "        v = m.group(0)",
            "        if v in ('true', 'false', 'null'):",
            "            return v",
            "        elif v in ('undefined', 'void 0'):",
            "            return 'null'",
            "        elif v.startswith('/*') or v.startswith('//') or v.startswith('!') or v == ',':",
            "            return ''",
            "",
            "        if v[0] in STRING_QUOTES:",
            "            v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]",
            "            escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)",
            "            return f'\"{escaped}\"'",
            "",
            "        for regex, base in INTEGER_TABLE:",
            "            im = re.match(regex, v)",
            "            if im:",
            "                i = int(im.group(1), base)",
            "                return f'\"{i}\":' if v.endswith(':') else str(i)",
            "",
            "        if v in vars:",
            "            try:",
            "                if not strict:",
            "                    json.loads(vars[v])",
            "            except json.JSONDecodeError:",
            "                return json.dumps(vars[v])",
            "            else:",
            "                return vars[v]",
            "",
            "        if not strict:",
            "            return f'\"{v}\"'",
            "",
            "        raise ValueError(f'Unknown value: {v}')",
            "",
            "    def create_map(mobj):",
            "        return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))",
            "",
            "    code = re.sub(r'(?:new\\s+)?Array\\((.*?)\\)', r'[\\g<1>]', code)",
            "    code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)",
            "    if not strict:",
            "        code = re.sub(rf'new Date\\(({STRING_RE})\\)', r'\\g<1>', code)",
            "        code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)",
            "        code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)",
            "        code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)",
            "",
            "    return re.sub(rf'''(?sx)",
            "        {STRING_RE}|",
            "        {COMMENT_RE}|,(?={SKIP_RE}[\\]}}])|",
            "        void\\s0|(?:(?<![0-9])[eE]|[a-df-zA-DF-Z_$])[.a-zA-Z_$0-9]*|",
            "        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{SKIP_RE}:)?|",
            "        [0-9]+(?={SKIP_RE}:)|",
            "        !+",
            "        ''', fix_kv, code)",
            "",
            "",
            "def qualities(quality_ids):",
            "    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"",
            "    def q(qid):",
            "        try:",
            "            return quality_ids.index(qid)",
            "        except ValueError:",
            "            return -1",
            "    return q",
            "",
            "",
            "POSTPROCESS_WHEN = ('pre_process', 'after_filter', 'video', 'before_dl', 'post_process', 'after_move', 'after_video', 'playlist')",
            "",
            "",
            "DEFAULT_OUTTMPL = {",
            "    'default': '%(title)s [%(id)s].%(ext)s',",
            "    'chapter': '%(title)s - %(section_number)03d %(section_title)s [%(id)s].%(ext)s',",
            "}",
            "OUTTMPL_TYPES = {",
            "    'chapter': None,",
            "    'subtitle': None,",
            "    'thumbnail': None,",
            "    'description': 'description',",
            "    'annotation': 'annotations.xml',",
            "    'infojson': 'info.json',",
            "    'link': None,",
            "    'pl_video': None,",
            "    'pl_thumbnail': None,",
            "    'pl_description': 'description',",
            "    'pl_infojson': 'info.json',",
            "}",
            "",
            "# As of [1] format syntax is:",
            "#  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type",
            "# 1. https://docs.python.org/2/library/stdtypes.html#string-formatting",
            "STR_FORMAT_RE_TMPL = r'''(?x)",
            "    (?<!%)(?P<prefix>(?:%%)*)",
            "    %",
            "    (?P<has_key>\\((?P<key>{0})\\))?",
            "    (?P<format>",
            "        (?P<conversion>[#0\\-+ ]+)?",
            "        (?P<min_width>\\d+)?",
            "        (?P<precision>\\.\\d+)?",
            "        (?P<len_mod>[hlL])?  # unused in python",
            "        {1}  # conversion type",
            "    )",
            "'''",
            "",
            "",
            "STR_FORMAT_TYPES = 'diouxXeEfFgGcrsa'",
            "",
            "",
            "def limit_length(s, length):",
            "    \"\"\" Add ellipses to overly long strings \"\"\"",
            "    if s is None:",
            "        return None",
            "    ELLIPSES = '...'",
            "    if len(s) > length:",
            "        return s[:length - len(ELLIPSES)] + ELLIPSES",
            "    return s",
            "",
            "",
            "def version_tuple(v):",
            "    return tuple(int(e) for e in re.split(r'[-.]', v))",
            "",
            "",
            "def is_outdated_version(version, limit, assume_new=True):",
            "    if not version:",
            "        return not assume_new",
            "    try:",
            "        return version_tuple(version) < version_tuple(limit)",
            "    except ValueError:",
            "        return not assume_new",
            "",
            "",
            "def ytdl_is_updateable():",
            "    \"\"\" Returns if yt-dlp can be updated with -U \"\"\"",
            "",
            "    from ..update import is_non_updateable",
            "",
            "    return not is_non_updateable()",
            "",
            "",
            "def args_to_str(args):",
            "    # Get a short string representation for a subprocess command",
            "    return ' '.join(compat_shlex_quote(a) for a in args)",
            "",
            "",
            "def error_to_str(err):",
            "    return f'{type(err).__name__}: {err}'",
            "",
            "",
            "def mimetype2ext(mt, default=NO_DEFAULT):",
            "    if not isinstance(mt, str):",
            "        if default is not NO_DEFAULT:",
            "            return default",
            "        return None",
            "",
            "    MAP = {",
            "        # video",
            "        '3gpp': '3gp',",
            "        'mp2t': 'ts',",
            "        'mp4': 'mp4',",
            "        'mpeg': 'mpeg',",
            "        'mpegurl': 'm3u8',",
            "        'quicktime': 'mov',",
            "        'webm': 'webm',",
            "        'vp9': 'vp9',",
            "        'video/ogg': 'ogv',",
            "        'x-flv': 'flv',",
            "        'x-m4v': 'm4v',",
            "        'x-matroska': 'mkv',",
            "        'x-mng': 'mng',",
            "        'x-mp4-fragmented': 'mp4',",
            "        'x-ms-asf': 'asf',",
            "        'x-ms-wmv': 'wmv',",
            "        'x-msvideo': 'avi',",
            "",
            "        # application (streaming playlists)",
            "        'dash+xml': 'mpd',",
            "        'f4m+xml': 'f4m',",
            "        'hds+xml': 'f4m',",
            "        'vnd.apple.mpegurl': 'm3u8',",
            "        'vnd.ms-sstr+xml': 'ism',",
            "        'x-mpegurl': 'm3u8',",
            "",
            "        # audio",
            "        'audio/mp4': 'm4a',",
            "        # Per RFC 3003, audio/mpeg can be .mp1, .mp2 or .mp3.",
            "        # Using .mp3 as it's the most popular one",
            "        'audio/mpeg': 'mp3',",
            "        'audio/webm': 'webm',",
            "        'audio/x-matroska': 'mka',",
            "        'audio/x-mpegurl': 'm3u',",
            "        'midi': 'mid',",
            "        'ogg': 'ogg',",
            "        'wav': 'wav',",
            "        'wave': 'wav',",
            "        'x-aac': 'aac',",
            "        'x-flac': 'flac',",
            "        'x-m4a': 'm4a',",
            "        'x-realaudio': 'ra',",
            "        'x-wav': 'wav',",
            "",
            "        # image",
            "        'avif': 'avif',",
            "        'bmp': 'bmp',",
            "        'gif': 'gif',",
            "        'jpeg': 'jpg',",
            "        'png': 'png',",
            "        'svg+xml': 'svg',",
            "        'tiff': 'tif',",
            "        'vnd.wap.wbmp': 'wbmp',",
            "        'webp': 'webp',",
            "        'x-icon': 'ico',",
            "        'x-jng': 'jng',",
            "        'x-ms-bmp': 'bmp',",
            "",
            "        # caption",
            "        'filmstrip+json': 'fs',",
            "        'smptett+xml': 'tt',",
            "        'ttaf+xml': 'dfxp',",
            "        'ttml+xml': 'ttml',",
            "        'x-ms-sami': 'sami',",
            "",
            "        # misc",
            "        'gzip': 'gz',",
            "        'json': 'json',",
            "        'xml': 'xml',",
            "        'zip': 'zip',",
            "    }",
            "",
            "    mimetype = mt.partition(';')[0].strip().lower()",
            "    _, _, subtype = mimetype.rpartition('/')",
            "",
            "    ext = traversal.traverse_obj(MAP, mimetype, subtype, subtype.rsplit('+')[-1])",
            "    if ext:",
            "        return ext",
            "    elif default is not NO_DEFAULT:",
            "        return default",
            "    return subtype.replace('+', '.')",
            "",
            "",
            "def ext2mimetype(ext_or_url):",
            "    if not ext_or_url:",
            "        return None",
            "    if '.' not in ext_or_url:",
            "        ext_or_url = f'file.{ext_or_url}'",
            "    return mimetypes.guess_type(ext_or_url)[0]",
            "",
            "",
            "def parse_codecs(codecs_str):",
            "    # http://tools.ietf.org/html/rfc6381",
            "    if not codecs_str:",
            "        return {}",
            "    split_codecs = list(filter(None, map(",
            "        str.strip, codecs_str.strip().strip(',').split(','))))",
            "    vcodec, acodec, scodec, hdr = None, None, None, None",
            "    for full_codec in split_codecs:",
            "        parts = re.sub(r'0+(?=\\d)', '', full_codec).split('.')",
            "        if parts[0] in ('avc1', 'avc2', 'avc3', 'avc4', 'vp9', 'vp8', 'hev1', 'hev2',",
            "                        'h263', 'h264', 'mp4v', 'hvc1', 'av1', 'theora', 'dvh1', 'dvhe'):",
            "            if vcodec:",
            "                continue",
            "            vcodec = full_codec",
            "            if parts[0] in ('dvh1', 'dvhe'):",
            "                hdr = 'DV'",
            "            elif parts[0] == 'av1' and traversal.traverse_obj(parts, 3) == '10':",
            "                hdr = 'HDR10'",
            "            elif parts[:2] == ['vp9', '2']:",
            "                hdr = 'HDR10'",
            "        elif parts[0] in ('flac', 'mp4a', 'opus', 'vorbis', 'mp3', 'aac', 'ac-4',",
            "                          'ac-3', 'ec-3', 'eac3', 'dtsc', 'dtse', 'dtsh', 'dtsl'):",
            "            acodec = acodec or full_codec",
            "        elif parts[0] in ('stpp', 'wvtt'):",
            "            scodec = scodec or full_codec",
            "        else:",
            "            write_string(f'WARNING: Unknown codec {full_codec}\\n')",
            "    if vcodec or acodec or scodec:",
            "        return {",
            "            'vcodec': vcodec or 'none',",
            "            'acodec': acodec or 'none',",
            "            'dynamic_range': hdr,",
            "            **({'scodec': scodec} if scodec is not None else {}),",
            "        }",
            "    elif len(split_codecs) == 2:",
            "        return {",
            "            'vcodec': split_codecs[0],",
            "            'acodec': split_codecs[1],",
            "        }",
            "    return {}",
            "",
            "",
            "def get_compatible_ext(*, vcodecs, acodecs, vexts, aexts, preferences=None):",
            "    assert len(vcodecs) == len(vexts) and len(acodecs) == len(aexts)",
            "",
            "    allow_mkv = not preferences or 'mkv' in preferences",
            "",
            "    if allow_mkv and max(len(acodecs), len(vcodecs)) > 1:",
            "        return 'mkv'  # TODO: any other format allows this?",
            "",
            "    # TODO: All codecs supported by parse_codecs isn't handled here",
            "    COMPATIBLE_CODECS = {",
            "        'mp4': {",
            "            'av1', 'hevc', 'avc1', 'mp4a', 'ac-4',  # fourcc (m3u8, mpd)",
            "            'h264', 'aacl', 'ec-3',  # Set in ISM",
            "        },",
            "        'webm': {",
            "            'av1', 'vp9', 'vp8', 'opus', 'vrbs',",
            "            'vp9x', 'vp8x',  # in the webm spec",
            "        },",
            "    }",
            "",
            "    sanitize_codec = functools.partial(",
            "        try_get, getter=lambda x: x[0].split('.')[0].replace('0', '').lower())",
            "    vcodec, acodec = sanitize_codec(vcodecs), sanitize_codec(acodecs)",
            "",
            "    for ext in preferences or COMPATIBLE_CODECS.keys():",
            "        codec_set = COMPATIBLE_CODECS.get(ext, set())",
            "        if ext == 'mkv' or codec_set.issuperset((vcodec, acodec)):",
            "            return ext",
            "",
            "    COMPATIBLE_EXTS = (",
            "        {'mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma', 'mov'},",
            "        {'webm', 'weba'},",
            "    )",
            "    for ext in preferences or vexts:",
            "        current_exts = {ext, *vexts, *aexts}",
            "        if ext == 'mkv' or current_exts == {ext} or any(",
            "                ext_sets.issuperset(current_exts) for ext_sets in COMPATIBLE_EXTS):",
            "            return ext",
            "    return 'mkv' if allow_mkv else preferences[-1]",
            "",
            "",
            "def urlhandle_detect_ext(url_handle, default=NO_DEFAULT):",
            "    getheader = url_handle.headers.get",
            "",
            "    cd = getheader('Content-Disposition')",
            "    if cd:",
            "        m = re.match(r'attachment;\\s*filename=\"(?P<filename>[^\"]+)\"', cd)",
            "        if m:",
            "            e = determine_ext(m.group('filename'), default_ext=None)",
            "            if e:",
            "                return e",
            "",
            "    meta_ext = getheader('x-amz-meta-name')",
            "    if meta_ext:",
            "        e = meta_ext.rpartition('.')[2]",
            "        if e:",
            "            return e",
            "",
            "    return mimetype2ext(getheader('Content-Type'), default=default)",
            "",
            "",
            "def encode_data_uri(data, mime_type):",
            "    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))",
            "",
            "",
            "def age_restricted(content_limit, age_limit):",
            "    \"\"\" Returns True iff the content should be blocked \"\"\"",
            "",
            "    if age_limit is None:  # No limit set",
            "        return False",
            "    if content_limit is None:",
            "        return False  # Content available for everyone",
            "    return age_limit < content_limit",
            "",
            "",
            "# List of known byte-order-marks (BOM)",
            "BOMS = [",
            "    (b'\\xef\\xbb\\xbf', 'utf-8'),",
            "    (b'\\x00\\x00\\xfe\\xff', 'utf-32-be'),",
            "    (b'\\xff\\xfe\\x00\\x00', 'utf-32-le'),",
            "    (b'\\xff\\xfe', 'utf-16-le'),",
            "    (b'\\xfe\\xff', 'utf-16-be'),",
            "]",
            "",
            "",
            "def is_html(first_bytes):",
            "    \"\"\" Detect whether a file contains HTML by examining its first bytes. \"\"\"",
            "",
            "    encoding = 'utf-8'",
            "    for bom, enc in BOMS:",
            "        while first_bytes.startswith(bom):",
            "            encoding, first_bytes = enc, first_bytes[len(bom):]",
            "",
            "    return re.match(r'^\\s*<', first_bytes.decode(encoding, 'replace'))",
            "",
            "",
            "def determine_protocol(info_dict):",
            "    protocol = info_dict.get('protocol')",
            "    if protocol is not None:",
            "        return protocol",
            "",
            "    url = sanitize_url(info_dict['url'])",
            "    if url.startswith('rtmp'):",
            "        return 'rtmp'",
            "    elif url.startswith('mms'):",
            "        return 'mms'",
            "    elif url.startswith('rtsp'):",
            "        return 'rtsp'",
            "",
            "    ext = determine_ext(url)",
            "    if ext == 'm3u8':",
            "        return 'm3u8' if info_dict.get('is_live') else 'm3u8_native'",
            "    elif ext == 'f4m':",
            "        return 'f4m'",
            "",
            "    return urllib.parse.urlparse(url).scheme",
            "",
            "",
            "def render_table(header_row, data, delim=False, extra_gap=0, hide_empty=False):",
            "    \"\"\" Render a list of rows, each as a list of values.",
            "    Text after a \\t will be right aligned \"\"\"",
            "    def width(string):",
            "        return len(remove_terminal_sequences(string).replace('\\t', ''))",
            "",
            "    def get_max_lens(table):",
            "        return [max(width(str(v)) for v in col) for col in zip(*table)]",
            "",
            "    def filter_using_list(row, filterArray):",
            "        return [col for take, col in itertools.zip_longest(filterArray, row, fillvalue=True) if take]",
            "",
            "    max_lens = get_max_lens(data) if hide_empty else []",
            "    header_row = filter_using_list(header_row, max_lens)",
            "    data = [filter_using_list(row, max_lens) for row in data]",
            "",
            "    table = [header_row] + data",
            "    max_lens = get_max_lens(table)",
            "    extra_gap += 1",
            "    if delim:",
            "        table = [header_row, [delim * (ml + extra_gap) for ml in max_lens]] + data",
            "        table[1][-1] = table[1][-1][:-extra_gap * len(delim)]  # Remove extra_gap from end of delimiter",
            "    for row in table:",
            "        for pos, text in enumerate(map(str, row)):",
            "            if '\\t' in text:",
            "                row[pos] = text.replace('\\t', ' ' * (max_lens[pos] - width(text))) + ' ' * extra_gap",
            "            else:",
            "                row[pos] = text + ' ' * (max_lens[pos] - width(text) + extra_gap)",
            "    ret = '\\n'.join(''.join(row).rstrip() for row in table)",
            "    return ret",
            "",
            "",
            "def _match_one(filter_part, dct, incomplete):",
            "    # TODO: Generalize code with YoutubeDL._build_format_filter",
            "    STRING_OPERATORS = {",
            "        '*=': operator.contains,",
            "        '^=': lambda attr, value: attr.startswith(value),",
            "        '$=': lambda attr, value: attr.endswith(value),",
            "        '~=': lambda attr, value: re.search(value, attr),",
            "    }",
            "    COMPARISON_OPERATORS = {",
            "        **STRING_OPERATORS,",
            "        '<=': operator.le,  # \"<=\" must be defined above \"<\"",
            "        '<': operator.lt,",
            "        '>=': operator.ge,",
            "        '>': operator.gt,",
            "        '=': operator.eq,",
            "    }",
            "",
            "    if isinstance(incomplete, bool):",
            "        is_incomplete = lambda _: incomplete",
            "    else:",
            "        is_incomplete = lambda k: k in incomplete",
            "",
            "    operator_rex = re.compile(r'''(?x)",
            "        (?P<key>[a-z_]+)",
            "        \\s*(?P<negation>!\\s*)?(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*",
            "        (?:",
            "            (?P<quote>[\"\\'])(?P<quotedstrval>.+?)(?P=quote)|",
            "            (?P<strval>.+?)",
            "        )",
            "        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))",
            "    m = operator_rex.fullmatch(filter_part.strip())",
            "    if m:",
            "        m = m.groupdict()",
            "        unnegated_op = COMPARISON_OPERATORS[m['op']]",
            "        if m['negation']:",
            "            op = lambda attr, value: not unnegated_op(attr, value)",
            "        else:",
            "            op = unnegated_op",
            "        comparison_value = m['quotedstrval'] or m['strval'] or m['intval']",
            "        if m['quote']:",
            "            comparison_value = comparison_value.replace(r'\\%s' % m['quote'], m['quote'])",
            "        actual_value = dct.get(m['key'])",
            "        numeric_comparison = None",
            "        if isinstance(actual_value, (int, float)):",
            "            # If the original field is a string and matching comparisonvalue is",
            "            # a number we should respect the origin of the original field",
            "            # and process comparison value as a string (see",
            "            # https://github.com/ytdl-org/youtube-dl/issues/11082)",
            "            try:",
            "                numeric_comparison = int(comparison_value)",
            "            except ValueError:",
            "                numeric_comparison = parse_filesize(comparison_value)",
            "                if numeric_comparison is None:",
            "                    numeric_comparison = parse_filesize(f'{comparison_value}B')",
            "                if numeric_comparison is None:",
            "                    numeric_comparison = parse_duration(comparison_value)",
            "        if numeric_comparison is not None and m['op'] in STRING_OPERATORS:",
            "            raise ValueError('Operator %s only supports string values!' % m['op'])",
            "        if actual_value is None:",
            "            return is_incomplete(m['key']) or m['none_inclusive']",
            "        return op(actual_value, comparison_value if numeric_comparison is None else numeric_comparison)",
            "",
            "    UNARY_OPERATORS = {",
            "        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),",
            "        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),",
            "    }",
            "    operator_rex = re.compile(r'''(?x)",
            "        (?P<op>%s)\\s*(?P<key>[a-z_]+)",
            "        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))",
            "    m = operator_rex.fullmatch(filter_part.strip())",
            "    if m:",
            "        op = UNARY_OPERATORS[m.group('op')]",
            "        actual_value = dct.get(m.group('key'))",
            "        if is_incomplete(m.group('key')) and actual_value is None:",
            "            return True",
            "        return op(actual_value)",
            "",
            "    raise ValueError('Invalid filter part %r' % filter_part)",
            "",
            "",
            "def match_str(filter_str, dct, incomplete=False):",
            "    \"\"\" Filter a dictionary with a simple string syntax.",
            "    @returns           Whether the filter passes",
            "    @param incomplete  Set of keys that is expected to be missing from dct.",
            "                       Can be True/False to indicate all/none of the keys may be missing.",
            "                       All conditions on incomplete keys pass if the key is missing",
            "    \"\"\"",
            "    return all(",
            "        _match_one(filter_part.replace(r'\\&', '&'), dct, incomplete)",
            "        for filter_part in re.split(r'(?<!\\\\)&', filter_str))",
            "",
            "",
            "def match_filter_func(filters, breaking_filters=None):",
            "    if not filters and not breaking_filters:",
            "        return None",
            "    repr_ = f'{match_filter_func.__module__}.{match_filter_func.__qualname__}({filters}, {breaking_filters})'",
            "",
            "    breaking_filters = match_filter_func(breaking_filters) or (lambda _, __: None)",
            "    filters = set(variadic(filters or []))",
            "",
            "    interactive = '-' in filters",
            "    if interactive:",
            "        filters.remove('-')",
            "",
            "    @function_with_repr.set_repr(repr_)",
            "    def _match_func(info_dict, incomplete=False):",
            "        ret = breaking_filters(info_dict, incomplete)",
            "        if ret is not None:",
            "            raise RejectedVideoReached(ret)",
            "",
            "        if not filters or any(match_str(f, info_dict, incomplete) for f in filters):",
            "            return NO_DEFAULT if interactive and not incomplete else None",
            "        else:",
            "            video_title = info_dict.get('title') or info_dict.get('id') or 'entry'",
            "            filter_str = ') | ('.join(map(str.strip, filters))",
            "            return f'{video_title} does not pass filter ({filter_str}), skipping ..'",
            "    return _match_func",
            "",
            "",
            "class download_range_func:",
            "    def __init__(self, chapters, ranges, from_info=False):",
            "        self.chapters, self.ranges, self.from_info = chapters, ranges, from_info",
            "",
            "    def __call__(self, info_dict, ydl):",
            "",
            "        warning = ('There are no chapters matching the regex' if info_dict.get('chapters')",
            "                   else 'Cannot match chapters since chapter information is unavailable')",
            "        for regex in self.chapters or []:",
            "            for i, chapter in enumerate(info_dict.get('chapters') or []):",
            "                if re.search(regex, chapter['title']):",
            "                    warning = None",
            "                    yield {**chapter, 'index': i}",
            "        if self.chapters and warning:",
            "            ydl.to_screen(f'[info] {info_dict[\"id\"]}: {warning}')",
            "",
            "        for start, end in self.ranges or []:",
            "            yield {",
            "                'start_time': self._handle_negative_timestamp(start, info_dict),",
            "                'end_time': self._handle_negative_timestamp(end, info_dict),",
            "            }",
            "",
            "        if self.from_info and (info_dict.get('start_time') or info_dict.get('end_time')):",
            "            yield {",
            "                'start_time': info_dict.get('start_time') or 0,",
            "                'end_time': info_dict.get('end_time') or float('inf'),",
            "            }",
            "        elif not self.ranges and not self.chapters:",
            "            yield {}",
            "",
            "    @staticmethod",
            "    def _handle_negative_timestamp(time, info):",
            "        return max(info['duration'] + time, 0) if info.get('duration') and time < 0 else time",
            "",
            "    def __eq__(self, other):",
            "        return (isinstance(other, download_range_func)",
            "                and self.chapters == other.chapters and self.ranges == other.ranges)",
            "",
            "    def __repr__(self):",
            "        return f'{__name__}.{type(self).__name__}({self.chapters}, {self.ranges})'",
            "",
            "",
            "def parse_dfxp_time_expr(time_expr):",
            "    if not time_expr:",
            "        return",
            "",
            "    mobj = re.match(rf'^(?P<time_offset>{NUMBER_RE})s?$', time_expr)",
            "    if mobj:",
            "        return float(mobj.group('time_offset'))",
            "",
            "    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)",
            "    if mobj:",
            "        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))",
            "",
            "",
            "def srt_subtitles_timecode(seconds):",
            "    return '%02d:%02d:%02d,%03d' % timetuple_from_msec(seconds * 1000)",
            "",
            "",
            "def ass_subtitles_timecode(seconds):",
            "    time = timetuple_from_msec(seconds * 1000)",
            "    return '%01d:%02d:%02d.%02d' % (*time[:-1], time.milliseconds / 10)",
            "",
            "",
            "def dfxp2srt(dfxp_data):",
            "    '''",
            "    @param dfxp_data A bytes-like object containing DFXP data",
            "    @returns A unicode object containing converted SRT data",
            "    '''",
            "    LEGACY_NAMESPACES = (",
            "        (b'http://www.w3.org/ns/ttml', [",
            "            b'http://www.w3.org/2004/11/ttaf1',",
            "            b'http://www.w3.org/2006/04/ttaf1',",
            "            b'http://www.w3.org/2006/10/ttaf1',",
            "        ]),",
            "        (b'http://www.w3.org/ns/ttml#styling', [",
            "            b'http://www.w3.org/ns/ttml#style',",
            "        ]),",
            "    )",
            "",
            "    SUPPORTED_STYLING = [",
            "        'color',",
            "        'fontFamily',",
            "        'fontSize',",
            "        'fontStyle',",
            "        'fontWeight',",
            "        'textDecoration'",
            "    ]",
            "",
            "    _x = functools.partial(xpath_with_ns, ns_map={",
            "        'xml': 'http://www.w3.org/XML/1998/namespace',",
            "        'ttml': 'http://www.w3.org/ns/ttml',",
            "        'tts': 'http://www.w3.org/ns/ttml#styling',",
            "    })",
            "",
            "    styles = {}",
            "    default_style = {}",
            "",
            "    class TTMLPElementParser:",
            "        _out = ''",
            "        _unclosed_elements = []",
            "        _applied_styles = []",
            "",
            "        def start(self, tag, attrib):",
            "            if tag in (_x('ttml:br'), 'br'):",
            "                self._out += '\\n'",
            "            else:",
            "                unclosed_elements = []",
            "                style = {}",
            "                element_style_id = attrib.get('style')",
            "                if default_style:",
            "                    style.update(default_style)",
            "                if element_style_id:",
            "                    style.update(styles.get(element_style_id, {}))",
            "                for prop in SUPPORTED_STYLING:",
            "                    prop_val = attrib.get(_x('tts:' + prop))",
            "                    if prop_val:",
            "                        style[prop] = prop_val",
            "                if style:",
            "                    font = ''",
            "                    for k, v in sorted(style.items()):",
            "                        if self._applied_styles and self._applied_styles[-1].get(k) == v:",
            "                            continue",
            "                        if k == 'color':",
            "                            font += ' color=\"%s\"' % v",
            "                        elif k == 'fontSize':",
            "                            font += ' size=\"%s\"' % v",
            "                        elif k == 'fontFamily':",
            "                            font += ' face=\"%s\"' % v",
            "                        elif k == 'fontWeight' and v == 'bold':",
            "                            self._out += '<b>'",
            "                            unclosed_elements.append('b')",
            "                        elif k == 'fontStyle' and v == 'italic':",
            "                            self._out += '<i>'",
            "                            unclosed_elements.append('i')",
            "                        elif k == 'textDecoration' and v == 'underline':",
            "                            self._out += '<u>'",
            "                            unclosed_elements.append('u')",
            "                    if font:",
            "                        self._out += '<font' + font + '>'",
            "                        unclosed_elements.append('font')",
            "                    applied_style = {}",
            "                    if self._applied_styles:",
            "                        applied_style.update(self._applied_styles[-1])",
            "                    applied_style.update(style)",
            "                    self._applied_styles.append(applied_style)",
            "                self._unclosed_elements.append(unclosed_elements)",
            "",
            "        def end(self, tag):",
            "            if tag not in (_x('ttml:br'), 'br'):",
            "                unclosed_elements = self._unclosed_elements.pop()",
            "                for element in reversed(unclosed_elements):",
            "                    self._out += '</%s>' % element",
            "                if unclosed_elements and self._applied_styles:",
            "                    self._applied_styles.pop()",
            "",
            "        def data(self, data):",
            "            self._out += data",
            "",
            "        def close(self):",
            "            return self._out.strip()",
            "",
            "    # Fix UTF-8 encoded file wrongly marked as UTF-16. See https://github.com/yt-dlp/yt-dlp/issues/6543#issuecomment-1477169870",
            "    # This will not trigger false positives since only UTF-8 text is being replaced",
            "    dfxp_data = dfxp_data.replace(b'encoding=\\'UTF-16\\'', b'encoding=\\'UTF-8\\'')",
            "",
            "    def parse_node(node):",
            "        target = TTMLPElementParser()",
            "        parser = xml.etree.ElementTree.XMLParser(target=target)",
            "        parser.feed(xml.etree.ElementTree.tostring(node))",
            "        return parser.close()",
            "",
            "    for k, v in LEGACY_NAMESPACES:",
            "        for ns in v:",
            "            dfxp_data = dfxp_data.replace(ns, k)",
            "",
            "    dfxp = compat_etree_fromstring(dfxp_data)",
            "    out = []",
            "    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')",
            "",
            "    if not paras:",
            "        raise ValueError('Invalid dfxp/TTML subtitle')",
            "",
            "    repeat = False",
            "    while True:",
            "        for style in dfxp.findall(_x('.//ttml:style')):",
            "            style_id = style.get('id') or style.get(_x('xml:id'))",
            "            if not style_id:",
            "                continue",
            "            parent_style_id = style.get('style')",
            "            if parent_style_id:",
            "                if parent_style_id not in styles:",
            "                    repeat = True",
            "                    continue",
            "                styles[style_id] = styles[parent_style_id].copy()",
            "            for prop in SUPPORTED_STYLING:",
            "                prop_val = style.get(_x('tts:' + prop))",
            "                if prop_val:",
            "                    styles.setdefault(style_id, {})[prop] = prop_val",
            "        if repeat:",
            "            repeat = False",
            "        else:",
            "            break",
            "",
            "    for p in ('body', 'div'):",
            "        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])",
            "        if ele is None:",
            "            continue",
            "        style = styles.get(ele.get('style'))",
            "        if not style:",
            "            continue",
            "        default_style.update(style)",
            "",
            "    for para, index in zip(paras, itertools.count(1)):",
            "        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))",
            "        end_time = parse_dfxp_time_expr(para.attrib.get('end'))",
            "        dur = parse_dfxp_time_expr(para.attrib.get('dur'))",
            "        if begin_time is None:",
            "            continue",
            "        if not end_time:",
            "            if not dur:",
            "                continue",
            "            end_time = begin_time + dur",
            "        out.append('%d\\n%s --> %s\\n%s\\n\\n' % (",
            "            index,",
            "            srt_subtitles_timecode(begin_time),",
            "            srt_subtitles_timecode(end_time),",
            "            parse_node(para)))",
            "",
            "    return ''.join(out)",
            "",
            "",
            "def cli_option(params, command_option, param, separator=None):",
            "    param = params.get(param)",
            "    return ([] if param is None",
            "            else [command_option, str(param)] if separator is None",
            "            else [f'{command_option}{separator}{param}'])",
            "",
            "",
            "def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):",
            "    param = params.get(param)",
            "    assert param in (True, False, None)",
            "    return cli_option({True: true_value, False: false_value}, command_option, param, separator)",
            "",
            "",
            "def cli_valueless_option(params, command_option, param, expected_value=True):",
            "    return [command_option] if params.get(param) == expected_value else []",
            "",
            "",
            "def cli_configuration_args(argdict, keys, default=[], use_compat=True):",
            "    if isinstance(argdict, (list, tuple)):  # for backward compatibility",
            "        if use_compat:",
            "            return argdict",
            "        else:",
            "            argdict = None",
            "    if argdict is None:",
            "        return default",
            "    assert isinstance(argdict, dict)",
            "",
            "    assert isinstance(keys, (list, tuple))",
            "    for key_list in keys:",
            "        arg_list = list(filter(",
            "            lambda x: x is not None,",
            "            [argdict.get(key.lower()) for key in variadic(key_list)]))",
            "        if arg_list:",
            "            return [arg for args in arg_list for arg in args]",
            "    return default",
            "",
            "",
            "def _configuration_args(main_key, argdict, exe, keys=None, default=[], use_compat=True):",
            "    main_key, exe = main_key.lower(), exe.lower()",
            "    root_key = exe if main_key == exe else f'{main_key}+{exe}'",
            "    keys = [f'{root_key}{k}' for k in (keys or [''])]",
            "    if root_key in keys:",
            "        if main_key != exe:",
            "            keys.append((main_key, exe))",
            "        keys.append('default')",
            "    else:",
            "        use_compat = False",
            "    return cli_configuration_args(argdict, keys, default, use_compat)",
            "",
            "",
            "class ISO639Utils:",
            "    # See http://www.loc.gov/standards/iso639-2/ISO-639-2_utf-8.txt",
            "    _lang_map = {",
            "        'aa': 'aar',",
            "        'ab': 'abk',",
            "        'ae': 'ave',",
            "        'af': 'afr',",
            "        'ak': 'aka',",
            "        'am': 'amh',",
            "        'an': 'arg',",
            "        'ar': 'ara',",
            "        'as': 'asm',",
            "        'av': 'ava',",
            "        'ay': 'aym',",
            "        'az': 'aze',",
            "        'ba': 'bak',",
            "        'be': 'bel',",
            "        'bg': 'bul',",
            "        'bh': 'bih',",
            "        'bi': 'bis',",
            "        'bm': 'bam',",
            "        'bn': 'ben',",
            "        'bo': 'bod',",
            "        'br': 'bre',",
            "        'bs': 'bos',",
            "        'ca': 'cat',",
            "        'ce': 'che',",
            "        'ch': 'cha',",
            "        'co': 'cos',",
            "        'cr': 'cre',",
            "        'cs': 'ces',",
            "        'cu': 'chu',",
            "        'cv': 'chv',",
            "        'cy': 'cym',",
            "        'da': 'dan',",
            "        'de': 'deu',",
            "        'dv': 'div',",
            "        'dz': 'dzo',",
            "        'ee': 'ewe',",
            "        'el': 'ell',",
            "        'en': 'eng',",
            "        'eo': 'epo',",
            "        'es': 'spa',",
            "        'et': 'est',",
            "        'eu': 'eus',",
            "        'fa': 'fas',",
            "        'ff': 'ful',",
            "        'fi': 'fin',",
            "        'fj': 'fij',",
            "        'fo': 'fao',",
            "        'fr': 'fra',",
            "        'fy': 'fry',",
            "        'ga': 'gle',",
            "        'gd': 'gla',",
            "        'gl': 'glg',",
            "        'gn': 'grn',",
            "        'gu': 'guj',",
            "        'gv': 'glv',",
            "        'ha': 'hau',",
            "        'he': 'heb',",
            "        'iw': 'heb',  # Replaced by he in 1989 revision",
            "        'hi': 'hin',",
            "        'ho': 'hmo',",
            "        'hr': 'hrv',",
            "        'ht': 'hat',",
            "        'hu': 'hun',",
            "        'hy': 'hye',",
            "        'hz': 'her',",
            "        'ia': 'ina',",
            "        'id': 'ind',",
            "        'in': 'ind',  # Replaced by id in 1989 revision",
            "        'ie': 'ile',",
            "        'ig': 'ibo',",
            "        'ii': 'iii',",
            "        'ik': 'ipk',",
            "        'io': 'ido',",
            "        'is': 'isl',",
            "        'it': 'ita',",
            "        'iu': 'iku',",
            "        'ja': 'jpn',",
            "        'jv': 'jav',",
            "        'ka': 'kat',",
            "        'kg': 'kon',",
            "        'ki': 'kik',",
            "        'kj': 'kua',",
            "        'kk': 'kaz',",
            "        'kl': 'kal',",
            "        'km': 'khm',",
            "        'kn': 'kan',",
            "        'ko': 'kor',",
            "        'kr': 'kau',",
            "        'ks': 'kas',",
            "        'ku': 'kur',",
            "        'kv': 'kom',",
            "        'kw': 'cor',",
            "        'ky': 'kir',",
            "        'la': 'lat',",
            "        'lb': 'ltz',",
            "        'lg': 'lug',",
            "        'li': 'lim',",
            "        'ln': 'lin',",
            "        'lo': 'lao',",
            "        'lt': 'lit',",
            "        'lu': 'lub',",
            "        'lv': 'lav',",
            "        'mg': 'mlg',",
            "        'mh': 'mah',",
            "        'mi': 'mri',",
            "        'mk': 'mkd',",
            "        'ml': 'mal',",
            "        'mn': 'mon',",
            "        'mr': 'mar',",
            "        'ms': 'msa',",
            "        'mt': 'mlt',",
            "        'my': 'mya',",
            "        'na': 'nau',",
            "        'nb': 'nob',",
            "        'nd': 'nde',",
            "        'ne': 'nep',",
            "        'ng': 'ndo',",
            "        'nl': 'nld',",
            "        'nn': 'nno',",
            "        'no': 'nor',",
            "        'nr': 'nbl',",
            "        'nv': 'nav',",
            "        'ny': 'nya',",
            "        'oc': 'oci',",
            "        'oj': 'oji',",
            "        'om': 'orm',",
            "        'or': 'ori',",
            "        'os': 'oss',",
            "        'pa': 'pan',",
            "        'pe': 'per',",
            "        'pi': 'pli',",
            "        'pl': 'pol',",
            "        'ps': 'pus',",
            "        'pt': 'por',",
            "        'qu': 'que',",
            "        'rm': 'roh',",
            "        'rn': 'run',",
            "        'ro': 'ron',",
            "        'ru': 'rus',",
            "        'rw': 'kin',",
            "        'sa': 'san',",
            "        'sc': 'srd',",
            "        'sd': 'snd',",
            "        'se': 'sme',",
            "        'sg': 'sag',",
            "        'si': 'sin',",
            "        'sk': 'slk',",
            "        'sl': 'slv',",
            "        'sm': 'smo',",
            "        'sn': 'sna',",
            "        'so': 'som',",
            "        'sq': 'sqi',",
            "        'sr': 'srp',",
            "        'ss': 'ssw',",
            "        'st': 'sot',",
            "        'su': 'sun',",
            "        'sv': 'swe',",
            "        'sw': 'swa',",
            "        'ta': 'tam',",
            "        'te': 'tel',",
            "        'tg': 'tgk',",
            "        'th': 'tha',",
            "        'ti': 'tir',",
            "        'tk': 'tuk',",
            "        'tl': 'tgl',",
            "        'tn': 'tsn',",
            "        'to': 'ton',",
            "        'tr': 'tur',",
            "        'ts': 'tso',",
            "        'tt': 'tat',",
            "        'tw': 'twi',",
            "        'ty': 'tah',",
            "        'ug': 'uig',",
            "        'uk': 'ukr',",
            "        'ur': 'urd',",
            "        'uz': 'uzb',",
            "        've': 'ven',",
            "        'vi': 'vie',",
            "        'vo': 'vol',",
            "        'wa': 'wln',",
            "        'wo': 'wol',",
            "        'xh': 'xho',",
            "        'yi': 'yid',",
            "        'ji': 'yid',  # Replaced by yi in 1989 revision",
            "        'yo': 'yor',",
            "        'za': 'zha',",
            "        'zh': 'zho',",
            "        'zu': 'zul',",
            "    }",
            "",
            "    @classmethod",
            "    def short2long(cls, code):",
            "        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"",
            "        return cls._lang_map.get(code[:2])",
            "",
            "    @classmethod",
            "    def long2short(cls, code):",
            "        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"",
            "        for short_name, long_name in cls._lang_map.items():",
            "            if long_name == code:",
            "                return short_name",
            "",
            "",
            "class ISO3166Utils:",
            "    # From http://data.okfn.org/data/core/country-list",
            "    _country_map = {",
            "        'AF': 'Afghanistan',",
            "        'AX': '\u00c5land Islands',",
            "        'AL': 'Albania',",
            "        'DZ': 'Algeria',",
            "        'AS': 'American Samoa',",
            "        'AD': 'Andorra',",
            "        'AO': 'Angola',",
            "        'AI': 'Anguilla',",
            "        'AQ': 'Antarctica',",
            "        'AG': 'Antigua and Barbuda',",
            "        'AR': 'Argentina',",
            "        'AM': 'Armenia',",
            "        'AW': 'Aruba',",
            "        'AU': 'Australia',",
            "        'AT': 'Austria',",
            "        'AZ': 'Azerbaijan',",
            "        'BS': 'Bahamas',",
            "        'BH': 'Bahrain',",
            "        'BD': 'Bangladesh',",
            "        'BB': 'Barbados',",
            "        'BY': 'Belarus',",
            "        'BE': 'Belgium',",
            "        'BZ': 'Belize',",
            "        'BJ': 'Benin',",
            "        'BM': 'Bermuda',",
            "        'BT': 'Bhutan',",
            "        'BO': 'Bolivia, Plurinational State of',",
            "        'BQ': 'Bonaire, Sint Eustatius and Saba',",
            "        'BA': 'Bosnia and Herzegovina',",
            "        'BW': 'Botswana',",
            "        'BV': 'Bouvet Island',",
            "        'BR': 'Brazil',",
            "        'IO': 'British Indian Ocean Territory',",
            "        'BN': 'Brunei Darussalam',",
            "        'BG': 'Bulgaria',",
            "        'BF': 'Burkina Faso',",
            "        'BI': 'Burundi',",
            "        'KH': 'Cambodia',",
            "        'CM': 'Cameroon',",
            "        'CA': 'Canada',",
            "        'CV': 'Cape Verde',",
            "        'KY': 'Cayman Islands',",
            "        'CF': 'Central African Republic',",
            "        'TD': 'Chad',",
            "        'CL': 'Chile',",
            "        'CN': 'China',",
            "        'CX': 'Christmas Island',",
            "        'CC': 'Cocos (Keeling) Islands',",
            "        'CO': 'Colombia',",
            "        'KM': 'Comoros',",
            "        'CG': 'Congo',",
            "        'CD': 'Congo, the Democratic Republic of the',",
            "        'CK': 'Cook Islands',",
            "        'CR': 'Costa Rica',",
            "        'CI': 'C\u00f4te d\\'Ivoire',",
            "        'HR': 'Croatia',",
            "        'CU': 'Cuba',",
            "        'CW': 'Cura\u00e7ao',",
            "        'CY': 'Cyprus',",
            "        'CZ': 'Czech Republic',",
            "        'DK': 'Denmark',",
            "        'DJ': 'Djibouti',",
            "        'DM': 'Dominica',",
            "        'DO': 'Dominican Republic',",
            "        'EC': 'Ecuador',",
            "        'EG': 'Egypt',",
            "        'SV': 'El Salvador',",
            "        'GQ': 'Equatorial Guinea',",
            "        'ER': 'Eritrea',",
            "        'EE': 'Estonia',",
            "        'ET': 'Ethiopia',",
            "        'FK': 'Falkland Islands (Malvinas)',",
            "        'FO': 'Faroe Islands',",
            "        'FJ': 'Fiji',",
            "        'FI': 'Finland',",
            "        'FR': 'France',",
            "        'GF': 'French Guiana',",
            "        'PF': 'French Polynesia',",
            "        'TF': 'French Southern Territories',",
            "        'GA': 'Gabon',",
            "        'GM': 'Gambia',",
            "        'GE': 'Georgia',",
            "        'DE': 'Germany',",
            "        'GH': 'Ghana',",
            "        'GI': 'Gibraltar',",
            "        'GR': 'Greece',",
            "        'GL': 'Greenland',",
            "        'GD': 'Grenada',",
            "        'GP': 'Guadeloupe',",
            "        'GU': 'Guam',",
            "        'GT': 'Guatemala',",
            "        'GG': 'Guernsey',",
            "        'GN': 'Guinea',",
            "        'GW': 'Guinea-Bissau',",
            "        'GY': 'Guyana',",
            "        'HT': 'Haiti',",
            "        'HM': 'Heard Island and McDonald Islands',",
            "        'VA': 'Holy See (Vatican City State)',",
            "        'HN': 'Honduras',",
            "        'HK': 'Hong Kong',",
            "        'HU': 'Hungary',",
            "        'IS': 'Iceland',",
            "        'IN': 'India',",
            "        'ID': 'Indonesia',",
            "        'IR': 'Iran, Islamic Republic of',",
            "        'IQ': 'Iraq',",
            "        'IE': 'Ireland',",
            "        'IM': 'Isle of Man',",
            "        'IL': 'Israel',",
            "        'IT': 'Italy',",
            "        'JM': 'Jamaica',",
            "        'JP': 'Japan',",
            "        'JE': 'Jersey',",
            "        'JO': 'Jordan',",
            "        'KZ': 'Kazakhstan',",
            "        'KE': 'Kenya',",
            "        'KI': 'Kiribati',",
            "        'KP': 'Korea, Democratic People\\'s Republic of',",
            "        'KR': 'Korea, Republic of',",
            "        'KW': 'Kuwait',",
            "        'KG': 'Kyrgyzstan',",
            "        'LA': 'Lao People\\'s Democratic Republic',",
            "        'LV': 'Latvia',",
            "        'LB': 'Lebanon',",
            "        'LS': 'Lesotho',",
            "        'LR': 'Liberia',",
            "        'LY': 'Libya',",
            "        'LI': 'Liechtenstein',",
            "        'LT': 'Lithuania',",
            "        'LU': 'Luxembourg',",
            "        'MO': 'Macao',",
            "        'MK': 'Macedonia, the Former Yugoslav Republic of',",
            "        'MG': 'Madagascar',",
            "        'MW': 'Malawi',",
            "        'MY': 'Malaysia',",
            "        'MV': 'Maldives',",
            "        'ML': 'Mali',",
            "        'MT': 'Malta',",
            "        'MH': 'Marshall Islands',",
            "        'MQ': 'Martinique',",
            "        'MR': 'Mauritania',",
            "        'MU': 'Mauritius',",
            "        'YT': 'Mayotte',",
            "        'MX': 'Mexico',",
            "        'FM': 'Micronesia, Federated States of',",
            "        'MD': 'Moldova, Republic of',",
            "        'MC': 'Monaco',",
            "        'MN': 'Mongolia',",
            "        'ME': 'Montenegro',",
            "        'MS': 'Montserrat',",
            "        'MA': 'Morocco',",
            "        'MZ': 'Mozambique',",
            "        'MM': 'Myanmar',",
            "        'NA': 'Namibia',",
            "        'NR': 'Nauru',",
            "        'NP': 'Nepal',",
            "        'NL': 'Netherlands',",
            "        'NC': 'New Caledonia',",
            "        'NZ': 'New Zealand',",
            "        'NI': 'Nicaragua',",
            "        'NE': 'Niger',",
            "        'NG': 'Nigeria',",
            "        'NU': 'Niue',",
            "        'NF': 'Norfolk Island',",
            "        'MP': 'Northern Mariana Islands',",
            "        'NO': 'Norway',",
            "        'OM': 'Oman',",
            "        'PK': 'Pakistan',",
            "        'PW': 'Palau',",
            "        'PS': 'Palestine, State of',",
            "        'PA': 'Panama',",
            "        'PG': 'Papua New Guinea',",
            "        'PY': 'Paraguay',",
            "        'PE': 'Peru',",
            "        'PH': 'Philippines',",
            "        'PN': 'Pitcairn',",
            "        'PL': 'Poland',",
            "        'PT': 'Portugal',",
            "        'PR': 'Puerto Rico',",
            "        'QA': 'Qatar',",
            "        'RE': 'R\u00e9union',",
            "        'RO': 'Romania',",
            "        'RU': 'Russian Federation',",
            "        'RW': 'Rwanda',",
            "        'BL': 'Saint Barth\u00e9lemy',",
            "        'SH': 'Saint Helena, Ascension and Tristan da Cunha',",
            "        'KN': 'Saint Kitts and Nevis',",
            "        'LC': 'Saint Lucia',",
            "        'MF': 'Saint Martin (French part)',",
            "        'PM': 'Saint Pierre and Miquelon',",
            "        'VC': 'Saint Vincent and the Grenadines',",
            "        'WS': 'Samoa',",
            "        'SM': 'San Marino',",
            "        'ST': 'Sao Tome and Principe',",
            "        'SA': 'Saudi Arabia',",
            "        'SN': 'Senegal',",
            "        'RS': 'Serbia',",
            "        'SC': 'Seychelles',",
            "        'SL': 'Sierra Leone',",
            "        'SG': 'Singapore',",
            "        'SX': 'Sint Maarten (Dutch part)',",
            "        'SK': 'Slovakia',",
            "        'SI': 'Slovenia',",
            "        'SB': 'Solomon Islands',",
            "        'SO': 'Somalia',",
            "        'ZA': 'South Africa',",
            "        'GS': 'South Georgia and the South Sandwich Islands',",
            "        'SS': 'South Sudan',",
            "        'ES': 'Spain',",
            "        'LK': 'Sri Lanka',",
            "        'SD': 'Sudan',",
            "        'SR': 'Suriname',",
            "        'SJ': 'Svalbard and Jan Mayen',",
            "        'SZ': 'Swaziland',",
            "        'SE': 'Sweden',",
            "        'CH': 'Switzerland',",
            "        'SY': 'Syrian Arab Republic',",
            "        'TW': 'Taiwan, Province of China',",
            "        'TJ': 'Tajikistan',",
            "        'TZ': 'Tanzania, United Republic of',",
            "        'TH': 'Thailand',",
            "        'TL': 'Timor-Leste',",
            "        'TG': 'Togo',",
            "        'TK': 'Tokelau',",
            "        'TO': 'Tonga',",
            "        'TT': 'Trinidad and Tobago',",
            "        'TN': 'Tunisia',",
            "        'TR': 'Turkey',",
            "        'TM': 'Turkmenistan',",
            "        'TC': 'Turks and Caicos Islands',",
            "        'TV': 'Tuvalu',",
            "        'UG': 'Uganda',",
            "        'UA': 'Ukraine',",
            "        'AE': 'United Arab Emirates',",
            "        'GB': 'United Kingdom',",
            "        'US': 'United States',",
            "        'UM': 'United States Minor Outlying Islands',",
            "        'UY': 'Uruguay',",
            "        'UZ': 'Uzbekistan',",
            "        'VU': 'Vanuatu',",
            "        'VE': 'Venezuela, Bolivarian Republic of',",
            "        'VN': 'Viet Nam',",
            "        'VG': 'Virgin Islands, British',",
            "        'VI': 'Virgin Islands, U.S.',",
            "        'WF': 'Wallis and Futuna',",
            "        'EH': 'Western Sahara',",
            "        'YE': 'Yemen',",
            "        'ZM': 'Zambia',",
            "        'ZW': 'Zimbabwe',",
            "        # Not ISO 3166 codes, but used for IP blocks",
            "        'AP': 'Asia/Pacific Region',",
            "        'EU': 'Europe',",
            "    }",
            "",
            "    @classmethod",
            "    def short2full(cls, code):",
            "        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"",
            "        return cls._country_map.get(code.upper())",
            "",
            "",
            "class GeoUtils:",
            "    # Major IPv4 address blocks per country",
            "    _country_ip_map = {",
            "        'AD': '46.172.224.0/19',",
            "        'AE': '94.200.0.0/13',",
            "        'AF': '149.54.0.0/17',",
            "        'AG': '209.59.64.0/18',",
            "        'AI': '204.14.248.0/21',",
            "        'AL': '46.99.0.0/16',",
            "        'AM': '46.70.0.0/15',",
            "        'AO': '105.168.0.0/13',",
            "        'AP': '182.50.184.0/21',",
            "        'AQ': '23.154.160.0/24',",
            "        'AR': '181.0.0.0/12',",
            "        'AS': '202.70.112.0/20',",
            "        'AT': '77.116.0.0/14',",
            "        'AU': '1.128.0.0/11',",
            "        'AW': '181.41.0.0/18',",
            "        'AX': '185.217.4.0/22',",
            "        'AZ': '5.197.0.0/16',",
            "        'BA': '31.176.128.0/17',",
            "        'BB': '65.48.128.0/17',",
            "        'BD': '114.130.0.0/16',",
            "        'BE': '57.0.0.0/8',",
            "        'BF': '102.178.0.0/15',",
            "        'BG': '95.42.0.0/15',",
            "        'BH': '37.131.0.0/17',",
            "        'BI': '154.117.192.0/18',",
            "        'BJ': '137.255.0.0/16',",
            "        'BL': '185.212.72.0/23',",
            "        'BM': '196.12.64.0/18',",
            "        'BN': '156.31.0.0/16',",
            "        'BO': '161.56.0.0/16',",
            "        'BQ': '161.0.80.0/20',",
            "        'BR': '191.128.0.0/12',",
            "        'BS': '24.51.64.0/18',",
            "        'BT': '119.2.96.0/19',",
            "        'BW': '168.167.0.0/16',",
            "        'BY': '178.120.0.0/13',",
            "        'BZ': '179.42.192.0/18',",
            "        'CA': '99.224.0.0/11',",
            "        'CD': '41.243.0.0/16',",
            "        'CF': '197.242.176.0/21',",
            "        'CG': '160.113.0.0/16',",
            "        'CH': '85.0.0.0/13',",
            "        'CI': '102.136.0.0/14',",
            "        'CK': '202.65.32.0/19',",
            "        'CL': '152.172.0.0/14',",
            "        'CM': '102.244.0.0/14',",
            "        'CN': '36.128.0.0/10',",
            "        'CO': '181.240.0.0/12',",
            "        'CR': '201.192.0.0/12',",
            "        'CU': '152.206.0.0/15',",
            "        'CV': '165.90.96.0/19',",
            "        'CW': '190.88.128.0/17',",
            "        'CY': '31.153.0.0/16',",
            "        'CZ': '88.100.0.0/14',",
            "        'DE': '53.0.0.0/8',",
            "        'DJ': '197.241.0.0/17',",
            "        'DK': '87.48.0.0/12',",
            "        'DM': '192.243.48.0/20',",
            "        'DO': '152.166.0.0/15',",
            "        'DZ': '41.96.0.0/12',",
            "        'EC': '186.68.0.0/15',",
            "        'EE': '90.190.0.0/15',",
            "        'EG': '156.160.0.0/11',",
            "        'ER': '196.200.96.0/20',",
            "        'ES': '88.0.0.0/11',",
            "        'ET': '196.188.0.0/14',",
            "        'EU': '2.16.0.0/13',",
            "        'FI': '91.152.0.0/13',",
            "        'FJ': '144.120.0.0/16',",
            "        'FK': '80.73.208.0/21',",
            "        'FM': '119.252.112.0/20',",
            "        'FO': '88.85.32.0/19',",
            "        'FR': '90.0.0.0/9',",
            "        'GA': '41.158.0.0/15',",
            "        'GB': '25.0.0.0/8',",
            "        'GD': '74.122.88.0/21',",
            "        'GE': '31.146.0.0/16',",
            "        'GF': '161.22.64.0/18',",
            "        'GG': '62.68.160.0/19',",
            "        'GH': '154.160.0.0/12',",
            "        'GI': '95.164.0.0/16',",
            "        'GL': '88.83.0.0/19',",
            "        'GM': '160.182.0.0/15',",
            "        'GN': '197.149.192.0/18',",
            "        'GP': '104.250.0.0/19',",
            "        'GQ': '105.235.224.0/20',",
            "        'GR': '94.64.0.0/13',",
            "        'GT': '168.234.0.0/16',",
            "        'GU': '168.123.0.0/16',",
            "        'GW': '197.214.80.0/20',",
            "        'GY': '181.41.64.0/18',",
            "        'HK': '113.252.0.0/14',",
            "        'HN': '181.210.0.0/16',",
            "        'HR': '93.136.0.0/13',",
            "        'HT': '148.102.128.0/17',",
            "        'HU': '84.0.0.0/14',",
            "        'ID': '39.192.0.0/10',",
            "        'IE': '87.32.0.0/12',",
            "        'IL': '79.176.0.0/13',",
            "        'IM': '5.62.80.0/20',",
            "        'IN': '117.192.0.0/10',",
            "        'IO': '203.83.48.0/21',",
            "        'IQ': '37.236.0.0/14',",
            "        'IR': '2.176.0.0/12',",
            "        'IS': '82.221.0.0/16',",
            "        'IT': '79.0.0.0/10',",
            "        'JE': '87.244.64.0/18',",
            "        'JM': '72.27.0.0/17',",
            "        'JO': '176.29.0.0/16',",
            "        'JP': '133.0.0.0/8',",
            "        'KE': '105.48.0.0/12',",
            "        'KG': '158.181.128.0/17',",
            "        'KH': '36.37.128.0/17',",
            "        'KI': '103.25.140.0/22',",
            "        'KM': '197.255.224.0/20',",
            "        'KN': '198.167.192.0/19',",
            "        'KP': '175.45.176.0/22',",
            "        'KR': '175.192.0.0/10',",
            "        'KW': '37.36.0.0/14',",
            "        'KY': '64.96.0.0/15',",
            "        'KZ': '2.72.0.0/13',",
            "        'LA': '115.84.64.0/18',",
            "        'LB': '178.135.0.0/16',",
            "        'LC': '24.92.144.0/20',",
            "        'LI': '82.117.0.0/19',",
            "        'LK': '112.134.0.0/15',",
            "        'LR': '102.183.0.0/16',",
            "        'LS': '129.232.0.0/17',",
            "        'LT': '78.56.0.0/13',",
            "        'LU': '188.42.0.0/16',",
            "        'LV': '46.109.0.0/16',",
            "        'LY': '41.252.0.0/14',",
            "        'MA': '105.128.0.0/11',",
            "        'MC': '88.209.64.0/18',",
            "        'MD': '37.246.0.0/16',",
            "        'ME': '178.175.0.0/17',",
            "        'MF': '74.112.232.0/21',",
            "        'MG': '154.126.0.0/17',",
            "        'MH': '117.103.88.0/21',",
            "        'MK': '77.28.0.0/15',",
            "        'ML': '154.118.128.0/18',",
            "        'MM': '37.111.0.0/17',",
            "        'MN': '49.0.128.0/17',",
            "        'MO': '60.246.0.0/16',",
            "        'MP': '202.88.64.0/20',",
            "        'MQ': '109.203.224.0/19',",
            "        'MR': '41.188.64.0/18',",
            "        'MS': '208.90.112.0/22',",
            "        'MT': '46.11.0.0/16',",
            "        'MU': '105.16.0.0/12',",
            "        'MV': '27.114.128.0/18',",
            "        'MW': '102.70.0.0/15',",
            "        'MX': '187.192.0.0/11',",
            "        'MY': '175.136.0.0/13',",
            "        'MZ': '197.218.0.0/15',",
            "        'NA': '41.182.0.0/16',",
            "        'NC': '101.101.0.0/18',",
            "        'NE': '197.214.0.0/18',",
            "        'NF': '203.17.240.0/22',",
            "        'NG': '105.112.0.0/12',",
            "        'NI': '186.76.0.0/15',",
            "        'NL': '145.96.0.0/11',",
            "        'NO': '84.208.0.0/13',",
            "        'NP': '36.252.0.0/15',",
            "        'NR': '203.98.224.0/19',",
            "        'NU': '49.156.48.0/22',",
            "        'NZ': '49.224.0.0/14',",
            "        'OM': '5.36.0.0/15',",
            "        'PA': '186.72.0.0/15',",
            "        'PE': '186.160.0.0/14',",
            "        'PF': '123.50.64.0/18',",
            "        'PG': '124.240.192.0/19',",
            "        'PH': '49.144.0.0/13',",
            "        'PK': '39.32.0.0/11',",
            "        'PL': '83.0.0.0/11',",
            "        'PM': '70.36.0.0/20',",
            "        'PR': '66.50.0.0/16',",
            "        'PS': '188.161.0.0/16',",
            "        'PT': '85.240.0.0/13',",
            "        'PW': '202.124.224.0/20',",
            "        'PY': '181.120.0.0/14',",
            "        'QA': '37.210.0.0/15',",
            "        'RE': '102.35.0.0/16',",
            "        'RO': '79.112.0.0/13',",
            "        'RS': '93.86.0.0/15',",
            "        'RU': '5.136.0.0/13',",
            "        'RW': '41.186.0.0/16',",
            "        'SA': '188.48.0.0/13',",
            "        'SB': '202.1.160.0/19',",
            "        'SC': '154.192.0.0/11',",
            "        'SD': '102.120.0.0/13',",
            "        'SE': '78.64.0.0/12',",
            "        'SG': '8.128.0.0/10',",
            "        'SI': '188.196.0.0/14',",
            "        'SK': '78.98.0.0/15',",
            "        'SL': '102.143.0.0/17',",
            "        'SM': '89.186.32.0/19',",
            "        'SN': '41.82.0.0/15',",
            "        'SO': '154.115.192.0/18',",
            "        'SR': '186.179.128.0/17',",
            "        'SS': '105.235.208.0/21',",
            "        'ST': '197.159.160.0/19',",
            "        'SV': '168.243.0.0/16',",
            "        'SX': '190.102.0.0/20',",
            "        'SY': '5.0.0.0/16',",
            "        'SZ': '41.84.224.0/19',",
            "        'TC': '65.255.48.0/20',",
            "        'TD': '154.68.128.0/19',",
            "        'TG': '196.168.0.0/14',",
            "        'TH': '171.96.0.0/13',",
            "        'TJ': '85.9.128.0/18',",
            "        'TK': '27.96.24.0/21',",
            "        'TL': '180.189.160.0/20',",
            "        'TM': '95.85.96.0/19',",
            "        'TN': '197.0.0.0/11',",
            "        'TO': '175.176.144.0/21',",
            "        'TR': '78.160.0.0/11',",
            "        'TT': '186.44.0.0/15',",
            "        'TV': '202.2.96.0/19',",
            "        'TW': '120.96.0.0/11',",
            "        'TZ': '156.156.0.0/14',",
            "        'UA': '37.52.0.0/14',",
            "        'UG': '102.80.0.0/13',",
            "        'US': '6.0.0.0/8',",
            "        'UY': '167.56.0.0/13',",
            "        'UZ': '84.54.64.0/18',",
            "        'VA': '212.77.0.0/19',",
            "        'VC': '207.191.240.0/21',",
            "        'VE': '186.88.0.0/13',",
            "        'VG': '66.81.192.0/20',",
            "        'VI': '146.226.0.0/16',",
            "        'VN': '14.160.0.0/11',",
            "        'VU': '202.80.32.0/20',",
            "        'WF': '117.20.32.0/21',",
            "        'WS': '202.4.32.0/19',",
            "        'YE': '134.35.0.0/16',",
            "        'YT': '41.242.116.0/22',",
            "        'ZA': '41.0.0.0/11',",
            "        'ZM': '102.144.0.0/13',",
            "        'ZW': '102.177.192.0/18',",
            "    }",
            "",
            "    @classmethod",
            "    def random_ipv4(cls, code_or_block):",
            "        if len(code_or_block) == 2:",
            "            block = cls._country_ip_map.get(code_or_block.upper())",
            "            if not block:",
            "                return None",
            "        else:",
            "            block = code_or_block",
            "        addr, preflen = block.split('/')",
            "        addr_min = struct.unpack('!L', socket.inet_aton(addr))[0]",
            "        addr_max = addr_min | (0xffffffff >> int(preflen))",
            "        return str(socket.inet_ntoa(",
            "            struct.pack('!L', random.randint(addr_min, addr_max))))",
            "",
            "",
            "# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is",
            "# released into Public Domain",
            "# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387",
            "",
            "def long_to_bytes(n, blocksize=0):",
            "    \"\"\"long_to_bytes(n:long, blocksize:int) : string",
            "    Convert a long integer to a byte string.",
            "",
            "    If optional blocksize is given and greater than zero, pad the front of the",
            "    byte string with binary zeros so that the length is a multiple of",
            "    blocksize.",
            "    \"\"\"",
            "    # after much testing, this algorithm was deemed to be the fastest",
            "    s = b''",
            "    n = int(n)",
            "    while n > 0:",
            "        s = struct.pack('>I', n & 0xffffffff) + s",
            "        n = n >> 32",
            "    # strip off leading zeros",
            "    for i in range(len(s)):",
            "        if s[i] != b'\\000'[0]:",
            "            break",
            "    else:",
            "        # only happens when n == 0",
            "        s = b'\\000'",
            "        i = 0",
            "    s = s[i:]",
            "    # add back some pad bytes.  this could be done more efficiently w.r.t. the",
            "    # de-padding being done above, but sigh...",
            "    if blocksize > 0 and len(s) % blocksize:",
            "        s = (blocksize - len(s) % blocksize) * b'\\000' + s",
            "    return s",
            "",
            "",
            "def bytes_to_long(s):",
            "    \"\"\"bytes_to_long(string) : long",
            "    Convert a byte string to a long integer.",
            "",
            "    This is (essentially) the inverse of long_to_bytes().",
            "    \"\"\"",
            "    acc = 0",
            "    length = len(s)",
            "    if length % 4:",
            "        extra = (4 - length % 4)",
            "        s = b'\\000' * extra + s",
            "        length = length + extra",
            "    for i in range(0, length, 4):",
            "        acc = (acc << 32) + struct.unpack('>I', s[i:i + 4])[0]",
            "    return acc",
            "",
            "",
            "def ohdave_rsa_encrypt(data, exponent, modulus):",
            "    '''",
            "    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/",
            "",
            "    Input:",
            "        data: data to encrypt, bytes-like object",
            "        exponent, modulus: parameter e and N of RSA algorithm, both integer",
            "    Output: hex string of encrypted data",
            "",
            "    Limitation: supports one block encryption only",
            "    '''",
            "",
            "    payload = int(binascii.hexlify(data[::-1]), 16)",
            "    encrypted = pow(payload, exponent, modulus)",
            "    return '%x' % encrypted",
            "",
            "",
            "def pkcs1pad(data, length):",
            "    \"\"\"",
            "    Padding input data with PKCS#1 scheme",
            "",
            "    @param {int[]} data        input data",
            "    @param {int}   length      target length",
            "    @returns {int[]}           padded data",
            "    \"\"\"",
            "    if len(data) > length - 11:",
            "        raise ValueError('Input data too long for PKCS#1 padding')",
            "",
            "    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]",
            "    return [0, 2] + pseudo_random + [0] + data",
            "",
            "",
            "def _base_n_table(n, table):",
            "    if not table and not n:",
            "        raise ValueError('Either table or n must be specified')",
            "    table = (table or '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')[:n]",
            "",
            "    if n and n != len(table):",
            "        raise ValueError(f'base {n} exceeds table length {len(table)}')",
            "    return table",
            "",
            "",
            "def encode_base_n(num, n=None, table=None):",
            "    \"\"\"Convert given int to a base-n string\"\"\"",
            "    table = _base_n_table(n, table)",
            "    if not num:",
            "        return table[0]",
            "",
            "    result, base = '', len(table)",
            "    while num:",
            "        result = table[num % base] + result",
            "        num = num // base",
            "    return result",
            "",
            "",
            "def decode_base_n(string, n=None, table=None):",
            "    \"\"\"Convert given base-n string to int\"\"\"",
            "    table = {char: index for index, char in enumerate(_base_n_table(n, table))}",
            "    result, base = 0, len(table)",
            "    for char in string:",
            "        result = result * base + table[char]",
            "    return result",
            "",
            "",
            "def decode_packed_codes(code):",
            "    mobj = re.search(PACKED_CODES_RE, code)",
            "    obfuscated_code, base, count, symbols = mobj.groups()",
            "    base = int(base)",
            "    count = int(count)",
            "    symbols = symbols.split('|')",
            "    symbol_table = {}",
            "",
            "    while count:",
            "        count -= 1",
            "        base_n_count = encode_base_n(count, base)",
            "        symbol_table[base_n_count] = symbols[count] or base_n_count",
            "",
            "    return re.sub(",
            "        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],",
            "        obfuscated_code)",
            "",
            "",
            "def caesar(s, alphabet, shift):",
            "    if shift == 0:",
            "        return s",
            "    l = len(alphabet)",
            "    return ''.join(",
            "        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c",
            "        for c in s)",
            "",
            "",
            "def rot47(s):",
            "    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)",
            "",
            "",
            "def parse_m3u8_attributes(attrib):",
            "    info = {}",
            "    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):",
            "        if val.startswith('\"'):",
            "            val = val[1:-1]",
            "        info[key] = val",
            "    return info",
            "",
            "",
            "def urshift(val, n):",
            "    return val >> n if val >= 0 else (val + 0x100000000) >> n",
            "",
            "",
            "def write_xattr(path, key, value):",
            "    # Windows: Write xattrs to NTFS Alternate Data Streams:",
            "    # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29",
            "    if compat_os_name == 'nt':",
            "        assert ':' not in key",
            "        assert os.path.exists(path)",
            "",
            "        try:",
            "            with open(f'{path}:{key}', 'wb') as f:",
            "                f.write(value)",
            "        except OSError as e:",
            "            raise XAttrMetadataError(e.errno, e.strerror)",
            "        return",
            "",
            "    # UNIX Method 1. Use os.setxattr/xattrs/pyxattrs modules",
            "",
            "    setxattr = None",
            "    if callable(getattr(os, 'setxattr', None)):",
            "        setxattr = os.setxattr",
            "    elif getattr(xattr, '_yt_dlp__identifier', None) == 'pyxattr':",
            "        # Unicode arguments are not supported in pyxattr until version 0.5.0",
            "        # See https://github.com/ytdl-org/youtube-dl/issues/5498",
            "        if version_tuple(xattr.__version__) >= (0, 5, 0):",
            "            setxattr = xattr.set",
            "    elif xattr:",
            "        setxattr = xattr.setxattr",
            "",
            "    if setxattr:",
            "        try:",
            "            setxattr(path, key, value)",
            "        except OSError as e:",
            "            raise XAttrMetadataError(e.errno, e.strerror)",
            "        return",
            "",
            "    # UNIX Method 2. Use setfattr/xattr executables",
            "    exe = ('setfattr' if check_executable('setfattr', ['--version'])",
            "           else 'xattr' if check_executable('xattr', ['-h']) else None)",
            "    if not exe:",
            "        raise XAttrUnavailableError(",
            "            'Couldn\\'t find a tool to set the xattrs. Install either the \"xattr\" or \"pyxattr\" Python modules or the '",
            "            + ('\"xattr\" binary' if sys.platform != 'linux' else 'GNU \"attr\" package (which contains the \"setfattr\" tool)'))",
            "",
            "    value = value.decode()",
            "    try:",
            "        _, stderr, returncode = Popen.run(",
            "            [exe, '-w', key, value, path] if exe == 'xattr' else [exe, '-n', key, '-v', value, path],",
            "            text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)",
            "    except OSError as e:",
            "        raise XAttrMetadataError(e.errno, e.strerror)",
            "    if returncode:",
            "        raise XAttrMetadataError(returncode, stderr)",
            "",
            "",
            "def random_birthday(year_field, month_field, day_field):",
            "    start_date = dt.date(1950, 1, 1)",
            "    end_date = dt.date(1995, 12, 31)",
            "    offset = random.randint(0, (end_date - start_date).days)",
            "    random_date = start_date + dt.timedelta(offset)",
            "    return {",
            "        year_field: str(random_date.year),",
            "        month_field: str(random_date.month),",
            "        day_field: str(random_date.day),",
            "    }",
            "",
            "",
            "def find_available_port(interface=''):",
            "    try:",
            "        with socket.socket() as sock:",
            "            sock.bind((interface, 0))",
            "            return sock.getsockname()[1]",
            "    except OSError:",
            "        return None",
            "",
            "",
            "# Templates for internet shortcut files, which are plain text files.",
            "DOT_URL_LINK_TEMPLATE = '''\\",
            "[InternetShortcut]",
            "URL=%(url)s",
            "'''",
            "",
            "DOT_WEBLOC_LINK_TEMPLATE = '''\\",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
            "<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">",
            "<plist version=\"1.0\">",
            "<dict>",
            "\\t<key>URL</key>",
            "\\t<string>%(url)s</string>",
            "</dict>",
            "</plist>",
            "'''",
            "",
            "DOT_DESKTOP_LINK_TEMPLATE = '''\\",
            "[Desktop Entry]",
            "Encoding=UTF-8",
            "Name=%(filename)s",
            "Type=Link",
            "URL=%(url)s",
            "Icon=text-html",
            "'''",
            "",
            "LINK_TEMPLATES = {",
            "    'url': DOT_URL_LINK_TEMPLATE,",
            "    'desktop': DOT_DESKTOP_LINK_TEMPLATE,",
            "    'webloc': DOT_WEBLOC_LINK_TEMPLATE,",
            "}",
            "",
            "",
            "def iri_to_uri(iri):",
            "    \"\"\"",
            "    Converts an IRI (Internationalized Resource Identifier, allowing Unicode characters) to a URI (Uniform Resource Identifier, ASCII-only).",
            "",
            "    The function doesn't add an additional layer of escaping; e.g., it doesn't escape `%3C` as `%253C`. Instead, it percent-escapes characters with an underlying UTF-8 encoding *besides* those already escaped, leaving the URI intact.",
            "    \"\"\"",
            "",
            "    iri_parts = urllib.parse.urlparse(iri)",
            "",
            "    if '[' in iri_parts.netloc:",
            "        raise ValueError('IPv6 URIs are not, yet, supported.')",
            "        # Querying `.netloc`, when there's only one bracket, also raises a ValueError.",
            "",
            "    # The `safe` argument values, that the following code uses, contain the characters that should not be percent-encoded. Everything else but letters, digits and '_.-' will be percent-encoded with an underlying UTF-8 encoding. Everything already percent-encoded will be left as is.",
            "",
            "    net_location = ''",
            "    if iri_parts.username:",
            "        net_location += urllib.parse.quote(iri_parts.username, safe=r\"!$%&'()*+,~\")",
            "        if iri_parts.password is not None:",
            "            net_location += ':' + urllib.parse.quote(iri_parts.password, safe=r\"!$%&'()*+,~\")",
            "        net_location += '@'",
            "",
            "    net_location += iri_parts.hostname.encode('idna').decode()  # Punycode for Unicode hostnames.",
            "    # The 'idna' encoding produces ASCII text.",
            "    if iri_parts.port is not None and iri_parts.port != 80:",
            "        net_location += ':' + str(iri_parts.port)",
            "",
            "    return urllib.parse.urlunparse(",
            "        (iri_parts.scheme,",
            "            net_location,",
            "",
            "            urllib.parse.quote_plus(iri_parts.path, safe=r\"!$%&'()*+,/:;=@|~\"),",
            "",
            "            # Unsure about the `safe` argument, since this is a legacy way of handling parameters.",
            "            urllib.parse.quote_plus(iri_parts.params, safe=r\"!$%&'()*+,/:;=@|~\"),",
            "",
            "            # Not totally sure about the `safe` argument, since the source does not explicitly mention the query URI component.",
            "            urllib.parse.quote_plus(iri_parts.query, safe=r\"!$%&'()*+,/:;=?@{|}~\"),",
            "",
            "            urllib.parse.quote_plus(iri_parts.fragment, safe=r\"!#$%&'()*+,/:;=?@{|}~\")))",
            "",
            "    # Source for `safe` arguments: https://url.spec.whatwg.org/#percent-encoded-bytes.",
            "",
            "",
            "def to_high_limit_path(path):",
            "    if sys.platform in ['win32', 'cygwin']:",
            "        # Work around MAX_PATH limitation on Windows. The maximum allowed length for the individual path segments may still be quite limited.",
            "        return '\\\\\\\\?\\\\' + os.path.abspath(path)",
            "",
            "    return path",
            "",
            "",
            "def format_field(obj, field=None, template='%s', ignore=NO_DEFAULT, default='', func=IDENTITY):",
            "    val = traversal.traverse_obj(obj, *variadic(field))",
            "    if not val if ignore is NO_DEFAULT else val in variadic(ignore):",
            "        return default",
            "    return template % func(val)",
            "",
            "",
            "def clean_podcast_url(url):",
            "    url = re.sub(r'''(?x)",
            "        (?:",
            "            (?:",
            "                chtbl\\.com/track|",
            "                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/",
            "                play\\.podtrac\\.com|",
            "                chrt\\.fm/track|",
            "                mgln\\.ai/e",
            "            )(?:/[^/.]+)?|",
            "            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure",
            "            flex\\.acast\\.com|",
            "            pd(?:",
            "                cn\\.co| # https://podcorn.com/analytics-prefix/",
            "                st\\.fm # https://podsights.com/docs/",
            "            )/e|",
            "            [0-9]\\.gum\\.fm|",
            "            pscrb\\.fm/rss/p",
            "        )/''', '', url)",
            "    return re.sub(r'^\\w+://(\\w+://)', r'\\1', url)",
            "",
            "",
            "_HEX_TABLE = '0123456789abcdef'",
            "",
            "",
            "def random_uuidv4():",
            "    return re.sub(r'[xy]', lambda x: _HEX_TABLE[random.randint(0, 15)], 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx')",
            "",
            "",
            "def make_dir(path, to_screen=None):",
            "    try:",
            "        dn = os.path.dirname(path)",
            "        if dn:",
            "            os.makedirs(dn, exist_ok=True)",
            "        return True",
            "    except OSError as err:",
            "        if callable(to_screen) is not None:",
            "            to_screen(f'unable to create directory {err}')",
            "        return False",
            "",
            "",
            "def get_executable_path():",
            "    from ..update import _get_variant_and_executable_path",
            "",
            "    return os.path.dirname(os.path.abspath(_get_variant_and_executable_path()[1]))",
            "",
            "",
            "def get_user_config_dirs(package_name):",
            "    # .config (e.g. ~/.config/package_name)",
            "    xdg_config_home = os.getenv('XDG_CONFIG_HOME') or compat_expanduser('~/.config')",
            "    yield os.path.join(xdg_config_home, package_name)",
            "",
            "    # appdata (%APPDATA%/package_name)",
            "    appdata_dir = os.getenv('appdata')",
            "    if appdata_dir:",
            "        yield os.path.join(appdata_dir, package_name)",
            "",
            "    # home (~/.package_name)",
            "    yield os.path.join(compat_expanduser('~'), f'.{package_name}')",
            "",
            "",
            "def get_system_config_dirs(package_name):",
            "    # /etc/package_name",
            "    yield os.path.join('/etc', package_name)",
            "",
            "",
            "def time_seconds(**kwargs):",
            "    \"\"\"",
            "    Returns TZ-aware time in seconds since the epoch (1970-01-01T00:00:00Z)",
            "    \"\"\"",
            "    return time.time() + dt.timedelta(**kwargs).total_seconds()",
            "",
            "",
            "# create a JSON Web Signature (jws) with HS256 algorithm",
            "# the resulting format is in JWS Compact Serialization",
            "# implemented following JWT https://www.rfc-editor.org/rfc/rfc7519.html",
            "# implemented following JWS https://www.rfc-editor.org/rfc/rfc7515.html",
            "def jwt_encode_hs256(payload_data, key, headers={}):",
            "    header_data = {",
            "        'alg': 'HS256',",
            "        'typ': 'JWT',",
            "    }",
            "    if headers:",
            "        header_data.update(headers)",
            "    header_b64 = base64.b64encode(json.dumps(header_data).encode())",
            "    payload_b64 = base64.b64encode(json.dumps(payload_data).encode())",
            "    h = hmac.new(key.encode(), header_b64 + b'.' + payload_b64, hashlib.sha256)",
            "    signature_b64 = base64.b64encode(h.digest())",
            "    token = header_b64 + b'.' + payload_b64 + b'.' + signature_b64",
            "    return token",
            "",
            "",
            "# can be extended in future to verify the signature and parse header and return the algorithm used if it's not HS256",
            "def jwt_decode_hs256(jwt):",
            "    header_b64, payload_b64, signature_b64 = jwt.split('.')",
            "    # add trailing ='s that may have been stripped, superfluous ='s are ignored",
            "    payload_data = json.loads(base64.urlsafe_b64decode(f'{payload_b64}==='))",
            "    return payload_data",
            "",
            "",
            "WINDOWS_VT_MODE = False if compat_os_name == 'nt' else None",
            "",
            "",
            "@functools.cache",
            "def supports_terminal_sequences(stream):",
            "    if compat_os_name == 'nt':",
            "        if not WINDOWS_VT_MODE:",
            "            return False",
            "    elif not os.getenv('TERM'):",
            "        return False",
            "    try:",
            "        return stream.isatty()",
            "    except BaseException:",
            "        return False",
            "",
            "",
            "def windows_enable_vt_mode():",
            "    \"\"\"Ref: https://bugs.python.org/issue30075 \"\"\"",
            "    if get_windows_version() < (10, 0, 10586):",
            "        return",
            "",
            "    import ctypes",
            "    import ctypes.wintypes",
            "    import msvcrt",
            "",
            "    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004",
            "",
            "    dll = ctypes.WinDLL('kernel32', use_last_error=False)",
            "    handle = os.open('CONOUT$', os.O_RDWR)",
            "    try:",
            "        h_out = ctypes.wintypes.HANDLE(msvcrt.get_osfhandle(handle))",
            "        dw_original_mode = ctypes.wintypes.DWORD()",
            "        success = dll.GetConsoleMode(h_out, ctypes.byref(dw_original_mode))",
            "        if not success:",
            "            raise Exception('GetConsoleMode failed')",
            "",
            "        success = dll.SetConsoleMode(h_out, ctypes.wintypes.DWORD(",
            "            dw_original_mode.value | ENABLE_VIRTUAL_TERMINAL_PROCESSING))",
            "        if not success:",
            "            raise Exception('SetConsoleMode failed')",
            "    finally:",
            "        os.close(handle)",
            "",
            "    global WINDOWS_VT_MODE",
            "    WINDOWS_VT_MODE = True",
            "    supports_terminal_sequences.cache_clear()",
            "",
            "",
            "_terminal_sequences_re = re.compile('\\033\\\\[[^m]+m')",
            "",
            "",
            "def remove_terminal_sequences(string):",
            "    return _terminal_sequences_re.sub('', string)",
            "",
            "",
            "def number_of_digits(number):",
            "    return len('%d' % number)",
            "",
            "",
            "def join_nonempty(*values, delim='-', from_dict=None):",
            "    if from_dict is not None:",
            "        values = (traversal.traverse_obj(from_dict, variadic(v)) for v in values)",
            "    return delim.join(map(str, filter(None, values)))",
            "",
            "",
            "def scale_thumbnails_to_max_format_width(formats, thumbnails, url_width_re):",
            "    \"\"\"",
            "    Find the largest format dimensions in terms of video width and, for each thumbnail:",
            "    * Modify the URL: Match the width with the provided regex and replace with the former width",
            "    * Update dimensions",
            "",
            "    This function is useful with video services that scale the provided thumbnails on demand",
            "    \"\"\"",
            "    _keys = ('width', 'height')",
            "    max_dimensions = max(",
            "        (tuple(format.get(k) or 0 for k in _keys) for format in formats),",
            "        default=(0, 0))",
            "    if not max_dimensions[0]:",
            "        return thumbnails",
            "    return [",
            "        merge_dicts(",
            "            {'url': re.sub(url_width_re, str(max_dimensions[0]), thumbnail['url'])},",
            "            dict(zip(_keys, max_dimensions)), thumbnail)",
            "        for thumbnail in thumbnails",
            "    ]",
            "",
            "",
            "def parse_http_range(range):",
            "    \"\"\" Parse value of \"Range\" or \"Content-Range\" HTTP header into tuple. \"\"\"",
            "    if not range:",
            "        return None, None, None",
            "    crg = re.search(r'bytes[ =](\\d+)-(\\d+)?(?:/(\\d+))?', range)",
            "    if not crg:",
            "        return None, None, None",
            "    return int(crg.group(1)), int_or_none(crg.group(2)), int_or_none(crg.group(3))",
            "",
            "",
            "def read_stdin(what):",
            "    if what:",
            "        eof = 'Ctrl+Z' if compat_os_name == 'nt' else 'Ctrl+D'",
            "        write_string(f'Reading {what} from STDIN - EOF ({eof}) to end:\\n')",
            "    return sys.stdin",
            "",
            "",
            "def determine_file_encoding(data):",
            "    \"\"\"",
            "    Detect the text encoding used",
            "    @returns (encoding, bytes to skip)",
            "    \"\"\"",
            "",
            "    # BOM marks are given priority over declarations",
            "    for bom, enc in BOMS:",
            "        if data.startswith(bom):",
            "            return enc, len(bom)",
            "",
            "    # Strip off all null bytes to match even when UTF-16 or UTF-32 is used.",
            "    # We ignore the endianness to get a good enough match",
            "    data = data.replace(b'\\0', b'')",
            "    mobj = re.match(rb'(?m)^#\\s*coding\\s*:\\s*(\\S+)\\s*$', data)",
            "    return mobj.group(1).decode() if mobj else None, 0",
            "",
            "",
            "class Config:",
            "    own_args = None",
            "    parsed_args = None",
            "    filename = None",
            "    __initialized = False",
            "",
            "    def __init__(self, parser, label=None):",
            "        self.parser, self.label = parser, label",
            "        self._loaded_paths, self.configs = set(), []",
            "",
            "    def init(self, args=None, filename=None):",
            "        assert not self.__initialized",
            "        self.own_args, self.filename = args, filename",
            "        return self.load_configs()",
            "",
            "    def load_configs(self):",
            "        directory = ''",
            "        if self.filename:",
            "            location = os.path.realpath(self.filename)",
            "            directory = os.path.dirname(location)",
            "            if location in self._loaded_paths:",
            "                return False",
            "            self._loaded_paths.add(location)",
            "",
            "        self.__initialized = True",
            "        opts, _ = self.parser.parse_known_args(self.own_args)",
            "        self.parsed_args = self.own_args",
            "        for location in opts.config_locations or []:",
            "            if location == '-':",
            "                if location in self._loaded_paths:",
            "                    continue",
            "                self._loaded_paths.add(location)",
            "                self.append_config(shlex.split(read_stdin('options'), comments=True), label='stdin')",
            "                continue",
            "            location = os.path.join(directory, expand_path(location))",
            "            if os.path.isdir(location):",
            "                location = os.path.join(location, 'yt-dlp.conf')",
            "            if not os.path.exists(location):",
            "                self.parser.error(f'config location {location} does not exist')",
            "            self.append_config(self.read_file(location), location)",
            "        return True",
            "",
            "    def __str__(self):",
            "        label = join_nonempty(",
            "            self.label, 'config', f'\"{self.filename}\"' if self.filename else '',",
            "            delim=' ')",
            "        return join_nonempty(",
            "            self.own_args is not None and f'{label[0].upper()}{label[1:]}: {self.hide_login_info(self.own_args)}',",
            "            *(f'\\n{c}'.replace('\\n', '\\n| ')[1:] for c in self.configs),",
            "            delim='\\n')",
            "",
            "    @staticmethod",
            "    def read_file(filename, default=[]):",
            "        try:",
            "            optionf = open(filename, 'rb')",
            "        except OSError:",
            "            return default  # silently skip if file is not present",
            "        try:",
            "            enc, skip = determine_file_encoding(optionf.read(512))",
            "            optionf.seek(skip, io.SEEK_SET)",
            "        except OSError:",
            "            enc = None  # silently skip read errors",
            "        try:",
            "            # FIXME: https://github.com/ytdl-org/youtube-dl/commit/dfe5fa49aed02cf36ba9f743b11b0903554b5e56",
            "            contents = optionf.read().decode(enc or preferredencoding())",
            "            res = shlex.split(contents, comments=True)",
            "        except Exception as err:",
            "            raise ValueError(f'Unable to parse \"{filename}\": {err}')",
            "        finally:",
            "            optionf.close()",
            "        return res",
            "",
            "    @staticmethod",
            "    def hide_login_info(opts):",
            "        PRIVATE_OPTS = {'-p', '--password', '-u', '--username', '--video-password', '--ap-password', '--ap-username'}",
            "        eqre = re.compile('^(?P<key>' + ('|'.join(re.escape(po) for po in PRIVATE_OPTS)) + ')=.+$')",
            "",
            "        def _scrub_eq(o):",
            "            m = eqre.match(o)",
            "            if m:",
            "                return m.group('key') + '=PRIVATE'",
            "            else:",
            "                return o",
            "",
            "        opts = list(map(_scrub_eq, opts))",
            "        for idx, opt in enumerate(opts):",
            "            if opt in PRIVATE_OPTS and idx + 1 < len(opts):",
            "                opts[idx + 1] = 'PRIVATE'",
            "        return opts",
            "",
            "    def append_config(self, *args, label=None):",
            "        config = type(self)(self.parser, label)",
            "        config._loaded_paths = self._loaded_paths",
            "        if config.init(*args):",
            "            self.configs.append(config)",
            "",
            "    @property",
            "    def all_args(self):",
            "        for config in reversed(self.configs):",
            "            yield from config.all_args",
            "        yield from self.parsed_args or []",
            "",
            "    def parse_known_args(self, **kwargs):",
            "        return self.parser.parse_known_args(self.all_args, **kwargs)",
            "",
            "    def parse_args(self):",
            "        return self.parser.parse_args(self.all_args)",
            "",
            "",
            "def merge_headers(*dicts):",
            "    \"\"\"Merge dicts of http headers case insensitively, prioritizing the latter ones\"\"\"",
            "    return {k.title(): v for k, v in itertools.chain.from_iterable(map(dict.items, dicts))}",
            "",
            "",
            "def cached_method(f):",
            "    \"\"\"Cache a method\"\"\"",
            "    signature = inspect.signature(f)",
            "",
            "    @functools.wraps(f)",
            "    def wrapper(self, *args, **kwargs):",
            "        bound_args = signature.bind(self, *args, **kwargs)",
            "        bound_args.apply_defaults()",
            "        key = tuple(bound_args.arguments.values())[1:]",
            "",
            "        cache = vars(self).setdefault('_cached_method__cache', {}).setdefault(f.__name__, {})",
            "        if key not in cache:",
            "            cache[key] = f(self, *args, **kwargs)",
            "        return cache[key]",
            "    return wrapper",
            "",
            "",
            "class classproperty:",
            "    \"\"\"property access for class methods with optional caching\"\"\"",
            "    def __new__(cls, func=None, *args, **kwargs):",
            "        if not func:",
            "            return functools.partial(cls, *args, **kwargs)",
            "        return super().__new__(cls)",
            "",
            "    def __init__(self, func, *, cache=False):",
            "        functools.update_wrapper(self, func)",
            "        self.func = func",
            "        self._cache = {} if cache else None",
            "",
            "    def __get__(self, _, cls):",
            "        if self._cache is None:",
            "            return self.func(cls)",
            "        elif cls not in self._cache:",
            "            self._cache[cls] = self.func(cls)",
            "        return self._cache[cls]",
            "",
            "",
            "class function_with_repr:",
            "    def __init__(self, func, repr_=None):",
            "        functools.update_wrapper(self, func)",
            "        self.func, self.__repr = func, repr_",
            "",
            "    def __call__(self, *args, **kwargs):",
            "        return self.func(*args, **kwargs)",
            "",
            "    @classmethod",
            "    def set_repr(cls, repr_):",
            "        return functools.partial(cls, repr_=repr_)",
            "",
            "    def __repr__(self):",
            "        if self.__repr:",
            "            return self.__repr",
            "        return f'{self.func.__module__}.{self.func.__qualname__}'",
            "",
            "",
            "class Namespace(types.SimpleNamespace):",
            "    \"\"\"Immutable namespace\"\"\"",
            "",
            "    def __iter__(self):",
            "        return iter(self.__dict__.values())",
            "",
            "    @property",
            "    def items_(self):",
            "        return self.__dict__.items()",
            "",
            "",
            "MEDIA_EXTENSIONS = Namespace(",
            "    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),",
            "    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),",
            "    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),",
            "    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),",
            "    thumbnails=('jpg', 'png', 'webp'),",
            "    storyboards=('mhtml', ),",
            "    subtitles=('srt', 'vtt', 'ass', 'lrc'),",
            "    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),",
            ")",
            "MEDIA_EXTENSIONS.video += MEDIA_EXTENSIONS.common_video",
            "MEDIA_EXTENSIONS.audio += MEDIA_EXTENSIONS.common_audio",
            "",
            "KNOWN_EXTENSIONS = (*MEDIA_EXTENSIONS.video, *MEDIA_EXTENSIONS.audio, *MEDIA_EXTENSIONS.manifests)",
            "",
            "",
            "class RetryManager:",
            "    \"\"\"Usage:",
            "        for retry in RetryManager(...):",
            "            try:",
            "                ...",
            "            except SomeException as err:",
            "                retry.error = err",
            "                continue",
            "    \"\"\"",
            "    attempt, _error = 0, None",
            "",
            "    def __init__(self, _retries, _error_callback, **kwargs):",
            "        self.retries = _retries or 0",
            "        self.error_callback = functools.partial(_error_callback, **kwargs)",
            "",
            "    def _should_retry(self):",
            "        return self._error is not NO_DEFAULT and self.attempt <= self.retries",
            "",
            "    @property",
            "    def error(self):",
            "        if self._error is NO_DEFAULT:",
            "            return None",
            "        return self._error",
            "",
            "    @error.setter",
            "    def error(self, value):",
            "        self._error = value",
            "",
            "    def __iter__(self):",
            "        while self._should_retry():",
            "            self.error = NO_DEFAULT",
            "            self.attempt += 1",
            "            yield self",
            "            if self.error:",
            "                self.error_callback(self.error, self.attempt, self.retries)",
            "",
            "    @staticmethod",
            "    def report_retry(e, count, retries, *, sleep_func, info, warn, error=None, suffix=None):",
            "        \"\"\"Utility function for reporting retries\"\"\"",
            "        if count > retries:",
            "            if error:",
            "                return error(f'{e}. Giving up after {count - 1} retries') if count > 1 else error(str(e))",
            "            raise e",
            "",
            "        if not count:",
            "            return warn(e)",
            "        elif isinstance(e, ExtractorError):",
            "            e = remove_end(str_or_none(e.cause) or e.orig_msg, '.')",
            "        warn(f'{e}. Retrying{format_field(suffix, None, \" %s\")} ({count}/{retries})...')",
            "",
            "        delay = float_or_none(sleep_func(n=count - 1)) if callable(sleep_func) else sleep_func",
            "        if delay:",
            "            info(f'Sleeping {delay:.2f} seconds ...')",
            "            time.sleep(delay)",
            "",
            "",
            "def make_archive_id(ie, video_id):",
            "    ie_key = ie if isinstance(ie, str) else ie.ie_key()",
            "    return f'{ie_key.lower()} {video_id}'",
            "",
            "",
            "def truncate_string(s, left, right=0):",
            "    assert left > 3 and right >= 0",
            "    if s is None or len(s) <= left + right:",
            "        return s",
            "    return f'{s[:left - 3]}...{s[-right:] if right else \"\"}'",
            "",
            "",
            "def orderedSet_from_options(options, alias_dict, *, use_regex=False, start=None):",
            "    assert 'all' in alias_dict, '\"all\" alias is required'",
            "    requested = list(start or [])",
            "    for val in options:",
            "        discard = val.startswith('-')",
            "        if discard:",
            "            val = val[1:]",
            "",
            "        if val in alias_dict:",
            "            val = alias_dict[val] if not discard else [",
            "                i[1:] if i.startswith('-') else f'-{i}' for i in alias_dict[val]]",
            "            # NB: Do not allow regex in aliases for performance",
            "            requested = orderedSet_from_options(val, alias_dict, start=requested)",
            "            continue",
            "",
            "        current = (filter(re.compile(val, re.I).fullmatch, alias_dict['all']) if use_regex",
            "                   else [val] if val in alias_dict['all'] else None)",
            "        if current is None:",
            "            raise ValueError(val)",
            "",
            "        if discard:",
            "            for item in current:",
            "                while item in requested:",
            "                    requested.remove(item)",
            "        else:",
            "            requested.extend(current)",
            "",
            "    return orderedSet(requested)",
            "",
            "",
            "# TODO: Rewrite",
            "class FormatSorter:",
            "    regex = r' *((?P<reverse>\\+)?(?P<field>[a-zA-Z0-9_]+)((?P<separator>[~:])(?P<limit>.*?))?)? *$'",
            "",
            "    default = ('hidden', 'aud_or_vid', 'hasvid', 'ie_pref', 'lang', 'quality',",
            "               'res', 'fps', 'hdr:12', 'vcodec:vp9.2', 'channels', 'acodec',",
            "               'size', 'br', 'asr', 'proto', 'ext', 'hasaud', 'source', 'id')  # These must not be aliases",
            "    ytdl_default = ('hasaud', 'lang', 'quality', 'tbr', 'filesize', 'vbr',",
            "                    'height', 'width', 'proto', 'vext', 'abr', 'aext',",
            "                    'fps', 'fs_approx', 'source', 'id')",
            "",
            "    settings = {",
            "        'vcodec': {'type': 'ordered', 'regex': True,",
            "                   'order': ['av0?1', 'vp0?9.2', 'vp0?9', '[hx]265|he?vc?', '[hx]264|avc', 'vp0?8', 'mp4v|h263', 'theora', '', None, 'none']},",
            "        'acodec': {'type': 'ordered', 'regex': True,",
            "                   'order': ['[af]lac', 'wav|aiff', 'opus', 'vorbis|ogg', 'aac', 'mp?4a?', 'mp3', 'ac-?4', 'e-?a?c-?3', 'ac-?3', 'dts', '', None, 'none']},",
            "        'hdr': {'type': 'ordered', 'regex': True, 'field': 'dynamic_range',",
            "                'order': ['dv', '(hdr)?12', r'(hdr)?10\\+', '(hdr)?10', 'hlg', '', 'sdr', None]},",
            "        'proto': {'type': 'ordered', 'regex': True, 'field': 'protocol',",
            "                  'order': ['(ht|f)tps', '(ht|f)tp$', 'm3u8.*', '.*dash', 'websocket_frag', 'rtmpe?', '', 'mms|rtsp', 'ws|websocket', 'f4']},",
            "        'vext': {'type': 'ordered', 'field': 'video_ext',",
            "                 'order': ('mp4', 'mov', 'webm', 'flv', '', 'none'),",
            "                 'order_free': ('webm', 'mp4', 'mov', 'flv', '', 'none')},",
            "        'aext': {'type': 'ordered', 'regex': True, 'field': 'audio_ext',",
            "                 'order': ('m4a', 'aac', 'mp3', 'ogg', 'opus', 'web[am]', '', 'none'),",
            "                 'order_free': ('ogg', 'opus', 'web[am]', 'mp3', 'm4a', 'aac', '', 'none')},",
            "        'hidden': {'visible': False, 'forced': True, 'type': 'extractor', 'max': -1000},",
            "        'aud_or_vid': {'visible': False, 'forced': True, 'type': 'multiple',",
            "                       'field': ('vcodec', 'acodec'),",
            "                       'function': lambda it: int(any(v != 'none' for v in it))},",
            "        'ie_pref': {'priority': True, 'type': 'extractor'},",
            "        'hasvid': {'priority': True, 'field': 'vcodec', 'type': 'boolean', 'not_in_list': ('none',)},",
            "        'hasaud': {'field': 'acodec', 'type': 'boolean', 'not_in_list': ('none',)},",
            "        'lang': {'convert': 'float', 'field': 'language_preference', 'default': -1},",
            "        'quality': {'convert': 'float', 'default': -1},",
            "        'filesize': {'convert': 'bytes'},",
            "        'fs_approx': {'convert': 'bytes', 'field': 'filesize_approx'},",
            "        'id': {'convert': 'string', 'field': 'format_id'},",
            "        'height': {'convert': 'float_none'},",
            "        'width': {'convert': 'float_none'},",
            "        'fps': {'convert': 'float_none'},",
            "        'channels': {'convert': 'float_none', 'field': 'audio_channels'},",
            "        'tbr': {'convert': 'float_none'},",
            "        'vbr': {'convert': 'float_none'},",
            "        'abr': {'convert': 'float_none'},",
            "        'asr': {'convert': 'float_none'},",
            "        'source': {'convert': 'float', 'field': 'source_preference', 'default': -1},",
            "",
            "        'codec': {'type': 'combined', 'field': ('vcodec', 'acodec')},",
            "        'br': {'type': 'multiple', 'field': ('tbr', 'vbr', 'abr'), 'convert': 'float_none',",
            "               'function': lambda it: next(filter(None, it), None)},",
            "        'size': {'type': 'multiple', 'field': ('filesize', 'fs_approx'), 'convert': 'bytes',",
            "                 'function': lambda it: next(filter(None, it), None)},",
            "        'ext': {'type': 'combined', 'field': ('vext', 'aext')},",
            "        'res': {'type': 'multiple', 'field': ('height', 'width'),",
            "                'function': lambda it: (lambda l: min(l) if l else 0)(tuple(filter(None, it)))},",
            "",
            "        # Actual field names",
            "        'format_id': {'type': 'alias', 'field': 'id'},",
            "        'preference': {'type': 'alias', 'field': 'ie_pref'},",
            "        'language_preference': {'type': 'alias', 'field': 'lang'},",
            "        'source_preference': {'type': 'alias', 'field': 'source'},",
            "        'protocol': {'type': 'alias', 'field': 'proto'},",
            "        'filesize_approx': {'type': 'alias', 'field': 'fs_approx'},",
            "        'audio_channels': {'type': 'alias', 'field': 'channels'},",
            "",
            "        # Deprecated",
            "        'dimension': {'type': 'alias', 'field': 'res', 'deprecated': True},",
            "        'resolution': {'type': 'alias', 'field': 'res', 'deprecated': True},",
            "        'extension': {'type': 'alias', 'field': 'ext', 'deprecated': True},",
            "        'bitrate': {'type': 'alias', 'field': 'br', 'deprecated': True},",
            "        'total_bitrate': {'type': 'alias', 'field': 'tbr', 'deprecated': True},",
            "        'video_bitrate': {'type': 'alias', 'field': 'vbr', 'deprecated': True},",
            "        'audio_bitrate': {'type': 'alias', 'field': 'abr', 'deprecated': True},",
            "        'framerate': {'type': 'alias', 'field': 'fps', 'deprecated': True},",
            "        'filesize_estimate': {'type': 'alias', 'field': 'size', 'deprecated': True},",
            "        'samplerate': {'type': 'alias', 'field': 'asr', 'deprecated': True},",
            "        'video_ext': {'type': 'alias', 'field': 'vext', 'deprecated': True},",
            "        'audio_ext': {'type': 'alias', 'field': 'aext', 'deprecated': True},",
            "        'video_codec': {'type': 'alias', 'field': 'vcodec', 'deprecated': True},",
            "        'audio_codec': {'type': 'alias', 'field': 'acodec', 'deprecated': True},",
            "        'video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},",
            "        'has_video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},",
            "        'audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},",
            "        'has_audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},",
            "        'extractor': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},",
            "        'extractor_preference': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},",
            "    }",
            "",
            "    def __init__(self, ydl, field_preference):",
            "        self.ydl = ydl",
            "        self._order = []",
            "        self.evaluate_params(self.ydl.params, field_preference)",
            "        if ydl.params.get('verbose'):",
            "            self.print_verbose_info(self.ydl.write_debug)",
            "",
            "    def _get_field_setting(self, field, key):",
            "        if field not in self.settings:",
            "            if key in ('forced', 'priority'):",
            "                return False",
            "            self.ydl.deprecated_feature(f'Using arbitrary fields ({field}) for format sorting is '",
            "                                        'deprecated and may be removed in a future version')",
            "            self.settings[field] = {}",
            "        propObj = self.settings[field]",
            "        if key not in propObj:",
            "            type = propObj.get('type')",
            "            if key == 'field':",
            "                default = 'preference' if type == 'extractor' else (field,) if type in ('combined', 'multiple') else field",
            "            elif key == 'convert':",
            "                default = 'order' if type == 'ordered' else 'float_string' if field else 'ignore'",
            "            else:",
            "                default = {'type': 'field', 'visible': True, 'order': [], 'not_in_list': (None,)}.get(key, None)",
            "            propObj[key] = default",
            "        return propObj[key]",
            "",
            "    def _resolve_field_value(self, field, value, convertNone=False):",
            "        if value is None:",
            "            if not convertNone:",
            "                return None",
            "        else:",
            "            value = value.lower()",
            "        conversion = self._get_field_setting(field, 'convert')",
            "        if conversion == 'ignore':",
            "            return None",
            "        if conversion == 'string':",
            "            return value",
            "        elif conversion == 'float_none':",
            "            return float_or_none(value)",
            "        elif conversion == 'bytes':",
            "            return parse_bytes(value)",
            "        elif conversion == 'order':",
            "            order_list = (self._use_free_order and self._get_field_setting(field, 'order_free')) or self._get_field_setting(field, 'order')",
            "            use_regex = self._get_field_setting(field, 'regex')",
            "            list_length = len(order_list)",
            "            empty_pos = order_list.index('') if '' in order_list else list_length + 1",
            "            if use_regex and value is not None:",
            "                for i, regex in enumerate(order_list):",
            "                    if regex and re.match(regex, value):",
            "                        return list_length - i",
            "                return list_length - empty_pos  # not in list",
            "            else:  # not regex or  value = None",
            "                return list_length - (order_list.index(value) if value in order_list else empty_pos)",
            "        else:",
            "            if value.isnumeric():",
            "                return float(value)",
            "            else:",
            "                self.settings[field]['convert'] = 'string'",
            "                return value",
            "",
            "    def evaluate_params(self, params, sort_extractor):",
            "        self._use_free_order = params.get('prefer_free_formats', False)",
            "        self._sort_user = params.get('format_sort', [])",
            "        self._sort_extractor = sort_extractor",
            "",
            "        def add_item(field, reverse, closest, limit_text):",
            "            field = field.lower()",
            "            if field in self._order:",
            "                return",
            "            self._order.append(field)",
            "            limit = self._resolve_field_value(field, limit_text)",
            "            data = {",
            "                'reverse': reverse,",
            "                'closest': False if limit is None else closest,",
            "                'limit_text': limit_text,",
            "                'limit': limit}",
            "            if field in self.settings:",
            "                self.settings[field].update(data)",
            "            else:",
            "                self.settings[field] = data",
            "",
            "        sort_list = (",
            "            tuple(field for field in self.default if self._get_field_setting(field, 'forced'))",
            "            + (tuple() if params.get('format_sort_force', False)",
            "                else tuple(field for field in self.default if self._get_field_setting(field, 'priority')))",
            "            + tuple(self._sort_user) + tuple(sort_extractor) + self.default)",
            "",
            "        for item in sort_list:",
            "            match = re.match(self.regex, item)",
            "            if match is None:",
            "                raise ExtractorError('Invalid format sort string \"%s\" given by extractor' % item)",
            "            field = match.group('field')",
            "            if field is None:",
            "                continue",
            "            if self._get_field_setting(field, 'type') == 'alias':",
            "                alias, field = field, self._get_field_setting(field, 'field')",
            "                if self._get_field_setting(alias, 'deprecated'):",
            "                    self.ydl.deprecated_feature(f'Format sorting alias {alias} is deprecated and may '",
            "                                                f'be removed in a future version. Please use {field} instead')",
            "            reverse = match.group('reverse') is not None",
            "            closest = match.group('separator') == '~'",
            "            limit_text = match.group('limit')",
            "",
            "            has_limit = limit_text is not None",
            "            has_multiple_fields = self._get_field_setting(field, 'type') == 'combined'",
            "            has_multiple_limits = has_limit and has_multiple_fields and not self._get_field_setting(field, 'same_limit')",
            "",
            "            fields = self._get_field_setting(field, 'field') if has_multiple_fields else (field,)",
            "            limits = limit_text.split(':') if has_multiple_limits else (limit_text,) if has_limit else tuple()",
            "            limit_count = len(limits)",
            "            for (i, f) in enumerate(fields):",
            "                add_item(f, reverse, closest,",
            "                         limits[i] if i < limit_count",
            "                         else limits[0] if has_limit and not has_multiple_limits",
            "                         else None)",
            "",
            "    def print_verbose_info(self, write_debug):",
            "        if self._sort_user:",
            "            write_debug('Sort order given by user: %s' % ', '.join(self._sort_user))",
            "        if self._sort_extractor:",
            "            write_debug('Sort order given by extractor: %s' % ', '.join(self._sort_extractor))",
            "        write_debug('Formats sorted by: %s' % ', '.join(['%s%s%s' % (",
            "            '+' if self._get_field_setting(field, 'reverse') else '', field,",
            "            '%s%s(%s)' % ('~' if self._get_field_setting(field, 'closest') else ':',",
            "                          self._get_field_setting(field, 'limit_text'),",
            "                          self._get_field_setting(field, 'limit'))",
            "            if self._get_field_setting(field, 'limit_text') is not None else '')",
            "            for field in self._order if self._get_field_setting(field, 'visible')]))",
            "",
            "    def _calculate_field_preference_from_value(self, format, field, type, value):",
            "        reverse = self._get_field_setting(field, 'reverse')",
            "        closest = self._get_field_setting(field, 'closest')",
            "        limit = self._get_field_setting(field, 'limit')",
            "",
            "        if type == 'extractor':",
            "            maximum = self._get_field_setting(field, 'max')",
            "            if value is None or (maximum is not None and value >= maximum):",
            "                value = -1",
            "        elif type == 'boolean':",
            "            in_list = self._get_field_setting(field, 'in_list')",
            "            not_in_list = self._get_field_setting(field, 'not_in_list')",
            "            value = 0 if ((in_list is None or value in in_list) and (not_in_list is None or value not in not_in_list)) else -1",
            "        elif type == 'ordered':",
            "            value = self._resolve_field_value(field, value, True)",
            "",
            "        # try to convert to number",
            "        val_num = float_or_none(value, default=self._get_field_setting(field, 'default'))",
            "        is_num = self._get_field_setting(field, 'convert') != 'string' and val_num is not None",
            "        if is_num:",
            "            value = val_num",
            "",
            "        return ((-10, 0) if value is None",
            "                else (1, value, 0) if not is_num  # if a field has mixed strings and numbers, strings are sorted higher",
            "                else (0, -abs(value - limit), value - limit if reverse else limit - value) if closest",
            "                else (0, value, 0) if not reverse and (limit is None or value <= limit)",
            "                else (0, -value, 0) if limit is None or (reverse and value == limit) or value > limit",
            "                else (-1, value, 0))",
            "",
            "    def _calculate_field_preference(self, format, field):",
            "        type = self._get_field_setting(field, 'type')  # extractor, boolean, ordered, field, multiple",
            "        get_value = lambda f: format.get(self._get_field_setting(f, 'field'))",
            "        if type == 'multiple':",
            "            type = 'field'  # Only 'field' is allowed in multiple for now",
            "            actual_fields = self._get_field_setting(field, 'field')",
            "",
            "            value = self._get_field_setting(field, 'function')(get_value(f) for f in actual_fields)",
            "        else:",
            "            value = get_value(field)",
            "        return self._calculate_field_preference_from_value(format, field, type, value)",
            "",
            "    def calculate_preference(self, format):",
            "        # Determine missing protocol",
            "        if not format.get('protocol'):",
            "            format['protocol'] = determine_protocol(format)",
            "",
            "        # Determine missing ext",
            "        if not format.get('ext') and 'url' in format:",
            "            format['ext'] = determine_ext(format['url'])",
            "        if format.get('vcodec') == 'none':",
            "            format['audio_ext'] = format['ext'] if format.get('acodec') != 'none' else 'none'",
            "            format['video_ext'] = 'none'",
            "        else:",
            "            format['video_ext'] = format['ext']",
            "            format['audio_ext'] = 'none'",
            "        # if format.get('preference') is None and format.get('ext') in ('f4f', 'f4m'):  # Not supported?",
            "        #    format['preference'] = -1000",
            "",
            "        if format.get('preference') is None and format.get('ext') == 'flv' and re.match('[hx]265|he?vc?', format.get('vcodec') or ''):",
            "            # HEVC-over-FLV is out-of-spec by FLV's original spec",
            "            # ref. https://trac.ffmpeg.org/ticket/6389",
            "            # ref. https://github.com/yt-dlp/yt-dlp/pull/5821",
            "            format['preference'] = -100",
            "",
            "        # Determine missing bitrates",
            "        if format.get('vcodec') == 'none':",
            "            format['vbr'] = 0",
            "        if format.get('acodec') == 'none':",
            "            format['abr'] = 0",
            "        if not format.get('vbr') and format.get('vcodec') != 'none':",
            "            format['vbr'] = try_call(lambda: format['tbr'] - format['abr']) or None",
            "        if not format.get('abr') and format.get('acodec') != 'none':",
            "            format['abr'] = try_call(lambda: format['tbr'] - format['vbr']) or None",
            "        if not format.get('tbr'):",
            "            format['tbr'] = try_call(lambda: format['vbr'] + format['abr']) or None",
            "",
            "        return tuple(self._calculate_field_preference(format, field) for field in self._order)",
            "",
            "",
            "def filesize_from_tbr(tbr, duration):",
            "    \"\"\"",
            "    @param tbr:      Total bitrate in kbps (1000 bits/sec)",
            "    @param duration: Duration in seconds",
            "    @returns         Filesize in bytes",
            "    \"\"\"",
            "    if tbr is None or duration is None:",
            "        return None",
            "    return int(duration * tbr * (1000 / 8))",
            "",
            "",
            "# XXX: Temporary",
            "class _YDLLogger:",
            "    def __init__(self, ydl=None):",
            "        self._ydl = ydl",
            "",
            "    def debug(self, message):",
            "        if self._ydl:",
            "            self._ydl.write_debug(message)",
            "",
            "    def info(self, message):",
            "        if self._ydl:",
            "            self._ydl.to_screen(message)",
            "",
            "    def warning(self, message, *, once=False):",
            "        if self._ydl:",
            "            self._ydl.report_warning(message, once)",
            "",
            "    def error(self, message, *, is_error=True):",
            "        if self._ydl:",
            "            self._ydl.report_error(message, is_error=is_error)",
            "",
            "    def stdout(self, message):",
            "        if self._ydl:",
            "            self._ydl.to_stdout(message)",
            "",
            "    def stderr(self, message):",
            "        if self._ydl:",
            "            self._ydl.to_stderr(message)"
        ],
        "afterPatchFile": [
            "import base64",
            "import binascii",
            "import calendar",
            "import codecs",
            "import collections",
            "import collections.abc",
            "import contextlib",
            "import datetime as dt",
            "import email.header",
            "import email.utils",
            "import errno",
            "import hashlib",
            "import hmac",
            "import html.entities",
            "import html.parser",
            "import inspect",
            "import io",
            "import itertools",
            "import json",
            "import locale",
            "import math",
            "import mimetypes",
            "import netrc",
            "import operator",
            "import os",
            "import platform",
            "import random",
            "import re",
            "import shlex",
            "import socket",
            "import ssl",
            "import struct",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import traceback",
            "import types",
            "import unicodedata",
            "import urllib.error",
            "import urllib.parse",
            "import urllib.request",
            "import xml.etree.ElementTree",
            "",
            "from . import traversal",
            "",
            "from ..compat import functools  # isort: split",
            "from ..compat import (",
            "    compat_etree_fromstring,",
            "    compat_expanduser,",
            "    compat_HTMLParseError,",
            "    compat_os_name,",
            ")",
            "from ..dependencies import xattr",
            "",
            "__name__ = __name__.rsplit('.', 1)[0]  # Pretend to be the parent module",
            "",
            "# This is not clearly defined otherwise",
            "compiled_regex_type = type(re.compile(''))",
            "",
            "",
            "class NO_DEFAULT:",
            "    pass",
            "",
            "",
            "def IDENTITY(x):",
            "    return x",
            "",
            "",
            "ENGLISH_MONTH_NAMES = [",
            "    'January', 'February', 'March', 'April', 'May', 'June',",
            "    'July', 'August', 'September', 'October', 'November', 'December']",
            "",
            "MONTH_NAMES = {",
            "    'en': ENGLISH_MONTH_NAMES,",
            "    'fr': [",
            "        'janvier', 'f\u00e9vrier', 'mars', 'avril', 'mai', 'juin',",
            "        'juillet', 'ao\u00fbt', 'septembre', 'octobre', 'novembre', 'd\u00e9cembre'],",
            "    # these follow the genitive grammatical case (dope\u0142niacz)",
            "    # some websites might be using nominative, which will require another month list",
            "    # https://en.wikibooks.org/wiki/Polish/Noun_cases",
            "    'pl': ['stycznia', 'lutego', 'marca', 'kwietnia', 'maja', 'czerwca',",
            "           'lipca', 'sierpnia', 'wrze\u015bnia', 'pa\u017adziernika', 'listopada', 'grudnia'],",
            "}",
            "",
            "# From https://github.com/python/cpython/blob/3.11/Lib/email/_parseaddr.py#L36-L42",
            "TIMEZONE_NAMES = {",
            "    'UT': 0, 'UTC': 0, 'GMT': 0, 'Z': 0,",
            "    'AST': -4, 'ADT': -3,  # Atlantic (used in Canada)",
            "    'EST': -5, 'EDT': -4,  # Eastern",
            "    'CST': -6, 'CDT': -5,  # Central",
            "    'MST': -7, 'MDT': -6,  # Mountain",
            "    'PST': -8, 'PDT': -7   # Pacific",
            "}",
            "",
            "# needed for sanitizing filenames in restricted mode",
            "ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',",
            "                        itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],",
            "                                        'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))",
            "",
            "DATE_FORMATS = (",
            "    '%d %B %Y',",
            "    '%d %b %Y',",
            "    '%B %d %Y',",
            "    '%B %dst %Y',",
            "    '%B %dnd %Y',",
            "    '%B %drd %Y',",
            "    '%B %dth %Y',",
            "    '%b %d %Y',",
            "    '%b %dst %Y',",
            "    '%b %dnd %Y',",
            "    '%b %drd %Y',",
            "    '%b %dth %Y',",
            "    '%b %dst %Y %I:%M',",
            "    '%b %dnd %Y %I:%M',",
            "    '%b %drd %Y %I:%M',",
            "    '%b %dth %Y %I:%M',",
            "    '%Y %m %d',",
            "    '%Y-%m-%d',",
            "    '%Y.%m.%d.',",
            "    '%Y/%m/%d',",
            "    '%Y/%m/%d %H:%M',",
            "    '%Y/%m/%d %H:%M:%S',",
            "    '%Y%m%d%H%M',",
            "    '%Y%m%d%H%M%S',",
            "    '%Y%m%d',",
            "    '%Y-%m-%d %H:%M',",
            "    '%Y-%m-%d %H:%M:%S',",
            "    '%Y-%m-%d %H:%M:%S.%f',",
            "    '%Y-%m-%d %H:%M:%S:%f',",
            "    '%d.%m.%Y %H:%M',",
            "    '%d.%m.%Y %H.%M',",
            "    '%Y-%m-%dT%H:%M:%SZ',",
            "    '%Y-%m-%dT%H:%M:%S.%fZ',",
            "    '%Y-%m-%dT%H:%M:%S.%f0Z',",
            "    '%Y-%m-%dT%H:%M:%S',",
            "    '%Y-%m-%dT%H:%M:%S.%f',",
            "    '%Y-%m-%dT%H:%M',",
            "    '%b %d %Y at %H:%M',",
            "    '%b %d %Y at %H:%M:%S',",
            "    '%B %d %Y at %H:%M',",
            "    '%B %d %Y at %H:%M:%S',",
            "    '%H:%M %d-%b-%Y',",
            ")",
            "",
            "DATE_FORMATS_DAY_FIRST = list(DATE_FORMATS)",
            "DATE_FORMATS_DAY_FIRST.extend([",
            "    '%d-%m-%Y',",
            "    '%d.%m.%Y',",
            "    '%d.%m.%y',",
            "    '%d/%m/%Y',",
            "    '%d/%m/%y',",
            "    '%d/%m/%Y %H:%M:%S',",
            "    '%d-%m-%Y %H:%M',",
            "    '%H:%M %d/%m/%Y',",
            "])",
            "",
            "DATE_FORMATS_MONTH_FIRST = list(DATE_FORMATS)",
            "DATE_FORMATS_MONTH_FIRST.extend([",
            "    '%m-%d-%Y',",
            "    '%m.%d.%Y',",
            "    '%m/%d/%Y',",
            "    '%m/%d/%y',",
            "    '%m/%d/%Y %H:%M:%S',",
            "])",
            "",
            "PACKED_CODES_RE = r\"}\\('(.+)',(\\d+),(\\d+),'([^']+)'\\.split\\('\\|'\\)\"",
            "JSON_LD_RE = r'(?is)<script[^>]+type=([\"\\']?)application/ld\\+json\\1[^>]*>\\s*(?P<json_ld>{.+?}|\\[.+?\\])\\s*</script>'",
            "",
            "NUMBER_RE = r'\\d+(?:\\.\\d+)?'",
            "",
            "",
            "@functools.cache",
            "def preferredencoding():",
            "    \"\"\"Get preferred encoding.",
            "",
            "    Returns the best encoding scheme for the system, based on",
            "    locale.getpreferredencoding() and some further tweaks.",
            "    \"\"\"",
            "    try:",
            "        pref = locale.getpreferredencoding()",
            "        'TEST'.encode(pref)",
            "    except Exception:",
            "        pref = 'UTF-8'",
            "",
            "    return pref",
            "",
            "",
            "def write_json_file(obj, fn):",
            "    \"\"\" Encode obj as JSON and write it to fn, atomically if possible \"\"\"",
            "",
            "    tf = tempfile.NamedTemporaryFile(",
            "        prefix=f'{os.path.basename(fn)}.', dir=os.path.dirname(fn),",
            "        suffix='.tmp', delete=False, mode='w', encoding='utf-8')",
            "",
            "    try:",
            "        with tf:",
            "            json.dump(obj, tf, ensure_ascii=False)",
            "        if sys.platform == 'win32':",
            "            # Need to remove existing file on Windows, else os.rename raises",
            "            # WindowsError or FileExistsError.",
            "            with contextlib.suppress(OSError):",
            "                os.unlink(fn)",
            "        with contextlib.suppress(OSError):",
            "            mask = os.umask(0)",
            "            os.umask(mask)",
            "            os.chmod(tf.name, 0o666 & ~mask)",
            "        os.rename(tf.name, fn)",
            "    except Exception:",
            "        with contextlib.suppress(OSError):",
            "            os.remove(tf.name)",
            "        raise",
            "",
            "",
            "def find_xpath_attr(node, xpath, key, val=None):",
            "    \"\"\" Find the xpath xpath[@key=val] \"\"\"",
            "    assert re.match(r'^[a-zA-Z_-]+$', key)",
            "    expr = xpath + ('[@%s]' % key if val is None else f\"[@{key}='{val}']\")",
            "    return node.find(expr)",
            "",
            "# On python2.6 the xml.etree.ElementTree.Element methods don't support",
            "# the namespace parameter",
            "",
            "",
            "def xpath_with_ns(path, ns_map):",
            "    components = [c.split(':') for c in path.split('/')]",
            "    replaced = []",
            "    for c in components:",
            "        if len(c) == 1:",
            "            replaced.append(c[0])",
            "        else:",
            "            ns, tag = c",
            "            replaced.append('{%s}%s' % (ns_map[ns], tag))",
            "    return '/'.join(replaced)",
            "",
            "",
            "def xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):",
            "    def _find_xpath(xpath):",
            "        return node.find(xpath)",
            "",
            "    if isinstance(xpath, str):",
            "        n = _find_xpath(xpath)",
            "    else:",
            "        for xp in xpath:",
            "            n = _find_xpath(xp)",
            "            if n is not None:",
            "                break",
            "",
            "    if n is None:",
            "        if default is not NO_DEFAULT:",
            "            return default",
            "        elif fatal:",
            "            name = xpath if name is None else name",
            "            raise ExtractorError('Could not find XML element %s' % name)",
            "        else:",
            "            return None",
            "    return n",
            "",
            "",
            "def xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):",
            "    n = xpath_element(node, xpath, name, fatal=fatal, default=default)",
            "    if n is None or n == default:",
            "        return n",
            "    if n.text is None:",
            "        if default is not NO_DEFAULT:",
            "            return default",
            "        elif fatal:",
            "            name = xpath if name is None else name",
            "            raise ExtractorError('Could not find XML element\\'s text %s' % name)",
            "        else:",
            "            return None",
            "    return n.text",
            "",
            "",
            "def xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):",
            "    n = find_xpath_attr(node, xpath, key)",
            "    if n is None:",
            "        if default is not NO_DEFAULT:",
            "            return default",
            "        elif fatal:",
            "            name = f'{xpath}[@{key}]' if name is None else name",
            "            raise ExtractorError('Could not find XML attribute %s' % name)",
            "        else:",
            "            return None",
            "    return n.attrib[key]",
            "",
            "",
            "def get_element_by_id(id, html, **kwargs):",
            "    \"\"\"Return the content of the tag with the specified ID in the passed HTML document\"\"\"",
            "    return get_element_by_attribute('id', id, html, **kwargs)",
            "",
            "",
            "def get_element_html_by_id(id, html, **kwargs):",
            "    \"\"\"Return the html of the tag with the specified ID in the passed HTML document\"\"\"",
            "    return get_element_html_by_attribute('id', id, html, **kwargs)",
            "",
            "",
            "def get_element_by_class(class_name, html):",
            "    \"\"\"Return the content of the first tag with the specified class in the passed HTML document\"\"\"",
            "    retval = get_elements_by_class(class_name, html)",
            "    return retval[0] if retval else None",
            "",
            "",
            "def get_element_html_by_class(class_name, html):",
            "    \"\"\"Return the html of the first tag with the specified class in the passed HTML document\"\"\"",
            "    retval = get_elements_html_by_class(class_name, html)",
            "    return retval[0] if retval else None",
            "",
            "",
            "def get_element_by_attribute(attribute, value, html, **kwargs):",
            "    retval = get_elements_by_attribute(attribute, value, html, **kwargs)",
            "    return retval[0] if retval else None",
            "",
            "",
            "def get_element_html_by_attribute(attribute, value, html, **kargs):",
            "    retval = get_elements_html_by_attribute(attribute, value, html, **kargs)",
            "    return retval[0] if retval else None",
            "",
            "",
            "def get_elements_by_class(class_name, html, **kargs):",
            "    \"\"\"Return the content of all tags with the specified class in the passed HTML document as a list\"\"\"",
            "    return get_elements_by_attribute(",
            "        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),",
            "        html, escape_value=False)",
            "",
            "",
            "def get_elements_html_by_class(class_name, html):",
            "    \"\"\"Return the html of all tags with the specified class in the passed HTML document as a list\"\"\"",
            "    return get_elements_html_by_attribute(",
            "        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),",
            "        html, escape_value=False)",
            "",
            "",
            "def get_elements_by_attribute(*args, **kwargs):",
            "    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"",
            "    return [content for content, _ in get_elements_text_and_html_by_attribute(*args, **kwargs)]",
            "",
            "",
            "def get_elements_html_by_attribute(*args, **kwargs):",
            "    \"\"\"Return the html of the tag with the specified attribute in the passed HTML document\"\"\"",
            "    return [whole for _, whole in get_elements_text_and_html_by_attribute(*args, **kwargs)]",
            "",
            "",
            "def get_elements_text_and_html_by_attribute(attribute, value, html, *, tag=r'[\\w:.-]+', escape_value=True):",
            "    \"\"\"",
            "    Return the text (content) and the html (whole) of the tag with the specified",
            "    attribute in the passed HTML document",
            "    \"\"\"",
            "    if not value:",
            "        return",
            "",
            "    quote = '' if re.match(r'''[\\s\"'`=<>]''', value) else '?'",
            "",
            "    value = re.escape(value) if escape_value else value",
            "",
            "    partial_element_re = rf'''(?x)",
            "        <(?P<tag>{tag})",
            "         (?:\\s(?:[^>\"']|\"[^\"]*\"|'[^']*')*)?",
            "         \\s{re.escape(attribute)}\\s*=\\s*(?P<_q>['\"]{quote})(?-x:{value})(?P=_q)",
            "        '''",
            "",
            "    for m in re.finditer(partial_element_re, html):",
            "        content, whole = get_element_text_and_html_by_tag(m.group('tag'), html[m.start():])",
            "",
            "        yield (",
            "            unescapeHTML(re.sub(r'^(?P<q>[\"\\'])(?P<content>.*)(?P=q)$', r'\\g<content>', content, flags=re.DOTALL)),",
            "            whole",
            "        )",
            "",
            "",
            "class HTMLBreakOnClosingTagParser(html.parser.HTMLParser):",
            "    \"\"\"",
            "    HTML parser which raises HTMLBreakOnClosingTagException upon reaching the",
            "    closing tag for the first opening tag it has encountered, and can be used",
            "    as a context manager",
            "    \"\"\"",
            "",
            "    class HTMLBreakOnClosingTagException(Exception):",
            "        pass",
            "",
            "    def __init__(self):",
            "        self.tagstack = collections.deque()",
            "        html.parser.HTMLParser.__init__(self)",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, *_):",
            "        self.close()",
            "",
            "    def close(self):",
            "        # handle_endtag does not return upon raising HTMLBreakOnClosingTagException,",
            "        # so data remains buffered; we no longer have any interest in it, thus",
            "        # override this method to discard it",
            "        pass",
            "",
            "    def handle_starttag(self, tag, _):",
            "        self.tagstack.append(tag)",
            "",
            "    def handle_endtag(self, tag):",
            "        if not self.tagstack:",
            "            raise compat_HTMLParseError('no tags in the stack')",
            "        while self.tagstack:",
            "            inner_tag = self.tagstack.pop()",
            "            if inner_tag == tag:",
            "                break",
            "        else:",
            "            raise compat_HTMLParseError(f'matching opening tag for closing {tag} tag not found')",
            "        if not self.tagstack:",
            "            raise self.HTMLBreakOnClosingTagException()",
            "",
            "",
            "# XXX: This should be far less strict",
            "def get_element_text_and_html_by_tag(tag, html):",
            "    \"\"\"",
            "    For the first element with the specified tag in the passed HTML document",
            "    return its' content (text) and the whole element (html)",
            "    \"\"\"",
            "    def find_or_raise(haystack, needle, exc):",
            "        try:",
            "            return haystack.index(needle)",
            "        except ValueError:",
            "            raise exc",
            "    closing_tag = f'</{tag}>'",
            "    whole_start = find_or_raise(",
            "        html, f'<{tag}', compat_HTMLParseError(f'opening {tag} tag not found'))",
            "    content_start = find_or_raise(",
            "        html[whole_start:], '>', compat_HTMLParseError(f'malformed opening {tag} tag'))",
            "    content_start += whole_start + 1",
            "    with HTMLBreakOnClosingTagParser() as parser:",
            "        parser.feed(html[whole_start:content_start])",
            "        if not parser.tagstack or parser.tagstack[0] != tag:",
            "            raise compat_HTMLParseError(f'parser did not match opening {tag} tag')",
            "        offset = content_start",
            "        while offset < len(html):",
            "            next_closing_tag_start = find_or_raise(",
            "                html[offset:], closing_tag,",
            "                compat_HTMLParseError(f'closing {tag} tag not found'))",
            "            next_closing_tag_end = next_closing_tag_start + len(closing_tag)",
            "            try:",
            "                parser.feed(html[offset:offset + next_closing_tag_end])",
            "                offset += next_closing_tag_end",
            "            except HTMLBreakOnClosingTagParser.HTMLBreakOnClosingTagException:",
            "                return html[content_start:offset + next_closing_tag_start], \\",
            "                    html[whole_start:offset + next_closing_tag_end]",
            "        raise compat_HTMLParseError('unexpected end of html')",
            "",
            "",
            "class HTMLAttributeParser(html.parser.HTMLParser):",
            "    \"\"\"Trivial HTML parser to gather the attributes for a single element\"\"\"",
            "",
            "    def __init__(self):",
            "        self.attrs = {}",
            "        html.parser.HTMLParser.__init__(self)",
            "",
            "    def handle_starttag(self, tag, attrs):",
            "        self.attrs = dict(attrs)",
            "        raise compat_HTMLParseError('done')",
            "",
            "",
            "class HTMLListAttrsParser(html.parser.HTMLParser):",
            "    \"\"\"HTML parser to gather the attributes for the elements of a list\"\"\"",
            "",
            "    def __init__(self):",
            "        html.parser.HTMLParser.__init__(self)",
            "        self.items = []",
            "        self._level = 0",
            "",
            "    def handle_starttag(self, tag, attrs):",
            "        if tag == 'li' and self._level == 0:",
            "            self.items.append(dict(attrs))",
            "        self._level += 1",
            "",
            "    def handle_endtag(self, tag):",
            "        self._level -= 1",
            "",
            "",
            "def extract_attributes(html_element):",
            "    \"\"\"Given a string for an HTML element such as",
            "    <el",
            "         a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz",
            "         empty= noval entity=\"&amp;\"",
            "         sq='\"' dq=\"'\"",
            "    >",
            "    Decode and return a dictionary of attributes.",
            "    {",
            "        'a': 'foo', 'b': 'bar', c: 'baz', d: 'boz',",
            "        'empty': '', 'noval': None, 'entity': '&',",
            "        'sq': '\"', 'dq': '\\''",
            "    }.",
            "    \"\"\"",
            "    parser = HTMLAttributeParser()",
            "    with contextlib.suppress(compat_HTMLParseError):",
            "        parser.feed(html_element)",
            "        parser.close()",
            "    return parser.attrs",
            "",
            "",
            "def parse_list(webpage):",
            "    \"\"\"Given a string for an series of HTML <li> elements,",
            "    return a dictionary of their attributes\"\"\"",
            "    parser = HTMLListAttrsParser()",
            "    parser.feed(webpage)",
            "    parser.close()",
            "    return parser.items",
            "",
            "",
            "def clean_html(html):",
            "    \"\"\"Clean an HTML snippet into a readable string\"\"\"",
            "",
            "    if html is None:  # Convenience for sanitizing descriptions etc.",
            "        return html",
            "",
            "    html = re.sub(r'\\s+', ' ', html)",
            "    html = re.sub(r'(?u)\\s?<\\s?br\\s?/?\\s?>\\s?', '\\n', html)",
            "    html = re.sub(r'(?u)<\\s?/\\s?p\\s?>\\s?<\\s?p[^>]*>', '\\n', html)",
            "    # Strip html tags",
            "    html = re.sub('<.*?>', '', html)",
            "    # Replace html entities",
            "    html = unescapeHTML(html)",
            "    return html.strip()",
            "",
            "",
            "class LenientJSONDecoder(json.JSONDecoder):",
            "    # TODO: Write tests",
            "    def __init__(self, *args, transform_source=None, ignore_extra=False, close_objects=0, **kwargs):",
            "        self.transform_source, self.ignore_extra = transform_source, ignore_extra",
            "        self._close_attempts = 2 * close_objects",
            "        super().__init__(*args, **kwargs)",
            "",
            "    @staticmethod",
            "    def _close_object(err):",
            "        doc = err.doc[:err.pos]",
            "        # We need to add comma first to get the correct error message",
            "        if err.msg.startswith('Expecting \\',\\''):",
            "            return doc + ','",
            "        elif not doc.endswith(','):",
            "            return",
            "",
            "        if err.msg.startswith('Expecting property name'):",
            "            return doc[:-1] + '}'",
            "        elif err.msg.startswith('Expecting value'):",
            "            return doc[:-1] + ']'",
            "",
            "    def decode(self, s):",
            "        if self.transform_source:",
            "            s = self.transform_source(s)",
            "        for attempt in range(self._close_attempts + 1):",
            "            try:",
            "                if self.ignore_extra:",
            "                    return self.raw_decode(s.lstrip())[0]",
            "                return super().decode(s)",
            "            except json.JSONDecodeError as e:",
            "                if e.pos is None:",
            "                    raise",
            "                elif attempt < self._close_attempts:",
            "                    s = self._close_object(e)",
            "                    if s is not None:",
            "                        continue",
            "                raise type(e)(f'{e.msg} in {s[e.pos - 10:e.pos + 10]!r}', s, e.pos)",
            "        assert False, 'Too many attempts to decode JSON'",
            "",
            "",
            "def sanitize_open(filename, open_mode):",
            "    \"\"\"Try to open the given filename, and slightly tweak it if this fails.",
            "",
            "    Attempts to open the given filename. If this fails, it tries to change",
            "    the filename slightly, step by step, until it's either able to open it",
            "    or it fails and raises a final exception, like the standard open()",
            "    function.",
            "",
            "    It returns the tuple (stream, definitive_file_name).",
            "    \"\"\"",
            "    if filename == '-':",
            "        if sys.platform == 'win32':",
            "            import msvcrt",
            "",
            "            # stdout may be any IO stream, e.g. when using contextlib.redirect_stdout",
            "            with contextlib.suppress(io.UnsupportedOperation):",
            "                msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)",
            "        return (sys.stdout.buffer if hasattr(sys.stdout, 'buffer') else sys.stdout, filename)",
            "",
            "    for attempt in range(2):",
            "        try:",
            "            try:",
            "                if sys.platform == 'win32':",
            "                    # FIXME: An exclusive lock also locks the file from being read.",
            "                    # Since windows locks are mandatory, don't lock the file on windows (for now).",
            "                    # Ref: https://github.com/yt-dlp/yt-dlp/issues/3124",
            "                    raise LockingUnsupportedError()",
            "                stream = locked_file(filename, open_mode, block=False).__enter__()",
            "            except OSError:",
            "                stream = open(filename, open_mode)",
            "            return stream, filename",
            "        except OSError as err:",
            "            if attempt or err.errno in (errno.EACCES,):",
            "                raise",
            "            old_filename, filename = filename, sanitize_path(filename)",
            "            if old_filename == filename:",
            "                raise",
            "",
            "",
            "def timeconvert(timestr):",
            "    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"",
            "    timestamp = None",
            "    timetuple = email.utils.parsedate_tz(timestr)",
            "    if timetuple is not None:",
            "        timestamp = email.utils.mktime_tz(timetuple)",
            "    return timestamp",
            "",
            "",
            "def sanitize_filename(s, restricted=False, is_id=NO_DEFAULT):",
            "    \"\"\"Sanitizes a string so it could be used as part of a filename.",
            "    @param restricted   Use a stricter subset of allowed characters",
            "    @param is_id        Whether this is an ID that should be kept unchanged if possible.",
            "                        If unset, yt-dlp's new sanitization rules are in effect",
            "    \"\"\"",
            "    if s == '':",
            "        return ''",
            "",
            "    def replace_insane(char):",
            "        if restricted and char in ACCENT_CHARS:",
            "            return ACCENT_CHARS[char]",
            "        elif not restricted and char == '\\n':",
            "            return '\\0 '",
            "        elif is_id is NO_DEFAULT and not restricted and char in '\"*:<>?|/\\\\':",
            "            # Replace with their full-width unicode counterparts",
            "            return {'/': '\\u29F8', '\\\\': '\\u29f9'}.get(char, chr(ord(char) + 0xfee0))",
            "        elif char == '?' or ord(char) < 32 or ord(char) == 127:",
            "            return ''",
            "        elif char == '\"':",
            "            return '' if restricted else '\\''",
            "        elif char == ':':",
            "            return '\\0_\\0-' if restricted else '\\0 \\0-'",
            "        elif char in '\\\\/|*<>':",
            "            return '\\0_'",
            "        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace() or ord(char) > 127):",
            "            return '' if unicodedata.category(char)[0] in 'CM' else '\\0_'",
            "        return char",
            "",
            "    # Replace look-alike Unicode glyphs",
            "    if restricted and (is_id is NO_DEFAULT or not is_id):",
            "        s = unicodedata.normalize('NFKC', s)",
            "    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)  # Handle timestamps",
            "    result = ''.join(map(replace_insane, s))",
            "    if is_id is NO_DEFAULT:",
            "        result = re.sub(r'(\\0.)(?:(?=\\1)..)+', r'\\1', result)  # Remove repeated substitute chars",
            "        STRIP_RE = r'(?:\\0.|[ _-])*'",
            "        result = re.sub(f'^\\0.{STRIP_RE}|{STRIP_RE}\\0.$', '', result)  # Remove substitute chars from start/end",
            "    result = result.replace('\\0', '') or '_'",
            "",
            "    if not is_id:",
            "        while '__' in result:",
            "            result = result.replace('__', '_')",
            "        result = result.strip('_')",
            "        # Common case of \"Foreign band name - English song title\"",
            "        if restricted and result.startswith('-_'):",
            "            result = result[2:]",
            "        if result.startswith('-'):",
            "            result = '_' + result[len('-'):]",
            "        result = result.lstrip('.')",
            "        if not result:",
            "            result = '_'",
            "    return result",
            "",
            "",
            "def sanitize_path(s, force=False):",
            "    \"\"\"Sanitizes and normalizes path on Windows\"\"\"",
            "    # XXX: this handles drive relative paths (c:sth) incorrectly",
            "    if sys.platform == 'win32':",
            "        force = False",
            "        drive_or_unc, _ = os.path.splitdrive(s)",
            "    elif force:",
            "        drive_or_unc = ''",
            "    else:",
            "        return s",
            "",
            "    norm_path = os.path.normpath(remove_start(s, drive_or_unc)).split(os.path.sep)",
            "    if drive_or_unc:",
            "        norm_path.pop(0)",
            "    sanitized_path = [",
            "        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s.]$)', '#', path_part)",
            "        for path_part in norm_path]",
            "    if drive_or_unc:",
            "        sanitized_path.insert(0, drive_or_unc + os.path.sep)",
            "    elif force and s and s[0] == os.path.sep:",
            "        sanitized_path.insert(0, os.path.sep)",
            "    # TODO: Fix behavioral differences <3.12",
            "    # The workaround using `normpath` only superficially passes tests",
            "    # Ref: https://github.com/python/cpython/pull/100351",
            "    return os.path.normpath(os.path.join(*sanitized_path))",
            "",
            "",
            "def sanitize_url(url, *, scheme='http'):",
            "    # Prepend protocol-less URLs with `http:` scheme in order to mitigate",
            "    # the number of unwanted failures due to missing protocol",
            "    if url is None:",
            "        return",
            "    elif url.startswith('//'):",
            "        return f'{scheme}:{url}'",
            "    # Fix some common typos seen so far",
            "    COMMON_TYPOS = (",
            "        # https://github.com/ytdl-org/youtube-dl/issues/15649",
            "        (r'^httpss://', r'https://'),",
            "        # https://bx1.be/lives/direct-tv/",
            "        (r'^rmtp([es]?)://', r'rtmp\\1://'),",
            "    )",
            "    for mistake, fixup in COMMON_TYPOS:",
            "        if re.match(mistake, url):",
            "            return re.sub(mistake, fixup, url)",
            "    return url",
            "",
            "",
            "def extract_basic_auth(url):",
            "    parts = urllib.parse.urlsplit(url)",
            "    if parts.username is None:",
            "        return url, None",
            "    url = urllib.parse.urlunsplit(parts._replace(netloc=(",
            "        parts.hostname if parts.port is None",
            "        else '%s:%d' % (parts.hostname, parts.port))))",
            "    auth_payload = base64.b64encode(",
            "        ('%s:%s' % (parts.username, parts.password or '')).encode())",
            "    return url, f'Basic {auth_payload.decode()}'",
            "",
            "",
            "def expand_path(s):",
            "    \"\"\"Expand shell variables and ~\"\"\"",
            "    return os.path.expandvars(compat_expanduser(s))",
            "",
            "",
            "def orderedSet(iterable, *, lazy=False):",
            "    \"\"\"Remove all duplicates from the input iterable\"\"\"",
            "    def _iter():",
            "        seen = []  # Do not use set since the items can be unhashable",
            "        for x in iterable:",
            "            if x not in seen:",
            "                seen.append(x)",
            "                yield x",
            "",
            "    return _iter() if lazy else list(_iter())",
            "",
            "",
            "def _htmlentity_transform(entity_with_semicolon):",
            "    \"\"\"Transforms an HTML entity to a character.\"\"\"",
            "    entity = entity_with_semicolon[:-1]",
            "",
            "    # Known non-numeric HTML entity",
            "    if entity in html.entities.name2codepoint:",
            "        return chr(html.entities.name2codepoint[entity])",
            "",
            "    # TODO: HTML5 allows entities without a semicolon.",
            "    # E.g. '&Eacuteric' should be decoded as '\u00c9ric'.",
            "    if entity_with_semicolon in html.entities.html5:",
            "        return html.entities.html5[entity_with_semicolon]",
            "",
            "    mobj = re.match(r'#(x[0-9a-fA-F]+|[0-9]+)', entity)",
            "    if mobj is not None:",
            "        numstr = mobj.group(1)",
            "        if numstr.startswith('x'):",
            "            base = 16",
            "            numstr = '0%s' % numstr",
            "        else:",
            "            base = 10",
            "        # See https://github.com/ytdl-org/youtube-dl/issues/7518",
            "        with contextlib.suppress(ValueError):",
            "            return chr(int(numstr, base))",
            "",
            "    # Unknown entity in name, return its literal representation",
            "    return '&%s;' % entity",
            "",
            "",
            "def unescapeHTML(s):",
            "    if s is None:",
            "        return None",
            "    assert isinstance(s, str)",
            "",
            "    return re.sub(",
            "        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)",
            "",
            "",
            "def escapeHTML(text):",
            "    return (",
            "        text",
            "        .replace('&', '&amp;')",
            "        .replace('<', '&lt;')",
            "        .replace('>', '&gt;')",
            "        .replace('\"', '&quot;')",
            "        .replace(\"'\", '&#39;')",
            "    )",
            "",
            "",
            "class netrc_from_content(netrc.netrc):",
            "    def __init__(self, content):",
            "        self.hosts, self.macros = {}, {}",
            "        with io.StringIO(content) as stream:",
            "            self._parse('-', stream, False)",
            "",
            "",
            "class Popen(subprocess.Popen):",
            "    if sys.platform == 'win32':",
            "        _startupinfo = subprocess.STARTUPINFO()",
            "        _startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW",
            "    else:",
            "        _startupinfo = None",
            "",
            "    @staticmethod",
            "    def _fix_pyinstaller_ld_path(env):",
            "        \"\"\"Restore LD_LIBRARY_PATH when using PyInstaller",
            "            Ref: https://github.com/pyinstaller/pyinstaller/blob/develop/doc/runtime-information.rst#ld_library_path--libpath-considerations",
            "                 https://github.com/yt-dlp/yt-dlp/issues/4573",
            "        \"\"\"",
            "        if not hasattr(sys, '_MEIPASS'):",
            "            return",
            "",
            "        def _fix(key):",
            "            orig = env.get(f'{key}_ORIG')",
            "            if orig is None:",
            "                env.pop(key, None)",
            "            else:",
            "                env[key] = orig",
            "",
            "        _fix('LD_LIBRARY_PATH')  # Linux",
            "        _fix('DYLD_LIBRARY_PATH')  # macOS",
            "",
            "    def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):",
            "        if env is None:",
            "            env = os.environ.copy()",
            "        self._fix_pyinstaller_ld_path(env)",
            "",
            "        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')",
            "        if text is True:",
            "            kwargs['universal_newlines'] = True  # For 3.6 compatibility",
            "            kwargs.setdefault('encoding', 'utf-8')",
            "            kwargs.setdefault('errors', 'replace')",
            "",
            "        if shell and compat_os_name == 'nt' and kwargs.get('executable') is None:",
            "            if not isinstance(args, str):",
            "                args = shell_quote(args, shell=True)",
            "            shell = False",
            "            # Set variable for `cmd.exe` newline escaping (see `utils.shell_quote`)",
            "            env['='] = '\"^\\n\\n\"'",
            "            args = f'{self.__comspec()} /Q /S /D /V:OFF /E:ON /C \"{args}\"'",
            "",
            "        super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)",
            "",
            "    def __comspec(self):",
            "        comspec = os.environ.get('ComSpec') or os.path.join(",
            "            os.environ.get('SystemRoot', ''), 'System32', 'cmd.exe')",
            "        if os.path.isabs(comspec):",
            "            return comspec",
            "        raise FileNotFoundError('shell not found: neither %ComSpec% nor %SystemRoot% is set')",
            "",
            "    def communicate_or_kill(self, *args, **kwargs):",
            "        try:",
            "            return self.communicate(*args, **kwargs)",
            "        except BaseException:  # Including KeyboardInterrupt",
            "            self.kill(timeout=None)",
            "            raise",
            "",
            "    def kill(self, *, timeout=0):",
            "        super().kill()",
            "        if timeout != 0:",
            "            self.wait(timeout=timeout)",
            "",
            "    @classmethod",
            "    def run(cls, *args, timeout=None, **kwargs):",
            "        with cls(*args, **kwargs) as proc:",
            "            default = '' if proc.__text_mode else b''",
            "            stdout, stderr = proc.communicate_or_kill(timeout=timeout)",
            "            return stdout or default, stderr or default, proc.returncode",
            "",
            "",
            "def encodeArgument(s):",
            "    # Legacy code that uses byte strings",
            "    # Uncomment the following line after fixing all post processors",
            "    # assert isinstance(s, str), 'Internal error: %r should be of type %r, is %r' % (s, str, type(s))",
            "    return s if isinstance(s, str) else s.decode('ascii')",
            "",
            "",
            "_timetuple = collections.namedtuple('Time', ('hours', 'minutes', 'seconds', 'milliseconds'))",
            "",
            "",
            "def timetuple_from_msec(msec):",
            "    secs, msec = divmod(msec, 1000)",
            "    mins, secs = divmod(secs, 60)",
            "    hrs, mins = divmod(mins, 60)",
            "    return _timetuple(hrs, mins, secs, msec)",
            "",
            "",
            "def formatSeconds(secs, delim=':', msec=False):",
            "    time = timetuple_from_msec(secs * 1000)",
            "    if time.hours:",
            "        ret = '%d%s%02d%s%02d' % (time.hours, delim, time.minutes, delim, time.seconds)",
            "    elif time.minutes:",
            "        ret = '%d%s%02d' % (time.minutes, delim, time.seconds)",
            "    else:",
            "        ret = '%d' % time.seconds",
            "    return '%s.%03d' % (ret, time.milliseconds) if msec else ret",
            "",
            "",
            "def bug_reports_message(before=';'):",
            "    from ..update import REPOSITORY",
            "",
            "    msg = (f'please report this issue on  https://github.com/{REPOSITORY}/issues?q= , '",
            "           'filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U')",
            "",
            "    before = before.rstrip()",
            "    if not before or before.endswith(('.', '!', '?')):",
            "        msg = msg[0].title() + msg[1:]",
            "",
            "    return (before + ' ' if before else '') + msg",
            "",
            "",
            "class YoutubeDLError(Exception):",
            "    \"\"\"Base exception for YoutubeDL errors.\"\"\"",
            "    msg = None",
            "",
            "    def __init__(self, msg=None):",
            "        if msg is not None:",
            "            self.msg = msg",
            "        elif self.msg is None:",
            "            self.msg = type(self).__name__",
            "        super().__init__(self.msg)",
            "",
            "",
            "class ExtractorError(YoutubeDLError):",
            "    \"\"\"Error during info extraction.\"\"\"",
            "",
            "    def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None, ie=None):",
            "        \"\"\" tb, if given, is the original traceback (so that it can be printed out).",
            "        If expected is set, this is a normal error message and most likely not a bug in yt-dlp.",
            "        \"\"\"",
            "        from ..networking.exceptions import network_exceptions",
            "        if sys.exc_info()[0] in network_exceptions:",
            "            expected = True",
            "",
            "        self.orig_msg = str(msg)",
            "        self.traceback = tb",
            "        self.expected = expected",
            "        self.cause = cause",
            "        self.video_id = video_id",
            "        self.ie = ie",
            "        self.exc_info = sys.exc_info()  # preserve original exception",
            "        if isinstance(self.exc_info[1], ExtractorError):",
            "            self.exc_info = self.exc_info[1].exc_info",
            "        super().__init__(self.__msg)",
            "",
            "    @property",
            "    def __msg(self):",
            "        return ''.join((",
            "            format_field(self.ie, None, '[%s] '),",
            "            format_field(self.video_id, None, '%s: '),",
            "            self.orig_msg,",
            "            format_field(self.cause, None, ' (caused by %r)'),",
            "            '' if self.expected else bug_reports_message()))",
            "",
            "    def format_traceback(self):",
            "        return join_nonempty(",
            "            self.traceback and ''.join(traceback.format_tb(self.traceback)),",
            "            self.cause and ''.join(traceback.format_exception(None, self.cause, self.cause.__traceback__)[1:]),",
            "            delim='\\n') or None",
            "",
            "    def __setattr__(self, name, value):",
            "        super().__setattr__(name, value)",
            "        if getattr(self, 'msg', None) and name not in ('msg', 'args'):",
            "            self.msg = self.__msg or type(self).__name__",
            "            self.args = (self.msg, )  # Cannot be property",
            "",
            "",
            "class UnsupportedError(ExtractorError):",
            "    def __init__(self, url):",
            "        super().__init__(",
            "            'Unsupported URL: %s' % url, expected=True)",
            "        self.url = url",
            "",
            "",
            "class RegexNotFoundError(ExtractorError):",
            "    \"\"\"Error when a regex didn't match\"\"\"",
            "    pass",
            "",
            "",
            "class GeoRestrictedError(ExtractorError):",
            "    \"\"\"Geographic restriction Error exception.",
            "",
            "    This exception may be thrown when a video is not available from your",
            "    geographic location due to geographic restrictions imposed by a website.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg, countries=None, **kwargs):",
            "        kwargs['expected'] = True",
            "        super().__init__(msg, **kwargs)",
            "        self.countries = countries",
            "",
            "",
            "class UserNotLive(ExtractorError):",
            "    \"\"\"Error when a channel/user is not live\"\"\"",
            "",
            "    def __init__(self, msg=None, **kwargs):",
            "        kwargs['expected'] = True",
            "        super().__init__(msg or 'The channel is not currently live', **kwargs)",
            "",
            "",
            "class DownloadError(YoutubeDLError):",
            "    \"\"\"Download Error exception.",
            "",
            "    This exception may be thrown by FileDownloader objects if they are not",
            "    configured to continue on errors. They will contain the appropriate",
            "    error message.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg, exc_info=None):",
            "        \"\"\" exc_info, if given, is the original exception that caused the trouble (as returned by sys.exc_info()). \"\"\"",
            "        super().__init__(msg)",
            "        self.exc_info = exc_info",
            "",
            "",
            "class EntryNotInPlaylist(YoutubeDLError):",
            "    \"\"\"Entry not in playlist exception.",
            "",
            "    This exception will be thrown by YoutubeDL when a requested entry",
            "    is not found in the playlist info_dict",
            "    \"\"\"",
            "    msg = 'Entry not found in info'",
            "",
            "",
            "class SameFileError(YoutubeDLError):",
            "    \"\"\"Same File exception.",
            "",
            "    This exception will be thrown by FileDownloader objects if they detect",
            "    multiple files would have to be downloaded to the same file on disk.",
            "    \"\"\"",
            "    msg = 'Fixed output name but more than one file to download'",
            "",
            "    def __init__(self, filename=None):",
            "        if filename is not None:",
            "            self.msg += f': {filename}'",
            "        super().__init__(self.msg)",
            "",
            "",
            "class PostProcessingError(YoutubeDLError):",
            "    \"\"\"Post Processing exception.",
            "",
            "    This exception may be raised by PostProcessor's .run() method to",
            "    indicate an error in the postprocessing task.",
            "    \"\"\"",
            "",
            "",
            "class DownloadCancelled(YoutubeDLError):",
            "    \"\"\" Exception raised when the download queue should be interrupted \"\"\"",
            "    msg = 'The download was cancelled'",
            "",
            "",
            "class ExistingVideoReached(DownloadCancelled):",
            "    \"\"\" --break-on-existing triggered \"\"\"",
            "    msg = 'Encountered a video that is already in the archive, stopping due to --break-on-existing'",
            "",
            "",
            "class RejectedVideoReached(DownloadCancelled):",
            "    \"\"\" --break-match-filter triggered \"\"\"",
            "    msg = 'Encountered a video that did not match filter, stopping due to --break-match-filter'",
            "",
            "",
            "class MaxDownloadsReached(DownloadCancelled):",
            "    \"\"\" --max-downloads limit has been reached. \"\"\"",
            "    msg = 'Maximum number of downloads reached, stopping due to --max-downloads'",
            "",
            "",
            "class ReExtractInfo(YoutubeDLError):",
            "    \"\"\" Video info needs to be re-extracted. \"\"\"",
            "",
            "    def __init__(self, msg, expected=False):",
            "        super().__init__(msg)",
            "        self.expected = expected",
            "",
            "",
            "class ThrottledDownload(ReExtractInfo):",
            "    \"\"\" Download speed below --throttled-rate. \"\"\"",
            "    msg = 'The download speed is below throttle limit'",
            "",
            "    def __init__(self):",
            "        super().__init__(self.msg, expected=False)",
            "",
            "",
            "class UnavailableVideoError(YoutubeDLError):",
            "    \"\"\"Unavailable Format exception.",
            "",
            "    This exception will be thrown when a video is requested",
            "    in a format that is not available for that video.",
            "    \"\"\"",
            "    msg = 'Unable to download video'",
            "",
            "    def __init__(self, err=None):",
            "        if err is not None:",
            "            self.msg += f': {err}'",
            "        super().__init__(self.msg)",
            "",
            "",
            "class ContentTooShortError(YoutubeDLError):",
            "    \"\"\"Content Too Short exception.",
            "",
            "    This exception may be raised by FileDownloader objects when a file they",
            "    download is too small for what the server announced first, indicating",
            "    the connection was probably interrupted.",
            "    \"\"\"",
            "",
            "    def __init__(self, downloaded, expected):",
            "        super().__init__(f'Downloaded {downloaded} bytes, expected {expected} bytes')",
            "        # Both in bytes",
            "        self.downloaded = downloaded",
            "        self.expected = expected",
            "",
            "",
            "class XAttrMetadataError(YoutubeDLError):",
            "    def __init__(self, code=None, msg='Unknown error'):",
            "        super().__init__(msg)",
            "        self.code = code",
            "        self.msg = msg",
            "",
            "        # Parsing code and msg",
            "        if (self.code in (errno.ENOSPC, errno.EDQUOT)",
            "                or 'No space left' in self.msg or 'Disk quota exceeded' in self.msg):",
            "            self.reason = 'NO_SPACE'",
            "        elif self.code == errno.E2BIG or 'Argument list too long' in self.msg:",
            "            self.reason = 'VALUE_TOO_LONG'",
            "        else:",
            "            self.reason = 'NOT_SUPPORTED'",
            "",
            "",
            "class XAttrUnavailableError(YoutubeDLError):",
            "    pass",
            "",
            "",
            "def is_path_like(f):",
            "    return isinstance(f, (str, bytes, os.PathLike))",
            "",
            "",
            "def extract_timezone(date_str):",
            "    m = re.search(",
            "        r'''(?x)",
            "            ^.{8,}?                                              # >=8 char non-TZ prefix, if present",
            "            (?P<tz>Z|                                            # just the UTC Z, or",
            "                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|                   # preceded by 4 digits or hh:mm or",
            "                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))     # not preceded by 3 alpha word or >= 4 alpha or 2 digits",
            "                   [ ]?                                          # optional space",
            "                (?P<sign>\\+|-)                                   # +/-",
            "                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})       # hh[:]mm",
            "            $)",
            "        ''', date_str)",
            "    if not m:",
            "        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)",
            "        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())",
            "        if timezone is not None:",
            "            date_str = date_str[:-len(m.group('tz'))]",
            "        timezone = dt.timedelta(hours=timezone or 0)",
            "    else:",
            "        date_str = date_str[:-len(m.group('tz'))]",
            "        if not m.group('sign'):",
            "            timezone = dt.timedelta()",
            "        else:",
            "            sign = 1 if m.group('sign') == '+' else -1",
            "            timezone = dt.timedelta(",
            "                hours=sign * int(m.group('hours')),",
            "                minutes=sign * int(m.group('minutes')))",
            "    return timezone, date_str",
            "",
            "",
            "def parse_iso8601(date_str, delimiter='T', timezone=None):",
            "    \"\"\" Return a UNIX timestamp from the given date \"\"\"",
            "",
            "    if date_str is None:",
            "        return None",
            "",
            "    date_str = re.sub(r'\\.[0-9]+', '', date_str)",
            "",
            "    if timezone is None:",
            "        timezone, date_str = extract_timezone(date_str)",
            "",
            "    with contextlib.suppress(ValueError):",
            "        date_format = f'%Y-%m-%d{delimiter}%H:%M:%S'",
            "        dt_ = dt.datetime.strptime(date_str, date_format) - timezone",
            "        return calendar.timegm(dt_.timetuple())",
            "",
            "",
            "def date_formats(day_first=True):",
            "    return DATE_FORMATS_DAY_FIRST if day_first else DATE_FORMATS_MONTH_FIRST",
            "",
            "",
            "def unified_strdate(date_str, day_first=True):",
            "    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"",
            "",
            "    if date_str is None:",
            "        return None",
            "    upload_date = None",
            "    # Replace commas",
            "    date_str = date_str.replace(',', ' ')",
            "    # Remove AM/PM + timezone",
            "    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)",
            "    _, date_str = extract_timezone(date_str)",
            "",
            "    for expression in date_formats(day_first):",
            "        with contextlib.suppress(ValueError):",
            "            upload_date = dt.datetime.strptime(date_str, expression).strftime('%Y%m%d')",
            "    if upload_date is None:",
            "        timetuple = email.utils.parsedate_tz(date_str)",
            "        if timetuple:",
            "            with contextlib.suppress(ValueError):",
            "                upload_date = dt.datetime(*timetuple[:6]).strftime('%Y%m%d')",
            "    if upload_date is not None:",
            "        return str(upload_date)",
            "",
            "",
            "def unified_timestamp(date_str, day_first=True):",
            "    if not isinstance(date_str, str):",
            "        return None",
            "",
            "    date_str = re.sub(r'\\s+', ' ', re.sub(",
            "        r'(?i)[,|]|(mon|tues?|wed(nes)?|thu(rs)?|fri|sat(ur)?)(day)?', '', date_str))",
            "",
            "    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0",
            "    timezone, date_str = extract_timezone(date_str)",
            "",
            "    # Remove AM/PM + timezone",
            "    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)",
            "",
            "    # Remove unrecognized timezones from ISO 8601 alike timestamps",
            "    m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)",
            "    if m:",
            "        date_str = date_str[:-len(m.group('tz'))]",
            "",
            "    # Python only supports microseconds, so remove nanoseconds",
            "    m = re.search(r'^([0-9]{4,}-[0-9]{1,2}-[0-9]{1,2}T[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{6})[0-9]+$', date_str)",
            "    if m:",
            "        date_str = m.group(1)",
            "",
            "    for expression in date_formats(day_first):",
            "        with contextlib.suppress(ValueError):",
            "            dt_ = dt.datetime.strptime(date_str, expression) - timezone + dt.timedelta(hours=pm_delta)",
            "            return calendar.timegm(dt_.timetuple())",
            "",
            "    timetuple = email.utils.parsedate_tz(date_str)",
            "    if timetuple:",
            "        return calendar.timegm(timetuple) + pm_delta * 3600 - timezone.total_seconds()",
            "",
            "",
            "def determine_ext(url, default_ext='unknown_video'):",
            "    if url is None or '.' not in url:",
            "        return default_ext",
            "    guess = url.partition('?')[0].rpartition('.')[2]",
            "    if re.match(r'^[A-Za-z0-9]+$', guess):",
            "        return guess",
            "    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download",
            "    elif guess.rstrip('/') in KNOWN_EXTENSIONS:",
            "        return guess.rstrip('/')",
            "    else:",
            "        return default_ext",
            "",
            "",
            "def subtitles_filename(filename, sub_lang, sub_format, expected_real_ext=None):",
            "    return replace_extension(filename, sub_lang + '.' + sub_format, expected_real_ext)",
            "",
            "",
            "def datetime_from_str(date_str, precision='auto', format='%Y%m%d'):",
            "    R\"\"\"",
            "    Return a datetime object from a string.",
            "    Supported format:",
            "        (now|today|yesterday|DATE)([+-]\\d+(microsecond|second|minute|hour|day|week|month|year)s?)?",
            "",
            "    @param format       strftime format of DATE",
            "    @param precision    Round the datetime object: auto|microsecond|second|minute|hour|day",
            "                        auto: round to the unit provided in date_str (if applicable).",
            "    \"\"\"",
            "    auto_precision = False",
            "    if precision == 'auto':",
            "        auto_precision = True",
            "        precision = 'microsecond'",
            "    today = datetime_round(dt.datetime.now(dt.timezone.utc), precision)",
            "    if date_str in ('now', 'today'):",
            "        return today",
            "    if date_str == 'yesterday':",
            "        return today - dt.timedelta(days=1)",
            "    match = re.match(",
            "        r'(?P<start>.+)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>microsecond|second|minute|hour|day|week|month|year)s?',",
            "        date_str)",
            "    if match is not None:",
            "        start_time = datetime_from_str(match.group('start'), precision, format)",
            "        time = int(match.group('time')) * (-1 if match.group('sign') == '-' else 1)",
            "        unit = match.group('unit')",
            "        if unit == 'month' or unit == 'year':",
            "            new_date = datetime_add_months(start_time, time * 12 if unit == 'year' else time)",
            "            unit = 'day'",
            "        else:",
            "            if unit == 'week':",
            "                unit = 'day'",
            "                time *= 7",
            "            delta = dt.timedelta(**{unit + 's': time})",
            "            new_date = start_time + delta",
            "        if auto_precision:",
            "            return datetime_round(new_date, unit)",
            "        return new_date",
            "",
            "    return datetime_round(dt.datetime.strptime(date_str, format), precision)",
            "",
            "",
            "def date_from_str(date_str, format='%Y%m%d', strict=False):",
            "    R\"\"\"",
            "    Return a date object from a string using datetime_from_str",
            "",
            "    @param strict  Restrict allowed patterns to \"YYYYMMDD\" and",
            "                   (now|today|yesterday)(-\\d+(day|week|month|year)s?)?",
            "    \"\"\"",
            "    if strict and not re.fullmatch(r'\\d{8}|(now|today|yesterday)(-\\d+(day|week|month|year)s?)?', date_str):",
            "        raise ValueError(f'Invalid date format \"{date_str}\"')",
            "    return datetime_from_str(date_str, precision='microsecond', format=format).date()",
            "",
            "",
            "def datetime_add_months(dt_, months):",
            "    \"\"\"Increment/Decrement a datetime object by months.\"\"\"",
            "    month = dt_.month + months - 1",
            "    year = dt_.year + month // 12",
            "    month = month % 12 + 1",
            "    day = min(dt_.day, calendar.monthrange(year, month)[1])",
            "    return dt_.replace(year, month, day)",
            "",
            "",
            "def datetime_round(dt_, precision='day'):",
            "    \"\"\"",
            "    Round a datetime object's time to a specific precision",
            "    \"\"\"",
            "    if precision == 'microsecond':",
            "        return dt_",
            "",
            "    unit_seconds = {",
            "        'day': 86400,",
            "        'hour': 3600,",
            "        'minute': 60,",
            "        'second': 1,",
            "    }",
            "    roundto = lambda x, n: ((x + n / 2) // n) * n",
            "    timestamp = roundto(calendar.timegm(dt_.timetuple()), unit_seconds[precision])",
            "    return dt.datetime.fromtimestamp(timestamp, dt.timezone.utc)",
            "",
            "",
            "def hyphenate_date(date_str):",
            "    \"\"\"",
            "    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format\"\"\"",
            "    match = re.match(r'^(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)$', date_str)",
            "    if match is not None:",
            "        return '-'.join(match.groups())",
            "    else:",
            "        return date_str",
            "",
            "",
            "class DateRange:",
            "    \"\"\"Represents a time interval between two dates\"\"\"",
            "",
            "    def __init__(self, start=None, end=None):",
            "        \"\"\"start and end must be strings in the format accepted by date\"\"\"",
            "        if start is not None:",
            "            self.start = date_from_str(start, strict=True)",
            "        else:",
            "            self.start = dt.datetime.min.date()",
            "        if end is not None:",
            "            self.end = date_from_str(end, strict=True)",
            "        else:",
            "            self.end = dt.datetime.max.date()",
            "        if self.start > self.end:",
            "            raise ValueError('Date range: \"%s\" , the start date must be before the end date' % self)",
            "",
            "    @classmethod",
            "    def day(cls, day):",
            "        \"\"\"Returns a range that only contains the given day\"\"\"",
            "        return cls(day, day)",
            "",
            "    def __contains__(self, date):",
            "        \"\"\"Check if the date is in the range\"\"\"",
            "        if not isinstance(date, dt.date):",
            "            date = date_from_str(date)",
            "        return self.start <= date <= self.end",
            "",
            "    def __repr__(self):",
            "        return f'{__name__}.{type(self).__name__}({self.start.isoformat()!r}, {self.end.isoformat()!r})'",
            "",
            "    def __str__(self):",
            "        return f'{self.start} to {self.end}'",
            "",
            "    def __eq__(self, other):",
            "        return (isinstance(other, DateRange)",
            "                and self.start == other.start and self.end == other.end)",
            "",
            "",
            "@functools.cache",
            "def system_identifier():",
            "    python_implementation = platform.python_implementation()",
            "    if python_implementation == 'PyPy' and hasattr(sys, 'pypy_version_info'):",
            "        python_implementation += ' version %d.%d.%d' % sys.pypy_version_info[:3]",
            "    libc_ver = []",
            "    with contextlib.suppress(OSError):  # We may not have access to the executable",
            "        libc_ver = platform.libc_ver()",
            "",
            "    return 'Python %s (%s %s %s) - %s (%s%s)' % (",
            "        platform.python_version(),",
            "        python_implementation,",
            "        platform.machine(),",
            "        platform.architecture()[0],",
            "        platform.platform(),",
            "        ssl.OPENSSL_VERSION,",
            "        format_field(join_nonempty(*libc_ver, delim=' '), None, ', %s'),",
            "    )",
            "",
            "",
            "@functools.cache",
            "def get_windows_version():",
            "    ''' Get Windows version. returns () if it's not running on Windows '''",
            "    if compat_os_name == 'nt':",
            "        return version_tuple(platform.win32_ver()[1])",
            "    else:",
            "        return ()",
            "",
            "",
            "def write_string(s, out=None, encoding=None):",
            "    assert isinstance(s, str)",
            "    out = out or sys.stderr",
            "    # `sys.stderr` might be `None` (Ref: https://github.com/pyinstaller/pyinstaller/pull/7217)",
            "    if not out:",
            "        return",
            "",
            "    if compat_os_name == 'nt' and supports_terminal_sequences(out):",
            "        s = re.sub(r'([\\r\\n]+)', r' \\1', s)",
            "",
            "    enc, buffer = None, out",
            "    # `mode` might be `None` (Ref: https://github.com/yt-dlp/yt-dlp/issues/8816)",
            "    if 'b' in (getattr(out, 'mode', None) or ''):",
            "        enc = encoding or preferredencoding()",
            "    elif hasattr(out, 'buffer'):",
            "        buffer = out.buffer",
            "        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()",
            "",
            "    buffer.write(s.encode(enc, 'ignore') if enc else s)",
            "    out.flush()",
            "",
            "",
            "# TODO: Use global logger",
            "def deprecation_warning(msg, *, printer=None, stacklevel=0, **kwargs):",
            "    from .. import _IN_CLI",
            "    if _IN_CLI:",
            "        if msg in deprecation_warning._cache:",
            "            return",
            "        deprecation_warning._cache.add(msg)",
            "        if printer:",
            "            return printer(f'{msg}{bug_reports_message()}', **kwargs)",
            "        return write_string(f'ERROR: {msg}{bug_reports_message()}\\n', **kwargs)",
            "    else:",
            "        import warnings",
            "        warnings.warn(DeprecationWarning(msg), stacklevel=stacklevel + 3)",
            "",
            "",
            "deprecation_warning._cache = set()",
            "",
            "",
            "def bytes_to_intlist(bs):",
            "    if not bs:",
            "        return []",
            "    if isinstance(bs[0], int):  # Python 3",
            "        return list(bs)",
            "    else:",
            "        return [ord(c) for c in bs]",
            "",
            "",
            "def intlist_to_bytes(xs):",
            "    if not xs:",
            "        return b''",
            "    return struct.pack('%dB' % len(xs), *xs)",
            "",
            "",
            "class LockingUnsupportedError(OSError):",
            "    msg = 'File locking is not supported'",
            "",
            "    def __init__(self):",
            "        super().__init__(self.msg)",
            "",
            "",
            "# Cross-platform file locking",
            "if sys.platform == 'win32':",
            "    import ctypes",
            "    import ctypes.wintypes",
            "    import msvcrt",
            "",
            "    class OVERLAPPED(ctypes.Structure):",
            "        _fields_ = [",
            "            ('Internal', ctypes.wintypes.LPVOID),",
            "            ('InternalHigh', ctypes.wintypes.LPVOID),",
            "            ('Offset', ctypes.wintypes.DWORD),",
            "            ('OffsetHigh', ctypes.wintypes.DWORD),",
            "            ('hEvent', ctypes.wintypes.HANDLE),",
            "        ]",
            "",
            "    kernel32 = ctypes.WinDLL('kernel32')",
            "    LockFileEx = kernel32.LockFileEx",
            "    LockFileEx.argtypes = [",
            "        ctypes.wintypes.HANDLE,     # hFile",
            "        ctypes.wintypes.DWORD,      # dwFlags",
            "        ctypes.wintypes.DWORD,      # dwReserved",
            "        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow",
            "        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh",
            "        ctypes.POINTER(OVERLAPPED)  # Overlapped",
            "    ]",
            "    LockFileEx.restype = ctypes.wintypes.BOOL",
            "    UnlockFileEx = kernel32.UnlockFileEx",
            "    UnlockFileEx.argtypes = [",
            "        ctypes.wintypes.HANDLE,     # hFile",
            "        ctypes.wintypes.DWORD,      # dwReserved",
            "        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow",
            "        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh",
            "        ctypes.POINTER(OVERLAPPED)  # Overlapped",
            "    ]",
            "    UnlockFileEx.restype = ctypes.wintypes.BOOL",
            "    whole_low = 0xffffffff",
            "    whole_high = 0x7fffffff",
            "",
            "    def _lock_file(f, exclusive, block):",
            "        overlapped = OVERLAPPED()",
            "        overlapped.Offset = 0",
            "        overlapped.OffsetHigh = 0",
            "        overlapped.hEvent = 0",
            "        f._lock_file_overlapped_p = ctypes.pointer(overlapped)",
            "",
            "        if not LockFileEx(msvcrt.get_osfhandle(f.fileno()),",
            "                          (0x2 if exclusive else 0x0) | (0x0 if block else 0x1),",
            "                          0, whole_low, whole_high, f._lock_file_overlapped_p):",
            "            # NB: No argument form of \"ctypes.FormatError\" does not work on PyPy",
            "            raise BlockingIOError(f'Locking file failed: {ctypes.FormatError(ctypes.GetLastError())!r}')",
            "",
            "    def _unlock_file(f):",
            "        assert f._lock_file_overlapped_p",
            "        handle = msvcrt.get_osfhandle(f.fileno())",
            "        if not UnlockFileEx(handle, 0, whole_low, whole_high, f._lock_file_overlapped_p):",
            "            raise OSError('Unlocking file failed: %r' % ctypes.FormatError())",
            "",
            "else:",
            "    try:",
            "        import fcntl",
            "",
            "        def _lock_file(f, exclusive, block):",
            "            flags = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH",
            "            if not block:",
            "                flags |= fcntl.LOCK_NB",
            "            try:",
            "                fcntl.flock(f, flags)",
            "            except BlockingIOError:",
            "                raise",
            "            except OSError:  # AOSP does not have flock()",
            "                fcntl.lockf(f, flags)",
            "",
            "        def _unlock_file(f):",
            "            with contextlib.suppress(OSError):",
            "                return fcntl.flock(f, fcntl.LOCK_UN)",
            "            with contextlib.suppress(OSError):",
            "                return fcntl.lockf(f, fcntl.LOCK_UN)  # AOSP does not have flock()",
            "            return fcntl.flock(f, fcntl.LOCK_UN | fcntl.LOCK_NB)  # virtiofs needs LOCK_NB on unlocking",
            "",
            "    except ImportError:",
            "",
            "        def _lock_file(f, exclusive, block):",
            "            raise LockingUnsupportedError()",
            "",
            "        def _unlock_file(f):",
            "            raise LockingUnsupportedError()",
            "",
            "",
            "class locked_file:",
            "    locked = False",
            "",
            "    def __init__(self, filename, mode, block=True, encoding=None):",
            "        if mode not in {'r', 'rb', 'a', 'ab', 'w', 'wb'}:",
            "            raise NotImplementedError(mode)",
            "        self.mode, self.block = mode, block",
            "",
            "        writable = any(f in mode for f in 'wax+')",
            "        readable = any(f in mode for f in 'r+')",
            "        flags = functools.reduce(operator.ior, (",
            "            getattr(os, 'O_CLOEXEC', 0),  # UNIX only",
            "            getattr(os, 'O_BINARY', 0),  # Windows only",
            "            getattr(os, 'O_NOINHERIT', 0),  # Windows only",
            "            os.O_CREAT if writable else 0,  # O_TRUNC only after locking",
            "            os.O_APPEND if 'a' in mode else 0,",
            "            os.O_EXCL if 'x' in mode else 0,",
            "            os.O_RDONLY if not writable else os.O_RDWR if readable else os.O_WRONLY,",
            "        ))",
            "",
            "        self.f = os.fdopen(os.open(filename, flags, 0o666), mode, encoding=encoding)",
            "",
            "    def __enter__(self):",
            "        exclusive = 'r' not in self.mode",
            "        try:",
            "            _lock_file(self.f, exclusive, self.block)",
            "            self.locked = True",
            "        except OSError:",
            "            self.f.close()",
            "            raise",
            "        if 'w' in self.mode:",
            "            try:",
            "                self.f.truncate()",
            "            except OSError as e:",
            "                if e.errno not in (",
            "                    errno.ESPIPE,  # Illegal seek - expected for FIFO",
            "                    errno.EINVAL,  # Invalid argument - expected for /dev/null",
            "                ):",
            "                    raise",
            "        return self",
            "",
            "    def unlock(self):",
            "        if not self.locked:",
            "            return",
            "        try:",
            "            _unlock_file(self.f)",
            "        finally:",
            "            self.locked = False",
            "",
            "    def __exit__(self, *_):",
            "        try:",
            "            self.unlock()",
            "        finally:",
            "            self.f.close()",
            "",
            "    open = __enter__",
            "    close = __exit__",
            "",
            "    def __getattr__(self, attr):",
            "        return getattr(self.f, attr)",
            "",
            "    def __iter__(self):",
            "        return iter(self.f)",
            "",
            "",
            "@functools.cache",
            "def get_filesystem_encoding():",
            "    encoding = sys.getfilesystemencoding()",
            "    return encoding if encoding is not None else 'utf-8'",
            "",
            "",
            "_WINDOWS_QUOTE_TRANS = str.maketrans({'\"': '\\\\\"', '\\\\': '\\\\\\\\'})",
            "_CMD_QUOTE_TRANS = str.maketrans({",
            "    # Keep quotes balanced by replacing them with `\"\"` instead of `\\\\\"`",
            "    '\"': '\"\"',",
            "    # Requires a variable `=` containing `\"^\\n\\n\"` (set in `utils.Popen`)",
            "    # `=` should be unique since variables containing `=` cannot be set using cmd",
            "    '\\n': '%=%',",
            "    # While we are only required to escape backslashes immediately before quotes,",
            "    # we instead escape all of 'em anyways to be consistent",
            "    '\\\\': '\\\\\\\\',",
            "    # Use zero length variable replacement so `%` doesn't get expanded",
            "    # `cd` is always set as long as extensions are enabled (`/E:ON` in `utils.Popen`)",
            "    '%': '%%cd:~,%',",
            "})",
            "",
            "",
            "def shell_quote(args, *, shell=False):",
            "    args = list(variadic(args))",
            "    if any(isinstance(item, bytes) for item in args):",
            "        deprecation_warning('Passing bytes to utils.shell_quote is deprecated')",
            "        encoding = get_filesystem_encoding()",
            "        for index, item in enumerate(args):",
            "            if isinstance(item, bytes):",
            "                args[index] = item.decode(encoding)",
            "",
            "    if compat_os_name != 'nt':",
            "        return shlex.join(args)",
            "",
            "    trans = _CMD_QUOTE_TRANS if shell else _WINDOWS_QUOTE_TRANS",
            "    return ' '.join(",
            "        s if re.fullmatch(r'[\\w#$*\\-+./:?@\\\\]+', s, re.ASCII) else s.translate(trans).join('\"\"')",
            "        for s in args)",
            "",
            "",
            "def smuggle_url(url, data):",
            "    \"\"\" Pass additional data in a URL for internal use. \"\"\"",
            "",
            "    url, idata = unsmuggle_url(url, {})",
            "    data.update(idata)",
            "    sdata = urllib.parse.urlencode(",
            "        {'__youtubedl_smuggle': json.dumps(data)})",
            "    return url + '#' + sdata",
            "",
            "",
            "def unsmuggle_url(smug_url, default=None):",
            "    if '#__youtubedl_smuggle' not in smug_url:",
            "        return smug_url, default",
            "    url, _, sdata = smug_url.rpartition('#')",
            "    jsond = urllib.parse.parse_qs(sdata)['__youtubedl_smuggle'][0]",
            "    data = json.loads(jsond)",
            "    return url, data",
            "",
            "",
            "def format_decimal_suffix(num, fmt='%d%s', *, factor=1000):",
            "    \"\"\" Formats numbers with decimal sufixes like K, M, etc \"\"\"",
            "    num, factor = float_or_none(num), float(factor)",
            "    if num is None or num < 0:",
            "        return None",
            "    POSSIBLE_SUFFIXES = 'kMGTPEZY'",
            "    exponent = 0 if num == 0 else min(int(math.log(num, factor)), len(POSSIBLE_SUFFIXES))",
            "    suffix = ['', *POSSIBLE_SUFFIXES][exponent]",
            "    if factor == 1024:",
            "        suffix = {'k': 'Ki', '': ''}.get(suffix, f'{suffix}i')",
            "    converted = num / (factor ** exponent)",
            "    return fmt % (converted, suffix)",
            "",
            "",
            "def format_bytes(bytes):",
            "    return format_decimal_suffix(bytes, '%.2f%sB', factor=1024) or 'N/A'",
            "",
            "",
            "def lookup_unit_table(unit_table, s, strict=False):",
            "    num_re = NUMBER_RE if strict else NUMBER_RE.replace(R'\\.', '[,.]')",
            "    units_re = '|'.join(re.escape(u) for u in unit_table)",
            "    m = (re.fullmatch if strict else re.match)(",
            "        rf'(?P<num>{num_re})\\s*(?P<unit>{units_re})\\b', s)",
            "    if not m:",
            "        return None",
            "",
            "    num = float(m.group('num').replace(',', '.'))",
            "    mult = unit_table[m.group('unit')]",
            "    return round(num * mult)",
            "",
            "",
            "def parse_bytes(s):",
            "    \"\"\"Parse a string indicating a byte quantity into an integer\"\"\"",
            "    return lookup_unit_table(",
            "        {u: 1024**i for i, u in enumerate(['', *'KMGTPEZY'])},",
            "        s.upper(), strict=True)",
            "",
            "",
            "def parse_filesize(s):",
            "    if s is None:",
            "        return None",
            "",
            "    # The lower-case forms are of course incorrect and unofficial,",
            "    # but we support those too",
            "    _UNIT_TABLE = {",
            "        'B': 1,",
            "        'b': 1,",
            "        'bytes': 1,",
            "        'KiB': 1024,",
            "        'KB': 1000,",
            "        'kB': 1024,",
            "        'Kb': 1000,",
            "        'kb': 1000,",
            "        'kilobytes': 1000,",
            "        'kibibytes': 1024,",
            "        'MiB': 1024 ** 2,",
            "        'MB': 1000 ** 2,",
            "        'mB': 1024 ** 2,",
            "        'Mb': 1000 ** 2,",
            "        'mb': 1000 ** 2,",
            "        'megabytes': 1000 ** 2,",
            "        'mebibytes': 1024 ** 2,",
            "        'GiB': 1024 ** 3,",
            "        'GB': 1000 ** 3,",
            "        'gB': 1024 ** 3,",
            "        'Gb': 1000 ** 3,",
            "        'gb': 1000 ** 3,",
            "        'gigabytes': 1000 ** 3,",
            "        'gibibytes': 1024 ** 3,",
            "        'TiB': 1024 ** 4,",
            "        'TB': 1000 ** 4,",
            "        'tB': 1024 ** 4,",
            "        'Tb': 1000 ** 4,",
            "        'tb': 1000 ** 4,",
            "        'terabytes': 1000 ** 4,",
            "        'tebibytes': 1024 ** 4,",
            "        'PiB': 1024 ** 5,",
            "        'PB': 1000 ** 5,",
            "        'pB': 1024 ** 5,",
            "        'Pb': 1000 ** 5,",
            "        'pb': 1000 ** 5,",
            "        'petabytes': 1000 ** 5,",
            "        'pebibytes': 1024 ** 5,",
            "        'EiB': 1024 ** 6,",
            "        'EB': 1000 ** 6,",
            "        'eB': 1024 ** 6,",
            "        'Eb': 1000 ** 6,",
            "        'eb': 1000 ** 6,",
            "        'exabytes': 1000 ** 6,",
            "        'exbibytes': 1024 ** 6,",
            "        'ZiB': 1024 ** 7,",
            "        'ZB': 1000 ** 7,",
            "        'zB': 1024 ** 7,",
            "        'Zb': 1000 ** 7,",
            "        'zb': 1000 ** 7,",
            "        'zettabytes': 1000 ** 7,",
            "        'zebibytes': 1024 ** 7,",
            "        'YiB': 1024 ** 8,",
            "        'YB': 1000 ** 8,",
            "        'yB': 1024 ** 8,",
            "        'Yb': 1000 ** 8,",
            "        'yb': 1000 ** 8,",
            "        'yottabytes': 1000 ** 8,",
            "        'yobibytes': 1024 ** 8,",
            "    }",
            "",
            "    return lookup_unit_table(_UNIT_TABLE, s)",
            "",
            "",
            "def parse_count(s):",
            "    if s is None:",
            "        return None",
            "",
            "    s = re.sub(r'^[^\\d]+\\s', '', s).strip()",
            "",
            "    if re.match(r'^[\\d,.]+$', s):",
            "        return str_to_int(s)",
            "",
            "    _UNIT_TABLE = {",
            "        'k': 1000,",
            "        'K': 1000,",
            "        'm': 1000 ** 2,",
            "        'M': 1000 ** 2,",
            "        'kk': 1000 ** 2,",
            "        'KK': 1000 ** 2,",
            "        'b': 1000 ** 3,",
            "        'B': 1000 ** 3,",
            "    }",
            "",
            "    ret = lookup_unit_table(_UNIT_TABLE, s)",
            "    if ret is not None:",
            "        return ret",
            "",
            "    mobj = re.match(r'([\\d,.]+)(?:$|\\s)', s)",
            "    if mobj:",
            "        return str_to_int(mobj.group(1))",
            "",
            "",
            "def parse_resolution(s, *, lenient=False):",
            "    if s is None:",
            "        return {}",
            "",
            "    if lenient:",
            "        mobj = re.search(r'(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)', s)",
            "    else:",
            "        mobj = re.search(r'(?<![a-zA-Z0-9])(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)(?![a-zA-Z0-9])', s)",
            "    if mobj:",
            "        return {",
            "            'width': int(mobj.group('w')),",
            "            'height': int(mobj.group('h')),",
            "        }",
            "",
            "    mobj = re.search(r'(?<![a-zA-Z0-9])(\\d+)[pPiI](?![a-zA-Z0-9])', s)",
            "    if mobj:",
            "        return {'height': int(mobj.group(1))}",
            "",
            "    mobj = re.search(r'\\b([48])[kK]\\b', s)",
            "    if mobj:",
            "        return {'height': int(mobj.group(1)) * 540}",
            "",
            "    return {}",
            "",
            "",
            "def parse_bitrate(s):",
            "    if not isinstance(s, str):",
            "        return",
            "    mobj = re.search(r'\\b(\\d+)\\s*kbps', s)",
            "    if mobj:",
            "        return int(mobj.group(1))",
            "",
            "",
            "def month_by_name(name, lang='en'):",
            "    \"\"\" Return the number of a month by (locale-independently) English name \"\"\"",
            "",
            "    month_names = MONTH_NAMES.get(lang, MONTH_NAMES['en'])",
            "",
            "    try:",
            "        return month_names.index(name) + 1",
            "    except ValueError:",
            "        return None",
            "",
            "",
            "def month_by_abbreviation(abbrev):",
            "    \"\"\" Return the number of a month by (locale-independently) English",
            "        abbreviations \"\"\"",
            "",
            "    try:",
            "        return [s[:3] for s in ENGLISH_MONTH_NAMES].index(abbrev) + 1",
            "    except ValueError:",
            "        return None",
            "",
            "",
            "def fix_xml_ampersands(xml_str):",
            "    \"\"\"Replace all the '&' by '&amp;' in XML\"\"\"",
            "    return re.sub(",
            "        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',",
            "        '&amp;',",
            "        xml_str)",
            "",
            "",
            "def setproctitle(title):",
            "    assert isinstance(title, str)",
            "",
            "    # Workaround for https://github.com/yt-dlp/yt-dlp/issues/4541",
            "    try:",
            "        import ctypes",
            "    except ImportError:",
            "        return",
            "",
            "    try:",
            "        libc = ctypes.cdll.LoadLibrary('libc.so.6')",
            "    except OSError:",
            "        return",
            "    except TypeError:",
            "        # LoadLibrary in Windows Python 2.7.13 only expects",
            "        # a bytestring, but since unicode_literals turns",
            "        # every string into a unicode string, it fails.",
            "        return",
            "    title_bytes = title.encode()",
            "    buf = ctypes.create_string_buffer(len(title_bytes))",
            "    buf.value = title_bytes",
            "    try:",
            "        # PR_SET_NAME = 15      Ref: /usr/include/linux/prctl.h",
            "        libc.prctl(15, buf, 0, 0, 0)",
            "    except AttributeError:",
            "        return  # Strange libc, just skip this",
            "",
            "",
            "def remove_start(s, start):",
            "    return s[len(start):] if s is not None and s.startswith(start) else s",
            "",
            "",
            "def remove_end(s, end):",
            "    return s[:-len(end)] if s is not None and s.endswith(end) else s",
            "",
            "",
            "def remove_quotes(s):",
            "    if s is None or len(s) < 2:",
            "        return s",
            "    for quote in ('\"', \"'\", ):",
            "        if s[0] == quote and s[-1] == quote:",
            "            return s[1:-1]",
            "    return s",
            "",
            "",
            "def get_domain(url):",
            "    \"\"\"",
            "    This implementation is inconsistent, but is kept for compatibility.",
            "    Use this only for \"webpage_url_domain\"",
            "    \"\"\"",
            "    return remove_start(urllib.parse.urlparse(url).netloc, 'www.') or None",
            "",
            "",
            "def url_basename(url):",
            "    path = urllib.parse.urlparse(url).path",
            "    return path.strip('/').split('/')[-1]",
            "",
            "",
            "def base_url(url):",
            "    return re.match(r'https?://[^?#]+/', url).group()",
            "",
            "",
            "def urljoin(base, path):",
            "    if isinstance(path, bytes):",
            "        path = path.decode()",
            "    if not isinstance(path, str) or not path:",
            "        return None",
            "    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):",
            "        return path",
            "    if isinstance(base, bytes):",
            "        base = base.decode()",
            "    if not isinstance(base, str) or not re.match(",
            "            r'^(?:https?:)?//', base):",
            "        return None",
            "    return urllib.parse.urljoin(base, path)",
            "",
            "",
            "def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):",
            "    if get_attr and v is not None:",
            "        v = getattr(v, get_attr, None)",
            "    try:",
            "        return int(v) * invscale // scale",
            "    except (ValueError, TypeError, OverflowError):",
            "        return default",
            "",
            "",
            "def str_or_none(v, default=None):",
            "    return default if v is None else str(v)",
            "",
            "",
            "def str_to_int(int_str):",
            "    \"\"\" A more relaxed version of int_or_none \"\"\"",
            "    if isinstance(int_str, int):",
            "        return int_str",
            "    elif isinstance(int_str, str):",
            "        int_str = re.sub(r'[,\\.\\+]', '', int_str)",
            "        return int_or_none(int_str)",
            "",
            "",
            "def float_or_none(v, scale=1, invscale=1, default=None):",
            "    if v is None:",
            "        return default",
            "    try:",
            "        return float(v) * invscale / scale",
            "    except (ValueError, TypeError):",
            "        return default",
            "",
            "",
            "def bool_or_none(v, default=None):",
            "    return v if isinstance(v, bool) else default",
            "",
            "",
            "def strip_or_none(v, default=None):",
            "    return v.strip() if isinstance(v, str) else default",
            "",
            "",
            "def url_or_none(url):",
            "    if not url or not isinstance(url, str):",
            "        return None",
            "    url = url.strip()",
            "    return url if re.match(r'^(?:(?:https?|rt(?:m(?:pt?[es]?|fp)|sp[su]?)|mms|ftps?):)?//', url) else None",
            "",
            "",
            "def strftime_or_none(timestamp, date_format='%Y%m%d', default=None):",
            "    datetime_object = None",
            "    try:",
            "        if isinstance(timestamp, (int, float)):  # unix timestamp",
            "            # Using naive datetime here can break timestamp() in Windows",
            "            # Ref: https://github.com/yt-dlp/yt-dlp/issues/5185, https://github.com/python/cpython/issues/94414",
            "            # Also, dt.datetime.fromtimestamp breaks for negative timestamps",
            "            # Ref: https://github.com/yt-dlp/yt-dlp/issues/6706#issuecomment-1496842642",
            "            datetime_object = (dt.datetime.fromtimestamp(0, dt.timezone.utc)",
            "                               + dt.timedelta(seconds=timestamp))",
            "        elif isinstance(timestamp, str):  # assume YYYYMMDD",
            "            datetime_object = dt.datetime.strptime(timestamp, '%Y%m%d')",
            "        date_format = re.sub(  # Support %s on windows",
            "            r'(?<!%)(%%)*%s', rf'\\g<1>{int(datetime_object.timestamp())}', date_format)",
            "        return datetime_object.strftime(date_format)",
            "    except (ValueError, TypeError, AttributeError):",
            "        return default",
            "",
            "",
            "def parse_duration(s):",
            "    if not isinstance(s, str):",
            "        return None",
            "    s = s.strip()",
            "    if not s:",
            "        return None",
            "",
            "    days, hours, mins, secs, ms = [None] * 5",
            "    m = re.match(r'''(?x)",
            "            (?P<before_secs>",
            "                (?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?",
            "            (?P<secs>(?(before_secs)[0-9]{1,2}|[0-9]+))",
            "            (?P<ms>[.:][0-9]+)?Z?$",
            "        ''', s)",
            "    if m:",
            "        days, hours, mins, secs, ms = m.group('days', 'hours', 'mins', 'secs', 'ms')",
            "    else:",
            "        m = re.match(",
            "            r'''(?ix)(?:P?",
            "                (?:",
            "                    [0-9]+\\s*y(?:ears?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    [0-9]+\\s*m(?:onths?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    [0-9]+\\s*w(?:eeks?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    (?P<days>[0-9]+)\\s*d(?:ays?)?,?\\s*",
            "                )?",
            "                T)?",
            "                (?:",
            "                    (?P<hours>[0-9]+)\\s*h(?:(?:ou)?rs?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    (?P<mins>[0-9]+)\\s*m(?:in(?:ute)?s?)?,?\\s*",
            "                )?",
            "                (?:",
            "                    (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*s(?:ec(?:ond)?s?)?\\s*",
            "                )?Z?$''', s)",
            "        if m:",
            "            days, hours, mins, secs, ms = m.groups()",
            "        else:",
            "            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\\s*(?:hours?)|(?P<mins>[0-9.]+)\\s*(?:mins?\\.?|minutes?)\\s*)Z?$', s)",
            "            if m:",
            "                hours, mins = m.groups()",
            "            else:",
            "                return None",
            "",
            "    if ms:",
            "        ms = ms.replace(':', '.')",
            "    return sum(float(part or 0) * mult for part, mult in (",
            "        (days, 86400), (hours, 3600), (mins, 60), (secs, 1), (ms, 1)))",
            "",
            "",
            "def prepend_extension(filename, ext, expected_real_ext=None):",
            "    name, real_ext = os.path.splitext(filename)",
            "    return (",
            "        f'{name}.{ext}{real_ext}'",
            "        if not expected_real_ext or real_ext[1:] == expected_real_ext",
            "        else f'{filename}.{ext}')",
            "",
            "",
            "def replace_extension(filename, ext, expected_real_ext=None):",
            "    name, real_ext = os.path.splitext(filename)",
            "    return '{}.{}'.format(",
            "        name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename,",
            "        ext)",
            "",
            "",
            "def check_executable(exe, args=[]):",
            "    \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.",
            "    args can be a list of arguments for a short output (like -version) \"\"\"",
            "    try:",
            "        Popen.run([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)",
            "    except OSError:",
            "        return False",
            "    return exe",
            "",
            "",
            "def _get_exe_version_output(exe, args):",
            "    try:",
            "        # STDIN should be redirected too. On UNIX-like systems, ffmpeg triggers",
            "        # SIGTTOU if yt-dlp is run in the background.",
            "        # See https://github.com/ytdl-org/youtube-dl/issues/955#issuecomment-209789656",
            "        stdout, _, ret = Popen.run([encodeArgument(exe)] + args, text=True,",
            "                                   stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)",
            "        if ret:",
            "            return None",
            "    except OSError:",
            "        return False",
            "    return stdout",
            "",
            "",
            "def detect_exe_version(output, version_re=None, unrecognized='present'):",
            "    assert isinstance(output, str)",
            "    if version_re is None:",
            "        version_re = r'version\\s+([-0-9._a-zA-Z]+)'",
            "    m = re.search(version_re, output)",
            "    if m:",
            "        return m.group(1)",
            "    else:",
            "        return unrecognized",
            "",
            "",
            "def get_exe_version(exe, args=['--version'],",
            "                    version_re=None, unrecognized=('present', 'broken')):",
            "    \"\"\" Returns the version of the specified executable,",
            "    or False if the executable is not present \"\"\"",
            "    unrecognized = variadic(unrecognized)",
            "    assert len(unrecognized) in (1, 2)",
            "    out = _get_exe_version_output(exe, args)",
            "    if out is None:",
            "        return unrecognized[-1]",
            "    return out and detect_exe_version(out, version_re, unrecognized[0])",
            "",
            "",
            "def frange(start=0, stop=None, step=1):",
            "    \"\"\"Float range\"\"\"",
            "    if stop is None:",
            "        start, stop = 0, start",
            "    sign = [-1, 1][step > 0] if step else 0",
            "    while sign * start < sign * stop:",
            "        yield start",
            "        start += step",
            "",
            "",
            "class LazyList(collections.abc.Sequence):",
            "    \"\"\"Lazy immutable list from an iterable",
            "    Note that slices of a LazyList are lists and not LazyList\"\"\"",
            "",
            "    class IndexError(IndexError):",
            "        pass",
            "",
            "    def __init__(self, iterable, *, reverse=False, _cache=None):",
            "        self._iterable = iter(iterable)",
            "        self._cache = [] if _cache is None else _cache",
            "        self._reversed = reverse",
            "",
            "    def __iter__(self):",
            "        if self._reversed:",
            "            # We need to consume the entire iterable to iterate in reverse",
            "            yield from self.exhaust()",
            "            return",
            "        yield from self._cache",
            "        for item in self._iterable:",
            "            self._cache.append(item)",
            "            yield item",
            "",
            "    def _exhaust(self):",
            "        self._cache.extend(self._iterable)",
            "        self._iterable = []  # Discard the emptied iterable to make it pickle-able",
            "        return self._cache",
            "",
            "    def exhaust(self):",
            "        \"\"\"Evaluate the entire iterable\"\"\"",
            "        return self._exhaust()[::-1 if self._reversed else 1]",
            "",
            "    @staticmethod",
            "    def _reverse_index(x):",
            "        return None if x is None else ~x",
            "",
            "    def __getitem__(self, idx):",
            "        if isinstance(idx, slice):",
            "            if self._reversed:",
            "                idx = slice(self._reverse_index(idx.start), self._reverse_index(idx.stop), -(idx.step or 1))",
            "            start, stop, step = idx.start, idx.stop, idx.step or 1",
            "        elif isinstance(idx, int):",
            "            if self._reversed:",
            "                idx = self._reverse_index(idx)",
            "            start, stop, step = idx, idx, 0",
            "        else:",
            "            raise TypeError('indices must be integers or slices')",
            "        if ((start or 0) < 0 or (stop or 0) < 0",
            "                or (start is None and step < 0)",
            "                or (stop is None and step > 0)):",
            "            # We need to consume the entire iterable to be able to slice from the end",
            "            # Obviously, never use this with infinite iterables",
            "            self._exhaust()",
            "            try:",
            "                return self._cache[idx]",
            "            except IndexError as e:",
            "                raise self.IndexError(e) from e",
            "        n = max(start or 0, stop or 0) - len(self._cache) + 1",
            "        if n > 0:",
            "            self._cache.extend(itertools.islice(self._iterable, n))",
            "        try:",
            "            return self._cache[idx]",
            "        except IndexError as e:",
            "            raise self.IndexError(e) from e",
            "",
            "    def __bool__(self):",
            "        try:",
            "            self[-1] if self._reversed else self[0]",
            "        except self.IndexError:",
            "            return False",
            "        return True",
            "",
            "    def __len__(self):",
            "        self._exhaust()",
            "        return len(self._cache)",
            "",
            "    def __reversed__(self):",
            "        return type(self)(self._iterable, reverse=not self._reversed, _cache=self._cache)",
            "",
            "    def __copy__(self):",
            "        return type(self)(self._iterable, reverse=self._reversed, _cache=self._cache)",
            "",
            "    def __repr__(self):",
            "        # repr and str should mimic a list. So we exhaust the iterable",
            "        return repr(self.exhaust())",
            "",
            "    def __str__(self):",
            "        return repr(self.exhaust())",
            "",
            "",
            "class PagedList:",
            "",
            "    class IndexError(IndexError):",
            "        pass",
            "",
            "    def __len__(self):",
            "        # This is only useful for tests",
            "        return len(self.getslice())",
            "",
            "    def __init__(self, pagefunc, pagesize, use_cache=True):",
            "        self._pagefunc = pagefunc",
            "        self._pagesize = pagesize",
            "        self._pagecount = float('inf')",
            "        self._use_cache = use_cache",
            "        self._cache = {}",
            "",
            "    def getpage(self, pagenum):",
            "        page_results = self._cache.get(pagenum)",
            "        if page_results is None:",
            "            page_results = [] if pagenum > self._pagecount else list(self._pagefunc(pagenum))",
            "        if self._use_cache:",
            "            self._cache[pagenum] = page_results",
            "        return page_results",
            "",
            "    def getslice(self, start=0, end=None):",
            "        return list(self._getslice(start, end))",
            "",
            "    def _getslice(self, start, end):",
            "        raise NotImplementedError('This method must be implemented by subclasses')",
            "",
            "    def __getitem__(self, idx):",
            "        assert self._use_cache, 'Indexing PagedList requires cache'",
            "        if not isinstance(idx, int) or idx < 0:",
            "            raise TypeError('indices must be non-negative integers')",
            "        entries = self.getslice(idx, idx + 1)",
            "        if not entries:",
            "            raise self.IndexError()",
            "        return entries[0]",
            "",
            "    def __bool__(self):",
            "        return bool(self.getslice(0, 1))",
            "",
            "",
            "class OnDemandPagedList(PagedList):",
            "    \"\"\"Download pages until a page with less than maximum results\"\"\"",
            "",
            "    def _getslice(self, start, end):",
            "        for pagenum in itertools.count(start // self._pagesize):",
            "            firstid = pagenum * self._pagesize",
            "            nextfirstid = pagenum * self._pagesize + self._pagesize",
            "            if start >= nextfirstid:",
            "                continue",
            "",
            "            startv = (",
            "                start % self._pagesize",
            "                if firstid <= start < nextfirstid",
            "                else 0)",
            "            endv = (",
            "                ((end - 1) % self._pagesize) + 1",
            "                if (end is not None and firstid <= end <= nextfirstid)",
            "                else None)",
            "",
            "            try:",
            "                page_results = self.getpage(pagenum)",
            "            except Exception:",
            "                self._pagecount = pagenum - 1",
            "                raise",
            "            if startv != 0 or endv is not None:",
            "                page_results = page_results[startv:endv]",
            "            yield from page_results",
            "",
            "            # A little optimization - if current page is not \"full\", ie. does",
            "            # not contain page_size videos then we can assume that this page",
            "            # is the last one - there are no more ids on further pages -",
            "            # i.e. no need to query again.",
            "            if len(page_results) + startv < self._pagesize:",
            "                break",
            "",
            "            # If we got the whole page, but the next page is not interesting,",
            "            # break out early as well",
            "            if end == nextfirstid:",
            "                break",
            "",
            "",
            "class InAdvancePagedList(PagedList):",
            "    \"\"\"PagedList with total number of pages known in advance\"\"\"",
            "",
            "    def __init__(self, pagefunc, pagecount, pagesize):",
            "        PagedList.__init__(self, pagefunc, pagesize, True)",
            "        self._pagecount = pagecount",
            "",
            "    def _getslice(self, start, end):",
            "        start_page = start // self._pagesize",
            "        end_page = self._pagecount if end is None else min(self._pagecount, end // self._pagesize + 1)",
            "        skip_elems = start - start_page * self._pagesize",
            "        only_more = None if end is None else end - start",
            "        for pagenum in range(start_page, end_page):",
            "            page_results = self.getpage(pagenum)",
            "            if skip_elems:",
            "                page_results = page_results[skip_elems:]",
            "                skip_elems = None",
            "            if only_more is not None:",
            "                if len(page_results) < only_more:",
            "                    only_more -= len(page_results)",
            "                else:",
            "                    yield from page_results[:only_more]",
            "                    break",
            "            yield from page_results",
            "",
            "",
            "class PlaylistEntries:",
            "    MissingEntry = object()",
            "    is_exhausted = False",
            "",
            "    def __init__(self, ydl, info_dict):",
            "        self.ydl = ydl",
            "",
            "        # _entries must be assigned now since infodict can change during iteration",
            "        entries = info_dict.get('entries')",
            "        if entries is None:",
            "            raise EntryNotInPlaylist('There are no entries')",
            "        elif isinstance(entries, list):",
            "            self.is_exhausted = True",
            "",
            "        requested_entries = info_dict.get('requested_entries')",
            "        self.is_incomplete = requested_entries is not None",
            "        if self.is_incomplete:",
            "            assert self.is_exhausted",
            "            self._entries = [self.MissingEntry] * max(requested_entries or [0])",
            "            for i, entry in zip(requested_entries, entries):",
            "                self._entries[i - 1] = entry",
            "        elif isinstance(entries, (list, PagedList, LazyList)):",
            "            self._entries = entries",
            "        else:",
            "            self._entries = LazyList(entries)",
            "",
            "    PLAYLIST_ITEMS_RE = re.compile(r'''(?x)",
            "        (?P<start>[+-]?\\d+)?",
            "        (?P<range>[:-]",
            "            (?P<end>[+-]?\\d+|inf(?:inite)?)?",
            "            (?::(?P<step>[+-]?\\d+))?",
            "        )?''')",
            "",
            "    @classmethod",
            "    def parse_playlist_items(cls, string):",
            "        for segment in string.split(','):",
            "            if not segment:",
            "                raise ValueError('There is two or more consecutive commas')",
            "            mobj = cls.PLAYLIST_ITEMS_RE.fullmatch(segment)",
            "            if not mobj:",
            "                raise ValueError(f'{segment!r} is not a valid specification')",
            "            start, end, step, has_range = mobj.group('start', 'end', 'step', 'range')",
            "            if int_or_none(step) == 0:",
            "                raise ValueError(f'Step in {segment!r} cannot be zero')",
            "            yield slice(int_or_none(start), float_or_none(end), int_or_none(step)) if has_range else int(start)",
            "",
            "    def get_requested_items(self):",
            "        playlist_items = self.ydl.params.get('playlist_items')",
            "        playlist_start = self.ydl.params.get('playliststart', 1)",
            "        playlist_end = self.ydl.params.get('playlistend')",
            "        # For backwards compatibility, interpret -1 as whole list",
            "        if playlist_end in (-1, None):",
            "            playlist_end = ''",
            "        if not playlist_items:",
            "            playlist_items = f'{playlist_start}:{playlist_end}'",
            "        elif playlist_start != 1 or playlist_end:",
            "            self.ydl.report_warning('Ignoring playliststart and playlistend because playlistitems was given', only_once=True)",
            "",
            "        for index in self.parse_playlist_items(playlist_items):",
            "            for i, entry in self[index]:",
            "                yield i, entry",
            "                if not entry:",
            "                    continue",
            "                try:",
            "                    # The item may have just been added to archive. Don't break due to it",
            "                    if not self.ydl.params.get('lazy_playlist'):",
            "                        # TODO: Add auto-generated fields",
            "                        self.ydl._match_entry(entry, incomplete=True, silent=True)",
            "                except (ExistingVideoReached, RejectedVideoReached):",
            "                    return",
            "",
            "    def get_full_count(self):",
            "        if self.is_exhausted and not self.is_incomplete:",
            "            return len(self)",
            "        elif isinstance(self._entries, InAdvancePagedList):",
            "            if self._entries._pagesize == 1:",
            "                return self._entries._pagecount",
            "",
            "    @functools.cached_property",
            "    def _getter(self):",
            "        if isinstance(self._entries, list):",
            "            def get_entry(i):",
            "                try:",
            "                    entry = self._entries[i]",
            "                except IndexError:",
            "                    entry = self.MissingEntry",
            "                    if not self.is_incomplete:",
            "                        raise self.IndexError()",
            "                if entry is self.MissingEntry:",
            "                    raise EntryNotInPlaylist(f'Entry {i + 1} cannot be found')",
            "                return entry",
            "        else:",
            "            def get_entry(i):",
            "                try:",
            "                    return type(self.ydl)._handle_extraction_exceptions(lambda _, i: self._entries[i])(self.ydl, i)",
            "                except (LazyList.IndexError, PagedList.IndexError):",
            "                    raise self.IndexError()",
            "        return get_entry",
            "",
            "    def __getitem__(self, idx):",
            "        if isinstance(idx, int):",
            "            idx = slice(idx, idx)",
            "",
            "        # NB: PlaylistEntries[1:10] => (0, 1, ... 9)",
            "        step = 1 if idx.step is None else idx.step",
            "        if idx.start is None:",
            "            start = 0 if step > 0 else len(self) - 1",
            "        else:",
            "            start = idx.start - 1 if idx.start >= 0 else len(self) + idx.start",
            "",
            "        # NB: Do not call len(self) when idx == [:]",
            "        if idx.stop is None:",
            "            stop = 0 if step < 0 else float('inf')",
            "        else:",
            "            stop = idx.stop - 1 if idx.stop >= 0 else len(self) + idx.stop",
            "        stop += [-1, 1][step > 0]",
            "",
            "        for i in frange(start, stop, step):",
            "            if i < 0:",
            "                continue",
            "            try:",
            "                entry = self._getter(i)",
            "            except self.IndexError:",
            "                self.is_exhausted = True",
            "                if step > 0:",
            "                    break",
            "                continue",
            "            yield i + 1, entry",
            "",
            "    def __len__(self):",
            "        return len(tuple(self[:]))",
            "",
            "    class IndexError(IndexError):",
            "        pass",
            "",
            "",
            "def uppercase_escape(s):",
            "    unicode_escape = codecs.getdecoder('unicode_escape')",
            "    return re.sub(",
            "        r'\\\\U[0-9a-fA-F]{8}',",
            "        lambda m: unicode_escape(m.group(0))[0],",
            "        s)",
            "",
            "",
            "def lowercase_escape(s):",
            "    unicode_escape = codecs.getdecoder('unicode_escape')",
            "    return re.sub(",
            "        r'\\\\u[0-9a-fA-F]{4}',",
            "        lambda m: unicode_escape(m.group(0))[0],",
            "        s)",
            "",
            "",
            "def parse_qs(url, **kwargs):",
            "    return urllib.parse.parse_qs(urllib.parse.urlparse(url).query, **kwargs)",
            "",
            "",
            "def read_batch_urls(batch_fd):",
            "    def fixup(url):",
            "        if not isinstance(url, str):",
            "            url = url.decode('utf-8', 'replace')",
            "        BOM_UTF8 = ('\\xef\\xbb\\xbf', '\\ufeff')",
            "        for bom in BOM_UTF8:",
            "            if url.startswith(bom):",
            "                url = url[len(bom):]",
            "        url = url.lstrip()",
            "        if not url or url.startswith(('#', ';', ']')):",
            "            return False",
            "        # \"#\" cannot be stripped out since it is part of the URI",
            "        # However, it can be safely stripped out if following a whitespace",
            "        return re.split(r'\\s#', url, 1)[0].rstrip()",
            "",
            "    with contextlib.closing(batch_fd) as fd:",
            "        return [url for url in map(fixup, fd) if url]",
            "",
            "",
            "def urlencode_postdata(*args, **kargs):",
            "    return urllib.parse.urlencode(*args, **kargs).encode('ascii')",
            "",
            "",
            "def update_url(url, *, query_update=None, **kwargs):",
            "    \"\"\"Replace URL components specified by kwargs",
            "       @param url           str or parse url tuple",
            "       @param query_update  update query",
            "       @returns             str",
            "    \"\"\"",
            "    if isinstance(url, str):",
            "        if not kwargs and not query_update:",
            "            return url",
            "        else:",
            "            url = urllib.parse.urlparse(url)",
            "    if query_update:",
            "        assert 'query' not in kwargs, 'query_update and query cannot be specified at the same time'",
            "        kwargs['query'] = urllib.parse.urlencode({",
            "            **urllib.parse.parse_qs(url.query),",
            "            **query_update",
            "        }, True)",
            "    return urllib.parse.urlunparse(url._replace(**kwargs))",
            "",
            "",
            "def update_url_query(url, query):",
            "    return update_url(url, query_update=query)",
            "",
            "",
            "def _multipart_encode_impl(data, boundary):",
            "    content_type = 'multipart/form-data; boundary=%s' % boundary",
            "",
            "    out = b''",
            "    for k, v in data.items():",
            "        out += b'--' + boundary.encode('ascii') + b'\\r\\n'",
            "        if isinstance(k, str):",
            "            k = k.encode()",
            "        if isinstance(v, str):",
            "            v = v.encode()",
            "        # RFC 2047 requires non-ASCII field names to be encoded, while RFC 7578",
            "        # suggests sending UTF-8 directly. Firefox sends UTF-8, too",
            "        content = b'Content-Disposition: form-data; name=\"' + k + b'\"\\r\\n\\r\\n' + v + b'\\r\\n'",
            "        if boundary.encode('ascii') in content:",
            "            raise ValueError('Boundary overlaps with data')",
            "        out += content",
            "",
            "    out += b'--' + boundary.encode('ascii') + b'--\\r\\n'",
            "",
            "    return out, content_type",
            "",
            "",
            "def multipart_encode(data, boundary=None):",
            "    '''",
            "    Encode a dict to RFC 7578-compliant form-data",
            "",
            "    data:",
            "        A dict where keys and values can be either Unicode or bytes-like",
            "        objects.",
            "    boundary:",
            "        If specified a Unicode object, it's used as the boundary. Otherwise",
            "        a random boundary is generated.",
            "",
            "    Reference: https://tools.ietf.org/html/rfc7578",
            "    '''",
            "    has_specified_boundary = boundary is not None",
            "",
            "    while True:",
            "        if boundary is None:",
            "            boundary = '---------------' + str(random.randrange(0x0fffffff, 0xffffffff))",
            "",
            "        try:",
            "            out, content_type = _multipart_encode_impl(data, boundary)",
            "            break",
            "        except ValueError:",
            "            if has_specified_boundary:",
            "                raise",
            "            boundary = None",
            "",
            "    return out, content_type",
            "",
            "",
            "def is_iterable_like(x, allowed_types=collections.abc.Iterable, blocked_types=NO_DEFAULT):",
            "    if blocked_types is NO_DEFAULT:",
            "        blocked_types = (str, bytes, collections.abc.Mapping)",
            "    return isinstance(x, allowed_types) and not isinstance(x, blocked_types)",
            "",
            "",
            "def variadic(x, allowed_types=NO_DEFAULT):",
            "    if not isinstance(allowed_types, (tuple, type)):",
            "        deprecation_warning('allowed_types should be a tuple or a type')",
            "        allowed_types = tuple(allowed_types)",
            "    return x if is_iterable_like(x, blocked_types=allowed_types) else (x, )",
            "",
            "",
            "def try_call(*funcs, expected_type=None, args=[], kwargs={}):",
            "    for f in funcs:",
            "        try:",
            "            val = f(*args, **kwargs)",
            "        except (AttributeError, KeyError, TypeError, IndexError, ValueError, ZeroDivisionError):",
            "            pass",
            "        else:",
            "            if expected_type is None or isinstance(val, expected_type):",
            "                return val",
            "",
            "",
            "def try_get(src, getter, expected_type=None):",
            "    return try_call(*variadic(getter), args=(src,), expected_type=expected_type)",
            "",
            "",
            "def filter_dict(dct, cndn=lambda _, v: v is not None):",
            "    return {k: v for k, v in dct.items() if cndn(k, v)}",
            "",
            "",
            "def merge_dicts(*dicts):",
            "    merged = {}",
            "    for a_dict in dicts:",
            "        for k, v in a_dict.items():",
            "            if (v is not None and k not in merged",
            "                    or isinstance(v, str) and merged[k] == ''):",
            "                merged[k] = v",
            "    return merged",
            "",
            "",
            "def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):",
            "    return string if isinstance(string, str) else str(string, encoding, errors)",
            "",
            "",
            "US_RATINGS = {",
            "    'G': 0,",
            "    'PG': 10,",
            "    'PG-13': 13,",
            "    'R': 16,",
            "    'NC': 18,",
            "}",
            "",
            "",
            "TV_PARENTAL_GUIDELINES = {",
            "    'TV-Y': 0,",
            "    'TV-Y7': 7,",
            "    'TV-G': 0,",
            "    'TV-PG': 0,",
            "    'TV-14': 14,",
            "    'TV-MA': 17,",
            "}",
            "",
            "",
            "def parse_age_limit(s):",
            "    # isinstance(False, int) is True. So type() must be used instead",
            "    if type(s) is int:  # noqa: E721",
            "        return s if 0 <= s <= 21 else None",
            "    elif not isinstance(s, str):",
            "        return None",
            "    m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)",
            "    if m:",
            "        return int(m.group('age'))",
            "    s = s.upper()",
            "    if s in US_RATINGS:",
            "        return US_RATINGS[s]",
            "    m = re.match(r'^TV[_-]?(%s)$' % '|'.join(k[3:] for k in TV_PARENTAL_GUIDELINES), s)",
            "    if m:",
            "        return TV_PARENTAL_GUIDELINES['TV-' + m.group(1)]",
            "    return None",
            "",
            "",
            "def strip_jsonp(code):",
            "    return re.sub(",
            "        r'''(?sx)^",
            "            (?:window\\.)?(?P<func_name>[a-zA-Z0-9_.$]*)",
            "            (?:\\s*&&\\s*(?P=func_name))?",
            "            \\s*\\(\\s*(?P<callback_data>.*)\\);?",
            "            \\s*?(?://[^\\n]*)*$''',",
            "        r'\\g<callback_data>', code)",
            "",
            "",
            "def js_to_json(code, vars={}, *, strict=False):",
            "    # vars is a dict of var, val pairs to substitute",
            "    STRING_QUOTES = '\\'\"`'",
            "    STRING_RE = '|'.join(rf'{q}(?:\\\\.|[^\\\\{q}])*{q}' for q in STRING_QUOTES)",
            "    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'",
            "    SKIP_RE = fr'\\s*(?:{COMMENT_RE})?\\s*'",
            "    INTEGER_TABLE = (",
            "        (fr'(?s)^(0[xX][0-9a-fA-F]+){SKIP_RE}:?$', 16),",
            "        (fr'(?s)^(0+[0-7]+){SKIP_RE}:?$', 8),",
            "    )",
            "",
            "    def process_escape(match):",
            "        JSON_PASSTHROUGH_ESCAPES = R'\"\\bfnrtu'",
            "        escape = match.group(1) or match.group(2)",
            "",
            "        return (Rf'\\{escape}' if escape in JSON_PASSTHROUGH_ESCAPES",
            "                else R'\\u00' if escape == 'x'",
            "                else '' if escape == '\\n'",
            "                else escape)",
            "",
            "    def template_substitute(match):",
            "        evaluated = js_to_json(match.group(1), vars, strict=strict)",
            "        if evaluated[0] == '\"':",
            "            return json.loads(evaluated)",
            "        return evaluated",
            "",
            "    def fix_kv(m):",
            "        v = m.group(0)",
            "        if v in ('true', 'false', 'null'):",
            "            return v",
            "        elif v in ('undefined', 'void 0'):",
            "            return 'null'",
            "        elif v.startswith('/*') or v.startswith('//') or v.startswith('!') or v == ',':",
            "            return ''",
            "",
            "        if v[0] in STRING_QUOTES:",
            "            v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]",
            "            escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)",
            "            return f'\"{escaped}\"'",
            "",
            "        for regex, base in INTEGER_TABLE:",
            "            im = re.match(regex, v)",
            "            if im:",
            "                i = int(im.group(1), base)",
            "                return f'\"{i}\":' if v.endswith(':') else str(i)",
            "",
            "        if v in vars:",
            "            try:",
            "                if not strict:",
            "                    json.loads(vars[v])",
            "            except json.JSONDecodeError:",
            "                return json.dumps(vars[v])",
            "            else:",
            "                return vars[v]",
            "",
            "        if not strict:",
            "            return f'\"{v}\"'",
            "",
            "        raise ValueError(f'Unknown value: {v}')",
            "",
            "    def create_map(mobj):",
            "        return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))",
            "",
            "    code = re.sub(r'(?:new\\s+)?Array\\((.*?)\\)', r'[\\g<1>]', code)",
            "    code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)",
            "    if not strict:",
            "        code = re.sub(rf'new Date\\(({STRING_RE})\\)', r'\\g<1>', code)",
            "        code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)",
            "        code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)",
            "        code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)",
            "",
            "    return re.sub(rf'''(?sx)",
            "        {STRING_RE}|",
            "        {COMMENT_RE}|,(?={SKIP_RE}[\\]}}])|",
            "        void\\s0|(?:(?<![0-9])[eE]|[a-df-zA-DF-Z_$])[.a-zA-Z_$0-9]*|",
            "        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{SKIP_RE}:)?|",
            "        [0-9]+(?={SKIP_RE}:)|",
            "        !+",
            "        ''', fix_kv, code)",
            "",
            "",
            "def qualities(quality_ids):",
            "    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"",
            "    def q(qid):",
            "        try:",
            "            return quality_ids.index(qid)",
            "        except ValueError:",
            "            return -1",
            "    return q",
            "",
            "",
            "POSTPROCESS_WHEN = ('pre_process', 'after_filter', 'video', 'before_dl', 'post_process', 'after_move', 'after_video', 'playlist')",
            "",
            "",
            "DEFAULT_OUTTMPL = {",
            "    'default': '%(title)s [%(id)s].%(ext)s',",
            "    'chapter': '%(title)s - %(section_number)03d %(section_title)s [%(id)s].%(ext)s',",
            "}",
            "OUTTMPL_TYPES = {",
            "    'chapter': None,",
            "    'subtitle': None,",
            "    'thumbnail': None,",
            "    'description': 'description',",
            "    'annotation': 'annotations.xml',",
            "    'infojson': 'info.json',",
            "    'link': None,",
            "    'pl_video': None,",
            "    'pl_thumbnail': None,",
            "    'pl_description': 'description',",
            "    'pl_infojson': 'info.json',",
            "}",
            "",
            "# As of [1] format syntax is:",
            "#  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type",
            "# 1. https://docs.python.org/2/library/stdtypes.html#string-formatting",
            "STR_FORMAT_RE_TMPL = r'''(?x)",
            "    (?<!%)(?P<prefix>(?:%%)*)",
            "    %",
            "    (?P<has_key>\\((?P<key>{0})\\))?",
            "    (?P<format>",
            "        (?P<conversion>[#0\\-+ ]+)?",
            "        (?P<min_width>\\d+)?",
            "        (?P<precision>\\.\\d+)?",
            "        (?P<len_mod>[hlL])?  # unused in python",
            "        {1}  # conversion type",
            "    )",
            "'''",
            "",
            "",
            "STR_FORMAT_TYPES = 'diouxXeEfFgGcrsa'",
            "",
            "",
            "def limit_length(s, length):",
            "    \"\"\" Add ellipses to overly long strings \"\"\"",
            "    if s is None:",
            "        return None",
            "    ELLIPSES = '...'",
            "    if len(s) > length:",
            "        return s[:length - len(ELLIPSES)] + ELLIPSES",
            "    return s",
            "",
            "",
            "def version_tuple(v):",
            "    return tuple(int(e) for e in re.split(r'[-.]', v))",
            "",
            "",
            "def is_outdated_version(version, limit, assume_new=True):",
            "    if not version:",
            "        return not assume_new",
            "    try:",
            "        return version_tuple(version) < version_tuple(limit)",
            "    except ValueError:",
            "        return not assume_new",
            "",
            "",
            "def ytdl_is_updateable():",
            "    \"\"\" Returns if yt-dlp can be updated with -U \"\"\"",
            "",
            "    from ..update import is_non_updateable",
            "",
            "    return not is_non_updateable()",
            "",
            "",
            "def args_to_str(args):",
            "    # Get a short string representation for a subprocess command",
            "    return shell_quote(args)",
            "",
            "",
            "def error_to_str(err):",
            "    return f'{type(err).__name__}: {err}'",
            "",
            "",
            "def mimetype2ext(mt, default=NO_DEFAULT):",
            "    if not isinstance(mt, str):",
            "        if default is not NO_DEFAULT:",
            "            return default",
            "        return None",
            "",
            "    MAP = {",
            "        # video",
            "        '3gpp': '3gp',",
            "        'mp2t': 'ts',",
            "        'mp4': 'mp4',",
            "        'mpeg': 'mpeg',",
            "        'mpegurl': 'm3u8',",
            "        'quicktime': 'mov',",
            "        'webm': 'webm',",
            "        'vp9': 'vp9',",
            "        'video/ogg': 'ogv',",
            "        'x-flv': 'flv',",
            "        'x-m4v': 'm4v',",
            "        'x-matroska': 'mkv',",
            "        'x-mng': 'mng',",
            "        'x-mp4-fragmented': 'mp4',",
            "        'x-ms-asf': 'asf',",
            "        'x-ms-wmv': 'wmv',",
            "        'x-msvideo': 'avi',",
            "",
            "        # application (streaming playlists)",
            "        'dash+xml': 'mpd',",
            "        'f4m+xml': 'f4m',",
            "        'hds+xml': 'f4m',",
            "        'vnd.apple.mpegurl': 'm3u8',",
            "        'vnd.ms-sstr+xml': 'ism',",
            "        'x-mpegurl': 'm3u8',",
            "",
            "        # audio",
            "        'audio/mp4': 'm4a',",
            "        # Per RFC 3003, audio/mpeg can be .mp1, .mp2 or .mp3.",
            "        # Using .mp3 as it's the most popular one",
            "        'audio/mpeg': 'mp3',",
            "        'audio/webm': 'webm',",
            "        'audio/x-matroska': 'mka',",
            "        'audio/x-mpegurl': 'm3u',",
            "        'midi': 'mid',",
            "        'ogg': 'ogg',",
            "        'wav': 'wav',",
            "        'wave': 'wav',",
            "        'x-aac': 'aac',",
            "        'x-flac': 'flac',",
            "        'x-m4a': 'm4a',",
            "        'x-realaudio': 'ra',",
            "        'x-wav': 'wav',",
            "",
            "        # image",
            "        'avif': 'avif',",
            "        'bmp': 'bmp',",
            "        'gif': 'gif',",
            "        'jpeg': 'jpg',",
            "        'png': 'png',",
            "        'svg+xml': 'svg',",
            "        'tiff': 'tif',",
            "        'vnd.wap.wbmp': 'wbmp',",
            "        'webp': 'webp',",
            "        'x-icon': 'ico',",
            "        'x-jng': 'jng',",
            "        'x-ms-bmp': 'bmp',",
            "",
            "        # caption",
            "        'filmstrip+json': 'fs',",
            "        'smptett+xml': 'tt',",
            "        'ttaf+xml': 'dfxp',",
            "        'ttml+xml': 'ttml',",
            "        'x-ms-sami': 'sami',",
            "",
            "        # misc",
            "        'gzip': 'gz',",
            "        'json': 'json',",
            "        'xml': 'xml',",
            "        'zip': 'zip',",
            "    }",
            "",
            "    mimetype = mt.partition(';')[0].strip().lower()",
            "    _, _, subtype = mimetype.rpartition('/')",
            "",
            "    ext = traversal.traverse_obj(MAP, mimetype, subtype, subtype.rsplit('+')[-1])",
            "    if ext:",
            "        return ext",
            "    elif default is not NO_DEFAULT:",
            "        return default",
            "    return subtype.replace('+', '.')",
            "",
            "",
            "def ext2mimetype(ext_or_url):",
            "    if not ext_or_url:",
            "        return None",
            "    if '.' not in ext_or_url:",
            "        ext_or_url = f'file.{ext_or_url}'",
            "    return mimetypes.guess_type(ext_or_url)[0]",
            "",
            "",
            "def parse_codecs(codecs_str):",
            "    # http://tools.ietf.org/html/rfc6381",
            "    if not codecs_str:",
            "        return {}",
            "    split_codecs = list(filter(None, map(",
            "        str.strip, codecs_str.strip().strip(',').split(','))))",
            "    vcodec, acodec, scodec, hdr = None, None, None, None",
            "    for full_codec in split_codecs:",
            "        parts = re.sub(r'0+(?=\\d)', '', full_codec).split('.')",
            "        if parts[0] in ('avc1', 'avc2', 'avc3', 'avc4', 'vp9', 'vp8', 'hev1', 'hev2',",
            "                        'h263', 'h264', 'mp4v', 'hvc1', 'av1', 'theora', 'dvh1', 'dvhe'):",
            "            if vcodec:",
            "                continue",
            "            vcodec = full_codec",
            "            if parts[0] in ('dvh1', 'dvhe'):",
            "                hdr = 'DV'",
            "            elif parts[0] == 'av1' and traversal.traverse_obj(parts, 3) == '10':",
            "                hdr = 'HDR10'",
            "            elif parts[:2] == ['vp9', '2']:",
            "                hdr = 'HDR10'",
            "        elif parts[0] in ('flac', 'mp4a', 'opus', 'vorbis', 'mp3', 'aac', 'ac-4',",
            "                          'ac-3', 'ec-3', 'eac3', 'dtsc', 'dtse', 'dtsh', 'dtsl'):",
            "            acodec = acodec or full_codec",
            "        elif parts[0] in ('stpp', 'wvtt'):",
            "            scodec = scodec or full_codec",
            "        else:",
            "            write_string(f'WARNING: Unknown codec {full_codec}\\n')",
            "    if vcodec or acodec or scodec:",
            "        return {",
            "            'vcodec': vcodec or 'none',",
            "            'acodec': acodec or 'none',",
            "            'dynamic_range': hdr,",
            "            **({'scodec': scodec} if scodec is not None else {}),",
            "        }",
            "    elif len(split_codecs) == 2:",
            "        return {",
            "            'vcodec': split_codecs[0],",
            "            'acodec': split_codecs[1],",
            "        }",
            "    return {}",
            "",
            "",
            "def get_compatible_ext(*, vcodecs, acodecs, vexts, aexts, preferences=None):",
            "    assert len(vcodecs) == len(vexts) and len(acodecs) == len(aexts)",
            "",
            "    allow_mkv = not preferences or 'mkv' in preferences",
            "",
            "    if allow_mkv and max(len(acodecs), len(vcodecs)) > 1:",
            "        return 'mkv'  # TODO: any other format allows this?",
            "",
            "    # TODO: All codecs supported by parse_codecs isn't handled here",
            "    COMPATIBLE_CODECS = {",
            "        'mp4': {",
            "            'av1', 'hevc', 'avc1', 'mp4a', 'ac-4',  # fourcc (m3u8, mpd)",
            "            'h264', 'aacl', 'ec-3',  # Set in ISM",
            "        },",
            "        'webm': {",
            "            'av1', 'vp9', 'vp8', 'opus', 'vrbs',",
            "            'vp9x', 'vp8x',  # in the webm spec",
            "        },",
            "    }",
            "",
            "    sanitize_codec = functools.partial(",
            "        try_get, getter=lambda x: x[0].split('.')[0].replace('0', '').lower())",
            "    vcodec, acodec = sanitize_codec(vcodecs), sanitize_codec(acodecs)",
            "",
            "    for ext in preferences or COMPATIBLE_CODECS.keys():",
            "        codec_set = COMPATIBLE_CODECS.get(ext, set())",
            "        if ext == 'mkv' or codec_set.issuperset((vcodec, acodec)):",
            "            return ext",
            "",
            "    COMPATIBLE_EXTS = (",
            "        {'mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma', 'mov'},",
            "        {'webm', 'weba'},",
            "    )",
            "    for ext in preferences or vexts:",
            "        current_exts = {ext, *vexts, *aexts}",
            "        if ext == 'mkv' or current_exts == {ext} or any(",
            "                ext_sets.issuperset(current_exts) for ext_sets in COMPATIBLE_EXTS):",
            "            return ext",
            "    return 'mkv' if allow_mkv else preferences[-1]",
            "",
            "",
            "def urlhandle_detect_ext(url_handle, default=NO_DEFAULT):",
            "    getheader = url_handle.headers.get",
            "",
            "    cd = getheader('Content-Disposition')",
            "    if cd:",
            "        m = re.match(r'attachment;\\s*filename=\"(?P<filename>[^\"]+)\"', cd)",
            "        if m:",
            "            e = determine_ext(m.group('filename'), default_ext=None)",
            "            if e:",
            "                return e",
            "",
            "    meta_ext = getheader('x-amz-meta-name')",
            "    if meta_ext:",
            "        e = meta_ext.rpartition('.')[2]",
            "        if e:",
            "            return e",
            "",
            "    return mimetype2ext(getheader('Content-Type'), default=default)",
            "",
            "",
            "def encode_data_uri(data, mime_type):",
            "    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))",
            "",
            "",
            "def age_restricted(content_limit, age_limit):",
            "    \"\"\" Returns True iff the content should be blocked \"\"\"",
            "",
            "    if age_limit is None:  # No limit set",
            "        return False",
            "    if content_limit is None:",
            "        return False  # Content available for everyone",
            "    return age_limit < content_limit",
            "",
            "",
            "# List of known byte-order-marks (BOM)",
            "BOMS = [",
            "    (b'\\xef\\xbb\\xbf', 'utf-8'),",
            "    (b'\\x00\\x00\\xfe\\xff', 'utf-32-be'),",
            "    (b'\\xff\\xfe\\x00\\x00', 'utf-32-le'),",
            "    (b'\\xff\\xfe', 'utf-16-le'),",
            "    (b'\\xfe\\xff', 'utf-16-be'),",
            "]",
            "",
            "",
            "def is_html(first_bytes):",
            "    \"\"\" Detect whether a file contains HTML by examining its first bytes. \"\"\"",
            "",
            "    encoding = 'utf-8'",
            "    for bom, enc in BOMS:",
            "        while first_bytes.startswith(bom):",
            "            encoding, first_bytes = enc, first_bytes[len(bom):]",
            "",
            "    return re.match(r'^\\s*<', first_bytes.decode(encoding, 'replace'))",
            "",
            "",
            "def determine_protocol(info_dict):",
            "    protocol = info_dict.get('protocol')",
            "    if protocol is not None:",
            "        return protocol",
            "",
            "    url = sanitize_url(info_dict['url'])",
            "    if url.startswith('rtmp'):",
            "        return 'rtmp'",
            "    elif url.startswith('mms'):",
            "        return 'mms'",
            "    elif url.startswith('rtsp'):",
            "        return 'rtsp'",
            "",
            "    ext = determine_ext(url)",
            "    if ext == 'm3u8':",
            "        return 'm3u8' if info_dict.get('is_live') else 'm3u8_native'",
            "    elif ext == 'f4m':",
            "        return 'f4m'",
            "",
            "    return urllib.parse.urlparse(url).scheme",
            "",
            "",
            "def render_table(header_row, data, delim=False, extra_gap=0, hide_empty=False):",
            "    \"\"\" Render a list of rows, each as a list of values.",
            "    Text after a \\t will be right aligned \"\"\"",
            "    def width(string):",
            "        return len(remove_terminal_sequences(string).replace('\\t', ''))",
            "",
            "    def get_max_lens(table):",
            "        return [max(width(str(v)) for v in col) for col in zip(*table)]",
            "",
            "    def filter_using_list(row, filterArray):",
            "        return [col for take, col in itertools.zip_longest(filterArray, row, fillvalue=True) if take]",
            "",
            "    max_lens = get_max_lens(data) if hide_empty else []",
            "    header_row = filter_using_list(header_row, max_lens)",
            "    data = [filter_using_list(row, max_lens) for row in data]",
            "",
            "    table = [header_row] + data",
            "    max_lens = get_max_lens(table)",
            "    extra_gap += 1",
            "    if delim:",
            "        table = [header_row, [delim * (ml + extra_gap) for ml in max_lens]] + data",
            "        table[1][-1] = table[1][-1][:-extra_gap * len(delim)]  # Remove extra_gap from end of delimiter",
            "    for row in table:",
            "        for pos, text in enumerate(map(str, row)):",
            "            if '\\t' in text:",
            "                row[pos] = text.replace('\\t', ' ' * (max_lens[pos] - width(text))) + ' ' * extra_gap",
            "            else:",
            "                row[pos] = text + ' ' * (max_lens[pos] - width(text) + extra_gap)",
            "    ret = '\\n'.join(''.join(row).rstrip() for row in table)",
            "    return ret",
            "",
            "",
            "def _match_one(filter_part, dct, incomplete):",
            "    # TODO: Generalize code with YoutubeDL._build_format_filter",
            "    STRING_OPERATORS = {",
            "        '*=': operator.contains,",
            "        '^=': lambda attr, value: attr.startswith(value),",
            "        '$=': lambda attr, value: attr.endswith(value),",
            "        '~=': lambda attr, value: re.search(value, attr),",
            "    }",
            "    COMPARISON_OPERATORS = {",
            "        **STRING_OPERATORS,",
            "        '<=': operator.le,  # \"<=\" must be defined above \"<\"",
            "        '<': operator.lt,",
            "        '>=': operator.ge,",
            "        '>': operator.gt,",
            "        '=': operator.eq,",
            "    }",
            "",
            "    if isinstance(incomplete, bool):",
            "        is_incomplete = lambda _: incomplete",
            "    else:",
            "        is_incomplete = lambda k: k in incomplete",
            "",
            "    operator_rex = re.compile(r'''(?x)",
            "        (?P<key>[a-z_]+)",
            "        \\s*(?P<negation>!\\s*)?(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*",
            "        (?:",
            "            (?P<quote>[\"\\'])(?P<quotedstrval>.+?)(?P=quote)|",
            "            (?P<strval>.+?)",
            "        )",
            "        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))",
            "    m = operator_rex.fullmatch(filter_part.strip())",
            "    if m:",
            "        m = m.groupdict()",
            "        unnegated_op = COMPARISON_OPERATORS[m['op']]",
            "        if m['negation']:",
            "            op = lambda attr, value: not unnegated_op(attr, value)",
            "        else:",
            "            op = unnegated_op",
            "        comparison_value = m['quotedstrval'] or m['strval'] or m['intval']",
            "        if m['quote']:",
            "            comparison_value = comparison_value.replace(r'\\%s' % m['quote'], m['quote'])",
            "        actual_value = dct.get(m['key'])",
            "        numeric_comparison = None",
            "        if isinstance(actual_value, (int, float)):",
            "            # If the original field is a string and matching comparisonvalue is",
            "            # a number we should respect the origin of the original field",
            "            # and process comparison value as a string (see",
            "            # https://github.com/ytdl-org/youtube-dl/issues/11082)",
            "            try:",
            "                numeric_comparison = int(comparison_value)",
            "            except ValueError:",
            "                numeric_comparison = parse_filesize(comparison_value)",
            "                if numeric_comparison is None:",
            "                    numeric_comparison = parse_filesize(f'{comparison_value}B')",
            "                if numeric_comparison is None:",
            "                    numeric_comparison = parse_duration(comparison_value)",
            "        if numeric_comparison is not None and m['op'] in STRING_OPERATORS:",
            "            raise ValueError('Operator %s only supports string values!' % m['op'])",
            "        if actual_value is None:",
            "            return is_incomplete(m['key']) or m['none_inclusive']",
            "        return op(actual_value, comparison_value if numeric_comparison is None else numeric_comparison)",
            "",
            "    UNARY_OPERATORS = {",
            "        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),",
            "        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),",
            "    }",
            "    operator_rex = re.compile(r'''(?x)",
            "        (?P<op>%s)\\s*(?P<key>[a-z_]+)",
            "        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))",
            "    m = operator_rex.fullmatch(filter_part.strip())",
            "    if m:",
            "        op = UNARY_OPERATORS[m.group('op')]",
            "        actual_value = dct.get(m.group('key'))",
            "        if is_incomplete(m.group('key')) and actual_value is None:",
            "            return True",
            "        return op(actual_value)",
            "",
            "    raise ValueError('Invalid filter part %r' % filter_part)",
            "",
            "",
            "def match_str(filter_str, dct, incomplete=False):",
            "    \"\"\" Filter a dictionary with a simple string syntax.",
            "    @returns           Whether the filter passes",
            "    @param incomplete  Set of keys that is expected to be missing from dct.",
            "                       Can be True/False to indicate all/none of the keys may be missing.",
            "                       All conditions on incomplete keys pass if the key is missing",
            "    \"\"\"",
            "    return all(",
            "        _match_one(filter_part.replace(r'\\&', '&'), dct, incomplete)",
            "        for filter_part in re.split(r'(?<!\\\\)&', filter_str))",
            "",
            "",
            "def match_filter_func(filters, breaking_filters=None):",
            "    if not filters and not breaking_filters:",
            "        return None",
            "    repr_ = f'{match_filter_func.__module__}.{match_filter_func.__qualname__}({filters}, {breaking_filters})'",
            "",
            "    breaking_filters = match_filter_func(breaking_filters) or (lambda _, __: None)",
            "    filters = set(variadic(filters or []))",
            "",
            "    interactive = '-' in filters",
            "    if interactive:",
            "        filters.remove('-')",
            "",
            "    @function_with_repr.set_repr(repr_)",
            "    def _match_func(info_dict, incomplete=False):",
            "        ret = breaking_filters(info_dict, incomplete)",
            "        if ret is not None:",
            "            raise RejectedVideoReached(ret)",
            "",
            "        if not filters or any(match_str(f, info_dict, incomplete) for f in filters):",
            "            return NO_DEFAULT if interactive and not incomplete else None",
            "        else:",
            "            video_title = info_dict.get('title') or info_dict.get('id') or 'entry'",
            "            filter_str = ') | ('.join(map(str.strip, filters))",
            "            return f'{video_title} does not pass filter ({filter_str}), skipping ..'",
            "    return _match_func",
            "",
            "",
            "class download_range_func:",
            "    def __init__(self, chapters, ranges, from_info=False):",
            "        self.chapters, self.ranges, self.from_info = chapters, ranges, from_info",
            "",
            "    def __call__(self, info_dict, ydl):",
            "",
            "        warning = ('There are no chapters matching the regex' if info_dict.get('chapters')",
            "                   else 'Cannot match chapters since chapter information is unavailable')",
            "        for regex in self.chapters or []:",
            "            for i, chapter in enumerate(info_dict.get('chapters') or []):",
            "                if re.search(regex, chapter['title']):",
            "                    warning = None",
            "                    yield {**chapter, 'index': i}",
            "        if self.chapters and warning:",
            "            ydl.to_screen(f'[info] {info_dict[\"id\"]}: {warning}')",
            "",
            "        for start, end in self.ranges or []:",
            "            yield {",
            "                'start_time': self._handle_negative_timestamp(start, info_dict),",
            "                'end_time': self._handle_negative_timestamp(end, info_dict),",
            "            }",
            "",
            "        if self.from_info and (info_dict.get('start_time') or info_dict.get('end_time')):",
            "            yield {",
            "                'start_time': info_dict.get('start_time') or 0,",
            "                'end_time': info_dict.get('end_time') or float('inf'),",
            "            }",
            "        elif not self.ranges and not self.chapters:",
            "            yield {}",
            "",
            "    @staticmethod",
            "    def _handle_negative_timestamp(time, info):",
            "        return max(info['duration'] + time, 0) if info.get('duration') and time < 0 else time",
            "",
            "    def __eq__(self, other):",
            "        return (isinstance(other, download_range_func)",
            "                and self.chapters == other.chapters and self.ranges == other.ranges)",
            "",
            "    def __repr__(self):",
            "        return f'{__name__}.{type(self).__name__}({self.chapters}, {self.ranges})'",
            "",
            "",
            "def parse_dfxp_time_expr(time_expr):",
            "    if not time_expr:",
            "        return",
            "",
            "    mobj = re.match(rf'^(?P<time_offset>{NUMBER_RE})s?$', time_expr)",
            "    if mobj:",
            "        return float(mobj.group('time_offset'))",
            "",
            "    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)",
            "    if mobj:",
            "        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))",
            "",
            "",
            "def srt_subtitles_timecode(seconds):",
            "    return '%02d:%02d:%02d,%03d' % timetuple_from_msec(seconds * 1000)",
            "",
            "",
            "def ass_subtitles_timecode(seconds):",
            "    time = timetuple_from_msec(seconds * 1000)",
            "    return '%01d:%02d:%02d.%02d' % (*time[:-1], time.milliseconds / 10)",
            "",
            "",
            "def dfxp2srt(dfxp_data):",
            "    '''",
            "    @param dfxp_data A bytes-like object containing DFXP data",
            "    @returns A unicode object containing converted SRT data",
            "    '''",
            "    LEGACY_NAMESPACES = (",
            "        (b'http://www.w3.org/ns/ttml', [",
            "            b'http://www.w3.org/2004/11/ttaf1',",
            "            b'http://www.w3.org/2006/04/ttaf1',",
            "            b'http://www.w3.org/2006/10/ttaf1',",
            "        ]),",
            "        (b'http://www.w3.org/ns/ttml#styling', [",
            "            b'http://www.w3.org/ns/ttml#style',",
            "        ]),",
            "    )",
            "",
            "    SUPPORTED_STYLING = [",
            "        'color',",
            "        'fontFamily',",
            "        'fontSize',",
            "        'fontStyle',",
            "        'fontWeight',",
            "        'textDecoration'",
            "    ]",
            "",
            "    _x = functools.partial(xpath_with_ns, ns_map={",
            "        'xml': 'http://www.w3.org/XML/1998/namespace',",
            "        'ttml': 'http://www.w3.org/ns/ttml',",
            "        'tts': 'http://www.w3.org/ns/ttml#styling',",
            "    })",
            "",
            "    styles = {}",
            "    default_style = {}",
            "",
            "    class TTMLPElementParser:",
            "        _out = ''",
            "        _unclosed_elements = []",
            "        _applied_styles = []",
            "",
            "        def start(self, tag, attrib):",
            "            if tag in (_x('ttml:br'), 'br'):",
            "                self._out += '\\n'",
            "            else:",
            "                unclosed_elements = []",
            "                style = {}",
            "                element_style_id = attrib.get('style')",
            "                if default_style:",
            "                    style.update(default_style)",
            "                if element_style_id:",
            "                    style.update(styles.get(element_style_id, {}))",
            "                for prop in SUPPORTED_STYLING:",
            "                    prop_val = attrib.get(_x('tts:' + prop))",
            "                    if prop_val:",
            "                        style[prop] = prop_val",
            "                if style:",
            "                    font = ''",
            "                    for k, v in sorted(style.items()):",
            "                        if self._applied_styles and self._applied_styles[-1].get(k) == v:",
            "                            continue",
            "                        if k == 'color':",
            "                            font += ' color=\"%s\"' % v",
            "                        elif k == 'fontSize':",
            "                            font += ' size=\"%s\"' % v",
            "                        elif k == 'fontFamily':",
            "                            font += ' face=\"%s\"' % v",
            "                        elif k == 'fontWeight' and v == 'bold':",
            "                            self._out += '<b>'",
            "                            unclosed_elements.append('b')",
            "                        elif k == 'fontStyle' and v == 'italic':",
            "                            self._out += '<i>'",
            "                            unclosed_elements.append('i')",
            "                        elif k == 'textDecoration' and v == 'underline':",
            "                            self._out += '<u>'",
            "                            unclosed_elements.append('u')",
            "                    if font:",
            "                        self._out += '<font' + font + '>'",
            "                        unclosed_elements.append('font')",
            "                    applied_style = {}",
            "                    if self._applied_styles:",
            "                        applied_style.update(self._applied_styles[-1])",
            "                    applied_style.update(style)",
            "                    self._applied_styles.append(applied_style)",
            "                self._unclosed_elements.append(unclosed_elements)",
            "",
            "        def end(self, tag):",
            "            if tag not in (_x('ttml:br'), 'br'):",
            "                unclosed_elements = self._unclosed_elements.pop()",
            "                for element in reversed(unclosed_elements):",
            "                    self._out += '</%s>' % element",
            "                if unclosed_elements and self._applied_styles:",
            "                    self._applied_styles.pop()",
            "",
            "        def data(self, data):",
            "            self._out += data",
            "",
            "        def close(self):",
            "            return self._out.strip()",
            "",
            "    # Fix UTF-8 encoded file wrongly marked as UTF-16. See https://github.com/yt-dlp/yt-dlp/issues/6543#issuecomment-1477169870",
            "    # This will not trigger false positives since only UTF-8 text is being replaced",
            "    dfxp_data = dfxp_data.replace(b'encoding=\\'UTF-16\\'', b'encoding=\\'UTF-8\\'')",
            "",
            "    def parse_node(node):",
            "        target = TTMLPElementParser()",
            "        parser = xml.etree.ElementTree.XMLParser(target=target)",
            "        parser.feed(xml.etree.ElementTree.tostring(node))",
            "        return parser.close()",
            "",
            "    for k, v in LEGACY_NAMESPACES:",
            "        for ns in v:",
            "            dfxp_data = dfxp_data.replace(ns, k)",
            "",
            "    dfxp = compat_etree_fromstring(dfxp_data)",
            "    out = []",
            "    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')",
            "",
            "    if not paras:",
            "        raise ValueError('Invalid dfxp/TTML subtitle')",
            "",
            "    repeat = False",
            "    while True:",
            "        for style in dfxp.findall(_x('.//ttml:style')):",
            "            style_id = style.get('id') or style.get(_x('xml:id'))",
            "            if not style_id:",
            "                continue",
            "            parent_style_id = style.get('style')",
            "            if parent_style_id:",
            "                if parent_style_id not in styles:",
            "                    repeat = True",
            "                    continue",
            "                styles[style_id] = styles[parent_style_id].copy()",
            "            for prop in SUPPORTED_STYLING:",
            "                prop_val = style.get(_x('tts:' + prop))",
            "                if prop_val:",
            "                    styles.setdefault(style_id, {})[prop] = prop_val",
            "        if repeat:",
            "            repeat = False",
            "        else:",
            "            break",
            "",
            "    for p in ('body', 'div'):",
            "        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])",
            "        if ele is None:",
            "            continue",
            "        style = styles.get(ele.get('style'))",
            "        if not style:",
            "            continue",
            "        default_style.update(style)",
            "",
            "    for para, index in zip(paras, itertools.count(1)):",
            "        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))",
            "        end_time = parse_dfxp_time_expr(para.attrib.get('end'))",
            "        dur = parse_dfxp_time_expr(para.attrib.get('dur'))",
            "        if begin_time is None:",
            "            continue",
            "        if not end_time:",
            "            if not dur:",
            "                continue",
            "            end_time = begin_time + dur",
            "        out.append('%d\\n%s --> %s\\n%s\\n\\n' % (",
            "            index,",
            "            srt_subtitles_timecode(begin_time),",
            "            srt_subtitles_timecode(end_time),",
            "            parse_node(para)))",
            "",
            "    return ''.join(out)",
            "",
            "",
            "def cli_option(params, command_option, param, separator=None):",
            "    param = params.get(param)",
            "    return ([] if param is None",
            "            else [command_option, str(param)] if separator is None",
            "            else [f'{command_option}{separator}{param}'])",
            "",
            "",
            "def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):",
            "    param = params.get(param)",
            "    assert param in (True, False, None)",
            "    return cli_option({True: true_value, False: false_value}, command_option, param, separator)",
            "",
            "",
            "def cli_valueless_option(params, command_option, param, expected_value=True):",
            "    return [command_option] if params.get(param) == expected_value else []",
            "",
            "",
            "def cli_configuration_args(argdict, keys, default=[], use_compat=True):",
            "    if isinstance(argdict, (list, tuple)):  # for backward compatibility",
            "        if use_compat:",
            "            return argdict",
            "        else:",
            "            argdict = None",
            "    if argdict is None:",
            "        return default",
            "    assert isinstance(argdict, dict)",
            "",
            "    assert isinstance(keys, (list, tuple))",
            "    for key_list in keys:",
            "        arg_list = list(filter(",
            "            lambda x: x is not None,",
            "            [argdict.get(key.lower()) for key in variadic(key_list)]))",
            "        if arg_list:",
            "            return [arg for args in arg_list for arg in args]",
            "    return default",
            "",
            "",
            "def _configuration_args(main_key, argdict, exe, keys=None, default=[], use_compat=True):",
            "    main_key, exe = main_key.lower(), exe.lower()",
            "    root_key = exe if main_key == exe else f'{main_key}+{exe}'",
            "    keys = [f'{root_key}{k}' for k in (keys or [''])]",
            "    if root_key in keys:",
            "        if main_key != exe:",
            "            keys.append((main_key, exe))",
            "        keys.append('default')",
            "    else:",
            "        use_compat = False",
            "    return cli_configuration_args(argdict, keys, default, use_compat)",
            "",
            "",
            "class ISO639Utils:",
            "    # See http://www.loc.gov/standards/iso639-2/ISO-639-2_utf-8.txt",
            "    _lang_map = {",
            "        'aa': 'aar',",
            "        'ab': 'abk',",
            "        'ae': 'ave',",
            "        'af': 'afr',",
            "        'ak': 'aka',",
            "        'am': 'amh',",
            "        'an': 'arg',",
            "        'ar': 'ara',",
            "        'as': 'asm',",
            "        'av': 'ava',",
            "        'ay': 'aym',",
            "        'az': 'aze',",
            "        'ba': 'bak',",
            "        'be': 'bel',",
            "        'bg': 'bul',",
            "        'bh': 'bih',",
            "        'bi': 'bis',",
            "        'bm': 'bam',",
            "        'bn': 'ben',",
            "        'bo': 'bod',",
            "        'br': 'bre',",
            "        'bs': 'bos',",
            "        'ca': 'cat',",
            "        'ce': 'che',",
            "        'ch': 'cha',",
            "        'co': 'cos',",
            "        'cr': 'cre',",
            "        'cs': 'ces',",
            "        'cu': 'chu',",
            "        'cv': 'chv',",
            "        'cy': 'cym',",
            "        'da': 'dan',",
            "        'de': 'deu',",
            "        'dv': 'div',",
            "        'dz': 'dzo',",
            "        'ee': 'ewe',",
            "        'el': 'ell',",
            "        'en': 'eng',",
            "        'eo': 'epo',",
            "        'es': 'spa',",
            "        'et': 'est',",
            "        'eu': 'eus',",
            "        'fa': 'fas',",
            "        'ff': 'ful',",
            "        'fi': 'fin',",
            "        'fj': 'fij',",
            "        'fo': 'fao',",
            "        'fr': 'fra',",
            "        'fy': 'fry',",
            "        'ga': 'gle',",
            "        'gd': 'gla',",
            "        'gl': 'glg',",
            "        'gn': 'grn',",
            "        'gu': 'guj',",
            "        'gv': 'glv',",
            "        'ha': 'hau',",
            "        'he': 'heb',",
            "        'iw': 'heb',  # Replaced by he in 1989 revision",
            "        'hi': 'hin',",
            "        'ho': 'hmo',",
            "        'hr': 'hrv',",
            "        'ht': 'hat',",
            "        'hu': 'hun',",
            "        'hy': 'hye',",
            "        'hz': 'her',",
            "        'ia': 'ina',",
            "        'id': 'ind',",
            "        'in': 'ind',  # Replaced by id in 1989 revision",
            "        'ie': 'ile',",
            "        'ig': 'ibo',",
            "        'ii': 'iii',",
            "        'ik': 'ipk',",
            "        'io': 'ido',",
            "        'is': 'isl',",
            "        'it': 'ita',",
            "        'iu': 'iku',",
            "        'ja': 'jpn',",
            "        'jv': 'jav',",
            "        'ka': 'kat',",
            "        'kg': 'kon',",
            "        'ki': 'kik',",
            "        'kj': 'kua',",
            "        'kk': 'kaz',",
            "        'kl': 'kal',",
            "        'km': 'khm',",
            "        'kn': 'kan',",
            "        'ko': 'kor',",
            "        'kr': 'kau',",
            "        'ks': 'kas',",
            "        'ku': 'kur',",
            "        'kv': 'kom',",
            "        'kw': 'cor',",
            "        'ky': 'kir',",
            "        'la': 'lat',",
            "        'lb': 'ltz',",
            "        'lg': 'lug',",
            "        'li': 'lim',",
            "        'ln': 'lin',",
            "        'lo': 'lao',",
            "        'lt': 'lit',",
            "        'lu': 'lub',",
            "        'lv': 'lav',",
            "        'mg': 'mlg',",
            "        'mh': 'mah',",
            "        'mi': 'mri',",
            "        'mk': 'mkd',",
            "        'ml': 'mal',",
            "        'mn': 'mon',",
            "        'mr': 'mar',",
            "        'ms': 'msa',",
            "        'mt': 'mlt',",
            "        'my': 'mya',",
            "        'na': 'nau',",
            "        'nb': 'nob',",
            "        'nd': 'nde',",
            "        'ne': 'nep',",
            "        'ng': 'ndo',",
            "        'nl': 'nld',",
            "        'nn': 'nno',",
            "        'no': 'nor',",
            "        'nr': 'nbl',",
            "        'nv': 'nav',",
            "        'ny': 'nya',",
            "        'oc': 'oci',",
            "        'oj': 'oji',",
            "        'om': 'orm',",
            "        'or': 'ori',",
            "        'os': 'oss',",
            "        'pa': 'pan',",
            "        'pe': 'per',",
            "        'pi': 'pli',",
            "        'pl': 'pol',",
            "        'ps': 'pus',",
            "        'pt': 'por',",
            "        'qu': 'que',",
            "        'rm': 'roh',",
            "        'rn': 'run',",
            "        'ro': 'ron',",
            "        'ru': 'rus',",
            "        'rw': 'kin',",
            "        'sa': 'san',",
            "        'sc': 'srd',",
            "        'sd': 'snd',",
            "        'se': 'sme',",
            "        'sg': 'sag',",
            "        'si': 'sin',",
            "        'sk': 'slk',",
            "        'sl': 'slv',",
            "        'sm': 'smo',",
            "        'sn': 'sna',",
            "        'so': 'som',",
            "        'sq': 'sqi',",
            "        'sr': 'srp',",
            "        'ss': 'ssw',",
            "        'st': 'sot',",
            "        'su': 'sun',",
            "        'sv': 'swe',",
            "        'sw': 'swa',",
            "        'ta': 'tam',",
            "        'te': 'tel',",
            "        'tg': 'tgk',",
            "        'th': 'tha',",
            "        'ti': 'tir',",
            "        'tk': 'tuk',",
            "        'tl': 'tgl',",
            "        'tn': 'tsn',",
            "        'to': 'ton',",
            "        'tr': 'tur',",
            "        'ts': 'tso',",
            "        'tt': 'tat',",
            "        'tw': 'twi',",
            "        'ty': 'tah',",
            "        'ug': 'uig',",
            "        'uk': 'ukr',",
            "        'ur': 'urd',",
            "        'uz': 'uzb',",
            "        've': 'ven',",
            "        'vi': 'vie',",
            "        'vo': 'vol',",
            "        'wa': 'wln',",
            "        'wo': 'wol',",
            "        'xh': 'xho',",
            "        'yi': 'yid',",
            "        'ji': 'yid',  # Replaced by yi in 1989 revision",
            "        'yo': 'yor',",
            "        'za': 'zha',",
            "        'zh': 'zho',",
            "        'zu': 'zul',",
            "    }",
            "",
            "    @classmethod",
            "    def short2long(cls, code):",
            "        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"",
            "        return cls._lang_map.get(code[:2])",
            "",
            "    @classmethod",
            "    def long2short(cls, code):",
            "        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"",
            "        for short_name, long_name in cls._lang_map.items():",
            "            if long_name == code:",
            "                return short_name",
            "",
            "",
            "class ISO3166Utils:",
            "    # From http://data.okfn.org/data/core/country-list",
            "    _country_map = {",
            "        'AF': 'Afghanistan',",
            "        'AX': '\u00c5land Islands',",
            "        'AL': 'Albania',",
            "        'DZ': 'Algeria',",
            "        'AS': 'American Samoa',",
            "        'AD': 'Andorra',",
            "        'AO': 'Angola',",
            "        'AI': 'Anguilla',",
            "        'AQ': 'Antarctica',",
            "        'AG': 'Antigua and Barbuda',",
            "        'AR': 'Argentina',",
            "        'AM': 'Armenia',",
            "        'AW': 'Aruba',",
            "        'AU': 'Australia',",
            "        'AT': 'Austria',",
            "        'AZ': 'Azerbaijan',",
            "        'BS': 'Bahamas',",
            "        'BH': 'Bahrain',",
            "        'BD': 'Bangladesh',",
            "        'BB': 'Barbados',",
            "        'BY': 'Belarus',",
            "        'BE': 'Belgium',",
            "        'BZ': 'Belize',",
            "        'BJ': 'Benin',",
            "        'BM': 'Bermuda',",
            "        'BT': 'Bhutan',",
            "        'BO': 'Bolivia, Plurinational State of',",
            "        'BQ': 'Bonaire, Sint Eustatius and Saba',",
            "        'BA': 'Bosnia and Herzegovina',",
            "        'BW': 'Botswana',",
            "        'BV': 'Bouvet Island',",
            "        'BR': 'Brazil',",
            "        'IO': 'British Indian Ocean Territory',",
            "        'BN': 'Brunei Darussalam',",
            "        'BG': 'Bulgaria',",
            "        'BF': 'Burkina Faso',",
            "        'BI': 'Burundi',",
            "        'KH': 'Cambodia',",
            "        'CM': 'Cameroon',",
            "        'CA': 'Canada',",
            "        'CV': 'Cape Verde',",
            "        'KY': 'Cayman Islands',",
            "        'CF': 'Central African Republic',",
            "        'TD': 'Chad',",
            "        'CL': 'Chile',",
            "        'CN': 'China',",
            "        'CX': 'Christmas Island',",
            "        'CC': 'Cocos (Keeling) Islands',",
            "        'CO': 'Colombia',",
            "        'KM': 'Comoros',",
            "        'CG': 'Congo',",
            "        'CD': 'Congo, the Democratic Republic of the',",
            "        'CK': 'Cook Islands',",
            "        'CR': 'Costa Rica',",
            "        'CI': 'C\u00f4te d\\'Ivoire',",
            "        'HR': 'Croatia',",
            "        'CU': 'Cuba',",
            "        'CW': 'Cura\u00e7ao',",
            "        'CY': 'Cyprus',",
            "        'CZ': 'Czech Republic',",
            "        'DK': 'Denmark',",
            "        'DJ': 'Djibouti',",
            "        'DM': 'Dominica',",
            "        'DO': 'Dominican Republic',",
            "        'EC': 'Ecuador',",
            "        'EG': 'Egypt',",
            "        'SV': 'El Salvador',",
            "        'GQ': 'Equatorial Guinea',",
            "        'ER': 'Eritrea',",
            "        'EE': 'Estonia',",
            "        'ET': 'Ethiopia',",
            "        'FK': 'Falkland Islands (Malvinas)',",
            "        'FO': 'Faroe Islands',",
            "        'FJ': 'Fiji',",
            "        'FI': 'Finland',",
            "        'FR': 'France',",
            "        'GF': 'French Guiana',",
            "        'PF': 'French Polynesia',",
            "        'TF': 'French Southern Territories',",
            "        'GA': 'Gabon',",
            "        'GM': 'Gambia',",
            "        'GE': 'Georgia',",
            "        'DE': 'Germany',",
            "        'GH': 'Ghana',",
            "        'GI': 'Gibraltar',",
            "        'GR': 'Greece',",
            "        'GL': 'Greenland',",
            "        'GD': 'Grenada',",
            "        'GP': 'Guadeloupe',",
            "        'GU': 'Guam',",
            "        'GT': 'Guatemala',",
            "        'GG': 'Guernsey',",
            "        'GN': 'Guinea',",
            "        'GW': 'Guinea-Bissau',",
            "        'GY': 'Guyana',",
            "        'HT': 'Haiti',",
            "        'HM': 'Heard Island and McDonald Islands',",
            "        'VA': 'Holy See (Vatican City State)',",
            "        'HN': 'Honduras',",
            "        'HK': 'Hong Kong',",
            "        'HU': 'Hungary',",
            "        'IS': 'Iceland',",
            "        'IN': 'India',",
            "        'ID': 'Indonesia',",
            "        'IR': 'Iran, Islamic Republic of',",
            "        'IQ': 'Iraq',",
            "        'IE': 'Ireland',",
            "        'IM': 'Isle of Man',",
            "        'IL': 'Israel',",
            "        'IT': 'Italy',",
            "        'JM': 'Jamaica',",
            "        'JP': 'Japan',",
            "        'JE': 'Jersey',",
            "        'JO': 'Jordan',",
            "        'KZ': 'Kazakhstan',",
            "        'KE': 'Kenya',",
            "        'KI': 'Kiribati',",
            "        'KP': 'Korea, Democratic People\\'s Republic of',",
            "        'KR': 'Korea, Republic of',",
            "        'KW': 'Kuwait',",
            "        'KG': 'Kyrgyzstan',",
            "        'LA': 'Lao People\\'s Democratic Republic',",
            "        'LV': 'Latvia',",
            "        'LB': 'Lebanon',",
            "        'LS': 'Lesotho',",
            "        'LR': 'Liberia',",
            "        'LY': 'Libya',",
            "        'LI': 'Liechtenstein',",
            "        'LT': 'Lithuania',",
            "        'LU': 'Luxembourg',",
            "        'MO': 'Macao',",
            "        'MK': 'Macedonia, the Former Yugoslav Republic of',",
            "        'MG': 'Madagascar',",
            "        'MW': 'Malawi',",
            "        'MY': 'Malaysia',",
            "        'MV': 'Maldives',",
            "        'ML': 'Mali',",
            "        'MT': 'Malta',",
            "        'MH': 'Marshall Islands',",
            "        'MQ': 'Martinique',",
            "        'MR': 'Mauritania',",
            "        'MU': 'Mauritius',",
            "        'YT': 'Mayotte',",
            "        'MX': 'Mexico',",
            "        'FM': 'Micronesia, Federated States of',",
            "        'MD': 'Moldova, Republic of',",
            "        'MC': 'Monaco',",
            "        'MN': 'Mongolia',",
            "        'ME': 'Montenegro',",
            "        'MS': 'Montserrat',",
            "        'MA': 'Morocco',",
            "        'MZ': 'Mozambique',",
            "        'MM': 'Myanmar',",
            "        'NA': 'Namibia',",
            "        'NR': 'Nauru',",
            "        'NP': 'Nepal',",
            "        'NL': 'Netherlands',",
            "        'NC': 'New Caledonia',",
            "        'NZ': 'New Zealand',",
            "        'NI': 'Nicaragua',",
            "        'NE': 'Niger',",
            "        'NG': 'Nigeria',",
            "        'NU': 'Niue',",
            "        'NF': 'Norfolk Island',",
            "        'MP': 'Northern Mariana Islands',",
            "        'NO': 'Norway',",
            "        'OM': 'Oman',",
            "        'PK': 'Pakistan',",
            "        'PW': 'Palau',",
            "        'PS': 'Palestine, State of',",
            "        'PA': 'Panama',",
            "        'PG': 'Papua New Guinea',",
            "        'PY': 'Paraguay',",
            "        'PE': 'Peru',",
            "        'PH': 'Philippines',",
            "        'PN': 'Pitcairn',",
            "        'PL': 'Poland',",
            "        'PT': 'Portugal',",
            "        'PR': 'Puerto Rico',",
            "        'QA': 'Qatar',",
            "        'RE': 'R\u00e9union',",
            "        'RO': 'Romania',",
            "        'RU': 'Russian Federation',",
            "        'RW': 'Rwanda',",
            "        'BL': 'Saint Barth\u00e9lemy',",
            "        'SH': 'Saint Helena, Ascension and Tristan da Cunha',",
            "        'KN': 'Saint Kitts and Nevis',",
            "        'LC': 'Saint Lucia',",
            "        'MF': 'Saint Martin (French part)',",
            "        'PM': 'Saint Pierre and Miquelon',",
            "        'VC': 'Saint Vincent and the Grenadines',",
            "        'WS': 'Samoa',",
            "        'SM': 'San Marino',",
            "        'ST': 'Sao Tome and Principe',",
            "        'SA': 'Saudi Arabia',",
            "        'SN': 'Senegal',",
            "        'RS': 'Serbia',",
            "        'SC': 'Seychelles',",
            "        'SL': 'Sierra Leone',",
            "        'SG': 'Singapore',",
            "        'SX': 'Sint Maarten (Dutch part)',",
            "        'SK': 'Slovakia',",
            "        'SI': 'Slovenia',",
            "        'SB': 'Solomon Islands',",
            "        'SO': 'Somalia',",
            "        'ZA': 'South Africa',",
            "        'GS': 'South Georgia and the South Sandwich Islands',",
            "        'SS': 'South Sudan',",
            "        'ES': 'Spain',",
            "        'LK': 'Sri Lanka',",
            "        'SD': 'Sudan',",
            "        'SR': 'Suriname',",
            "        'SJ': 'Svalbard and Jan Mayen',",
            "        'SZ': 'Swaziland',",
            "        'SE': 'Sweden',",
            "        'CH': 'Switzerland',",
            "        'SY': 'Syrian Arab Republic',",
            "        'TW': 'Taiwan, Province of China',",
            "        'TJ': 'Tajikistan',",
            "        'TZ': 'Tanzania, United Republic of',",
            "        'TH': 'Thailand',",
            "        'TL': 'Timor-Leste',",
            "        'TG': 'Togo',",
            "        'TK': 'Tokelau',",
            "        'TO': 'Tonga',",
            "        'TT': 'Trinidad and Tobago',",
            "        'TN': 'Tunisia',",
            "        'TR': 'Turkey',",
            "        'TM': 'Turkmenistan',",
            "        'TC': 'Turks and Caicos Islands',",
            "        'TV': 'Tuvalu',",
            "        'UG': 'Uganda',",
            "        'UA': 'Ukraine',",
            "        'AE': 'United Arab Emirates',",
            "        'GB': 'United Kingdom',",
            "        'US': 'United States',",
            "        'UM': 'United States Minor Outlying Islands',",
            "        'UY': 'Uruguay',",
            "        'UZ': 'Uzbekistan',",
            "        'VU': 'Vanuatu',",
            "        'VE': 'Venezuela, Bolivarian Republic of',",
            "        'VN': 'Viet Nam',",
            "        'VG': 'Virgin Islands, British',",
            "        'VI': 'Virgin Islands, U.S.',",
            "        'WF': 'Wallis and Futuna',",
            "        'EH': 'Western Sahara',",
            "        'YE': 'Yemen',",
            "        'ZM': 'Zambia',",
            "        'ZW': 'Zimbabwe',",
            "        # Not ISO 3166 codes, but used for IP blocks",
            "        'AP': 'Asia/Pacific Region',",
            "        'EU': 'Europe',",
            "    }",
            "",
            "    @classmethod",
            "    def short2full(cls, code):",
            "        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"",
            "        return cls._country_map.get(code.upper())",
            "",
            "",
            "class GeoUtils:",
            "    # Major IPv4 address blocks per country",
            "    _country_ip_map = {",
            "        'AD': '46.172.224.0/19',",
            "        'AE': '94.200.0.0/13',",
            "        'AF': '149.54.0.0/17',",
            "        'AG': '209.59.64.0/18',",
            "        'AI': '204.14.248.0/21',",
            "        'AL': '46.99.0.0/16',",
            "        'AM': '46.70.0.0/15',",
            "        'AO': '105.168.0.0/13',",
            "        'AP': '182.50.184.0/21',",
            "        'AQ': '23.154.160.0/24',",
            "        'AR': '181.0.0.0/12',",
            "        'AS': '202.70.112.0/20',",
            "        'AT': '77.116.0.0/14',",
            "        'AU': '1.128.0.0/11',",
            "        'AW': '181.41.0.0/18',",
            "        'AX': '185.217.4.0/22',",
            "        'AZ': '5.197.0.0/16',",
            "        'BA': '31.176.128.0/17',",
            "        'BB': '65.48.128.0/17',",
            "        'BD': '114.130.0.0/16',",
            "        'BE': '57.0.0.0/8',",
            "        'BF': '102.178.0.0/15',",
            "        'BG': '95.42.0.0/15',",
            "        'BH': '37.131.0.0/17',",
            "        'BI': '154.117.192.0/18',",
            "        'BJ': '137.255.0.0/16',",
            "        'BL': '185.212.72.0/23',",
            "        'BM': '196.12.64.0/18',",
            "        'BN': '156.31.0.0/16',",
            "        'BO': '161.56.0.0/16',",
            "        'BQ': '161.0.80.0/20',",
            "        'BR': '191.128.0.0/12',",
            "        'BS': '24.51.64.0/18',",
            "        'BT': '119.2.96.0/19',",
            "        'BW': '168.167.0.0/16',",
            "        'BY': '178.120.0.0/13',",
            "        'BZ': '179.42.192.0/18',",
            "        'CA': '99.224.0.0/11',",
            "        'CD': '41.243.0.0/16',",
            "        'CF': '197.242.176.0/21',",
            "        'CG': '160.113.0.0/16',",
            "        'CH': '85.0.0.0/13',",
            "        'CI': '102.136.0.0/14',",
            "        'CK': '202.65.32.0/19',",
            "        'CL': '152.172.0.0/14',",
            "        'CM': '102.244.0.0/14',",
            "        'CN': '36.128.0.0/10',",
            "        'CO': '181.240.0.0/12',",
            "        'CR': '201.192.0.0/12',",
            "        'CU': '152.206.0.0/15',",
            "        'CV': '165.90.96.0/19',",
            "        'CW': '190.88.128.0/17',",
            "        'CY': '31.153.0.0/16',",
            "        'CZ': '88.100.0.0/14',",
            "        'DE': '53.0.0.0/8',",
            "        'DJ': '197.241.0.0/17',",
            "        'DK': '87.48.0.0/12',",
            "        'DM': '192.243.48.0/20',",
            "        'DO': '152.166.0.0/15',",
            "        'DZ': '41.96.0.0/12',",
            "        'EC': '186.68.0.0/15',",
            "        'EE': '90.190.0.0/15',",
            "        'EG': '156.160.0.0/11',",
            "        'ER': '196.200.96.0/20',",
            "        'ES': '88.0.0.0/11',",
            "        'ET': '196.188.0.0/14',",
            "        'EU': '2.16.0.0/13',",
            "        'FI': '91.152.0.0/13',",
            "        'FJ': '144.120.0.0/16',",
            "        'FK': '80.73.208.0/21',",
            "        'FM': '119.252.112.0/20',",
            "        'FO': '88.85.32.0/19',",
            "        'FR': '90.0.0.0/9',",
            "        'GA': '41.158.0.0/15',",
            "        'GB': '25.0.0.0/8',",
            "        'GD': '74.122.88.0/21',",
            "        'GE': '31.146.0.0/16',",
            "        'GF': '161.22.64.0/18',",
            "        'GG': '62.68.160.0/19',",
            "        'GH': '154.160.0.0/12',",
            "        'GI': '95.164.0.0/16',",
            "        'GL': '88.83.0.0/19',",
            "        'GM': '160.182.0.0/15',",
            "        'GN': '197.149.192.0/18',",
            "        'GP': '104.250.0.0/19',",
            "        'GQ': '105.235.224.0/20',",
            "        'GR': '94.64.0.0/13',",
            "        'GT': '168.234.0.0/16',",
            "        'GU': '168.123.0.0/16',",
            "        'GW': '197.214.80.0/20',",
            "        'GY': '181.41.64.0/18',",
            "        'HK': '113.252.0.0/14',",
            "        'HN': '181.210.0.0/16',",
            "        'HR': '93.136.0.0/13',",
            "        'HT': '148.102.128.0/17',",
            "        'HU': '84.0.0.0/14',",
            "        'ID': '39.192.0.0/10',",
            "        'IE': '87.32.0.0/12',",
            "        'IL': '79.176.0.0/13',",
            "        'IM': '5.62.80.0/20',",
            "        'IN': '117.192.0.0/10',",
            "        'IO': '203.83.48.0/21',",
            "        'IQ': '37.236.0.0/14',",
            "        'IR': '2.176.0.0/12',",
            "        'IS': '82.221.0.0/16',",
            "        'IT': '79.0.0.0/10',",
            "        'JE': '87.244.64.0/18',",
            "        'JM': '72.27.0.0/17',",
            "        'JO': '176.29.0.0/16',",
            "        'JP': '133.0.0.0/8',",
            "        'KE': '105.48.0.0/12',",
            "        'KG': '158.181.128.0/17',",
            "        'KH': '36.37.128.0/17',",
            "        'KI': '103.25.140.0/22',",
            "        'KM': '197.255.224.0/20',",
            "        'KN': '198.167.192.0/19',",
            "        'KP': '175.45.176.0/22',",
            "        'KR': '175.192.0.0/10',",
            "        'KW': '37.36.0.0/14',",
            "        'KY': '64.96.0.0/15',",
            "        'KZ': '2.72.0.0/13',",
            "        'LA': '115.84.64.0/18',",
            "        'LB': '178.135.0.0/16',",
            "        'LC': '24.92.144.0/20',",
            "        'LI': '82.117.0.0/19',",
            "        'LK': '112.134.0.0/15',",
            "        'LR': '102.183.0.0/16',",
            "        'LS': '129.232.0.0/17',",
            "        'LT': '78.56.0.0/13',",
            "        'LU': '188.42.0.0/16',",
            "        'LV': '46.109.0.0/16',",
            "        'LY': '41.252.0.0/14',",
            "        'MA': '105.128.0.0/11',",
            "        'MC': '88.209.64.0/18',",
            "        'MD': '37.246.0.0/16',",
            "        'ME': '178.175.0.0/17',",
            "        'MF': '74.112.232.0/21',",
            "        'MG': '154.126.0.0/17',",
            "        'MH': '117.103.88.0/21',",
            "        'MK': '77.28.0.0/15',",
            "        'ML': '154.118.128.0/18',",
            "        'MM': '37.111.0.0/17',",
            "        'MN': '49.0.128.0/17',",
            "        'MO': '60.246.0.0/16',",
            "        'MP': '202.88.64.0/20',",
            "        'MQ': '109.203.224.0/19',",
            "        'MR': '41.188.64.0/18',",
            "        'MS': '208.90.112.0/22',",
            "        'MT': '46.11.0.0/16',",
            "        'MU': '105.16.0.0/12',",
            "        'MV': '27.114.128.0/18',",
            "        'MW': '102.70.0.0/15',",
            "        'MX': '187.192.0.0/11',",
            "        'MY': '175.136.0.0/13',",
            "        'MZ': '197.218.0.0/15',",
            "        'NA': '41.182.0.0/16',",
            "        'NC': '101.101.0.0/18',",
            "        'NE': '197.214.0.0/18',",
            "        'NF': '203.17.240.0/22',",
            "        'NG': '105.112.0.0/12',",
            "        'NI': '186.76.0.0/15',",
            "        'NL': '145.96.0.0/11',",
            "        'NO': '84.208.0.0/13',",
            "        'NP': '36.252.0.0/15',",
            "        'NR': '203.98.224.0/19',",
            "        'NU': '49.156.48.0/22',",
            "        'NZ': '49.224.0.0/14',",
            "        'OM': '5.36.0.0/15',",
            "        'PA': '186.72.0.0/15',",
            "        'PE': '186.160.0.0/14',",
            "        'PF': '123.50.64.0/18',",
            "        'PG': '124.240.192.0/19',",
            "        'PH': '49.144.0.0/13',",
            "        'PK': '39.32.0.0/11',",
            "        'PL': '83.0.0.0/11',",
            "        'PM': '70.36.0.0/20',",
            "        'PR': '66.50.0.0/16',",
            "        'PS': '188.161.0.0/16',",
            "        'PT': '85.240.0.0/13',",
            "        'PW': '202.124.224.0/20',",
            "        'PY': '181.120.0.0/14',",
            "        'QA': '37.210.0.0/15',",
            "        'RE': '102.35.0.0/16',",
            "        'RO': '79.112.0.0/13',",
            "        'RS': '93.86.0.0/15',",
            "        'RU': '5.136.0.0/13',",
            "        'RW': '41.186.0.0/16',",
            "        'SA': '188.48.0.0/13',",
            "        'SB': '202.1.160.0/19',",
            "        'SC': '154.192.0.0/11',",
            "        'SD': '102.120.0.0/13',",
            "        'SE': '78.64.0.0/12',",
            "        'SG': '8.128.0.0/10',",
            "        'SI': '188.196.0.0/14',",
            "        'SK': '78.98.0.0/15',",
            "        'SL': '102.143.0.0/17',",
            "        'SM': '89.186.32.0/19',",
            "        'SN': '41.82.0.0/15',",
            "        'SO': '154.115.192.0/18',",
            "        'SR': '186.179.128.0/17',",
            "        'SS': '105.235.208.0/21',",
            "        'ST': '197.159.160.0/19',",
            "        'SV': '168.243.0.0/16',",
            "        'SX': '190.102.0.0/20',",
            "        'SY': '5.0.0.0/16',",
            "        'SZ': '41.84.224.0/19',",
            "        'TC': '65.255.48.0/20',",
            "        'TD': '154.68.128.0/19',",
            "        'TG': '196.168.0.0/14',",
            "        'TH': '171.96.0.0/13',",
            "        'TJ': '85.9.128.0/18',",
            "        'TK': '27.96.24.0/21',",
            "        'TL': '180.189.160.0/20',",
            "        'TM': '95.85.96.0/19',",
            "        'TN': '197.0.0.0/11',",
            "        'TO': '175.176.144.0/21',",
            "        'TR': '78.160.0.0/11',",
            "        'TT': '186.44.0.0/15',",
            "        'TV': '202.2.96.0/19',",
            "        'TW': '120.96.0.0/11',",
            "        'TZ': '156.156.0.0/14',",
            "        'UA': '37.52.0.0/14',",
            "        'UG': '102.80.0.0/13',",
            "        'US': '6.0.0.0/8',",
            "        'UY': '167.56.0.0/13',",
            "        'UZ': '84.54.64.0/18',",
            "        'VA': '212.77.0.0/19',",
            "        'VC': '207.191.240.0/21',",
            "        'VE': '186.88.0.0/13',",
            "        'VG': '66.81.192.0/20',",
            "        'VI': '146.226.0.0/16',",
            "        'VN': '14.160.0.0/11',",
            "        'VU': '202.80.32.0/20',",
            "        'WF': '117.20.32.0/21',",
            "        'WS': '202.4.32.0/19',",
            "        'YE': '134.35.0.0/16',",
            "        'YT': '41.242.116.0/22',",
            "        'ZA': '41.0.0.0/11',",
            "        'ZM': '102.144.0.0/13',",
            "        'ZW': '102.177.192.0/18',",
            "    }",
            "",
            "    @classmethod",
            "    def random_ipv4(cls, code_or_block):",
            "        if len(code_or_block) == 2:",
            "            block = cls._country_ip_map.get(code_or_block.upper())",
            "            if not block:",
            "                return None",
            "        else:",
            "            block = code_or_block",
            "        addr, preflen = block.split('/')",
            "        addr_min = struct.unpack('!L', socket.inet_aton(addr))[0]",
            "        addr_max = addr_min | (0xffffffff >> int(preflen))",
            "        return str(socket.inet_ntoa(",
            "            struct.pack('!L', random.randint(addr_min, addr_max))))",
            "",
            "",
            "# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is",
            "# released into Public Domain",
            "# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387",
            "",
            "def long_to_bytes(n, blocksize=0):",
            "    \"\"\"long_to_bytes(n:long, blocksize:int) : string",
            "    Convert a long integer to a byte string.",
            "",
            "    If optional blocksize is given and greater than zero, pad the front of the",
            "    byte string with binary zeros so that the length is a multiple of",
            "    blocksize.",
            "    \"\"\"",
            "    # after much testing, this algorithm was deemed to be the fastest",
            "    s = b''",
            "    n = int(n)",
            "    while n > 0:",
            "        s = struct.pack('>I', n & 0xffffffff) + s",
            "        n = n >> 32",
            "    # strip off leading zeros",
            "    for i in range(len(s)):",
            "        if s[i] != b'\\000'[0]:",
            "            break",
            "    else:",
            "        # only happens when n == 0",
            "        s = b'\\000'",
            "        i = 0",
            "    s = s[i:]",
            "    # add back some pad bytes.  this could be done more efficiently w.r.t. the",
            "    # de-padding being done above, but sigh...",
            "    if blocksize > 0 and len(s) % blocksize:",
            "        s = (blocksize - len(s) % blocksize) * b'\\000' + s",
            "    return s",
            "",
            "",
            "def bytes_to_long(s):",
            "    \"\"\"bytes_to_long(string) : long",
            "    Convert a byte string to a long integer.",
            "",
            "    This is (essentially) the inverse of long_to_bytes().",
            "    \"\"\"",
            "    acc = 0",
            "    length = len(s)",
            "    if length % 4:",
            "        extra = (4 - length % 4)",
            "        s = b'\\000' * extra + s",
            "        length = length + extra",
            "    for i in range(0, length, 4):",
            "        acc = (acc << 32) + struct.unpack('>I', s[i:i + 4])[0]",
            "    return acc",
            "",
            "",
            "def ohdave_rsa_encrypt(data, exponent, modulus):",
            "    '''",
            "    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/",
            "",
            "    Input:",
            "        data: data to encrypt, bytes-like object",
            "        exponent, modulus: parameter e and N of RSA algorithm, both integer",
            "    Output: hex string of encrypted data",
            "",
            "    Limitation: supports one block encryption only",
            "    '''",
            "",
            "    payload = int(binascii.hexlify(data[::-1]), 16)",
            "    encrypted = pow(payload, exponent, modulus)",
            "    return '%x' % encrypted",
            "",
            "",
            "def pkcs1pad(data, length):",
            "    \"\"\"",
            "    Padding input data with PKCS#1 scheme",
            "",
            "    @param {int[]} data        input data",
            "    @param {int}   length      target length",
            "    @returns {int[]}           padded data",
            "    \"\"\"",
            "    if len(data) > length - 11:",
            "        raise ValueError('Input data too long for PKCS#1 padding')",
            "",
            "    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]",
            "    return [0, 2] + pseudo_random + [0] + data",
            "",
            "",
            "def _base_n_table(n, table):",
            "    if not table and not n:",
            "        raise ValueError('Either table or n must be specified')",
            "    table = (table or '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')[:n]",
            "",
            "    if n and n != len(table):",
            "        raise ValueError(f'base {n} exceeds table length {len(table)}')",
            "    return table",
            "",
            "",
            "def encode_base_n(num, n=None, table=None):",
            "    \"\"\"Convert given int to a base-n string\"\"\"",
            "    table = _base_n_table(n, table)",
            "    if not num:",
            "        return table[0]",
            "",
            "    result, base = '', len(table)",
            "    while num:",
            "        result = table[num % base] + result",
            "        num = num // base",
            "    return result",
            "",
            "",
            "def decode_base_n(string, n=None, table=None):",
            "    \"\"\"Convert given base-n string to int\"\"\"",
            "    table = {char: index for index, char in enumerate(_base_n_table(n, table))}",
            "    result, base = 0, len(table)",
            "    for char in string:",
            "        result = result * base + table[char]",
            "    return result",
            "",
            "",
            "def decode_packed_codes(code):",
            "    mobj = re.search(PACKED_CODES_RE, code)",
            "    obfuscated_code, base, count, symbols = mobj.groups()",
            "    base = int(base)",
            "    count = int(count)",
            "    symbols = symbols.split('|')",
            "    symbol_table = {}",
            "",
            "    while count:",
            "        count -= 1",
            "        base_n_count = encode_base_n(count, base)",
            "        symbol_table[base_n_count] = symbols[count] or base_n_count",
            "",
            "    return re.sub(",
            "        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],",
            "        obfuscated_code)",
            "",
            "",
            "def caesar(s, alphabet, shift):",
            "    if shift == 0:",
            "        return s",
            "    l = len(alphabet)",
            "    return ''.join(",
            "        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c",
            "        for c in s)",
            "",
            "",
            "def rot47(s):",
            "    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)",
            "",
            "",
            "def parse_m3u8_attributes(attrib):",
            "    info = {}",
            "    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):",
            "        if val.startswith('\"'):",
            "            val = val[1:-1]",
            "        info[key] = val",
            "    return info",
            "",
            "",
            "def urshift(val, n):",
            "    return val >> n if val >= 0 else (val + 0x100000000) >> n",
            "",
            "",
            "def write_xattr(path, key, value):",
            "    # Windows: Write xattrs to NTFS Alternate Data Streams:",
            "    # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29",
            "    if compat_os_name == 'nt':",
            "        assert ':' not in key",
            "        assert os.path.exists(path)",
            "",
            "        try:",
            "            with open(f'{path}:{key}', 'wb') as f:",
            "                f.write(value)",
            "        except OSError as e:",
            "            raise XAttrMetadataError(e.errno, e.strerror)",
            "        return",
            "",
            "    # UNIX Method 1. Use os.setxattr/xattrs/pyxattrs modules",
            "",
            "    setxattr = None",
            "    if callable(getattr(os, 'setxattr', None)):",
            "        setxattr = os.setxattr",
            "    elif getattr(xattr, '_yt_dlp__identifier', None) == 'pyxattr':",
            "        # Unicode arguments are not supported in pyxattr until version 0.5.0",
            "        # See https://github.com/ytdl-org/youtube-dl/issues/5498",
            "        if version_tuple(xattr.__version__) >= (0, 5, 0):",
            "            setxattr = xattr.set",
            "    elif xattr:",
            "        setxattr = xattr.setxattr",
            "",
            "    if setxattr:",
            "        try:",
            "            setxattr(path, key, value)",
            "        except OSError as e:",
            "            raise XAttrMetadataError(e.errno, e.strerror)",
            "        return",
            "",
            "    # UNIX Method 2. Use setfattr/xattr executables",
            "    exe = ('setfattr' if check_executable('setfattr', ['--version'])",
            "           else 'xattr' if check_executable('xattr', ['-h']) else None)",
            "    if not exe:",
            "        raise XAttrUnavailableError(",
            "            'Couldn\\'t find a tool to set the xattrs. Install either the \"xattr\" or \"pyxattr\" Python modules or the '",
            "            + ('\"xattr\" binary' if sys.platform != 'linux' else 'GNU \"attr\" package (which contains the \"setfattr\" tool)'))",
            "",
            "    value = value.decode()",
            "    try:",
            "        _, stderr, returncode = Popen.run(",
            "            [exe, '-w', key, value, path] if exe == 'xattr' else [exe, '-n', key, '-v', value, path],",
            "            text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)",
            "    except OSError as e:",
            "        raise XAttrMetadataError(e.errno, e.strerror)",
            "    if returncode:",
            "        raise XAttrMetadataError(returncode, stderr)",
            "",
            "",
            "def random_birthday(year_field, month_field, day_field):",
            "    start_date = dt.date(1950, 1, 1)",
            "    end_date = dt.date(1995, 12, 31)",
            "    offset = random.randint(0, (end_date - start_date).days)",
            "    random_date = start_date + dt.timedelta(offset)",
            "    return {",
            "        year_field: str(random_date.year),",
            "        month_field: str(random_date.month),",
            "        day_field: str(random_date.day),",
            "    }",
            "",
            "",
            "def find_available_port(interface=''):",
            "    try:",
            "        with socket.socket() as sock:",
            "            sock.bind((interface, 0))",
            "            return sock.getsockname()[1]",
            "    except OSError:",
            "        return None",
            "",
            "",
            "# Templates for internet shortcut files, which are plain text files.",
            "DOT_URL_LINK_TEMPLATE = '''\\",
            "[InternetShortcut]",
            "URL=%(url)s",
            "'''",
            "",
            "DOT_WEBLOC_LINK_TEMPLATE = '''\\",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
            "<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">",
            "<plist version=\"1.0\">",
            "<dict>",
            "\\t<key>URL</key>",
            "\\t<string>%(url)s</string>",
            "</dict>",
            "</plist>",
            "'''",
            "",
            "DOT_DESKTOP_LINK_TEMPLATE = '''\\",
            "[Desktop Entry]",
            "Encoding=UTF-8",
            "Name=%(filename)s",
            "Type=Link",
            "URL=%(url)s",
            "Icon=text-html",
            "'''",
            "",
            "LINK_TEMPLATES = {",
            "    'url': DOT_URL_LINK_TEMPLATE,",
            "    'desktop': DOT_DESKTOP_LINK_TEMPLATE,",
            "    'webloc': DOT_WEBLOC_LINK_TEMPLATE,",
            "}",
            "",
            "",
            "def iri_to_uri(iri):",
            "    \"\"\"",
            "    Converts an IRI (Internationalized Resource Identifier, allowing Unicode characters) to a URI (Uniform Resource Identifier, ASCII-only).",
            "",
            "    The function doesn't add an additional layer of escaping; e.g., it doesn't escape `%3C` as `%253C`. Instead, it percent-escapes characters with an underlying UTF-8 encoding *besides* those already escaped, leaving the URI intact.",
            "    \"\"\"",
            "",
            "    iri_parts = urllib.parse.urlparse(iri)",
            "",
            "    if '[' in iri_parts.netloc:",
            "        raise ValueError('IPv6 URIs are not, yet, supported.')",
            "        # Querying `.netloc`, when there's only one bracket, also raises a ValueError.",
            "",
            "    # The `safe` argument values, that the following code uses, contain the characters that should not be percent-encoded. Everything else but letters, digits and '_.-' will be percent-encoded with an underlying UTF-8 encoding. Everything already percent-encoded will be left as is.",
            "",
            "    net_location = ''",
            "    if iri_parts.username:",
            "        net_location += urllib.parse.quote(iri_parts.username, safe=r\"!$%&'()*+,~\")",
            "        if iri_parts.password is not None:",
            "            net_location += ':' + urllib.parse.quote(iri_parts.password, safe=r\"!$%&'()*+,~\")",
            "        net_location += '@'",
            "",
            "    net_location += iri_parts.hostname.encode('idna').decode()  # Punycode for Unicode hostnames.",
            "    # The 'idna' encoding produces ASCII text.",
            "    if iri_parts.port is not None and iri_parts.port != 80:",
            "        net_location += ':' + str(iri_parts.port)",
            "",
            "    return urllib.parse.urlunparse(",
            "        (iri_parts.scheme,",
            "            net_location,",
            "",
            "            urllib.parse.quote_plus(iri_parts.path, safe=r\"!$%&'()*+,/:;=@|~\"),",
            "",
            "            # Unsure about the `safe` argument, since this is a legacy way of handling parameters.",
            "            urllib.parse.quote_plus(iri_parts.params, safe=r\"!$%&'()*+,/:;=@|~\"),",
            "",
            "            # Not totally sure about the `safe` argument, since the source does not explicitly mention the query URI component.",
            "            urllib.parse.quote_plus(iri_parts.query, safe=r\"!$%&'()*+,/:;=?@{|}~\"),",
            "",
            "            urllib.parse.quote_plus(iri_parts.fragment, safe=r\"!#$%&'()*+,/:;=?@{|}~\")))",
            "",
            "    # Source for `safe` arguments: https://url.spec.whatwg.org/#percent-encoded-bytes.",
            "",
            "",
            "def to_high_limit_path(path):",
            "    if sys.platform in ['win32', 'cygwin']:",
            "        # Work around MAX_PATH limitation on Windows. The maximum allowed length for the individual path segments may still be quite limited.",
            "        return '\\\\\\\\?\\\\' + os.path.abspath(path)",
            "",
            "    return path",
            "",
            "",
            "def format_field(obj, field=None, template='%s', ignore=NO_DEFAULT, default='', func=IDENTITY):",
            "    val = traversal.traverse_obj(obj, *variadic(field))",
            "    if not val if ignore is NO_DEFAULT else val in variadic(ignore):",
            "        return default",
            "    return template % func(val)",
            "",
            "",
            "def clean_podcast_url(url):",
            "    url = re.sub(r'''(?x)",
            "        (?:",
            "            (?:",
            "                chtbl\\.com/track|",
            "                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/",
            "                play\\.podtrac\\.com|",
            "                chrt\\.fm/track|",
            "                mgln\\.ai/e",
            "            )(?:/[^/.]+)?|",
            "            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure",
            "            flex\\.acast\\.com|",
            "            pd(?:",
            "                cn\\.co| # https://podcorn.com/analytics-prefix/",
            "                st\\.fm # https://podsights.com/docs/",
            "            )/e|",
            "            [0-9]\\.gum\\.fm|",
            "            pscrb\\.fm/rss/p",
            "        )/''', '', url)",
            "    return re.sub(r'^\\w+://(\\w+://)', r'\\1', url)",
            "",
            "",
            "_HEX_TABLE = '0123456789abcdef'",
            "",
            "",
            "def random_uuidv4():",
            "    return re.sub(r'[xy]', lambda x: _HEX_TABLE[random.randint(0, 15)], 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx')",
            "",
            "",
            "def make_dir(path, to_screen=None):",
            "    try:",
            "        dn = os.path.dirname(path)",
            "        if dn:",
            "            os.makedirs(dn, exist_ok=True)",
            "        return True",
            "    except OSError as err:",
            "        if callable(to_screen) is not None:",
            "            to_screen(f'unable to create directory {err}')",
            "        return False",
            "",
            "",
            "def get_executable_path():",
            "    from ..update import _get_variant_and_executable_path",
            "",
            "    return os.path.dirname(os.path.abspath(_get_variant_and_executable_path()[1]))",
            "",
            "",
            "def get_user_config_dirs(package_name):",
            "    # .config (e.g. ~/.config/package_name)",
            "    xdg_config_home = os.getenv('XDG_CONFIG_HOME') or compat_expanduser('~/.config')",
            "    yield os.path.join(xdg_config_home, package_name)",
            "",
            "    # appdata (%APPDATA%/package_name)",
            "    appdata_dir = os.getenv('appdata')",
            "    if appdata_dir:",
            "        yield os.path.join(appdata_dir, package_name)",
            "",
            "    # home (~/.package_name)",
            "    yield os.path.join(compat_expanduser('~'), f'.{package_name}')",
            "",
            "",
            "def get_system_config_dirs(package_name):",
            "    # /etc/package_name",
            "    yield os.path.join('/etc', package_name)",
            "",
            "",
            "def time_seconds(**kwargs):",
            "    \"\"\"",
            "    Returns TZ-aware time in seconds since the epoch (1970-01-01T00:00:00Z)",
            "    \"\"\"",
            "    return time.time() + dt.timedelta(**kwargs).total_seconds()",
            "",
            "",
            "# create a JSON Web Signature (jws) with HS256 algorithm",
            "# the resulting format is in JWS Compact Serialization",
            "# implemented following JWT https://www.rfc-editor.org/rfc/rfc7519.html",
            "# implemented following JWS https://www.rfc-editor.org/rfc/rfc7515.html",
            "def jwt_encode_hs256(payload_data, key, headers={}):",
            "    header_data = {",
            "        'alg': 'HS256',",
            "        'typ': 'JWT',",
            "    }",
            "    if headers:",
            "        header_data.update(headers)",
            "    header_b64 = base64.b64encode(json.dumps(header_data).encode())",
            "    payload_b64 = base64.b64encode(json.dumps(payload_data).encode())",
            "    h = hmac.new(key.encode(), header_b64 + b'.' + payload_b64, hashlib.sha256)",
            "    signature_b64 = base64.b64encode(h.digest())",
            "    token = header_b64 + b'.' + payload_b64 + b'.' + signature_b64",
            "    return token",
            "",
            "",
            "# can be extended in future to verify the signature and parse header and return the algorithm used if it's not HS256",
            "def jwt_decode_hs256(jwt):",
            "    header_b64, payload_b64, signature_b64 = jwt.split('.')",
            "    # add trailing ='s that may have been stripped, superfluous ='s are ignored",
            "    payload_data = json.loads(base64.urlsafe_b64decode(f'{payload_b64}==='))",
            "    return payload_data",
            "",
            "",
            "WINDOWS_VT_MODE = False if compat_os_name == 'nt' else None",
            "",
            "",
            "@functools.cache",
            "def supports_terminal_sequences(stream):",
            "    if compat_os_name == 'nt':",
            "        if not WINDOWS_VT_MODE:",
            "            return False",
            "    elif not os.getenv('TERM'):",
            "        return False",
            "    try:",
            "        return stream.isatty()",
            "    except BaseException:",
            "        return False",
            "",
            "",
            "def windows_enable_vt_mode():",
            "    \"\"\"Ref: https://bugs.python.org/issue30075 \"\"\"",
            "    if get_windows_version() < (10, 0, 10586):",
            "        return",
            "",
            "    import ctypes",
            "    import ctypes.wintypes",
            "    import msvcrt",
            "",
            "    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004",
            "",
            "    dll = ctypes.WinDLL('kernel32', use_last_error=False)",
            "    handle = os.open('CONOUT$', os.O_RDWR)",
            "    try:",
            "        h_out = ctypes.wintypes.HANDLE(msvcrt.get_osfhandle(handle))",
            "        dw_original_mode = ctypes.wintypes.DWORD()",
            "        success = dll.GetConsoleMode(h_out, ctypes.byref(dw_original_mode))",
            "        if not success:",
            "            raise Exception('GetConsoleMode failed')",
            "",
            "        success = dll.SetConsoleMode(h_out, ctypes.wintypes.DWORD(",
            "            dw_original_mode.value | ENABLE_VIRTUAL_TERMINAL_PROCESSING))",
            "        if not success:",
            "            raise Exception('SetConsoleMode failed')",
            "    finally:",
            "        os.close(handle)",
            "",
            "    global WINDOWS_VT_MODE",
            "    WINDOWS_VT_MODE = True",
            "    supports_terminal_sequences.cache_clear()",
            "",
            "",
            "_terminal_sequences_re = re.compile('\\033\\\\[[^m]+m')",
            "",
            "",
            "def remove_terminal_sequences(string):",
            "    return _terminal_sequences_re.sub('', string)",
            "",
            "",
            "def number_of_digits(number):",
            "    return len('%d' % number)",
            "",
            "",
            "def join_nonempty(*values, delim='-', from_dict=None):",
            "    if from_dict is not None:",
            "        values = (traversal.traverse_obj(from_dict, variadic(v)) for v in values)",
            "    return delim.join(map(str, filter(None, values)))",
            "",
            "",
            "def scale_thumbnails_to_max_format_width(formats, thumbnails, url_width_re):",
            "    \"\"\"",
            "    Find the largest format dimensions in terms of video width and, for each thumbnail:",
            "    * Modify the URL: Match the width with the provided regex and replace with the former width",
            "    * Update dimensions",
            "",
            "    This function is useful with video services that scale the provided thumbnails on demand",
            "    \"\"\"",
            "    _keys = ('width', 'height')",
            "    max_dimensions = max(",
            "        (tuple(format.get(k) or 0 for k in _keys) for format in formats),",
            "        default=(0, 0))",
            "    if not max_dimensions[0]:",
            "        return thumbnails",
            "    return [",
            "        merge_dicts(",
            "            {'url': re.sub(url_width_re, str(max_dimensions[0]), thumbnail['url'])},",
            "            dict(zip(_keys, max_dimensions)), thumbnail)",
            "        for thumbnail in thumbnails",
            "    ]",
            "",
            "",
            "def parse_http_range(range):",
            "    \"\"\" Parse value of \"Range\" or \"Content-Range\" HTTP header into tuple. \"\"\"",
            "    if not range:",
            "        return None, None, None",
            "    crg = re.search(r'bytes[ =](\\d+)-(\\d+)?(?:/(\\d+))?', range)",
            "    if not crg:",
            "        return None, None, None",
            "    return int(crg.group(1)), int_or_none(crg.group(2)), int_or_none(crg.group(3))",
            "",
            "",
            "def read_stdin(what):",
            "    if what:",
            "        eof = 'Ctrl+Z' if compat_os_name == 'nt' else 'Ctrl+D'",
            "        write_string(f'Reading {what} from STDIN - EOF ({eof}) to end:\\n')",
            "    return sys.stdin",
            "",
            "",
            "def determine_file_encoding(data):",
            "    \"\"\"",
            "    Detect the text encoding used",
            "    @returns (encoding, bytes to skip)",
            "    \"\"\"",
            "",
            "    # BOM marks are given priority over declarations",
            "    for bom, enc in BOMS:",
            "        if data.startswith(bom):",
            "            return enc, len(bom)",
            "",
            "    # Strip off all null bytes to match even when UTF-16 or UTF-32 is used.",
            "    # We ignore the endianness to get a good enough match",
            "    data = data.replace(b'\\0', b'')",
            "    mobj = re.match(rb'(?m)^#\\s*coding\\s*:\\s*(\\S+)\\s*$', data)",
            "    return mobj.group(1).decode() if mobj else None, 0",
            "",
            "",
            "class Config:",
            "    own_args = None",
            "    parsed_args = None",
            "    filename = None",
            "    __initialized = False",
            "",
            "    def __init__(self, parser, label=None):",
            "        self.parser, self.label = parser, label",
            "        self._loaded_paths, self.configs = set(), []",
            "",
            "    def init(self, args=None, filename=None):",
            "        assert not self.__initialized",
            "        self.own_args, self.filename = args, filename",
            "        return self.load_configs()",
            "",
            "    def load_configs(self):",
            "        directory = ''",
            "        if self.filename:",
            "            location = os.path.realpath(self.filename)",
            "            directory = os.path.dirname(location)",
            "            if location in self._loaded_paths:",
            "                return False",
            "            self._loaded_paths.add(location)",
            "",
            "        self.__initialized = True",
            "        opts, _ = self.parser.parse_known_args(self.own_args)",
            "        self.parsed_args = self.own_args",
            "        for location in opts.config_locations or []:",
            "            if location == '-':",
            "                if location in self._loaded_paths:",
            "                    continue",
            "                self._loaded_paths.add(location)",
            "                self.append_config(shlex.split(read_stdin('options'), comments=True), label='stdin')",
            "                continue",
            "            location = os.path.join(directory, expand_path(location))",
            "            if os.path.isdir(location):",
            "                location = os.path.join(location, 'yt-dlp.conf')",
            "            if not os.path.exists(location):",
            "                self.parser.error(f'config location {location} does not exist')",
            "            self.append_config(self.read_file(location), location)",
            "        return True",
            "",
            "    def __str__(self):",
            "        label = join_nonempty(",
            "            self.label, 'config', f'\"{self.filename}\"' if self.filename else '',",
            "            delim=' ')",
            "        return join_nonempty(",
            "            self.own_args is not None and f'{label[0].upper()}{label[1:]}: {self.hide_login_info(self.own_args)}',",
            "            *(f'\\n{c}'.replace('\\n', '\\n| ')[1:] for c in self.configs),",
            "            delim='\\n')",
            "",
            "    @staticmethod",
            "    def read_file(filename, default=[]):",
            "        try:",
            "            optionf = open(filename, 'rb')",
            "        except OSError:",
            "            return default  # silently skip if file is not present",
            "        try:",
            "            enc, skip = determine_file_encoding(optionf.read(512))",
            "            optionf.seek(skip, io.SEEK_SET)",
            "        except OSError:",
            "            enc = None  # silently skip read errors",
            "        try:",
            "            # FIXME: https://github.com/ytdl-org/youtube-dl/commit/dfe5fa49aed02cf36ba9f743b11b0903554b5e56",
            "            contents = optionf.read().decode(enc or preferredencoding())",
            "            res = shlex.split(contents, comments=True)",
            "        except Exception as err:",
            "            raise ValueError(f'Unable to parse \"{filename}\": {err}')",
            "        finally:",
            "            optionf.close()",
            "        return res",
            "",
            "    @staticmethod",
            "    def hide_login_info(opts):",
            "        PRIVATE_OPTS = {'-p', '--password', '-u', '--username', '--video-password', '--ap-password', '--ap-username'}",
            "        eqre = re.compile('^(?P<key>' + ('|'.join(re.escape(po) for po in PRIVATE_OPTS)) + ')=.+$')",
            "",
            "        def _scrub_eq(o):",
            "            m = eqre.match(o)",
            "            if m:",
            "                return m.group('key') + '=PRIVATE'",
            "            else:",
            "                return o",
            "",
            "        opts = list(map(_scrub_eq, opts))",
            "        for idx, opt in enumerate(opts):",
            "            if opt in PRIVATE_OPTS and idx + 1 < len(opts):",
            "                opts[idx + 1] = 'PRIVATE'",
            "        return opts",
            "",
            "    def append_config(self, *args, label=None):",
            "        config = type(self)(self.parser, label)",
            "        config._loaded_paths = self._loaded_paths",
            "        if config.init(*args):",
            "            self.configs.append(config)",
            "",
            "    @property",
            "    def all_args(self):",
            "        for config in reversed(self.configs):",
            "            yield from config.all_args",
            "        yield from self.parsed_args or []",
            "",
            "    def parse_known_args(self, **kwargs):",
            "        return self.parser.parse_known_args(self.all_args, **kwargs)",
            "",
            "    def parse_args(self):",
            "        return self.parser.parse_args(self.all_args)",
            "",
            "",
            "def merge_headers(*dicts):",
            "    \"\"\"Merge dicts of http headers case insensitively, prioritizing the latter ones\"\"\"",
            "    return {k.title(): v for k, v in itertools.chain.from_iterable(map(dict.items, dicts))}",
            "",
            "",
            "def cached_method(f):",
            "    \"\"\"Cache a method\"\"\"",
            "    signature = inspect.signature(f)",
            "",
            "    @functools.wraps(f)",
            "    def wrapper(self, *args, **kwargs):",
            "        bound_args = signature.bind(self, *args, **kwargs)",
            "        bound_args.apply_defaults()",
            "        key = tuple(bound_args.arguments.values())[1:]",
            "",
            "        cache = vars(self).setdefault('_cached_method__cache', {}).setdefault(f.__name__, {})",
            "        if key not in cache:",
            "            cache[key] = f(self, *args, **kwargs)",
            "        return cache[key]",
            "    return wrapper",
            "",
            "",
            "class classproperty:",
            "    \"\"\"property access for class methods with optional caching\"\"\"",
            "    def __new__(cls, func=None, *args, **kwargs):",
            "        if not func:",
            "            return functools.partial(cls, *args, **kwargs)",
            "        return super().__new__(cls)",
            "",
            "    def __init__(self, func, *, cache=False):",
            "        functools.update_wrapper(self, func)",
            "        self.func = func",
            "        self._cache = {} if cache else None",
            "",
            "    def __get__(self, _, cls):",
            "        if self._cache is None:",
            "            return self.func(cls)",
            "        elif cls not in self._cache:",
            "            self._cache[cls] = self.func(cls)",
            "        return self._cache[cls]",
            "",
            "",
            "class function_with_repr:",
            "    def __init__(self, func, repr_=None):",
            "        functools.update_wrapper(self, func)",
            "        self.func, self.__repr = func, repr_",
            "",
            "    def __call__(self, *args, **kwargs):",
            "        return self.func(*args, **kwargs)",
            "",
            "    @classmethod",
            "    def set_repr(cls, repr_):",
            "        return functools.partial(cls, repr_=repr_)",
            "",
            "    def __repr__(self):",
            "        if self.__repr:",
            "            return self.__repr",
            "        return f'{self.func.__module__}.{self.func.__qualname__}'",
            "",
            "",
            "class Namespace(types.SimpleNamespace):",
            "    \"\"\"Immutable namespace\"\"\"",
            "",
            "    def __iter__(self):",
            "        return iter(self.__dict__.values())",
            "",
            "    @property",
            "    def items_(self):",
            "        return self.__dict__.items()",
            "",
            "",
            "MEDIA_EXTENSIONS = Namespace(",
            "    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),",
            "    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),",
            "    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),",
            "    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),",
            "    thumbnails=('jpg', 'png', 'webp'),",
            "    storyboards=('mhtml', ),",
            "    subtitles=('srt', 'vtt', 'ass', 'lrc'),",
            "    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),",
            ")",
            "MEDIA_EXTENSIONS.video += MEDIA_EXTENSIONS.common_video",
            "MEDIA_EXTENSIONS.audio += MEDIA_EXTENSIONS.common_audio",
            "",
            "KNOWN_EXTENSIONS = (*MEDIA_EXTENSIONS.video, *MEDIA_EXTENSIONS.audio, *MEDIA_EXTENSIONS.manifests)",
            "",
            "",
            "class RetryManager:",
            "    \"\"\"Usage:",
            "        for retry in RetryManager(...):",
            "            try:",
            "                ...",
            "            except SomeException as err:",
            "                retry.error = err",
            "                continue",
            "    \"\"\"",
            "    attempt, _error = 0, None",
            "",
            "    def __init__(self, _retries, _error_callback, **kwargs):",
            "        self.retries = _retries or 0",
            "        self.error_callback = functools.partial(_error_callback, **kwargs)",
            "",
            "    def _should_retry(self):",
            "        return self._error is not NO_DEFAULT and self.attempt <= self.retries",
            "",
            "    @property",
            "    def error(self):",
            "        if self._error is NO_DEFAULT:",
            "            return None",
            "        return self._error",
            "",
            "    @error.setter",
            "    def error(self, value):",
            "        self._error = value",
            "",
            "    def __iter__(self):",
            "        while self._should_retry():",
            "            self.error = NO_DEFAULT",
            "            self.attempt += 1",
            "            yield self",
            "            if self.error:",
            "                self.error_callback(self.error, self.attempt, self.retries)",
            "",
            "    @staticmethod",
            "    def report_retry(e, count, retries, *, sleep_func, info, warn, error=None, suffix=None):",
            "        \"\"\"Utility function for reporting retries\"\"\"",
            "        if count > retries:",
            "            if error:",
            "                return error(f'{e}. Giving up after {count - 1} retries') if count > 1 else error(str(e))",
            "            raise e",
            "",
            "        if not count:",
            "            return warn(e)",
            "        elif isinstance(e, ExtractorError):",
            "            e = remove_end(str_or_none(e.cause) or e.orig_msg, '.')",
            "        warn(f'{e}. Retrying{format_field(suffix, None, \" %s\")} ({count}/{retries})...')",
            "",
            "        delay = float_or_none(sleep_func(n=count - 1)) if callable(sleep_func) else sleep_func",
            "        if delay:",
            "            info(f'Sleeping {delay:.2f} seconds ...')",
            "            time.sleep(delay)",
            "",
            "",
            "def make_archive_id(ie, video_id):",
            "    ie_key = ie if isinstance(ie, str) else ie.ie_key()",
            "    return f'{ie_key.lower()} {video_id}'",
            "",
            "",
            "def truncate_string(s, left, right=0):",
            "    assert left > 3 and right >= 0",
            "    if s is None or len(s) <= left + right:",
            "        return s",
            "    return f'{s[:left - 3]}...{s[-right:] if right else \"\"}'",
            "",
            "",
            "def orderedSet_from_options(options, alias_dict, *, use_regex=False, start=None):",
            "    assert 'all' in alias_dict, '\"all\" alias is required'",
            "    requested = list(start or [])",
            "    for val in options:",
            "        discard = val.startswith('-')",
            "        if discard:",
            "            val = val[1:]",
            "",
            "        if val in alias_dict:",
            "            val = alias_dict[val] if not discard else [",
            "                i[1:] if i.startswith('-') else f'-{i}' for i in alias_dict[val]]",
            "            # NB: Do not allow regex in aliases for performance",
            "            requested = orderedSet_from_options(val, alias_dict, start=requested)",
            "            continue",
            "",
            "        current = (filter(re.compile(val, re.I).fullmatch, alias_dict['all']) if use_regex",
            "                   else [val] if val in alias_dict['all'] else None)",
            "        if current is None:",
            "            raise ValueError(val)",
            "",
            "        if discard:",
            "            for item in current:",
            "                while item in requested:",
            "                    requested.remove(item)",
            "        else:",
            "            requested.extend(current)",
            "",
            "    return orderedSet(requested)",
            "",
            "",
            "# TODO: Rewrite",
            "class FormatSorter:",
            "    regex = r' *((?P<reverse>\\+)?(?P<field>[a-zA-Z0-9_]+)((?P<separator>[~:])(?P<limit>.*?))?)? *$'",
            "",
            "    default = ('hidden', 'aud_or_vid', 'hasvid', 'ie_pref', 'lang', 'quality',",
            "               'res', 'fps', 'hdr:12', 'vcodec:vp9.2', 'channels', 'acodec',",
            "               'size', 'br', 'asr', 'proto', 'ext', 'hasaud', 'source', 'id')  # These must not be aliases",
            "    ytdl_default = ('hasaud', 'lang', 'quality', 'tbr', 'filesize', 'vbr',",
            "                    'height', 'width', 'proto', 'vext', 'abr', 'aext',",
            "                    'fps', 'fs_approx', 'source', 'id')",
            "",
            "    settings = {",
            "        'vcodec': {'type': 'ordered', 'regex': True,",
            "                   'order': ['av0?1', 'vp0?9.2', 'vp0?9', '[hx]265|he?vc?', '[hx]264|avc', 'vp0?8', 'mp4v|h263', 'theora', '', None, 'none']},",
            "        'acodec': {'type': 'ordered', 'regex': True,",
            "                   'order': ['[af]lac', 'wav|aiff', 'opus', 'vorbis|ogg', 'aac', 'mp?4a?', 'mp3', 'ac-?4', 'e-?a?c-?3', 'ac-?3', 'dts', '', None, 'none']},",
            "        'hdr': {'type': 'ordered', 'regex': True, 'field': 'dynamic_range',",
            "                'order': ['dv', '(hdr)?12', r'(hdr)?10\\+', '(hdr)?10', 'hlg', '', 'sdr', None]},",
            "        'proto': {'type': 'ordered', 'regex': True, 'field': 'protocol',",
            "                  'order': ['(ht|f)tps', '(ht|f)tp$', 'm3u8.*', '.*dash', 'websocket_frag', 'rtmpe?', '', 'mms|rtsp', 'ws|websocket', 'f4']},",
            "        'vext': {'type': 'ordered', 'field': 'video_ext',",
            "                 'order': ('mp4', 'mov', 'webm', 'flv', '', 'none'),",
            "                 'order_free': ('webm', 'mp4', 'mov', 'flv', '', 'none')},",
            "        'aext': {'type': 'ordered', 'regex': True, 'field': 'audio_ext',",
            "                 'order': ('m4a', 'aac', 'mp3', 'ogg', 'opus', 'web[am]', '', 'none'),",
            "                 'order_free': ('ogg', 'opus', 'web[am]', 'mp3', 'm4a', 'aac', '', 'none')},",
            "        'hidden': {'visible': False, 'forced': True, 'type': 'extractor', 'max': -1000},",
            "        'aud_or_vid': {'visible': False, 'forced': True, 'type': 'multiple',",
            "                       'field': ('vcodec', 'acodec'),",
            "                       'function': lambda it: int(any(v != 'none' for v in it))},",
            "        'ie_pref': {'priority': True, 'type': 'extractor'},",
            "        'hasvid': {'priority': True, 'field': 'vcodec', 'type': 'boolean', 'not_in_list': ('none',)},",
            "        'hasaud': {'field': 'acodec', 'type': 'boolean', 'not_in_list': ('none',)},",
            "        'lang': {'convert': 'float', 'field': 'language_preference', 'default': -1},",
            "        'quality': {'convert': 'float', 'default': -1},",
            "        'filesize': {'convert': 'bytes'},",
            "        'fs_approx': {'convert': 'bytes', 'field': 'filesize_approx'},",
            "        'id': {'convert': 'string', 'field': 'format_id'},",
            "        'height': {'convert': 'float_none'},",
            "        'width': {'convert': 'float_none'},",
            "        'fps': {'convert': 'float_none'},",
            "        'channels': {'convert': 'float_none', 'field': 'audio_channels'},",
            "        'tbr': {'convert': 'float_none'},",
            "        'vbr': {'convert': 'float_none'},",
            "        'abr': {'convert': 'float_none'},",
            "        'asr': {'convert': 'float_none'},",
            "        'source': {'convert': 'float', 'field': 'source_preference', 'default': -1},",
            "",
            "        'codec': {'type': 'combined', 'field': ('vcodec', 'acodec')},",
            "        'br': {'type': 'multiple', 'field': ('tbr', 'vbr', 'abr'), 'convert': 'float_none',",
            "               'function': lambda it: next(filter(None, it), None)},",
            "        'size': {'type': 'multiple', 'field': ('filesize', 'fs_approx'), 'convert': 'bytes',",
            "                 'function': lambda it: next(filter(None, it), None)},",
            "        'ext': {'type': 'combined', 'field': ('vext', 'aext')},",
            "        'res': {'type': 'multiple', 'field': ('height', 'width'),",
            "                'function': lambda it: (lambda l: min(l) if l else 0)(tuple(filter(None, it)))},",
            "",
            "        # Actual field names",
            "        'format_id': {'type': 'alias', 'field': 'id'},",
            "        'preference': {'type': 'alias', 'field': 'ie_pref'},",
            "        'language_preference': {'type': 'alias', 'field': 'lang'},",
            "        'source_preference': {'type': 'alias', 'field': 'source'},",
            "        'protocol': {'type': 'alias', 'field': 'proto'},",
            "        'filesize_approx': {'type': 'alias', 'field': 'fs_approx'},",
            "        'audio_channels': {'type': 'alias', 'field': 'channels'},",
            "",
            "        # Deprecated",
            "        'dimension': {'type': 'alias', 'field': 'res', 'deprecated': True},",
            "        'resolution': {'type': 'alias', 'field': 'res', 'deprecated': True},",
            "        'extension': {'type': 'alias', 'field': 'ext', 'deprecated': True},",
            "        'bitrate': {'type': 'alias', 'field': 'br', 'deprecated': True},",
            "        'total_bitrate': {'type': 'alias', 'field': 'tbr', 'deprecated': True},",
            "        'video_bitrate': {'type': 'alias', 'field': 'vbr', 'deprecated': True},",
            "        'audio_bitrate': {'type': 'alias', 'field': 'abr', 'deprecated': True},",
            "        'framerate': {'type': 'alias', 'field': 'fps', 'deprecated': True},",
            "        'filesize_estimate': {'type': 'alias', 'field': 'size', 'deprecated': True},",
            "        'samplerate': {'type': 'alias', 'field': 'asr', 'deprecated': True},",
            "        'video_ext': {'type': 'alias', 'field': 'vext', 'deprecated': True},",
            "        'audio_ext': {'type': 'alias', 'field': 'aext', 'deprecated': True},",
            "        'video_codec': {'type': 'alias', 'field': 'vcodec', 'deprecated': True},",
            "        'audio_codec': {'type': 'alias', 'field': 'acodec', 'deprecated': True},",
            "        'video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},",
            "        'has_video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},",
            "        'audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},",
            "        'has_audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},",
            "        'extractor': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},",
            "        'extractor_preference': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},",
            "    }",
            "",
            "    def __init__(self, ydl, field_preference):",
            "        self.ydl = ydl",
            "        self._order = []",
            "        self.evaluate_params(self.ydl.params, field_preference)",
            "        if ydl.params.get('verbose'):",
            "            self.print_verbose_info(self.ydl.write_debug)",
            "",
            "    def _get_field_setting(self, field, key):",
            "        if field not in self.settings:",
            "            if key in ('forced', 'priority'):",
            "                return False",
            "            self.ydl.deprecated_feature(f'Using arbitrary fields ({field}) for format sorting is '",
            "                                        'deprecated and may be removed in a future version')",
            "            self.settings[field] = {}",
            "        propObj = self.settings[field]",
            "        if key not in propObj:",
            "            type = propObj.get('type')",
            "            if key == 'field':",
            "                default = 'preference' if type == 'extractor' else (field,) if type in ('combined', 'multiple') else field",
            "            elif key == 'convert':",
            "                default = 'order' if type == 'ordered' else 'float_string' if field else 'ignore'",
            "            else:",
            "                default = {'type': 'field', 'visible': True, 'order': [], 'not_in_list': (None,)}.get(key, None)",
            "            propObj[key] = default",
            "        return propObj[key]",
            "",
            "    def _resolve_field_value(self, field, value, convertNone=False):",
            "        if value is None:",
            "            if not convertNone:",
            "                return None",
            "        else:",
            "            value = value.lower()",
            "        conversion = self._get_field_setting(field, 'convert')",
            "        if conversion == 'ignore':",
            "            return None",
            "        if conversion == 'string':",
            "            return value",
            "        elif conversion == 'float_none':",
            "            return float_or_none(value)",
            "        elif conversion == 'bytes':",
            "            return parse_bytes(value)",
            "        elif conversion == 'order':",
            "            order_list = (self._use_free_order and self._get_field_setting(field, 'order_free')) or self._get_field_setting(field, 'order')",
            "            use_regex = self._get_field_setting(field, 'regex')",
            "            list_length = len(order_list)",
            "            empty_pos = order_list.index('') if '' in order_list else list_length + 1",
            "            if use_regex and value is not None:",
            "                for i, regex in enumerate(order_list):",
            "                    if regex and re.match(regex, value):",
            "                        return list_length - i",
            "                return list_length - empty_pos  # not in list",
            "            else:  # not regex or  value = None",
            "                return list_length - (order_list.index(value) if value in order_list else empty_pos)",
            "        else:",
            "            if value.isnumeric():",
            "                return float(value)",
            "            else:",
            "                self.settings[field]['convert'] = 'string'",
            "                return value",
            "",
            "    def evaluate_params(self, params, sort_extractor):",
            "        self._use_free_order = params.get('prefer_free_formats', False)",
            "        self._sort_user = params.get('format_sort', [])",
            "        self._sort_extractor = sort_extractor",
            "",
            "        def add_item(field, reverse, closest, limit_text):",
            "            field = field.lower()",
            "            if field in self._order:",
            "                return",
            "            self._order.append(field)",
            "            limit = self._resolve_field_value(field, limit_text)",
            "            data = {",
            "                'reverse': reverse,",
            "                'closest': False if limit is None else closest,",
            "                'limit_text': limit_text,",
            "                'limit': limit}",
            "            if field in self.settings:",
            "                self.settings[field].update(data)",
            "            else:",
            "                self.settings[field] = data",
            "",
            "        sort_list = (",
            "            tuple(field for field in self.default if self._get_field_setting(field, 'forced'))",
            "            + (tuple() if params.get('format_sort_force', False)",
            "                else tuple(field for field in self.default if self._get_field_setting(field, 'priority')))",
            "            + tuple(self._sort_user) + tuple(sort_extractor) + self.default)",
            "",
            "        for item in sort_list:",
            "            match = re.match(self.regex, item)",
            "            if match is None:",
            "                raise ExtractorError('Invalid format sort string \"%s\" given by extractor' % item)",
            "            field = match.group('field')",
            "            if field is None:",
            "                continue",
            "            if self._get_field_setting(field, 'type') == 'alias':",
            "                alias, field = field, self._get_field_setting(field, 'field')",
            "                if self._get_field_setting(alias, 'deprecated'):",
            "                    self.ydl.deprecated_feature(f'Format sorting alias {alias} is deprecated and may '",
            "                                                f'be removed in a future version. Please use {field} instead')",
            "            reverse = match.group('reverse') is not None",
            "            closest = match.group('separator') == '~'",
            "            limit_text = match.group('limit')",
            "",
            "            has_limit = limit_text is not None",
            "            has_multiple_fields = self._get_field_setting(field, 'type') == 'combined'",
            "            has_multiple_limits = has_limit and has_multiple_fields and not self._get_field_setting(field, 'same_limit')",
            "",
            "            fields = self._get_field_setting(field, 'field') if has_multiple_fields else (field,)",
            "            limits = limit_text.split(':') if has_multiple_limits else (limit_text,) if has_limit else tuple()",
            "            limit_count = len(limits)",
            "            for (i, f) in enumerate(fields):",
            "                add_item(f, reverse, closest,",
            "                         limits[i] if i < limit_count",
            "                         else limits[0] if has_limit and not has_multiple_limits",
            "                         else None)",
            "",
            "    def print_verbose_info(self, write_debug):",
            "        if self._sort_user:",
            "            write_debug('Sort order given by user: %s' % ', '.join(self._sort_user))",
            "        if self._sort_extractor:",
            "            write_debug('Sort order given by extractor: %s' % ', '.join(self._sort_extractor))",
            "        write_debug('Formats sorted by: %s' % ', '.join(['%s%s%s' % (",
            "            '+' if self._get_field_setting(field, 'reverse') else '', field,",
            "            '%s%s(%s)' % ('~' if self._get_field_setting(field, 'closest') else ':',",
            "                          self._get_field_setting(field, 'limit_text'),",
            "                          self._get_field_setting(field, 'limit'))",
            "            if self._get_field_setting(field, 'limit_text') is not None else '')",
            "            for field in self._order if self._get_field_setting(field, 'visible')]))",
            "",
            "    def _calculate_field_preference_from_value(self, format, field, type, value):",
            "        reverse = self._get_field_setting(field, 'reverse')",
            "        closest = self._get_field_setting(field, 'closest')",
            "        limit = self._get_field_setting(field, 'limit')",
            "",
            "        if type == 'extractor':",
            "            maximum = self._get_field_setting(field, 'max')",
            "            if value is None or (maximum is not None and value >= maximum):",
            "                value = -1",
            "        elif type == 'boolean':",
            "            in_list = self._get_field_setting(field, 'in_list')",
            "            not_in_list = self._get_field_setting(field, 'not_in_list')",
            "            value = 0 if ((in_list is None or value in in_list) and (not_in_list is None or value not in not_in_list)) else -1",
            "        elif type == 'ordered':",
            "            value = self._resolve_field_value(field, value, True)",
            "",
            "        # try to convert to number",
            "        val_num = float_or_none(value, default=self._get_field_setting(field, 'default'))",
            "        is_num = self._get_field_setting(field, 'convert') != 'string' and val_num is not None",
            "        if is_num:",
            "            value = val_num",
            "",
            "        return ((-10, 0) if value is None",
            "                else (1, value, 0) if not is_num  # if a field has mixed strings and numbers, strings are sorted higher",
            "                else (0, -abs(value - limit), value - limit if reverse else limit - value) if closest",
            "                else (0, value, 0) if not reverse and (limit is None or value <= limit)",
            "                else (0, -value, 0) if limit is None or (reverse and value == limit) or value > limit",
            "                else (-1, value, 0))",
            "",
            "    def _calculate_field_preference(self, format, field):",
            "        type = self._get_field_setting(field, 'type')  # extractor, boolean, ordered, field, multiple",
            "        get_value = lambda f: format.get(self._get_field_setting(f, 'field'))",
            "        if type == 'multiple':",
            "            type = 'field'  # Only 'field' is allowed in multiple for now",
            "            actual_fields = self._get_field_setting(field, 'field')",
            "",
            "            value = self._get_field_setting(field, 'function')(get_value(f) for f in actual_fields)",
            "        else:",
            "            value = get_value(field)",
            "        return self._calculate_field_preference_from_value(format, field, type, value)",
            "",
            "    def calculate_preference(self, format):",
            "        # Determine missing protocol",
            "        if not format.get('protocol'):",
            "            format['protocol'] = determine_protocol(format)",
            "",
            "        # Determine missing ext",
            "        if not format.get('ext') and 'url' in format:",
            "            format['ext'] = determine_ext(format['url'])",
            "        if format.get('vcodec') == 'none':",
            "            format['audio_ext'] = format['ext'] if format.get('acodec') != 'none' else 'none'",
            "            format['video_ext'] = 'none'",
            "        else:",
            "            format['video_ext'] = format['ext']",
            "            format['audio_ext'] = 'none'",
            "        # if format.get('preference') is None and format.get('ext') in ('f4f', 'f4m'):  # Not supported?",
            "        #    format['preference'] = -1000",
            "",
            "        if format.get('preference') is None and format.get('ext') == 'flv' and re.match('[hx]265|he?vc?', format.get('vcodec') or ''):",
            "            # HEVC-over-FLV is out-of-spec by FLV's original spec",
            "            # ref. https://trac.ffmpeg.org/ticket/6389",
            "            # ref. https://github.com/yt-dlp/yt-dlp/pull/5821",
            "            format['preference'] = -100",
            "",
            "        # Determine missing bitrates",
            "        if format.get('vcodec') == 'none':",
            "            format['vbr'] = 0",
            "        if format.get('acodec') == 'none':",
            "            format['abr'] = 0",
            "        if not format.get('vbr') and format.get('vcodec') != 'none':",
            "            format['vbr'] = try_call(lambda: format['tbr'] - format['abr']) or None",
            "        if not format.get('abr') and format.get('acodec') != 'none':",
            "            format['abr'] = try_call(lambda: format['tbr'] - format['vbr']) or None",
            "        if not format.get('tbr'):",
            "            format['tbr'] = try_call(lambda: format['vbr'] + format['abr']) or None",
            "",
            "        return tuple(self._calculate_field_preference(format, field) for field in self._order)",
            "",
            "",
            "def filesize_from_tbr(tbr, duration):",
            "    \"\"\"",
            "    @param tbr:      Total bitrate in kbps (1000 bits/sec)",
            "    @param duration: Duration in seconds",
            "    @returns         Filesize in bytes",
            "    \"\"\"",
            "    if tbr is None or duration is None:",
            "        return None",
            "    return int(duration * tbr * (1000 / 8))",
            "",
            "",
            "# XXX: Temporary",
            "class _YDLLogger:",
            "    def __init__(self, ydl=None):",
            "        self._ydl = ydl",
            "",
            "    def debug(self, message):",
            "        if self._ydl:",
            "            self._ydl.write_debug(message)",
            "",
            "    def info(self, message):",
            "        if self._ydl:",
            "            self._ydl.to_screen(message)",
            "",
            "    def warning(self, message, *, once=False):",
            "        if self._ydl:",
            "            self._ydl.report_warning(message, once)",
            "",
            "    def error(self, message, *, is_error=True):",
            "        if self._ydl:",
            "            self._ydl.report_error(message, is_error=is_error)",
            "",
            "    def stdout(self, message):",
            "        if self._ydl:",
            "            self._ydl.to_stdout(message)",
            "",
            "    def stderr(self, message):",
            "        if self._ydl:",
            "            self._ydl.to_stderr(message)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "53": [],
            "839": [
                "Popen",
                "__init__"
            ],
            "841": [
                "Popen",
                "__init__"
            ],
            "1640": [
                "shell_quote"
            ],
            "1641": [
                "shell_quote"
            ],
            "1642": [
                "shell_quote"
            ],
            "1643": [
                "shell_quote"
            ],
            "1644": [
                "shell_quote"
            ],
            "1645": [
                "shell_quote"
            ],
            "1646": [
                "shell_quote"
            ],
            "1647": [
                "shell_quote"
            ],
            "1648": [
                "shell_quote"
            ],
            "2852": [
                "args_to_str"
            ]
        },
        "addLocation": []
    }
}