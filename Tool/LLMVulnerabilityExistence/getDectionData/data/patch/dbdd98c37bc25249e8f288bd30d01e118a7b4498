{
    "tensorflow/python/kernel_tests/summary_ops/summary_ops_test.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 985,
                "afterPatchRowNumber": 985,
                "PatchRowcode": "         self.assertEqual(3, get_total())"
            },
            "1": {
                "beforePatchRowNumber": 986,
                "afterPatchRowNumber": 986,
                "PatchRowcode": "         summary_ops.flush(writer=writer)"
            },
            "2": {
                "beforePatchRowNumber": 987,
                "afterPatchRowNumber": 987,
                "PatchRowcode": "         self.assertEqual(4, get_total())"
            },
            "3": {
                "beforePatchRowNumber": 988,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        summary_ops.write('tag', 1, step=0)"
            },
            "4": {
                "beforePatchRowNumber": 989,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.assertEqual(4, get_total())"
            },
            "5": {
                "beforePatchRowNumber": 990,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        summary_ops.flush(writer=writer._resource)  # pylint:disable=protected-access"
            },
            "6": {
                "beforePatchRowNumber": 991,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.assertEqual(5, get_total())"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 988,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 989,
                "PatchRowcode": "+  # Regression test for b/228097117."
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 990,
                "PatchRowcode": "+  def testFlushFunction_disallowsInvalidWriterInput(self):"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 991,
                "PatchRowcode": "+    with context.eager_mode():"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 992,
                "PatchRowcode": "+      with self.assertRaisesRegex(ValueError, 'Invalid argument to flush'):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 993,
                "PatchRowcode": "+        summary_ops.flush(writer=())"
            },
            "13": {
                "beforePatchRowNumber": 992,
                "afterPatchRowNumber": 994,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 993,
                "afterPatchRowNumber": 995,
                "PatchRowcode": "   @test_util.assert_no_new_tensors"
            },
            "15": {
                "beforePatchRowNumber": 994,
                "afterPatchRowNumber": 996,
                "PatchRowcode": "   def testNoMemoryLeak_graphMode(self):"
            }
        },
        "frontPatchFile": [
            "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Tests for V2 summary ops from summary_ops_v2.\"\"\"",
            "",
            "import os",
            "import unittest",
            "",
            "import six",
            "",
            "from tensorflow.core.framework import graph_pb2",
            "from tensorflow.core.framework import node_def_pb2",
            "from tensorflow.core.framework import step_stats_pb2",
            "from tensorflow.core.framework import summary_pb2",
            "from tensorflow.core.protobuf import config_pb2",
            "from tensorflow.core.util import event_pb2",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.eager import def_function",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import errors",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.framework import tensor_spec",
            "from tensorflow.python.framework import tensor_util",
            "from tensorflow.python.framework import test_util",
            "from tensorflow.python.lib.io import tf_record",
            "from tensorflow.python.module import module",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import summary_ops_v2 as summary_ops",
            "from tensorflow.python.ops import variables",
            "from tensorflow.python.platform import gfile",
            "from tensorflow.python.platform import test",
            "from tensorflow.python.platform import tf_logging as logging",
            "from tensorflow.python.saved_model import load as saved_model_load",
            "from tensorflow.python.saved_model import loader as saved_model_loader",
            "from tensorflow.python.saved_model import save as saved_model_save",
            "from tensorflow.python.saved_model import tag_constants",
            "",
            "",
            "class SummaryOpsCoreTest(test_util.TensorFlowTestCase):",
            "",
            "  def testWrite(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        output = summary_ops.write('tag', 42, step=12)",
            "        self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    value = events[1].summary.value[0]",
            "    self.assertEqual('tag', value.tag)",
            "    self.assertEqual(42, to_numpy(value))",
            "",
            "  def testWrite_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function",
            "      def f():",
            "        with writer.as_default():",
            "          return summary_ops.write('tag', 42, step=12)",
            "      output = f()",
            "      self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    value = events[1].summary.value[0]",
            "    self.assertEqual('tag', value.tag)",
            "    self.assertEqual(42, to_numpy(value))",
            "",
            "  def testWrite_metadata(self):",
            "    logdir = self.get_temp_dir()",
            "    metadata = summary_pb2.SummaryMetadata()",
            "    metadata.plugin_data.plugin_name = 'foo'",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.write('obj', 0, 0, metadata=metadata)",
            "        summary_ops.write('bytes', 0, 0, metadata=metadata.SerializeToString())",
            "        m = constant_op.constant(metadata.SerializeToString())",
            "        summary_ops.write('string_tensor', 0, 0, metadata=m)",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(4, len(events))",
            "    self.assertEqual(metadata, events[1].summary.value[0].metadata)",
            "    self.assertEqual(metadata, events[2].summary.value[0].metadata)",
            "    self.assertEqual(metadata, events[3].summary.value[0].metadata)",
            "",
            "  def testWrite_name(self):",
            "    @def_function.function",
            "    def f():",
            "      output = summary_ops.write('tag', 42, step=12, name='anonymous')",
            "      self.assertTrue(output.name.startswith('anonymous'))",
            "    f()",
            "",
            "  def testWrite_ndarray(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.write('tag', [[1, 2], [3, 4]], step=12)",
            "    events = events_from_logdir(logdir)",
            "    value = events[1].summary.value[0]",
            "    self.assertAllEqual([[1, 2], [3, 4]], to_numpy(value))",
            "",
            "  def testWrite_tensor(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      t = constant_op.constant([[1, 2], [3, 4]])",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.write('tag', t, step=12)",
            "      expected = t.numpy()",
            "    events = events_from_logdir(logdir)",
            "    value = events[1].summary.value[0]",
            "    self.assertAllEqual(expected, to_numpy(value))",
            "",
            "  def testWrite_tensor_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function",
            "      def f(t):",
            "        with writer.as_default():",
            "          summary_ops.write('tag', t, step=12)",
            "      t = constant_op.constant([[1, 2], [3, 4]])",
            "      f(t)",
            "      expected = t.numpy()",
            "    events = events_from_logdir(logdir)",
            "    value = events[1].summary.value[0]",
            "    self.assertAllEqual(expected, to_numpy(value))",
            "",
            "  def testWrite_stringTensor(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.write('tag', [b'foo', b'bar'], step=12)",
            "    events = events_from_logdir(logdir)",
            "    value = events[1].summary.value[0]",
            "    self.assertAllEqual([b'foo', b'bar'], to_numpy(value))",
            "",
            "  @test_util.run_gpu_only",
            "  def testWrite_gpuDeviceContext(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with ops.device('/GPU:0'):",
            "          value = constant_op.constant(42.0)",
            "          step = constant_op.constant(12, dtype=dtypes.int64)",
            "          summary_ops.write('tag', value, step=step).numpy()",
            "    empty_metadata = summary_pb2.SummaryMetadata()",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    self.assertEqual(42, to_numpy(events[1].summary.value[0]))",
            "    self.assertEqual(empty_metadata, events[1].summary.value[0].metadata)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testWrite_noDefaultWriter(self):",
            "    # Use assertAllEqual instead of assertFalse since it works in a defun.",
            "    self.assertAllEqual(False, summary_ops.write('tag', 42, step=0))",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testWrite_noStep_okayIfAlsoNoDefaultWriter(self):",
            "    # Use assertAllEqual instead of assertFalse since it works in a defun.",
            "    self.assertAllEqual(False, summary_ops.write('tag', 42))",
            "",
            "  def testWrite_noStep(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with self.assertRaisesRegex(ValueError, 'No step set'):",
            "          summary_ops.write('tag', 42)",
            "",
            "  def testWrite_noStep_okayIfNotRecordingSummaries(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with summary_ops.record_if(False):",
            "          self.assertFalse(summary_ops.write('tag', 42))",
            "",
            "  def testWrite_usingDefaultStep(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        with summary_ops.create_file_writer_v2(logdir).as_default():",
            "          summary_ops.set_step(1)",
            "          summary_ops.write('tag', 1.0)",
            "          summary_ops.set_step(2)",
            "          summary_ops.write('tag', 1.0)",
            "          mystep = variables.Variable(10, dtype=dtypes.int64)",
            "          summary_ops.set_step(mystep)",
            "          summary_ops.write('tag', 1.0)",
            "          mystep.assign_add(1)",
            "          summary_ops.write('tag', 1.0)",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(5, len(events))",
            "      self.assertEqual(1, events[1].step)",
            "      self.assertEqual(2, events[2].step)",
            "      self.assertEqual(10, events[3].step)",
            "      self.assertEqual(11, events[4].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepConstant_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        @def_function.function",
            "        def f():",
            "          with writer.as_default():",
            "            summary_ops.write('tag', 1.0)",
            "        summary_ops.set_step(1)",
            "        f()",
            "        summary_ops.set_step(2)",
            "        f()",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(3, len(events))",
            "      self.assertEqual(1, events[1].step)",
            "      # The step value will still be 1 because the value was captured at the",
            "      # time the function was first traced.",
            "      self.assertEqual(1, events[2].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepVariable_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        @def_function.function",
            "        def f():",
            "          with writer.as_default():",
            "            summary_ops.write('tag', 1.0)",
            "        mystep = variables.Variable(0, dtype=dtypes.int64)",
            "        summary_ops.set_step(mystep)",
            "        f()",
            "        mystep.assign_add(1)",
            "        f()",
            "        mystep.assign(10)",
            "        f()",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(4, len(events))",
            "      self.assertEqual(0, events[1].step)",
            "      self.assertEqual(1, events[2].step)",
            "      self.assertEqual(10, events[3].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepConstant_fromLegacyGraph(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.graph_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        summary_ops.set_step(1)",
            "        with writer.as_default():",
            "          write_op = summary_ops.write('tag', 1.0)",
            "        summary_ops.set_step(2)",
            "        with self.cached_session() as sess:",
            "          sess.run(writer.init())",
            "          sess.run(write_op)",
            "          sess.run(write_op)",
            "          sess.run(writer.flush())",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(3, len(events))",
            "      self.assertEqual(1, events[1].step)",
            "      # The step value will still be 1 because the value was captured at the",
            "      # time the graph was constructed.",
            "      self.assertEqual(1, events[2].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepVariable_fromLegacyGraph(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.graph_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        mystep = variables.Variable(0, dtype=dtypes.int64)",
            "        summary_ops.set_step(mystep)",
            "        with writer.as_default():",
            "          write_op = summary_ops.write('tag', 1.0)",
            "        first_assign_op = mystep.assign_add(1)",
            "        second_assign_op = mystep.assign(10)",
            "        with self.cached_session() as sess:",
            "          sess.run(writer.init())",
            "          sess.run(mystep.initializer)",
            "          sess.run(write_op)",
            "          sess.run(first_assign_op)",
            "          sess.run(write_op)",
            "          sess.run(second_assign_op)",
            "          sess.run(write_op)",
            "          sess.run(writer.flush())",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(4, len(events))",
            "      self.assertEqual(0, events[1].step)",
            "      self.assertEqual(1, events[2].step)",
            "      self.assertEqual(10, events[3].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStep_fromAsDefault(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        with writer.as_default(step=1):",
            "          summary_ops.write('tag', 1.0)",
            "          with writer.as_default():",
            "            summary_ops.write('tag', 1.0)",
            "            with writer.as_default(step=2):",
            "              summary_ops.write('tag', 1.0)",
            "            summary_ops.write('tag', 1.0)",
            "            summary_ops.set_step(3)",
            "          summary_ops.write('tag', 1.0)",
            "      events = events_from_logdir(logdir)",
            "      self.assertListEqual([1, 1, 2, 1, 3], [e.step for e in events[1:]])",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepVariable_fromAsDefault(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        mystep = variables.Variable(1, dtype=dtypes.int64)",
            "        with writer.as_default(step=mystep):",
            "          summary_ops.write('tag', 1.0)",
            "          with writer.as_default():",
            "            mystep.assign(2)",
            "            summary_ops.write('tag', 1.0)",
            "            with writer.as_default(step=3):",
            "              summary_ops.write('tag', 1.0)",
            "            summary_ops.write('tag', 1.0)",
            "            mystep.assign(4)",
            "          summary_ops.write('tag', 1.0)",
            "      events = events_from_logdir(logdir)",
            "      self.assertListEqual([1, 2, 3, 2, 4], [e.step for e in events[1:]])",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStep_fromSetAsDefault(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        mystep = variables.Variable(1, dtype=dtypes.int64)",
            "        writer.set_as_default(step=mystep)",
            "        summary_ops.write('tag', 1.0)",
            "        mystep.assign(2)",
            "        summary_ops.write('tag', 1.0)",
            "        writer.set_as_default(step=3)",
            "        summary_ops.write('tag', 1.0)",
            "        writer.flush()",
            "      events = events_from_logdir(logdir)",
            "      self.assertListEqual([1, 2, 3], [e.step for e in events[1:]])",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepVariable_fromSetAsDefault(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        writer.set_as_default(step=1)",
            "        summary_ops.write('tag', 1.0)",
            "        writer.set_as_default(step=2)",
            "        summary_ops.write('tag', 1.0)",
            "        writer.set_as_default()",
            "        summary_ops.write('tag', 1.0)",
            "        writer.flush()",
            "      events = events_from_logdir(logdir)",
            "      self.assertListEqual([1, 2, 2], [e.step for e in events[1:]])",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_recordIf_constant(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        self.assertTrue(summary_ops.write('default', 1, step=0))",
            "        with summary_ops.record_if(True):",
            "          self.assertTrue(summary_ops.write('set_on', 1, step=0))",
            "        with summary_ops.record_if(False):",
            "          self.assertFalse(summary_ops.write('set_off', 1, step=0))",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(3, len(events))",
            "    self.assertEqual('default', events[1].summary.value[0].tag)",
            "    self.assertEqual('set_on', events[2].summary.value[0].tag)",
            "",
            "  def testWrite_recordIf_constant_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function",
            "      def f():",
            "        with writer.as_default():",
            "          # Use assertAllEqual instead of assertTrue since it works in a defun.",
            "          self.assertAllEqual(summary_ops.write('default', 1, step=0), True)",
            "          with summary_ops.record_if(True):",
            "            self.assertAllEqual(summary_ops.write('set_on', 1, step=0), True)",
            "          with summary_ops.record_if(False):",
            "            self.assertAllEqual(summary_ops.write('set_off', 1, step=0), False)",
            "      f()",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(3, len(events))",
            "    self.assertEqual('default', events[1].summary.value[0].tag)",
            "    self.assertEqual('set_on', events[2].summary.value[0].tag)",
            "",
            "  def testWrite_recordIf_callable(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      step = variables.Variable(-1, dtype=dtypes.int64)",
            "      def record_fn():",
            "        step.assign_add(1)",
            "        return int(step % 2) == 0",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with summary_ops.record_if(record_fn):",
            "          self.assertTrue(summary_ops.write('tag', 1, step=step))",
            "          self.assertFalse(summary_ops.write('tag', 1, step=step))",
            "          self.assertTrue(summary_ops.write('tag', 1, step=step))",
            "          self.assertFalse(summary_ops.write('tag', 1, step=step))",
            "          self.assertTrue(summary_ops.write('tag', 1, step=step))",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(4, len(events))",
            "    self.assertEqual(0, events[1].step)",
            "    self.assertEqual(2, events[2].step)",
            "    self.assertEqual(4, events[3].step)",
            "",
            "  def testWrite_recordIf_callable_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      step = variables.Variable(-1, dtype=dtypes.int64)",
            "      @def_function.function",
            "      def record_fn():",
            "        step.assign_add(1)",
            "        return math_ops.equal(step % 2, 0)",
            "      @def_function.function",
            "      def f():",
            "        with writer.as_default():",
            "          with summary_ops.record_if(record_fn):",
            "            return [",
            "                summary_ops.write('tag', 1, step=step),",
            "                summary_ops.write('tag', 1, step=step),",
            "                summary_ops.write('tag', 1, step=step)]",
            "      self.assertAllEqual(f(), [True, False, True])",
            "      self.assertAllEqual(f(), [False, True, False])",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(4, len(events))",
            "    self.assertEqual(0, events[1].step)",
            "    self.assertEqual(2, events[2].step)",
            "    self.assertEqual(4, events[3].step)",
            "",
            "  def testWrite_recordIf_tensorInput_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function(input_signature=[",
            "          tensor_spec.TensorSpec(shape=[], dtype=dtypes.int64)])",
            "      def f(step):",
            "        with writer.as_default():",
            "          with summary_ops.record_if(math_ops.equal(step % 2, 0)):",
            "            return summary_ops.write('tag', 1, step=step)",
            "      self.assertTrue(f(0))",
            "      self.assertFalse(f(1))",
            "      self.assertTrue(f(2))",
            "      self.assertFalse(f(3))",
            "      self.assertTrue(f(4))",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(4, len(events))",
            "    self.assertEqual(0, events[1].step)",
            "    self.assertEqual(2, events[2].step)",
            "    self.assertEqual(4, events[3].step)",
            "",
            "  def testWriteRawPb(self):",
            "    logdir = self.get_temp_dir()",
            "    pb = summary_pb2.Summary()",
            "    pb.value.add().simple_value = 42.0",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        output = summary_ops.write_raw_pb(pb.SerializeToString(), step=12)",
            "        self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    self.assertProtoEquals(pb, events[1].summary)",
            "",
            "  def testWriteRawPb_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    pb = summary_pb2.Summary()",
            "    pb.value.add().simple_value = 42.0",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function",
            "      def f():",
            "        with writer.as_default():",
            "          return summary_ops.write_raw_pb(pb.SerializeToString(), step=12)",
            "      output = f()",
            "      self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    self.assertProtoEquals(pb, events[1].summary)",
            "",
            "  def testWriteRawPb_multipleValues(self):",
            "    logdir = self.get_temp_dir()",
            "    pb1 = summary_pb2.Summary()",
            "    pb1.value.add().simple_value = 1.0",
            "    pb1.value.add().simple_value = 2.0",
            "    pb2 = summary_pb2.Summary()",
            "    pb2.value.add().simple_value = 3.0",
            "    pb3 = summary_pb2.Summary()",
            "    pb3.value.add().simple_value = 4.0",
            "    pb3.value.add().simple_value = 5.0",
            "    pb3.value.add().simple_value = 6.0",
            "    pbs = [pb.SerializeToString() for pb in (pb1, pb2, pb3)]",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        output = summary_ops.write_raw_pb(pbs, step=12)",
            "        self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    expected_pb = summary_pb2.Summary()",
            "    for i in range(6):",
            "      expected_pb.value.add().simple_value = i + 1.0",
            "    self.assertProtoEquals(expected_pb, events[1].summary)",
            "",
            "  def testWriteRawPb_invalidValue(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with self.assertRaisesRegex(",
            "            errors.DataLossError,",
            "            'Bad tf.compat.v1.Summary binary proto tensor string'):",
            "          summary_ops.write_raw_pb('notaproto', step=12)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testGetSetStep(self):",
            "    try:",
            "      self.assertIsNone(summary_ops.get_step())",
            "      summary_ops.set_step(1)",
            "      # Use assertAllEqual instead of assertEqual since it works in a defun.",
            "      self.assertAllEqual(1, summary_ops.get_step())",
            "      summary_ops.set_step(constant_op.constant(2))",
            "      self.assertAllEqual(2, summary_ops.get_step())",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testGetSetStep_variable(self):",
            "    with context.eager_mode():",
            "      try:",
            "        mystep = variables.Variable(0)",
            "        summary_ops.set_step(mystep)",
            "        self.assertAllEqual(0, summary_ops.get_step().read_value())",
            "        mystep.assign_add(1)",
            "        self.assertAllEqual(1, summary_ops.get_step().read_value())",
            "        # Check that set_step() properly maintains reference to variable.",
            "        del mystep",
            "        self.assertAllEqual(1, summary_ops.get_step().read_value())",
            "        summary_ops.get_step().assign_add(1)",
            "        self.assertAllEqual(2, summary_ops.get_step().read_value())",
            "      finally:",
            "        # Reset to default state for other tests.",
            "        summary_ops.set_step(None)",
            "",
            "  def testGetSetStep_variable_fromFunction(self):",
            "    with context.eager_mode():",
            "      try:",
            "        @def_function.function",
            "        def set_step(step):",
            "          summary_ops.set_step(step)",
            "          return summary_ops.get_step()",
            "        @def_function.function",
            "        def get_and_increment():",
            "          summary_ops.get_step().assign_add(1)",
            "          return summary_ops.get_step()",
            "        mystep = variables.Variable(0)",
            "        self.assertAllEqual(0, set_step(mystep))",
            "        self.assertAllEqual(0, summary_ops.get_step().read_value())",
            "        self.assertAllEqual(1, get_and_increment())",
            "        self.assertAllEqual(2, get_and_increment())",
            "        # Check that set_step() properly maintains reference to variable.",
            "        del mystep",
            "        self.assertAllEqual(3, get_and_increment())",
            "      finally:",
            "        # Reset to default state for other tests.",
            "        summary_ops.set_step(None)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testSummaryScope(self):",
            "    with summary_ops.summary_scope('foo') as (tag, scope):",
            "      self.assertEqual('foo', tag)",
            "      self.assertEqual('foo/', scope)",
            "      with summary_ops.summary_scope('bar') as (tag, scope):",
            "        self.assertEqual('foo/bar', tag)",
            "        self.assertEqual('foo/bar/', scope)",
            "      with summary_ops.summary_scope('with/slash') as (tag, scope):",
            "        self.assertEqual('foo/with/slash', tag)",
            "        self.assertEqual('foo/with/slash/', scope)",
            "      with ops.name_scope(None, skip_on_eager=False):",
            "        with summary_ops.summary_scope('unnested') as (tag, scope):",
            "          self.assertEqual('unnested', tag)",
            "          self.assertEqual('unnested/', scope)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testSummaryScope_defaultName(self):",
            "    with summary_ops.summary_scope(None) as (tag, scope):",
            "      self.assertEqual('summary', tag)",
            "      self.assertEqual('summary/', scope)",
            "    with summary_ops.summary_scope(None, 'backup') as (tag, scope):",
            "      self.assertEqual('backup', tag)",
            "      self.assertEqual('backup/', scope)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testSummaryScope_handlesCharactersIllegalForScope(self):",
            "    with summary_ops.summary_scope('f?o?o') as (tag, scope):",
            "      self.assertEqual('f?o?o', tag)",
            "      self.assertEqual('foo/', scope)",
            "    # If all characters aren't legal for a scope name, use default name.",
            "    with summary_ops.summary_scope('???', 'backup') as (tag, scope):",
            "      self.assertEqual('???', tag)",
            "      self.assertEqual('backup/', scope)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testSummaryScope_nameNotUniquifiedForTag(self):",
            "    constant_op.constant(0, name='foo')",
            "    with summary_ops.summary_scope('foo') as (tag, _):",
            "      self.assertEqual('foo', tag)",
            "    with summary_ops.summary_scope('foo') as (tag, _):",
            "      self.assertEqual('foo', tag)",
            "    with ops.name_scope('with', skip_on_eager=False):",
            "      constant_op.constant(0, name='slash')",
            "    with summary_ops.summary_scope('with/slash') as (tag, _):",
            "      self.assertEqual('with/slash', tag)",
            "",
            "  def testAllV2SummaryOps(self):",
            "    logdir = self.get_temp_dir()",
            "    def define_ops():",
            "      result = []",
            "      # TF 2.0 summary ops",
            "      result.append(summary_ops.write('write', 1, step=0))",
            "      result.append(summary_ops.write_raw_pb(b'', step=0, name='raw_pb'))",
            "      # TF 1.x tf.contrib.summary ops",
            "      result.append(summary_ops.generic('tensor', 1, step=1))",
            "      result.append(summary_ops.scalar('scalar', 2.0, step=1))",
            "      result.append(summary_ops.histogram('histogram', [1.0], step=1))",
            "      result.append(summary_ops.image('image', [[[[1.0]]]], step=1))",
            "      result.append(summary_ops.audio('audio', [[1.0]], 1.0, 1, step=1))",
            "      return result",
            "    with context.graph_mode():",
            "      ops_without_writer = define_ops()",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with summary_ops.record_if(True):",
            "          ops_recording_on = define_ops()",
            "        with summary_ops.record_if(False):",
            "          ops_recording_off = define_ops()",
            "      # We should be collecting all ops defined with a default writer present,",
            "      # regardless of whether recording was set on or off, but not those defined",
            "      # without a writer at all.",
            "      del ops_without_writer",
            "      expected_ops = ops_recording_on + ops_recording_off",
            "      self.assertCountEqual(expected_ops, summary_ops.all_v2_summary_ops())",
            "",
            "  def testShouldRecordSummaries_defaultState(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "      w = summary_ops.create_file_writer_v2(logdir)",
            "      self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "      with w.as_default():",
            "        # Should be enabled only when default writer is registered.",
            "        self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "      self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "      with summary_ops.record_if(True):",
            "        # Should be disabled when no default writer, even with record_if(True).",
            "        self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "",
            "  def testShouldRecordSummaries_constants(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with summary_ops.record_if(True):",
            "          self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "        with summary_ops.record_if(False):",
            "          self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "          with summary_ops.record_if(True):",
            "            self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "",
            "  def testShouldRecordSummaries_variable(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        cond = variables.Variable(False)",
            "        with summary_ops.record_if(cond):",
            "          self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "          cond.assign(True)",
            "          self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "",
            "  def testShouldRecordSummaries_callable(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        cond_box = [False]",
            "        cond = lambda: cond_box[0]",
            "        with summary_ops.record_if(cond):",
            "          self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "          cond_box[0] = True",
            "          self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "",
            "  def testShouldRecordSummaries_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function(input_signature=[",
            "          tensor_spec.TensorSpec(shape=[], dtype=dtypes.bool)])",
            "      def f(cond):",
            "        results = []",
            "        results.append(summary_ops.should_record_summaries())",
            "        with writer.as_default():",
            "          results.append(summary_ops.should_record_summaries())",
            "          with summary_ops.record_if(False):",
            "            results.append(summary_ops.should_record_summaries())",
            "          with summary_ops.record_if(cond):",
            "            results.append(summary_ops.should_record_summaries())",
            "        return results",
            "      self.assertAllEqual([False, True, False, True], f(True))",
            "      self.assertAllEqual([False, True, False, False], f(False))",
            "",
            "  def testHasDefaultWriter_checkWriter(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with self.subTest(name='has_writer'):",
            "        with summary_ops.create_file_writer_v2(logdir).as_default():",
            "          self.assertTrue(summary_ops.has_default_writer())",
            "      with self.subTest(name='no_writer'):",
            "        self.assertFalse(summary_ops.has_default_writer())",
            "",
            "",
            "class SummaryWriterTest(test_util.TensorFlowTestCase):",
            "",
            "  def testCreate_withInitAndClose(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(",
            "          logdir, max_queue=1000, flush_millis=1000000)",
            "      get_total = lambda: len(events_from_logdir(logdir))",
            "      self.assertEqual(1, get_total())  # file_version Event",
            "      # Calling init() again while writer is open has no effect",
            "      writer.init()",
            "      self.assertEqual(1, get_total())",
            "      with writer.as_default():",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(1, get_total())",
            "        # Calling .close() should do an implicit flush",
            "        writer.close()",
            "        self.assertEqual(2, get_total())",
            "",
            "  def testCreate_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    @def_function.function",
            "    def f():",
            "      # Returned SummaryWriter must be stored in a non-local variable so it",
            "      # lives throughout the function execution.",
            "      if not hasattr(f, 'writer'):",
            "        f.writer = summary_ops.create_file_writer_v2(logdir)",
            "    with context.eager_mode():",
            "      f()",
            "    event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "    self.assertEqual(1, len(event_files))",
            "",
            "  def testCreate_graphTensorArgument_raisesError(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.graph_mode():",
            "      logdir_tensor = constant_op.constant(logdir)",
            "    with context.eager_mode():",
            "      with self.assertRaisesRegex(",
            "          ValueError, 'Invalid graph Tensor argument.*logdir'):",
            "        summary_ops.create_file_writer_v2(logdir_tensor)",
            "    self.assertEmpty(gfile.Glob(os.path.join(logdir, '*')))",
            "",
            "  def testCreate_fromFunction_graphTensorArgument_raisesError(self):",
            "    logdir = self.get_temp_dir()",
            "    @def_function.function",
            "    def f():",
            "      summary_ops.create_file_writer_v2(constant_op.constant(logdir))",
            "    with context.eager_mode():",
            "      with self.assertRaisesRegex(",
            "          ValueError, 'Invalid graph Tensor argument.*logdir'):",
            "        f()",
            "    self.assertEmpty(gfile.Glob(os.path.join(logdir, '*')))",
            "",
            "  def testCreate_fromFunction_unpersistedResource_raisesError(self):",
            "    logdir = self.get_temp_dir()",
            "    @def_function.function",
            "    def f():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        pass  # Calling .as_default() is enough to indicate use.",
            "    with context.eager_mode():",
            "      # TODO(nickfelt): change this to a better error",
            "      with self.assertRaisesRegex(",
            "          errors.NotFoundError, 'Resource.*does not exist'):",
            "        f()",
            "    # Even though we didn't use it, an event file will have been created.",
            "    self.assertEqual(1, len(gfile.Glob(os.path.join(logdir, '*'))))",
            "",
            "  def testCreate_immediateSetAsDefault_retainsReference(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        summary_ops.create_file_writer_v2(logdir).set_as_default()",
            "        summary_ops.flush()",
            "    finally:",
            "      # Ensure we clean up no matter how the test executes.",
            "      summary_ops._summary_state.writer = None  # pylint: disable=protected-access",
            "",
            "  def testCreate_immediateAsDefault_retainsReference(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.flush()",
            "",
            "  def testCreate_avoidsFilenameCollision(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      for _ in range(10):",
            "        summary_ops.create_file_writer_v2(logdir)",
            "    event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "    self.assertLen(event_files, 10)",
            "",
            "  def testCreate_graphMode_avoidsFilenameCollision(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.graph_mode(), ops.Graph().as_default():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      with self.cached_session() as sess:",
            "        for _ in range(10):",
            "          sess.run(writer.init())",
            "          sess.run(writer.close())",
            "    event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "    self.assertLen(event_files, 10)",
            "",
            "  def testNoSharing(self):",
            "    # Two writers with the same logdir should not share state.",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer1 = summary_ops.create_file_writer_v2(logdir)",
            "      with writer1.as_default():",
            "        summary_ops.write('tag', 1, step=1)",
            "      event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(1, len(event_files))",
            "      file1 = event_files[0]",
            "",
            "      writer2 = summary_ops.create_file_writer_v2(logdir)",
            "      with writer2.as_default():",
            "        summary_ops.write('tag', 1, step=2)",
            "      event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(2, len(event_files))",
            "      event_files.remove(file1)",
            "      file2 = event_files[0]",
            "",
            "      # Extra writes to ensure interleaved usage works.",
            "      with writer1.as_default():",
            "        summary_ops.write('tag', 1, step=1)",
            "      with writer2.as_default():",
            "        summary_ops.write('tag', 1, step=2)",
            "",
            "    events = iter(events_from_file(file1))",
            "    self.assertEqual('brain.Event:2', next(events).file_version)",
            "    self.assertEqual(1, next(events).step)",
            "    self.assertEqual(1, next(events).step)",
            "    self.assertRaises(StopIteration, lambda: next(events))",
            "    events = iter(events_from_file(file2))",
            "    self.assertEqual('brain.Event:2', next(events).file_version)",
            "    self.assertEqual(2, next(events).step)",
            "    self.assertEqual(2, next(events).step)",
            "    self.assertRaises(StopIteration, lambda: next(events))",
            "",
            "  def testNoSharing_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    @def_function.function",
            "    def f1():",
            "      if not hasattr(f1, 'writer'):",
            "        f1.writer = summary_ops.create_file_writer_v2(logdir)",
            "      with f1.writer.as_default():",
            "        summary_ops.write('tag', 1, step=1)",
            "    @def_function.function",
            "    def f2():",
            "      if not hasattr(f2, 'writer'):",
            "        f2.writer = summary_ops.create_file_writer_v2(logdir)",
            "      with f2.writer.as_default():",
            "        summary_ops.write('tag', 1, step=2)",
            "    with context.eager_mode():",
            "      f1()",
            "      event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(1, len(event_files))",
            "      file1 = event_files[0]",
            "",
            "      f2()",
            "      event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(2, len(event_files))",
            "      event_files.remove(file1)",
            "      file2 = event_files[0]",
            "",
            "      # Extra writes to ensure interleaved usage works.",
            "      f1()",
            "      f2()",
            "",
            "    events = iter(events_from_file(file1))",
            "    self.assertEqual('brain.Event:2', next(events).file_version)",
            "    self.assertEqual(1, next(events).step)",
            "    self.assertEqual(1, next(events).step)",
            "    self.assertRaises(StopIteration, lambda: next(events))",
            "    events = iter(events_from_file(file2))",
            "    self.assertEqual('brain.Event:2', next(events).file_version)",
            "    self.assertEqual(2, next(events).step)",
            "    self.assertEqual(2, next(events).step)",
            "    self.assertRaises(StopIteration, lambda: next(events))",
            "",
            "  def testMaxQueue(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(",
            "          logdir, max_queue=1, flush_millis=999999).as_default():",
            "        get_total = lambda: len(events_from_logdir(logdir))",
            "        # Note: First tf.compat.v1.Event is always file_version.",
            "        self.assertEqual(1, get_total())",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(1, get_total())",
            "        # Should flush after second summary since max_queue = 1",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(3, get_total())",
            "",
            "  def testWriterFlush(self):",
            "    logdir = self.get_temp_dir()",
            "    get_total = lambda: len(events_from_logdir(logdir))",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(",
            "          logdir, max_queue=1000, flush_millis=1000000)",
            "      self.assertEqual(1, get_total())  # file_version Event",
            "      with writer.as_default():",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(1, get_total())",
            "        writer.flush()",
            "        self.assertEqual(2, get_total())",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(2, get_total())",
            "      # Exiting the \"as_default()\" should do an implicit flush",
            "      self.assertEqual(3, get_total())",
            "",
            "  def testFlushFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(",
            "          logdir, max_queue=999999, flush_millis=999999)",
            "      with writer.as_default():",
            "        get_total = lambda: len(events_from_logdir(logdir))",
            "        # Note: First tf.compat.v1.Event is always file_version.",
            "        self.assertEqual(1, get_total())",
            "        summary_ops.write('tag', 1, step=0)",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(1, get_total())",
            "        summary_ops.flush()",
            "        self.assertEqual(3, get_total())",
            "        # Test \"writer\" parameter",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(3, get_total())",
            "        summary_ops.flush(writer=writer)",
            "        self.assertEqual(4, get_total())",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(4, get_total())",
            "        summary_ops.flush(writer=writer._resource)  # pylint:disable=protected-access",
            "        self.assertEqual(5, get_total())",
            "",
            "  @test_util.assert_no_new_tensors",
            "  def testNoMemoryLeak_graphMode(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.graph_mode(), ops.Graph().as_default():",
            "      summary_ops.create_file_writer_v2(logdir)",
            "",
            "  @test_util.assert_no_new_pyobjects_executing_eagerly",
            "  def testNoMemoryLeak_eagerMode(self):",
            "    logdir = self.get_temp_dir()",
            "    with summary_ops.create_file_writer_v2(logdir).as_default():",
            "      summary_ops.write('tag', 1, step=0)",
            "",
            "  def testClose_preventsLaterUse(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      writer.close()",
            "      writer.close()  # redundant close() is a no-op",
            "      writer.flush()  # redundant flush() is a no-op",
            "      with self.assertRaisesRegex(RuntimeError, 'already closed'):",
            "        writer.init()",
            "      with self.assertRaisesRegex(RuntimeError, 'already closed'):",
            "        with writer.as_default():",
            "          self.fail('should not get here')",
            "      with self.assertRaisesRegex(RuntimeError, 'already closed'):",
            "        writer.set_as_default()",
            "",
            "  def testClose_closesOpenFile(self):",
            "    try:",
            "      import psutil  # pylint: disable=g-import-not-at-top",
            "    except ImportError:",
            "      raise unittest.SkipTest('test requires psutil')",
            "    proc = psutil.Process()",
            "    get_open_filenames = lambda: set(info[0] for info in proc.open_files())",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(1, len(files))",
            "      eventfile = files[0]",
            "      self.assertIn(eventfile, get_open_filenames())",
            "      writer.close()",
            "      self.assertNotIn(eventfile, get_open_filenames())",
            "",
            "  def testDereference_closesOpenFile(self):",
            "    try:",
            "      import psutil  # pylint: disable=g-import-not-at-top",
            "    except ImportError:",
            "      raise unittest.SkipTest('test requires psutil')",
            "    proc = psutil.Process()",
            "    get_open_filenames = lambda: set(info[0] for info in proc.open_files())",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(1, len(files))",
            "      eventfile = files[0]",
            "      self.assertIn(eventfile, get_open_filenames())",
            "      del writer",
            "      self.assertNotIn(eventfile, get_open_filenames())",
            "",
            "",
            "class SummaryWriterSavedModelTest(test_util.TensorFlowTestCase):",
            "",
            "  def testWriter_savedAsModuleProperty_loadInEagerMode(self):",
            "    with context.eager_mode():",
            "      class Model(module.Module):",
            "",
            "        def __init__(self, model_dir):",
            "          self._writer = summary_ops.create_file_writer_v2(",
            "              model_dir, experimental_trackable=True)",
            "",
            "        @def_function.function(input_signature=[",
            "            tensor_spec.TensorSpec(shape=[], dtype=dtypes.int64)",
            "        ])",
            "        def train(self, step):",
            "          with self._writer.as_default():",
            "            summary_ops.write('tag', 'foo', step=step)",
            "          return constant_op.constant(0)",
            "",
            "      logdir = self.get_temp_dir()",
            "      to_export = Model(logdir)",
            "      pre_save_files = set(events_from_multifile_logdir(logdir))",
            "      export_dir = os.path.join(logdir, 'export')",
            "      saved_model_save.save(",
            "          to_export, export_dir, signatures={'train': to_export.train})",
            "",
            "    # Reset context to ensure we don't share any resources with saving code.",
            "    context._reset_context()  # pylint: disable=protected-access",
            "    with context.eager_mode():",
            "      restored = saved_model_load.load(export_dir)",
            "      restored.train(1)",
            "      restored.train(2)",
            "      post_restore_files = set(events_from_multifile_logdir(logdir))",
            "      restored2 = saved_model_load.load(export_dir)",
            "      restored2.train(3)",
            "      restored2.train(4)",
            "      files_to_events = events_from_multifile_logdir(logdir)",
            "      post_restore2_files = set(files_to_events)",
            "      self.assertLen(files_to_events, 3)",
            "      def unwrap_singleton(iterable):",
            "        self.assertLen(iterable, 1)",
            "        return next(iter(iterable))",
            "      restore_file = unwrap_singleton(post_restore_files - pre_save_files)",
            "      restore2_file = unwrap_singleton(post_restore2_files - post_restore_files)",
            "      restore_events = files_to_events[restore_file]",
            "      restore2_events = files_to_events[restore2_file]",
            "      self.assertLen(restore_events, 3)",
            "      self.assertEqual(1, restore_events[1].step)",
            "      self.assertEqual(2, restore_events[2].step)",
            "      self.assertLen(restore2_events, 3)",
            "      self.assertEqual(3, restore2_events[1].step)",
            "      self.assertEqual(4, restore2_events[2].step)",
            "",
            "  def testWriter_savedAsModuleProperty_loadInGraphMode(self):",
            "    with context.eager_mode():",
            "",
            "      class Model(module.Module):",
            "",
            "        def __init__(self, model_dir):",
            "          self._writer = summary_ops.create_file_writer_v2(",
            "              model_dir, experimental_trackable=True)",
            "",
            "        @def_function.function(input_signature=[",
            "            tensor_spec.TensorSpec(shape=[], dtype=dtypes.int64)",
            "        ])",
            "        def train(self, step):",
            "          with self._writer.as_default():",
            "            summary_ops.write('tag', 'foo', step=step)",
            "          return constant_op.constant(0)",
            "",
            "      logdir = self.get_temp_dir()",
            "      to_export = Model(logdir)",
            "      pre_save_files = set(events_from_multifile_logdir(logdir))",
            "      export_dir = os.path.join(logdir, 'export')",
            "      saved_model_save.save(",
            "          to_export, export_dir, signatures={'train': to_export.train})",
            "",
            "    # Reset context to ensure we don't share any resources with saving code.",
            "    context._reset_context()  # pylint: disable=protected-access",
            "",
            "    def load_and_run_model(sess, input_values):",
            "      \"\"\"Load and run the SavedModel signature in the TF 1.x style.\"\"\"",
            "      model = saved_model_loader.load(sess, [tag_constants.SERVING], export_dir)",
            "      signature = model.signature_def['train']",
            "      inputs = list(signature.inputs.values())",
            "      assert len(inputs) == 1, inputs",
            "      outputs = list(signature.outputs.values())",
            "      assert len(outputs) == 1, outputs",
            "      input_tensor = sess.graph.get_tensor_by_name(inputs[0].name)",
            "      output_tensor = sess.graph.get_tensor_by_name(outputs[0].name)",
            "      for v in input_values:",
            "        sess.run(output_tensor, feed_dict={input_tensor: v})",
            "",
            "    with context.graph_mode(), ops.Graph().as_default():",
            "      # Since writer shared_name is fixed, within a single session, all loads of",
            "      # this SavedModel will refer to a single writer resouce, so it will be",
            "      # initialized only once and write to a single file.",
            "      with self.session() as sess:",
            "        load_and_run_model(sess, [1, 2])",
            "        load_and_run_model(sess, [3, 4])",
            "      post_restore_files = set(events_from_multifile_logdir(logdir))",
            "      # New session will recreate the resource and write to a second file.",
            "      with self.session() as sess:",
            "        load_and_run_model(sess, [5, 6])",
            "      files_to_events = events_from_multifile_logdir(logdir)",
            "      post_restore2_files = set(files_to_events)",
            "",
            "    self.assertLen(files_to_events, 3)",
            "    def unwrap_singleton(iterable):",
            "      self.assertLen(iterable, 1)",
            "      return next(iter(iterable))",
            "    restore_file = unwrap_singleton(post_restore_files - pre_save_files)",
            "    restore2_file = unwrap_singleton(post_restore2_files - post_restore_files)",
            "    restore_events = files_to_events[restore_file]",
            "    restore2_events = files_to_events[restore2_file]",
            "    self.assertLen(restore_events, 5)",
            "    self.assertEqual(1, restore_events[1].step)",
            "    self.assertEqual(2, restore_events[2].step)",
            "    self.assertEqual(3, restore_events[3].step)",
            "    self.assertEqual(4, restore_events[4].step)",
            "    self.assertLen(restore2_events, 3)",
            "    self.assertEqual(5, restore2_events[1].step)",
            "    self.assertEqual(6, restore2_events[2].step)",
            "",
            "",
            "class NoopWriterTest(test_util.TensorFlowTestCase):",
            "",
            "  def testNoopWriter_doesNothing(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_noop_writer()",
            "      writer.init()",
            "      with writer.as_default():",
            "        result = summary_ops.write('test', 1.0, step=0)",
            "      writer.flush()",
            "      writer.close()",
            "    self.assertFalse(result)  # Should have found no active writer",
            "    files = gfile.Glob(os.path.join(logdir, '*'))",
            "    self.assertLen(files, 0)",
            "",
            "  def testNoopWriter_asNestedContext_isTransparent(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      noop_writer = summary_ops.create_noop_writer()",
            "      with writer.as_default():",
            "        result1 = summary_ops.write('first', 1.0, step=0)",
            "        with noop_writer.as_default():",
            "          result2 = summary_ops.write('second', 1.0, step=0)",
            "        result3 = summary_ops.write('third', 1.0, step=0)",
            "    # All ops should have written, including the one inside the no-op writer,",
            "    # since it doesn't actively *disable* writing - it just behaves as if that",
            "    # entire `with` block wasn't there at all.",
            "    self.assertAllEqual([result1, result2, result3], [True, True, True])",
            "",
            "  def testNoopWriter_setAsDefault(self):",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_noop_writer()",
            "        writer.set_as_default()",
            "        result = summary_ops.write('test', 1.0, step=0)",
            "      self.assertFalse(result)  # Should have found no active writer",
            "    finally:",
            "      # Ensure we clean up no matter how the test executes.",
            "      summary_ops._summary_state.writer = None  # pylint: disable=protected-access",
            "",
            "",
            "class SummaryOpsTest(test_util.TensorFlowTestCase):",
            "",
            "  def tearDown(self):",
            "    summary_ops.trace_off()",
            "    super(SummaryOpsTest, self).tearDown()",
            "",
            "  def exec_summary_op(self, summary_op_fn):",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    with writer.as_default():",
            "      summary_op_fn()",
            "    writer.close()",
            "    events = events_from_logdir(logdir)",
            "    return events[1]",
            "",
            "  def run_metadata(self, *args, **kwargs):",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    with writer.as_default():",
            "      summary_ops.run_metadata(*args, **kwargs)",
            "    writer.close()",
            "    events = events_from_logdir(logdir)",
            "    return events[1]",
            "",
            "  def run_metadata_graphs(self, *args, **kwargs):",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    with writer.as_default():",
            "      summary_ops.run_metadata_graphs(*args, **kwargs)",
            "    writer.close()",
            "    events = events_from_logdir(logdir)",
            "    return events[1]",
            "",
            "  def create_run_metadata(self):",
            "    step_stats = step_stats_pb2.StepStats(dev_stats=[",
            "        step_stats_pb2.DeviceStepStats(",
            "            device='cpu:0',",
            "            node_stats=[step_stats_pb2.NodeExecStats(node_name='hello')])",
            "    ])",
            "    return config_pb2.RunMetadata(",
            "        function_graphs=[",
            "            config_pb2.RunMetadata.FunctionGraphs(",
            "                pre_optimization_graph=graph_pb2.GraphDef(",
            "                    node=[node_def_pb2.NodeDef(name='foo')]))",
            "        ],",
            "        step_stats=step_stats)",
            "",
            "  def run_trace(self, f, step=1):",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    summary_ops.trace_on(graph=True, profiler=False)",
            "    with writer.as_default():",
            "      f()",
            "      summary_ops.trace_export(name='foo', step=step)",
            "    writer.close()",
            "    events = events_from_logdir(logdir)",
            "    return events[1]",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadata_usesNameAsTag(self):",
            "    meta = config_pb2.RunMetadata()",
            "",
            "    with ops.name_scope('foo', skip_on_eager=False):",
            "      event = self.run_metadata(name='my_name', data=meta, step=1)",
            "      first_val = event.summary.value[0]",
            "",
            "    self.assertEqual('foo/my_name', first_val.tag)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadata_summaryMetadata(self):",
            "    expected_summary_metadata = \"\"\"",
            "      plugin_data {",
            "        plugin_name: \"graph_run_metadata\"",
            "        content: \"1\"",
            "      }",
            "    \"\"\"",
            "    meta = config_pb2.RunMetadata()",
            "    event = self.run_metadata(name='my_name', data=meta, step=1)",
            "    actual_summary_metadata = event.summary.value[0].metadata",
            "    self.assertProtoEquals(expected_summary_metadata, actual_summary_metadata)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadata_wholeRunMetadata(self):",
            "    expected_run_metadata = \"\"\"",
            "      step_stats {",
            "        dev_stats {",
            "          device: \"cpu:0\"",
            "          node_stats {",
            "            node_name: \"hello\"",
            "          }",
            "        }",
            "      }",
            "      function_graphs {",
            "        pre_optimization_graph {",
            "          node {",
            "            name: \"foo\"",
            "          }",
            "        }",
            "      }",
            "    \"\"\"",
            "    meta = self.create_run_metadata()",
            "    event = self.run_metadata(name='my_name', data=meta, step=1)",
            "    first_val = event.summary.value[0]",
            "",
            "    actual_run_metadata = config_pb2.RunMetadata.FromString(",
            "        first_val.tensor.string_val[0])",
            "    self.assertProtoEquals(expected_run_metadata, actual_run_metadata)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadata_usesDefaultStep(self):",
            "    meta = config_pb2.RunMetadata()",
            "    try:",
            "      summary_ops.set_step(42)",
            "      event = self.run_metadata(name='my_name', data=meta)",
            "      self.assertEqual(42, event.step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadataGraph_usesNameAsTag(self):",
            "    meta = config_pb2.RunMetadata()",
            "",
            "    with ops.name_scope('foo', skip_on_eager=False):",
            "      event = self.run_metadata_graphs(name='my_name', data=meta, step=1)",
            "      first_val = event.summary.value[0]",
            "",
            "    self.assertEqual('foo/my_name', first_val.tag)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadataGraph_summaryMetadata(self):",
            "    expected_summary_metadata = \"\"\"",
            "      plugin_data {",
            "        plugin_name: \"graph_run_metadata_graph\"",
            "        content: \"1\"",
            "      }",
            "    \"\"\"",
            "    meta = config_pb2.RunMetadata()",
            "    event = self.run_metadata_graphs(name='my_name', data=meta, step=1)",
            "    actual_summary_metadata = event.summary.value[0].metadata",
            "    self.assertProtoEquals(expected_summary_metadata, actual_summary_metadata)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadataGraph_runMetadataFragment(self):",
            "    expected_run_metadata = \"\"\"",
            "      function_graphs {",
            "        pre_optimization_graph {",
            "          node {",
            "            name: \"foo\"",
            "          }",
            "        }",
            "      }",
            "    \"\"\"",
            "    meta = self.create_run_metadata()",
            "",
            "    event = self.run_metadata_graphs(name='my_name', data=meta, step=1)",
            "    first_val = event.summary.value[0]",
            "",
            "    actual_run_metadata = config_pb2.RunMetadata.FromString(",
            "        first_val.tensor.string_val[0])",
            "    self.assertProtoEquals(expected_run_metadata, actual_run_metadata)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadataGraph_usesDefaultStep(self):",
            "    meta = config_pb2.RunMetadata()",
            "    try:",
            "      summary_ops.set_step(42)",
            "      event = self.run_metadata_graphs(name='my_name', data=meta)",
            "      self.assertEqual(42, event.step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    event = self.run_trace(f)",
            "",
            "    first_val = event.summary.value[0]",
            "    actual_run_metadata = config_pb2.RunMetadata.FromString(",
            "        first_val.tensor.string_val[0])",
            "",
            "    # Content of function_graphs is large and, for instance, device can change.",
            "    self.assertTrue(hasattr(actual_run_metadata, 'function_graphs'))",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotEnableTraceInFunction(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      summary_ops.trace_on(graph=True, profiler=False)",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    with test.mock.patch.object(logging, 'warn') as mock_log:",
            "      f()",
            "      self.assertRegex(",
            "          str(mock_log.call_args), 'Cannot enable trace inside a tf.function.')",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotEnableTraceInGraphMode(self):",
            "    with test.mock.patch.object(logging, 'warn') as mock_log:",
            "      with context.graph_mode():",
            "        summary_ops.trace_on(graph=True, profiler=False)",
            "      self.assertRegex(",
            "          str(mock_log.call_args), 'Must enable trace in eager mode.')",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotExportTraceWithoutTrace(self):",
            "    with six.assertRaisesRegex(self, ValueError,",
            "                               'Must enable trace before export.'):",
            "      summary_ops.trace_export(name='foo', step=1)",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotExportTraceInFunction(self):",
            "    summary_ops.trace_on(graph=True, profiler=False)",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      summary_ops.trace_export(name='foo', step=1)",
            "      return x**y",
            "",
            "    with test.mock.patch.object(logging, 'warn') as mock_log:",
            "      f()",
            "      self.assertRegex(",
            "          str(mock_log.call_args), 'Cannot export trace inside a tf.function.')",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotExportTraceInGraphMode(self):",
            "    with test.mock.patch.object(logging, 'warn') as mock_log:",
            "      with context.graph_mode():",
            "        summary_ops.trace_export(name='foo', step=1)",
            "      self.assertRegex(",
            "          str(mock_log.call_args),",
            "          'Can only export trace while executing eagerly.')",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_usesDefaultStep(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    try:",
            "      summary_ops.set_step(42)",
            "      event = self.run_trace(f, step=None)",
            "      self.assertEqual(42, event.step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_withProfiler(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    summary_ops.trace_on(graph=True, profiler=True)",
            "    profiler_outdir = self.get_temp_dir()",
            "    with writer.as_default():",
            "      f()",
            "      summary_ops.trace_export(",
            "          name='foo', step=1, profiler_outdir=profiler_outdir)",
            "    writer.close()",
            "",
            "  @test_util.run_v2_only",
            "  def testGraph_graph(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    def summary_op_fn():",
            "      summary_ops.graph(f.get_concrete_function().graph)",
            "",
            "    event = self.exec_summary_op(summary_op_fn)",
            "    self.assertIsNotNone(event.graph_def)",
            "",
            "  @test_util.run_v2_only",
            "  def testGraph_graphDef(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    def summary_op_fn():",
            "      summary_ops.graph(f.get_concrete_function().graph.as_graph_def())",
            "",
            "    event = self.exec_summary_op(summary_op_fn)",
            "    self.assertIsNotNone(event.graph_def)",
            "",
            "  @test_util.run_v2_only",
            "  def testGraph_invalidData(self):",
            "    def summary_op_fn():",
            "      summary_ops.graph('hello')",
            "",
            "    with self.assertRaisesRegex(",
            "        ValueError,",
            "        r'\\'graph_data\\' is not tf.Graph or tf.compat.v1.GraphDef',",
            "    ):",
            "      self.exec_summary_op(summary_op_fn)",
            "",
            "  @test_util.run_v2_only",
            "  def testGraph_fromGraphMode(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    @def_function.function",
            "    def g(graph):",
            "      summary_ops.graph(graph)",
            "",
            "    def summary_op_fn():",
            "      graph_def = f.get_concrete_function().graph.as_graph_def(add_shapes=True)",
            "      func_graph = constant_op.constant(graph_def.SerializeToString())",
            "      g(func_graph)",
            "",
            "    with self.assertRaisesRegex(",
            "        ValueError,",
            "        r'graph\\(\\) cannot be invoked inside a graph context.',",
            "    ):",
            "      self.exec_summary_op(summary_op_fn)",
            "",
            "",
            "def events_from_file(filepath):",
            "  \"\"\"Returns all events in a single event file.",
            "",
            "  Args:",
            "    filepath: Path to the event file.",
            "",
            "  Returns:",
            "    A list of all tf.Event protos in the event file.",
            "  \"\"\"",
            "  records = list(tf_record.tf_record_iterator(filepath))",
            "  result = []",
            "  for r in records:",
            "    event = event_pb2.Event()",
            "    event.ParseFromString(r)",
            "    result.append(event)",
            "  return result",
            "",
            "",
            "def events_from_logdir(logdir):",
            "  \"\"\"Returns all events in the single eventfile in logdir.",
            "",
            "  Args:",
            "    logdir: The directory in which the single event file is sought.",
            "",
            "  Returns:",
            "    A list of all tf.Event protos from the single event file.",
            "",
            "  Raises:",
            "    AssertionError: If logdir does not contain exactly one file.",
            "  \"\"\"",
            "  assert gfile.Exists(logdir)",
            "  files = gfile.ListDirectory(logdir)",
            "  assert len(files) == 1, 'Found not exactly one file in logdir: %s' % files",
            "  return events_from_file(os.path.join(logdir, files[0]))",
            "",
            "",
            "def events_from_multifile_logdir(logdir):",
            "  \"\"\"Returns map of filename to events for all `tfevents` files in the logdir.",
            "",
            "  Args:",
            "    logdir: The directory from which to load events.",
            "",
            "  Returns:",
            "    A dict mapping from relative filenames to lists of tf.Event protos.",
            "",
            "  Raises:",
            "    AssertionError: If logdir does not contain exactly one file.",
            "  \"\"\"",
            "  assert gfile.Exists(logdir)",
            "  files = [file for file in gfile.ListDirectory(logdir) if 'tfevents' in file]",
            "  return {file: events_from_file(os.path.join(logdir, file)) for file in files}",
            "",
            "",
            "def to_numpy(summary_value):",
            "  return tensor_util.MakeNdarray(summary_value.tensor)",
            "",
            "",
            "if __name__ == '__main__':",
            "  test.main()"
        ],
        "afterPatchFile": [
            "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Tests for V2 summary ops from summary_ops_v2.\"\"\"",
            "",
            "import os",
            "import unittest",
            "",
            "import six",
            "",
            "from tensorflow.core.framework import graph_pb2",
            "from tensorflow.core.framework import node_def_pb2",
            "from tensorflow.core.framework import step_stats_pb2",
            "from tensorflow.core.framework import summary_pb2",
            "from tensorflow.core.protobuf import config_pb2",
            "from tensorflow.core.util import event_pb2",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.eager import def_function",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import errors",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.framework import tensor_spec",
            "from tensorflow.python.framework import tensor_util",
            "from tensorflow.python.framework import test_util",
            "from tensorflow.python.lib.io import tf_record",
            "from tensorflow.python.module import module",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import summary_ops_v2 as summary_ops",
            "from tensorflow.python.ops import variables",
            "from tensorflow.python.platform import gfile",
            "from tensorflow.python.platform import test",
            "from tensorflow.python.platform import tf_logging as logging",
            "from tensorflow.python.saved_model import load as saved_model_load",
            "from tensorflow.python.saved_model import loader as saved_model_loader",
            "from tensorflow.python.saved_model import save as saved_model_save",
            "from tensorflow.python.saved_model import tag_constants",
            "",
            "",
            "class SummaryOpsCoreTest(test_util.TensorFlowTestCase):",
            "",
            "  def testWrite(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        output = summary_ops.write('tag', 42, step=12)",
            "        self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    value = events[1].summary.value[0]",
            "    self.assertEqual('tag', value.tag)",
            "    self.assertEqual(42, to_numpy(value))",
            "",
            "  def testWrite_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function",
            "      def f():",
            "        with writer.as_default():",
            "          return summary_ops.write('tag', 42, step=12)",
            "      output = f()",
            "      self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    value = events[1].summary.value[0]",
            "    self.assertEqual('tag', value.tag)",
            "    self.assertEqual(42, to_numpy(value))",
            "",
            "  def testWrite_metadata(self):",
            "    logdir = self.get_temp_dir()",
            "    metadata = summary_pb2.SummaryMetadata()",
            "    metadata.plugin_data.plugin_name = 'foo'",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.write('obj', 0, 0, metadata=metadata)",
            "        summary_ops.write('bytes', 0, 0, metadata=metadata.SerializeToString())",
            "        m = constant_op.constant(metadata.SerializeToString())",
            "        summary_ops.write('string_tensor', 0, 0, metadata=m)",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(4, len(events))",
            "    self.assertEqual(metadata, events[1].summary.value[0].metadata)",
            "    self.assertEqual(metadata, events[2].summary.value[0].metadata)",
            "    self.assertEqual(metadata, events[3].summary.value[0].metadata)",
            "",
            "  def testWrite_name(self):",
            "    @def_function.function",
            "    def f():",
            "      output = summary_ops.write('tag', 42, step=12, name='anonymous')",
            "      self.assertTrue(output.name.startswith('anonymous'))",
            "    f()",
            "",
            "  def testWrite_ndarray(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.write('tag', [[1, 2], [3, 4]], step=12)",
            "    events = events_from_logdir(logdir)",
            "    value = events[1].summary.value[0]",
            "    self.assertAllEqual([[1, 2], [3, 4]], to_numpy(value))",
            "",
            "  def testWrite_tensor(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      t = constant_op.constant([[1, 2], [3, 4]])",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.write('tag', t, step=12)",
            "      expected = t.numpy()",
            "    events = events_from_logdir(logdir)",
            "    value = events[1].summary.value[0]",
            "    self.assertAllEqual(expected, to_numpy(value))",
            "",
            "  def testWrite_tensor_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function",
            "      def f(t):",
            "        with writer.as_default():",
            "          summary_ops.write('tag', t, step=12)",
            "      t = constant_op.constant([[1, 2], [3, 4]])",
            "      f(t)",
            "      expected = t.numpy()",
            "    events = events_from_logdir(logdir)",
            "    value = events[1].summary.value[0]",
            "    self.assertAllEqual(expected, to_numpy(value))",
            "",
            "  def testWrite_stringTensor(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.write('tag', [b'foo', b'bar'], step=12)",
            "    events = events_from_logdir(logdir)",
            "    value = events[1].summary.value[0]",
            "    self.assertAllEqual([b'foo', b'bar'], to_numpy(value))",
            "",
            "  @test_util.run_gpu_only",
            "  def testWrite_gpuDeviceContext(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with ops.device('/GPU:0'):",
            "          value = constant_op.constant(42.0)",
            "          step = constant_op.constant(12, dtype=dtypes.int64)",
            "          summary_ops.write('tag', value, step=step).numpy()",
            "    empty_metadata = summary_pb2.SummaryMetadata()",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    self.assertEqual(42, to_numpy(events[1].summary.value[0]))",
            "    self.assertEqual(empty_metadata, events[1].summary.value[0].metadata)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testWrite_noDefaultWriter(self):",
            "    # Use assertAllEqual instead of assertFalse since it works in a defun.",
            "    self.assertAllEqual(False, summary_ops.write('tag', 42, step=0))",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testWrite_noStep_okayIfAlsoNoDefaultWriter(self):",
            "    # Use assertAllEqual instead of assertFalse since it works in a defun.",
            "    self.assertAllEqual(False, summary_ops.write('tag', 42))",
            "",
            "  def testWrite_noStep(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with self.assertRaisesRegex(ValueError, 'No step set'):",
            "          summary_ops.write('tag', 42)",
            "",
            "  def testWrite_noStep_okayIfNotRecordingSummaries(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with summary_ops.record_if(False):",
            "          self.assertFalse(summary_ops.write('tag', 42))",
            "",
            "  def testWrite_usingDefaultStep(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        with summary_ops.create_file_writer_v2(logdir).as_default():",
            "          summary_ops.set_step(1)",
            "          summary_ops.write('tag', 1.0)",
            "          summary_ops.set_step(2)",
            "          summary_ops.write('tag', 1.0)",
            "          mystep = variables.Variable(10, dtype=dtypes.int64)",
            "          summary_ops.set_step(mystep)",
            "          summary_ops.write('tag', 1.0)",
            "          mystep.assign_add(1)",
            "          summary_ops.write('tag', 1.0)",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(5, len(events))",
            "      self.assertEqual(1, events[1].step)",
            "      self.assertEqual(2, events[2].step)",
            "      self.assertEqual(10, events[3].step)",
            "      self.assertEqual(11, events[4].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepConstant_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        @def_function.function",
            "        def f():",
            "          with writer.as_default():",
            "            summary_ops.write('tag', 1.0)",
            "        summary_ops.set_step(1)",
            "        f()",
            "        summary_ops.set_step(2)",
            "        f()",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(3, len(events))",
            "      self.assertEqual(1, events[1].step)",
            "      # The step value will still be 1 because the value was captured at the",
            "      # time the function was first traced.",
            "      self.assertEqual(1, events[2].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepVariable_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        @def_function.function",
            "        def f():",
            "          with writer.as_default():",
            "            summary_ops.write('tag', 1.0)",
            "        mystep = variables.Variable(0, dtype=dtypes.int64)",
            "        summary_ops.set_step(mystep)",
            "        f()",
            "        mystep.assign_add(1)",
            "        f()",
            "        mystep.assign(10)",
            "        f()",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(4, len(events))",
            "      self.assertEqual(0, events[1].step)",
            "      self.assertEqual(1, events[2].step)",
            "      self.assertEqual(10, events[3].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepConstant_fromLegacyGraph(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.graph_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        summary_ops.set_step(1)",
            "        with writer.as_default():",
            "          write_op = summary_ops.write('tag', 1.0)",
            "        summary_ops.set_step(2)",
            "        with self.cached_session() as sess:",
            "          sess.run(writer.init())",
            "          sess.run(write_op)",
            "          sess.run(write_op)",
            "          sess.run(writer.flush())",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(3, len(events))",
            "      self.assertEqual(1, events[1].step)",
            "      # The step value will still be 1 because the value was captured at the",
            "      # time the graph was constructed.",
            "      self.assertEqual(1, events[2].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepVariable_fromLegacyGraph(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.graph_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        mystep = variables.Variable(0, dtype=dtypes.int64)",
            "        summary_ops.set_step(mystep)",
            "        with writer.as_default():",
            "          write_op = summary_ops.write('tag', 1.0)",
            "        first_assign_op = mystep.assign_add(1)",
            "        second_assign_op = mystep.assign(10)",
            "        with self.cached_session() as sess:",
            "          sess.run(writer.init())",
            "          sess.run(mystep.initializer)",
            "          sess.run(write_op)",
            "          sess.run(first_assign_op)",
            "          sess.run(write_op)",
            "          sess.run(second_assign_op)",
            "          sess.run(write_op)",
            "          sess.run(writer.flush())",
            "      events = events_from_logdir(logdir)",
            "      self.assertEqual(4, len(events))",
            "      self.assertEqual(0, events[1].step)",
            "      self.assertEqual(1, events[2].step)",
            "      self.assertEqual(10, events[3].step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStep_fromAsDefault(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        with writer.as_default(step=1):",
            "          summary_ops.write('tag', 1.0)",
            "          with writer.as_default():",
            "            summary_ops.write('tag', 1.0)",
            "            with writer.as_default(step=2):",
            "              summary_ops.write('tag', 1.0)",
            "            summary_ops.write('tag', 1.0)",
            "            summary_ops.set_step(3)",
            "          summary_ops.write('tag', 1.0)",
            "      events = events_from_logdir(logdir)",
            "      self.assertListEqual([1, 1, 2, 1, 3], [e.step for e in events[1:]])",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepVariable_fromAsDefault(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        mystep = variables.Variable(1, dtype=dtypes.int64)",
            "        with writer.as_default(step=mystep):",
            "          summary_ops.write('tag', 1.0)",
            "          with writer.as_default():",
            "            mystep.assign(2)",
            "            summary_ops.write('tag', 1.0)",
            "            with writer.as_default(step=3):",
            "              summary_ops.write('tag', 1.0)",
            "            summary_ops.write('tag', 1.0)",
            "            mystep.assign(4)",
            "          summary_ops.write('tag', 1.0)",
            "      events = events_from_logdir(logdir)",
            "      self.assertListEqual([1, 2, 3, 2, 4], [e.step for e in events[1:]])",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStep_fromSetAsDefault(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        mystep = variables.Variable(1, dtype=dtypes.int64)",
            "        writer.set_as_default(step=mystep)",
            "        summary_ops.write('tag', 1.0)",
            "        mystep.assign(2)",
            "        summary_ops.write('tag', 1.0)",
            "        writer.set_as_default(step=3)",
            "        summary_ops.write('tag', 1.0)",
            "        writer.flush()",
            "      events = events_from_logdir(logdir)",
            "      self.assertListEqual([1, 2, 3], [e.step for e in events[1:]])",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_usingDefaultStepVariable_fromSetAsDefault(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_file_writer_v2(logdir)",
            "        writer.set_as_default(step=1)",
            "        summary_ops.write('tag', 1.0)",
            "        writer.set_as_default(step=2)",
            "        summary_ops.write('tag', 1.0)",
            "        writer.set_as_default()",
            "        summary_ops.write('tag', 1.0)",
            "        writer.flush()",
            "      events = events_from_logdir(logdir)",
            "      self.assertListEqual([1, 2, 2], [e.step for e in events[1:]])",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testWrite_recordIf_constant(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        self.assertTrue(summary_ops.write('default', 1, step=0))",
            "        with summary_ops.record_if(True):",
            "          self.assertTrue(summary_ops.write('set_on', 1, step=0))",
            "        with summary_ops.record_if(False):",
            "          self.assertFalse(summary_ops.write('set_off', 1, step=0))",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(3, len(events))",
            "    self.assertEqual('default', events[1].summary.value[0].tag)",
            "    self.assertEqual('set_on', events[2].summary.value[0].tag)",
            "",
            "  def testWrite_recordIf_constant_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function",
            "      def f():",
            "        with writer.as_default():",
            "          # Use assertAllEqual instead of assertTrue since it works in a defun.",
            "          self.assertAllEqual(summary_ops.write('default', 1, step=0), True)",
            "          with summary_ops.record_if(True):",
            "            self.assertAllEqual(summary_ops.write('set_on', 1, step=0), True)",
            "          with summary_ops.record_if(False):",
            "            self.assertAllEqual(summary_ops.write('set_off', 1, step=0), False)",
            "      f()",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(3, len(events))",
            "    self.assertEqual('default', events[1].summary.value[0].tag)",
            "    self.assertEqual('set_on', events[2].summary.value[0].tag)",
            "",
            "  def testWrite_recordIf_callable(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      step = variables.Variable(-1, dtype=dtypes.int64)",
            "      def record_fn():",
            "        step.assign_add(1)",
            "        return int(step % 2) == 0",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with summary_ops.record_if(record_fn):",
            "          self.assertTrue(summary_ops.write('tag', 1, step=step))",
            "          self.assertFalse(summary_ops.write('tag', 1, step=step))",
            "          self.assertTrue(summary_ops.write('tag', 1, step=step))",
            "          self.assertFalse(summary_ops.write('tag', 1, step=step))",
            "          self.assertTrue(summary_ops.write('tag', 1, step=step))",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(4, len(events))",
            "    self.assertEqual(0, events[1].step)",
            "    self.assertEqual(2, events[2].step)",
            "    self.assertEqual(4, events[3].step)",
            "",
            "  def testWrite_recordIf_callable_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      step = variables.Variable(-1, dtype=dtypes.int64)",
            "      @def_function.function",
            "      def record_fn():",
            "        step.assign_add(1)",
            "        return math_ops.equal(step % 2, 0)",
            "      @def_function.function",
            "      def f():",
            "        with writer.as_default():",
            "          with summary_ops.record_if(record_fn):",
            "            return [",
            "                summary_ops.write('tag', 1, step=step),",
            "                summary_ops.write('tag', 1, step=step),",
            "                summary_ops.write('tag', 1, step=step)]",
            "      self.assertAllEqual(f(), [True, False, True])",
            "      self.assertAllEqual(f(), [False, True, False])",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(4, len(events))",
            "    self.assertEqual(0, events[1].step)",
            "    self.assertEqual(2, events[2].step)",
            "    self.assertEqual(4, events[3].step)",
            "",
            "  def testWrite_recordIf_tensorInput_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function(input_signature=[",
            "          tensor_spec.TensorSpec(shape=[], dtype=dtypes.int64)])",
            "      def f(step):",
            "        with writer.as_default():",
            "          with summary_ops.record_if(math_ops.equal(step % 2, 0)):",
            "            return summary_ops.write('tag', 1, step=step)",
            "      self.assertTrue(f(0))",
            "      self.assertFalse(f(1))",
            "      self.assertTrue(f(2))",
            "      self.assertFalse(f(3))",
            "      self.assertTrue(f(4))",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(4, len(events))",
            "    self.assertEqual(0, events[1].step)",
            "    self.assertEqual(2, events[2].step)",
            "    self.assertEqual(4, events[3].step)",
            "",
            "  def testWriteRawPb(self):",
            "    logdir = self.get_temp_dir()",
            "    pb = summary_pb2.Summary()",
            "    pb.value.add().simple_value = 42.0",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        output = summary_ops.write_raw_pb(pb.SerializeToString(), step=12)",
            "        self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    self.assertProtoEquals(pb, events[1].summary)",
            "",
            "  def testWriteRawPb_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    pb = summary_pb2.Summary()",
            "    pb.value.add().simple_value = 42.0",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function",
            "      def f():",
            "        with writer.as_default():",
            "          return summary_ops.write_raw_pb(pb.SerializeToString(), step=12)",
            "      output = f()",
            "      self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    self.assertProtoEquals(pb, events[1].summary)",
            "",
            "  def testWriteRawPb_multipleValues(self):",
            "    logdir = self.get_temp_dir()",
            "    pb1 = summary_pb2.Summary()",
            "    pb1.value.add().simple_value = 1.0",
            "    pb1.value.add().simple_value = 2.0",
            "    pb2 = summary_pb2.Summary()",
            "    pb2.value.add().simple_value = 3.0",
            "    pb3 = summary_pb2.Summary()",
            "    pb3.value.add().simple_value = 4.0",
            "    pb3.value.add().simple_value = 5.0",
            "    pb3.value.add().simple_value = 6.0",
            "    pbs = [pb.SerializeToString() for pb in (pb1, pb2, pb3)]",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        output = summary_ops.write_raw_pb(pbs, step=12)",
            "        self.assertTrue(output.numpy())",
            "    events = events_from_logdir(logdir)",
            "    self.assertEqual(2, len(events))",
            "    self.assertEqual(12, events[1].step)",
            "    expected_pb = summary_pb2.Summary()",
            "    for i in range(6):",
            "      expected_pb.value.add().simple_value = i + 1.0",
            "    self.assertProtoEquals(expected_pb, events[1].summary)",
            "",
            "  def testWriteRawPb_invalidValue(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with self.assertRaisesRegex(",
            "            errors.DataLossError,",
            "            'Bad tf.compat.v1.Summary binary proto tensor string'):",
            "          summary_ops.write_raw_pb('notaproto', step=12)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testGetSetStep(self):",
            "    try:",
            "      self.assertIsNone(summary_ops.get_step())",
            "      summary_ops.set_step(1)",
            "      # Use assertAllEqual instead of assertEqual since it works in a defun.",
            "      self.assertAllEqual(1, summary_ops.get_step())",
            "      summary_ops.set_step(constant_op.constant(2))",
            "      self.assertAllEqual(2, summary_ops.get_step())",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  def testGetSetStep_variable(self):",
            "    with context.eager_mode():",
            "      try:",
            "        mystep = variables.Variable(0)",
            "        summary_ops.set_step(mystep)",
            "        self.assertAllEqual(0, summary_ops.get_step().read_value())",
            "        mystep.assign_add(1)",
            "        self.assertAllEqual(1, summary_ops.get_step().read_value())",
            "        # Check that set_step() properly maintains reference to variable.",
            "        del mystep",
            "        self.assertAllEqual(1, summary_ops.get_step().read_value())",
            "        summary_ops.get_step().assign_add(1)",
            "        self.assertAllEqual(2, summary_ops.get_step().read_value())",
            "      finally:",
            "        # Reset to default state for other tests.",
            "        summary_ops.set_step(None)",
            "",
            "  def testGetSetStep_variable_fromFunction(self):",
            "    with context.eager_mode():",
            "      try:",
            "        @def_function.function",
            "        def set_step(step):",
            "          summary_ops.set_step(step)",
            "          return summary_ops.get_step()",
            "        @def_function.function",
            "        def get_and_increment():",
            "          summary_ops.get_step().assign_add(1)",
            "          return summary_ops.get_step()",
            "        mystep = variables.Variable(0)",
            "        self.assertAllEqual(0, set_step(mystep))",
            "        self.assertAllEqual(0, summary_ops.get_step().read_value())",
            "        self.assertAllEqual(1, get_and_increment())",
            "        self.assertAllEqual(2, get_and_increment())",
            "        # Check that set_step() properly maintains reference to variable.",
            "        del mystep",
            "        self.assertAllEqual(3, get_and_increment())",
            "      finally:",
            "        # Reset to default state for other tests.",
            "        summary_ops.set_step(None)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testSummaryScope(self):",
            "    with summary_ops.summary_scope('foo') as (tag, scope):",
            "      self.assertEqual('foo', tag)",
            "      self.assertEqual('foo/', scope)",
            "      with summary_ops.summary_scope('bar') as (tag, scope):",
            "        self.assertEqual('foo/bar', tag)",
            "        self.assertEqual('foo/bar/', scope)",
            "      with summary_ops.summary_scope('with/slash') as (tag, scope):",
            "        self.assertEqual('foo/with/slash', tag)",
            "        self.assertEqual('foo/with/slash/', scope)",
            "      with ops.name_scope(None, skip_on_eager=False):",
            "        with summary_ops.summary_scope('unnested') as (tag, scope):",
            "          self.assertEqual('unnested', tag)",
            "          self.assertEqual('unnested/', scope)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testSummaryScope_defaultName(self):",
            "    with summary_ops.summary_scope(None) as (tag, scope):",
            "      self.assertEqual('summary', tag)",
            "      self.assertEqual('summary/', scope)",
            "    with summary_ops.summary_scope(None, 'backup') as (tag, scope):",
            "      self.assertEqual('backup', tag)",
            "      self.assertEqual('backup/', scope)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testSummaryScope_handlesCharactersIllegalForScope(self):",
            "    with summary_ops.summary_scope('f?o?o') as (tag, scope):",
            "      self.assertEqual('f?o?o', tag)",
            "      self.assertEqual('foo/', scope)",
            "    # If all characters aren't legal for a scope name, use default name.",
            "    with summary_ops.summary_scope('???', 'backup') as (tag, scope):",
            "      self.assertEqual('???', tag)",
            "      self.assertEqual('backup/', scope)",
            "",
            "  @test_util.also_run_as_tf_function",
            "  def testSummaryScope_nameNotUniquifiedForTag(self):",
            "    constant_op.constant(0, name='foo')",
            "    with summary_ops.summary_scope('foo') as (tag, _):",
            "      self.assertEqual('foo', tag)",
            "    with summary_ops.summary_scope('foo') as (tag, _):",
            "      self.assertEqual('foo', tag)",
            "    with ops.name_scope('with', skip_on_eager=False):",
            "      constant_op.constant(0, name='slash')",
            "    with summary_ops.summary_scope('with/slash') as (tag, _):",
            "      self.assertEqual('with/slash', tag)",
            "",
            "  def testAllV2SummaryOps(self):",
            "    logdir = self.get_temp_dir()",
            "    def define_ops():",
            "      result = []",
            "      # TF 2.0 summary ops",
            "      result.append(summary_ops.write('write', 1, step=0))",
            "      result.append(summary_ops.write_raw_pb(b'', step=0, name='raw_pb'))",
            "      # TF 1.x tf.contrib.summary ops",
            "      result.append(summary_ops.generic('tensor', 1, step=1))",
            "      result.append(summary_ops.scalar('scalar', 2.0, step=1))",
            "      result.append(summary_ops.histogram('histogram', [1.0], step=1))",
            "      result.append(summary_ops.image('image', [[[[1.0]]]], step=1))",
            "      result.append(summary_ops.audio('audio', [[1.0]], 1.0, 1, step=1))",
            "      return result",
            "    with context.graph_mode():",
            "      ops_without_writer = define_ops()",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with summary_ops.record_if(True):",
            "          ops_recording_on = define_ops()",
            "        with summary_ops.record_if(False):",
            "          ops_recording_off = define_ops()",
            "      # We should be collecting all ops defined with a default writer present,",
            "      # regardless of whether recording was set on or off, but not those defined",
            "      # without a writer at all.",
            "      del ops_without_writer",
            "      expected_ops = ops_recording_on + ops_recording_off",
            "      self.assertCountEqual(expected_ops, summary_ops.all_v2_summary_ops())",
            "",
            "  def testShouldRecordSummaries_defaultState(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "      w = summary_ops.create_file_writer_v2(logdir)",
            "      self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "      with w.as_default():",
            "        # Should be enabled only when default writer is registered.",
            "        self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "      self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "      with summary_ops.record_if(True):",
            "        # Should be disabled when no default writer, even with record_if(True).",
            "        self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "",
            "  def testShouldRecordSummaries_constants(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        with summary_ops.record_if(True):",
            "          self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "        with summary_ops.record_if(False):",
            "          self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "          with summary_ops.record_if(True):",
            "            self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "",
            "  def testShouldRecordSummaries_variable(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        cond = variables.Variable(False)",
            "        with summary_ops.record_if(cond):",
            "          self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "          cond.assign(True)",
            "          self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "",
            "  def testShouldRecordSummaries_callable(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        cond_box = [False]",
            "        cond = lambda: cond_box[0]",
            "        with summary_ops.record_if(cond):",
            "          self.assertAllEqual(False, summary_ops.should_record_summaries())",
            "          cond_box[0] = True",
            "          self.assertAllEqual(True, summary_ops.should_record_summaries())",
            "",
            "  def testShouldRecordSummaries_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      @def_function.function(input_signature=[",
            "          tensor_spec.TensorSpec(shape=[], dtype=dtypes.bool)])",
            "      def f(cond):",
            "        results = []",
            "        results.append(summary_ops.should_record_summaries())",
            "        with writer.as_default():",
            "          results.append(summary_ops.should_record_summaries())",
            "          with summary_ops.record_if(False):",
            "            results.append(summary_ops.should_record_summaries())",
            "          with summary_ops.record_if(cond):",
            "            results.append(summary_ops.should_record_summaries())",
            "        return results",
            "      self.assertAllEqual([False, True, False, True], f(True))",
            "      self.assertAllEqual([False, True, False, False], f(False))",
            "",
            "  def testHasDefaultWriter_checkWriter(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with self.subTest(name='has_writer'):",
            "        with summary_ops.create_file_writer_v2(logdir).as_default():",
            "          self.assertTrue(summary_ops.has_default_writer())",
            "      with self.subTest(name='no_writer'):",
            "        self.assertFalse(summary_ops.has_default_writer())",
            "",
            "",
            "class SummaryWriterTest(test_util.TensorFlowTestCase):",
            "",
            "  def testCreate_withInitAndClose(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(",
            "          logdir, max_queue=1000, flush_millis=1000000)",
            "      get_total = lambda: len(events_from_logdir(logdir))",
            "      self.assertEqual(1, get_total())  # file_version Event",
            "      # Calling init() again while writer is open has no effect",
            "      writer.init()",
            "      self.assertEqual(1, get_total())",
            "      with writer.as_default():",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(1, get_total())",
            "        # Calling .close() should do an implicit flush",
            "        writer.close()",
            "        self.assertEqual(2, get_total())",
            "",
            "  def testCreate_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    @def_function.function",
            "    def f():",
            "      # Returned SummaryWriter must be stored in a non-local variable so it",
            "      # lives throughout the function execution.",
            "      if not hasattr(f, 'writer'):",
            "        f.writer = summary_ops.create_file_writer_v2(logdir)",
            "    with context.eager_mode():",
            "      f()",
            "    event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "    self.assertEqual(1, len(event_files))",
            "",
            "  def testCreate_graphTensorArgument_raisesError(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.graph_mode():",
            "      logdir_tensor = constant_op.constant(logdir)",
            "    with context.eager_mode():",
            "      with self.assertRaisesRegex(",
            "          ValueError, 'Invalid graph Tensor argument.*logdir'):",
            "        summary_ops.create_file_writer_v2(logdir_tensor)",
            "    self.assertEmpty(gfile.Glob(os.path.join(logdir, '*')))",
            "",
            "  def testCreate_fromFunction_graphTensorArgument_raisesError(self):",
            "    logdir = self.get_temp_dir()",
            "    @def_function.function",
            "    def f():",
            "      summary_ops.create_file_writer_v2(constant_op.constant(logdir))",
            "    with context.eager_mode():",
            "      with self.assertRaisesRegex(",
            "          ValueError, 'Invalid graph Tensor argument.*logdir'):",
            "        f()",
            "    self.assertEmpty(gfile.Glob(os.path.join(logdir, '*')))",
            "",
            "  def testCreate_fromFunction_unpersistedResource_raisesError(self):",
            "    logdir = self.get_temp_dir()",
            "    @def_function.function",
            "    def f():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        pass  # Calling .as_default() is enough to indicate use.",
            "    with context.eager_mode():",
            "      # TODO(nickfelt): change this to a better error",
            "      with self.assertRaisesRegex(",
            "          errors.NotFoundError, 'Resource.*does not exist'):",
            "        f()",
            "    # Even though we didn't use it, an event file will have been created.",
            "    self.assertEqual(1, len(gfile.Glob(os.path.join(logdir, '*'))))",
            "",
            "  def testCreate_immediateSetAsDefault_retainsReference(self):",
            "    logdir = self.get_temp_dir()",
            "    try:",
            "      with context.eager_mode():",
            "        summary_ops.create_file_writer_v2(logdir).set_as_default()",
            "        summary_ops.flush()",
            "    finally:",
            "      # Ensure we clean up no matter how the test executes.",
            "      summary_ops._summary_state.writer = None  # pylint: disable=protected-access",
            "",
            "  def testCreate_immediateAsDefault_retainsReference(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(logdir).as_default():",
            "        summary_ops.flush()",
            "",
            "  def testCreate_avoidsFilenameCollision(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      for _ in range(10):",
            "        summary_ops.create_file_writer_v2(logdir)",
            "    event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "    self.assertLen(event_files, 10)",
            "",
            "  def testCreate_graphMode_avoidsFilenameCollision(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.graph_mode(), ops.Graph().as_default():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      with self.cached_session() as sess:",
            "        for _ in range(10):",
            "          sess.run(writer.init())",
            "          sess.run(writer.close())",
            "    event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "    self.assertLen(event_files, 10)",
            "",
            "  def testNoSharing(self):",
            "    # Two writers with the same logdir should not share state.",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer1 = summary_ops.create_file_writer_v2(logdir)",
            "      with writer1.as_default():",
            "        summary_ops.write('tag', 1, step=1)",
            "      event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(1, len(event_files))",
            "      file1 = event_files[0]",
            "",
            "      writer2 = summary_ops.create_file_writer_v2(logdir)",
            "      with writer2.as_default():",
            "        summary_ops.write('tag', 1, step=2)",
            "      event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(2, len(event_files))",
            "      event_files.remove(file1)",
            "      file2 = event_files[0]",
            "",
            "      # Extra writes to ensure interleaved usage works.",
            "      with writer1.as_default():",
            "        summary_ops.write('tag', 1, step=1)",
            "      with writer2.as_default():",
            "        summary_ops.write('tag', 1, step=2)",
            "",
            "    events = iter(events_from_file(file1))",
            "    self.assertEqual('brain.Event:2', next(events).file_version)",
            "    self.assertEqual(1, next(events).step)",
            "    self.assertEqual(1, next(events).step)",
            "    self.assertRaises(StopIteration, lambda: next(events))",
            "    events = iter(events_from_file(file2))",
            "    self.assertEqual('brain.Event:2', next(events).file_version)",
            "    self.assertEqual(2, next(events).step)",
            "    self.assertEqual(2, next(events).step)",
            "    self.assertRaises(StopIteration, lambda: next(events))",
            "",
            "  def testNoSharing_fromFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    @def_function.function",
            "    def f1():",
            "      if not hasattr(f1, 'writer'):",
            "        f1.writer = summary_ops.create_file_writer_v2(logdir)",
            "      with f1.writer.as_default():",
            "        summary_ops.write('tag', 1, step=1)",
            "    @def_function.function",
            "    def f2():",
            "      if not hasattr(f2, 'writer'):",
            "        f2.writer = summary_ops.create_file_writer_v2(logdir)",
            "      with f2.writer.as_default():",
            "        summary_ops.write('tag', 1, step=2)",
            "    with context.eager_mode():",
            "      f1()",
            "      event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(1, len(event_files))",
            "      file1 = event_files[0]",
            "",
            "      f2()",
            "      event_files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(2, len(event_files))",
            "      event_files.remove(file1)",
            "      file2 = event_files[0]",
            "",
            "      # Extra writes to ensure interleaved usage works.",
            "      f1()",
            "      f2()",
            "",
            "    events = iter(events_from_file(file1))",
            "    self.assertEqual('brain.Event:2', next(events).file_version)",
            "    self.assertEqual(1, next(events).step)",
            "    self.assertEqual(1, next(events).step)",
            "    self.assertRaises(StopIteration, lambda: next(events))",
            "    events = iter(events_from_file(file2))",
            "    self.assertEqual('brain.Event:2', next(events).file_version)",
            "    self.assertEqual(2, next(events).step)",
            "    self.assertEqual(2, next(events).step)",
            "    self.assertRaises(StopIteration, lambda: next(events))",
            "",
            "  def testMaxQueue(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      with summary_ops.create_file_writer_v2(",
            "          logdir, max_queue=1, flush_millis=999999).as_default():",
            "        get_total = lambda: len(events_from_logdir(logdir))",
            "        # Note: First tf.compat.v1.Event is always file_version.",
            "        self.assertEqual(1, get_total())",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(1, get_total())",
            "        # Should flush after second summary since max_queue = 1",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(3, get_total())",
            "",
            "  def testWriterFlush(self):",
            "    logdir = self.get_temp_dir()",
            "    get_total = lambda: len(events_from_logdir(logdir))",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(",
            "          logdir, max_queue=1000, flush_millis=1000000)",
            "      self.assertEqual(1, get_total())  # file_version Event",
            "      with writer.as_default():",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(1, get_total())",
            "        writer.flush()",
            "        self.assertEqual(2, get_total())",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(2, get_total())",
            "      # Exiting the \"as_default()\" should do an implicit flush",
            "      self.assertEqual(3, get_total())",
            "",
            "  def testFlushFunction(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(",
            "          logdir, max_queue=999999, flush_millis=999999)",
            "      with writer.as_default():",
            "        get_total = lambda: len(events_from_logdir(logdir))",
            "        # Note: First tf.compat.v1.Event is always file_version.",
            "        self.assertEqual(1, get_total())",
            "        summary_ops.write('tag', 1, step=0)",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(1, get_total())",
            "        summary_ops.flush()",
            "        self.assertEqual(3, get_total())",
            "        # Test \"writer\" parameter",
            "        summary_ops.write('tag', 1, step=0)",
            "        self.assertEqual(3, get_total())",
            "        summary_ops.flush(writer=writer)",
            "        self.assertEqual(4, get_total())",
            "",
            "  # Regression test for b/228097117.",
            "  def testFlushFunction_disallowsInvalidWriterInput(self):",
            "    with context.eager_mode():",
            "      with self.assertRaisesRegex(ValueError, 'Invalid argument to flush'):",
            "        summary_ops.flush(writer=())",
            "",
            "  @test_util.assert_no_new_tensors",
            "  def testNoMemoryLeak_graphMode(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.graph_mode(), ops.Graph().as_default():",
            "      summary_ops.create_file_writer_v2(logdir)",
            "",
            "  @test_util.assert_no_new_pyobjects_executing_eagerly",
            "  def testNoMemoryLeak_eagerMode(self):",
            "    logdir = self.get_temp_dir()",
            "    with summary_ops.create_file_writer_v2(logdir).as_default():",
            "      summary_ops.write('tag', 1, step=0)",
            "",
            "  def testClose_preventsLaterUse(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      writer.close()",
            "      writer.close()  # redundant close() is a no-op",
            "      writer.flush()  # redundant flush() is a no-op",
            "      with self.assertRaisesRegex(RuntimeError, 'already closed'):",
            "        writer.init()",
            "      with self.assertRaisesRegex(RuntimeError, 'already closed'):",
            "        with writer.as_default():",
            "          self.fail('should not get here')",
            "      with self.assertRaisesRegex(RuntimeError, 'already closed'):",
            "        writer.set_as_default()",
            "",
            "  def testClose_closesOpenFile(self):",
            "    try:",
            "      import psutil  # pylint: disable=g-import-not-at-top",
            "    except ImportError:",
            "      raise unittest.SkipTest('test requires psutil')",
            "    proc = psutil.Process()",
            "    get_open_filenames = lambda: set(info[0] for info in proc.open_files())",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(1, len(files))",
            "      eventfile = files[0]",
            "      self.assertIn(eventfile, get_open_filenames())",
            "      writer.close()",
            "      self.assertNotIn(eventfile, get_open_filenames())",
            "",
            "  def testDereference_closesOpenFile(self):",
            "    try:",
            "      import psutil  # pylint: disable=g-import-not-at-top",
            "    except ImportError:",
            "      raise unittest.SkipTest('test requires psutil')",
            "    proc = psutil.Process()",
            "    get_open_filenames = lambda: set(info[0] for info in proc.open_files())",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      files = gfile.Glob(os.path.join(logdir, '*'))",
            "      self.assertEqual(1, len(files))",
            "      eventfile = files[0]",
            "      self.assertIn(eventfile, get_open_filenames())",
            "      del writer",
            "      self.assertNotIn(eventfile, get_open_filenames())",
            "",
            "",
            "class SummaryWriterSavedModelTest(test_util.TensorFlowTestCase):",
            "",
            "  def testWriter_savedAsModuleProperty_loadInEagerMode(self):",
            "    with context.eager_mode():",
            "      class Model(module.Module):",
            "",
            "        def __init__(self, model_dir):",
            "          self._writer = summary_ops.create_file_writer_v2(",
            "              model_dir, experimental_trackable=True)",
            "",
            "        @def_function.function(input_signature=[",
            "            tensor_spec.TensorSpec(shape=[], dtype=dtypes.int64)",
            "        ])",
            "        def train(self, step):",
            "          with self._writer.as_default():",
            "            summary_ops.write('tag', 'foo', step=step)",
            "          return constant_op.constant(0)",
            "",
            "      logdir = self.get_temp_dir()",
            "      to_export = Model(logdir)",
            "      pre_save_files = set(events_from_multifile_logdir(logdir))",
            "      export_dir = os.path.join(logdir, 'export')",
            "      saved_model_save.save(",
            "          to_export, export_dir, signatures={'train': to_export.train})",
            "",
            "    # Reset context to ensure we don't share any resources with saving code.",
            "    context._reset_context()  # pylint: disable=protected-access",
            "    with context.eager_mode():",
            "      restored = saved_model_load.load(export_dir)",
            "      restored.train(1)",
            "      restored.train(2)",
            "      post_restore_files = set(events_from_multifile_logdir(logdir))",
            "      restored2 = saved_model_load.load(export_dir)",
            "      restored2.train(3)",
            "      restored2.train(4)",
            "      files_to_events = events_from_multifile_logdir(logdir)",
            "      post_restore2_files = set(files_to_events)",
            "      self.assertLen(files_to_events, 3)",
            "      def unwrap_singleton(iterable):",
            "        self.assertLen(iterable, 1)",
            "        return next(iter(iterable))",
            "      restore_file = unwrap_singleton(post_restore_files - pre_save_files)",
            "      restore2_file = unwrap_singleton(post_restore2_files - post_restore_files)",
            "      restore_events = files_to_events[restore_file]",
            "      restore2_events = files_to_events[restore2_file]",
            "      self.assertLen(restore_events, 3)",
            "      self.assertEqual(1, restore_events[1].step)",
            "      self.assertEqual(2, restore_events[2].step)",
            "      self.assertLen(restore2_events, 3)",
            "      self.assertEqual(3, restore2_events[1].step)",
            "      self.assertEqual(4, restore2_events[2].step)",
            "",
            "  def testWriter_savedAsModuleProperty_loadInGraphMode(self):",
            "    with context.eager_mode():",
            "",
            "      class Model(module.Module):",
            "",
            "        def __init__(self, model_dir):",
            "          self._writer = summary_ops.create_file_writer_v2(",
            "              model_dir, experimental_trackable=True)",
            "",
            "        @def_function.function(input_signature=[",
            "            tensor_spec.TensorSpec(shape=[], dtype=dtypes.int64)",
            "        ])",
            "        def train(self, step):",
            "          with self._writer.as_default():",
            "            summary_ops.write('tag', 'foo', step=step)",
            "          return constant_op.constant(0)",
            "",
            "      logdir = self.get_temp_dir()",
            "      to_export = Model(logdir)",
            "      pre_save_files = set(events_from_multifile_logdir(logdir))",
            "      export_dir = os.path.join(logdir, 'export')",
            "      saved_model_save.save(",
            "          to_export, export_dir, signatures={'train': to_export.train})",
            "",
            "    # Reset context to ensure we don't share any resources with saving code.",
            "    context._reset_context()  # pylint: disable=protected-access",
            "",
            "    def load_and_run_model(sess, input_values):",
            "      \"\"\"Load and run the SavedModel signature in the TF 1.x style.\"\"\"",
            "      model = saved_model_loader.load(sess, [tag_constants.SERVING], export_dir)",
            "      signature = model.signature_def['train']",
            "      inputs = list(signature.inputs.values())",
            "      assert len(inputs) == 1, inputs",
            "      outputs = list(signature.outputs.values())",
            "      assert len(outputs) == 1, outputs",
            "      input_tensor = sess.graph.get_tensor_by_name(inputs[0].name)",
            "      output_tensor = sess.graph.get_tensor_by_name(outputs[0].name)",
            "      for v in input_values:",
            "        sess.run(output_tensor, feed_dict={input_tensor: v})",
            "",
            "    with context.graph_mode(), ops.Graph().as_default():",
            "      # Since writer shared_name is fixed, within a single session, all loads of",
            "      # this SavedModel will refer to a single writer resouce, so it will be",
            "      # initialized only once and write to a single file.",
            "      with self.session() as sess:",
            "        load_and_run_model(sess, [1, 2])",
            "        load_and_run_model(sess, [3, 4])",
            "      post_restore_files = set(events_from_multifile_logdir(logdir))",
            "      # New session will recreate the resource and write to a second file.",
            "      with self.session() as sess:",
            "        load_and_run_model(sess, [5, 6])",
            "      files_to_events = events_from_multifile_logdir(logdir)",
            "      post_restore2_files = set(files_to_events)",
            "",
            "    self.assertLen(files_to_events, 3)",
            "    def unwrap_singleton(iterable):",
            "      self.assertLen(iterable, 1)",
            "      return next(iter(iterable))",
            "    restore_file = unwrap_singleton(post_restore_files - pre_save_files)",
            "    restore2_file = unwrap_singleton(post_restore2_files - post_restore_files)",
            "    restore_events = files_to_events[restore_file]",
            "    restore2_events = files_to_events[restore2_file]",
            "    self.assertLen(restore_events, 5)",
            "    self.assertEqual(1, restore_events[1].step)",
            "    self.assertEqual(2, restore_events[2].step)",
            "    self.assertEqual(3, restore_events[3].step)",
            "    self.assertEqual(4, restore_events[4].step)",
            "    self.assertLen(restore2_events, 3)",
            "    self.assertEqual(5, restore2_events[1].step)",
            "    self.assertEqual(6, restore2_events[2].step)",
            "",
            "",
            "class NoopWriterTest(test_util.TensorFlowTestCase):",
            "",
            "  def testNoopWriter_doesNothing(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_noop_writer()",
            "      writer.init()",
            "      with writer.as_default():",
            "        result = summary_ops.write('test', 1.0, step=0)",
            "      writer.flush()",
            "      writer.close()",
            "    self.assertFalse(result)  # Should have found no active writer",
            "    files = gfile.Glob(os.path.join(logdir, '*'))",
            "    self.assertLen(files, 0)",
            "",
            "  def testNoopWriter_asNestedContext_isTransparent(self):",
            "    logdir = self.get_temp_dir()",
            "    with context.eager_mode():",
            "      writer = summary_ops.create_file_writer_v2(logdir)",
            "      noop_writer = summary_ops.create_noop_writer()",
            "      with writer.as_default():",
            "        result1 = summary_ops.write('first', 1.0, step=0)",
            "        with noop_writer.as_default():",
            "          result2 = summary_ops.write('second', 1.0, step=0)",
            "        result3 = summary_ops.write('third', 1.0, step=0)",
            "    # All ops should have written, including the one inside the no-op writer,",
            "    # since it doesn't actively *disable* writing - it just behaves as if that",
            "    # entire `with` block wasn't there at all.",
            "    self.assertAllEqual([result1, result2, result3], [True, True, True])",
            "",
            "  def testNoopWriter_setAsDefault(self):",
            "    try:",
            "      with context.eager_mode():",
            "        writer = summary_ops.create_noop_writer()",
            "        writer.set_as_default()",
            "        result = summary_ops.write('test', 1.0, step=0)",
            "      self.assertFalse(result)  # Should have found no active writer",
            "    finally:",
            "      # Ensure we clean up no matter how the test executes.",
            "      summary_ops._summary_state.writer = None  # pylint: disable=protected-access",
            "",
            "",
            "class SummaryOpsTest(test_util.TensorFlowTestCase):",
            "",
            "  def tearDown(self):",
            "    summary_ops.trace_off()",
            "    super(SummaryOpsTest, self).tearDown()",
            "",
            "  def exec_summary_op(self, summary_op_fn):",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    with writer.as_default():",
            "      summary_op_fn()",
            "    writer.close()",
            "    events = events_from_logdir(logdir)",
            "    return events[1]",
            "",
            "  def run_metadata(self, *args, **kwargs):",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    with writer.as_default():",
            "      summary_ops.run_metadata(*args, **kwargs)",
            "    writer.close()",
            "    events = events_from_logdir(logdir)",
            "    return events[1]",
            "",
            "  def run_metadata_graphs(self, *args, **kwargs):",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    with writer.as_default():",
            "      summary_ops.run_metadata_graphs(*args, **kwargs)",
            "    writer.close()",
            "    events = events_from_logdir(logdir)",
            "    return events[1]",
            "",
            "  def create_run_metadata(self):",
            "    step_stats = step_stats_pb2.StepStats(dev_stats=[",
            "        step_stats_pb2.DeviceStepStats(",
            "            device='cpu:0',",
            "            node_stats=[step_stats_pb2.NodeExecStats(node_name='hello')])",
            "    ])",
            "    return config_pb2.RunMetadata(",
            "        function_graphs=[",
            "            config_pb2.RunMetadata.FunctionGraphs(",
            "                pre_optimization_graph=graph_pb2.GraphDef(",
            "                    node=[node_def_pb2.NodeDef(name='foo')]))",
            "        ],",
            "        step_stats=step_stats)",
            "",
            "  def run_trace(self, f, step=1):",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    summary_ops.trace_on(graph=True, profiler=False)",
            "    with writer.as_default():",
            "      f()",
            "      summary_ops.trace_export(name='foo', step=step)",
            "    writer.close()",
            "    events = events_from_logdir(logdir)",
            "    return events[1]",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadata_usesNameAsTag(self):",
            "    meta = config_pb2.RunMetadata()",
            "",
            "    with ops.name_scope('foo', skip_on_eager=False):",
            "      event = self.run_metadata(name='my_name', data=meta, step=1)",
            "      first_val = event.summary.value[0]",
            "",
            "    self.assertEqual('foo/my_name', first_val.tag)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadata_summaryMetadata(self):",
            "    expected_summary_metadata = \"\"\"",
            "      plugin_data {",
            "        plugin_name: \"graph_run_metadata\"",
            "        content: \"1\"",
            "      }",
            "    \"\"\"",
            "    meta = config_pb2.RunMetadata()",
            "    event = self.run_metadata(name='my_name', data=meta, step=1)",
            "    actual_summary_metadata = event.summary.value[0].metadata",
            "    self.assertProtoEquals(expected_summary_metadata, actual_summary_metadata)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadata_wholeRunMetadata(self):",
            "    expected_run_metadata = \"\"\"",
            "      step_stats {",
            "        dev_stats {",
            "          device: \"cpu:0\"",
            "          node_stats {",
            "            node_name: \"hello\"",
            "          }",
            "        }",
            "      }",
            "      function_graphs {",
            "        pre_optimization_graph {",
            "          node {",
            "            name: \"foo\"",
            "          }",
            "        }",
            "      }",
            "    \"\"\"",
            "    meta = self.create_run_metadata()",
            "    event = self.run_metadata(name='my_name', data=meta, step=1)",
            "    first_val = event.summary.value[0]",
            "",
            "    actual_run_metadata = config_pb2.RunMetadata.FromString(",
            "        first_val.tensor.string_val[0])",
            "    self.assertProtoEquals(expected_run_metadata, actual_run_metadata)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadata_usesDefaultStep(self):",
            "    meta = config_pb2.RunMetadata()",
            "    try:",
            "      summary_ops.set_step(42)",
            "      event = self.run_metadata(name='my_name', data=meta)",
            "      self.assertEqual(42, event.step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadataGraph_usesNameAsTag(self):",
            "    meta = config_pb2.RunMetadata()",
            "",
            "    with ops.name_scope('foo', skip_on_eager=False):",
            "      event = self.run_metadata_graphs(name='my_name', data=meta, step=1)",
            "      first_val = event.summary.value[0]",
            "",
            "    self.assertEqual('foo/my_name', first_val.tag)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadataGraph_summaryMetadata(self):",
            "    expected_summary_metadata = \"\"\"",
            "      plugin_data {",
            "        plugin_name: \"graph_run_metadata_graph\"",
            "        content: \"1\"",
            "      }",
            "    \"\"\"",
            "    meta = config_pb2.RunMetadata()",
            "    event = self.run_metadata_graphs(name='my_name', data=meta, step=1)",
            "    actual_summary_metadata = event.summary.value[0].metadata",
            "    self.assertProtoEquals(expected_summary_metadata, actual_summary_metadata)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadataGraph_runMetadataFragment(self):",
            "    expected_run_metadata = \"\"\"",
            "      function_graphs {",
            "        pre_optimization_graph {",
            "          node {",
            "            name: \"foo\"",
            "          }",
            "        }",
            "      }",
            "    \"\"\"",
            "    meta = self.create_run_metadata()",
            "",
            "    event = self.run_metadata_graphs(name='my_name', data=meta, step=1)",
            "    first_val = event.summary.value[0]",
            "",
            "    actual_run_metadata = config_pb2.RunMetadata.FromString(",
            "        first_val.tensor.string_val[0])",
            "    self.assertProtoEquals(expected_run_metadata, actual_run_metadata)",
            "",
            "  @test_util.run_v2_only",
            "  def testRunMetadataGraph_usesDefaultStep(self):",
            "    meta = config_pb2.RunMetadata()",
            "    try:",
            "      summary_ops.set_step(42)",
            "      event = self.run_metadata_graphs(name='my_name', data=meta)",
            "      self.assertEqual(42, event.step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    event = self.run_trace(f)",
            "",
            "    first_val = event.summary.value[0]",
            "    actual_run_metadata = config_pb2.RunMetadata.FromString(",
            "        first_val.tensor.string_val[0])",
            "",
            "    # Content of function_graphs is large and, for instance, device can change.",
            "    self.assertTrue(hasattr(actual_run_metadata, 'function_graphs'))",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotEnableTraceInFunction(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      summary_ops.trace_on(graph=True, profiler=False)",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    with test.mock.patch.object(logging, 'warn') as mock_log:",
            "      f()",
            "      self.assertRegex(",
            "          str(mock_log.call_args), 'Cannot enable trace inside a tf.function.')",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotEnableTraceInGraphMode(self):",
            "    with test.mock.patch.object(logging, 'warn') as mock_log:",
            "      with context.graph_mode():",
            "        summary_ops.trace_on(graph=True, profiler=False)",
            "      self.assertRegex(",
            "          str(mock_log.call_args), 'Must enable trace in eager mode.')",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotExportTraceWithoutTrace(self):",
            "    with six.assertRaisesRegex(self, ValueError,",
            "                               'Must enable trace before export.'):",
            "      summary_ops.trace_export(name='foo', step=1)",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotExportTraceInFunction(self):",
            "    summary_ops.trace_on(graph=True, profiler=False)",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      summary_ops.trace_export(name='foo', step=1)",
            "      return x**y",
            "",
            "    with test.mock.patch.object(logging, 'warn') as mock_log:",
            "      f()",
            "      self.assertRegex(",
            "          str(mock_log.call_args), 'Cannot export trace inside a tf.function.')",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_cannotExportTraceInGraphMode(self):",
            "    with test.mock.patch.object(logging, 'warn') as mock_log:",
            "      with context.graph_mode():",
            "        summary_ops.trace_export(name='foo', step=1)",
            "      self.assertRegex(",
            "          str(mock_log.call_args),",
            "          'Can only export trace while executing eagerly.')",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_usesDefaultStep(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    try:",
            "      summary_ops.set_step(42)",
            "      event = self.run_trace(f, step=None)",
            "      self.assertEqual(42, event.step)",
            "    finally:",
            "      # Reset to default state for other tests.",
            "      summary_ops.set_step(None)",
            "",
            "  @test_util.run_v2_only",
            "  def testTrace_withProfiler(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    assert context.executing_eagerly()",
            "    logdir = self.get_temp_dir()",
            "    writer = summary_ops.create_file_writer_v2(logdir)",
            "    summary_ops.trace_on(graph=True, profiler=True)",
            "    profiler_outdir = self.get_temp_dir()",
            "    with writer.as_default():",
            "      f()",
            "      summary_ops.trace_export(",
            "          name='foo', step=1, profiler_outdir=profiler_outdir)",
            "    writer.close()",
            "",
            "  @test_util.run_v2_only",
            "  def testGraph_graph(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    def summary_op_fn():",
            "      summary_ops.graph(f.get_concrete_function().graph)",
            "",
            "    event = self.exec_summary_op(summary_op_fn)",
            "    self.assertIsNotNone(event.graph_def)",
            "",
            "  @test_util.run_v2_only",
            "  def testGraph_graphDef(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    def summary_op_fn():",
            "      summary_ops.graph(f.get_concrete_function().graph.as_graph_def())",
            "",
            "    event = self.exec_summary_op(summary_op_fn)",
            "    self.assertIsNotNone(event.graph_def)",
            "",
            "  @test_util.run_v2_only",
            "  def testGraph_invalidData(self):",
            "    def summary_op_fn():",
            "      summary_ops.graph('hello')",
            "",
            "    with self.assertRaisesRegex(",
            "        ValueError,",
            "        r'\\'graph_data\\' is not tf.Graph or tf.compat.v1.GraphDef',",
            "    ):",
            "      self.exec_summary_op(summary_op_fn)",
            "",
            "  @test_util.run_v2_only",
            "  def testGraph_fromGraphMode(self):",
            "",
            "    @def_function.function",
            "    def f():",
            "      x = constant_op.constant(2)",
            "      y = constant_op.constant(3)",
            "      return x**y",
            "",
            "    @def_function.function",
            "    def g(graph):",
            "      summary_ops.graph(graph)",
            "",
            "    def summary_op_fn():",
            "      graph_def = f.get_concrete_function().graph.as_graph_def(add_shapes=True)",
            "      func_graph = constant_op.constant(graph_def.SerializeToString())",
            "      g(func_graph)",
            "",
            "    with self.assertRaisesRegex(",
            "        ValueError,",
            "        r'graph\\(\\) cannot be invoked inside a graph context.',",
            "    ):",
            "      self.exec_summary_op(summary_op_fn)",
            "",
            "",
            "def events_from_file(filepath):",
            "  \"\"\"Returns all events in a single event file.",
            "",
            "  Args:",
            "    filepath: Path to the event file.",
            "",
            "  Returns:",
            "    A list of all tf.Event protos in the event file.",
            "  \"\"\"",
            "  records = list(tf_record.tf_record_iterator(filepath))",
            "  result = []",
            "  for r in records:",
            "    event = event_pb2.Event()",
            "    event.ParseFromString(r)",
            "    result.append(event)",
            "  return result",
            "",
            "",
            "def events_from_logdir(logdir):",
            "  \"\"\"Returns all events in the single eventfile in logdir.",
            "",
            "  Args:",
            "    logdir: The directory in which the single event file is sought.",
            "",
            "  Returns:",
            "    A list of all tf.Event protos from the single event file.",
            "",
            "  Raises:",
            "    AssertionError: If logdir does not contain exactly one file.",
            "  \"\"\"",
            "  assert gfile.Exists(logdir)",
            "  files = gfile.ListDirectory(logdir)",
            "  assert len(files) == 1, 'Found not exactly one file in logdir: %s' % files",
            "  return events_from_file(os.path.join(logdir, files[0]))",
            "",
            "",
            "def events_from_multifile_logdir(logdir):",
            "  \"\"\"Returns map of filename to events for all `tfevents` files in the logdir.",
            "",
            "  Args:",
            "    logdir: The directory from which to load events.",
            "",
            "  Returns:",
            "    A dict mapping from relative filenames to lists of tf.Event protos.",
            "",
            "  Raises:",
            "    AssertionError: If logdir does not contain exactly one file.",
            "  \"\"\"",
            "  assert gfile.Exists(logdir)",
            "  files = [file for file in gfile.ListDirectory(logdir) if 'tfevents' in file]",
            "  return {file: events_from_file(os.path.join(logdir, file)) for file in files}",
            "",
            "",
            "def to_numpy(summary_value):",
            "  return tensor_util.MakeNdarray(summary_value.tensor)",
            "",
            "",
            "if __name__ == '__main__':",
            "  test.main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "988": [
                "SummaryWriterTest",
                "testFlushFunction"
            ],
            "989": [
                "SummaryWriterTest",
                "testFlushFunction"
            ],
            "990": [
                "SummaryWriterTest",
                "testFlushFunction"
            ],
            "991": [
                "SummaryWriterTest",
                "testFlushFunction"
            ]
        },
        "addLocation": []
    },
    "tensorflow/python/ops/summary_ops_v2.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1111,
                "afterPatchRowNumber": 1111,
                "PatchRowcode": "   Returns:"
            },
            "1": {
                "beforePatchRowNumber": 1112,
                "afterPatchRowNumber": 1112,
                "PatchRowcode": "     The created `tf.Operation`."
            },
            "2": {
                "beforePatchRowNumber": 1113,
                "afterPatchRowNumber": 1113,
                "PatchRowcode": "   \"\"\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1114,
                "PatchRowcode": "+  del name  # unused"
            },
            "4": {
                "beforePatchRowNumber": 1114,
                "afterPatchRowNumber": 1115,
                "PatchRowcode": "   if writer is None:"
            },
            "5": {
                "beforePatchRowNumber": 1115,
                "afterPatchRowNumber": 1116,
                "PatchRowcode": "     writer = _summary_state.writer"
            },
            "6": {
                "beforePatchRowNumber": 1116,
                "afterPatchRowNumber": 1117,
                "PatchRowcode": "     if writer is None:"
            },
            "7": {
                "beforePatchRowNumber": 1117,
                "afterPatchRowNumber": 1118,
                "PatchRowcode": "       return control_flow_ops.no_op()"
            },
            "8": {
                "beforePatchRowNumber": 1118,
                "afterPatchRowNumber": 1119,
                "PatchRowcode": "   if isinstance(writer, SummaryWriter):"
            },
            "9": {
                "beforePatchRowNumber": 1119,
                "afterPatchRowNumber": 1120,
                "PatchRowcode": "     return writer.flush()"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1121,
                "PatchRowcode": "+  raise ValueError(\"Invalid argument to flush(): %r\" % (writer,))"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1122,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1123,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1124,
                "PatchRowcode": "+def legacy_raw_flush(writer=None, name=None):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1125,
                "PatchRowcode": "+  \"\"\"Legacy version of flush() that accepts a raw resource tensor for `writer`."
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1126,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1127,
                "PatchRowcode": "+  Do not use this function in any new code. Not supported and not part of the"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1128,
                "PatchRowcode": "+  public TF APIs."
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1129,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1130,
                "PatchRowcode": "+  Args:"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1131,
                "PatchRowcode": "+    writer: The `tf.summary.SummaryWriter` to flush. If None, the current"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1132,
                "PatchRowcode": "+      default writer will be used instead; if there is no current writer, this"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1133,
                "PatchRowcode": "+      returns `tf.no_op`. For this legacy version only, also accepts a raw"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1134,
                "PatchRowcode": "+      resource tensor pointing to the underlying C++ writer resource."
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1135,
                "PatchRowcode": "+    name: Ignored legacy argument for a name for the operation."
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1136,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1137,
                "PatchRowcode": "+  Returns:"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1138,
                "PatchRowcode": "+    The created `tf.Operation`."
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1139,
                "PatchRowcode": "+  \"\"\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1140,
                "PatchRowcode": "+  if writer is None or isinstance(writer, SummaryWriter):"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1141,
                "PatchRowcode": "+    # Forward to the TF2 implementation of flush() when possible."
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1142,
                "PatchRowcode": "+    return flush(writer, name)"
            },
            "32": {
                "beforePatchRowNumber": 1120,
                "afterPatchRowNumber": 1143,
                "PatchRowcode": "   else:"
            },
            "33": {
                "beforePatchRowNumber": 1121,
                "afterPatchRowNumber": 1144,
                "PatchRowcode": "     # Legacy fallback in case we were passed a raw resource tensor."
            },
            "34": {
                "beforePatchRowNumber": 1122,
                "afterPatchRowNumber": 1145,
                "PatchRowcode": "     with ops.device(\"cpu:0\"):"
            }
        },
        "frontPatchFile": [
            "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "",
            "\"\"\"Operations to emit summaries.\"\"\"",
            "",
            "import abc",
            "import collections",
            "import functools",
            "import os",
            "import re",
            "import threading",
            "",
            "from tensorflow.core.framework import graph_pb2",
            "from tensorflow.core.framework import summary_pb2",
            "from tensorflow.core.protobuf import config_pb2",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.eager import profiler as _profiler",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.framework import smart_cond",
            "from tensorflow.python.framework import tensor_util",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import control_flow_ops",
            "from tensorflow.python.ops import gen_resource_variable_ops",
            "from tensorflow.python.ops import gen_summary_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import resource_variable_ops",
            "from tensorflow.python.ops import summary_op_util",
            "from tensorflow.python.platform import tf_logging as logging",
            "from tensorflow.python.training import training_util",
            "from tensorflow.python.training.tracking import tracking",
            "from tensorflow.python.util import deprecation",
            "from tensorflow.python.util import tf_contextlib",
            "from tensorflow.python.util.tf_export import tf_export",
            "",
            "# Name for graph collection of summary writer init ops, which is only exposed",
            "# as a legacy API for tf.contrib.summary in TF 1.x.",
            "_SUMMARY_WRITER_INIT_COLLECTION_NAME = \"_SUMMARY_WRITER_V2\"",
            "",
            "",
            "class _SummaryState(threading.local):",
            "",
            "  def __init__(self):",
            "    super(_SummaryState, self).__init__()",
            "    self.is_recording = None",
            "    # TODO(slebedev): why a separate flag for DS and is it on by default?",
            "    self.is_recording_distribution_strategy = True",
            "    self.writer = None",
            "    self.step = None",
            "",
            "",
            "_summary_state = _SummaryState()",
            "",
            "",
            "class _SummaryContextManager:",
            "  \"\"\"Context manager to implement SummaryWriter.as_default().\"\"\"",
            "  # Note: this is a class so that it's possible to implement `set_as_default()`",
            "  # simply via `as_default().__enter__()`. We can't do that with @contextmanager",
            "  # because the `finally` block will be executed when the generator is GCed.",
            "",
            "  def __init__(self, writer, step=None):",
            "    self._writer = writer",
            "    self._step = step",
            "    self._old_writer = None",
            "    self._old_step = None",
            "",
            "  def __enter__(self):",
            "    self._old_writer = _summary_state.writer",
            "    _summary_state.writer = self._writer",
            "    if self._step is not None:",
            "      self._old_step = _summary_state.step",
            "      _summary_state.step = self._step",
            "    return self._writer",
            "",
            "  def __exit__(self, *exc):",
            "    # Flushes the summary writer in eager mode or in graph functions, but",
            "    # not in legacy graph mode (you're on your own there).",
            "    _summary_state.writer.flush()",
            "    _summary_state.writer = self._old_writer",
            "    if self._step is not None:",
            "      _summary_state.step = self._old_step",
            "    return False",
            "",
            "",
            "def _should_record_summaries_internal(default_state):",
            "  \"\"\"Returns boolean Tensor if summaries should/shouldn't be recorded.",
            "",
            "  Now the summary condition is decided by logical \"and\" of below conditions:",
            "  First, summary writer must be set. Given this constraint is met,",
            "  ctx.summary_recording and ctx.summary_recording_distribution_strategy.",
            "  The former one is usually set by user, and the latter one is controlled",
            "  by DistributionStrategy (tf.distribute.ReplicaContext).",
            "",
            "  Args:",
            "    default_state: can be True or False. The default summary behavior when",
            "    summary writer is set and the user does not specify",
            "    ctx.summary_recording and ctx.summary_recording_distribution_strategy",
            "    is True.",
            "  \"\"\"",
            "  if _summary_state.writer is None:",
            "    return constant_op.constant(False)",
            "",
            "  if not callable(_summary_state.is_recording):",
            "    static_cond = tensor_util.constant_value(_summary_state.is_recording)",
            "    if static_cond is not None and not static_cond:",
            "      return constant_op.constant(False)",
            "",
            "  resolve = lambda x: x() if callable(x) else x",
            "  cond_distributed = resolve(_summary_state.is_recording_distribution_strategy)",
            "  cond = resolve(_summary_state.is_recording)",
            "  if cond is None:",
            "    cond = default_state",
            "  return math_ops.logical_and(cond_distributed, cond)",
            "",
            "",
            "@tf_export(\"summary.should_record_summaries\", v1=[])",
            "def should_record_summaries():",
            "  \"\"\"Returns boolean Tensor which is True if summaries will be recorded.",
            "",
            "  If no default summary writer is currently registered, this always returns",
            "  False. Otherwise, this reflects the recording condition has been set via",
            "  `tf.summary.record_if()` (except that it may return False for some replicas",
            "  when using `tf.distribute.Strategy`). If no recording condition is active,",
            "  it defaults to True.",
            "  \"\"\"",
            "  return _should_record_summaries_internal(default_state=True)",
            "",
            "",
            "# Legacy symbol used by tf.contrib.summary.should_record_summaries.",
            "def _legacy_contrib_should_record_summaries():",
            "  \"\"\"Returns boolean Tensor which is true if summaries should be recorded.\"\"\"",
            "  return _should_record_summaries_internal(default_state=False)",
            "",
            "",
            "@tf_export(\"summary.record_if\", v1=[])",
            "@tf_contextlib.contextmanager",
            "def record_if(condition):",
            "  \"\"\"Sets summary recording on or off per the provided boolean value.",
            "",
            "  The provided value can be a python boolean, a scalar boolean Tensor, or",
            "  or a callable providing such a value; if a callable is passed it will be",
            "  invoked on-demand to determine whether summary writing will occur.  Note that",
            "  when calling record_if() in an eager mode context, if you intend to provide a",
            "  varying condition like `step % 100 == 0`, you must wrap this in a",
            "  callable to avoid immediate eager evaluation of the condition.  In particular,",
            "  using a callable is the only way to have your condition evaluated as part of",
            "  the traced body of an @tf.function that is invoked from within the",
            "  `record_if()` context.",
            "",
            "  Args:",
            "    condition: can be True, False, a bool Tensor, or a callable providing such.",
            "",
            "  Yields:",
            "    Returns a context manager that sets this value on enter and restores the",
            "    previous value on exit.",
            "  \"\"\"",
            "  old = _summary_state.is_recording",
            "  try:",
            "    _summary_state.is_recording = condition",
            "    yield",
            "  finally:",
            "    _summary_state.is_recording = old",
            "",
            "",
            "def has_default_writer():",
            "  \"\"\"Returns a boolean indicating whether a default summary writer exists.\"\"\"",
            "  return _summary_state.writer is not None",
            "",
            "",
            "# TODO(apassos) consider how to handle local step here.",
            "def record_summaries_every_n_global_steps(n, global_step=None):",
            "  \"\"\"Sets the should_record_summaries Tensor to true if global_step % n == 0.\"\"\"",
            "  if global_step is None:",
            "    global_step = training_util.get_or_create_global_step()",
            "  with ops.device(\"cpu:0\"):",
            "    should = lambda: math_ops.equal(global_step % n, 0)",
            "    if not context.executing_eagerly():",
            "      should = should()",
            "  return record_if(should)",
            "",
            "",
            "def always_record_summaries():",
            "  \"\"\"Sets the should_record_summaries Tensor to always true.\"\"\"",
            "  return record_if(True)",
            "",
            "",
            "def never_record_summaries():",
            "  \"\"\"Sets the should_record_summaries Tensor to always false.\"\"\"",
            "  return record_if(False)",
            "",
            "",
            "@tf_export(\"summary.experimental.get_step\", v1=[])",
            "def get_step():",
            "  \"\"\"Returns the default summary step for the current thread.",
            "",
            "  Returns:",
            "    The step set by `tf.summary.experimental.set_step()` if one has been set,",
            "    otherwise None.",
            "  \"\"\"",
            "  return _summary_state.step",
            "",
            "",
            "@tf_export(\"summary.experimental.set_step\", v1=[])",
            "def set_step(step):",
            "  \"\"\"Sets the default summary step for the current thread.",
            "",
            "  For convenience, this function sets a default value for the `step` parameter",
            "  used in summary-writing functions elsewhere in the API so that it need not",
            "  be explicitly passed in every such invocation. The value can be a constant",
            "  or a variable, and can be retrieved via `tf.summary.experimental.get_step()`.",
            "",
            "  Note: when using this with @tf.functions, the step value will be captured at",
            "  the time the function is traced, so changes to the step outside the function",
            "  will not be reflected inside the function unless using a `tf.Variable` step.",
            "",
            "  Args:",
            "    step: An `int64`-castable default step value, or None to unset.",
            "  \"\"\"",
            "  _summary_state.step = step",
            "",
            "",
            "@tf_export(\"summary.SummaryWriter\", v1=[])",
            "class SummaryWriter(metaclass=abc.ABCMeta):",
            "  \"\"\"Interface representing a stateful summary writer object.\"\"\"",
            "",
            "  def set_as_default(self, step=None):",
            "    \"\"\"Enables this summary writer for the current thread.",
            "",
            "    For convenience, if `step` is not None, this function also sets a default",
            "    value for the `step` parameter used in summary-writing functions elsewhere",
            "    in the API so that it need not be explicitly passed in every such",
            "    invocation. The value can be a constant or a variable.",
            "",
            "    Note: when setting `step` in a @tf.function, the step value will be",
            "    captured at the time the function is traced, so changes to the step outside",
            "    the function will not be reflected inside the function unless using",
            "    a `tf.Variable` step.",
            "",
            "    Args:",
            "      step: An `int64`-castable default step value, or `None`. When not `None`,",
            "        the current step is modified to the given value. When `None`, the",
            "        current step is not modified.",
            "    \"\"\"",
            "    self.as_default(step).__enter__()",
            "",
            "  def as_default(self, step=None):",
            "    \"\"\"Returns a context manager that enables summary writing.",
            "",
            "    For convenience, if `step` is not None, this function also sets a default",
            "    value for the `step` parameter used in summary-writing functions elsewhere",
            "    in the API so that it need not be explicitly passed in every such",
            "    invocation. The value can be a constant or a variable.",
            "",
            "    Note: when setting `step` in a @tf.function, the step value will be",
            "    captured at the time the function is traced, so changes to the step outside",
            "    the function will not be reflected inside the function unless using",
            "    a `tf.Variable` step.",
            "",
            "    For example, `step` can be used as:",
            "",
            "    ```python",
            "    with writer_a.as_default(step=10):",
            "      tf.summary.scalar(tag, value)   # Logged to writer_a with step 10",
            "      with writer_b.as_default(step=20):",
            "        tf.summary.scalar(tag, value) # Logged to writer_b with step 20",
            "      tf.summary.scalar(tag, value)   # Logged to writer_a with step 10",
            "    ```",
            "",
            "    Args:",
            "      step: An `int64`-castable default step value, or `None`. When not `None`,",
            "        the current step is captured, replaced by a given one, and the original",
            "        one is restored when the context manager exits. When `None`, the current",
            "        step is not modified (and not restored when the context manager exits).",
            "",
            "    Returns:",
            "      The context manager.",
            "    \"\"\"",
            "    return _SummaryContextManager(self, step)",
            "",
            "  def init(self):",
            "    \"\"\"Initializes the summary writer.\"\"\"",
            "    raise NotImplementedError()",
            "",
            "  def flush(self):",
            "    \"\"\"Flushes any buffered data.\"\"\"",
            "    raise NotImplementedError()",
            "",
            "  def close(self):",
            "    \"\"\"Flushes and closes the summary writer.\"\"\"",
            "    raise NotImplementedError()",
            "",
            "",
            "class _ResourceSummaryWriter(SummaryWriter):",
            "  \"\"\"Implementation of SummaryWriter using a SummaryWriterInterface resource.\"\"\"",
            "",
            "  def __init__(self, create_fn, init_op_fn):",
            "    self._resource = create_fn()",
            "    self._init_op = init_op_fn(self._resource)",
            "    self._closed = False",
            "    if context.executing_eagerly():",
            "      self._set_up_resource_deleter()",
            "    else:",
            "      ops.add_to_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME, self._init_op)",
            "",
            "  # Extension point to be overridden by subclasses to customize deletion.",
            "",
            "  def _set_up_resource_deleter(self):",
            "    self._resource_deleter = resource_variable_ops.EagerResourceDeleter(",
            "        handle=self._resource, handle_device=\"cpu:0\")",
            "",
            "  def set_as_default(self, step=None):",
            "    \"\"\"See `SummaryWriter.set_as_default`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      raise RuntimeError(f\"SummaryWriter {self!r} is already closed\")",
            "    super().set_as_default(step)",
            "",
            "  def as_default(self, step=None):",
            "    \"\"\"See `SummaryWriter.as_default`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      raise RuntimeError(f\"SummaryWriter {self!r} is already closed\")",
            "    return super().as_default(step)",
            "",
            "  def init(self):",
            "    \"\"\"See `SummaryWriter.init`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      raise RuntimeError(f\"SummaryWriter {self!r} is already closed\")",
            "    return self._init_op",
            "",
            "  def flush(self):",
            "    \"\"\"See `SummaryWriter.flush`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      return",
            "    with ops.device(\"cpu:0\"):",
            "      return gen_summary_ops.flush_summary_writer(self._resource)",
            "",
            "  def close(self):",
            "    \"\"\"See `SummaryWriter.close`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      return",
            "    try:",
            "      with ops.control_dependencies([self.flush()]):",
            "        with ops.device(\"cpu:0\"):",
            "          return gen_summary_ops.close_summary_writer(self._resource)",
            "    finally:",
            "      if context.executing_eagerly():",
            "        self._closed = True",
            "",
            "",
            "class _MultiMetaclass(",
            "    type(_ResourceSummaryWriter), type(tracking.TrackableResource)):",
            "  pass",
            "",
            "",
            "class _TrackableResourceSummaryWriter(",
            "    _ResourceSummaryWriter,",
            "    tracking.TrackableResource,",
            "    metaclass=_MultiMetaclass):",
            "  \"\"\"A `_ResourceSummaryWriter` subclass that implements `TrackableResource`.\"\"\"",
            "",
            "  def __init__(self, create_fn, init_op_fn):",
            "    # Resolve multiple inheritance via explicit calls to __init__() on parents.",
            "    tracking.TrackableResource.__init__(self, device=\"/CPU:0\")",
            "    self._create_fn = create_fn",
            "    self._init_op_fn = init_op_fn",
            "    # Pass .resource_handle into _ResourceSummaryWriter parent class rather than",
            "    # create_fn, to ensure it accesses the resource handle only through the",
            "    # cached property so that everything is using a single resource handle.",
            "    _ResourceSummaryWriter.__init__(",
            "        self, create_fn=lambda: self.resource_handle, init_op_fn=init_op_fn)",
            "",
            "  # Override for TrackableResource implementation.",
            "  def _create_resource(self):",
            "    return self._create_fn()",
            "",
            "  # Override for TrackableResource implementation.",
            "  def _initialize(self):",
            "    return self._init_op_fn(self.resource_handle)",
            "",
            "  # Override for TrackableResource implementation.",
            "  def _destroy_resource(self):",
            "    gen_resource_variable_ops.destroy_resource_op(",
            "        self.resource_handle, ignore_lookup_error=True)",
            "",
            "  def _set_up_resource_deleter(self):",
            "    # Override to suppress ResourceSummaryWriter implementation; we don't need",
            "    # the deleter since TrackableResource.__del__() handles it for us.",
            "    pass",
            "",
            "",
            "class _LegacyResourceSummaryWriter(SummaryWriter):",
            "  \"\"\"Legacy resource-backed SummaryWriter for tf.contrib.summary.\"\"\"",
            "",
            "  def  __init__(self, resource, init_op_fn):",
            "    self._resource = resource",
            "    self._init_op_fn = init_op_fn",
            "    init_op = self.init()",
            "    if context.executing_eagerly():",
            "      self._resource_deleter = resource_variable_ops.EagerResourceDeleter(",
            "          handle=self._resource, handle_device=\"cpu:0\")",
            "    else:",
            "      ops.add_to_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME, init_op)",
            "",
            "  def init(self):",
            "    \"\"\"See `SummaryWriter.init`.\"\"\"",
            "    return self._init_op_fn(self._resource)",
            "",
            "  def flush(self):",
            "    \"\"\"See `SummaryWriter.flush`.\"\"\"",
            "    with ops.device(\"cpu:0\"):",
            "      return gen_summary_ops.flush_summary_writer(self._resource)",
            "",
            "  def close(self):",
            "    \"\"\"See `SummaryWriter.close`.\"\"\"",
            "    with ops.control_dependencies([self.flush()]):",
            "      with ops.device(\"cpu:0\"):",
            "        return gen_summary_ops.close_summary_writer(self._resource)",
            "",
            "",
            "class _NoopSummaryWriter(SummaryWriter):",
            "  \"\"\"A summary writer that does nothing, for create_noop_writer().\"\"\"",
            "",
            "  def set_as_default(self, step=None):",
            "    pass",
            "",
            "  @tf_contextlib.contextmanager",
            "  def as_default(self, step=None):",
            "    yield",
            "",
            "  def init(self):",
            "    pass",
            "",
            "  def flush(self):",
            "    pass",
            "",
            "  def close(self):",
            "    pass",
            "",
            "",
            "@tf_export(v1=[\"summary.initialize\"])",
            "def initialize(",
            "    graph=None,  # pylint: disable=redefined-outer-name",
            "    session=None):",
            "  \"\"\"Initializes summary writing for graph execution mode.",
            "",
            "  This operation is a no-op when executing eagerly.",
            "",
            "  This helper method provides a higher-level alternative to using",
            "  `tf.contrib.summary.summary_writer_initializer_op` and",
            "  `tf.contrib.summary.graph`.",
            "",
            "  Most users will also want to call `tf.compat.v1.train.create_global_step`",
            "  which can happen before or after this function is called.",
            "",
            "  Args:",
            "    graph: A `tf.Graph` or `tf.compat.v1.GraphDef` to output to the writer.",
            "      This function will not write the default graph by default. When",
            "      writing to an event log file, the associated step will be zero.",
            "    session: So this method can call `tf.Session.run`. This defaults",
            "      to `tf.compat.v1.get_default_session`.",
            "",
            "  Raises:",
            "    RuntimeError: If  the current thread has no default",
            "      `tf.contrib.summary.SummaryWriter`.",
            "    ValueError: If session wasn't passed and no default session.",
            "  \"\"\"",
            "  if context.executing_eagerly():",
            "    return",
            "  if _summary_state.writer is None:",
            "    raise RuntimeError(\"No default tf.contrib.summary.SummaryWriter found\")",
            "  if session is None:",
            "    session = ops.get_default_session()",
            "    if session is None:",
            "      raise ValueError(\"Argument `session must be passed if no default \"",
            "                       \"session exists\")",
            "  session.run(summary_writer_initializer_op())",
            "  if graph is not None:",
            "    data = _serialize_graph(graph)",
            "    x = array_ops.placeholder(dtypes.string)",
            "    session.run(graph_v1(x, 0), feed_dict={x: data})",
            "",
            "",
            "@tf_export(\"summary.create_file_writer\", v1=[])",
            "def create_file_writer_v2(logdir,",
            "                          max_queue=None,",
            "                          flush_millis=None,",
            "                          filename_suffix=None,",
            "                          name=None,",
            "                          experimental_trackable=False):",
            "  \"\"\"Creates a summary file writer for the given log directory.",
            "",
            "  Args:",
            "    logdir: a string specifying the directory in which to write an event file.",
            "    max_queue: the largest number of summaries to keep in a queue; will",
            "     flush once the queue gets bigger than this. Defaults to 10.",
            "    flush_millis: the largest interval between flushes. Defaults to 120,000.",
            "    filename_suffix: optional suffix for the event file name. Defaults to `.v2`.",
            "    name: a name for the op that creates the writer.",
            "    experimental_trackable: a boolean that controls whether the returned writer",
            "      will be a `TrackableResource`, which makes it compatible with SavedModel",
            "      when used as a `tf.Module` property.",
            "",
            "  Returns:",
            "    A SummaryWriter object.",
            "  \"\"\"",
            "  if logdir is None:",
            "    raise ValueError(\"Argument `logdir` cannot be None\")",
            "  inside_function = ops.inside_function()",
            "  with ops.name_scope(name, \"create_file_writer\") as scope, ops.device(\"cpu:0\"):",
            "    # Run init inside an init_scope() to hoist it out of tf.functions.",
            "    with ops.init_scope():",
            "      if context.executing_eagerly():",
            "        _check_create_file_writer_args(",
            "            inside_function,",
            "            logdir=logdir,",
            "            max_queue=max_queue,",
            "            flush_millis=flush_millis,",
            "            filename_suffix=filename_suffix)",
            "      logdir = ops.convert_to_tensor(logdir, dtype=dtypes.string)",
            "      if max_queue is None:",
            "        max_queue = constant_op.constant(10)",
            "      if flush_millis is None:",
            "        flush_millis = constant_op.constant(2 * 60 * 1000)",
            "      if filename_suffix is None:",
            "        filename_suffix = constant_op.constant(\".v2\")",
            "",
            "      def create_fn():",
            "        # Use unique shared_name to prevent resource sharing in eager mode, but",
            "        # otherwise use a fixed shared_name to allow SavedModel TF 1.x loading.",
            "        if context.executing_eagerly():",
            "          shared_name = context.anonymous_name()",
            "        else:",
            "          shared_name = ops.name_from_scope_name(scope)  # pylint: disable=protected-access",
            "        return gen_summary_ops.summary_writer(",
            "            shared_name=shared_name, name=name)",
            "",
            "      init_op_fn = functools.partial(",
            "          gen_summary_ops.create_summary_file_writer,",
            "          logdir=logdir,",
            "          max_queue=max_queue,",
            "          flush_millis=flush_millis,",
            "          filename_suffix=filename_suffix)",
            "      if experimental_trackable:",
            "        return _TrackableResourceSummaryWriter(",
            "            create_fn=create_fn, init_op_fn=init_op_fn)",
            "      else:",
            "        return _ResourceSummaryWriter(",
            "            create_fn=create_fn, init_op_fn=init_op_fn)",
            "",
            "",
            "def create_file_writer(logdir,",
            "                       max_queue=None,",
            "                       flush_millis=None,",
            "                       filename_suffix=None,",
            "                       name=None):",
            "  \"\"\"Creates a summary file writer in the current context under the given name.",
            "",
            "  Args:",
            "    logdir: a string, or None. If a string, creates a summary file writer",
            "     which writes to the directory named by the string. If None, returns",
            "     a mock object which acts like a summary writer but does nothing,",
            "     useful to use as a context manager.",
            "    max_queue: the largest number of summaries to keep in a queue; will",
            "     flush once the queue gets bigger than this. Defaults to 10.",
            "    flush_millis: the largest interval between flushes. Defaults to 120,000.",
            "    filename_suffix: optional suffix for the event file name. Defaults to `.v2`.",
            "    name: Shared name for this SummaryWriter resource stored to default",
            "      Graph. Defaults to the provided logdir prefixed with `logdir:`. Note: if a",
            "      summary writer resource with this shared name already exists, the returned",
            "      SummaryWriter wraps that resource and the other arguments have no effect.",
            "",
            "  Returns:",
            "    Either a summary writer or an empty object which can be used as a",
            "    summary writer.",
            "  \"\"\"",
            "  if logdir is None:",
            "    return _NoopSummaryWriter()",
            "  logdir = str(logdir)",
            "  with ops.device(\"cpu:0\"):",
            "    if max_queue is None:",
            "      max_queue = constant_op.constant(10)",
            "    if flush_millis is None:",
            "      flush_millis = constant_op.constant(2 * 60 * 1000)",
            "    if filename_suffix is None:",
            "      filename_suffix = constant_op.constant(\".v2\")",
            "    if name is None:",
            "      name = \"logdir:\" + logdir",
            "    resource = gen_summary_ops.summary_writer(shared_name=name)",
            "    return _LegacyResourceSummaryWriter(",
            "        resource=resource,",
            "        init_op_fn=functools.partial(",
            "            gen_summary_ops.create_summary_file_writer,",
            "            logdir=logdir,",
            "            max_queue=max_queue,",
            "            flush_millis=flush_millis,",
            "            filename_suffix=filename_suffix))",
            "",
            "",
            "@tf_export(\"summary.create_noop_writer\", v1=[])",
            "def create_noop_writer():",
            "  \"\"\"Returns a summary writer that does nothing.",
            "",
            "  This is useful as a placeholder in code that expects a context manager.",
            "  \"\"\"",
            "  return _NoopSummaryWriter()",
            "",
            "",
            "def _cleanse_string(name, pattern, value):",
            "  if isinstance(value, str) and pattern.search(value) is None:",
            "    raise ValueError(f\"{name} ({value}) must match {pattern.pattern}\")",
            "  return ops.convert_to_tensor(value, dtypes.string)",
            "",
            "",
            "def _nothing():",
            "  \"\"\"Convenient else branch for when summaries do not record.\"\"\"",
            "  return constant_op.constant(False)",
            "",
            "",
            "@tf_export(v1=[\"summary.all_v2_summary_ops\"])",
            "def all_v2_summary_ops():",
            "  \"\"\"Returns all V2-style summary ops defined in the current default graph.",
            "",
            "  This includes ops from TF 2.0 tf.summary and TF 1.x tf.contrib.summary (except",
            "  for `tf.contrib.summary.graph` and `tf.contrib.summary.import_event`), but",
            "  does *not* include TF 1.x tf.summary ops.",
            "",
            "  Returns:",
            "    List of summary ops, or None if called under eager execution.",
            "  \"\"\"",
            "  if context.executing_eagerly():",
            "    return None",
            "  return ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access",
            "",
            "",
            "def summary_writer_initializer_op():",
            "  \"\"\"Graph-mode only. Returns the list of ops to create all summary writers.",
            "",
            "  Returns:",
            "    The initializer ops.",
            "",
            "  Raises:",
            "    RuntimeError: If in Eager mode.",
            "  \"\"\"",
            "  if context.executing_eagerly():",
            "    raise RuntimeError(",
            "        \"tf.contrib.summary.summary_writer_initializer_op is only \"",
            "        \"supported in graph mode.\")",
            "  return ops.get_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME)",
            "",
            "",
            "_INVALID_SCOPE_CHARACTERS = re.compile(r\"[^-_/.A-Za-z0-9]\")",
            "",
            "",
            "@tf_export(\"summary.experimental.summary_scope\", v1=[])",
            "@tf_contextlib.contextmanager",
            "def summary_scope(name, default_name=\"summary\", values=None):",
            "  \"\"\"Experimental context manager for use when defining a custom summary op.",
            "",
            "  This behaves similarly to `tf.name_scope`, except that it returns a generated",
            "  summary tag in addition to the scope name. The tag is structurally similar to",
            "  the scope name - derived from the user-provided name, prefixed with enclosing",
            "  name scopes if any - but we relax the constraint that it be uniquified, as",
            "  well as the character set limitation (so the user-provided name can contain",
            "  characters not legal for scope names; in the scope name these are removed).",
            "",
            "  This makes the summary tag more predictable and consistent for the user.",
            "",
            "  For example, to define a new summary op called `my_op`:",
            "",
            "  ```python",
            "  def my_op(name, my_value, step):",
            "    with tf.summary.summary_scope(name, \"MyOp\", [my_value]) as (tag, scope):",
            "      my_value = tf.convert_to_tensor(my_value)",
            "      return tf.summary.write(tag, my_value, step=step)",
            "  ```",
            "",
            "  Args:",
            "    name: string name for the summary.",
            "    default_name: Optional; if provided, used as default name of the summary.",
            "    values: Optional; passed as `values` parameter to name_scope.",
            "",
            "  Yields:",
            "    A tuple `(tag, scope)` as described above.",
            "  \"\"\"",
            "  name = name or default_name",
            "  current_scope = ops.get_name_scope()",
            "  tag = current_scope + \"/\" + name if current_scope else name",
            "  # Strip illegal characters from the scope name, and if that leaves nothing,",
            "  # use None instead so we pick up the default name.",
            "  name = _INVALID_SCOPE_CHARACTERS.sub(\"\", name) or None",
            "  with ops.name_scope(name, default_name, values, skip_on_eager=False) as scope:",
            "    yield tag, scope",
            "",
            "",
            "@tf_export(\"summary.write\", v1=[])",
            "def write(tag, tensor, step=None, metadata=None, name=None):",
            "  \"\"\"Writes a generic summary to the default SummaryWriter if one exists.",
            "",
            "  This exists primarily to support the definition of type-specific summary ops",
            "  like scalar() and image(), and is not intended for direct use unless defining",
            "  a new type-specific summary op.",
            "",
            "  Args:",
            "    tag: string tag used to identify the summary (e.g. in TensorBoard), usually",
            "      generated with `tf.summary.summary_scope`",
            "    tensor: the Tensor holding the summary data to write or a callable that",
            "      returns this Tensor. If a callable is passed, it will only be called when",
            "      a default SummaryWriter exists and the recording condition specified by",
            "      `record_if()` is met.",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "    metadata: Optional SummaryMetadata, as a proto or serialized bytes",
            "    name: Optional string name for this op.",
            "",
            "  Returns:",
            "    True on success, or false if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  with ops.name_scope(name, \"write_summary\") as scope:",
            "    if _summary_state.writer is None:",
            "      return constant_op.constant(False)",
            "    if step is None:",
            "      step = get_step()",
            "    if metadata is None:",
            "      serialized_metadata = b\"\"",
            "    elif hasattr(metadata, \"SerializeToString\"):",
            "      serialized_metadata = metadata.SerializeToString()",
            "    else:",
            "      serialized_metadata = metadata",
            "",
            "    def record():",
            "      \"\"\"Record the actual summary and return True.\"\"\"",
            "      if step is None:",
            "        raise ValueError(\"No step set. Please specify one either through the \"",
            "                         \"`step` argument or through \"",
            "                         \"tf.summary.experimental.set_step()\")",
            "",
            "      # Note the identity to move the tensor to the CPU.",
            "      with ops.device(\"cpu:0\"):",
            "        summary_tensor = tensor() if callable(tensor) else array_ops.identity(",
            "            tensor)",
            "        write_summary_op = gen_summary_ops.write_summary(",
            "            _summary_state.writer._resource,  # pylint: disable=protected-access",
            "            step,",
            "            summary_tensor,",
            "            tag,",
            "            serialized_metadata,",
            "            name=scope)",
            "        with ops.control_dependencies([write_summary_op]):",
            "          return constant_op.constant(True)",
            "",
            "    op = smart_cond.smart_cond(",
            "        should_record_summaries(), record, _nothing, name=\"summary_cond\")",
            "    if not context.executing_eagerly():",
            "      ops.add_to_collection(ops.GraphKeys._SUMMARY_COLLECTION, op)  # pylint: disable=protected-access",
            "    return op",
            "",
            "",
            "@tf_export(\"summary.experimental.write_raw_pb\", v1=[])",
            "def write_raw_pb(tensor, step=None, name=None):",
            "  \"\"\"Writes a summary using raw `tf.compat.v1.Summary` protocol buffers.",
            "",
            "  Experimental: this exists to support the usage of V1-style manual summary",
            "  writing (via the construction of a `tf.compat.v1.Summary` protocol buffer)",
            "  with the V2 summary writing API.",
            "",
            "  Args:",
            "    tensor: the string Tensor holding one or more serialized `Summary` protobufs",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "    name: Optional string name for this op.",
            "",
            "  Returns:",
            "    True on success, or false if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  with ops.name_scope(name, \"write_raw_pb\") as scope:",
            "    if _summary_state.writer is None:",
            "      return constant_op.constant(False)",
            "    if step is None:",
            "      step = get_step()",
            "      if step is None:",
            "        raise ValueError(\"No step set. Please specify one either through the \"",
            "                         \"`step` argument or through \"",
            "                         \"tf.summary.experimental.set_step()\")",
            "",
            "    def record():",
            "      \"\"\"Record the actual summary and return True.\"\"\"",
            "      # Note the identity to move the tensor to the CPU.",
            "      with ops.device(\"cpu:0\"):",
            "        raw_summary_op = gen_summary_ops.write_raw_proto_summary(",
            "            _summary_state.writer._resource,  # pylint: disable=protected-access",
            "            step,",
            "            array_ops.identity(tensor),",
            "            name=scope)",
            "        with ops.control_dependencies([raw_summary_op]):",
            "          return constant_op.constant(True)",
            "",
            "    with ops.device(\"cpu:0\"):",
            "      op = smart_cond.smart_cond(",
            "          should_record_summaries(), record, _nothing, name=\"summary_cond\")",
            "      if not context.executing_eagerly():",
            "        ops.add_to_collection(ops.GraphKeys._SUMMARY_COLLECTION, op)  # pylint: disable=protected-access",
            "      return op",
            "",
            "",
            "def summary_writer_function(name, tensor, function, family=None):",
            "  \"\"\"Helper function to write summaries.",
            "",
            "  Args:",
            "    name: name of the summary",
            "    tensor: main tensor to form the summary",
            "    function: function taking a tag and a scope which writes the summary",
            "    family: optional, the summary's family",
            "",
            "  Returns:",
            "    The result of writing the summary.",
            "  \"\"\"",
            "  name_scope = ops.get_name_scope()",
            "  if name_scope:",
            "    # Add a slash to allow reentering the name scope.",
            "    name_scope += \"/\"",
            "  def record():",
            "    with ops.name_scope(name_scope), summary_op_util.summary_scope(",
            "        name, family, values=[tensor]) as (tag, scope):",
            "      with ops.control_dependencies([function(tag, scope)]):",
            "        return constant_op.constant(True)",
            "",
            "  if _summary_state.writer is None:",
            "    return control_flow_ops.no_op()",
            "  with ops.device(\"cpu:0\"):",
            "    op = smart_cond.smart_cond(",
            "        _legacy_contrib_should_record_summaries(), record, _nothing, name=\"\")",
            "    if not context.executing_eagerly():",
            "      ops.add_to_collection(ops.GraphKeys._SUMMARY_COLLECTION, op)  # pylint: disable=protected-access",
            "  return op",
            "",
            "",
            "def generic(name, tensor, metadata=None, family=None, step=None):",
            "  \"\"\"Writes a tensor summary if possible.\"\"\"",
            "",
            "  def function(tag, scope):",
            "    if metadata is None:",
            "      serialized_metadata = constant_op.constant(\"\")",
            "    elif hasattr(metadata, \"SerializeToString\"):",
            "      serialized_metadata = constant_op.constant(metadata.SerializeToString())",
            "    else:",
            "      serialized_metadata = metadata",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        array_ops.identity(tensor),",
            "        tag,",
            "        serialized_metadata,",
            "        name=scope)",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def scalar(name, tensor, family=None, step=None):",
            "  \"\"\"Writes a scalar summary if possible.",
            "",
            "  Unlike `tf.contrib.summary.generic` this op may change the dtype",
            "  depending on the writer, for both practical and efficiency concerns.",
            "",
            "  Args:",
            "    name: An arbitrary name for this summary.",
            "    tensor: A `tf.Tensor` Must be one of the following types:",
            "      `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`,",
            "      `int8`, `uint16`, `half`, `uint32`, `uint64`.",
            "    family: Optional, the summary's family.",
            "    step: The `int64` monotonic step variable, which defaults",
            "      to `tf.compat.v1.train.get_global_step`.",
            "",
            "  Returns:",
            "    The created `tf.Operation` or a `tf.no_op` if summary writing has",
            "    not been enabled for this context.",
            "  \"\"\"",
            "",
            "  def function(tag, scope):",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_scalar_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        tag,",
            "        array_ops.identity(tensor),",
            "        name=scope)",
            "",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def histogram(name, tensor, family=None, step=None):",
            "  \"\"\"Writes a histogram summary if possible.\"\"\"",
            "",
            "  def function(tag, scope):",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_histogram_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        tag,",
            "        array_ops.identity(tensor),",
            "        name=scope)",
            "",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def image(name, tensor, bad_color=None, max_images=3, family=None, step=None):",
            "  \"\"\"Writes an image summary if possible.\"\"\"",
            "",
            "  def function(tag, scope):",
            "    bad_color_ = (constant_op.constant([255, 0, 0, 255], dtype=dtypes.uint8)",
            "                  if bad_color is None else bad_color)",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_image_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        tag,",
            "        array_ops.identity(tensor),",
            "        bad_color_,",
            "        max_images,",
            "        name=scope)",
            "",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def audio(name, tensor, sample_rate, max_outputs, family=None, step=None):",
            "  \"\"\"Writes an audio summary if possible.\"\"\"",
            "",
            "  def function(tag, scope):",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_audio_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        tag,",
            "        array_ops.identity(tensor),",
            "        sample_rate=sample_rate,",
            "        max_outputs=max_outputs,",
            "        name=scope)",
            "",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def graph_v1(param, step=None, name=None):",
            "  \"\"\"Writes a TensorFlow graph to the summary interface.",
            "",
            "  The graph summary is, strictly speaking, not a summary. Conditions",
            "  like `tf.summary.should_record_summaries` do not apply. Only",
            "  a single graph can be associated with a particular run. If multiple",
            "  graphs are written, then only the last one will be considered by",
            "  TensorBoard.",
            "",
            "  When not using eager execution mode, the user should consider passing",
            "  the `graph` parameter to `tf.compat.v1.summary.initialize` instead of",
            "  calling this function. Otherwise special care needs to be taken when",
            "  using the graph to record the graph.",
            "",
            "  Args:",
            "    param: A `tf.Tensor` containing a serialized graph proto. When",
            "      eager execution is enabled, this function will automatically",
            "      coerce `tf.Graph`, `tf.compat.v1.GraphDef`, and string types.",
            "    step: The global step variable. This doesn't have useful semantics",
            "      for graph summaries, but is used anyway, due to the structure of",
            "      event log files. This defaults to the global step.",
            "    name: A name for the operation (optional).",
            "",
            "  Returns:",
            "    The created `tf.Operation` or a `tf.no_op` if summary writing has",
            "    not been enabled for this context.",
            "",
            "  Raises:",
            "    TypeError: If `param` isn't already a `tf.Tensor` in graph mode.",
            "  \"\"\"",
            "  if not context.executing_eagerly() and not isinstance(param, ops.Tensor):",
            "    raise TypeError(\"graph() needs a argument `param` to be tf.Tensor \"",
            "                    \"(e.g. tf.placeholder) in graph mode, but received \"",
            "                    f\"param={param} of type {type(param).__name__}.\")",
            "  writer = _summary_state.writer",
            "  if writer is None:",
            "    return control_flow_ops.no_op()",
            "  with ops.device(\"cpu:0\"):",
            "    if isinstance(param, (ops.Graph, graph_pb2.GraphDef)):",
            "      tensor = ops.convert_to_tensor(_serialize_graph(param), dtypes.string)",
            "    else:",
            "      tensor = array_ops.identity(param)",
            "    return gen_summary_ops.write_graph_summary(",
            "        writer._resource, _choose_step(step), tensor, name=name)  # pylint: disable=protected-access",
            "",
            "",
            "@tf_export(\"summary.graph\", v1=[])",
            "def graph(graph_data):",
            "  \"\"\"Writes a TensorFlow graph summary.",
            "",
            "  Write an instance of `tf.Graph` or `tf.compat.v1.GraphDef` as summary only",
            "  in an eager mode. Please prefer to use the trace APIs (`tf.summary.trace_on`,",
            "  `tf.summary.trace_off`, and `tf.summary.trace_export`) when using",
            "  `tf.function` which can automatically collect and record graphs from",
            "  executions.",
            "",
            "  Usage Example:",
            "  ```py",
            "  writer = tf.summary.create_file_writer(\"/tmp/mylogs\")",
            "",
            "  @tf.function",
            "  def f():",
            "    x = constant_op.constant(2)",
            "    y = constant_op.constant(3)",
            "    return x**y",
            "",
            "  with writer.as_default():",
            "    tf.summary.graph(f.get_concrete_function().graph)",
            "",
            "  # Another example: in a very rare use case, when you are dealing with a TF v1",
            "  # graph.",
            "  graph = tf.Graph()",
            "  with graph.as_default():",
            "    c = tf.constant(30.0)",
            "  with writer.as_default():",
            "    tf.summary.graph(graph)",
            "  ```",
            "",
            "  Args:",
            "    graph_data: The TensorFlow graph to write, as a `tf.Graph` or a",
            "      `tf.compat.v1.GraphDef`.",
            "",
            "  Returns:",
            "    True on success, or False if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: `graph` summary API is invoked in a graph mode.",
            "  \"\"\"",
            "  if not context.executing_eagerly():",
            "    raise ValueError(\"graph() cannot be invoked inside a graph context.\")",
            "  writer = _summary_state.writer",
            "  if writer is None:",
            "    return constant_op.constant(False)",
            "  with ops.device(\"cpu:0\"):",
            "    if not should_record_summaries():",
            "      return constant_op.constant(False)",
            "",
            "    if isinstance(graph_data, (ops.Graph, graph_pb2.GraphDef)):",
            "      tensor = ops.convert_to_tensor(",
            "          _serialize_graph(graph_data), dtypes.string)",
            "    else:",
            "      raise ValueError(\"Argument 'graph_data' is not tf.Graph or \"",
            "                       \"tf.compat.v1.GraphDef. Received graph_data=\"",
            "                       f\"{graph_data} of type {type(graph_data).__name__}.\")",
            "",
            "    gen_summary_ops.write_graph_summary(",
            "        writer._resource,  # pylint: disable=protected-access",
            "        # Graph does not have step. Set to 0.",
            "        0,",
            "        tensor,",
            "    )",
            "    return constant_op.constant(True)",
            "",
            "",
            "def import_event(tensor, name=None):",
            "  \"\"\"Writes a `tf.compat.v1.Event` binary proto.",
            "",
            "  This can be used to import existing event logs into a new summary writer sink.",
            "  Please note that this is lower level than the other summary functions and",
            "  will ignore the `tf.summary.should_record_summaries` setting.",
            "",
            "  Args:",
            "    tensor: A `tf.Tensor` of type `string` containing a serialized",
            "      `tf.compat.v1.Event` proto.",
            "    name: A name for the operation (optional).",
            "",
            "  Returns:",
            "    The created `tf.Operation`.",
            "  \"\"\"",
            "  return gen_summary_ops.import_event(",
            "      _summary_state.writer._resource, tensor, name=name)  # pylint: disable=protected-access",
            "",
            "",
            "@tf_export(\"summary.flush\", v1=[])",
            "def flush(writer=None, name=None):",
            "  \"\"\"Forces summary writer to send any buffered data to storage.",
            "",
            "  This operation blocks until that finishes.",
            "",
            "  Args:",
            "    writer: The `tf.summary.SummaryWriter` to flush. If None, the current",
            "      default writer will be used instead; if there is no current writer, this",
            "      returns `tf.no_op`.",
            "    name: Ignored legacy argument for a name for the operation.",
            "",
            "  Returns:",
            "    The created `tf.Operation`.",
            "  \"\"\"",
            "  if writer is None:",
            "    writer = _summary_state.writer",
            "    if writer is None:",
            "      return control_flow_ops.no_op()",
            "  if isinstance(writer, SummaryWriter):",
            "    return writer.flush()",
            "  else:",
            "    # Legacy fallback in case we were passed a raw resource tensor.",
            "    with ops.device(\"cpu:0\"):",
            "      return gen_summary_ops.flush_summary_writer(writer, name=name)",
            "",
            "",
            "def eval_dir(model_dir, name=None):",
            "  \"\"\"Construct a logdir for an eval summary writer.\"\"\"",
            "  return os.path.join(model_dir, \"eval\" if not name else \"eval_\" + name)",
            "",
            "",
            "@deprecation.deprecated(date=None,",
            "                        instructions=\"Renamed to create_file_writer().\")",
            "def create_summary_file_writer(*args, **kwargs):",
            "  \"\"\"Please use `tf.contrib.summary.create_file_writer`.\"\"\"",
            "  logging.warning(\"Deprecation Warning: create_summary_file_writer was renamed \"",
            "                  \"to create_file_writer\")",
            "  return create_file_writer(*args, **kwargs)",
            "",
            "",
            "def _serialize_graph(arbitrary_graph):",
            "  if isinstance(arbitrary_graph, ops.Graph):",
            "    return arbitrary_graph.as_graph_def(add_shapes=True).SerializeToString()",
            "  else:",
            "    return arbitrary_graph.SerializeToString()",
            "",
            "",
            "def _choose_step(step):",
            "  if step is None:",
            "    return training_util.get_or_create_global_step()",
            "  if not isinstance(step, ops.Tensor):",
            "    return ops.convert_to_tensor(step, dtypes.int64)",
            "  return step",
            "",
            "",
            "def _check_create_file_writer_args(inside_function, **kwargs):",
            "  \"\"\"Helper to check the validity of arguments to a create_file_writer() call.",
            "",
            "  Args:",
            "    inside_function: whether the create_file_writer() call is in a tf.function",
            "    **kwargs: the arguments to check, as kwargs to give them names.",
            "",
            "  Raises:",
            "    ValueError: if the arguments are graph tensors.",
            "  \"\"\"",
            "  for arg_name, arg in kwargs.items():",
            "    if not isinstance(arg, ops.EagerTensor) and tensor_util.is_tf_type(arg):",
            "      if inside_function:",
            "        raise ValueError(",
            "            f\"Invalid graph Tensor argument '{arg_name}={arg}' to \"",
            "            \"create_file_writer() inside an @tf.function. The create call will \"",
            "            \"be lifted into the outer eager execution context, so it cannot \"",
            "            \"consume graph tensors defined inside the function body.\")",
            "      else:",
            "        raise ValueError(",
            "            f\"Invalid graph Tensor argument '{arg_name}={arg}' to eagerly \"",
            "            \"executed create_file_writer().\")",
            "",
            "",
            "def run_metadata(name, data, step=None):",
            "  \"\"\"Writes entire RunMetadata summary.",
            "",
            "  A RunMetadata can contain DeviceStats, partition graphs, and function graphs.",
            "  Please refer to the proto for definition of each field.",
            "",
            "  Args:",
            "    name: A name for this summary. The summary tag used for TensorBoard will be",
            "      this name prefixed by any active name scopes.",
            "    data: A RunMetadata proto to write.",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "",
            "  Returns:",
            "    True on success, or false if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  summary_metadata = summary_pb2.SummaryMetadata()",
            "  # Hard coding a plugin name. Please refer to go/tb-plugin-name-hardcode for",
            "  # the rationale.",
            "  summary_metadata.plugin_data.plugin_name = \"graph_run_metadata\"",
            "  # version number = 1",
            "  summary_metadata.plugin_data.content = b\"1\"",
            "",
            "  with summary_scope(name,",
            "                     \"graph_run_metadata_summary\",",
            "                     [data, step]) as (tag, _):",
            "    with ops.device(\"cpu:0\"):",
            "      tensor = constant_op.constant(data.SerializeToString(),",
            "                                    dtype=dtypes.string)",
            "    return write(",
            "        tag=tag,",
            "        tensor=tensor,",
            "        step=step,",
            "        metadata=summary_metadata)",
            "",
            "",
            "def run_metadata_graphs(name, data, step=None):",
            "  \"\"\"Writes graphs from a RunMetadata summary.",
            "",
            "  Args:",
            "    name: A name for this summary. The summary tag used for TensorBoard will be",
            "      this name prefixed by any active name scopes.",
            "    data: A RunMetadata proto to write.",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "",
            "  Returns:",
            "    True on success, or false if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  summary_metadata = summary_pb2.SummaryMetadata()",
            "  # Hard coding a plugin name. Please refer to go/tb-plugin-name-hardcode for",
            "  # the rationale.",
            "  summary_metadata.plugin_data.plugin_name = \"graph_run_metadata_graph\"",
            "  # version number = 1",
            "  summary_metadata.plugin_data.content = b\"1\"",
            "",
            "  data = config_pb2.RunMetadata(",
            "      function_graphs=data.function_graphs,",
            "      partition_graphs=data.partition_graphs)",
            "",
            "  with summary_scope(name,",
            "                     \"graph_run_metadata_graph_summary\",",
            "                     [data, step]) as (tag, _):",
            "    with ops.device(\"cpu:0\"):",
            "      tensor = constant_op.constant(data.SerializeToString(),",
            "                                    dtype=dtypes.string)",
            "    return write(",
            "        tag=tag,",
            "        tensor=tensor,",
            "        step=step,",
            "        metadata=summary_metadata)",
            "",
            "",
            "_TraceContext = collections.namedtuple(\"TraceContext\", (\"graph\", \"profiler\"))",
            "_current_trace_context_lock = threading.Lock()",
            "_current_trace_context = None",
            "",
            "",
            "@tf_export(\"summary.trace_on\", v1=[])",
            "def trace_on(graph=True, profiler=False):  # pylint: disable=redefined-outer-name",
            "  \"\"\"Starts a trace to record computation graphs and profiling information.",
            "",
            "  Must be invoked in eager mode.",
            "",
            "  When enabled, TensorFlow runtime will collect information that can later be",
            "  exported and consumed by TensorBoard. The trace is activated across the entire",
            "  TensorFlow runtime and affects all threads of execution.",
            "",
            "  To stop the trace and export the collected information, use",
            "  `tf.summary.trace_export`. To stop the trace without exporting, use",
            "  `tf.summary.trace_off`.",
            "",
            "  Args:",
            "    graph: If True, enables collection of executed graphs. It includes ones from",
            "        tf.function invocation and ones from the legacy graph mode. The default",
            "        is True.",
            "    profiler: If True, enables the advanced profiler. Enabling profiler",
            "        implicitly enables the graph collection. The profiler may incur a high",
            "        memory overhead. The default is False.",
            "",
            "  \"\"\"",
            "  if ops.inside_function():",
            "    logging.warn(\"Cannot enable trace inside a tf.function.\")",
            "    return",
            "  if not context.executing_eagerly():",
            "    logging.warn(\"Must enable trace in eager mode.\")",
            "    return",
            "",
            "  global _current_trace_context",
            "  with _current_trace_context_lock:",
            "    if _current_trace_context:",
            "      logging.warn(\"Trace already enabled\")",
            "      return",
            "",
            "    if graph and not profiler:",
            "      context.context().enable_graph_collection()",
            "    if profiler:",
            "      context.context().enable_run_metadata()",
            "      _profiler.start()",
            "",
            "    _current_trace_context = _TraceContext(graph=graph, profiler=profiler)",
            "",
            "",
            "@tf_export(\"summary.trace_export\", v1=[])",
            "def trace_export(name, step=None, profiler_outdir=None):",
            "  \"\"\"Stops and exports the active trace as a Summary and/or profile file.",
            "",
            "  Stops the trace and exports all metadata collected during the trace to the",
            "  default SummaryWriter, if one has been set.",
            "",
            "  Args:",
            "    name: A name for the summary to be written.",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "    profiler_outdir: Output directory for profiler. It is required when profiler",
            "      is enabled when trace was started. Otherwise, it is ignored.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  # TODO(stephanlee): See if we can remove profiler_outdir and infer it from",
            "  # the SummaryWriter's logdir.",
            "  global _current_trace_context",
            "",
            "  if ops.inside_function():",
            "    logging.warn(\"Cannot export trace inside a tf.function.\")",
            "    return",
            "  if not context.executing_eagerly():",
            "    logging.warn(\"Can only export trace while executing eagerly.\")",
            "    return",
            "",
            "  with _current_trace_context_lock:",
            "    if _current_trace_context is None:",
            "      raise ValueError(\"Must enable trace before export through \"",
            "                       \"tf.summary.trace_on.\")",
            "    graph, profiler = _current_trace_context  # pylint: disable=redefined-outer-name",
            "    if profiler and profiler_outdir is None:",
            "      raise ValueError(\"Argument `profiler_outdir` is not specified.\")",
            "",
            "  run_meta = context.context().export_run_metadata()",
            "",
            "  if graph and not profiler:",
            "    run_metadata_graphs(name, run_meta, step)",
            "  else:",
            "    run_metadata(name, run_meta, step)",
            "",
            "  if profiler:",
            "    _profiler.save(profiler_outdir, _profiler.stop())",
            "",
            "  trace_off()",
            "",
            "",
            "@tf_export(\"summary.trace_off\", v1=[])",
            "def trace_off():",
            "  \"\"\"Stops the current trace and discards any collected information.\"\"\"",
            "  global _current_trace_context",
            "  with _current_trace_context_lock:",
            "    if _current_trace_context is None:",
            "      return  # tracing already off",
            "    graph, profiler = _current_trace_context  # pylint: disable=redefined-outer-name, unpacking-non-sequence",
            "    _current_trace_context = None",
            "",
            "  if graph:",
            "    # Disabling run_metadata disables graph collection as well.",
            "    context.context().disable_run_metadata()",
            "",
            "  if profiler:",
            "    try:",
            "      _profiler.stop()",
            "    except _profiler.ProfilerNotRunningError:",
            "      pass"
        ],
        "afterPatchFile": [
            "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "",
            "\"\"\"Operations to emit summaries.\"\"\"",
            "",
            "import abc",
            "import collections",
            "import functools",
            "import os",
            "import re",
            "import threading",
            "",
            "from tensorflow.core.framework import graph_pb2",
            "from tensorflow.core.framework import summary_pb2",
            "from tensorflow.core.protobuf import config_pb2",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.eager import profiler as _profiler",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.framework import smart_cond",
            "from tensorflow.python.framework import tensor_util",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import control_flow_ops",
            "from tensorflow.python.ops import gen_resource_variable_ops",
            "from tensorflow.python.ops import gen_summary_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import resource_variable_ops",
            "from tensorflow.python.ops import summary_op_util",
            "from tensorflow.python.platform import tf_logging as logging",
            "from tensorflow.python.training import training_util",
            "from tensorflow.python.training.tracking import tracking",
            "from tensorflow.python.util import deprecation",
            "from tensorflow.python.util import tf_contextlib",
            "from tensorflow.python.util.tf_export import tf_export",
            "",
            "# Name for graph collection of summary writer init ops, which is only exposed",
            "# as a legacy API for tf.contrib.summary in TF 1.x.",
            "_SUMMARY_WRITER_INIT_COLLECTION_NAME = \"_SUMMARY_WRITER_V2\"",
            "",
            "",
            "class _SummaryState(threading.local):",
            "",
            "  def __init__(self):",
            "    super(_SummaryState, self).__init__()",
            "    self.is_recording = None",
            "    # TODO(slebedev): why a separate flag for DS and is it on by default?",
            "    self.is_recording_distribution_strategy = True",
            "    self.writer = None",
            "    self.step = None",
            "",
            "",
            "_summary_state = _SummaryState()",
            "",
            "",
            "class _SummaryContextManager:",
            "  \"\"\"Context manager to implement SummaryWriter.as_default().\"\"\"",
            "  # Note: this is a class so that it's possible to implement `set_as_default()`",
            "  # simply via `as_default().__enter__()`. We can't do that with @contextmanager",
            "  # because the `finally` block will be executed when the generator is GCed.",
            "",
            "  def __init__(self, writer, step=None):",
            "    self._writer = writer",
            "    self._step = step",
            "    self._old_writer = None",
            "    self._old_step = None",
            "",
            "  def __enter__(self):",
            "    self._old_writer = _summary_state.writer",
            "    _summary_state.writer = self._writer",
            "    if self._step is not None:",
            "      self._old_step = _summary_state.step",
            "      _summary_state.step = self._step",
            "    return self._writer",
            "",
            "  def __exit__(self, *exc):",
            "    # Flushes the summary writer in eager mode or in graph functions, but",
            "    # not in legacy graph mode (you're on your own there).",
            "    _summary_state.writer.flush()",
            "    _summary_state.writer = self._old_writer",
            "    if self._step is not None:",
            "      _summary_state.step = self._old_step",
            "    return False",
            "",
            "",
            "def _should_record_summaries_internal(default_state):",
            "  \"\"\"Returns boolean Tensor if summaries should/shouldn't be recorded.",
            "",
            "  Now the summary condition is decided by logical \"and\" of below conditions:",
            "  First, summary writer must be set. Given this constraint is met,",
            "  ctx.summary_recording and ctx.summary_recording_distribution_strategy.",
            "  The former one is usually set by user, and the latter one is controlled",
            "  by DistributionStrategy (tf.distribute.ReplicaContext).",
            "",
            "  Args:",
            "    default_state: can be True or False. The default summary behavior when",
            "    summary writer is set and the user does not specify",
            "    ctx.summary_recording and ctx.summary_recording_distribution_strategy",
            "    is True.",
            "  \"\"\"",
            "  if _summary_state.writer is None:",
            "    return constant_op.constant(False)",
            "",
            "  if not callable(_summary_state.is_recording):",
            "    static_cond = tensor_util.constant_value(_summary_state.is_recording)",
            "    if static_cond is not None and not static_cond:",
            "      return constant_op.constant(False)",
            "",
            "  resolve = lambda x: x() if callable(x) else x",
            "  cond_distributed = resolve(_summary_state.is_recording_distribution_strategy)",
            "  cond = resolve(_summary_state.is_recording)",
            "  if cond is None:",
            "    cond = default_state",
            "  return math_ops.logical_and(cond_distributed, cond)",
            "",
            "",
            "@tf_export(\"summary.should_record_summaries\", v1=[])",
            "def should_record_summaries():",
            "  \"\"\"Returns boolean Tensor which is True if summaries will be recorded.",
            "",
            "  If no default summary writer is currently registered, this always returns",
            "  False. Otherwise, this reflects the recording condition has been set via",
            "  `tf.summary.record_if()` (except that it may return False for some replicas",
            "  when using `tf.distribute.Strategy`). If no recording condition is active,",
            "  it defaults to True.",
            "  \"\"\"",
            "  return _should_record_summaries_internal(default_state=True)",
            "",
            "",
            "# Legacy symbol used by tf.contrib.summary.should_record_summaries.",
            "def _legacy_contrib_should_record_summaries():",
            "  \"\"\"Returns boolean Tensor which is true if summaries should be recorded.\"\"\"",
            "  return _should_record_summaries_internal(default_state=False)",
            "",
            "",
            "@tf_export(\"summary.record_if\", v1=[])",
            "@tf_contextlib.contextmanager",
            "def record_if(condition):",
            "  \"\"\"Sets summary recording on or off per the provided boolean value.",
            "",
            "  The provided value can be a python boolean, a scalar boolean Tensor, or",
            "  or a callable providing such a value; if a callable is passed it will be",
            "  invoked on-demand to determine whether summary writing will occur.  Note that",
            "  when calling record_if() in an eager mode context, if you intend to provide a",
            "  varying condition like `step % 100 == 0`, you must wrap this in a",
            "  callable to avoid immediate eager evaluation of the condition.  In particular,",
            "  using a callable is the only way to have your condition evaluated as part of",
            "  the traced body of an @tf.function that is invoked from within the",
            "  `record_if()` context.",
            "",
            "  Args:",
            "    condition: can be True, False, a bool Tensor, or a callable providing such.",
            "",
            "  Yields:",
            "    Returns a context manager that sets this value on enter and restores the",
            "    previous value on exit.",
            "  \"\"\"",
            "  old = _summary_state.is_recording",
            "  try:",
            "    _summary_state.is_recording = condition",
            "    yield",
            "  finally:",
            "    _summary_state.is_recording = old",
            "",
            "",
            "def has_default_writer():",
            "  \"\"\"Returns a boolean indicating whether a default summary writer exists.\"\"\"",
            "  return _summary_state.writer is not None",
            "",
            "",
            "# TODO(apassos) consider how to handle local step here.",
            "def record_summaries_every_n_global_steps(n, global_step=None):",
            "  \"\"\"Sets the should_record_summaries Tensor to true if global_step % n == 0.\"\"\"",
            "  if global_step is None:",
            "    global_step = training_util.get_or_create_global_step()",
            "  with ops.device(\"cpu:0\"):",
            "    should = lambda: math_ops.equal(global_step % n, 0)",
            "    if not context.executing_eagerly():",
            "      should = should()",
            "  return record_if(should)",
            "",
            "",
            "def always_record_summaries():",
            "  \"\"\"Sets the should_record_summaries Tensor to always true.\"\"\"",
            "  return record_if(True)",
            "",
            "",
            "def never_record_summaries():",
            "  \"\"\"Sets the should_record_summaries Tensor to always false.\"\"\"",
            "  return record_if(False)",
            "",
            "",
            "@tf_export(\"summary.experimental.get_step\", v1=[])",
            "def get_step():",
            "  \"\"\"Returns the default summary step for the current thread.",
            "",
            "  Returns:",
            "    The step set by `tf.summary.experimental.set_step()` if one has been set,",
            "    otherwise None.",
            "  \"\"\"",
            "  return _summary_state.step",
            "",
            "",
            "@tf_export(\"summary.experimental.set_step\", v1=[])",
            "def set_step(step):",
            "  \"\"\"Sets the default summary step for the current thread.",
            "",
            "  For convenience, this function sets a default value for the `step` parameter",
            "  used in summary-writing functions elsewhere in the API so that it need not",
            "  be explicitly passed in every such invocation. The value can be a constant",
            "  or a variable, and can be retrieved via `tf.summary.experimental.get_step()`.",
            "",
            "  Note: when using this with @tf.functions, the step value will be captured at",
            "  the time the function is traced, so changes to the step outside the function",
            "  will not be reflected inside the function unless using a `tf.Variable` step.",
            "",
            "  Args:",
            "    step: An `int64`-castable default step value, or None to unset.",
            "  \"\"\"",
            "  _summary_state.step = step",
            "",
            "",
            "@tf_export(\"summary.SummaryWriter\", v1=[])",
            "class SummaryWriter(metaclass=abc.ABCMeta):",
            "  \"\"\"Interface representing a stateful summary writer object.\"\"\"",
            "",
            "  def set_as_default(self, step=None):",
            "    \"\"\"Enables this summary writer for the current thread.",
            "",
            "    For convenience, if `step` is not None, this function also sets a default",
            "    value for the `step` parameter used in summary-writing functions elsewhere",
            "    in the API so that it need not be explicitly passed in every such",
            "    invocation. The value can be a constant or a variable.",
            "",
            "    Note: when setting `step` in a @tf.function, the step value will be",
            "    captured at the time the function is traced, so changes to the step outside",
            "    the function will not be reflected inside the function unless using",
            "    a `tf.Variable` step.",
            "",
            "    Args:",
            "      step: An `int64`-castable default step value, or `None`. When not `None`,",
            "        the current step is modified to the given value. When `None`, the",
            "        current step is not modified.",
            "    \"\"\"",
            "    self.as_default(step).__enter__()",
            "",
            "  def as_default(self, step=None):",
            "    \"\"\"Returns a context manager that enables summary writing.",
            "",
            "    For convenience, if `step` is not None, this function also sets a default",
            "    value for the `step` parameter used in summary-writing functions elsewhere",
            "    in the API so that it need not be explicitly passed in every such",
            "    invocation. The value can be a constant or a variable.",
            "",
            "    Note: when setting `step` in a @tf.function, the step value will be",
            "    captured at the time the function is traced, so changes to the step outside",
            "    the function will not be reflected inside the function unless using",
            "    a `tf.Variable` step.",
            "",
            "    For example, `step` can be used as:",
            "",
            "    ```python",
            "    with writer_a.as_default(step=10):",
            "      tf.summary.scalar(tag, value)   # Logged to writer_a with step 10",
            "      with writer_b.as_default(step=20):",
            "        tf.summary.scalar(tag, value) # Logged to writer_b with step 20",
            "      tf.summary.scalar(tag, value)   # Logged to writer_a with step 10",
            "    ```",
            "",
            "    Args:",
            "      step: An `int64`-castable default step value, or `None`. When not `None`,",
            "        the current step is captured, replaced by a given one, and the original",
            "        one is restored when the context manager exits. When `None`, the current",
            "        step is not modified (and not restored when the context manager exits).",
            "",
            "    Returns:",
            "      The context manager.",
            "    \"\"\"",
            "    return _SummaryContextManager(self, step)",
            "",
            "  def init(self):",
            "    \"\"\"Initializes the summary writer.\"\"\"",
            "    raise NotImplementedError()",
            "",
            "  def flush(self):",
            "    \"\"\"Flushes any buffered data.\"\"\"",
            "    raise NotImplementedError()",
            "",
            "  def close(self):",
            "    \"\"\"Flushes and closes the summary writer.\"\"\"",
            "    raise NotImplementedError()",
            "",
            "",
            "class _ResourceSummaryWriter(SummaryWriter):",
            "  \"\"\"Implementation of SummaryWriter using a SummaryWriterInterface resource.\"\"\"",
            "",
            "  def __init__(self, create_fn, init_op_fn):",
            "    self._resource = create_fn()",
            "    self._init_op = init_op_fn(self._resource)",
            "    self._closed = False",
            "    if context.executing_eagerly():",
            "      self._set_up_resource_deleter()",
            "    else:",
            "      ops.add_to_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME, self._init_op)",
            "",
            "  # Extension point to be overridden by subclasses to customize deletion.",
            "",
            "  def _set_up_resource_deleter(self):",
            "    self._resource_deleter = resource_variable_ops.EagerResourceDeleter(",
            "        handle=self._resource, handle_device=\"cpu:0\")",
            "",
            "  def set_as_default(self, step=None):",
            "    \"\"\"See `SummaryWriter.set_as_default`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      raise RuntimeError(f\"SummaryWriter {self!r} is already closed\")",
            "    super().set_as_default(step)",
            "",
            "  def as_default(self, step=None):",
            "    \"\"\"See `SummaryWriter.as_default`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      raise RuntimeError(f\"SummaryWriter {self!r} is already closed\")",
            "    return super().as_default(step)",
            "",
            "  def init(self):",
            "    \"\"\"See `SummaryWriter.init`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      raise RuntimeError(f\"SummaryWriter {self!r} is already closed\")",
            "    return self._init_op",
            "",
            "  def flush(self):",
            "    \"\"\"See `SummaryWriter.flush`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      return",
            "    with ops.device(\"cpu:0\"):",
            "      return gen_summary_ops.flush_summary_writer(self._resource)",
            "",
            "  def close(self):",
            "    \"\"\"See `SummaryWriter.close`.\"\"\"",
            "    if context.executing_eagerly() and self._closed:",
            "      return",
            "    try:",
            "      with ops.control_dependencies([self.flush()]):",
            "        with ops.device(\"cpu:0\"):",
            "          return gen_summary_ops.close_summary_writer(self._resource)",
            "    finally:",
            "      if context.executing_eagerly():",
            "        self._closed = True",
            "",
            "",
            "class _MultiMetaclass(",
            "    type(_ResourceSummaryWriter), type(tracking.TrackableResource)):",
            "  pass",
            "",
            "",
            "class _TrackableResourceSummaryWriter(",
            "    _ResourceSummaryWriter,",
            "    tracking.TrackableResource,",
            "    metaclass=_MultiMetaclass):",
            "  \"\"\"A `_ResourceSummaryWriter` subclass that implements `TrackableResource`.\"\"\"",
            "",
            "  def __init__(self, create_fn, init_op_fn):",
            "    # Resolve multiple inheritance via explicit calls to __init__() on parents.",
            "    tracking.TrackableResource.__init__(self, device=\"/CPU:0\")",
            "    self._create_fn = create_fn",
            "    self._init_op_fn = init_op_fn",
            "    # Pass .resource_handle into _ResourceSummaryWriter parent class rather than",
            "    # create_fn, to ensure it accesses the resource handle only through the",
            "    # cached property so that everything is using a single resource handle.",
            "    _ResourceSummaryWriter.__init__(",
            "        self, create_fn=lambda: self.resource_handle, init_op_fn=init_op_fn)",
            "",
            "  # Override for TrackableResource implementation.",
            "  def _create_resource(self):",
            "    return self._create_fn()",
            "",
            "  # Override for TrackableResource implementation.",
            "  def _initialize(self):",
            "    return self._init_op_fn(self.resource_handle)",
            "",
            "  # Override for TrackableResource implementation.",
            "  def _destroy_resource(self):",
            "    gen_resource_variable_ops.destroy_resource_op(",
            "        self.resource_handle, ignore_lookup_error=True)",
            "",
            "  def _set_up_resource_deleter(self):",
            "    # Override to suppress ResourceSummaryWriter implementation; we don't need",
            "    # the deleter since TrackableResource.__del__() handles it for us.",
            "    pass",
            "",
            "",
            "class _LegacyResourceSummaryWriter(SummaryWriter):",
            "  \"\"\"Legacy resource-backed SummaryWriter for tf.contrib.summary.\"\"\"",
            "",
            "  def  __init__(self, resource, init_op_fn):",
            "    self._resource = resource",
            "    self._init_op_fn = init_op_fn",
            "    init_op = self.init()",
            "    if context.executing_eagerly():",
            "      self._resource_deleter = resource_variable_ops.EagerResourceDeleter(",
            "          handle=self._resource, handle_device=\"cpu:0\")",
            "    else:",
            "      ops.add_to_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME, init_op)",
            "",
            "  def init(self):",
            "    \"\"\"See `SummaryWriter.init`.\"\"\"",
            "    return self._init_op_fn(self._resource)",
            "",
            "  def flush(self):",
            "    \"\"\"See `SummaryWriter.flush`.\"\"\"",
            "    with ops.device(\"cpu:0\"):",
            "      return gen_summary_ops.flush_summary_writer(self._resource)",
            "",
            "  def close(self):",
            "    \"\"\"See `SummaryWriter.close`.\"\"\"",
            "    with ops.control_dependencies([self.flush()]):",
            "      with ops.device(\"cpu:0\"):",
            "        return gen_summary_ops.close_summary_writer(self._resource)",
            "",
            "",
            "class _NoopSummaryWriter(SummaryWriter):",
            "  \"\"\"A summary writer that does nothing, for create_noop_writer().\"\"\"",
            "",
            "  def set_as_default(self, step=None):",
            "    pass",
            "",
            "  @tf_contextlib.contextmanager",
            "  def as_default(self, step=None):",
            "    yield",
            "",
            "  def init(self):",
            "    pass",
            "",
            "  def flush(self):",
            "    pass",
            "",
            "  def close(self):",
            "    pass",
            "",
            "",
            "@tf_export(v1=[\"summary.initialize\"])",
            "def initialize(",
            "    graph=None,  # pylint: disable=redefined-outer-name",
            "    session=None):",
            "  \"\"\"Initializes summary writing for graph execution mode.",
            "",
            "  This operation is a no-op when executing eagerly.",
            "",
            "  This helper method provides a higher-level alternative to using",
            "  `tf.contrib.summary.summary_writer_initializer_op` and",
            "  `tf.contrib.summary.graph`.",
            "",
            "  Most users will also want to call `tf.compat.v1.train.create_global_step`",
            "  which can happen before or after this function is called.",
            "",
            "  Args:",
            "    graph: A `tf.Graph` or `tf.compat.v1.GraphDef` to output to the writer.",
            "      This function will not write the default graph by default. When",
            "      writing to an event log file, the associated step will be zero.",
            "    session: So this method can call `tf.Session.run`. This defaults",
            "      to `tf.compat.v1.get_default_session`.",
            "",
            "  Raises:",
            "    RuntimeError: If  the current thread has no default",
            "      `tf.contrib.summary.SummaryWriter`.",
            "    ValueError: If session wasn't passed and no default session.",
            "  \"\"\"",
            "  if context.executing_eagerly():",
            "    return",
            "  if _summary_state.writer is None:",
            "    raise RuntimeError(\"No default tf.contrib.summary.SummaryWriter found\")",
            "  if session is None:",
            "    session = ops.get_default_session()",
            "    if session is None:",
            "      raise ValueError(\"Argument `session must be passed if no default \"",
            "                       \"session exists\")",
            "  session.run(summary_writer_initializer_op())",
            "  if graph is not None:",
            "    data = _serialize_graph(graph)",
            "    x = array_ops.placeholder(dtypes.string)",
            "    session.run(graph_v1(x, 0), feed_dict={x: data})",
            "",
            "",
            "@tf_export(\"summary.create_file_writer\", v1=[])",
            "def create_file_writer_v2(logdir,",
            "                          max_queue=None,",
            "                          flush_millis=None,",
            "                          filename_suffix=None,",
            "                          name=None,",
            "                          experimental_trackable=False):",
            "  \"\"\"Creates a summary file writer for the given log directory.",
            "",
            "  Args:",
            "    logdir: a string specifying the directory in which to write an event file.",
            "    max_queue: the largest number of summaries to keep in a queue; will",
            "     flush once the queue gets bigger than this. Defaults to 10.",
            "    flush_millis: the largest interval between flushes. Defaults to 120,000.",
            "    filename_suffix: optional suffix for the event file name. Defaults to `.v2`.",
            "    name: a name for the op that creates the writer.",
            "    experimental_trackable: a boolean that controls whether the returned writer",
            "      will be a `TrackableResource`, which makes it compatible with SavedModel",
            "      when used as a `tf.Module` property.",
            "",
            "  Returns:",
            "    A SummaryWriter object.",
            "  \"\"\"",
            "  if logdir is None:",
            "    raise ValueError(\"Argument `logdir` cannot be None\")",
            "  inside_function = ops.inside_function()",
            "  with ops.name_scope(name, \"create_file_writer\") as scope, ops.device(\"cpu:0\"):",
            "    # Run init inside an init_scope() to hoist it out of tf.functions.",
            "    with ops.init_scope():",
            "      if context.executing_eagerly():",
            "        _check_create_file_writer_args(",
            "            inside_function,",
            "            logdir=logdir,",
            "            max_queue=max_queue,",
            "            flush_millis=flush_millis,",
            "            filename_suffix=filename_suffix)",
            "      logdir = ops.convert_to_tensor(logdir, dtype=dtypes.string)",
            "      if max_queue is None:",
            "        max_queue = constant_op.constant(10)",
            "      if flush_millis is None:",
            "        flush_millis = constant_op.constant(2 * 60 * 1000)",
            "      if filename_suffix is None:",
            "        filename_suffix = constant_op.constant(\".v2\")",
            "",
            "      def create_fn():",
            "        # Use unique shared_name to prevent resource sharing in eager mode, but",
            "        # otherwise use a fixed shared_name to allow SavedModel TF 1.x loading.",
            "        if context.executing_eagerly():",
            "          shared_name = context.anonymous_name()",
            "        else:",
            "          shared_name = ops.name_from_scope_name(scope)  # pylint: disable=protected-access",
            "        return gen_summary_ops.summary_writer(",
            "            shared_name=shared_name, name=name)",
            "",
            "      init_op_fn = functools.partial(",
            "          gen_summary_ops.create_summary_file_writer,",
            "          logdir=logdir,",
            "          max_queue=max_queue,",
            "          flush_millis=flush_millis,",
            "          filename_suffix=filename_suffix)",
            "      if experimental_trackable:",
            "        return _TrackableResourceSummaryWriter(",
            "            create_fn=create_fn, init_op_fn=init_op_fn)",
            "      else:",
            "        return _ResourceSummaryWriter(",
            "            create_fn=create_fn, init_op_fn=init_op_fn)",
            "",
            "",
            "def create_file_writer(logdir,",
            "                       max_queue=None,",
            "                       flush_millis=None,",
            "                       filename_suffix=None,",
            "                       name=None):",
            "  \"\"\"Creates a summary file writer in the current context under the given name.",
            "",
            "  Args:",
            "    logdir: a string, or None. If a string, creates a summary file writer",
            "     which writes to the directory named by the string. If None, returns",
            "     a mock object which acts like a summary writer but does nothing,",
            "     useful to use as a context manager.",
            "    max_queue: the largest number of summaries to keep in a queue; will",
            "     flush once the queue gets bigger than this. Defaults to 10.",
            "    flush_millis: the largest interval between flushes. Defaults to 120,000.",
            "    filename_suffix: optional suffix for the event file name. Defaults to `.v2`.",
            "    name: Shared name for this SummaryWriter resource stored to default",
            "      Graph. Defaults to the provided logdir prefixed with `logdir:`. Note: if a",
            "      summary writer resource with this shared name already exists, the returned",
            "      SummaryWriter wraps that resource and the other arguments have no effect.",
            "",
            "  Returns:",
            "    Either a summary writer or an empty object which can be used as a",
            "    summary writer.",
            "  \"\"\"",
            "  if logdir is None:",
            "    return _NoopSummaryWriter()",
            "  logdir = str(logdir)",
            "  with ops.device(\"cpu:0\"):",
            "    if max_queue is None:",
            "      max_queue = constant_op.constant(10)",
            "    if flush_millis is None:",
            "      flush_millis = constant_op.constant(2 * 60 * 1000)",
            "    if filename_suffix is None:",
            "      filename_suffix = constant_op.constant(\".v2\")",
            "    if name is None:",
            "      name = \"logdir:\" + logdir",
            "    resource = gen_summary_ops.summary_writer(shared_name=name)",
            "    return _LegacyResourceSummaryWriter(",
            "        resource=resource,",
            "        init_op_fn=functools.partial(",
            "            gen_summary_ops.create_summary_file_writer,",
            "            logdir=logdir,",
            "            max_queue=max_queue,",
            "            flush_millis=flush_millis,",
            "            filename_suffix=filename_suffix))",
            "",
            "",
            "@tf_export(\"summary.create_noop_writer\", v1=[])",
            "def create_noop_writer():",
            "  \"\"\"Returns a summary writer that does nothing.",
            "",
            "  This is useful as a placeholder in code that expects a context manager.",
            "  \"\"\"",
            "  return _NoopSummaryWriter()",
            "",
            "",
            "def _cleanse_string(name, pattern, value):",
            "  if isinstance(value, str) and pattern.search(value) is None:",
            "    raise ValueError(f\"{name} ({value}) must match {pattern.pattern}\")",
            "  return ops.convert_to_tensor(value, dtypes.string)",
            "",
            "",
            "def _nothing():",
            "  \"\"\"Convenient else branch for when summaries do not record.\"\"\"",
            "  return constant_op.constant(False)",
            "",
            "",
            "@tf_export(v1=[\"summary.all_v2_summary_ops\"])",
            "def all_v2_summary_ops():",
            "  \"\"\"Returns all V2-style summary ops defined in the current default graph.",
            "",
            "  This includes ops from TF 2.0 tf.summary and TF 1.x tf.contrib.summary (except",
            "  for `tf.contrib.summary.graph` and `tf.contrib.summary.import_event`), but",
            "  does *not* include TF 1.x tf.summary ops.",
            "",
            "  Returns:",
            "    List of summary ops, or None if called under eager execution.",
            "  \"\"\"",
            "  if context.executing_eagerly():",
            "    return None",
            "  return ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access",
            "",
            "",
            "def summary_writer_initializer_op():",
            "  \"\"\"Graph-mode only. Returns the list of ops to create all summary writers.",
            "",
            "  Returns:",
            "    The initializer ops.",
            "",
            "  Raises:",
            "    RuntimeError: If in Eager mode.",
            "  \"\"\"",
            "  if context.executing_eagerly():",
            "    raise RuntimeError(",
            "        \"tf.contrib.summary.summary_writer_initializer_op is only \"",
            "        \"supported in graph mode.\")",
            "  return ops.get_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME)",
            "",
            "",
            "_INVALID_SCOPE_CHARACTERS = re.compile(r\"[^-_/.A-Za-z0-9]\")",
            "",
            "",
            "@tf_export(\"summary.experimental.summary_scope\", v1=[])",
            "@tf_contextlib.contextmanager",
            "def summary_scope(name, default_name=\"summary\", values=None):",
            "  \"\"\"Experimental context manager for use when defining a custom summary op.",
            "",
            "  This behaves similarly to `tf.name_scope`, except that it returns a generated",
            "  summary tag in addition to the scope name. The tag is structurally similar to",
            "  the scope name - derived from the user-provided name, prefixed with enclosing",
            "  name scopes if any - but we relax the constraint that it be uniquified, as",
            "  well as the character set limitation (so the user-provided name can contain",
            "  characters not legal for scope names; in the scope name these are removed).",
            "",
            "  This makes the summary tag more predictable and consistent for the user.",
            "",
            "  For example, to define a new summary op called `my_op`:",
            "",
            "  ```python",
            "  def my_op(name, my_value, step):",
            "    with tf.summary.summary_scope(name, \"MyOp\", [my_value]) as (tag, scope):",
            "      my_value = tf.convert_to_tensor(my_value)",
            "      return tf.summary.write(tag, my_value, step=step)",
            "  ```",
            "",
            "  Args:",
            "    name: string name for the summary.",
            "    default_name: Optional; if provided, used as default name of the summary.",
            "    values: Optional; passed as `values` parameter to name_scope.",
            "",
            "  Yields:",
            "    A tuple `(tag, scope)` as described above.",
            "  \"\"\"",
            "  name = name or default_name",
            "  current_scope = ops.get_name_scope()",
            "  tag = current_scope + \"/\" + name if current_scope else name",
            "  # Strip illegal characters from the scope name, and if that leaves nothing,",
            "  # use None instead so we pick up the default name.",
            "  name = _INVALID_SCOPE_CHARACTERS.sub(\"\", name) or None",
            "  with ops.name_scope(name, default_name, values, skip_on_eager=False) as scope:",
            "    yield tag, scope",
            "",
            "",
            "@tf_export(\"summary.write\", v1=[])",
            "def write(tag, tensor, step=None, metadata=None, name=None):",
            "  \"\"\"Writes a generic summary to the default SummaryWriter if one exists.",
            "",
            "  This exists primarily to support the definition of type-specific summary ops",
            "  like scalar() and image(), and is not intended for direct use unless defining",
            "  a new type-specific summary op.",
            "",
            "  Args:",
            "    tag: string tag used to identify the summary (e.g. in TensorBoard), usually",
            "      generated with `tf.summary.summary_scope`",
            "    tensor: the Tensor holding the summary data to write or a callable that",
            "      returns this Tensor. If a callable is passed, it will only be called when",
            "      a default SummaryWriter exists and the recording condition specified by",
            "      `record_if()` is met.",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "    metadata: Optional SummaryMetadata, as a proto or serialized bytes",
            "    name: Optional string name for this op.",
            "",
            "  Returns:",
            "    True on success, or false if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  with ops.name_scope(name, \"write_summary\") as scope:",
            "    if _summary_state.writer is None:",
            "      return constant_op.constant(False)",
            "    if step is None:",
            "      step = get_step()",
            "    if metadata is None:",
            "      serialized_metadata = b\"\"",
            "    elif hasattr(metadata, \"SerializeToString\"):",
            "      serialized_metadata = metadata.SerializeToString()",
            "    else:",
            "      serialized_metadata = metadata",
            "",
            "    def record():",
            "      \"\"\"Record the actual summary and return True.\"\"\"",
            "      if step is None:",
            "        raise ValueError(\"No step set. Please specify one either through the \"",
            "                         \"`step` argument or through \"",
            "                         \"tf.summary.experimental.set_step()\")",
            "",
            "      # Note the identity to move the tensor to the CPU.",
            "      with ops.device(\"cpu:0\"):",
            "        summary_tensor = tensor() if callable(tensor) else array_ops.identity(",
            "            tensor)",
            "        write_summary_op = gen_summary_ops.write_summary(",
            "            _summary_state.writer._resource,  # pylint: disable=protected-access",
            "            step,",
            "            summary_tensor,",
            "            tag,",
            "            serialized_metadata,",
            "            name=scope)",
            "        with ops.control_dependencies([write_summary_op]):",
            "          return constant_op.constant(True)",
            "",
            "    op = smart_cond.smart_cond(",
            "        should_record_summaries(), record, _nothing, name=\"summary_cond\")",
            "    if not context.executing_eagerly():",
            "      ops.add_to_collection(ops.GraphKeys._SUMMARY_COLLECTION, op)  # pylint: disable=protected-access",
            "    return op",
            "",
            "",
            "@tf_export(\"summary.experimental.write_raw_pb\", v1=[])",
            "def write_raw_pb(tensor, step=None, name=None):",
            "  \"\"\"Writes a summary using raw `tf.compat.v1.Summary` protocol buffers.",
            "",
            "  Experimental: this exists to support the usage of V1-style manual summary",
            "  writing (via the construction of a `tf.compat.v1.Summary` protocol buffer)",
            "  with the V2 summary writing API.",
            "",
            "  Args:",
            "    tensor: the string Tensor holding one or more serialized `Summary` protobufs",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "    name: Optional string name for this op.",
            "",
            "  Returns:",
            "    True on success, or false if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  with ops.name_scope(name, \"write_raw_pb\") as scope:",
            "    if _summary_state.writer is None:",
            "      return constant_op.constant(False)",
            "    if step is None:",
            "      step = get_step()",
            "      if step is None:",
            "        raise ValueError(\"No step set. Please specify one either through the \"",
            "                         \"`step` argument or through \"",
            "                         \"tf.summary.experimental.set_step()\")",
            "",
            "    def record():",
            "      \"\"\"Record the actual summary and return True.\"\"\"",
            "      # Note the identity to move the tensor to the CPU.",
            "      with ops.device(\"cpu:0\"):",
            "        raw_summary_op = gen_summary_ops.write_raw_proto_summary(",
            "            _summary_state.writer._resource,  # pylint: disable=protected-access",
            "            step,",
            "            array_ops.identity(tensor),",
            "            name=scope)",
            "        with ops.control_dependencies([raw_summary_op]):",
            "          return constant_op.constant(True)",
            "",
            "    with ops.device(\"cpu:0\"):",
            "      op = smart_cond.smart_cond(",
            "          should_record_summaries(), record, _nothing, name=\"summary_cond\")",
            "      if not context.executing_eagerly():",
            "        ops.add_to_collection(ops.GraphKeys._SUMMARY_COLLECTION, op)  # pylint: disable=protected-access",
            "      return op",
            "",
            "",
            "def summary_writer_function(name, tensor, function, family=None):",
            "  \"\"\"Helper function to write summaries.",
            "",
            "  Args:",
            "    name: name of the summary",
            "    tensor: main tensor to form the summary",
            "    function: function taking a tag and a scope which writes the summary",
            "    family: optional, the summary's family",
            "",
            "  Returns:",
            "    The result of writing the summary.",
            "  \"\"\"",
            "  name_scope = ops.get_name_scope()",
            "  if name_scope:",
            "    # Add a slash to allow reentering the name scope.",
            "    name_scope += \"/\"",
            "  def record():",
            "    with ops.name_scope(name_scope), summary_op_util.summary_scope(",
            "        name, family, values=[tensor]) as (tag, scope):",
            "      with ops.control_dependencies([function(tag, scope)]):",
            "        return constant_op.constant(True)",
            "",
            "  if _summary_state.writer is None:",
            "    return control_flow_ops.no_op()",
            "  with ops.device(\"cpu:0\"):",
            "    op = smart_cond.smart_cond(",
            "        _legacy_contrib_should_record_summaries(), record, _nothing, name=\"\")",
            "    if not context.executing_eagerly():",
            "      ops.add_to_collection(ops.GraphKeys._SUMMARY_COLLECTION, op)  # pylint: disable=protected-access",
            "  return op",
            "",
            "",
            "def generic(name, tensor, metadata=None, family=None, step=None):",
            "  \"\"\"Writes a tensor summary if possible.\"\"\"",
            "",
            "  def function(tag, scope):",
            "    if metadata is None:",
            "      serialized_metadata = constant_op.constant(\"\")",
            "    elif hasattr(metadata, \"SerializeToString\"):",
            "      serialized_metadata = constant_op.constant(metadata.SerializeToString())",
            "    else:",
            "      serialized_metadata = metadata",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        array_ops.identity(tensor),",
            "        tag,",
            "        serialized_metadata,",
            "        name=scope)",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def scalar(name, tensor, family=None, step=None):",
            "  \"\"\"Writes a scalar summary if possible.",
            "",
            "  Unlike `tf.contrib.summary.generic` this op may change the dtype",
            "  depending on the writer, for both practical and efficiency concerns.",
            "",
            "  Args:",
            "    name: An arbitrary name for this summary.",
            "    tensor: A `tf.Tensor` Must be one of the following types:",
            "      `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`,",
            "      `int8`, `uint16`, `half`, `uint32`, `uint64`.",
            "    family: Optional, the summary's family.",
            "    step: The `int64` monotonic step variable, which defaults",
            "      to `tf.compat.v1.train.get_global_step`.",
            "",
            "  Returns:",
            "    The created `tf.Operation` or a `tf.no_op` if summary writing has",
            "    not been enabled for this context.",
            "  \"\"\"",
            "",
            "  def function(tag, scope):",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_scalar_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        tag,",
            "        array_ops.identity(tensor),",
            "        name=scope)",
            "",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def histogram(name, tensor, family=None, step=None):",
            "  \"\"\"Writes a histogram summary if possible.\"\"\"",
            "",
            "  def function(tag, scope):",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_histogram_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        tag,",
            "        array_ops.identity(tensor),",
            "        name=scope)",
            "",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def image(name, tensor, bad_color=None, max_images=3, family=None, step=None):",
            "  \"\"\"Writes an image summary if possible.\"\"\"",
            "",
            "  def function(tag, scope):",
            "    bad_color_ = (constant_op.constant([255, 0, 0, 255], dtype=dtypes.uint8)",
            "                  if bad_color is None else bad_color)",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_image_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        tag,",
            "        array_ops.identity(tensor),",
            "        bad_color_,",
            "        max_images,",
            "        name=scope)",
            "",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def audio(name, tensor, sample_rate, max_outputs, family=None, step=None):",
            "  \"\"\"Writes an audio summary if possible.\"\"\"",
            "",
            "  def function(tag, scope):",
            "    # Note the identity to move the tensor to the CPU.",
            "    return gen_summary_ops.write_audio_summary(",
            "        _summary_state.writer._resource,  # pylint: disable=protected-access",
            "        _choose_step(step),",
            "        tag,",
            "        array_ops.identity(tensor),",
            "        sample_rate=sample_rate,",
            "        max_outputs=max_outputs,",
            "        name=scope)",
            "",
            "  return summary_writer_function(name, tensor, function, family=family)",
            "",
            "",
            "def graph_v1(param, step=None, name=None):",
            "  \"\"\"Writes a TensorFlow graph to the summary interface.",
            "",
            "  The graph summary is, strictly speaking, not a summary. Conditions",
            "  like `tf.summary.should_record_summaries` do not apply. Only",
            "  a single graph can be associated with a particular run. If multiple",
            "  graphs are written, then only the last one will be considered by",
            "  TensorBoard.",
            "",
            "  When not using eager execution mode, the user should consider passing",
            "  the `graph` parameter to `tf.compat.v1.summary.initialize` instead of",
            "  calling this function. Otherwise special care needs to be taken when",
            "  using the graph to record the graph.",
            "",
            "  Args:",
            "    param: A `tf.Tensor` containing a serialized graph proto. When",
            "      eager execution is enabled, this function will automatically",
            "      coerce `tf.Graph`, `tf.compat.v1.GraphDef`, and string types.",
            "    step: The global step variable. This doesn't have useful semantics",
            "      for graph summaries, but is used anyway, due to the structure of",
            "      event log files. This defaults to the global step.",
            "    name: A name for the operation (optional).",
            "",
            "  Returns:",
            "    The created `tf.Operation` or a `tf.no_op` if summary writing has",
            "    not been enabled for this context.",
            "",
            "  Raises:",
            "    TypeError: If `param` isn't already a `tf.Tensor` in graph mode.",
            "  \"\"\"",
            "  if not context.executing_eagerly() and not isinstance(param, ops.Tensor):",
            "    raise TypeError(\"graph() needs a argument `param` to be tf.Tensor \"",
            "                    \"(e.g. tf.placeholder) in graph mode, but received \"",
            "                    f\"param={param} of type {type(param).__name__}.\")",
            "  writer = _summary_state.writer",
            "  if writer is None:",
            "    return control_flow_ops.no_op()",
            "  with ops.device(\"cpu:0\"):",
            "    if isinstance(param, (ops.Graph, graph_pb2.GraphDef)):",
            "      tensor = ops.convert_to_tensor(_serialize_graph(param), dtypes.string)",
            "    else:",
            "      tensor = array_ops.identity(param)",
            "    return gen_summary_ops.write_graph_summary(",
            "        writer._resource, _choose_step(step), tensor, name=name)  # pylint: disable=protected-access",
            "",
            "",
            "@tf_export(\"summary.graph\", v1=[])",
            "def graph(graph_data):",
            "  \"\"\"Writes a TensorFlow graph summary.",
            "",
            "  Write an instance of `tf.Graph` or `tf.compat.v1.GraphDef` as summary only",
            "  in an eager mode. Please prefer to use the trace APIs (`tf.summary.trace_on`,",
            "  `tf.summary.trace_off`, and `tf.summary.trace_export`) when using",
            "  `tf.function` which can automatically collect and record graphs from",
            "  executions.",
            "",
            "  Usage Example:",
            "  ```py",
            "  writer = tf.summary.create_file_writer(\"/tmp/mylogs\")",
            "",
            "  @tf.function",
            "  def f():",
            "    x = constant_op.constant(2)",
            "    y = constant_op.constant(3)",
            "    return x**y",
            "",
            "  with writer.as_default():",
            "    tf.summary.graph(f.get_concrete_function().graph)",
            "",
            "  # Another example: in a very rare use case, when you are dealing with a TF v1",
            "  # graph.",
            "  graph = tf.Graph()",
            "  with graph.as_default():",
            "    c = tf.constant(30.0)",
            "  with writer.as_default():",
            "    tf.summary.graph(graph)",
            "  ```",
            "",
            "  Args:",
            "    graph_data: The TensorFlow graph to write, as a `tf.Graph` or a",
            "      `tf.compat.v1.GraphDef`.",
            "",
            "  Returns:",
            "    True on success, or False if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: `graph` summary API is invoked in a graph mode.",
            "  \"\"\"",
            "  if not context.executing_eagerly():",
            "    raise ValueError(\"graph() cannot be invoked inside a graph context.\")",
            "  writer = _summary_state.writer",
            "  if writer is None:",
            "    return constant_op.constant(False)",
            "  with ops.device(\"cpu:0\"):",
            "    if not should_record_summaries():",
            "      return constant_op.constant(False)",
            "",
            "    if isinstance(graph_data, (ops.Graph, graph_pb2.GraphDef)):",
            "      tensor = ops.convert_to_tensor(",
            "          _serialize_graph(graph_data), dtypes.string)",
            "    else:",
            "      raise ValueError(\"Argument 'graph_data' is not tf.Graph or \"",
            "                       \"tf.compat.v1.GraphDef. Received graph_data=\"",
            "                       f\"{graph_data} of type {type(graph_data).__name__}.\")",
            "",
            "    gen_summary_ops.write_graph_summary(",
            "        writer._resource,  # pylint: disable=protected-access",
            "        # Graph does not have step. Set to 0.",
            "        0,",
            "        tensor,",
            "    )",
            "    return constant_op.constant(True)",
            "",
            "",
            "def import_event(tensor, name=None):",
            "  \"\"\"Writes a `tf.compat.v1.Event` binary proto.",
            "",
            "  This can be used to import existing event logs into a new summary writer sink.",
            "  Please note that this is lower level than the other summary functions and",
            "  will ignore the `tf.summary.should_record_summaries` setting.",
            "",
            "  Args:",
            "    tensor: A `tf.Tensor` of type `string` containing a serialized",
            "      `tf.compat.v1.Event` proto.",
            "    name: A name for the operation (optional).",
            "",
            "  Returns:",
            "    The created `tf.Operation`.",
            "  \"\"\"",
            "  return gen_summary_ops.import_event(",
            "      _summary_state.writer._resource, tensor, name=name)  # pylint: disable=protected-access",
            "",
            "",
            "@tf_export(\"summary.flush\", v1=[])",
            "def flush(writer=None, name=None):",
            "  \"\"\"Forces summary writer to send any buffered data to storage.",
            "",
            "  This operation blocks until that finishes.",
            "",
            "  Args:",
            "    writer: The `tf.summary.SummaryWriter` to flush. If None, the current",
            "      default writer will be used instead; if there is no current writer, this",
            "      returns `tf.no_op`.",
            "    name: Ignored legacy argument for a name for the operation.",
            "",
            "  Returns:",
            "    The created `tf.Operation`.",
            "  \"\"\"",
            "  del name  # unused",
            "  if writer is None:",
            "    writer = _summary_state.writer",
            "    if writer is None:",
            "      return control_flow_ops.no_op()",
            "  if isinstance(writer, SummaryWriter):",
            "    return writer.flush()",
            "  raise ValueError(\"Invalid argument to flush(): %r\" % (writer,))",
            "",
            "",
            "def legacy_raw_flush(writer=None, name=None):",
            "  \"\"\"Legacy version of flush() that accepts a raw resource tensor for `writer`.",
            "",
            "  Do not use this function in any new code. Not supported and not part of the",
            "  public TF APIs.",
            "",
            "  Args:",
            "    writer: The `tf.summary.SummaryWriter` to flush. If None, the current",
            "      default writer will be used instead; if there is no current writer, this",
            "      returns `tf.no_op`. For this legacy version only, also accepts a raw",
            "      resource tensor pointing to the underlying C++ writer resource.",
            "    name: Ignored legacy argument for a name for the operation.",
            "",
            "  Returns:",
            "    The created `tf.Operation`.",
            "  \"\"\"",
            "  if writer is None or isinstance(writer, SummaryWriter):",
            "    # Forward to the TF2 implementation of flush() when possible.",
            "    return flush(writer, name)",
            "  else:",
            "    # Legacy fallback in case we were passed a raw resource tensor.",
            "    with ops.device(\"cpu:0\"):",
            "      return gen_summary_ops.flush_summary_writer(writer, name=name)",
            "",
            "",
            "def eval_dir(model_dir, name=None):",
            "  \"\"\"Construct a logdir for an eval summary writer.\"\"\"",
            "  return os.path.join(model_dir, \"eval\" if not name else \"eval_\" + name)",
            "",
            "",
            "@deprecation.deprecated(date=None,",
            "                        instructions=\"Renamed to create_file_writer().\")",
            "def create_summary_file_writer(*args, **kwargs):",
            "  \"\"\"Please use `tf.contrib.summary.create_file_writer`.\"\"\"",
            "  logging.warning(\"Deprecation Warning: create_summary_file_writer was renamed \"",
            "                  \"to create_file_writer\")",
            "  return create_file_writer(*args, **kwargs)",
            "",
            "",
            "def _serialize_graph(arbitrary_graph):",
            "  if isinstance(arbitrary_graph, ops.Graph):",
            "    return arbitrary_graph.as_graph_def(add_shapes=True).SerializeToString()",
            "  else:",
            "    return arbitrary_graph.SerializeToString()",
            "",
            "",
            "def _choose_step(step):",
            "  if step is None:",
            "    return training_util.get_or_create_global_step()",
            "  if not isinstance(step, ops.Tensor):",
            "    return ops.convert_to_tensor(step, dtypes.int64)",
            "  return step",
            "",
            "",
            "def _check_create_file_writer_args(inside_function, **kwargs):",
            "  \"\"\"Helper to check the validity of arguments to a create_file_writer() call.",
            "",
            "  Args:",
            "    inside_function: whether the create_file_writer() call is in a tf.function",
            "    **kwargs: the arguments to check, as kwargs to give them names.",
            "",
            "  Raises:",
            "    ValueError: if the arguments are graph tensors.",
            "  \"\"\"",
            "  for arg_name, arg in kwargs.items():",
            "    if not isinstance(arg, ops.EagerTensor) and tensor_util.is_tf_type(arg):",
            "      if inside_function:",
            "        raise ValueError(",
            "            f\"Invalid graph Tensor argument '{arg_name}={arg}' to \"",
            "            \"create_file_writer() inside an @tf.function. The create call will \"",
            "            \"be lifted into the outer eager execution context, so it cannot \"",
            "            \"consume graph tensors defined inside the function body.\")",
            "      else:",
            "        raise ValueError(",
            "            f\"Invalid graph Tensor argument '{arg_name}={arg}' to eagerly \"",
            "            \"executed create_file_writer().\")",
            "",
            "",
            "def run_metadata(name, data, step=None):",
            "  \"\"\"Writes entire RunMetadata summary.",
            "",
            "  A RunMetadata can contain DeviceStats, partition graphs, and function graphs.",
            "  Please refer to the proto for definition of each field.",
            "",
            "  Args:",
            "    name: A name for this summary. The summary tag used for TensorBoard will be",
            "      this name prefixed by any active name scopes.",
            "    data: A RunMetadata proto to write.",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "",
            "  Returns:",
            "    True on success, or false if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  summary_metadata = summary_pb2.SummaryMetadata()",
            "  # Hard coding a plugin name. Please refer to go/tb-plugin-name-hardcode for",
            "  # the rationale.",
            "  summary_metadata.plugin_data.plugin_name = \"graph_run_metadata\"",
            "  # version number = 1",
            "  summary_metadata.plugin_data.content = b\"1\"",
            "",
            "  with summary_scope(name,",
            "                     \"graph_run_metadata_summary\",",
            "                     [data, step]) as (tag, _):",
            "    with ops.device(\"cpu:0\"):",
            "      tensor = constant_op.constant(data.SerializeToString(),",
            "                                    dtype=dtypes.string)",
            "    return write(",
            "        tag=tag,",
            "        tensor=tensor,",
            "        step=step,",
            "        metadata=summary_metadata)",
            "",
            "",
            "def run_metadata_graphs(name, data, step=None):",
            "  \"\"\"Writes graphs from a RunMetadata summary.",
            "",
            "  Args:",
            "    name: A name for this summary. The summary tag used for TensorBoard will be",
            "      this name prefixed by any active name scopes.",
            "    data: A RunMetadata proto to write.",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "",
            "  Returns:",
            "    True on success, or false if no summary was written because no default",
            "    summary writer was available.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  summary_metadata = summary_pb2.SummaryMetadata()",
            "  # Hard coding a plugin name. Please refer to go/tb-plugin-name-hardcode for",
            "  # the rationale.",
            "  summary_metadata.plugin_data.plugin_name = \"graph_run_metadata_graph\"",
            "  # version number = 1",
            "  summary_metadata.plugin_data.content = b\"1\"",
            "",
            "  data = config_pb2.RunMetadata(",
            "      function_graphs=data.function_graphs,",
            "      partition_graphs=data.partition_graphs)",
            "",
            "  with summary_scope(name,",
            "                     \"graph_run_metadata_graph_summary\",",
            "                     [data, step]) as (tag, _):",
            "    with ops.device(\"cpu:0\"):",
            "      tensor = constant_op.constant(data.SerializeToString(),",
            "                                    dtype=dtypes.string)",
            "    return write(",
            "        tag=tag,",
            "        tensor=tensor,",
            "        step=step,",
            "        metadata=summary_metadata)",
            "",
            "",
            "_TraceContext = collections.namedtuple(\"TraceContext\", (\"graph\", \"profiler\"))",
            "_current_trace_context_lock = threading.Lock()",
            "_current_trace_context = None",
            "",
            "",
            "@tf_export(\"summary.trace_on\", v1=[])",
            "def trace_on(graph=True, profiler=False):  # pylint: disable=redefined-outer-name",
            "  \"\"\"Starts a trace to record computation graphs and profiling information.",
            "",
            "  Must be invoked in eager mode.",
            "",
            "  When enabled, TensorFlow runtime will collect information that can later be",
            "  exported and consumed by TensorBoard. The trace is activated across the entire",
            "  TensorFlow runtime and affects all threads of execution.",
            "",
            "  To stop the trace and export the collected information, use",
            "  `tf.summary.trace_export`. To stop the trace without exporting, use",
            "  `tf.summary.trace_off`.",
            "",
            "  Args:",
            "    graph: If True, enables collection of executed graphs. It includes ones from",
            "        tf.function invocation and ones from the legacy graph mode. The default",
            "        is True.",
            "    profiler: If True, enables the advanced profiler. Enabling profiler",
            "        implicitly enables the graph collection. The profiler may incur a high",
            "        memory overhead. The default is False.",
            "",
            "  \"\"\"",
            "  if ops.inside_function():",
            "    logging.warn(\"Cannot enable trace inside a tf.function.\")",
            "    return",
            "  if not context.executing_eagerly():",
            "    logging.warn(\"Must enable trace in eager mode.\")",
            "    return",
            "",
            "  global _current_trace_context",
            "  with _current_trace_context_lock:",
            "    if _current_trace_context:",
            "      logging.warn(\"Trace already enabled\")",
            "      return",
            "",
            "    if graph and not profiler:",
            "      context.context().enable_graph_collection()",
            "    if profiler:",
            "      context.context().enable_run_metadata()",
            "      _profiler.start()",
            "",
            "    _current_trace_context = _TraceContext(graph=graph, profiler=profiler)",
            "",
            "",
            "@tf_export(\"summary.trace_export\", v1=[])",
            "def trace_export(name, step=None, profiler_outdir=None):",
            "  \"\"\"Stops and exports the active trace as a Summary and/or profile file.",
            "",
            "  Stops the trace and exports all metadata collected during the trace to the",
            "  default SummaryWriter, if one has been set.",
            "",
            "  Args:",
            "    name: A name for the summary to be written.",
            "    step: Explicit `int64`-castable monotonic step value for this summary. If",
            "      omitted, this defaults to `tf.summary.experimental.get_step()`, which must",
            "      not be None.",
            "    profiler_outdir: Output directory for profiler. It is required when profiler",
            "      is enabled when trace was started. Otherwise, it is ignored.",
            "",
            "  Raises:",
            "    ValueError: if a default writer exists, but no step was provided and",
            "      `tf.summary.experimental.get_step()` is None.",
            "  \"\"\"",
            "  # TODO(stephanlee): See if we can remove profiler_outdir and infer it from",
            "  # the SummaryWriter's logdir.",
            "  global _current_trace_context",
            "",
            "  if ops.inside_function():",
            "    logging.warn(\"Cannot export trace inside a tf.function.\")",
            "    return",
            "  if not context.executing_eagerly():",
            "    logging.warn(\"Can only export trace while executing eagerly.\")",
            "    return",
            "",
            "  with _current_trace_context_lock:",
            "    if _current_trace_context is None:",
            "      raise ValueError(\"Must enable trace before export through \"",
            "                       \"tf.summary.trace_on.\")",
            "    graph, profiler = _current_trace_context  # pylint: disable=redefined-outer-name",
            "    if profiler and profiler_outdir is None:",
            "      raise ValueError(\"Argument `profiler_outdir` is not specified.\")",
            "",
            "  run_meta = context.context().export_run_metadata()",
            "",
            "  if graph and not profiler:",
            "    run_metadata_graphs(name, run_meta, step)",
            "  else:",
            "    run_metadata(name, run_meta, step)",
            "",
            "  if profiler:",
            "    _profiler.save(profiler_outdir, _profiler.stop())",
            "",
            "  trace_off()",
            "",
            "",
            "@tf_export(\"summary.trace_off\", v1=[])",
            "def trace_off():",
            "  \"\"\"Stops the current trace and discards any collected information.\"\"\"",
            "  global _current_trace_context",
            "  with _current_trace_context_lock:",
            "    if _current_trace_context is None:",
            "      return  # tracing already off",
            "    graph, profiler = _current_trace_context  # pylint: disable=redefined-outer-name, unpacking-non-sequence",
            "    _current_trace_context = None",
            "",
            "  if graph:",
            "    # Disabling run_metadata disables graph collection as well.",
            "    context.context().disable_run_metadata()",
            "",
            "  if profiler:",
            "    try:",
            "      _profiler.stop()",
            "    except _profiler.ProfilerNotRunningError:",
            "      pass"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.www.views.Airflow"
        ]
    }
}