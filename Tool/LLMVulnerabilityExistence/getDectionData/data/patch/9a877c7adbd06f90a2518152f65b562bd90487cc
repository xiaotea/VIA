{
    "libs/community/langchain_community/document_loaders/sitemap.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import itertools"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " import re"
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from typing import Any, Callable, Generator, Iterable, Iterator, List, Optional, Tuple"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3,
                "PatchRowcode": "+from typing import ("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 4,
                "PatchRowcode": "+    Any,"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 5,
                "PatchRowcode": "+    Callable,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 6,
                "PatchRowcode": "+    Dict,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+    Generator,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 8,
                "PatchRowcode": "+    Iterable,"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+    Iterator,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 10,
                "PatchRowcode": "+    List,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 11,
                "PatchRowcode": "+    Optional,"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 12,
                "PatchRowcode": "+    Tuple,"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 13,
                "PatchRowcode": "+)"
            },
            "14": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from urllib.parse import urlparse"
            },
            "15": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from langchain_core.documents import Document"
            },
            "17": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "         is_local: bool = False,"
            },
            "18": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "         continue_on_failure: bool = False,"
            },
            "19": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 87,
                "PatchRowcode": "         restrict_to_same_domain: bool = True,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+        max_depth: int = 10,"
            },
            "21": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "         **kwargs: Any,"
            },
            "22": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "     ):"
            },
            "23": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "         \"\"\"Initialize with webpage path and optional filter URLs."
            },
            "24": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "             restrict_to_same_domain: whether to restrict loading to URLs to the same"
            },
            "25": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "                 domain as the sitemap. Attention: This is only applied if the sitemap"
            },
            "26": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "                 is not a local file!"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+            max_depth: maximum depth to follow sitemap links. Default: 10"
            },
            "28": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "         \"\"\""
            },
            "29": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 121,
                "PatchRowcode": " "
            },
            "30": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 122,
                "PatchRowcode": "         if blocksize is not None and blocksize < 1:"
            },
            "31": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": 146,
                "PatchRowcode": "         self.blocknum = blocknum"
            },
            "32": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": 147,
                "PatchRowcode": "         self.is_local = is_local"
            },
            "33": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 148,
                "PatchRowcode": "         self.continue_on_failure = continue_on_failure"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 149,
                "PatchRowcode": "+        self.max_depth = max_depth"
            },
            "35": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": 150,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def parse_sitemap(self, soup: Any) -> List[dict]:"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+    def parse_sitemap(self, soup: Any, *, depth: int = 0) -> List[dict]:"
            },
            "38": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "         \"\"\"Parse sitemap xml and load into a list of dicts."
            },
            "39": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": 153,
                "PatchRowcode": " "
            },
            "40": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "         Args:"
            },
            "41": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "             soup: BeautifulSoup object."
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 156,
                "PatchRowcode": "+            depth: current depth of the sitemap. Default: 0"
            },
            "43": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": 157,
                "PatchRowcode": " "
            },
            "44": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 158,
                "PatchRowcode": "         Returns:"
            },
            "45": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": 159,
                "PatchRowcode": "             List of dicts."
            },
            "46": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 160,
                "PatchRowcode": "         \"\"\""
            },
            "47": {
                "beforePatchRowNumber": 147,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        els = []"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 161,
                "PatchRowcode": "+        if depth >= self.max_depth:"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 162,
                "PatchRowcode": "+            return []"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 163,
                "PatchRowcode": "+"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 164,
                "PatchRowcode": "+        els: List[Dict] = []"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 165,
                "PatchRowcode": "+"
            },
            "53": {
                "beforePatchRowNumber": 148,
                "afterPatchRowNumber": 166,
                "PatchRowcode": "         for url in soup.find_all(\"url\"):"
            },
            "54": {
                "beforePatchRowNumber": 149,
                "afterPatchRowNumber": 167,
                "PatchRowcode": "             loc = url.find(\"loc\")"
            },
            "55": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 168,
                "PatchRowcode": "             if not loc:"
            },
            "56": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": 195,
                "PatchRowcode": "             loc = sitemap.find(\"loc\")"
            },
            "57": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": 196,
                "PatchRowcode": "             if not loc:"
            },
            "58": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 197,
                "PatchRowcode": "                 continue"
            },
            "59": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            soup_child = self.scrape_all([loc.text], \"xml\")[0]"
            },
            "60": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 198,
                "PatchRowcode": " "
            },
            "61": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            els.extend(self.parse_sitemap(soup_child))"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+            soup_child = self.scrape_all([loc.text], \"xml\")[0]"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+            els.extend(self.parse_sitemap(soup_child, depth=depth + 1))"
            },
            "64": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 201,
                "PatchRowcode": "         return els"
            },
            "65": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 202,
                "PatchRowcode": " "
            },
            "66": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 203,
                "PatchRowcode": "     def lazy_load(self) -> Iterator[Document]:"
            }
        },
        "frontPatchFile": [
            "import itertools",
            "import re",
            "from typing import Any, Callable, Generator, Iterable, Iterator, List, Optional, Tuple",
            "from urllib.parse import urlparse",
            "",
            "from langchain_core.documents import Document",
            "",
            "from langchain_community.document_loaders.web_base import WebBaseLoader",
            "",
            "",
            "def _default_parsing_function(content: Any) -> str:",
            "    return str(content.get_text())",
            "",
            "",
            "def _default_meta_function(meta: dict, _content: Any) -> dict:",
            "    return {\"source\": meta[\"loc\"], **meta}",
            "",
            "",
            "def _batch_block(iterable: Iterable, size: int) -> Generator[List[dict], None, None]:",
            "    it = iter(iterable)",
            "    while item := list(itertools.islice(it, size)):",
            "        yield item",
            "",
            "",
            "def _extract_scheme_and_domain(url: str) -> Tuple[str, str]:",
            "    \"\"\"Extract the scheme + domain from a given URL.",
            "",
            "    Args:",
            "        url (str): The input URL.",
            "",
            "    Returns:",
            "        return a 2-tuple of scheme and domain",
            "    \"\"\"",
            "    parsed_uri = urlparse(url)",
            "    return parsed_uri.scheme, parsed_uri.netloc",
            "",
            "",
            "class SitemapLoader(WebBaseLoader):",
            "    \"\"\"Load a sitemap and its URLs.",
            "",
            "    **Security Note**: This loader can be used to load all URLs specified in a sitemap.",
            "        If a malicious actor gets access to the sitemap, they could force",
            "        the server to load URLs from other domains by modifying the sitemap.",
            "        This could lead to server-side request forgery (SSRF) attacks; e.g.,",
            "        with the attacker forcing the server to load URLs from internal",
            "        service endpoints that are not publicly accessible. While the attacker",
            "        may not immediately gain access to this data, this data could leak",
            "        into downstream systems (e.g., data loader is used to load data for indexing).",
            "",
            "        This loader is a crawler and web crawlers should generally NOT be deployed",
            "        with network access to any internal servers.",
            "",
            "        Control access to who can submit crawling requests and what network access",
            "        the crawler has.",
            "",
            "        By default, the loader will only load URLs from the same domain as the sitemap",
            "        if the site map is not a local file. This can be disabled by setting",
            "        restrict_to_same_domain to False (not recommended).",
            "",
            "        If the site map is a local file, no such risk mitigation is applied by default.",
            "",
            "        Use the filter URLs argument to limit which URLs can be loaded.",
            "",
            "        See https://python.langchain.com/docs/security",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        web_path: str,",
            "        filter_urls: Optional[List[str]] = None,",
            "        parsing_function: Optional[Callable] = None,",
            "        blocksize: Optional[int] = None,",
            "        blocknum: int = 0,",
            "        meta_function: Optional[Callable] = None,",
            "        is_local: bool = False,",
            "        continue_on_failure: bool = False,",
            "        restrict_to_same_domain: bool = True,",
            "        **kwargs: Any,",
            "    ):",
            "        \"\"\"Initialize with webpage path and optional filter URLs.",
            "",
            "        Args:",
            "            web_path: url of the sitemap. can also be a local path",
            "            filter_urls: a list of regexes. If specified, only",
            "                URLS that match one of the filter URLs will be loaded.",
            "                *WARNING* The filter URLs are interpreted as regular expressions.",
            "                Remember to escape special characters if you do not want them to be",
            "                interpreted as regular expression syntax. For example, `.` appears",
            "                frequently in URLs and should be escaped if you want to match a literal",
            "                `.` rather than any character.",
            "                restrict_to_same_domain takes precedence over filter_urls when",
            "                restrict_to_same_domain is True and the sitemap is not a local file.",
            "            parsing_function: Function to parse bs4.Soup output",
            "            blocksize: number of sitemap locations per block",
            "            blocknum: the number of the block that should be loaded - zero indexed.",
            "                Default: 0",
            "            meta_function: Function to parse bs4.Soup output for metadata",
            "                remember when setting this method to also copy metadata[\"loc\"]",
            "                to metadata[\"source\"] if you are using this field",
            "            is_local: whether the sitemap is a local file. Default: False",
            "            continue_on_failure: whether to continue loading the sitemap if an error",
            "                occurs loading a url, emitting a warning instead of raising an",
            "                exception. Setting this to True makes the loader more robust, but also",
            "                may result in missing data. Default: False",
            "            restrict_to_same_domain: whether to restrict loading to URLs to the same",
            "                domain as the sitemap. Attention: This is only applied if the sitemap",
            "                is not a local file!",
            "        \"\"\"",
            "",
            "        if blocksize is not None and blocksize < 1:",
            "            raise ValueError(\"Sitemap blocksize should be at least 1\")",
            "",
            "        if blocknum < 0:",
            "            raise ValueError(\"Sitemap blocknum can not be lower then 0\")",
            "",
            "        try:",
            "            import lxml  # noqa:F401",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"lxml package not found, please install it with `pip install lxml`\"",
            "            )",
            "",
            "        super().__init__(web_paths=[web_path], **kwargs)",
            "",
            "        # Define a list of URL patterns (interpreted as regular expressions) that",
            "        # will be allowed to be loaded.",
            "        # restrict_to_same_domain takes precedence over filter_urls when",
            "        # restrict_to_same_domain is True and the sitemap is not a local file.",
            "        self.allow_url_patterns = filter_urls",
            "        self.restrict_to_same_domain = restrict_to_same_domain",
            "        self.parsing_function = parsing_function or _default_parsing_function",
            "        self.meta_function = meta_function or _default_meta_function",
            "        self.blocksize = blocksize",
            "        self.blocknum = blocknum",
            "        self.is_local = is_local",
            "        self.continue_on_failure = continue_on_failure",
            "",
            "    def parse_sitemap(self, soup: Any) -> List[dict]:",
            "        \"\"\"Parse sitemap xml and load into a list of dicts.",
            "",
            "        Args:",
            "            soup: BeautifulSoup object.",
            "",
            "        Returns:",
            "            List of dicts.",
            "        \"\"\"",
            "        els = []",
            "        for url in soup.find_all(\"url\"):",
            "            loc = url.find(\"loc\")",
            "            if not loc:",
            "                continue",
            "",
            "            # Strip leading and trailing whitespace and newlines",
            "            loc_text = loc.text.strip()",
            "",
            "            if self.restrict_to_same_domain and not self.is_local:",
            "                if _extract_scheme_and_domain(loc_text) != _extract_scheme_and_domain(",
            "                    self.web_path",
            "                ):",
            "                    continue",
            "",
            "            if self.allow_url_patterns and not any(",
            "                re.match(regexp_pattern, loc_text)",
            "                for regexp_pattern in self.allow_url_patterns",
            "            ):",
            "                continue",
            "",
            "            els.append(",
            "                {",
            "                    tag: prop.text",
            "                    for tag in [\"loc\", \"lastmod\", \"changefreq\", \"priority\"]",
            "                    if (prop := url.find(tag))",
            "                }",
            "            )",
            "",
            "        for sitemap in soup.find_all(\"sitemap\"):",
            "            loc = sitemap.find(\"loc\")",
            "            if not loc:",
            "                continue",
            "            soup_child = self.scrape_all([loc.text], \"xml\")[0]",
            "",
            "            els.extend(self.parse_sitemap(soup_child))",
            "        return els",
            "",
            "    def lazy_load(self) -> Iterator[Document]:",
            "        \"\"\"Load sitemap.\"\"\"",
            "        if self.is_local:",
            "            try:",
            "                import bs4",
            "            except ImportError:",
            "                raise ImportError(",
            "                    \"beautifulsoup4 package not found, please install it\"",
            "                    \" with `pip install beautifulsoup4`\"",
            "                )",
            "            fp = open(self.web_path)",
            "            soup = bs4.BeautifulSoup(fp, \"xml\")",
            "        else:",
            "            soup = self._scrape(self.web_path, parser=\"xml\")",
            "",
            "        els = self.parse_sitemap(soup)",
            "",
            "        if self.blocksize is not None:",
            "            elblocks = list(_batch_block(els, self.blocksize))",
            "            blockcount = len(elblocks)",
            "            if blockcount - 1 < self.blocknum:",
            "                raise ValueError(",
            "                    \"Selected sitemap does not contain enough blocks for given blocknum\"",
            "                )",
            "            else:",
            "                els = elblocks[self.blocknum]",
            "",
            "        results = self.scrape_all([el[\"loc\"].strip() for el in els if \"loc\" in el])",
            "",
            "        for i, result in enumerate(results):",
            "            yield Document(",
            "                page_content=self.parsing_function(result),",
            "                metadata=self.meta_function(els[i], result),",
            "            )"
        ],
        "afterPatchFile": [
            "import itertools",
            "import re",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    Generator,",
            "    Iterable,",
            "    Iterator,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            ")",
            "from urllib.parse import urlparse",
            "",
            "from langchain_core.documents import Document",
            "",
            "from langchain_community.document_loaders.web_base import WebBaseLoader",
            "",
            "",
            "def _default_parsing_function(content: Any) -> str:",
            "    return str(content.get_text())",
            "",
            "",
            "def _default_meta_function(meta: dict, _content: Any) -> dict:",
            "    return {\"source\": meta[\"loc\"], **meta}",
            "",
            "",
            "def _batch_block(iterable: Iterable, size: int) -> Generator[List[dict], None, None]:",
            "    it = iter(iterable)",
            "    while item := list(itertools.islice(it, size)):",
            "        yield item",
            "",
            "",
            "def _extract_scheme_and_domain(url: str) -> Tuple[str, str]:",
            "    \"\"\"Extract the scheme + domain from a given URL.",
            "",
            "    Args:",
            "        url (str): The input URL.",
            "",
            "    Returns:",
            "        return a 2-tuple of scheme and domain",
            "    \"\"\"",
            "    parsed_uri = urlparse(url)",
            "    return parsed_uri.scheme, parsed_uri.netloc",
            "",
            "",
            "class SitemapLoader(WebBaseLoader):",
            "    \"\"\"Load a sitemap and its URLs.",
            "",
            "    **Security Note**: This loader can be used to load all URLs specified in a sitemap.",
            "        If a malicious actor gets access to the sitemap, they could force",
            "        the server to load URLs from other domains by modifying the sitemap.",
            "        This could lead to server-side request forgery (SSRF) attacks; e.g.,",
            "        with the attacker forcing the server to load URLs from internal",
            "        service endpoints that are not publicly accessible. While the attacker",
            "        may not immediately gain access to this data, this data could leak",
            "        into downstream systems (e.g., data loader is used to load data for indexing).",
            "",
            "        This loader is a crawler and web crawlers should generally NOT be deployed",
            "        with network access to any internal servers.",
            "",
            "        Control access to who can submit crawling requests and what network access",
            "        the crawler has.",
            "",
            "        By default, the loader will only load URLs from the same domain as the sitemap",
            "        if the site map is not a local file. This can be disabled by setting",
            "        restrict_to_same_domain to False (not recommended).",
            "",
            "        If the site map is a local file, no such risk mitigation is applied by default.",
            "",
            "        Use the filter URLs argument to limit which URLs can be loaded.",
            "",
            "        See https://python.langchain.com/docs/security",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        web_path: str,",
            "        filter_urls: Optional[List[str]] = None,",
            "        parsing_function: Optional[Callable] = None,",
            "        blocksize: Optional[int] = None,",
            "        blocknum: int = 0,",
            "        meta_function: Optional[Callable] = None,",
            "        is_local: bool = False,",
            "        continue_on_failure: bool = False,",
            "        restrict_to_same_domain: bool = True,",
            "        max_depth: int = 10,",
            "        **kwargs: Any,",
            "    ):",
            "        \"\"\"Initialize with webpage path and optional filter URLs.",
            "",
            "        Args:",
            "            web_path: url of the sitemap. can also be a local path",
            "            filter_urls: a list of regexes. If specified, only",
            "                URLS that match one of the filter URLs will be loaded.",
            "                *WARNING* The filter URLs are interpreted as regular expressions.",
            "                Remember to escape special characters if you do not want them to be",
            "                interpreted as regular expression syntax. For example, `.` appears",
            "                frequently in URLs and should be escaped if you want to match a literal",
            "                `.` rather than any character.",
            "                restrict_to_same_domain takes precedence over filter_urls when",
            "                restrict_to_same_domain is True and the sitemap is not a local file.",
            "            parsing_function: Function to parse bs4.Soup output",
            "            blocksize: number of sitemap locations per block",
            "            blocknum: the number of the block that should be loaded - zero indexed.",
            "                Default: 0",
            "            meta_function: Function to parse bs4.Soup output for metadata",
            "                remember when setting this method to also copy metadata[\"loc\"]",
            "                to metadata[\"source\"] if you are using this field",
            "            is_local: whether the sitemap is a local file. Default: False",
            "            continue_on_failure: whether to continue loading the sitemap if an error",
            "                occurs loading a url, emitting a warning instead of raising an",
            "                exception. Setting this to True makes the loader more robust, but also",
            "                may result in missing data. Default: False",
            "            restrict_to_same_domain: whether to restrict loading to URLs to the same",
            "                domain as the sitemap. Attention: This is only applied if the sitemap",
            "                is not a local file!",
            "            max_depth: maximum depth to follow sitemap links. Default: 10",
            "        \"\"\"",
            "",
            "        if blocksize is not None and blocksize < 1:",
            "            raise ValueError(\"Sitemap blocksize should be at least 1\")",
            "",
            "        if blocknum < 0:",
            "            raise ValueError(\"Sitemap blocknum can not be lower then 0\")",
            "",
            "        try:",
            "            import lxml  # noqa:F401",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"lxml package not found, please install it with `pip install lxml`\"",
            "            )",
            "",
            "        super().__init__(web_paths=[web_path], **kwargs)",
            "",
            "        # Define a list of URL patterns (interpreted as regular expressions) that",
            "        # will be allowed to be loaded.",
            "        # restrict_to_same_domain takes precedence over filter_urls when",
            "        # restrict_to_same_domain is True and the sitemap is not a local file.",
            "        self.allow_url_patterns = filter_urls",
            "        self.restrict_to_same_domain = restrict_to_same_domain",
            "        self.parsing_function = parsing_function or _default_parsing_function",
            "        self.meta_function = meta_function or _default_meta_function",
            "        self.blocksize = blocksize",
            "        self.blocknum = blocknum",
            "        self.is_local = is_local",
            "        self.continue_on_failure = continue_on_failure",
            "        self.max_depth = max_depth",
            "",
            "    def parse_sitemap(self, soup: Any, *, depth: int = 0) -> List[dict]:",
            "        \"\"\"Parse sitemap xml and load into a list of dicts.",
            "",
            "        Args:",
            "            soup: BeautifulSoup object.",
            "            depth: current depth of the sitemap. Default: 0",
            "",
            "        Returns:",
            "            List of dicts.",
            "        \"\"\"",
            "        if depth >= self.max_depth:",
            "            return []",
            "",
            "        els: List[Dict] = []",
            "",
            "        for url in soup.find_all(\"url\"):",
            "            loc = url.find(\"loc\")",
            "            if not loc:",
            "                continue",
            "",
            "            # Strip leading and trailing whitespace and newlines",
            "            loc_text = loc.text.strip()",
            "",
            "            if self.restrict_to_same_domain and not self.is_local:",
            "                if _extract_scheme_and_domain(loc_text) != _extract_scheme_and_domain(",
            "                    self.web_path",
            "                ):",
            "                    continue",
            "",
            "            if self.allow_url_patterns and not any(",
            "                re.match(regexp_pattern, loc_text)",
            "                for regexp_pattern in self.allow_url_patterns",
            "            ):",
            "                continue",
            "",
            "            els.append(",
            "                {",
            "                    tag: prop.text",
            "                    for tag in [\"loc\", \"lastmod\", \"changefreq\", \"priority\"]",
            "                    if (prop := url.find(tag))",
            "                }",
            "            )",
            "",
            "        for sitemap in soup.find_all(\"sitemap\"):",
            "            loc = sitemap.find(\"loc\")",
            "            if not loc:",
            "                continue",
            "",
            "            soup_child = self.scrape_all([loc.text], \"xml\")[0]",
            "            els.extend(self.parse_sitemap(soup_child, depth=depth + 1))",
            "        return els",
            "",
            "    def lazy_load(self) -> Iterator[Document]:",
            "        \"\"\"Load sitemap.\"\"\"",
            "        if self.is_local:",
            "            try:",
            "                import bs4",
            "            except ImportError:",
            "                raise ImportError(",
            "                    \"beautifulsoup4 package not found, please install it\"",
            "                    \" with `pip install beautifulsoup4`\"",
            "                )",
            "            fp = open(self.web_path)",
            "            soup = bs4.BeautifulSoup(fp, \"xml\")",
            "        else:",
            "            soup = self._scrape(self.web_path, parser=\"xml\")",
            "",
            "        els = self.parse_sitemap(soup)",
            "",
            "        if self.blocksize is not None:",
            "            elblocks = list(_batch_block(els, self.blocksize))",
            "            blockcount = len(elblocks)",
            "            if blockcount - 1 < self.blocknum:",
            "                raise ValueError(",
            "                    \"Selected sitemap does not contain enough blocks for given blocknum\"",
            "                )",
            "            else:",
            "                els = elblocks[self.blocknum]",
            "",
            "        results = self.scrape_all([el[\"loc\"].strip() for el in els if \"loc\" in el])",
            "",
            "        for i, result in enumerate(results):",
            "            yield Document(",
            "                page_content=self.parsing_function(result),",
            "                metadata=self.meta_function(els[i], result),",
            "            )"
        ],
        "action": [
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "3": [],
            "138": [
                "SitemapLoader",
                "parse_sitemap"
            ],
            "147": [
                "SitemapLoader",
                "parse_sitemap"
            ],
            "180": [
                "SitemapLoader",
                "parse_sitemap"
            ],
            "182": [
                "SitemapLoader",
                "parse_sitemap"
            ]
        },
        "addLocation": []
    }
}